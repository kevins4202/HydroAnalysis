index,text
26115,expert opinion is increasingly being used to inform bayesian belief networks in particular to define the conditional dependencies modelled by the graphical structure the elicitation of such expert opinion remains a major challenge due to both the quantity of information required and the ability of experts to quantify subjective beliefs effectively in this work we introduce a method designed to initialise conditional probability tables based on a small number of simple questions that capture the overall shape of a conditional probability distribution before enabling the expert to refine their results in an efficient way these methods have been incorporated into a software application for conditional probability elicitation ace freely available at https github com kirstylhassall ace hassall 2019 keywords bayes nets belief networks expert elicitation conditional probability tables software availability all methods described in this paper have been incorporated into an open source software application for conditional probability elicitation ace freely available at https github com kirstylhassall ace hassall 2019 1 introduction bayesian belief nets also referred to as bayes nets belief networks or often simply bbns have in recent years seen a dramatic increase in their use for describing and modelling natural systems examples include quantifying the risk of erosion in peat bogs aalders et al 2011 modelling ecosystem services haines young 2011 applications in natural resource management see 13 and references therein mapping risks of soil threats such as soil compaction troldborg et al 2013 predicting soil bulk density at landscape scales taalab et al 2015 and assessing the impact of buffer zones on water protection and biodiversity tattari et al 2003 this explosion in practical bbn modelling may in part be due to the relative simplicity of the intuitive graphical representation of multiple interrelated variables captured through conditional probabilities and more practically the increasing accessibility to specialist bbn software there has been much work in the development of bbn methodology to address the practicalities of bbn modelling see for example 20 bbn modelling largely consists of two interrelated steps defining the graphical structure and quantifying the form of the conditional dependencies through conditional probability distributions in each step one can incorporate both data and expert opinion pollino et al 2007 aalders et al 2011 our interest is to define a fully parameterised bbn that quantifies soil health by capturing the inherent knowledge of experts representative of all aspects of soil science such as soil microbiology soil chemistry soil physics and land management among others a full description of the construction and evolution of these soil health network structures will be the topic of a future paper although an early example is shown in fig 2 for demonstrative purposes in this paper we focus on the key issues surrounding the use of expert opinion in the characterisation of conditional probability tables 1 1 conditional probability tables or cpts described fully in section 1 1 1 parameterise a bbn by quantifying the relationship between variables bbns first saw an increase in popularity in the 1980s with applications to decision support and expert systems see for example 19 22 whereby a causal network description is used to express expert knowledge that can inform diagnoses and decisions in this work our motivation for using bbns differs from both the practical modelling approaches prolific in the literature and also the traditional usage in expert systems specifically there is an increasing desire to be able to derive and quantify metrics for often subjective concepts an example of such is soil quality and health which is a term frequently used albeit qualitatively and subjectively wienhold et al 2004 doran and zeiss 2000 definitions of soil quality and health vary in the literature but include the condition or state of soil relative to the requirements of one or more biotic species and or to any human need or purpose johnson et al 1997 the capacity of a specific kind of soil to function within natural or managed ecosystem boundaries to sustain plant and animal productivity maintain or enhance water and air quality and support human health and habitation friedman et al 2001 karlen et al 1997 the soil s ability to provide ecosystem and social services through its capacities to perform its functions under changing conditions tóth et al 2007 since these definitions include inherently subjective concepts quantifying them cannot be done without the use of expert opinion bbns are a natural framework for incorporating both expert opinion and data to conceptualise different model systems see for example 1 24 a bbn is a graphical model made up of nodes representing variables of interest and edges representing direct dependencies between variables specifically each edge represents a conditional distribution with nodes that are not connected considered to be conditionally independent given all other nodes in the network bbns are more precisely directed acyclic graphs dags meaning each edge has a direction and there are no feedback loops in the network the direction of each arrow represents the direction of conditioning and the node at the source of the arrow is referred to as a parent and the node at the sink of the arrow the child the joint distribution of all the variables within the graphical model can be represented as the product of the conditional distributions f x 1 x p x 1 x p i f x i parents x i x i parents x i where parents x i denotes the set of nodes connected by a directed edge to the node i for example fig 1 shows three nodes x y z connected by two edges where x and y are conditionally independent given the parent z moreover there exists a dependency between x and z and between y and z the graph shown in fig 1 has joint distribution f x y z x y z which can be decomposed as f x y z x y z f x z x z f y z y z f z z although the distribution of each node in a bbn can be general for the remainder of this paper we consider only discrete bbns where all random variables x 1 x p are categorical however we note here a discrepancy in the literature regarding bbns and the term bayesian korb and nicholson 2004 the notion of a prior in a bbn often refers to the distribution of an ancestral node this does not preclude the use of expert derived opinion but rather the opinions or beliefs are used to directly inform the conditional distribution of each node this differs from what we might term the truly bayesian approach which would consider prior information to be included through a hyperdistribution over the parameters of the node distribution i e the likelihood of node x i f x i parents x i x i parents x i θ is defined through the parameters θ prior information is defined through the distribution over θ and interest is in deriving a posterior distribution for θ x x more explicitly for a discrete node x i with a categorical multinomial distribution defined by a set of probabilities a prior distribution is defined over this set of probabilities by e g a dirichlet distribution in this way expert opinion or belief would be used to parameterise the dirichlet prior which when combined with the likelihood gives the posterior as with most practical bbns our focus is on the direct frequency representation and not the fully bayesian approach 1 1 example bbns throughout the remainder of this paper we will demonstrate our methods on the following bbn application shown in fig 3 this is an illustrative example aiming to define the concept of road safety this example was developed predominantly to aid in the exposition of bayesian belief networks to the subject specific experts we approached to define soil health and quality as an aside introducing the topic of bbns with an example unrelated to the topic we wished to focus on i e soil health was useful to put across the main concepts e g of conditional dependence and conditional independence without prejudicing the question at hand fig 3 shows how the subjective notion of road safety can be defined by four variables the presence of cycle lanes whether the road is on a main school route the number of car crashes and the number of fatalities furthermore the net shows how if data are not available on the number of car crashes we can infer this from other causally related variables such as the speed limit and weather conditions moreover our example stresses the point that graphical representations do not need to be causal specifically we include the somewhat artificial example that if data were available on the number of umbrella sales this could be used to infer the weather conditions which in turn can be used to infer the number of car crashes 1 1 1 conditional probability tables the conditional probability table cpt describes the distribution of the child node for every combination of states of the parent nodes for example shown in table 1 is an example cpt for the node representing the number of car crashes in the road safety network the top row describes the distribution of the number of car crashes when visibility is poor there is no surface water and the speed limit is 30 when a child node has multiple parents the number of entries in the conditional probability table can quickly become very large moreover the interdependence between the parent states can be difficult to identify thus motivating our research into the development of methods to aid the quantification of conditional probability tables from expert opinion 2 methods 2 1 bbn conditional probability elicitation despite the vast amount of literature and research into elicitation techniques see supplementary information appendices a and b it remains challenging to elicit conditional probability tables in practical bbn modelling the difficulty in eliciting cpts is predominantly due to the sheer number of cpt entries required in practical bbns often in the order of 100s of individual probability estimates this hampers the use of digital tools that require a separate input for every row of a cpt e g 26 thus many practical bbn papers resort to simply asking how many times out of 100 or what is the probability variable x takes value x for information y repeatedly without further aids for the numerical elicitation process see for example 24 27 however elicitation in this way can easily produce unrepresentative tables not least of all due to the sheer number of estimates required for example the relatively simple road safety network in fig 3 consists of 10 nodes the corresponding 6 conditional probability tables produce almost 70 individual combinations of parent states across all cpts each of which is associated with a distribution of the corresponding child node to be elicited in total this results in more than 150 individual probability estimates required from an elicitee even for this very simple network it can be difficult to complete all necessary cpts both because it is time consuming and also because it is difficult to maintain concentration consistently over so many distributions perhaps more importantly viewing each row of a cpt independently can make it very difficult to characterise an expert s belief about the relative changes in the different parent states see for example 20 our approach addresses these issues through the definition of a simple scoring system based on two questions per parent node that is then used to initialise a cpt this initial cpt provides a logical starting point for experts to fine tune whilst ensuring the higher level relationships between parent nodes has been efficiently encapsulated this has been implemented in ace a freely available r shiny software application for conditional probability elicitation hassall 2019 2 2 cpt scoring algorithm to specify a score that captures the relative effects of different parent nodes an expert first assigns a weight of relative importance to each parent node this weighting is used to define the relative effects of each parent on the probability distribution of the child node parents with a larger weight are assigned a greater level of influence in determining the conditional probability table such that changes in the states of the parent with the largest weight will result in the biggest differences in the distribution of the child node the second step is to define the direction of the relationship between each parent and child each parent can have either a positive negative or other relationship with the child node a relationship is considered positive if as the states of the parent changes according to the order they have been defined the probability the child node is in its higher states also increases conversely a negative relationship is appropriate if as the states of the parent changes according to the order they have been defined the probability the child node is in its higher states decreases not every parent child relationship can be categorised as having either a positive or negative relationship although it is impractical to incorporate a full set of relationships into the software implementation we have instead incorporated the option to define an other relationship this enables experts to define a relative order to the states of the parent node this relative order describes the order of the parent states that would result in an increasing probability that the child node is in its higher states fig 4 shows an example of specifying this information into the ace software for the car crash node of the road safety network here a change in the state of surface water will have an effect twice as large as a change in the state of visibility the effect of both visibility and surface water on the increasing number of car crashes is positive whilst the effect of speed limit is defined as non monotonic specifically the relative order of speed limit states describes the order that would result in an increasing probability that the child node is in its higher states in the example depicted in fig 4 a speed limit of 30 order 1 is associated with the fewest car crashes speed limits of 50 and 60 are associated with some car crashes and a speed limit of 40 order 4 is associated with the most car crashes mathematically this relative weighting and order relationship defines a score from which an initial draft cpt is created let p i j denote the score of the j th state of the i th parent which is given by p i j j 1 n i 1 if parent i has a positive relationship with the child node n i j n i 1 if parent i has a negative relationship with the child node o r d j 1 n i 1 if parent i has an other relationship with the child node where n i is the number of states of parent i and o r d j denotes the ordered index of state j an overall score is then calculated for each combination of parent states given by a weighted average of the constituent scores s c o r e k i w i p i k i w i where w i is the weight associated with parent i and k is the k th combination of parent states with p i k denoting the associated score of parent i for combination k for a child node with two states this score will correspond to the probability the child node is in its highest state for a child node with m 2 states a conversion is made for each parent combination k specifically the probability that the child node is in state m is given by twice the area of the m th trapezium formed when the linear line between the two probabilities of a corresponding two state child is cut into m equal intervals this is depicted in fig 5 for a s c o r e k 0 8 the distribution of a child node with two states is given by prob child state 1 0 2 and prob child state 2 0 8 for a s c o r e k 0 8 the distribution of a child node with four states a s c o r e k 0 8 corresponds to a distribution of the child node of prob child state 1 0 1375 prob child state 2 0 2125 prob child state 3 0 2875 and prob child state 4 0 3625 this scoring system assumes a that all states can be considered on an equally spaced linear scale and b that the range of cpt rows for a two state child node will contain values in the full range of 0 100 these assumptions act as a constraint on the construction of the scores which can be relaxed if needed within the underlying open source code due to the construction of this score one major limitation is in the mapping depicted in fig 5 for child nodes with an odd number of states m the middle category will always have a probability of 1 m we reiterate here that this score is not designed to fully define a cpt but rather to provide an initialisation that captures the relative effects of the parent nodes whilst still enabling experts to refine their beliefs through individual edits the ace software aids the process of elicitation by firstly using the above scoring system to provide a logical initial cpt based on an expert s belief of the overall effect of the different parent nodes secondly the ace software provides a fully editable interface with the initialised cpt for an expert to fine tune and edit individual values furthermore the software has been encoded with a number of warning messages that check the cpt for incongruities for users to refine for example if all middle categories are left unchanged an appropriate warning message is shown to further aid the elicitation a visual aid on a relative frequency scale is provided as shown in fig 6 this allows a user to visualise the full conditional distribution as well as to see the relative changes in the child node for different combinations of parent states this graphical representation of the cpt can be reordered according to the relative weights defined for the parent set thus providing a more intuitive display of the overall effects of the parent nodes 2 3 quantifying uncertainty throughout we have focused on the elicitation of cpts through the quantification of an expert s frequency distribution for a particular scenario the aleatoric uncertainty this does not capture a user s uncertainty in the resulting estimate as discussed in the appendix there are approaches available in the literature that look at formally quantifying the additional epistemic uncertainty in an expert s belief in a practical bbn application this would involve the elicitation of a multi dimensional hyperdistribution for each combination of parent states and rapidly becomes infeasible however we do believe it is important to capture this epistemic uncertainty as expressed in 20 as a pragmatic approach we included the notion of confidence in a user s estimate a confidence value can take one of three qualitative values low low confidence in the final beliefs and the expert would consider it likely the values could vary medium reasonably confident certain in the final beliefs although the final values could vary high highly confident in the final beliefs and the expert would not consider it likely for these values to vary much as stated above this definition of uncertainty in the probability estimate is pragmatic and qualitative more sophisticated measures include those aiming to numerically quantify the uncertainty in an estimate through the identification of quantiles of the distribution as developed in the shelf methodology oakley and o hagan 2016 idea hanea et al 2017 and approaches using cooke s classical model cooke 1991 aspinall and cooke 2013 and those that identify confidence intervals of the estimates see e g christophersen et al 2018 thus users and developers of ace can access the open source code to incorporate more precise definitions of uncertainty notions of confidence can be incorporated in the downstream analysis of an elicited bbn in multiple ways for example 24 combined the estimates for each probability from multiple experts through a weighting associated with the confidence in this way confidence was used to form an equivalent sample size for each experts contribution in comparison 32 showed how with an assessment of variance error bars can be incorporated into the bbn this does however rely on a numerical estimate of variance rather than a qualitative assessment an alternative would be to use the confidence as a form of sensitivity analysis e g through a monte carlo simulation study perturbing the derived conditional distributions relative to their associated confidence 3 results to investigate the potential efficiency gains in using the scoring system described above to initialise cpts we recruited 8 volunteers to test our methods using the ace software implementation each volunteer received training on graphical modelling and the association with conditional independence along with a description of the road safety network shown in fig 3 the volunteers were then allocated to 1 of 2 groups group a were given 25 minutes to complete as many cpts of the road safety network in a prespecified order as they could using the scoring system described above after a short interlude they were then given 25 minutes to fill in as many cpts in the same order as before without using the scoring system i e to fill in the tables manually although a graphical aid remained accessible in the software group b had the same tasks but in the reverse order i e to first fill the tables in manually and secondly to fill the table in using the scoring system in generality the majority of tables were found to be consistent across the two methods it was notable that the automated scoring algorithm enabled more tables to be completed in the given time however some discrepancies between the methods were seen three types of discrepancy were found 1 the automatic method could result in a more linear distribution compared to the manual process this is illustrated in fig 7a b which shows the elicited distribution for visibility conditional on the parent node of rain under the two approaches this suggests a tendency to stick with the default values when using the automated approach 2 although the shape of the distributions closely match there is a shift in the location this is demonstrated in fig 7c d which shows the elicited distribution for the number of car crashes conditional on the parent nodes visibility surface water and speed limit under the two approaches it can be seen that under the manual approach there is a consistently larger proportion of the distribution located in the 0 4 crash category compared to the automated approach for this particular example it demonstrates the limitations of the linear mapping of the score function in particular to obtain the same distribution as obtained under the manual process a non linear mapping of the score to three states would be required 3 the consistency of the relative importance of parent nodes by construction the default of the automated approach is to ensure the relative importance of the parent nodes is consistent over all combinations of parent node states this was not always observed in the corresponding manual tables following up on these discrepancies different views were expressed as to which table best represents the true belief a mistakes were made in the manual process due to i too many scenarios to follow the relative importance through logically ii a difficulty in expressing the relative importance in the parent nodes which could result in an equal weight given to each parent b the automated table was not edited to reflect the scenarios which do not follow the general trend a specific example was given that for a road with a high number of crashes 10 and more than 1 fatality the likelihood the road safety is good decreases when a cycle lane is present compared to being absent whereas in all other scenarios the presence of cycle lane increases the likelihood of good road safety this particular example was captured in the manual process but not in the automated process as the overall trend of cycle presence increasing the likelihood of good road safety was used throughout all scenarios it should be noted that one of the drawbacks to bbns in general is the sheer number of distributions to be elicited we have found that elicitees fatigue with this process regardless of method which may also be reflected in the discrepancies observed above it is difficult to motivate an individual to repeat the process twice with equal attentiveness after the volunteers completed the comparative exercise they were asked the following three questions 1 in general did you find it easy or difficult to quantify your beliefs numerically 2 which method did you prefer 3 why unanimously the automated method was preferred with the general process of numerical elicitation found to be difficult in particular most participants greatly favoured the automated process for nodes with multiple parents as they found it very difficult to translate the interrelationships between parents into a set of well defined consistent probability distributions this did sometimes come with the caveat that the manual process may have produced a more detailed representation although the study presented comes from a small set of volunteers both the manual elicitation and the automatic initialisation were used in expert elicitation workshops we have run to formulate a working definition of soil quality and health over four workshops 27 13 experts from across soil science and associated disciplines used the automated manual approach to initialise the cpts respectively the findings of the above study are largely representative of what we observed through these workshops particularly the ability for experts to complete the cpts in the time allowed for in the workshop the appreciation in being able to initialise a full cpt with the relative importance of parents defined consistently the observed linearity in the distributions from the automated approach and the fact that few edits were made to the initialised cpts a full description of these workshops and associated findings will be the topic of a future paper 3 1 additional guidance in implementing the ace software for capturing expert opinion we provide some additional guidance based on our own experiences avoid double negatives in the definition of nodes and their associated states a consistent ordering in node states can be particularly helpful when defining the direction of a relationship e g a cycle lane node with states a b s e n t p r e s e n t resulted in fewer mistakes compared to node states defined by p r e s e n t a b s e n t it is encouraged to keep the number of child states to a minimum this ensures a more dichotomous definition of the node many participants found a two state definition particularly intuitive for example a natural definition of an ordinal node is to have three states l o w m e d i u m h i g h in our experience we found that when experts were uncertain of the distribution of the child node they would intuitively put the majority of the distribution into the medium category however when a node is defined to have two states l o w h i g h an uncertain distribution would be intuitively reflected from an equal weight to each category this highlights the importance of communicating what the distribution actually represents a notion that is difficult for non domain specific experts as discussed in christophersen et al 2018 there are often circumstances where the discretisation of continuous variables is limiting and extensions to include continuous variables in a bbn are desirable if more than two states are necessary for a child node the scoring approach works best when there is an equidistant definition to the ordinal states of the child node on hand facilitation is needed along with clear guidance on what the numerical quantities mean in terms of the practical application practice via training exercises is a fundamental necessity in our experience experts became much more comfortable with the concept of graphical representations after they had practised the process of capturing their belief in conditional probability tables for example prior to training it was difficult to convince participants that a hierarchical graphical structure with intermediary parents was a desirable and meaningful structure see e g section 9 3 4 1 of korb and nicholson 2004 until they had experienced the process of translating their beliefs into the large interdependent conditional probability tables 4 discussion ultimately the optimal method by which the conditional probability tables are captured will differ depending on elicitee in practice we have found the automated method to be far less daunting to many experts who are not versed in probabilistic descriptions in addition the automated scoring enables an instinctive and qualitative knowledge to be captured numerically through a simpler definition of relative parent weights and directional relationships the automated approach was found to greatly ease the process of elicitation but at the cost of specificity although designed purely as an approach to initialise the cpts we found in practice relatively little editing of these initialised tables was done users tended to accept the prepopulated distributions and move on to the next cpt although we cannot say whether this phenomenon will occur in general we found that it did occur both in our soil health workshops and the volunteer study presented in section 3 this was primarily due to either time constraints with users daunted by the number of tables they needed to complete or low confidence in the elicited relationships with users opting for a generalised representation of their belief in the absence of any strong feelings to counter that state if time is the limiting factor our recommendation would be to encourage experts to identify the scenarios which deviate from the overall trend they have specified and to edit these specific individuals if the issue is low confidence this can be recorded directly in ace for use in downstream analysis the main drawback to the automated approach is the prescriptive conversion of the derived score into a distribution over multiple states fig 5 many extensions to this score could be considered predominantly to a non linear conversion between the score and the frequency distribution over multiple states however these conversions may be difficult to convey to individual experts an alternative approach would be to ensure the states of a node are defined to be equidistant e g in the road safety example above the states of the number of car crashes were defined as 0 4 5 10 10 a more equidistant definition might be 0 1 3 4 10 10 in all our studies the visualisation aids have greatly facilitated the process of numerically quantifying beliefs and may be improved through further developments it is well known that trellis graphics cleveland 1993 provide an intuitive display of multiple interrelated variables and could be incorporated into the software in addition an interactive graphical display could further aid the insight obtained from visualising such cpts it remains a major challenge to elicit cpts under uncertainty work by 36 address this by extending the ideas within the shelf package to elicit a dirichlet distribution over the set of probabilities however applying this methodology to the number of cases in a typical bbn remains impractical although certainly a desirable objective for the future 5 conclusion we have investigated two approaches to filling in the conditional probability tables of a bayesian belief net a manual approach requiring the expert to consider how many times out of 100 would a particular outcome be expected from a set of specified scenarios and an automatic approach which initialises the cpt based on two simple questions before enabling further edits to be made the manual approach was found to work well for experts with a good quantitative background who were practised at translating relative relationships into numerical form however for many domain specific experts quantifying interdependent relationships is incredibly difficult through the development of the ace software we have provided the community with a digitised data capture method for recording conditional probability tables in conjunction with both visual and numerical aids moreover through the development of a well defined score we have shown how a potentially large complex set of interactions can be encapsulated in a cpt without having to specify the outcome of every single scenario automating this approach in freely available software allows its incorporation in many elicitation techniques whether by group consensus independently or through a delphi recursive approach although bayesian belief networks are increasingly being used to describe and model both natural systems and public health concerns robust expert elicitation of the cpts remains a major bottleneck in the process this implementation has substantially reduced the burden associated with filling in cpts through expert derived opinion and provides efficient data collection for use in bayesian belief networks thus this methodology has wide applicability to the research community for modelling systems and developing policy support tools declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to thank all volunteers who gave their time to testing the software and providing valuable feedback both in the development stages and in the comparative methods study funding this work has been funded by the soil security programme through a natural environment research council grant ne p014313 and by the biotechnology and biological sciences research council through the soil to nutrition s2n strategic programme bbs e c 000i0330 appendix a general elicitation methods there is much debate around how expert opinion should be elicited with some controversy surrounding the allowance for experts to discuss and revise opinions see for example 5 despite this all approaches to structured elicitation aim to reduce known biases within the elicitation process these biases include kuhnert et al 2010 tversky and kahneman 1973 cooke 1991 availability bias whereby an expert s response is based on most recent available information and not considering past events hindsight bias where too much emphasis is placed on past events anchoring the tendency to anchor around initial first guesses irrespective of the accuracy of the initial estimate law of small numbers where opinions are based on small pieces of evidence which are then extrapolated representativeness where opinions are based on situations that are rightly or wrongly perceived to be similar to the scenario in question whether one chooses to use a process with discussion or not depends on which biases are most likely to occur and whether the group dynamics will add to or lessen such biases for example the presence of a single strong opinion may cause anchoring around this opinion the other consideration when choosing the elicitation approach is around how opinions from multiple experts will be combined opinions from multiple experts can be combined either through allowing a group of experts to reach a group consensus through repeated revisions and discussion or through mathematical aggregation there is evidence to suggest that allowing experts to interact and discuss may impact the validity of mathematical aggregation as it induces a dependence between responses see 11 and references therein appendix b numerical elicitation methods there is an additional layer of complexity when it comes to eliciting quantitative responses that characterise the relationships between variables it is well known that humans are inherently poor at estimating numerical quantities to this end there has been much work to overcome these shortcomings kuhnert et al 2010 research has focused on the task of effectively and accurately eliciting estimates of proportions from different experts for example it is considered that expressing this information in a frequency format enables more accurate estimates compared to specifying a proportion between 0 and 1 gigerenzer 1996 price 1998 other methods for eliciting proportions can be characterised through probability scale approaches and gambling methods jenkinson 2005 gambling methods assist experts to think about their probabilities in terms of an event upon which they might place a bet see 5 and references therein this can enable a greater engagement into the thinking behind each probability estimate however the basic gambling methods are focused on binary outcomes in all cases visual assessments are generally believed to greatly aid the elicitation process by allowing experts to see their beliefs quantified however a conditional probability table consists of multiple probability entries as discussed in section 1 1 1 cpts can become very large very quickly resulting in 100s of individual probability estimates to be filled in 33 provides a review of the methods aiming at reducing this burden in completing cpts the methods vary from piecewise interpolation based on the influence of parents wisse et al 2008 a method similar to that developed in ace to making use of the causal structure in a bbn e g the noisy or and noisy max methods pearl 2014 diez 1993 in addition it is a highly active area of research that focuses on eliciting beliefs under uncertainty through for example the methods of shelf oakley and o hagan 2016 and idea hanea et al 2017 these methods look to ascertain a prior distribution for a parameter of interest based on an expert s belief i e to isolate the epistemic uncertainty associated with a belief and separate this from the aleatoric uncertainty the uncertainty due to randomness in the process 
26115,expert opinion is increasingly being used to inform bayesian belief networks in particular to define the conditional dependencies modelled by the graphical structure the elicitation of such expert opinion remains a major challenge due to both the quantity of information required and the ability of experts to quantify subjective beliefs effectively in this work we introduce a method designed to initialise conditional probability tables based on a small number of simple questions that capture the overall shape of a conditional probability distribution before enabling the expert to refine their results in an efficient way these methods have been incorporated into a software application for conditional probability elicitation ace freely available at https github com kirstylhassall ace hassall 2019 keywords bayes nets belief networks expert elicitation conditional probability tables software availability all methods described in this paper have been incorporated into an open source software application for conditional probability elicitation ace freely available at https github com kirstylhassall ace hassall 2019 1 introduction bayesian belief nets also referred to as bayes nets belief networks or often simply bbns have in recent years seen a dramatic increase in their use for describing and modelling natural systems examples include quantifying the risk of erosion in peat bogs aalders et al 2011 modelling ecosystem services haines young 2011 applications in natural resource management see 13 and references therein mapping risks of soil threats such as soil compaction troldborg et al 2013 predicting soil bulk density at landscape scales taalab et al 2015 and assessing the impact of buffer zones on water protection and biodiversity tattari et al 2003 this explosion in practical bbn modelling may in part be due to the relative simplicity of the intuitive graphical representation of multiple interrelated variables captured through conditional probabilities and more practically the increasing accessibility to specialist bbn software there has been much work in the development of bbn methodology to address the practicalities of bbn modelling see for example 20 bbn modelling largely consists of two interrelated steps defining the graphical structure and quantifying the form of the conditional dependencies through conditional probability distributions in each step one can incorporate both data and expert opinion pollino et al 2007 aalders et al 2011 our interest is to define a fully parameterised bbn that quantifies soil health by capturing the inherent knowledge of experts representative of all aspects of soil science such as soil microbiology soil chemistry soil physics and land management among others a full description of the construction and evolution of these soil health network structures will be the topic of a future paper although an early example is shown in fig 2 for demonstrative purposes in this paper we focus on the key issues surrounding the use of expert opinion in the characterisation of conditional probability tables 1 1 conditional probability tables or cpts described fully in section 1 1 1 parameterise a bbn by quantifying the relationship between variables bbns first saw an increase in popularity in the 1980s with applications to decision support and expert systems see for example 19 22 whereby a causal network description is used to express expert knowledge that can inform diagnoses and decisions in this work our motivation for using bbns differs from both the practical modelling approaches prolific in the literature and also the traditional usage in expert systems specifically there is an increasing desire to be able to derive and quantify metrics for often subjective concepts an example of such is soil quality and health which is a term frequently used albeit qualitatively and subjectively wienhold et al 2004 doran and zeiss 2000 definitions of soil quality and health vary in the literature but include the condition or state of soil relative to the requirements of one or more biotic species and or to any human need or purpose johnson et al 1997 the capacity of a specific kind of soil to function within natural or managed ecosystem boundaries to sustain plant and animal productivity maintain or enhance water and air quality and support human health and habitation friedman et al 2001 karlen et al 1997 the soil s ability to provide ecosystem and social services through its capacities to perform its functions under changing conditions tóth et al 2007 since these definitions include inherently subjective concepts quantifying them cannot be done without the use of expert opinion bbns are a natural framework for incorporating both expert opinion and data to conceptualise different model systems see for example 1 24 a bbn is a graphical model made up of nodes representing variables of interest and edges representing direct dependencies between variables specifically each edge represents a conditional distribution with nodes that are not connected considered to be conditionally independent given all other nodes in the network bbns are more precisely directed acyclic graphs dags meaning each edge has a direction and there are no feedback loops in the network the direction of each arrow represents the direction of conditioning and the node at the source of the arrow is referred to as a parent and the node at the sink of the arrow the child the joint distribution of all the variables within the graphical model can be represented as the product of the conditional distributions f x 1 x p x 1 x p i f x i parents x i x i parents x i where parents x i denotes the set of nodes connected by a directed edge to the node i for example fig 1 shows three nodes x y z connected by two edges where x and y are conditionally independent given the parent z moreover there exists a dependency between x and z and between y and z the graph shown in fig 1 has joint distribution f x y z x y z which can be decomposed as f x y z x y z f x z x z f y z y z f z z although the distribution of each node in a bbn can be general for the remainder of this paper we consider only discrete bbns where all random variables x 1 x p are categorical however we note here a discrepancy in the literature regarding bbns and the term bayesian korb and nicholson 2004 the notion of a prior in a bbn often refers to the distribution of an ancestral node this does not preclude the use of expert derived opinion but rather the opinions or beliefs are used to directly inform the conditional distribution of each node this differs from what we might term the truly bayesian approach which would consider prior information to be included through a hyperdistribution over the parameters of the node distribution i e the likelihood of node x i f x i parents x i x i parents x i θ is defined through the parameters θ prior information is defined through the distribution over θ and interest is in deriving a posterior distribution for θ x x more explicitly for a discrete node x i with a categorical multinomial distribution defined by a set of probabilities a prior distribution is defined over this set of probabilities by e g a dirichlet distribution in this way expert opinion or belief would be used to parameterise the dirichlet prior which when combined with the likelihood gives the posterior as with most practical bbns our focus is on the direct frequency representation and not the fully bayesian approach 1 1 example bbns throughout the remainder of this paper we will demonstrate our methods on the following bbn application shown in fig 3 this is an illustrative example aiming to define the concept of road safety this example was developed predominantly to aid in the exposition of bayesian belief networks to the subject specific experts we approached to define soil health and quality as an aside introducing the topic of bbns with an example unrelated to the topic we wished to focus on i e soil health was useful to put across the main concepts e g of conditional dependence and conditional independence without prejudicing the question at hand fig 3 shows how the subjective notion of road safety can be defined by four variables the presence of cycle lanes whether the road is on a main school route the number of car crashes and the number of fatalities furthermore the net shows how if data are not available on the number of car crashes we can infer this from other causally related variables such as the speed limit and weather conditions moreover our example stresses the point that graphical representations do not need to be causal specifically we include the somewhat artificial example that if data were available on the number of umbrella sales this could be used to infer the weather conditions which in turn can be used to infer the number of car crashes 1 1 1 conditional probability tables the conditional probability table cpt describes the distribution of the child node for every combination of states of the parent nodes for example shown in table 1 is an example cpt for the node representing the number of car crashes in the road safety network the top row describes the distribution of the number of car crashes when visibility is poor there is no surface water and the speed limit is 30 when a child node has multiple parents the number of entries in the conditional probability table can quickly become very large moreover the interdependence between the parent states can be difficult to identify thus motivating our research into the development of methods to aid the quantification of conditional probability tables from expert opinion 2 methods 2 1 bbn conditional probability elicitation despite the vast amount of literature and research into elicitation techniques see supplementary information appendices a and b it remains challenging to elicit conditional probability tables in practical bbn modelling the difficulty in eliciting cpts is predominantly due to the sheer number of cpt entries required in practical bbns often in the order of 100s of individual probability estimates this hampers the use of digital tools that require a separate input for every row of a cpt e g 26 thus many practical bbn papers resort to simply asking how many times out of 100 or what is the probability variable x takes value x for information y repeatedly without further aids for the numerical elicitation process see for example 24 27 however elicitation in this way can easily produce unrepresentative tables not least of all due to the sheer number of estimates required for example the relatively simple road safety network in fig 3 consists of 10 nodes the corresponding 6 conditional probability tables produce almost 70 individual combinations of parent states across all cpts each of which is associated with a distribution of the corresponding child node to be elicited in total this results in more than 150 individual probability estimates required from an elicitee even for this very simple network it can be difficult to complete all necessary cpts both because it is time consuming and also because it is difficult to maintain concentration consistently over so many distributions perhaps more importantly viewing each row of a cpt independently can make it very difficult to characterise an expert s belief about the relative changes in the different parent states see for example 20 our approach addresses these issues through the definition of a simple scoring system based on two questions per parent node that is then used to initialise a cpt this initial cpt provides a logical starting point for experts to fine tune whilst ensuring the higher level relationships between parent nodes has been efficiently encapsulated this has been implemented in ace a freely available r shiny software application for conditional probability elicitation hassall 2019 2 2 cpt scoring algorithm to specify a score that captures the relative effects of different parent nodes an expert first assigns a weight of relative importance to each parent node this weighting is used to define the relative effects of each parent on the probability distribution of the child node parents with a larger weight are assigned a greater level of influence in determining the conditional probability table such that changes in the states of the parent with the largest weight will result in the biggest differences in the distribution of the child node the second step is to define the direction of the relationship between each parent and child each parent can have either a positive negative or other relationship with the child node a relationship is considered positive if as the states of the parent changes according to the order they have been defined the probability the child node is in its higher states also increases conversely a negative relationship is appropriate if as the states of the parent changes according to the order they have been defined the probability the child node is in its higher states decreases not every parent child relationship can be categorised as having either a positive or negative relationship although it is impractical to incorporate a full set of relationships into the software implementation we have instead incorporated the option to define an other relationship this enables experts to define a relative order to the states of the parent node this relative order describes the order of the parent states that would result in an increasing probability that the child node is in its higher states fig 4 shows an example of specifying this information into the ace software for the car crash node of the road safety network here a change in the state of surface water will have an effect twice as large as a change in the state of visibility the effect of both visibility and surface water on the increasing number of car crashes is positive whilst the effect of speed limit is defined as non monotonic specifically the relative order of speed limit states describes the order that would result in an increasing probability that the child node is in its higher states in the example depicted in fig 4 a speed limit of 30 order 1 is associated with the fewest car crashes speed limits of 50 and 60 are associated with some car crashes and a speed limit of 40 order 4 is associated with the most car crashes mathematically this relative weighting and order relationship defines a score from which an initial draft cpt is created let p i j denote the score of the j th state of the i th parent which is given by p i j j 1 n i 1 if parent i has a positive relationship with the child node n i j n i 1 if parent i has a negative relationship with the child node o r d j 1 n i 1 if parent i has an other relationship with the child node where n i is the number of states of parent i and o r d j denotes the ordered index of state j an overall score is then calculated for each combination of parent states given by a weighted average of the constituent scores s c o r e k i w i p i k i w i where w i is the weight associated with parent i and k is the k th combination of parent states with p i k denoting the associated score of parent i for combination k for a child node with two states this score will correspond to the probability the child node is in its highest state for a child node with m 2 states a conversion is made for each parent combination k specifically the probability that the child node is in state m is given by twice the area of the m th trapezium formed when the linear line between the two probabilities of a corresponding two state child is cut into m equal intervals this is depicted in fig 5 for a s c o r e k 0 8 the distribution of a child node with two states is given by prob child state 1 0 2 and prob child state 2 0 8 for a s c o r e k 0 8 the distribution of a child node with four states a s c o r e k 0 8 corresponds to a distribution of the child node of prob child state 1 0 1375 prob child state 2 0 2125 prob child state 3 0 2875 and prob child state 4 0 3625 this scoring system assumes a that all states can be considered on an equally spaced linear scale and b that the range of cpt rows for a two state child node will contain values in the full range of 0 100 these assumptions act as a constraint on the construction of the scores which can be relaxed if needed within the underlying open source code due to the construction of this score one major limitation is in the mapping depicted in fig 5 for child nodes with an odd number of states m the middle category will always have a probability of 1 m we reiterate here that this score is not designed to fully define a cpt but rather to provide an initialisation that captures the relative effects of the parent nodes whilst still enabling experts to refine their beliefs through individual edits the ace software aids the process of elicitation by firstly using the above scoring system to provide a logical initial cpt based on an expert s belief of the overall effect of the different parent nodes secondly the ace software provides a fully editable interface with the initialised cpt for an expert to fine tune and edit individual values furthermore the software has been encoded with a number of warning messages that check the cpt for incongruities for users to refine for example if all middle categories are left unchanged an appropriate warning message is shown to further aid the elicitation a visual aid on a relative frequency scale is provided as shown in fig 6 this allows a user to visualise the full conditional distribution as well as to see the relative changes in the child node for different combinations of parent states this graphical representation of the cpt can be reordered according to the relative weights defined for the parent set thus providing a more intuitive display of the overall effects of the parent nodes 2 3 quantifying uncertainty throughout we have focused on the elicitation of cpts through the quantification of an expert s frequency distribution for a particular scenario the aleatoric uncertainty this does not capture a user s uncertainty in the resulting estimate as discussed in the appendix there are approaches available in the literature that look at formally quantifying the additional epistemic uncertainty in an expert s belief in a practical bbn application this would involve the elicitation of a multi dimensional hyperdistribution for each combination of parent states and rapidly becomes infeasible however we do believe it is important to capture this epistemic uncertainty as expressed in 20 as a pragmatic approach we included the notion of confidence in a user s estimate a confidence value can take one of three qualitative values low low confidence in the final beliefs and the expert would consider it likely the values could vary medium reasonably confident certain in the final beliefs although the final values could vary high highly confident in the final beliefs and the expert would not consider it likely for these values to vary much as stated above this definition of uncertainty in the probability estimate is pragmatic and qualitative more sophisticated measures include those aiming to numerically quantify the uncertainty in an estimate through the identification of quantiles of the distribution as developed in the shelf methodology oakley and o hagan 2016 idea hanea et al 2017 and approaches using cooke s classical model cooke 1991 aspinall and cooke 2013 and those that identify confidence intervals of the estimates see e g christophersen et al 2018 thus users and developers of ace can access the open source code to incorporate more precise definitions of uncertainty notions of confidence can be incorporated in the downstream analysis of an elicited bbn in multiple ways for example 24 combined the estimates for each probability from multiple experts through a weighting associated with the confidence in this way confidence was used to form an equivalent sample size for each experts contribution in comparison 32 showed how with an assessment of variance error bars can be incorporated into the bbn this does however rely on a numerical estimate of variance rather than a qualitative assessment an alternative would be to use the confidence as a form of sensitivity analysis e g through a monte carlo simulation study perturbing the derived conditional distributions relative to their associated confidence 3 results to investigate the potential efficiency gains in using the scoring system described above to initialise cpts we recruited 8 volunteers to test our methods using the ace software implementation each volunteer received training on graphical modelling and the association with conditional independence along with a description of the road safety network shown in fig 3 the volunteers were then allocated to 1 of 2 groups group a were given 25 minutes to complete as many cpts of the road safety network in a prespecified order as they could using the scoring system described above after a short interlude they were then given 25 minutes to fill in as many cpts in the same order as before without using the scoring system i e to fill in the tables manually although a graphical aid remained accessible in the software group b had the same tasks but in the reverse order i e to first fill the tables in manually and secondly to fill the table in using the scoring system in generality the majority of tables were found to be consistent across the two methods it was notable that the automated scoring algorithm enabled more tables to be completed in the given time however some discrepancies between the methods were seen three types of discrepancy were found 1 the automatic method could result in a more linear distribution compared to the manual process this is illustrated in fig 7a b which shows the elicited distribution for visibility conditional on the parent node of rain under the two approaches this suggests a tendency to stick with the default values when using the automated approach 2 although the shape of the distributions closely match there is a shift in the location this is demonstrated in fig 7c d which shows the elicited distribution for the number of car crashes conditional on the parent nodes visibility surface water and speed limit under the two approaches it can be seen that under the manual approach there is a consistently larger proportion of the distribution located in the 0 4 crash category compared to the automated approach for this particular example it demonstrates the limitations of the linear mapping of the score function in particular to obtain the same distribution as obtained under the manual process a non linear mapping of the score to three states would be required 3 the consistency of the relative importance of parent nodes by construction the default of the automated approach is to ensure the relative importance of the parent nodes is consistent over all combinations of parent node states this was not always observed in the corresponding manual tables following up on these discrepancies different views were expressed as to which table best represents the true belief a mistakes were made in the manual process due to i too many scenarios to follow the relative importance through logically ii a difficulty in expressing the relative importance in the parent nodes which could result in an equal weight given to each parent b the automated table was not edited to reflect the scenarios which do not follow the general trend a specific example was given that for a road with a high number of crashes 10 and more than 1 fatality the likelihood the road safety is good decreases when a cycle lane is present compared to being absent whereas in all other scenarios the presence of cycle lane increases the likelihood of good road safety this particular example was captured in the manual process but not in the automated process as the overall trend of cycle presence increasing the likelihood of good road safety was used throughout all scenarios it should be noted that one of the drawbacks to bbns in general is the sheer number of distributions to be elicited we have found that elicitees fatigue with this process regardless of method which may also be reflected in the discrepancies observed above it is difficult to motivate an individual to repeat the process twice with equal attentiveness after the volunteers completed the comparative exercise they were asked the following three questions 1 in general did you find it easy or difficult to quantify your beliefs numerically 2 which method did you prefer 3 why unanimously the automated method was preferred with the general process of numerical elicitation found to be difficult in particular most participants greatly favoured the automated process for nodes with multiple parents as they found it very difficult to translate the interrelationships between parents into a set of well defined consistent probability distributions this did sometimes come with the caveat that the manual process may have produced a more detailed representation although the study presented comes from a small set of volunteers both the manual elicitation and the automatic initialisation were used in expert elicitation workshops we have run to formulate a working definition of soil quality and health over four workshops 27 13 experts from across soil science and associated disciplines used the automated manual approach to initialise the cpts respectively the findings of the above study are largely representative of what we observed through these workshops particularly the ability for experts to complete the cpts in the time allowed for in the workshop the appreciation in being able to initialise a full cpt with the relative importance of parents defined consistently the observed linearity in the distributions from the automated approach and the fact that few edits were made to the initialised cpts a full description of these workshops and associated findings will be the topic of a future paper 3 1 additional guidance in implementing the ace software for capturing expert opinion we provide some additional guidance based on our own experiences avoid double negatives in the definition of nodes and their associated states a consistent ordering in node states can be particularly helpful when defining the direction of a relationship e g a cycle lane node with states a b s e n t p r e s e n t resulted in fewer mistakes compared to node states defined by p r e s e n t a b s e n t it is encouraged to keep the number of child states to a minimum this ensures a more dichotomous definition of the node many participants found a two state definition particularly intuitive for example a natural definition of an ordinal node is to have three states l o w m e d i u m h i g h in our experience we found that when experts were uncertain of the distribution of the child node they would intuitively put the majority of the distribution into the medium category however when a node is defined to have two states l o w h i g h an uncertain distribution would be intuitively reflected from an equal weight to each category this highlights the importance of communicating what the distribution actually represents a notion that is difficult for non domain specific experts as discussed in christophersen et al 2018 there are often circumstances where the discretisation of continuous variables is limiting and extensions to include continuous variables in a bbn are desirable if more than two states are necessary for a child node the scoring approach works best when there is an equidistant definition to the ordinal states of the child node on hand facilitation is needed along with clear guidance on what the numerical quantities mean in terms of the practical application practice via training exercises is a fundamental necessity in our experience experts became much more comfortable with the concept of graphical representations after they had practised the process of capturing their belief in conditional probability tables for example prior to training it was difficult to convince participants that a hierarchical graphical structure with intermediary parents was a desirable and meaningful structure see e g section 9 3 4 1 of korb and nicholson 2004 until they had experienced the process of translating their beliefs into the large interdependent conditional probability tables 4 discussion ultimately the optimal method by which the conditional probability tables are captured will differ depending on elicitee in practice we have found the automated method to be far less daunting to many experts who are not versed in probabilistic descriptions in addition the automated scoring enables an instinctive and qualitative knowledge to be captured numerically through a simpler definition of relative parent weights and directional relationships the automated approach was found to greatly ease the process of elicitation but at the cost of specificity although designed purely as an approach to initialise the cpts we found in practice relatively little editing of these initialised tables was done users tended to accept the prepopulated distributions and move on to the next cpt although we cannot say whether this phenomenon will occur in general we found that it did occur both in our soil health workshops and the volunteer study presented in section 3 this was primarily due to either time constraints with users daunted by the number of tables they needed to complete or low confidence in the elicited relationships with users opting for a generalised representation of their belief in the absence of any strong feelings to counter that state if time is the limiting factor our recommendation would be to encourage experts to identify the scenarios which deviate from the overall trend they have specified and to edit these specific individuals if the issue is low confidence this can be recorded directly in ace for use in downstream analysis the main drawback to the automated approach is the prescriptive conversion of the derived score into a distribution over multiple states fig 5 many extensions to this score could be considered predominantly to a non linear conversion between the score and the frequency distribution over multiple states however these conversions may be difficult to convey to individual experts an alternative approach would be to ensure the states of a node are defined to be equidistant e g in the road safety example above the states of the number of car crashes were defined as 0 4 5 10 10 a more equidistant definition might be 0 1 3 4 10 10 in all our studies the visualisation aids have greatly facilitated the process of numerically quantifying beliefs and may be improved through further developments it is well known that trellis graphics cleveland 1993 provide an intuitive display of multiple interrelated variables and could be incorporated into the software in addition an interactive graphical display could further aid the insight obtained from visualising such cpts it remains a major challenge to elicit cpts under uncertainty work by 36 address this by extending the ideas within the shelf package to elicit a dirichlet distribution over the set of probabilities however applying this methodology to the number of cases in a typical bbn remains impractical although certainly a desirable objective for the future 5 conclusion we have investigated two approaches to filling in the conditional probability tables of a bayesian belief net a manual approach requiring the expert to consider how many times out of 100 would a particular outcome be expected from a set of specified scenarios and an automatic approach which initialises the cpt based on two simple questions before enabling further edits to be made the manual approach was found to work well for experts with a good quantitative background who were practised at translating relative relationships into numerical form however for many domain specific experts quantifying interdependent relationships is incredibly difficult through the development of the ace software we have provided the community with a digitised data capture method for recording conditional probability tables in conjunction with both visual and numerical aids moreover through the development of a well defined score we have shown how a potentially large complex set of interactions can be encapsulated in a cpt without having to specify the outcome of every single scenario automating this approach in freely available software allows its incorporation in many elicitation techniques whether by group consensus independently or through a delphi recursive approach although bayesian belief networks are increasingly being used to describe and model both natural systems and public health concerns robust expert elicitation of the cpts remains a major bottleneck in the process this implementation has substantially reduced the burden associated with filling in cpts through expert derived opinion and provides efficient data collection for use in bayesian belief networks thus this methodology has wide applicability to the research community for modelling systems and developing policy support tools declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to thank all volunteers who gave their time to testing the software and providing valuable feedback both in the development stages and in the comparative methods study funding this work has been funded by the soil security programme through a natural environment research council grant ne p014313 and by the biotechnology and biological sciences research council through the soil to nutrition s2n strategic programme bbs e c 000i0330 appendix a general elicitation methods there is much debate around how expert opinion should be elicited with some controversy surrounding the allowance for experts to discuss and revise opinions see for example 5 despite this all approaches to structured elicitation aim to reduce known biases within the elicitation process these biases include kuhnert et al 2010 tversky and kahneman 1973 cooke 1991 availability bias whereby an expert s response is based on most recent available information and not considering past events hindsight bias where too much emphasis is placed on past events anchoring the tendency to anchor around initial first guesses irrespective of the accuracy of the initial estimate law of small numbers where opinions are based on small pieces of evidence which are then extrapolated representativeness where opinions are based on situations that are rightly or wrongly perceived to be similar to the scenario in question whether one chooses to use a process with discussion or not depends on which biases are most likely to occur and whether the group dynamics will add to or lessen such biases for example the presence of a single strong opinion may cause anchoring around this opinion the other consideration when choosing the elicitation approach is around how opinions from multiple experts will be combined opinions from multiple experts can be combined either through allowing a group of experts to reach a group consensus through repeated revisions and discussion or through mathematical aggregation there is evidence to suggest that allowing experts to interact and discuss may impact the validity of mathematical aggregation as it induces a dependence between responses see 11 and references therein appendix b numerical elicitation methods there is an additional layer of complexity when it comes to eliciting quantitative responses that characterise the relationships between variables it is well known that humans are inherently poor at estimating numerical quantities to this end there has been much work to overcome these shortcomings kuhnert et al 2010 research has focused on the task of effectively and accurately eliciting estimates of proportions from different experts for example it is considered that expressing this information in a frequency format enables more accurate estimates compared to specifying a proportion between 0 and 1 gigerenzer 1996 price 1998 other methods for eliciting proportions can be characterised through probability scale approaches and gambling methods jenkinson 2005 gambling methods assist experts to think about their probabilities in terms of an event upon which they might place a bet see 5 and references therein this can enable a greater engagement into the thinking behind each probability estimate however the basic gambling methods are focused on binary outcomes in all cases visual assessments are generally believed to greatly aid the elicitation process by allowing experts to see their beliefs quantified however a conditional probability table consists of multiple probability entries as discussed in section 1 1 1 cpts can become very large very quickly resulting in 100s of individual probability estimates to be filled in 33 provides a review of the methods aiming at reducing this burden in completing cpts the methods vary from piecewise interpolation based on the influence of parents wisse et al 2008 a method similar to that developed in ace to making use of the causal structure in a bbn e g the noisy or and noisy max methods pearl 2014 diez 1993 in addition it is a highly active area of research that focuses on eliciting beliefs under uncertainty through for example the methods of shelf oakley and o hagan 2016 and idea hanea et al 2017 these methods look to ascertain a prior distribution for a parameter of interest based on an expert s belief i e to isolate the epistemic uncertainty associated with a belief and separate this from the aleatoric uncertainty the uncertainty due to randomness in the process 
26116,conducting global sensitivity analysis using variance decomposition methods in complex simulation models with many input factors is usually unaffordable an alternative is to first apply a screening method to reduce the number of input factors and then apply a variance decomposition method to the reduced model however usually selection of input factors is not done robustly and convergence of the screening method is not ensured we propose two new criteria a criterion that mimics the visual selection of the input factors and a convergence criterion in the application of the criteria to a complex model the morris screening method has needed 200 trajectories to converge and the visual criterion has outperformed other existing criteria our proposal ensures a robust combination of the morris and the sobol methods that provides an objective and automatic method to select the most important input factors with a feasible computing load to achieve convergence abbreviations aee absolute elementary effect cv coefficient of variation gsa global sensitivity analysis ssb spawning stock biomass tac total allowable catch keywords convergence criterion global sensitivity analysis morris elementary effect method selection criterion sobol variance decomposition method software availability the r functions to implement the selection and convergence criteria are available in zenodo https zenodo org record 3402534 garcia 2019 in the same repository there is an example showing how to use these functions in practice 1 introduction simulation models are useful tools to provide a better understanding of the environmental systems one of the big issues when using simulation models is their validation that is to ensure the model is good enough to meet its intended purpose rykiel 1996 schmolke et al 2010 balci 1997 presented a long list of techniques for the validation of simulation models divided in four groups informal techniques which rely on human reasoning and lack mathematical formalism static techniques which are concerned with accuracy assessment on the basis of characteristics of the static model design and source code dynamic techniques which evaluate the model based on its execution behavior and formal techniques which are based on the mathematical proof and correctness global sensitivity analysis gsa which has been proposed by several authors as a key ingredient in the validation process of simulation models saltelli et al 2000 rabitz 1989 is in the third group the dynamic techniques variance based gsa examines the relation between the variance of the output of the simulation models and the variance of their input factors saltelli et al 2008 several methods for performing gsa exist from simple scatterplots to the more complex sobol variance decomposition method the sobol method sobol 1993 see pianosi et al 2016 borgonovo and plischke 2016 or norton 2015 for recent reviews on existing methods and practices the sobol method is frequently considered the reference method for variance based gsa yang 2011 confalonieri et al 2010 sarrazin et al 2016 homma and saltelli 1996 the method can be used to rank the input factors according to their effect on the results and to estimate each input factor s contribution to the output variance two of the main drawbacks of this method are its high computational cost and its inability to represent the outputs uncertainties correctly if the model output is highly skewed borgonovo et al 2011 pianosi and wagener 2015 the computational cost of applying the sobol method on highly nonlinear simulation models with many input factors could be unaffordable in those cases a frequently used alternative is to combine the sobol method with the morris elementary effects method the morris method morris 1991 campolongo et al 2007 in this framework the morris method is used to select the input factors to be considered later on in the sobol method the morris method consists of calculating the elementary effect for each input factor on each output variable afterwards the most important input factors are selected visually by identifying the set of input factors that are distinguished from the others because of their high mean absolute elementary effect value this selection is feasible when the number of output variables is small however when this number is high it can be inaccurate and biased furthermore to assess the convergence or to calculate the confidence intervals using bootstrapping the selection procedure should be automated in the literature a pre specified number of input factors the same for all the output variables is often used to select the most important input factors in each output variable hussein et al 2011 morris et al 2014 dejonge et al 2012 campolongo et al 2007 proposed using savage scores savage 1956 in the ranking of each output variable and ordering the input factors according to the sum of their scores however we have not found any other example using this method in the literature both criteria would lead to the selection of unimportant input factors if for example for an output variable the variance is explained by only a few number of input factors the fixed number of factors criterion would select the agreed number of input factors even if some of them are unimportant and the savage criterion would assign a high score to all the input factors in the top of the ranking even if they have low relevance furthermore the criterion based on savage scores penalizes the input factors that are important in only one output variable in favor of those that are important in several output variables even if they are correlated sarrazin et al 2016 proposed three criteria to assess the convergence of the morris and sobol methods nevertheless none of these criteria was designed to ensure the convergence of the morris method when the objective is to select the most important input factors being the number of selected input factors equal to a pre specified number but this objective arises naturally when the goal is to combine the morris and sobol methods to reduce the computational cost of the analysis in this case the criteria defined by sarrazin et al 2016 could lead to a computational surcharge to overcome these problems new selection and convergence criteria are defined here for the morris method the selection criterion referred here as the calibrated visual criterion provides a systematic basis for the screening process in turn the convergence criterion ensures that the procedure has converged to the group of the most important input factors which are those that will be proposed to enter into the sobol method the objective of this study is to define a procedure for robustly selecting the input factors that will be considered in the sobol method after the morris method is applied we illustrate the approach using a complex implementation of the flbeia bio economic fisheries simulation model garcia et al 2017b where the number of input factors is 133 the performance of the calibrated visual criterion is compared with the performance of the criterion that selects a fixed number of factors and the criterion based on savage scores from now on the savage criterion the reference for the evaluation are the rankings obtained for each output variable using the sobol method and the ranking for multivariate output models obtained with the method by lamboni et al 2011 the convergence of the sobol method is assessed using the criterion defined by sarrazin et al 2016 2 material and methods several methods to perform sensitivity analysis saltelli et al 2008 norton 2015 are available for highly nonlinear and computationally costly models the combination of the morris and the sobol methods is recommended campolongo et al 2007 the first to identify the most important input factors at low computational cost and the second to obtain a detailed decomposition of the output variance as a function of the input factors identified by the morris method 2 1 the morris elementary effects method morris introduced the elementary effects method in 1991 morris 1991 and other authors developed it further campolongo et al 2007 2011 ruano et al 2012 it is an effective mean of identifying important input factors at a lower computational cost than the sobol method saltelli et al 2008 campolongo et al 2007 improved the method s convergence through more efficient sampling of the input space furthermore they developed an expression that allows grouping of input factors and their treatment as if they were a single input factor with the subsequent reduction in computational cost extension of the methods of campolongo et al 2007 and criticism of their examples appeared in norton 2009 the method consists of evaluating the simulation model φ along a set of trajectories p defined in the unit hypercube ω 0 1 k where k corresponds with the number of input factors when the existence domain of the model is different to the unit hypercube the trajectories are transformed into the model s original domain ω using a transformation function the absolute elementary effect aee is calculated for each input factor x k for k 1 k and for each trajectory in p for simplicity of notation we will omit the k subscript for the input factor whenever it is not necessary in the context therefore the aee for input factor x k and trajectory p is defined as 1 aee p x k x φ x φ x δ where φ denotes the simulation model φ x y where y y 1 y j represents the output of the model j denotes the number of output variables x and x are two consecutive points in the trajectory p that differ only in the value of x k and δ is the width of the subintervals in the morris method finally the aee of the input factor x k aee x k is equal to the mean of the aees along all the trajectories 2 aee x k p p aee p x k r k 1 k where r denotes the cardinality of p the aee s are calculated for each output variable hence for each input factor x k there is a set of absolute elementary effects a e e x k y j j 1 j j where j is the subscript for the output variable for simplicity of notation we will omit the j subscript for the output variable whenever it is not necessary in the context the following subsections present the calibrated visual criterion to select the most important input factors and the convergence criterion for the morris method 2 1 1 the calibrated visual criterion first we define three selection criteria that jointly provide mathematical sense to the criterion used in the visual selection to give a closed expression for the three criteria for each output variable y we order the input factors according to their aee value i e aee x 1 y aee x 2 y aee x k y and define f as the set of all the input factors 1 fixed number of factors the selected input factors are those that verify that their aee are among the δ f input factors with the highest aee for at least one output variable y j 0 the set of selected input factors is denoted as f f and it is defined as 3 f f x f j 0 1 j s t aee x y j 0 aee x k δ f y j 0 2 factors with high aee value the selected input factors are those that verify that their aee is higher than a proportion δ h of the maximum value of all the aee s for at least one output variable y j 0 the set of selected input factors is denoted as f h and it is defined as 4 f h x f j 0 1 j s t aee x y j 0 δ h max aee x k y j 0 k 1 k 3 factors distinguished from the others the selected input factors are those that verify that the difference between all the consecutive aee s is higher than a proportion δ d for all the aee s with a higher aee than the input factor itself for at least one output variable y j 0 the set of selected input factors is denoted by f d and it is defined as 5 f d x f j 0 1 j s t aee x k y j 0 aee x k 1 y j 0 aee x k y j 0 δ d x k aee x k y j 0 aee x y j 0 then given p a set of trajectories along ω and k e e k the number of input factors we intend to enter into the sobol method the calibrated visual criterion is defined as the weighting of the three criteria defined above and it is applied as follows 1 evaluate the model at the points that form the trajectories in p and calculate the a e e x k y j k 1 k for all j 1 j using eq 2 2 find the parameters δ f δ h and δ d that result in the selection of k e e input factors with the fixed number of factors criterion it may be impossible to select exactly k e e input factors in which case δ f is selected as the minimum number of input factors that results in selecting a total number of input factors equal or bigger than k e e 3 to support calibration of the selection criterion conduct a visual selection of the input factors a set of input factors is selected for each output variable and the resulting sets are then merged in a single set f v the selection is done in such a way that the cardinality of f v is equal to k e e 4 apply the weighted criterion for the 3 previously defined criteria using different combination of weights firstly define a three dimensional set of values that provide a good coverage of the unit hypercube secondly for each triplet in the set of weights and each output variable the number of input factors selected is equal to the weighted mean of those selected with each of the three criteria finally once the number of input factors to be selected for each triplet is decided the ones with the highest aee are selected then the set of input factors that corresponds to each triplet of weights f w is formed by the union of the sets of input factors selected for each output variable 5 for each triplet compare the corresponding set of input factors calculated in the previous step f w with f v then identify the weights w f w h and w d that produce the largest intersection between both sets and among those select the triplet that produces the smallest cardinal of f w thus we obtain a procedure that uses the same criterion for the selection of input factors in all the output variables furthermore the input factors selected with this procedure highly agree with the visually selected ones 2 1 2 convergence criterion we consider that the morris method has converged when the input factors identified as the most important do not change when the cardinal of p is increased we assess convergence using bootstrapping and the selection criterion defined previously first we generate randomly a sufficiently large set of trajectories p with cardinal r then using the method in campolongo et al 2007 we find the set of trajectories p r for different values of r such that r r in particular for each i and l such that r i r l once aee s are calculated for p r i we need only to evaluate the model in the trajectories that are not included in p r i in order to calculate aee s for p r l for each r we perform the bootstrapping in three steps using n boot iterations 1 apply the calibrated visual criterion to p r to obtain the weights w f w h w d as proposed for the calibrated visual criterion 2 sample with replacement r trajectories from the original set p r 3 find the value of the parameters δ f δ h and δ d as proposed for the calibrated visual criterion 4 apply the calibrated visual criterion to that sample using the set of parameters w f w h w d δ f δ h δ d obtained in previous steps 5 repeat the process steps 2 4 n boot times to assess convergence we define the indicator m x r for each r and each input factor x 6 m x r i 1 n boot π x r i where π x r is equal to 1 if input factor x has been selected in iteration i and 0 otherwise if an input factor is selected in all the bootstrap iterations i e if m x r n boot the input factor is among the most relevant ones therefore to identify the k e e most important input factors it would be sufficient to increase the number of trajectories r until k e e input factors are selected in all the bootstrap iterations however this condition could be very demanding and therefore the criterion can be relaxed using a proportion α of n b o o t we define f r as the set of input factors selected in at least α n b o o t bootstrap iterations when r trajectories are used 7 f r x f m x r α n b o o t if k r is the cardinality of f r k r increases with r and we consider that the process has converged when r 0 r such that 8 k r 0 k r 0 1 k r max in general k r max is lower than k e e because the number of input factors selected in each bootstrap iteration are constrained to result in the selection of k e e input factors hence in general those selected in α n b o o t bootstrap iterations will be equal or lower than k e e when convergence has been achieved for the number of input factors to be selected we define three criteria to select the input factors to be considered when applying the sobol method f m the set of input factors selected with the maximum r r max used in the application of the morris method 9 f m f r max the union 10 f m r r 0 r max f r the intersection 11 f m r r 0 r max f r the three criteria yield a different number of selected input factors because in the distribution tail of the aees some input factors go in and out of f r in terms of selecting a smaller number of input factors the most restrictive option is the third whereas the second is the most conservative and the first is intermediate as a general procedure we can examine the degree of difference between the three options in terms of the set of selected input factors fig 1 shows the application of the whole proposal including the two criteria the calibrated visual criterion for selection and the bootstrap for convergence 2 2 sobol variance decomposition method the sobol method consists of the decomposition of the output variance as a function of the variance of the conditional expectations of the output on any combination of input factors sobol 1993 homma and saltelli 1996 proposed summarizing the contribution of the input factors to the output variance using two sensitivity indices first order and total effects the first is equal to the ratio between the variance of the conditional expectation of the model output on k th input factor and the total variance of the model output mathematically 12 s k v e y x k v y where x k denotes the k th input factor y φ x is the unidimensional output of the simulation model represented by φ and x x 1 x k represents the model input this index represents the contribution of the k th input factor to the output variance in isolation in turn the total effect is equal to the expected value of the conditional variance of the model output on all the input factors but one the k th input factor denoted here as x k it represents the contribution to the variance of the k th input factor alone and in combination with the remaining input factors mathematically it is written as 13 s t k e v y x k v y in simple cases the sensitivity indices can be calculated analytically however in most cases the models are too complex to allow the derivation of analytical expressions for eqs 12 and 13 saltelli et al 2010 compared different approaches for calculating the sobol sensitivity indices using monte carlo simulations here we use the approach that was identified as the best in terms of the convergence rate first two independent matrices of dimension n m are constructed a and b the so called sample and re sample matrices where n and m are the number of base simulations and input factors of the model respectively the input factors can be multivariate and therefore m can be larger than the number of input factors in the model k when the input factors are divided in groups instead of considering every input factor alone the elements in the sobol decomposition that include this input factor represent the contribution to the variance of all the input factors in the group as a whole in isolation in the case of first order index and in combination with other sets of input factors in the case of the rest of the elements in the decomposition of variance hence the input factors should be grouped sensibly to obtain meaningful results second additional k matrices c k k 1 k are constructed from the a and b matrices each c k matrix is equal to a except in the columns that correspond to the k th input factor which are taken from matrix b finally the model is applied to each of the rows of a b and c k k 1 k matrices the numerator in eq 12 is then approximated by 14 v e y x k 1 n i 1 n φ b i φ c k i φ a i where a i b i and c k i denote the i th row of matrices a b and c k respectively in turn the numerator in eq 13 is estimated as 15 e v y x k 1 2 n i 1 n φ c k i φ a i 2 finally the total variance v y is approximated by 16 v y 1 n i 1 n φ a i 2 1 n i 1 n φ a i 2 the convergence of the estimators can be assessed using the bootstrap confidence intervals width sarrazin et al 2016 2 2 1 multivariate outputs the generalized sensitivity indices proposed by lamboni et al 2011 are the equivalent of the sensitivity indices defined in the previous section but for the overall variance of the output of a model with a multidimensional output the generalized indices are based on the work of campbell et al 2006 who proposed to decompose the multivariate output in an orthogonal system and then apply the sensitivity indices to the most informative components individually lamboni et al 2011 developed further the idea proposed by campbell et al 2006 and using principal component analysis as orthogonal decomposition proved that the first order and total effect indices calculated on the sum of the principal components are to multivariate outputs what the sobol sensitivity indices are to the univariate one 2 3 performance of the selection criterion two performance indicators are defined to evaluate the performance of the calibrated visual criterion and other two selection criteria the selection of a fixed number of factors for each output variable and the criterion based on savage scores campolongo et al 2007 they are based on the total effect indices calculated on the reduced simulation model obtained introducing variability exclusively in the k ee input factors selected with the morris method the first performance indicator uses the set of sobol s total effect sensitivity indices for each output variable y j s t j s t k j k 1 k ee to assess the performance of the criterion where s t k j denotes the total effect of the k th input factor for output variable y j in turn the second one the generalized performance indicator uses the generalized total effect indices for multivariate output defined by lamboni et al 2011 g t g t k k 1 k ee where g t k denotes the generalized total effect index of k th input factor to assess the performance of the criterion under different conditions the performance indicators are calculated for different sets of output variables γ and different number of input factors in the morris method let us z denote the number of input factors used in the fixed number of factors criterion to calculate the set of input factors in the morris method then the performance indicators are calculated as follows the fixed number of factors criterion is applied to the morris elementary effects selecting the z input factors with the highest elementary effect value the resulting number of selected input factors is denoted as k ee z the calibrated visual criterion is applied using k ee z number of input factors as threshold the savage criterion is applied selecting the k ee z input factors with the highest score for a given selection criterion to calculate the performance indicator for output variable y first the corresponding total effect values are assigned to the input factors selected in the application of the criterion x 1 x k ee z i e 17 ρ k j 0 if x k f m s t k j i 1 k ee s t i j otherwise where s t k j corresponds with the total effect of input factor x k for output variable y j then the first performance indicator θ is calculated as the ratio between the sum of all the ρ k j over all the input factors selected by the criterion and all the output variables in γ the sum is then divided by the number of output variables to place the possible values of the indicator between 0 and 1 18 θ 1 γ j 1 γ k 1 k ee z ρ k j the second performance indicator the generalized indicator θ g is calculated similarly but instead of having one total effect index per output variable y there is only one total effect index for all the output variables hence ρ depends only on the input factors and in eq 18 the sum along output variables and the division by the number of output variables disappear in the comparison of the three criteria the one with the highest θ is the criterion which produces the best selection of input factors values of θ equal to 1 indicate that the input factors selected by the criterion are the k ee z input factors in the top of the ranking for all the output variables in the case of the first indicator and for the ranking obtained with the generalized total effect index in the case of generalized one the procedure is not applied to z 1 because it implies to select δ h and δ d in such a way that only one input factor per output variable is selected i e the three criteria are equivalent 2 4 illustrative example 2 4 1 general description the approach is illustrated using a complex implementation of flbeia garcia et al 2017b a bio economic simulation model that is used to describe fishery systems in flbeia the fishery system is divided in two main components the real system that includes the fish stocks and the fishing fleets and the management system that is formed by the data collection the assessment model and the management advice the main components of flbeia are represented in fig 2 all the variables in the real system are subject to natural variability and the variables observed in the management procedure are subject to epistemic uncertainty the model has been applied to the demersal fishery operating around the iberian peninsula in southern europe this fishery comprises seven fleets the activity of which is divided into segments called metiers the model includes explicitly the stocks caught by the fleets for which absolute estimates of abundance are available hake horse mackerel four spot megrim megrim and monkfish furthermore the model includes three widely distributed stocks western horse mackerel mackerel and blue whiting because of their economic relevance however as the catch extracted is a marginal part of the total catch of these stocks the impact on their biomass is minor and therefore it has been assumed constant along the simulation the remaining stocks have been aggregated at metier level in one total stock called oth as no abundance estimate for any of these stocks exists it has been assumed that the catch is a function of the metier s effort and independent of biomass a brief description of the submodels used to describe the processes that constitute this specific implementation of flbeia is given in table 1 a detailed description of the case study appears in garcia et al 2017a 2 4 2 uncertainty conditioning table a 5 in appendix a shows a description of the k 133 input factors of the model some of them are single input factors and others correspond with a set of input factors introduced in the model as a group cariboni and campolongo 2004 as a general rule a uniform distribution has been used to simulate uncertainty in the input factors the exceptions are maturity and retention curves effort share along metiers and aging error the values of maturity and retention curves have been simulated using a beta distribution and effort share and aging error using a dirichlet distribution the parameters of the distributions have been obtained constraining the mean to the value in garcia et al 2017a and the coefficient of variation cv to a 30 in the case of multiplicative observation errors that are not included in garcia et al 2017a a mean equal to one has been used i e the errors are unbiased the aging error has been modeled using a square matrix in which elements a i l describe the probability of assigning age i to a fish of age l and corresponds to the expected value of the dirichlet distribution the matrix is the noise only unbiased matrix in reeves 2003 following recommendations in saltelli et al 2010 we have sampled the unit hypercube using the sobol pseudo random sequences sobol 1967 to accelerate convergence for univariate input factors the values have been transformed from the unit hypercube to the original space ω using inverse transformation method the conditioning and transformation of effort share and aging error has been done using the procedure proposed in devroye 1986 for dirichlet distribution 2 4 3 output variables the output of the simulation model has been summarized using five variables per stock and four variables per fleet which results in j 37 output variables per year the stock variables are the spawning stock biomass ssb and recruitment which are related to the stock abundance fishing mortality and catch which are representative of their exploitation level and the total allowable catch tac which is the output of the decision making process the fleets performance has been summarized using effort profits gross value added and number of vessels effort represents the fleets activity the profits represent their economic performance the gross value added is a measure of the goods produced by the fishing activity and the number of vessels shows the variation in the capital 3 results 3 1 morris elementary effects method first we have generated a set of r 1000 trajectories p along ω then for r 25 50 100 150 200 250 300 we have applied the procedure described in campolongo et al 2007 to find p r in terms of illustration the calibrated visual criterion has been applied with the objective of reducing the number of input factors to the half i e k e e 67 however any other objective would be also valid a set of weights covering the unit cube with intervals of 0 01 width has been used for the weighting procedure several weight combinations produce the best match with the visual selection and therefore the combination that minimizes the euclidean distance to 1 3 1 3 1 3 has been chosen for r 300 the greatest weight 0 53 has been assigned to the fixed number of factors criterion 0 35 to the high aee value criterion and 0 12 to the difference in aee criterion the method s convergence has been assessed using a bootstrap with n boot 500 iterations the number of input factors selected in the 500 iterations increases quickly with the number of trajectories with r 25 trajectories only 19 input factors have been selected in all the iterations and with r 300 this number has increased to 50 fig 3 when the criterion is relaxed to 95 of the iterations for r 25 42 input factors are selected and then the number of input factors increases steadily and becomes stable at 55 input factors for r 200 the sets f 200 f 250 and f 300 differ in one factor this occurs because the difference in the value of the aees of the input factors that are not in the top of the ranking is so small that the ranking in those positions needs more iterations to stabilize hence to be cautious we have used the union criterion for r 200 which results in the selection of the 56 input factors listed in table 2 although the objective is to select 67 input factors only 56 have been finally selected the number of input factors selected with the calibrated visual criterion in each bootstrap iteration varies between 60 and 72 with median equal to 66 and mode equal to 67 the number of input factors is not always equal to 67 even if the number of input factors selected by the fixed number of factors criterion is 67 the application of the weighted criterion does not guarantee that the number of input factors selected is 67 this happens because whereas the restriction of selecting 67 input factors is applied to the three criteria globally the weighted criterion is applied at output variable level afterwards in the analysis of convergence only 56 input factors have been selected in 95 of the bootstrap iterations therefore there is a set of more than 10 input factors entering and leaving the group of the most important 67 input factors that is the number of input factors in the calibrated visual criterion should be increased to end up with a larger group of selected input factors however the difference between the input factors that are not in the top of the ranking is that small that the ranking of those input factors is difficult to stabilize in general for recruitment spawning stock biomass tac and number of vessels there is a set of input factors that are differentiated from the rest because of their higher aee value see the graphs in the supplementary material s1 s2 and s3 however for the remaining variables the differentiation is not equally clear for most of the output variables the difference between the number of input factors selected visually and those selected with the calibrated visual criterion is equal or lower than one table 3 the biggest difference is obtained in the profits of trawlers where visually 12 input factors are selected and with the calibrated visual criterion only 7 when a set of input factors exists that is clearly distinguished from the rest the visual selection is more precise however when the differentiation between sets is unclear in the case of fishing mortality and output variables relative to hake for example the calibrated visual criterion tends to select more input factors furthermore the variability in the number of input factors selected is higher for the visual criterion and the number of input factors selected is lower in general although variable by variable some differences exist between the visual and calibrated criteria as the input factors are aggregated in a single set and the most important input factors appear at the top of many of the variables at the overall level the differences are small the application of the morris method results in the selection of most of the biological input factors 24 input factors out of 35 69 on the contrary only a few economic input factors have been selected 5 out of 33 15 in the observation error category almost half of the input factors have been selected 16 out of 34 47 and in the case of technical input factors one third 11 out of 33 33 the uncertainty derived from observation errors identified as important by the morris method in many cases can be reduced improving the sampling programs and the mathematical models used to estimate the stock status in this sense a variance decomposition gsa including those input factors would provide the basis to carry out a cost benefit analysis of improving the assessment process of these stocks although the uncertainty in the rest of the selected input factors cannot be reduced this analysis highlights the importance of considering uncertainty in these input factors when the performance of management strategies is evaluated in the long term for example natural mortality is often considered constant and has been identified as an important input factor for all the stocks the uncertainty related with the recruitment process has been classified as important in all the cases in line with common practice the tacs of the pelagic stocks considered non target stocks for this fishery and included in this analysis as secondary stocks have been identified as one of the most important input factors in line with the claims of the fishing sector most of the economic input factors have been rejected one of the reasons could be that the fleet dynamic model used to predict the effort allocation of the fleets does not consider any economic incentive and economic input factors are simply used to transform the fish tons caught into monetary terms the effort share input factor has been identified as important for all the fleets stressing the importance of considering fleet dynamic models in this kind of simulation models catchability the input factor that measures the productivity of the fleets has been selected only in one third of the cases in fact the input factor that differed in f 250 and f 300 is the catchability of pair trawlers on hake the aees for all the input factors and output variables for r 300 are provided as supplementary material in a shiny application https aztigps shinyapps io gsaapp password flbeiagsa the code and data to run the application locally can be downloaded from zenodo https zenodo org record 3402534 garcia 2019 3 2 sobol variance decomposition method we have analyzed the convergence of the sobol sensitivity indices examining the width of the bootstrap confidence intervals as proposed by sarrazin et al 2016 the width decreases rapidly with the number of base simulations n for n 2000 fig 4 for n 150 the width of the confidence interval of the total effect index of all the input factors and output variables in 2020 is greater than 0 5 but for n 1500 75 of the intervals are already narrower than 0 05 however the decrease rate slows for n 2000 and for n 10 000 4 of the confidence intervals are wider than 0 05 fig 4 in general most of the variance of the output variables is explained by the interaction between input factors the number of vessels the recruitment and the ssb are the variables of which the variance is explained by the smaller number of input factors on the opposite side the variance of the output variables related with effort fishing mortality effort itself profits and gva is explained by a great number of input factors fig 5 once the sensitivity indices have been calculated for n 10 000 we have used the method proposed by lamboni et al 2011 to calculate the generalized global sensitivity indices using the output variables in 2020 the main result obtained at output variable level is corroborated by the global index the output variance is largely explained by the interaction between input factors fig 6 when the 37 variables are used thirty input factors are lower sensitivity factors contributing less than 5 to the overall variance sarrazin et al 2016 i e only 26 input factors contribute considerably to the output variance the total effect of the generalized sensitivity index has been used to calculate the performance indicators of the selection criterion as the index depends on the output variable used it has been calculated for each set of the output variables a complete set of barplots with the first order and total effect indices and their confidence intervals is available in a shiny application https aztigps shinyapps io gsaapp password flbeiagsa the code and data to run the application locally can be downloaded from zenodo https zenodo org record 3402534 garcia 2019 3 3 performance of the selection criterion the individual and overall level output variables defined in section 2 3 have been calculated for z 2 3 4 and for the three criteria the calibrated visual criterion the fixed number criterion and the savage criterion for z 4 the number of input factors selected with the calibrated visual criterion is higher than 56 hence it makes no sense to calculate the performance indicator because all the input factors selected by the morris method are selected by the calibrated visual criterion furthermore we have evaluated the sensitivity of the performance of the calibrated visual criterion to the choice of the output variables we take three subsets of the output variables calculate the corresponding generalized sensitivity indices and apply the selection criterion using the output variables selected to calculate the performance indicator in the first set we use all the output variables i e a set with 37 variables in the second subset with 29 variables we remove the fishing mortality and the gross value added from the output variables because they are highly correlated with the other variables in the third subset with 21 variables besides fishing mortality and gross value added we also eliminate catch and effort hence in the subsets with 21 and 29 variables we remove the output variables that are highly correlated with the rest furthermore with this choice we make the output variables of which the variance is explained by few input factors predominant the performance indicator of the calibrated visual criterion is always closer to one than that of fixed number of factors criterion table 4 i e the input factors selected with the calibrated visual criterion correspond with input factors that are higher in the ranking of the total effects the indicator for savage criterion is the indicator closest to one only for the indicator at overall level when z 4 table 4 4 discussion we have defined a selection criterion for the morris elementary effects method that allows to select the most important input factors using a criterion that mimics the visual selection ideally the selection should be done visually however the visual selection is not easily applied consistently when the number of output variables is high and the discrimination among input factors is unclear furthermore it cannot be applied in an automatic way for example in bootstrap simulations the new criterion defined here provides a good approximation of the visual approach and has the advantages of being consistent along the whole selection process and of being able to be used in an automatic way other authors use the fixed number of factors criterion applied to each output variable hussein et al 2011 morris et al 2014 dejonge et al 2012 this approach is consistent along output variables but could lead to unimportant input factors being selected in some cases for example in recruitment and to important ones being discarded in others for example in profits campolongo et al 2007 use savage scores savage 1956 to identify the most important input factors in a multi dimensional output model however savage scores are mostly used to compare ranking of input factors obtained using different approaches confalonieri et al 2010 borgonovo et al 2003 cucurachi et al 2016 and their performance as a selection criterion has never been evaluated the calibrated visual criterion is better than fixed number of factors and savage criteria when comparing their performance for each output variable hence if the objective is to explain the variance of every single output variable the calibrated visual criterion would be always preferred for example in the case study used here the savage criterion discards the input factor that explains most of the variance in the number of vessels output variable this happens because savage criterion penalizes the input factors that are important in only one output variable in favor of those that are important in several variables even if the variables are correlated however at overall level if an small number of input factors are selected the performance of the savage criterion is better this is because the basis of the generalized sensitivity index is more similar to the savage criterion than to the calibrated visual one in summary if the interest is to select the input factors that are the most important at overall level even if the input factors that explain a significant part of the variance of a single output variable are left out and the number of input factors to be selected is low the savage criterion would be preferable however if the focus is on explaining the variance of every single output variable or the number of input factors to be selected is high the calibrated visual criterion would be better the performance of the criteria has been evaluated using the ranking of the total effects estimated by the sobol method considered as the reference method by many authors yang 2011 confalonieri et al 2010 sarrazin et al 2016 homma and saltelli 1996 for the input factors selected by one of the criteria evaluated here the calibrated visual criterion this fact may seem to produce a positive bias towards this criterion however the number of input factors selected by the criteria in the evaluation are lower than those considered in the sobol method especially for z 4 hence the ranking used for the performance evaluation is considered sufficiently broad to provide an unbiased assessment we select lamboni et al s 2011 method to calculate the multivariate indices because of its simplicity and ease of application we discard garcia cabrejo and valocchi s 2014 method because it requires adjusting a metamodel based on the polynomial chaos expansion the most recent method xu et al s 2018 uses an index to assess the inputs effect on the entire joint probability distribution of the multivariate output but its application is complex in the convergence assessment of the morris method we take 95 as a threshold to ensure convergence nevertheless other values could also be adequate considering that higher values slow the convergence and lower values could lead to the selection of unimportant input factors we recommend to use high values of α as long as computational resources allow it we could have assessed the convergence using the factor screening criterion in sarrazin et al 2016 this criterion focuses on the width of the confidence interval of the non selected input factors input factors x for which m x r 0 95 n b o o t and considers that it has converged when the width is narrower than 0 05 of the 77 input factors with m x r 0 95 n b o o t only 26 i e 34 have converged when r 300 therefore according to this criterion we should increase r with the subsequent increase in computational cost 5 conclusions we have defined a selection and a convergence criteria to ensure a robust combination of the morris method with the sobol method or other gsa methods with a high computational cost the calibrated visual criterion mimics the visual selection criterion combining three of the features that are considered when selecting the input factors visually the value of the absolute elementary effects in relation to the maximum the number of input factors selected and the difference in the absolute elementary effects between consecutive input factors the criterion provides an objective method to select the most important input factors and a procedure to automatize the process the automation allows its use in simulation mode which is essential for calculating the confidence intervals of the indices using bootstrapping in a comparison of the performance of three selection criteria the visual calibrated criterion has been the best of the three the convergence criterion has been specifically defined to ensure the convergence of the morris method when the objective is to select a maximum number of input factors moreover the computing load required to achieve convergence for this criterion has proved to be lower than the criterion focused in the width of the confidence intervals declaration of competing interest no author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have impending conflict with this work for full disclosure statements refer to https doi org 10 1016 j envsoft 2019 104517 acknowledgments financial support for this study was provided in part by the department of agriculture fishing and food from the basque government flbeia and impacpes grant agreement no 227390 and 289257 respectively the department of education language policy and culture from the basque government it1294 19 and berc 2018 2021 program the spanish ministry of economy and competitiveness mineco and feder mtm2016 74931 p and bcam severo ochoa excellence accreditation sev 2017 0718 we would like to thank the two anonymous reviewers for their detailed comments and suggestions that have helped to greatly improve the quality of the document and editage http www editage com for editing and reviewing this manuscript for english language this is publication number 936 of azti appendix a list of input factors see table a 5 appendix b notation a b the sample and re sample matrices used to compute the importance indices in sobol method a i b i c k i the i th row of the corresponding matrix c k the matrix that is equal to a except in the column s that correspond with the k th input factor that is are taken from b matrix f the set of all the input factors f f the set of input factors selected using the fixed number of factors criterion f h the set of input factors selected with the factors with high aee value criterion f d the set of input factors selected with the factors distinguished from the others criterion f m the set of input factors selected when the morris method is applied f r the set of input factors selected with morris method when r trajectories are used f v the set of input factors selected with the visual procedure f w the set of input factors selected with the weighted criterion g t k total effect generalized index of the k th input factor g t the set of total generalized indices k subscript used for input factors along the manuscript j subscript used in output variables along the manuscript j the dimension of the output of the simulation model k the number of input factors k ee the number of input factors chosen a priori to be selected with the morris method to be considered in the sobol method k ee z the number of input factors selected with the fixed number criterion when z input factors are selected for each output variable k r the cardinality of f r m number of input factors without grouping in the sobol method m x r the number of iterations in which a input factor x is selected in the bootstrap of the morris method with r trajectories n the base sample size in the sobol method n boot number of bootstrap iterations p a large enough set of trajectories defined in ω p r the r trajectories within p that provide the best coverage of ω p a trajectory in ω that belongs to p r the number of trajectories used in the morris method r the cardinality of p r m a x maximum number of trajectories used in the morris method s k first order index for the k th input factor s t k total effect index for the k th input factor s t k j total effect index for the k th input factor and output variable y j s t j the set of total effects of output variable y j w f the weight given to the fixed number of factors criterion in the computation of the calibrated visual criterion w h the weight given to the factors with high aee value criterion in the computation of the calibrated visual criterion w d the weight given to the factors distinguished from the others criterion in the computation of the calibrated visual criterion x input factor x or sampling point in ω or ω x k k th input factor x k a sampling point in ω or ω conditioned in all the input factors except the k th one y an unidimensional output variable y a multidimensional output variable z number of input factors selected for each indicator in the application of fixed number of factors in the evaluation of the performance of the selection indicators α the threshold used for proportion to select the important input factors in the bootstrap of the morris methods δ the width of the subintervals in the morris method δ f the number of input factors selected in the fixed number of factors selection criterion δ h the proportion used in the factors with high aee value selection criterion δ d the proportion used in the factors distinguished from the others criterion to select those input factors that are aside of the rest γ a set of output variables π x r i indicator variable of input factor x to be selected in iteration i of the morris method with r trajectories φ the simulation model ω the existence domain of the simulation model ω the 0 1 k unit hypercube ρ k j auxiliar variable used to calculate the performance indicators θ and θ g in the evaluation of the selection criteria corresponding to the k th input factor and the j th output variable θ the first performance indicator θ g the generalized performance indicator appendix c supplementary data supplementary material related to this article can be found online at http dx doi org 10 1016 j envsoft 2019 104517 appendix a supplementary data the following is the supplementary material related to this article mmc s1 
26116,conducting global sensitivity analysis using variance decomposition methods in complex simulation models with many input factors is usually unaffordable an alternative is to first apply a screening method to reduce the number of input factors and then apply a variance decomposition method to the reduced model however usually selection of input factors is not done robustly and convergence of the screening method is not ensured we propose two new criteria a criterion that mimics the visual selection of the input factors and a convergence criterion in the application of the criteria to a complex model the morris screening method has needed 200 trajectories to converge and the visual criterion has outperformed other existing criteria our proposal ensures a robust combination of the morris and the sobol methods that provides an objective and automatic method to select the most important input factors with a feasible computing load to achieve convergence abbreviations aee absolute elementary effect cv coefficient of variation gsa global sensitivity analysis ssb spawning stock biomass tac total allowable catch keywords convergence criterion global sensitivity analysis morris elementary effect method selection criterion sobol variance decomposition method software availability the r functions to implement the selection and convergence criteria are available in zenodo https zenodo org record 3402534 garcia 2019 in the same repository there is an example showing how to use these functions in practice 1 introduction simulation models are useful tools to provide a better understanding of the environmental systems one of the big issues when using simulation models is their validation that is to ensure the model is good enough to meet its intended purpose rykiel 1996 schmolke et al 2010 balci 1997 presented a long list of techniques for the validation of simulation models divided in four groups informal techniques which rely on human reasoning and lack mathematical formalism static techniques which are concerned with accuracy assessment on the basis of characteristics of the static model design and source code dynamic techniques which evaluate the model based on its execution behavior and formal techniques which are based on the mathematical proof and correctness global sensitivity analysis gsa which has been proposed by several authors as a key ingredient in the validation process of simulation models saltelli et al 2000 rabitz 1989 is in the third group the dynamic techniques variance based gsa examines the relation between the variance of the output of the simulation models and the variance of their input factors saltelli et al 2008 several methods for performing gsa exist from simple scatterplots to the more complex sobol variance decomposition method the sobol method sobol 1993 see pianosi et al 2016 borgonovo and plischke 2016 or norton 2015 for recent reviews on existing methods and practices the sobol method is frequently considered the reference method for variance based gsa yang 2011 confalonieri et al 2010 sarrazin et al 2016 homma and saltelli 1996 the method can be used to rank the input factors according to their effect on the results and to estimate each input factor s contribution to the output variance two of the main drawbacks of this method are its high computational cost and its inability to represent the outputs uncertainties correctly if the model output is highly skewed borgonovo et al 2011 pianosi and wagener 2015 the computational cost of applying the sobol method on highly nonlinear simulation models with many input factors could be unaffordable in those cases a frequently used alternative is to combine the sobol method with the morris elementary effects method the morris method morris 1991 campolongo et al 2007 in this framework the morris method is used to select the input factors to be considered later on in the sobol method the morris method consists of calculating the elementary effect for each input factor on each output variable afterwards the most important input factors are selected visually by identifying the set of input factors that are distinguished from the others because of their high mean absolute elementary effect value this selection is feasible when the number of output variables is small however when this number is high it can be inaccurate and biased furthermore to assess the convergence or to calculate the confidence intervals using bootstrapping the selection procedure should be automated in the literature a pre specified number of input factors the same for all the output variables is often used to select the most important input factors in each output variable hussein et al 2011 morris et al 2014 dejonge et al 2012 campolongo et al 2007 proposed using savage scores savage 1956 in the ranking of each output variable and ordering the input factors according to the sum of their scores however we have not found any other example using this method in the literature both criteria would lead to the selection of unimportant input factors if for example for an output variable the variance is explained by only a few number of input factors the fixed number of factors criterion would select the agreed number of input factors even if some of them are unimportant and the savage criterion would assign a high score to all the input factors in the top of the ranking even if they have low relevance furthermore the criterion based on savage scores penalizes the input factors that are important in only one output variable in favor of those that are important in several output variables even if they are correlated sarrazin et al 2016 proposed three criteria to assess the convergence of the morris and sobol methods nevertheless none of these criteria was designed to ensure the convergence of the morris method when the objective is to select the most important input factors being the number of selected input factors equal to a pre specified number but this objective arises naturally when the goal is to combine the morris and sobol methods to reduce the computational cost of the analysis in this case the criteria defined by sarrazin et al 2016 could lead to a computational surcharge to overcome these problems new selection and convergence criteria are defined here for the morris method the selection criterion referred here as the calibrated visual criterion provides a systematic basis for the screening process in turn the convergence criterion ensures that the procedure has converged to the group of the most important input factors which are those that will be proposed to enter into the sobol method the objective of this study is to define a procedure for robustly selecting the input factors that will be considered in the sobol method after the morris method is applied we illustrate the approach using a complex implementation of the flbeia bio economic fisheries simulation model garcia et al 2017b where the number of input factors is 133 the performance of the calibrated visual criterion is compared with the performance of the criterion that selects a fixed number of factors and the criterion based on savage scores from now on the savage criterion the reference for the evaluation are the rankings obtained for each output variable using the sobol method and the ranking for multivariate output models obtained with the method by lamboni et al 2011 the convergence of the sobol method is assessed using the criterion defined by sarrazin et al 2016 2 material and methods several methods to perform sensitivity analysis saltelli et al 2008 norton 2015 are available for highly nonlinear and computationally costly models the combination of the morris and the sobol methods is recommended campolongo et al 2007 the first to identify the most important input factors at low computational cost and the second to obtain a detailed decomposition of the output variance as a function of the input factors identified by the morris method 2 1 the morris elementary effects method morris introduced the elementary effects method in 1991 morris 1991 and other authors developed it further campolongo et al 2007 2011 ruano et al 2012 it is an effective mean of identifying important input factors at a lower computational cost than the sobol method saltelli et al 2008 campolongo et al 2007 improved the method s convergence through more efficient sampling of the input space furthermore they developed an expression that allows grouping of input factors and their treatment as if they were a single input factor with the subsequent reduction in computational cost extension of the methods of campolongo et al 2007 and criticism of their examples appeared in norton 2009 the method consists of evaluating the simulation model φ along a set of trajectories p defined in the unit hypercube ω 0 1 k where k corresponds with the number of input factors when the existence domain of the model is different to the unit hypercube the trajectories are transformed into the model s original domain ω using a transformation function the absolute elementary effect aee is calculated for each input factor x k for k 1 k and for each trajectory in p for simplicity of notation we will omit the k subscript for the input factor whenever it is not necessary in the context therefore the aee for input factor x k and trajectory p is defined as 1 aee p x k x φ x φ x δ where φ denotes the simulation model φ x y where y y 1 y j represents the output of the model j denotes the number of output variables x and x are two consecutive points in the trajectory p that differ only in the value of x k and δ is the width of the subintervals in the morris method finally the aee of the input factor x k aee x k is equal to the mean of the aees along all the trajectories 2 aee x k p p aee p x k r k 1 k where r denotes the cardinality of p the aee s are calculated for each output variable hence for each input factor x k there is a set of absolute elementary effects a e e x k y j j 1 j j where j is the subscript for the output variable for simplicity of notation we will omit the j subscript for the output variable whenever it is not necessary in the context the following subsections present the calibrated visual criterion to select the most important input factors and the convergence criterion for the morris method 2 1 1 the calibrated visual criterion first we define three selection criteria that jointly provide mathematical sense to the criterion used in the visual selection to give a closed expression for the three criteria for each output variable y we order the input factors according to their aee value i e aee x 1 y aee x 2 y aee x k y and define f as the set of all the input factors 1 fixed number of factors the selected input factors are those that verify that their aee are among the δ f input factors with the highest aee for at least one output variable y j 0 the set of selected input factors is denoted as f f and it is defined as 3 f f x f j 0 1 j s t aee x y j 0 aee x k δ f y j 0 2 factors with high aee value the selected input factors are those that verify that their aee is higher than a proportion δ h of the maximum value of all the aee s for at least one output variable y j 0 the set of selected input factors is denoted as f h and it is defined as 4 f h x f j 0 1 j s t aee x y j 0 δ h max aee x k y j 0 k 1 k 3 factors distinguished from the others the selected input factors are those that verify that the difference between all the consecutive aee s is higher than a proportion δ d for all the aee s with a higher aee than the input factor itself for at least one output variable y j 0 the set of selected input factors is denoted by f d and it is defined as 5 f d x f j 0 1 j s t aee x k y j 0 aee x k 1 y j 0 aee x k y j 0 δ d x k aee x k y j 0 aee x y j 0 then given p a set of trajectories along ω and k e e k the number of input factors we intend to enter into the sobol method the calibrated visual criterion is defined as the weighting of the three criteria defined above and it is applied as follows 1 evaluate the model at the points that form the trajectories in p and calculate the a e e x k y j k 1 k for all j 1 j using eq 2 2 find the parameters δ f δ h and δ d that result in the selection of k e e input factors with the fixed number of factors criterion it may be impossible to select exactly k e e input factors in which case δ f is selected as the minimum number of input factors that results in selecting a total number of input factors equal or bigger than k e e 3 to support calibration of the selection criterion conduct a visual selection of the input factors a set of input factors is selected for each output variable and the resulting sets are then merged in a single set f v the selection is done in such a way that the cardinality of f v is equal to k e e 4 apply the weighted criterion for the 3 previously defined criteria using different combination of weights firstly define a three dimensional set of values that provide a good coverage of the unit hypercube secondly for each triplet in the set of weights and each output variable the number of input factors selected is equal to the weighted mean of those selected with each of the three criteria finally once the number of input factors to be selected for each triplet is decided the ones with the highest aee are selected then the set of input factors that corresponds to each triplet of weights f w is formed by the union of the sets of input factors selected for each output variable 5 for each triplet compare the corresponding set of input factors calculated in the previous step f w with f v then identify the weights w f w h and w d that produce the largest intersection between both sets and among those select the triplet that produces the smallest cardinal of f w thus we obtain a procedure that uses the same criterion for the selection of input factors in all the output variables furthermore the input factors selected with this procedure highly agree with the visually selected ones 2 1 2 convergence criterion we consider that the morris method has converged when the input factors identified as the most important do not change when the cardinal of p is increased we assess convergence using bootstrapping and the selection criterion defined previously first we generate randomly a sufficiently large set of trajectories p with cardinal r then using the method in campolongo et al 2007 we find the set of trajectories p r for different values of r such that r r in particular for each i and l such that r i r l once aee s are calculated for p r i we need only to evaluate the model in the trajectories that are not included in p r i in order to calculate aee s for p r l for each r we perform the bootstrapping in three steps using n boot iterations 1 apply the calibrated visual criterion to p r to obtain the weights w f w h w d as proposed for the calibrated visual criterion 2 sample with replacement r trajectories from the original set p r 3 find the value of the parameters δ f δ h and δ d as proposed for the calibrated visual criterion 4 apply the calibrated visual criterion to that sample using the set of parameters w f w h w d δ f δ h δ d obtained in previous steps 5 repeat the process steps 2 4 n boot times to assess convergence we define the indicator m x r for each r and each input factor x 6 m x r i 1 n boot π x r i where π x r is equal to 1 if input factor x has been selected in iteration i and 0 otherwise if an input factor is selected in all the bootstrap iterations i e if m x r n boot the input factor is among the most relevant ones therefore to identify the k e e most important input factors it would be sufficient to increase the number of trajectories r until k e e input factors are selected in all the bootstrap iterations however this condition could be very demanding and therefore the criterion can be relaxed using a proportion α of n b o o t we define f r as the set of input factors selected in at least α n b o o t bootstrap iterations when r trajectories are used 7 f r x f m x r α n b o o t if k r is the cardinality of f r k r increases with r and we consider that the process has converged when r 0 r such that 8 k r 0 k r 0 1 k r max in general k r max is lower than k e e because the number of input factors selected in each bootstrap iteration are constrained to result in the selection of k e e input factors hence in general those selected in α n b o o t bootstrap iterations will be equal or lower than k e e when convergence has been achieved for the number of input factors to be selected we define three criteria to select the input factors to be considered when applying the sobol method f m the set of input factors selected with the maximum r r max used in the application of the morris method 9 f m f r max the union 10 f m r r 0 r max f r the intersection 11 f m r r 0 r max f r the three criteria yield a different number of selected input factors because in the distribution tail of the aees some input factors go in and out of f r in terms of selecting a smaller number of input factors the most restrictive option is the third whereas the second is the most conservative and the first is intermediate as a general procedure we can examine the degree of difference between the three options in terms of the set of selected input factors fig 1 shows the application of the whole proposal including the two criteria the calibrated visual criterion for selection and the bootstrap for convergence 2 2 sobol variance decomposition method the sobol method consists of the decomposition of the output variance as a function of the variance of the conditional expectations of the output on any combination of input factors sobol 1993 homma and saltelli 1996 proposed summarizing the contribution of the input factors to the output variance using two sensitivity indices first order and total effects the first is equal to the ratio between the variance of the conditional expectation of the model output on k th input factor and the total variance of the model output mathematically 12 s k v e y x k v y where x k denotes the k th input factor y φ x is the unidimensional output of the simulation model represented by φ and x x 1 x k represents the model input this index represents the contribution of the k th input factor to the output variance in isolation in turn the total effect is equal to the expected value of the conditional variance of the model output on all the input factors but one the k th input factor denoted here as x k it represents the contribution to the variance of the k th input factor alone and in combination with the remaining input factors mathematically it is written as 13 s t k e v y x k v y in simple cases the sensitivity indices can be calculated analytically however in most cases the models are too complex to allow the derivation of analytical expressions for eqs 12 and 13 saltelli et al 2010 compared different approaches for calculating the sobol sensitivity indices using monte carlo simulations here we use the approach that was identified as the best in terms of the convergence rate first two independent matrices of dimension n m are constructed a and b the so called sample and re sample matrices where n and m are the number of base simulations and input factors of the model respectively the input factors can be multivariate and therefore m can be larger than the number of input factors in the model k when the input factors are divided in groups instead of considering every input factor alone the elements in the sobol decomposition that include this input factor represent the contribution to the variance of all the input factors in the group as a whole in isolation in the case of first order index and in combination with other sets of input factors in the case of the rest of the elements in the decomposition of variance hence the input factors should be grouped sensibly to obtain meaningful results second additional k matrices c k k 1 k are constructed from the a and b matrices each c k matrix is equal to a except in the columns that correspond to the k th input factor which are taken from matrix b finally the model is applied to each of the rows of a b and c k k 1 k matrices the numerator in eq 12 is then approximated by 14 v e y x k 1 n i 1 n φ b i φ c k i φ a i where a i b i and c k i denote the i th row of matrices a b and c k respectively in turn the numerator in eq 13 is estimated as 15 e v y x k 1 2 n i 1 n φ c k i φ a i 2 finally the total variance v y is approximated by 16 v y 1 n i 1 n φ a i 2 1 n i 1 n φ a i 2 the convergence of the estimators can be assessed using the bootstrap confidence intervals width sarrazin et al 2016 2 2 1 multivariate outputs the generalized sensitivity indices proposed by lamboni et al 2011 are the equivalent of the sensitivity indices defined in the previous section but for the overall variance of the output of a model with a multidimensional output the generalized indices are based on the work of campbell et al 2006 who proposed to decompose the multivariate output in an orthogonal system and then apply the sensitivity indices to the most informative components individually lamboni et al 2011 developed further the idea proposed by campbell et al 2006 and using principal component analysis as orthogonal decomposition proved that the first order and total effect indices calculated on the sum of the principal components are to multivariate outputs what the sobol sensitivity indices are to the univariate one 2 3 performance of the selection criterion two performance indicators are defined to evaluate the performance of the calibrated visual criterion and other two selection criteria the selection of a fixed number of factors for each output variable and the criterion based on savage scores campolongo et al 2007 they are based on the total effect indices calculated on the reduced simulation model obtained introducing variability exclusively in the k ee input factors selected with the morris method the first performance indicator uses the set of sobol s total effect sensitivity indices for each output variable y j s t j s t k j k 1 k ee to assess the performance of the criterion where s t k j denotes the total effect of the k th input factor for output variable y j in turn the second one the generalized performance indicator uses the generalized total effect indices for multivariate output defined by lamboni et al 2011 g t g t k k 1 k ee where g t k denotes the generalized total effect index of k th input factor to assess the performance of the criterion under different conditions the performance indicators are calculated for different sets of output variables γ and different number of input factors in the morris method let us z denote the number of input factors used in the fixed number of factors criterion to calculate the set of input factors in the morris method then the performance indicators are calculated as follows the fixed number of factors criterion is applied to the morris elementary effects selecting the z input factors with the highest elementary effect value the resulting number of selected input factors is denoted as k ee z the calibrated visual criterion is applied using k ee z number of input factors as threshold the savage criterion is applied selecting the k ee z input factors with the highest score for a given selection criterion to calculate the performance indicator for output variable y first the corresponding total effect values are assigned to the input factors selected in the application of the criterion x 1 x k ee z i e 17 ρ k j 0 if x k f m s t k j i 1 k ee s t i j otherwise where s t k j corresponds with the total effect of input factor x k for output variable y j then the first performance indicator θ is calculated as the ratio between the sum of all the ρ k j over all the input factors selected by the criterion and all the output variables in γ the sum is then divided by the number of output variables to place the possible values of the indicator between 0 and 1 18 θ 1 γ j 1 γ k 1 k ee z ρ k j the second performance indicator the generalized indicator θ g is calculated similarly but instead of having one total effect index per output variable y there is only one total effect index for all the output variables hence ρ depends only on the input factors and in eq 18 the sum along output variables and the division by the number of output variables disappear in the comparison of the three criteria the one with the highest θ is the criterion which produces the best selection of input factors values of θ equal to 1 indicate that the input factors selected by the criterion are the k ee z input factors in the top of the ranking for all the output variables in the case of the first indicator and for the ranking obtained with the generalized total effect index in the case of generalized one the procedure is not applied to z 1 because it implies to select δ h and δ d in such a way that only one input factor per output variable is selected i e the three criteria are equivalent 2 4 illustrative example 2 4 1 general description the approach is illustrated using a complex implementation of flbeia garcia et al 2017b a bio economic simulation model that is used to describe fishery systems in flbeia the fishery system is divided in two main components the real system that includes the fish stocks and the fishing fleets and the management system that is formed by the data collection the assessment model and the management advice the main components of flbeia are represented in fig 2 all the variables in the real system are subject to natural variability and the variables observed in the management procedure are subject to epistemic uncertainty the model has been applied to the demersal fishery operating around the iberian peninsula in southern europe this fishery comprises seven fleets the activity of which is divided into segments called metiers the model includes explicitly the stocks caught by the fleets for which absolute estimates of abundance are available hake horse mackerel four spot megrim megrim and monkfish furthermore the model includes three widely distributed stocks western horse mackerel mackerel and blue whiting because of their economic relevance however as the catch extracted is a marginal part of the total catch of these stocks the impact on their biomass is minor and therefore it has been assumed constant along the simulation the remaining stocks have been aggregated at metier level in one total stock called oth as no abundance estimate for any of these stocks exists it has been assumed that the catch is a function of the metier s effort and independent of biomass a brief description of the submodels used to describe the processes that constitute this specific implementation of flbeia is given in table 1 a detailed description of the case study appears in garcia et al 2017a 2 4 2 uncertainty conditioning table a 5 in appendix a shows a description of the k 133 input factors of the model some of them are single input factors and others correspond with a set of input factors introduced in the model as a group cariboni and campolongo 2004 as a general rule a uniform distribution has been used to simulate uncertainty in the input factors the exceptions are maturity and retention curves effort share along metiers and aging error the values of maturity and retention curves have been simulated using a beta distribution and effort share and aging error using a dirichlet distribution the parameters of the distributions have been obtained constraining the mean to the value in garcia et al 2017a and the coefficient of variation cv to a 30 in the case of multiplicative observation errors that are not included in garcia et al 2017a a mean equal to one has been used i e the errors are unbiased the aging error has been modeled using a square matrix in which elements a i l describe the probability of assigning age i to a fish of age l and corresponds to the expected value of the dirichlet distribution the matrix is the noise only unbiased matrix in reeves 2003 following recommendations in saltelli et al 2010 we have sampled the unit hypercube using the sobol pseudo random sequences sobol 1967 to accelerate convergence for univariate input factors the values have been transformed from the unit hypercube to the original space ω using inverse transformation method the conditioning and transformation of effort share and aging error has been done using the procedure proposed in devroye 1986 for dirichlet distribution 2 4 3 output variables the output of the simulation model has been summarized using five variables per stock and four variables per fleet which results in j 37 output variables per year the stock variables are the spawning stock biomass ssb and recruitment which are related to the stock abundance fishing mortality and catch which are representative of their exploitation level and the total allowable catch tac which is the output of the decision making process the fleets performance has been summarized using effort profits gross value added and number of vessels effort represents the fleets activity the profits represent their economic performance the gross value added is a measure of the goods produced by the fishing activity and the number of vessels shows the variation in the capital 3 results 3 1 morris elementary effects method first we have generated a set of r 1000 trajectories p along ω then for r 25 50 100 150 200 250 300 we have applied the procedure described in campolongo et al 2007 to find p r in terms of illustration the calibrated visual criterion has been applied with the objective of reducing the number of input factors to the half i e k e e 67 however any other objective would be also valid a set of weights covering the unit cube with intervals of 0 01 width has been used for the weighting procedure several weight combinations produce the best match with the visual selection and therefore the combination that minimizes the euclidean distance to 1 3 1 3 1 3 has been chosen for r 300 the greatest weight 0 53 has been assigned to the fixed number of factors criterion 0 35 to the high aee value criterion and 0 12 to the difference in aee criterion the method s convergence has been assessed using a bootstrap with n boot 500 iterations the number of input factors selected in the 500 iterations increases quickly with the number of trajectories with r 25 trajectories only 19 input factors have been selected in all the iterations and with r 300 this number has increased to 50 fig 3 when the criterion is relaxed to 95 of the iterations for r 25 42 input factors are selected and then the number of input factors increases steadily and becomes stable at 55 input factors for r 200 the sets f 200 f 250 and f 300 differ in one factor this occurs because the difference in the value of the aees of the input factors that are not in the top of the ranking is so small that the ranking in those positions needs more iterations to stabilize hence to be cautious we have used the union criterion for r 200 which results in the selection of the 56 input factors listed in table 2 although the objective is to select 67 input factors only 56 have been finally selected the number of input factors selected with the calibrated visual criterion in each bootstrap iteration varies between 60 and 72 with median equal to 66 and mode equal to 67 the number of input factors is not always equal to 67 even if the number of input factors selected by the fixed number of factors criterion is 67 the application of the weighted criterion does not guarantee that the number of input factors selected is 67 this happens because whereas the restriction of selecting 67 input factors is applied to the three criteria globally the weighted criterion is applied at output variable level afterwards in the analysis of convergence only 56 input factors have been selected in 95 of the bootstrap iterations therefore there is a set of more than 10 input factors entering and leaving the group of the most important 67 input factors that is the number of input factors in the calibrated visual criterion should be increased to end up with a larger group of selected input factors however the difference between the input factors that are not in the top of the ranking is that small that the ranking of those input factors is difficult to stabilize in general for recruitment spawning stock biomass tac and number of vessels there is a set of input factors that are differentiated from the rest because of their higher aee value see the graphs in the supplementary material s1 s2 and s3 however for the remaining variables the differentiation is not equally clear for most of the output variables the difference between the number of input factors selected visually and those selected with the calibrated visual criterion is equal or lower than one table 3 the biggest difference is obtained in the profits of trawlers where visually 12 input factors are selected and with the calibrated visual criterion only 7 when a set of input factors exists that is clearly distinguished from the rest the visual selection is more precise however when the differentiation between sets is unclear in the case of fishing mortality and output variables relative to hake for example the calibrated visual criterion tends to select more input factors furthermore the variability in the number of input factors selected is higher for the visual criterion and the number of input factors selected is lower in general although variable by variable some differences exist between the visual and calibrated criteria as the input factors are aggregated in a single set and the most important input factors appear at the top of many of the variables at the overall level the differences are small the application of the morris method results in the selection of most of the biological input factors 24 input factors out of 35 69 on the contrary only a few economic input factors have been selected 5 out of 33 15 in the observation error category almost half of the input factors have been selected 16 out of 34 47 and in the case of technical input factors one third 11 out of 33 33 the uncertainty derived from observation errors identified as important by the morris method in many cases can be reduced improving the sampling programs and the mathematical models used to estimate the stock status in this sense a variance decomposition gsa including those input factors would provide the basis to carry out a cost benefit analysis of improving the assessment process of these stocks although the uncertainty in the rest of the selected input factors cannot be reduced this analysis highlights the importance of considering uncertainty in these input factors when the performance of management strategies is evaluated in the long term for example natural mortality is often considered constant and has been identified as an important input factor for all the stocks the uncertainty related with the recruitment process has been classified as important in all the cases in line with common practice the tacs of the pelagic stocks considered non target stocks for this fishery and included in this analysis as secondary stocks have been identified as one of the most important input factors in line with the claims of the fishing sector most of the economic input factors have been rejected one of the reasons could be that the fleet dynamic model used to predict the effort allocation of the fleets does not consider any economic incentive and economic input factors are simply used to transform the fish tons caught into monetary terms the effort share input factor has been identified as important for all the fleets stressing the importance of considering fleet dynamic models in this kind of simulation models catchability the input factor that measures the productivity of the fleets has been selected only in one third of the cases in fact the input factor that differed in f 250 and f 300 is the catchability of pair trawlers on hake the aees for all the input factors and output variables for r 300 are provided as supplementary material in a shiny application https aztigps shinyapps io gsaapp password flbeiagsa the code and data to run the application locally can be downloaded from zenodo https zenodo org record 3402534 garcia 2019 3 2 sobol variance decomposition method we have analyzed the convergence of the sobol sensitivity indices examining the width of the bootstrap confidence intervals as proposed by sarrazin et al 2016 the width decreases rapidly with the number of base simulations n for n 2000 fig 4 for n 150 the width of the confidence interval of the total effect index of all the input factors and output variables in 2020 is greater than 0 5 but for n 1500 75 of the intervals are already narrower than 0 05 however the decrease rate slows for n 2000 and for n 10 000 4 of the confidence intervals are wider than 0 05 fig 4 in general most of the variance of the output variables is explained by the interaction between input factors the number of vessels the recruitment and the ssb are the variables of which the variance is explained by the smaller number of input factors on the opposite side the variance of the output variables related with effort fishing mortality effort itself profits and gva is explained by a great number of input factors fig 5 once the sensitivity indices have been calculated for n 10 000 we have used the method proposed by lamboni et al 2011 to calculate the generalized global sensitivity indices using the output variables in 2020 the main result obtained at output variable level is corroborated by the global index the output variance is largely explained by the interaction between input factors fig 6 when the 37 variables are used thirty input factors are lower sensitivity factors contributing less than 5 to the overall variance sarrazin et al 2016 i e only 26 input factors contribute considerably to the output variance the total effect of the generalized sensitivity index has been used to calculate the performance indicators of the selection criterion as the index depends on the output variable used it has been calculated for each set of the output variables a complete set of barplots with the first order and total effect indices and their confidence intervals is available in a shiny application https aztigps shinyapps io gsaapp password flbeiagsa the code and data to run the application locally can be downloaded from zenodo https zenodo org record 3402534 garcia 2019 3 3 performance of the selection criterion the individual and overall level output variables defined in section 2 3 have been calculated for z 2 3 4 and for the three criteria the calibrated visual criterion the fixed number criterion and the savage criterion for z 4 the number of input factors selected with the calibrated visual criterion is higher than 56 hence it makes no sense to calculate the performance indicator because all the input factors selected by the morris method are selected by the calibrated visual criterion furthermore we have evaluated the sensitivity of the performance of the calibrated visual criterion to the choice of the output variables we take three subsets of the output variables calculate the corresponding generalized sensitivity indices and apply the selection criterion using the output variables selected to calculate the performance indicator in the first set we use all the output variables i e a set with 37 variables in the second subset with 29 variables we remove the fishing mortality and the gross value added from the output variables because they are highly correlated with the other variables in the third subset with 21 variables besides fishing mortality and gross value added we also eliminate catch and effort hence in the subsets with 21 and 29 variables we remove the output variables that are highly correlated with the rest furthermore with this choice we make the output variables of which the variance is explained by few input factors predominant the performance indicator of the calibrated visual criterion is always closer to one than that of fixed number of factors criterion table 4 i e the input factors selected with the calibrated visual criterion correspond with input factors that are higher in the ranking of the total effects the indicator for savage criterion is the indicator closest to one only for the indicator at overall level when z 4 table 4 4 discussion we have defined a selection criterion for the morris elementary effects method that allows to select the most important input factors using a criterion that mimics the visual selection ideally the selection should be done visually however the visual selection is not easily applied consistently when the number of output variables is high and the discrimination among input factors is unclear furthermore it cannot be applied in an automatic way for example in bootstrap simulations the new criterion defined here provides a good approximation of the visual approach and has the advantages of being consistent along the whole selection process and of being able to be used in an automatic way other authors use the fixed number of factors criterion applied to each output variable hussein et al 2011 morris et al 2014 dejonge et al 2012 this approach is consistent along output variables but could lead to unimportant input factors being selected in some cases for example in recruitment and to important ones being discarded in others for example in profits campolongo et al 2007 use savage scores savage 1956 to identify the most important input factors in a multi dimensional output model however savage scores are mostly used to compare ranking of input factors obtained using different approaches confalonieri et al 2010 borgonovo et al 2003 cucurachi et al 2016 and their performance as a selection criterion has never been evaluated the calibrated visual criterion is better than fixed number of factors and savage criteria when comparing their performance for each output variable hence if the objective is to explain the variance of every single output variable the calibrated visual criterion would be always preferred for example in the case study used here the savage criterion discards the input factor that explains most of the variance in the number of vessels output variable this happens because savage criterion penalizes the input factors that are important in only one output variable in favor of those that are important in several variables even if the variables are correlated however at overall level if an small number of input factors are selected the performance of the savage criterion is better this is because the basis of the generalized sensitivity index is more similar to the savage criterion than to the calibrated visual one in summary if the interest is to select the input factors that are the most important at overall level even if the input factors that explain a significant part of the variance of a single output variable are left out and the number of input factors to be selected is low the savage criterion would be preferable however if the focus is on explaining the variance of every single output variable or the number of input factors to be selected is high the calibrated visual criterion would be better the performance of the criteria has been evaluated using the ranking of the total effects estimated by the sobol method considered as the reference method by many authors yang 2011 confalonieri et al 2010 sarrazin et al 2016 homma and saltelli 1996 for the input factors selected by one of the criteria evaluated here the calibrated visual criterion this fact may seem to produce a positive bias towards this criterion however the number of input factors selected by the criteria in the evaluation are lower than those considered in the sobol method especially for z 4 hence the ranking used for the performance evaluation is considered sufficiently broad to provide an unbiased assessment we select lamboni et al s 2011 method to calculate the multivariate indices because of its simplicity and ease of application we discard garcia cabrejo and valocchi s 2014 method because it requires adjusting a metamodel based on the polynomial chaos expansion the most recent method xu et al s 2018 uses an index to assess the inputs effect on the entire joint probability distribution of the multivariate output but its application is complex in the convergence assessment of the morris method we take 95 as a threshold to ensure convergence nevertheless other values could also be adequate considering that higher values slow the convergence and lower values could lead to the selection of unimportant input factors we recommend to use high values of α as long as computational resources allow it we could have assessed the convergence using the factor screening criterion in sarrazin et al 2016 this criterion focuses on the width of the confidence interval of the non selected input factors input factors x for which m x r 0 95 n b o o t and considers that it has converged when the width is narrower than 0 05 of the 77 input factors with m x r 0 95 n b o o t only 26 i e 34 have converged when r 300 therefore according to this criterion we should increase r with the subsequent increase in computational cost 5 conclusions we have defined a selection and a convergence criteria to ensure a robust combination of the morris method with the sobol method or other gsa methods with a high computational cost the calibrated visual criterion mimics the visual selection criterion combining three of the features that are considered when selecting the input factors visually the value of the absolute elementary effects in relation to the maximum the number of input factors selected and the difference in the absolute elementary effects between consecutive input factors the criterion provides an objective method to select the most important input factors and a procedure to automatize the process the automation allows its use in simulation mode which is essential for calculating the confidence intervals of the indices using bootstrapping in a comparison of the performance of three selection criteria the visual calibrated criterion has been the best of the three the convergence criterion has been specifically defined to ensure the convergence of the morris method when the objective is to select a maximum number of input factors moreover the computing load required to achieve convergence for this criterion has proved to be lower than the criterion focused in the width of the confidence intervals declaration of competing interest no author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have impending conflict with this work for full disclosure statements refer to https doi org 10 1016 j envsoft 2019 104517 acknowledgments financial support for this study was provided in part by the department of agriculture fishing and food from the basque government flbeia and impacpes grant agreement no 227390 and 289257 respectively the department of education language policy and culture from the basque government it1294 19 and berc 2018 2021 program the spanish ministry of economy and competitiveness mineco and feder mtm2016 74931 p and bcam severo ochoa excellence accreditation sev 2017 0718 we would like to thank the two anonymous reviewers for their detailed comments and suggestions that have helped to greatly improve the quality of the document and editage http www editage com for editing and reviewing this manuscript for english language this is publication number 936 of azti appendix a list of input factors see table a 5 appendix b notation a b the sample and re sample matrices used to compute the importance indices in sobol method a i b i c k i the i th row of the corresponding matrix c k the matrix that is equal to a except in the column s that correspond with the k th input factor that is are taken from b matrix f the set of all the input factors f f the set of input factors selected using the fixed number of factors criterion f h the set of input factors selected with the factors with high aee value criterion f d the set of input factors selected with the factors distinguished from the others criterion f m the set of input factors selected when the morris method is applied f r the set of input factors selected with morris method when r trajectories are used f v the set of input factors selected with the visual procedure f w the set of input factors selected with the weighted criterion g t k total effect generalized index of the k th input factor g t the set of total generalized indices k subscript used for input factors along the manuscript j subscript used in output variables along the manuscript j the dimension of the output of the simulation model k the number of input factors k ee the number of input factors chosen a priori to be selected with the morris method to be considered in the sobol method k ee z the number of input factors selected with the fixed number criterion when z input factors are selected for each output variable k r the cardinality of f r m number of input factors without grouping in the sobol method m x r the number of iterations in which a input factor x is selected in the bootstrap of the morris method with r trajectories n the base sample size in the sobol method n boot number of bootstrap iterations p a large enough set of trajectories defined in ω p r the r trajectories within p that provide the best coverage of ω p a trajectory in ω that belongs to p r the number of trajectories used in the morris method r the cardinality of p r m a x maximum number of trajectories used in the morris method s k first order index for the k th input factor s t k total effect index for the k th input factor s t k j total effect index for the k th input factor and output variable y j s t j the set of total effects of output variable y j w f the weight given to the fixed number of factors criterion in the computation of the calibrated visual criterion w h the weight given to the factors with high aee value criterion in the computation of the calibrated visual criterion w d the weight given to the factors distinguished from the others criterion in the computation of the calibrated visual criterion x input factor x or sampling point in ω or ω x k k th input factor x k a sampling point in ω or ω conditioned in all the input factors except the k th one y an unidimensional output variable y a multidimensional output variable z number of input factors selected for each indicator in the application of fixed number of factors in the evaluation of the performance of the selection indicators α the threshold used for proportion to select the important input factors in the bootstrap of the morris methods δ the width of the subintervals in the morris method δ f the number of input factors selected in the fixed number of factors selection criterion δ h the proportion used in the factors with high aee value selection criterion δ d the proportion used in the factors distinguished from the others criterion to select those input factors that are aside of the rest γ a set of output variables π x r i indicator variable of input factor x to be selected in iteration i of the morris method with r trajectories φ the simulation model ω the existence domain of the simulation model ω the 0 1 k unit hypercube ρ k j auxiliar variable used to calculate the performance indicators θ and θ g in the evaluation of the selection criteria corresponding to the k th input factor and the j th output variable θ the first performance indicator θ g the generalized performance indicator appendix c supplementary data supplementary material related to this article can be found online at http dx doi org 10 1016 j envsoft 2019 104517 appendix a supplementary data the following is the supplementary material related to this article mmc s1 
26117,inefficient irrigation practices in the alluvial lower arkansas river basin larb of colorado are contributing to salinization waterlogging reduced crop yields and harmful concentrations of pollutants in the stream aquifer system intensive data collection and modeling efforts in the larb over the past 20 years have resulted in development of the gis based basin scale decision support system river geodss parallel efforts in regional scale calibration and application of the modflow sfr2 rt3d otis stream aquifer system model permit evaluation of best management practices bmps designed to mollify adverse environmental impacts since bmp implementation is allowable only if water laws are not violated a deep learning model is developed to serve as an accurate compute efficient surrogate of modflow sfr2 and is imbedded in river geodss for assessing basin scale impacts of bmp implementations on stream aquifer exchange and water rights it is shown that bmps can be implemented while maintaining reasonable water law compliance with development of a new reservoir storage account keywords irrigation stream aquifer systems river basin management groundwater modeling machine learning artificial neural networks geographic information systems water law compliance 1 introduction waterlogging and salinization are age old maladies that continue to plague irrigated areas worldwide causing an estimated annual loss of over 27 billion in crop production adeel 2014 in the u s an estimated 30 crop yield reduction occurs due to salinization of irrigated lands u s department of agriculture 2018 soils become waterlogged when saturated conditions predominate due to over irrigation and poor drainage often contributing to salinization by inhibiting the leaching of salts intrinsic to the applied waters and creating degraded conditions by the transport of salts from underlying shallow groundwater to the surface via capillary action furthermore intense irrigation and fertilization of alluvial soils contribute to excessive nitrate no3 concentrations that can accelerate the dissolution and mobilization of inherent salts and other mineral pollutants e g selenium se and uranium u into alluvial aquifers with hydraulic connection to rivers this in turn elevates surface water pollutant concentrations to levels that imperil the ecological health of the riverine environment bailey et al 2012 gates et al 2009 mueller price and gates 2008 seiler 1995 shultz et al 2018a it is clear that salinization and related nonpoint source pollution pose a serious threat to our most productive agro ecological systems and the long term sustainability of irrigated agriculture ozerol et al 2012 a variety of water and land best management practices bmps have the potential to lower solute concentrations toward boosting agricultural productivity while meeting regulatory water quality standards and reducing ecological damage gates et al 2012 shultz et al 2018b some of the most effective practices involve reducing irrigation diversions by increasing application efficiencies and lowering canal conveyance losses these measures however can lead to altered rates and patterns of surface return flows generated by precipitation and tailwater runoff from irrigated fields along with alterations to the more dominant groundwater return flows due to irrigation percolation and canal seepage into adjacent unconfined aquifers intersecting stream channels as illustrated in fig 1 a major constraining consequence is that flows in the receiving streams can be substantially changed by such bmps thereby potentially damaging legal access of downstream water users in river basins governed by some form of the prior appropriation doctrine first in time first in right and or interstate compacts fully integrated river basin management strategies that consider the important political legal and institutional aspects of water allocation in the basin along with realistic modeling of the complex interconnected stream aquifer system are required for evaluating the viability of proposed bmps there are several generalized river basin management software packages that can be used including modsim labadie 2010 riverware zagona et al 2001 ribasim krogt 2008 and weap yates et al 2005 unfortunately these models are ill suited to providing realistic analysis of complex stream aquifer interactions since they utilize simplistic lumped parameter groundwater models based on the glover method glover and balmer 1954 glover 1974 such as the stream depletion factor sdf method jenkins 1968 or they represent the aquifer as a simple linear reservoir attempts have been made to provide more realism by directly linking these models to 3 dimensional finite difference models such as the usgs modflow groundwater flow model langevin et al 2017 but at the price of exorbitant computational costs morway et al 2016 coupled modflow nwt niswonger et al 2011 with modsim using the unique customizing capabilities of modsim where users have direct access to all public variables and object classes for creating custom code in c net or vb net with the net clr producing high speed executable code custom code was created that directly executes compiled modflow libraries within the iterative computational structure of modsim for the current operational time step although morway et al 2016 allude to the computational cost due to numerous required iterations between modsim and modflow the cpu time requirements related to application of the coupled models to a hypothetical agricultural river basin are not mentioned as astutely noted by morway et al 2016 published attempts at linking other models with modflow fail to consider the need to perform multiple iterations within a single time step since the modflow results input into the river basin management model likely result in altered irrigation diversion rates thereby requiring re execution of modflow in response to those changes several iterations may be required until the calculated flows converge to consistent values before advancing to the next time step valerio 2008 documents the linkage of riverware with modflow using a less accurate single feed forward iteration required computer run times of up to 4 5 days for a single scenario the magnitude of computer run times required for iteratively convergent direct linkage between modflow and river basin management models is clearly unacceptable particularly when considering the many possible combinations of spatially distributed bmps that may be evaluated triana et al 2010 applied artificial neural networks ann for modeling a portion of the irrigated stream aquifer system of the lower arkansas river basin larb located in southeastern colorado measurable georeferenced spatial temporal and bmp scenario based explanatory variables were defined that were considered to be correlated with the calibrated modflow generated surface and groundwater return flow datasets these served as training input data to the anns which operated in a supervisory learning mode the anns were trained to closely match the modflow modeled return flow output datasets with portions of the datasets not included in the training reserved for testing and validation the anns were effective for modeling the highly nonlinear and complex relationships between the explanatory variables and the calculated groundwater return flow rates for a portion of colorado s larb referred to as the upstream study region usr and for assessing impacts of altered return flow patterns on downstream senior water rights while adhering to the kansas colorado interstate compact which governs required arkansas river flows available to kansas robbins and montgomery 2001 aside from the computational cost of executing the anns being a small fraction of modflow computer run times the enormous time and cost to monitor and collect the necessary field data required for modflow calibration over the entire larb would be prohibitive although the powerful interpolation capabilities of anns are well documented their effective extrapolation performance for stream aquifer modeling has been demonstrated by authors such as pektas and cigizoglu 2017 whereby anns developed from modflow modeling over a ron of an alluvial river basin can be extrapolated to similar unmodeled rons of the basin triana et al 2010 linked surrogate anns with geomodsim a geospatial version of modsim and incorporated the linked models into the river geodss decision support system for basin wide water management and evaluation of bmps by extrapolating the anns to unmodeled regions of the larb the disadvantage of this configuration is that ann development training and verification was disconnected from geomodsim rather than being fully integrated since publication of triana et al 2010 more comprehensive modflow models have been calibrated and tested for a portion of the larb east of john martin reservoir referred to as the downstream study region dsr morway et al 2013 along with updating and extending modflow modeling in the usr the updated modflow model for the usr now includes the sfr2 streamflow routing package niswonger and prudic 2005 along with the unsaturated zone flow uzf package niswonger et al 2006 and has been coupled with the rt3d reactive transport in 3 dimensions model clement 1997 bailey et al 2013 and otis one dimensional transport with inflow and storage runkel 1998 shultz et al 2018a for simulation of solute fate and transport use of a single regional modflow model to train the ann however has proven to be inapplicable to the entire basin with findings suggesting that the usr trained ann model does not perform well when tested with the dsr data and vice versa the ann architecture employed in triana et al 2010 was a single hidden layer shallow ann with radial basis activation functions with development training and testing of the ann performed outside of the river geodss software suite reported herein is the training and validation of a deep neural network dnn which is essentially an ann with many hidden processing layers and neurons in its architecture the advantage of dnns over shallow anns is that complex highly nonlinear relationships can be learned such as for modeling real world stream aquifer systems with the dnns providing enhanced generalization capabilities mhaskar et al 2017 the study presented herein uses the combined results from the two regional modflow sfr2 flow models for the usr and dsr based on datasets now available from field monitoring activities in the larb that extend several years beyond the original 1999 2001 period considered by triana et al 2010b 2010a the generalized dnn is fully integrated into geomodsim which serves as an updated version of the geospatial river basin management decision support system river geodss the renovated river geodss provides a firm foundation for developing measures for mitigating adverse impacts on senior water rights and interstate compact agreements resulting from implementation of bmps designed to quell waterlogging curb salinization and reduce toxic levels of no3 salts se and u in larb aquifers and streams 2 study area lower arkansas river basin colorado 2 1 general description fig 2 depicts the larb study area for conducting basin scale modeling which extends from the outlet of pueblo reservoir to the colorado kansas stateline and encompasses the boundaries of the irrigated river valley on both sides of the river inserts show the usr and dsr study regions where data have been gathered for calibration and validation of the modflow sfr2 and solute transport models specifically for these regions the usr upstream of john martin reservoir drains to a 78 km section of the arkansas river from manzanola eastward to adobe creek the total usr drainage area is approximately 50 000 ha with roughly half of the area devoted to irrigated agriculture the 55 000 ha dsr extends from may valley drain at lamar east of john martin reservoir for about 71 km to the colorado kansas border and includes 33 000 ha of irrigated fields field data collection for the usr model calibration and testing occurred primarily during the period 1999 to 2012 whereas most of the dsr data collection was between 2002 and 2012 average annual precipitation within the semi arid alluvial valley increases eastwardly from 284 mm just below pueblo reservoir to 386 mm at lamar in the dsr clifford and doesken 2009 report an average annual reference et of about 1295 mm in the alluvial valley during the irrigation season mar 15 to nov 15 the proportion of cultivated fields in the usr and dsr with very shallow water tables i e water table depth d wt 2 m as simulated by the models was 24 and 21 respectively as reported by morway et al 2013 indicating significant susceptibility to problems of waterlogging salinity and non beneficial water consumption gates et al 2016 2 2 surface and groundwater quality a series of sedimentary formations of late cambrian to tertiary ages comprise the larb main alluvial valley darton 1906 with strong hydraulic connections existing between the alluvium and the arkansas river and tributaries person and konikow 1986 evidence suggests that these rock formations and their weathered residuum yield a variety of salts along with se and u under the dissolving action of natural and irrigation flows bailey et al 2012 total dissolved solids tds in sampled arkansas river reaches of the larb are quite high with average values of tds around 930 mg l in the usr and 2930 mg l in the dsr gates et al 2016 posing a hazard to irrigated crops and markedly exceeding the epa drinking water limit usepa 2009 approximation of the loadings of major salt ions directly to the arkansas river not including tributary flows are estimated to occur at an average rate of about 7 5 metric tons per day per km and 15 4 metric tons per day per km along the arkansas river in the usr and dsr respectively gates et al 2016 the 85th percentile of nitrate nitrogen no3 n in river samples exceeds the total n interim standard of 2 mg l at many locations in the usr and at the two most downstream locations within the dsr gates et al 2016 dissolved se and u concentrations for all sampling events from 2006 to 2011 in the usr and from 2003 to 2011 in the dsr respectively reveal that 85th percentile values for se concentrations are about 3 and 3 3 times greater than the chronic standard of 4 6 μg l in the usr and dsr respectively the 85th percentile values of most river samples for u are just below the chronic standard of 30 μg l in the usr but 2 4 times greater in the dsr gates et al 2016 2 3 best management practices in irrigated agriculture the following bmps have been proposed for ameliorating the detrimental conditions of waterlogging salinization and nonpoint source pollution within the main alluvial valley of the larb 1 reduced irrigation ri by increasing irrigation efficiency 2 canal sealing cs to reduce seepage and 3 lease fallowing agreements lf shultz et al 2018b ri practices primarily involve altering application rates and land slopes in the traditional border and furrow water application methods and or converting to sprinkler and drip irrigation although efficiency improvements do not necessarily reduce crop water consumptive use they can lower water tables moderate waterlogging conditions and diminish return flows godbout and johnson 2018 canal sealing cs can be an effective means of reducing water losses and thereby reducing diversions and also is cost effective with the use of linear anionic polyacrylamide sealants martin and gates 2014 lease fallowing lf bmps primarily involve agreements with municipalities to receive additional water supply through intermittent fallowing of irrigated fields to avoid buy and dry scenarios that can degrade rural communities while allowing irrigators to receive an economic benefit without having to sell all their water rights other viable land bmps evaluated in shultz et al 2018b include reduced fertilizer applications rf and enhancement of vegetated riparian buffers erb to promote chemical reduction and volatilization of pollutants 2 4 stream aquifer system exchange and compact compliance flow rates in the arkansas river below pueblo dam are influenced primarily by snowmelt and runoff from the upper arkansas river basin groundwater base flow runoff from precipitation events on the eastern plains and releases from pueblo dam and john martin dam downstream the stream aquifer system of the central alluvial valley of the larb supplies water to municipalities and industry primarily using well pumping from the alluvium these flows however are small compared to stream aquifer system interactions between the river and irrigated agriculture in the valley which have the most significant impact on maintaining senior water rights and complying with the kansas colorado arkansas river compact the compact constrains the operation of irrigation systems in the larb by prohibiting any changes that would alter the amount and timing of groundwater return flows to the river colorado revised statutes 1949 to guarantee that provisions of the compact are maintained the office of the colorado state engineer has issued efficiency rules that prohibit implementation of bmps that would result in diminished return flows back to the river resulting from improved efficiency thereby risking violation of the compact the dilemma is that reductions in excess surface or subsurface flows resulting from increased irrigation efficiency which clearly improve the sustainability of irrigated agriculture by mitigating problems of waterlogging salinization and increased concentrations of nutrients and toxic trace elements morway et al 2013 shultz et al 2018 tavakoli kivi 2018 are prohibited unless otherwise augmented by appropriate changes in river operation such as with amended releases from reservoir storage 3 regional scale modflow models modflow is a popular open source software package developed by the usgs for 3d flow modeling of multi layer groundwater systems with complex boundary conditions modflow employs a numerical finite difference scheme to solve the boussinesq nonlinear parabolic partial differential equations governing groundwater flow in several aquifer layers which can be confined or unconfined harbaugh 2005 variants of modflow can also simulate unsaturated flow surface water runoff surface water storage pumping wells evapotranspiration and groundwater recharge flow and sink source output from modflow are commonly used to drive a number of solute transport models including rt3d otis qurban 2018 shultz et al 2018a 2018b colorado state university csu has been conducting continuous data collection and modeling efforts since 1999 in the usr and since 2002 in the dsr of the larb for calibration and application of regional modflow and related solute transport models for predicting impacts of bmps on the stream aquifer system it is believed that the usr and dsr are also highly representative of the un modeled regions of the lower arkansas river basin since they include about 54 of the total irrigated area in the basin the intensive data monitoring efforts have allowed construction of high spatial resolution modflow finite difference models of the usr and dsr with 3d grids including three overlapping vertical layers with 250 m 250 m cell size a weekly time step is used for all modflow simulations in the usr and dsr the newtonian structured modflow nwt niswonger et al 2011 version of modflow is linked with the uzf niswonger et al 2006 and sfr2 niswonger and prudic 2005 packages which incorporate unsaturated zone flow stream aquifer flow exchange and streamflow routing also in the usr the uzf package coupled with rt3d reactive transport in 3 dimensions bailey et al 2013 is linked with otis one dimensional transport with inflow and storage runkel 1998 and qual2e brown et al 1987 to form rt3d otis for simulating multi species transport in groundwater and interconnected streams rt3d otis has been applied to simulate no3 n and se transport in the usr shultz et al 2018a and in the dsr qurban 2018 both modflow models use the wel package for simulating specified point discharge at wells the res package simulates leakage from reservoir features such as ponds lakes and reservoirs performing similarly to the riv package by simulating leakage between a reservoir and the aquifer by acting as a head dependent flow boundary fenske et al 1996 the regional groundwater models for the usr and dsr were calibrated using a combination of manual and automated procedures bailey et al 2014 morway et al 2013 shultz et al 2018a the automated procedure applies ucode poeter and hill 1998 and pest doherty 1994 to minimize residuals between predicted and measured groundwater heads groundwater return flows canal seepage total evapotranspiration et groundwater upflux to et and recharge to infiltration ratio by adjusting parameter values for selected aquifer properties the calibrated model was then applied to simulate 67 alternative water and land bmp scenarios including 39 combined bmps for effectiveness in decreasing se and no3 contamination in the usr each combined bmp scenario along with four single bmps were simulated at basic intermediate and aggressive levels shultz et al 2018b qurban 2018 analyzed a similar though not as extensive array of bmp scenarios for mitigating se and no3 in the dsr additional water bmps were earlier considered by morway et al 2013 to examine impacts on groundwater table levels and return flows to the arkansas river and its tributaries 4 basin scale river basin management model 4 1 river geodss geospatial decision support system river geodss is a generalized geospatial decision support system for river basin management with integrated modules for river basin modeling database management and graphical user interfaces and is fully implemented in a geographic information system gis for geospatial modeling and analysis river geodss considers all the important physical and hydrologic characteristics required for developing river basin management strategies along with inclusion of complex legal and institutional mechanisms governing allocation and use of available flows in an over appropriated river basin river geodss is based on geomodsim a geospatial version of the modsim generalized river basin management model labadie 2006 2010 and is embedded as a custom extension in arcgis desktop gis 10 x environmental systems research institute 2011 arcmap the primary windows desktop application for arcgis serves as a georeferenced graphical user interface for river geodss an updated version of the basin scale decision support system river geodss is applied to generating river basin management strategies that consider the stream aquifer impacts of a wide range of bmp implementations for water quality improvement while complying with basin water rights and the colorado kansas arkansas river compact 4 2 unique capabilities of updated river geodss many of the capabilities of the original triana et al 2010 version of river geodss are retained in the updated version presented herein these include 1 automated construction of georeferenced modsim hydrologic networks generated from digital hydrographic map layers available from the national hydrography dataset nhdplusv2 2 a highly efficient network flow optimization algorithm for allocating flows in strict accordance with water and storage right priorities over monthly weekly daily and even sub daily time steps 3 tools for populating and editing the spatiotemporal database 4 setting geometric network properties in arcmap 5 access to the arcgis spatial analyst extension 6 execution of modsim directly within arcmap and 7 georeferenced display of graphical output results in the arcmap interface in the earlier version of river geodss triana et al 2010 all ann development was required to be performed outside of river geodss with the extracted georeferenced spatiotemporal explanatory variables input into the commercial modeling package matlab mathworks inc to develop the ann the trained ann was then inserted back into river geodss and executed for river basin management simulation in the updated version of river geodss presented herein all dnn model development occurs entirely within river geodss where the user selects and modifies dnn configurations in a dedicated tab in the river geodss dialog window seen in fig 3 thereby providing a seamless modeling pipeline that does not require the user to exit river geodss to complete dnn development the configurations include selection of the number of hidden layers for determining the use of either a shallow or deep neural network number of nodes per hidden layer training testing portion activation function the neural network solver and the regularization value a scalar introduced to the learning model to prevent overfitting and improve generalizability the river geodss dialog window also provides an interface for importing training data sets from numerous modflow simulations providing surface water and groundwater return flows to the mainstem river and tributaries based on a wide range of bmp implementations another important difference in the approach taken here is that inputs to the dnn constitute raw data in contrast with the manually extracted features employed in the previous work by triana et al 2010b 2010a for example in the earlier work aquifer recharge per unit area was assumed to have a direct significance to the ann output variables e g groundwater return flows with the deep learning approach minimal intervention to the dnn learning process is desired requiring that the raw variables e g aquifer thickness area stream lengths be used instead other significant updates and improvements in river geodss include 1 reduced redundancy in the coding through implementation of native arcobjects libraries environmental systems research institute 2019 instead of hard coded case specific implementations as in the original thereby providing more robust and seamless usage 2 updated georeferenced and non georeferenced databases such as inclusion of new spatiotemporal variables 3 migration from the commercial matlabtm based ann module to the scikit learn license free machine learning package pedregosa et al 2011 4 use of the significantly updated modflow models employing the sfr2 package as well as use of a more extensive simulation period and 5 combining datasets from both the usr and dsr of the larb as the source data for developing the dnn as opposed to previous usr only implementation of river geodss triana et al 2010b 2010a 5 compute efficient deep learning surrogate of regional scale models 5 1 anns and deep learning anns are a type of machine learning paradigm comprised of numerous combinations of simple processor units or neurons joined through interconnection links called synapses that result in massively parallel interconnected networks that allow application of connectionist learning procedures the synapses are assigned connection strengths or synaptic weights within which the acquired knowledge is stored the weights are used in the calculation of input activation for each neuron node in an ann layer where the weighted sum input signals from all feeder neurons to that node are essentially summed a feedforward algorithm is utilized where the activation function in each neuron processes the summed weighted inputs and passes neuron activation function output to the outgoing connected neurons in the next layer bias neurons can also be included in the input and hidden layers which along with the weighted sum of the inputs to a neuron provide an additional parameter similar to a constant intercept included in a linear equation which can provide additional flexibility by shifting the activation function in a desirable way to enhance learning in a supervisory learning mode the anns are trained by determining the weights that essentially result in a close match between measured or target outputs and the computed outputs of the trained ann where closeness can be defined in several ways the learning process usually employs the backpropagation algorithm where after information passes from the input layer to the final output layer of nodes the ann computed output values are then compared to the actual values the discrepancies are then transferred backward by progressing from the output layer back to the input layers to update the synapses connection weights that produce improved ann outputs at the end of the training process the final weight values attributed to the synapses are essentially the ann acquired knowledge from a dataset the canonical procedure after a machine learning training is then to test the learned model with an unseen data subset to validate its generalizability readers are referred to abu mostafa et al 2012 for in depth descriptions of ann methodologies advancements in computing power and affordability have propelled the development and widespread use of anns and further inspired the birth of the field of deep learning throughout the last half century significant research has been done to find accurate representations of complex data structures using the most efficient methodologies possible this research particularly blossomed in the field of computer vision where image classification was found to require highly multilayered or deep anns or dnns to circumvent this complexity a feature extraction approach can be taken which is essentially creating higher level abstractions e g lines and shapes of lower level features e g pixels that are then input to the machine learning algorithm this approach however is tedious and requires significant human intervention to the learning process particularly in the creation of higher level abstractions deep learning aims to better address the challenge its main idea being that of capturing multiple levels of knowledge representation from raw data with minimal manual interference alpaydin 2014 2016 lecun et al 2015 a dnn is a specific tool in the deep learning family which exploits multiple layers of representation to model complex relationships for supervised or unsupervised learning deng et al 2014 the aim is to allow a machine to be fed with raw data and automatically discover multiple levels of representation for regression and classification the key ingredient of deep learning is its raw data input and multilayered hidden units and in the case of dnn its employment of multiple layers of calculation nodes neurons similar to its shallow version feed forward and backpropagation algorithms are often employed in the training of dnns alpaydin 2014 2016 deng et al 2014 lecun et al 2015 5 2 deep learning model development the following steps for ann and dnn model development were suggested by wu et al 2014 1 input feature selection 2 data splitting 3 model architecture selection 4 model structure selection 5 model calibration and 6 model validation note that there is a distinction between model architecture and structure where the former relates to how information moves across the neural network such as the selection of feed forward recurrent jordan or recurrent elman architectures in contrast model structure focuses on the properties of the network itself e g the number of parallel layers of neuron nodes or processing elements numbers of nodes per layer and the activation function selected for the processing elements the development of dnn surrogates of the regional scale modflow models for this study deviates slightly from the protocol of wu et al 2014 with the steps taken here including 1 network architecture and feature selection 2 model structure selection 3 model calibration and 4 model validation since the model architecture step is combined with feature or input layer selection and this study only utilizes the feedforward neural network architecture discussion of this modeling step deals only with feature selection the data splitting step is also merged with model structure selection which is explained subsequently the model calibration step focuses on training and testing of the neural network where for example 10 of the entire input output dataset for supervisory learning may be utilized for performing the model validation step with the remaining 90 further sub divided into training and testing datasets the validated neural network model is then incorporated into river geodss as a compute efficient emulator of the modflow sfr2 stream aquifer system model 5 3 neural network architecture and feature selection fig 4 displays the input output structure of the feedforward dnn developed as a surrogate of modflow sfr2 for supervised learning neuron nodes in the input layer of the neural network represent measurable explanatory variables categorized as spatial temporal and scenario based inputs with the latter reflecting the wide range of water bmp combinations and intensities as modeled by modflow sfr2 the dnn output variables are groundwater and overland return flows to streams resulting from a large number of modflow sfr2 simulations for a wide range of bmps to predict those output variables spatial temporal and scenario based explanatory variables are selected using an ad hoc approach where the variables are expected to have a significant hydrologic impact on the behavior of the return flow output variables fff the spatial input variables are measured using gis spatial analysis operations on georeferenced maps with the temporal input variables including precipitation measurements and groundwater pumping rates the scenario based input variables are of two types management scenario dependent and geomodsim dependent where the latter are river flows and canal diversions calculated by geomodsim based on bmp impacts water right priorities and other administrative mechanisms all temporal explanatory variables i e precipitation pumping streamflow and average diversion are in weekly time increments ranging over the historical period 31 december 1998 through 30 december 2009 and spatially aggregated in buffer zones the buffer zones are defined as valley areas parallel to the main river channel with a longitudinal length of 15 km and incremental width of 3 km on both the north and south sides of the river the methodology for buffer zone aggregation is discussed in detail in triana et al 2010 maier et al 2010 stress the importance of ensuring input variable independence in the input data selection process where improperly accounting for input variable redundancy can result in increases in the number of neural network connection weights requiring training leading to multi modal fitting error surfaces and increasing the likelihood of overfitting extensive linear correlation analyses were conducted between all possible pairs of the original 15 explanatory variables with the results summarized in fig 5 three variables i e canal elevation stream elevation and buffer zone elevation are seen to be strongly correlated with each other requiring removal of two of them from the set of explanatory variables i e stream elevation and canal elevation 5 4 model structure selection for this study the model selection process includes selecting data splitting methods network architecture solver selection activation functions and the regularization value data splitting generally separates data into training testing and validation subsets the training subset is applied to training of the neural network whereas the testing subset determines if overfitting has occurred i e when training should be terminated the validation subset is used to assess the generalization capability of the trained neural network maier et al 2010 wu et al 2014 emphasize the importance of justifying the data splitting method by comparison with alternative methods for this study two types of data sampling methods were considered randomized and sequential sampling with each method having 10 of the data saved for the validation subset along with nine variations of training and testing percentage pairs of the remaining 90 data subset 10 90 20 80 30 70 40 60 50 50 60 40 70 30 80 20 and 90 10 where the first percentage applies to the training subset and the second to the testing subset for the randomized sampling method the 10 validation subset was sampled in advance in a randomized fashion with the remaining 90 sampled for the training subset also in a randomized manner leaving the remainder as the testing subset for sequential sampling the earliest 10 of the data were saved as the validation subset and the latest 90 portion serving as the training subset with the remaining datasets applied to model testing the decision to compare these two sampling groups i e randomized or sequential is based on the popularity of applying these two methods as seen in the literature he et al 2014 and gong et al 2016 applied sequential data sampling for training whereas triana et al 2010 and wu et al 2015 employed randomized data sampling wu et al 2014 also mention the importance of developing a well described and justified neural network architecture by comparing alternative architectures here the architecture selection is based on how many hidden layers are required as well as the number of neurons or nodes in each layer in this study up to 2000 nodes per layer were utilized with various types of solvers activation functions and regularization values also evaluated the hyperbolic tangent tanh rectified linear unit relu logistic and identity functions were considered as activation function alternatives with the following eight regularization values considered 0 0 00001 0 0001 0 001 0 01 0 1 1 and 10 the limited memory broyden fletcher goldfarb shanno algorithm lbfgs andrew and gao 2007 stochastic gradient descent sgd robbins and monro 1985 and adam kingma and ba 2015 were the solver alternatives considered 5 5 model testing and validation more than 40 000 neural network configurations were evaluated in this study out of this large number of trained networks only the best performing model was employed in the application step although a metamodeling approach could have been employed to find the best performing neural network broad et al 2015 a simple brute force approach was utilized with consideration of the available computing resources and the decision to avoid an extra layer of computing due to the large scale nature of the neural networks the replicative and predictive validation criteria were assessed which is consistent with two of the three validation methods suggested by humphrey et al 2017 replicative validity of a model can be confirmed using scatter plots of the predicted versus observed data where a good result indicates that the model captures the underlying characteristics in the data used for model training calibration predictive validation on the other hand is applied to determine the model generalization capability over the range of the calibration data where the validation of the trained neural network can be used for confirmation to avoid overfitting metrics used to assess performance of the trained neural networks are 1 the akaike information criterion aic akaike 1974 and 2 the amari number amari et al 1997 while r2 and rmse are the most commonly used performance metrics aic introduces more depth to the metrics aside from measuring model goodness of fit aic also penalizes model complexity this parsimony favoring nature of aic is useful for selecting the minimal model that best explains the observed data the amari number further explores model parsimony with overfitting assumed as linked to the ratio of the number of training samples to the number of connection weights where it has been shown that overfitting does not occur when the ratio exceeds 30 in this study the best performing neural network is evaluated with respect to predictive validity based on the lowest aic value while satisfying the condition of having an amari number larger than 30 the coefficient of determination r2 is also used to present the performance of the neural networks without penalization of the network complexity 6 dnn modeling results 6 1 deep neural network configuration training and testing of the many neural network configurations for the larb system required more than 5 days of computing with eleven desktop computers and servers mimicking parallel processing where the cpu specifications were multi core intel cpus at 3 4 4 0 ghz at 100 utilization each of the various run configurations required an average of 170 s per run which also represents the average cpu time required for simulating the impacts on arkansas river flows of a selected bmp scenario in geomodsim using the trained dnn for that scenario this is in contrast with the aforementioned linkage of riverware with modflow requiring up to 4 5 days for a single scenario valerio 2008 the best performing network was selected based on the lowest aic value while satisfying the amari number criteria fig 6 shows test aic values vs the number of hidden nodes per layer and network complexity for various numbers of layers and sampling methods four charts are shown random sampling with a single layer random sampling with two layers sequential sampling with one layer and sequential sampling with two layers a distinction is also made between neural networks prone to overfitting with amari numbers less than or equal to 30 shown in yellow and those considered to be safe from overfitting or having amari numbers greater than 30 shown in blue it is clear in this study that randomized sampling outperforms sequential sampling and that the two layer dnns generally outperformed the one layer shallow anns where the aic values of the two layer networks are lower at the boundary between the overfitting prone and overfitting safe points i e yellow and blue dots respectively this may indicate that the shallow anns are unable to capture the complexity and high nonlinearity of the stream aquifer system interactions being modeled in the same figure when comparing aic values against network complexity i e the secondary abscissa in fig 6 with network complexity defined as the number of connection weights estimated in the training the dnns outperform shallow anns at the same complexity this further reinforces the assertion that in this case the dnns outperform shallow anns by better capturing the system nonlinearity and complexity also evaluated were impacts on the testing aic values of changing other neural network properties although the results are not shown here it was found that randomly sampled neural networks with minimal regularization values are superior moreover test results indicated that the adam solver dominates training performance by requiring the lowest average number of iterations and average training time as compared with other solvers in comparing the various activation functions i e identity logistic relu and tanh the latter two methods outperformed the former with relu slightly leading although the identity function averaged the least amount of computation time and fastest convergence it yielded a relatively higher test aic value the logistic function required similar average computation times as relu and tanh but displayed greater variability and a higher average number of iterations along with poorer performance in terms of average and minimum test aic values 6 2 dnn training and testing another important parameter selection is the training testing percentage or data division governing the way data are divided between the training and testing datasets maier et al 2010 stress that this can have a significant impact on model performance and close attention should be given to it as indicated previously nine pairs of data division options were considered ranging from 10 training 90 testing to 90 training 10 testing with 10 increments although a higher training percentage might be selected since it produces well performing neural networks this can raise the specter of redundancy issues not related to structural configuration of the neural network due to over training since maier and dandy 2000 stress the importance of training and testing sets being representative of the same population a comparison of the nine data splitting methods was conducted using principal component analysis pearson 1901 to compare the data splitting options presented in fig 7 are density plots of the first principal component for the training and testing portions of the data as well as the nine training testing percentage options the chart generally indicates that most of the data splitting options having between a 40 and 80 training portion produce visually similar training and testing data density plot evaluating the merits of using dnns fig 8 provides a comparison of the r2 statistic and training time as affected by the number of hidden layers in this comparison all neural networks were trained with 50 nodes per hidden layer 50 training portion adam solver relaxed regularization relu activation function and the randomized sampling method although fig 8 shows a significant increase in performance using dnns instead of a shallow ann performance gained by increasing the depth of a neural network beyond 3 layers does not appear to be proportional to the increase in required training time moreover violation of the amari number criterion becomes apparent starting from a four layered dnn upwards therefore it is reasonable to conclude that two or three layered dnns are satisfactory for modeling stream aquifer interactions in the larb with deeper neural networks providing only minimal improvement but at the expense of increased training time and overfitting concerns based on the results presented herein the best performing dnn configuration was selected as randomized sampling method 50 training subset 3 layer 50 node hidden layers adam solver 0 00001 regularization value and relu activation function fig 9 compares the dnn vs modflow calculations for various return flow components i e target variables and data subsets including the validation subset the trained dnn generalizes well to the reserved validation subset with an r2 value of 0 89 as compared with performance of the dnns in the training and testing subsets i e with r2 values of 0 91 and 0 90 respectively as categorized according to the various return flow components fig 9 reveals that the dnn performs well in predicting the overland return flow component while slightly lower in the mainstream return flow component for the training testing and validation subsets respectively as well as the tributary return flow components it is concluded therefore that the trained tested and validated dnn reasonably emulates the compute intensive modflow stream aquifer system model for the larb and is suitable as a compute efficient emulator of modflow in river geodss for finding river basin management strates that accommodate implementation of water quality improvement bmps without violating basin water rights and the colorado kansas interstate compact 6 3 river geodss with dnn generated return flows to assess bmp impacts on streamflow the best performing dnns are employed in river geodss for accurate stream aquifer system analysis to calculate realistic spatially distributed return flows to the mainstem river and tributaries along the entire valley region within the larb return flows calculated by the neural networks are automatically assigned to the appropriate return flow nodes in river geodss geomodsim routes flows in the larb hydrologic network while ensuring full satisfaction of the prior appropriation water rights system where senior water rights are entitled to take an adjudicated flow rate for beneficial uses i e agricultural industrial or household prior to junior water right holders as shown subsequently the model also accounts for the goal of meeting the stateline flow requirements of the arkansas river compact river geodss was applied to simulating the impacts of 75 water related bmps on return flows and instream flow conditions along the entire extent of the larb these bmps were earlier assessed by morway et al 2013 and shultz et al 2018b to estimate their impacts on water table depth return flows water quality and related variables in the usr and dsr shultz et al 2018b modeled stand alone bmps involving not only improved water management but also land management i e enhancing the riparian buffer adjacent to the river and tributaries and reducing fertilizer applications as well as combinations of these water and land management bmps it should be noted that morway et al 2013 focused primarily on the impacts of selected bmps on groundwater levels which affect waterlogging and salinization groundwater return flows to the river system and non beneficial consumptive use fig 10 shows the effects of the 75 alternative bmp s simulated by river geodss on weekly water right shortages where shortages are defined as the average simulated delivered flow rate subtracted from the average demand and then divided by the average demand and expressed as a percentage the weekly demands are calculated based on actual historical diversion records it is assumed that whatever diversions an irrigator actually received during the historical period 1999 2009 were consistent with that irrigator s water right priority whether a junior or senior water right holder shortages due to bmp implementation are based on the difference between the actual flow delivery during 1999 2009 adjusted by the reduction in demand due to increased efficiency and what the irrigator would receive with implementation of a selected bmp scenario as an example a 30 reduced irrigation bmp ri30 would amount to 30 less water required at the field level thereby reducing the assigned flow demands for each canal moreover it is assumed that shortages occurring under a bmp implementation are sufficiently small so as to not significantly affect return flows computed by the dnn for that bmp these results indicate that water bmp implementations which reduce return flows and alter instream flow patterns could lead to shortages in the fulfillment of water rights along the river these shortages primarily occur during the dry period between 2002 and 2005 smaller shortages occur in the year 1999 which is associated with the priming of the simulation where numerical errors are present during the initial simulation timesteps fig 10 displays the shortages by dividing the water rights into those located upstream or downstream of john martin reservoir fig 2 as well as senior or junior water rights the division by seniority was made by sorting the rights in ascending order from the earliest 1 april 1861 to the most recent 31 july 2007 and then dividing them roughly in half at the date 1 march 1887 based on this division there are 68 senior rights and 39 junior rights upstream of john martin reservoir while downstream of the reservoir there are 5 senior and 14 junior rights shortages in meeting water rights associated with implementation of alternative bmps are negligible for the upstream and downstream senior water rights which include most of the oldest water rights along the river however shortages are predicted to occur for the upstream and downstream junior water rights albeit relatively small evaluating flows at the stateline simulated bmp alternatives were predicted to alter the pattern of flow delivery to kansas in the form of surpluses and deficits with more aggressive bmps introducing larger magnitudes of alteration fig 11 plots the simulated flows at the stateline resulting from implementation of the ri30 lf30 cs80 bmp scenario in relation to the baseline bl scenario where streamflow and diversions are based on data for the historical period 1999 2009 when colorado was in full compliance with the compact without a source of replacement water such as releases from a new storage account in john martin reservoir substantial flow deficits can occur at the stateline as indicated by negative flows on the plot without storage account resulting in potential violation of the compact agreement between colorado and kansas fig 12 shows average discrepancies from baseline stateline flows across all of the modeled bmps during periods of surplus as well as during periods of deficit the relatively large flows during periods of surplus are due in part to decreased non beneficial consumptive use of water derived from increased d wt and decreased water table upflux under non cultivated larb areas brought about by bmp efficiency improvements morway et al 2013 6 4 reservoir storage account for compliance with the interstate compact agreement one option to address the issue of compact violation is to store excess streamflow generated by efficient water use i e water remaining in the river due to reduced canal diversions resulting from bmp efficiency improvements in a new storage account in john martin reservoir this would allow timed releases to be made from the account to sustain compliance with the compact during later periods when return flows from the irrigated valley have diminished this possible augmentation plan has been simulated in river geodss whereby excess river flows specifically resulting from bmp implementation are captured and stored in the new storage account the establishment of such a storage account in john martin reservoir dedicated to providing replacement flows for compliance with the compact is currently under consideration by the arkansas river compact administration fig 12 displays flows at the stateline as simulated by river geodss for the ri30 lf30 cs80 bmp scenario relative to the bl flows with creation of a new storage account in john martin reservoir as compared with the current without storage account case results indicate that timed releases from the new storage account are capable of maintaining flows at the stateline at or above the bl scenario fig 11 compares the simulated flows at the stateline from implementation of the ri30 lf30 cs80 bmp scenario for the without storage account with assumed implementation of a new storage account in john martin reservoir it can be seen that with the assumed storage account full compliance with the compact is achieved another key finding is that use of a new storage account in john martin reservoir would completely eliminate shortages in fulfilling water rights downstream of the reservoir although negligible shortages upstream of john martin reservoir still exist this could be remedied by altering the operation of pueblo reservoir at the upstream end of the larb as shown by triana et al 2010 table 1 summarizes the potential beneficial impacts of some selected combined bmps namely cs20 rf10 cs40 rf20 cs60 rf30 and ri10 cs40 rf10 which are highlighted here since they have positive impacts on all studied pollutants the nomenclature for the bmps is defined as follows rix indicates a reduction in applied irrigation water over the region by x percent from current baseline bl conditions csx denotes canal sealing to reduce seepage losses by x percent from baseline conditions lfx denotes lease fallowing of x percent of the baseline irrigated land in concentrations shown are from simulations of long term conditions reported in shultz et al 2018b where the available dataset was repeated four times resulting in over 40 years of extended simulation data the estimated percent reductions in cropped area underlain by shallow saline water tables with d wt 1 m with 1 m d wt 2 m and with 2 m d wt 3 m are based on morway et al 2013 although studies of bmp impacts on salinization are still underway field data presented in morway and gates 2012 indicate that increased d wt corresponds to decreased soil salinity only the water bmp components of these combinations lead to altered irrigation return flow patterns the last two rows of table 1 summarize the storage account volumes in john martin reservoir required to offset return flow depletions from the bl resulting from implementation of the selected bmps so as to comply with the water right system and the compact as calculated using a linear reservoir method chow et al 1988 as indicated here the more aggressive bmps which reduce pollution more substantially were also found to require a larger storage account to maintain compact compliance nevertheless in the case of all modeled bmps the required capacity of the new storage account is only a small fraction 5 of the total available storage capacity in john martin reservoir further examination of water and land management bmps in addressing the lingering sustainability and productivity problems in the larb as well as formulation of optimal operating rules for a new storage account in john martin reservoir to mitigate side effects of the bmps are subjects of future research that can build upon this study future work also will include consideration of how altered releases from pueblo reservoir could potentially redress junior water rights shortages upstream of john martin reservoir 7 summary and conclusions the lower arkansas river basin larb of colorado similar to many irrigated alluvial river basins around the world is experiencing degradation of water quality and diminished crop yields due to inefficient irrigation water management a number of water and land best management practices bmps have been identified for alleviating these serious agro environmental impacts including increased irrigation efficiency canal sealing to reduce seepage lease fallowing programs reduced fertilizer applications and enhancing vegetated riparian buffers to promote chemical reduction and volatilization of pollutants the socio economic ramifications of attempting to implement these bmps require serious consideration but in many irrigated river basins governed by a prior appropriation doctrine of water rights and impacted by interstate compact agreements it is the political legal and institutional restrictions that can seriously inhibit implementation to explore ways to overcome these issues the gis based river basin decision support system river geodss created previously by triana et al 2010 but substantially updated in this work is applied to accurately model the implementation of bmp strategies in the larb with strict adherence to colorado water law and an interstate compact agreement a key requirement of river geodss is the accurate simulation of the complex spatiotemporal characteristics of the stream aquifer system extensive field data collection activities and modeling studies using modflow sfr2 coupled with the solute transport model rt3d otis have been carried out in upstream and downstream study regions of the larb for predicting the quantity and quality of return flows to the mainstem river and tributaries since the compute intensive nature of modflow sfr2 prevents its direct coupling with river geodss for modeling the entire larb valley deep neural networks dnns are developed to emulate modflow sfr2 for direct integration into river geodss utilizing large input output datasets resulting from numerous modflow sfr2 model executions for a wide range of bmp implementations detailed deep learning based modeling training and validation procedures have been conducted that yield modflow surrogate dnns with moderate complexity and low regularization values with the application of the adam solver and relu activation functions the dnns are shown to exhibit excellent generalization capability and are extrapolated over the entire larb valley area results of utilizing the river geodss dnn linkage show that use of a new account in john martin reservoir for storing replacement water from flows remaining in the river due to bmp efficiency improvements would enable judicious releases to curtail water right violations and augment depleted flows at the stateline a likely added benefit to bmp implementation inferred from the work of shultz et al 2018 is improvement of water quality at the stateline which deserves further investigation another important next step is to apply river geodss to developing integrated operating rules for both john martin reservoir and pueblo reservoir upstream for the timely intake of replacement water and release of augmentation flows that maintain water rights and assure compliance with the compact for a wide range of bmps acknowledgements the authors appreciate the financial support and cooperation for this study provided by the indonesia endowment fund for education and by the national integrated water quality program of the usda national institute of food and agriculture 2014 51130 22491 
26117,inefficient irrigation practices in the alluvial lower arkansas river basin larb of colorado are contributing to salinization waterlogging reduced crop yields and harmful concentrations of pollutants in the stream aquifer system intensive data collection and modeling efforts in the larb over the past 20 years have resulted in development of the gis based basin scale decision support system river geodss parallel efforts in regional scale calibration and application of the modflow sfr2 rt3d otis stream aquifer system model permit evaluation of best management practices bmps designed to mollify adverse environmental impacts since bmp implementation is allowable only if water laws are not violated a deep learning model is developed to serve as an accurate compute efficient surrogate of modflow sfr2 and is imbedded in river geodss for assessing basin scale impacts of bmp implementations on stream aquifer exchange and water rights it is shown that bmps can be implemented while maintaining reasonable water law compliance with development of a new reservoir storage account keywords irrigation stream aquifer systems river basin management groundwater modeling machine learning artificial neural networks geographic information systems water law compliance 1 introduction waterlogging and salinization are age old maladies that continue to plague irrigated areas worldwide causing an estimated annual loss of over 27 billion in crop production adeel 2014 in the u s an estimated 30 crop yield reduction occurs due to salinization of irrigated lands u s department of agriculture 2018 soils become waterlogged when saturated conditions predominate due to over irrigation and poor drainage often contributing to salinization by inhibiting the leaching of salts intrinsic to the applied waters and creating degraded conditions by the transport of salts from underlying shallow groundwater to the surface via capillary action furthermore intense irrigation and fertilization of alluvial soils contribute to excessive nitrate no3 concentrations that can accelerate the dissolution and mobilization of inherent salts and other mineral pollutants e g selenium se and uranium u into alluvial aquifers with hydraulic connection to rivers this in turn elevates surface water pollutant concentrations to levels that imperil the ecological health of the riverine environment bailey et al 2012 gates et al 2009 mueller price and gates 2008 seiler 1995 shultz et al 2018a it is clear that salinization and related nonpoint source pollution pose a serious threat to our most productive agro ecological systems and the long term sustainability of irrigated agriculture ozerol et al 2012 a variety of water and land best management practices bmps have the potential to lower solute concentrations toward boosting agricultural productivity while meeting regulatory water quality standards and reducing ecological damage gates et al 2012 shultz et al 2018b some of the most effective practices involve reducing irrigation diversions by increasing application efficiencies and lowering canal conveyance losses these measures however can lead to altered rates and patterns of surface return flows generated by precipitation and tailwater runoff from irrigated fields along with alterations to the more dominant groundwater return flows due to irrigation percolation and canal seepage into adjacent unconfined aquifers intersecting stream channels as illustrated in fig 1 a major constraining consequence is that flows in the receiving streams can be substantially changed by such bmps thereby potentially damaging legal access of downstream water users in river basins governed by some form of the prior appropriation doctrine first in time first in right and or interstate compacts fully integrated river basin management strategies that consider the important political legal and institutional aspects of water allocation in the basin along with realistic modeling of the complex interconnected stream aquifer system are required for evaluating the viability of proposed bmps there are several generalized river basin management software packages that can be used including modsim labadie 2010 riverware zagona et al 2001 ribasim krogt 2008 and weap yates et al 2005 unfortunately these models are ill suited to providing realistic analysis of complex stream aquifer interactions since they utilize simplistic lumped parameter groundwater models based on the glover method glover and balmer 1954 glover 1974 such as the stream depletion factor sdf method jenkins 1968 or they represent the aquifer as a simple linear reservoir attempts have been made to provide more realism by directly linking these models to 3 dimensional finite difference models such as the usgs modflow groundwater flow model langevin et al 2017 but at the price of exorbitant computational costs morway et al 2016 coupled modflow nwt niswonger et al 2011 with modsim using the unique customizing capabilities of modsim where users have direct access to all public variables and object classes for creating custom code in c net or vb net with the net clr producing high speed executable code custom code was created that directly executes compiled modflow libraries within the iterative computational structure of modsim for the current operational time step although morway et al 2016 allude to the computational cost due to numerous required iterations between modsim and modflow the cpu time requirements related to application of the coupled models to a hypothetical agricultural river basin are not mentioned as astutely noted by morway et al 2016 published attempts at linking other models with modflow fail to consider the need to perform multiple iterations within a single time step since the modflow results input into the river basin management model likely result in altered irrigation diversion rates thereby requiring re execution of modflow in response to those changes several iterations may be required until the calculated flows converge to consistent values before advancing to the next time step valerio 2008 documents the linkage of riverware with modflow using a less accurate single feed forward iteration required computer run times of up to 4 5 days for a single scenario the magnitude of computer run times required for iteratively convergent direct linkage between modflow and river basin management models is clearly unacceptable particularly when considering the many possible combinations of spatially distributed bmps that may be evaluated triana et al 2010 applied artificial neural networks ann for modeling a portion of the irrigated stream aquifer system of the lower arkansas river basin larb located in southeastern colorado measurable georeferenced spatial temporal and bmp scenario based explanatory variables were defined that were considered to be correlated with the calibrated modflow generated surface and groundwater return flow datasets these served as training input data to the anns which operated in a supervisory learning mode the anns were trained to closely match the modflow modeled return flow output datasets with portions of the datasets not included in the training reserved for testing and validation the anns were effective for modeling the highly nonlinear and complex relationships between the explanatory variables and the calculated groundwater return flow rates for a portion of colorado s larb referred to as the upstream study region usr and for assessing impacts of altered return flow patterns on downstream senior water rights while adhering to the kansas colorado interstate compact which governs required arkansas river flows available to kansas robbins and montgomery 2001 aside from the computational cost of executing the anns being a small fraction of modflow computer run times the enormous time and cost to monitor and collect the necessary field data required for modflow calibration over the entire larb would be prohibitive although the powerful interpolation capabilities of anns are well documented their effective extrapolation performance for stream aquifer modeling has been demonstrated by authors such as pektas and cigizoglu 2017 whereby anns developed from modflow modeling over a ron of an alluvial river basin can be extrapolated to similar unmodeled rons of the basin triana et al 2010 linked surrogate anns with geomodsim a geospatial version of modsim and incorporated the linked models into the river geodss decision support system for basin wide water management and evaluation of bmps by extrapolating the anns to unmodeled regions of the larb the disadvantage of this configuration is that ann development training and verification was disconnected from geomodsim rather than being fully integrated since publication of triana et al 2010 more comprehensive modflow models have been calibrated and tested for a portion of the larb east of john martin reservoir referred to as the downstream study region dsr morway et al 2013 along with updating and extending modflow modeling in the usr the updated modflow model for the usr now includes the sfr2 streamflow routing package niswonger and prudic 2005 along with the unsaturated zone flow uzf package niswonger et al 2006 and has been coupled with the rt3d reactive transport in 3 dimensions model clement 1997 bailey et al 2013 and otis one dimensional transport with inflow and storage runkel 1998 shultz et al 2018a for simulation of solute fate and transport use of a single regional modflow model to train the ann however has proven to be inapplicable to the entire basin with findings suggesting that the usr trained ann model does not perform well when tested with the dsr data and vice versa the ann architecture employed in triana et al 2010 was a single hidden layer shallow ann with radial basis activation functions with development training and testing of the ann performed outside of the river geodss software suite reported herein is the training and validation of a deep neural network dnn which is essentially an ann with many hidden processing layers and neurons in its architecture the advantage of dnns over shallow anns is that complex highly nonlinear relationships can be learned such as for modeling real world stream aquifer systems with the dnns providing enhanced generalization capabilities mhaskar et al 2017 the study presented herein uses the combined results from the two regional modflow sfr2 flow models for the usr and dsr based on datasets now available from field monitoring activities in the larb that extend several years beyond the original 1999 2001 period considered by triana et al 2010b 2010a the generalized dnn is fully integrated into geomodsim which serves as an updated version of the geospatial river basin management decision support system river geodss the renovated river geodss provides a firm foundation for developing measures for mitigating adverse impacts on senior water rights and interstate compact agreements resulting from implementation of bmps designed to quell waterlogging curb salinization and reduce toxic levels of no3 salts se and u in larb aquifers and streams 2 study area lower arkansas river basin colorado 2 1 general description fig 2 depicts the larb study area for conducting basin scale modeling which extends from the outlet of pueblo reservoir to the colorado kansas stateline and encompasses the boundaries of the irrigated river valley on both sides of the river inserts show the usr and dsr study regions where data have been gathered for calibration and validation of the modflow sfr2 and solute transport models specifically for these regions the usr upstream of john martin reservoir drains to a 78 km section of the arkansas river from manzanola eastward to adobe creek the total usr drainage area is approximately 50 000 ha with roughly half of the area devoted to irrigated agriculture the 55 000 ha dsr extends from may valley drain at lamar east of john martin reservoir for about 71 km to the colorado kansas border and includes 33 000 ha of irrigated fields field data collection for the usr model calibration and testing occurred primarily during the period 1999 to 2012 whereas most of the dsr data collection was between 2002 and 2012 average annual precipitation within the semi arid alluvial valley increases eastwardly from 284 mm just below pueblo reservoir to 386 mm at lamar in the dsr clifford and doesken 2009 report an average annual reference et of about 1295 mm in the alluvial valley during the irrigation season mar 15 to nov 15 the proportion of cultivated fields in the usr and dsr with very shallow water tables i e water table depth d wt 2 m as simulated by the models was 24 and 21 respectively as reported by morway et al 2013 indicating significant susceptibility to problems of waterlogging salinity and non beneficial water consumption gates et al 2016 2 2 surface and groundwater quality a series of sedimentary formations of late cambrian to tertiary ages comprise the larb main alluvial valley darton 1906 with strong hydraulic connections existing between the alluvium and the arkansas river and tributaries person and konikow 1986 evidence suggests that these rock formations and their weathered residuum yield a variety of salts along with se and u under the dissolving action of natural and irrigation flows bailey et al 2012 total dissolved solids tds in sampled arkansas river reaches of the larb are quite high with average values of tds around 930 mg l in the usr and 2930 mg l in the dsr gates et al 2016 posing a hazard to irrigated crops and markedly exceeding the epa drinking water limit usepa 2009 approximation of the loadings of major salt ions directly to the arkansas river not including tributary flows are estimated to occur at an average rate of about 7 5 metric tons per day per km and 15 4 metric tons per day per km along the arkansas river in the usr and dsr respectively gates et al 2016 the 85th percentile of nitrate nitrogen no3 n in river samples exceeds the total n interim standard of 2 mg l at many locations in the usr and at the two most downstream locations within the dsr gates et al 2016 dissolved se and u concentrations for all sampling events from 2006 to 2011 in the usr and from 2003 to 2011 in the dsr respectively reveal that 85th percentile values for se concentrations are about 3 and 3 3 times greater than the chronic standard of 4 6 μg l in the usr and dsr respectively the 85th percentile values of most river samples for u are just below the chronic standard of 30 μg l in the usr but 2 4 times greater in the dsr gates et al 2016 2 3 best management practices in irrigated agriculture the following bmps have been proposed for ameliorating the detrimental conditions of waterlogging salinization and nonpoint source pollution within the main alluvial valley of the larb 1 reduced irrigation ri by increasing irrigation efficiency 2 canal sealing cs to reduce seepage and 3 lease fallowing agreements lf shultz et al 2018b ri practices primarily involve altering application rates and land slopes in the traditional border and furrow water application methods and or converting to sprinkler and drip irrigation although efficiency improvements do not necessarily reduce crop water consumptive use they can lower water tables moderate waterlogging conditions and diminish return flows godbout and johnson 2018 canal sealing cs can be an effective means of reducing water losses and thereby reducing diversions and also is cost effective with the use of linear anionic polyacrylamide sealants martin and gates 2014 lease fallowing lf bmps primarily involve agreements with municipalities to receive additional water supply through intermittent fallowing of irrigated fields to avoid buy and dry scenarios that can degrade rural communities while allowing irrigators to receive an economic benefit without having to sell all their water rights other viable land bmps evaluated in shultz et al 2018b include reduced fertilizer applications rf and enhancement of vegetated riparian buffers erb to promote chemical reduction and volatilization of pollutants 2 4 stream aquifer system exchange and compact compliance flow rates in the arkansas river below pueblo dam are influenced primarily by snowmelt and runoff from the upper arkansas river basin groundwater base flow runoff from precipitation events on the eastern plains and releases from pueblo dam and john martin dam downstream the stream aquifer system of the central alluvial valley of the larb supplies water to municipalities and industry primarily using well pumping from the alluvium these flows however are small compared to stream aquifer system interactions between the river and irrigated agriculture in the valley which have the most significant impact on maintaining senior water rights and complying with the kansas colorado arkansas river compact the compact constrains the operation of irrigation systems in the larb by prohibiting any changes that would alter the amount and timing of groundwater return flows to the river colorado revised statutes 1949 to guarantee that provisions of the compact are maintained the office of the colorado state engineer has issued efficiency rules that prohibit implementation of bmps that would result in diminished return flows back to the river resulting from improved efficiency thereby risking violation of the compact the dilemma is that reductions in excess surface or subsurface flows resulting from increased irrigation efficiency which clearly improve the sustainability of irrigated agriculture by mitigating problems of waterlogging salinization and increased concentrations of nutrients and toxic trace elements morway et al 2013 shultz et al 2018 tavakoli kivi 2018 are prohibited unless otherwise augmented by appropriate changes in river operation such as with amended releases from reservoir storage 3 regional scale modflow models modflow is a popular open source software package developed by the usgs for 3d flow modeling of multi layer groundwater systems with complex boundary conditions modflow employs a numerical finite difference scheme to solve the boussinesq nonlinear parabolic partial differential equations governing groundwater flow in several aquifer layers which can be confined or unconfined harbaugh 2005 variants of modflow can also simulate unsaturated flow surface water runoff surface water storage pumping wells evapotranspiration and groundwater recharge flow and sink source output from modflow are commonly used to drive a number of solute transport models including rt3d otis qurban 2018 shultz et al 2018a 2018b colorado state university csu has been conducting continuous data collection and modeling efforts since 1999 in the usr and since 2002 in the dsr of the larb for calibration and application of regional modflow and related solute transport models for predicting impacts of bmps on the stream aquifer system it is believed that the usr and dsr are also highly representative of the un modeled regions of the lower arkansas river basin since they include about 54 of the total irrigated area in the basin the intensive data monitoring efforts have allowed construction of high spatial resolution modflow finite difference models of the usr and dsr with 3d grids including three overlapping vertical layers with 250 m 250 m cell size a weekly time step is used for all modflow simulations in the usr and dsr the newtonian structured modflow nwt niswonger et al 2011 version of modflow is linked with the uzf niswonger et al 2006 and sfr2 niswonger and prudic 2005 packages which incorporate unsaturated zone flow stream aquifer flow exchange and streamflow routing also in the usr the uzf package coupled with rt3d reactive transport in 3 dimensions bailey et al 2013 is linked with otis one dimensional transport with inflow and storage runkel 1998 and qual2e brown et al 1987 to form rt3d otis for simulating multi species transport in groundwater and interconnected streams rt3d otis has been applied to simulate no3 n and se transport in the usr shultz et al 2018a and in the dsr qurban 2018 both modflow models use the wel package for simulating specified point discharge at wells the res package simulates leakage from reservoir features such as ponds lakes and reservoirs performing similarly to the riv package by simulating leakage between a reservoir and the aquifer by acting as a head dependent flow boundary fenske et al 1996 the regional groundwater models for the usr and dsr were calibrated using a combination of manual and automated procedures bailey et al 2014 morway et al 2013 shultz et al 2018a the automated procedure applies ucode poeter and hill 1998 and pest doherty 1994 to minimize residuals between predicted and measured groundwater heads groundwater return flows canal seepage total evapotranspiration et groundwater upflux to et and recharge to infiltration ratio by adjusting parameter values for selected aquifer properties the calibrated model was then applied to simulate 67 alternative water and land bmp scenarios including 39 combined bmps for effectiveness in decreasing se and no3 contamination in the usr each combined bmp scenario along with four single bmps were simulated at basic intermediate and aggressive levels shultz et al 2018b qurban 2018 analyzed a similar though not as extensive array of bmp scenarios for mitigating se and no3 in the dsr additional water bmps were earlier considered by morway et al 2013 to examine impacts on groundwater table levels and return flows to the arkansas river and its tributaries 4 basin scale river basin management model 4 1 river geodss geospatial decision support system river geodss is a generalized geospatial decision support system for river basin management with integrated modules for river basin modeling database management and graphical user interfaces and is fully implemented in a geographic information system gis for geospatial modeling and analysis river geodss considers all the important physical and hydrologic characteristics required for developing river basin management strategies along with inclusion of complex legal and institutional mechanisms governing allocation and use of available flows in an over appropriated river basin river geodss is based on geomodsim a geospatial version of the modsim generalized river basin management model labadie 2006 2010 and is embedded as a custom extension in arcgis desktop gis 10 x environmental systems research institute 2011 arcmap the primary windows desktop application for arcgis serves as a georeferenced graphical user interface for river geodss an updated version of the basin scale decision support system river geodss is applied to generating river basin management strategies that consider the stream aquifer impacts of a wide range of bmp implementations for water quality improvement while complying with basin water rights and the colorado kansas arkansas river compact 4 2 unique capabilities of updated river geodss many of the capabilities of the original triana et al 2010 version of river geodss are retained in the updated version presented herein these include 1 automated construction of georeferenced modsim hydrologic networks generated from digital hydrographic map layers available from the national hydrography dataset nhdplusv2 2 a highly efficient network flow optimization algorithm for allocating flows in strict accordance with water and storage right priorities over monthly weekly daily and even sub daily time steps 3 tools for populating and editing the spatiotemporal database 4 setting geometric network properties in arcmap 5 access to the arcgis spatial analyst extension 6 execution of modsim directly within arcmap and 7 georeferenced display of graphical output results in the arcmap interface in the earlier version of river geodss triana et al 2010 all ann development was required to be performed outside of river geodss with the extracted georeferenced spatiotemporal explanatory variables input into the commercial modeling package matlab mathworks inc to develop the ann the trained ann was then inserted back into river geodss and executed for river basin management simulation in the updated version of river geodss presented herein all dnn model development occurs entirely within river geodss where the user selects and modifies dnn configurations in a dedicated tab in the river geodss dialog window seen in fig 3 thereby providing a seamless modeling pipeline that does not require the user to exit river geodss to complete dnn development the configurations include selection of the number of hidden layers for determining the use of either a shallow or deep neural network number of nodes per hidden layer training testing portion activation function the neural network solver and the regularization value a scalar introduced to the learning model to prevent overfitting and improve generalizability the river geodss dialog window also provides an interface for importing training data sets from numerous modflow simulations providing surface water and groundwater return flows to the mainstem river and tributaries based on a wide range of bmp implementations another important difference in the approach taken here is that inputs to the dnn constitute raw data in contrast with the manually extracted features employed in the previous work by triana et al 2010b 2010a for example in the earlier work aquifer recharge per unit area was assumed to have a direct significance to the ann output variables e g groundwater return flows with the deep learning approach minimal intervention to the dnn learning process is desired requiring that the raw variables e g aquifer thickness area stream lengths be used instead other significant updates and improvements in river geodss include 1 reduced redundancy in the coding through implementation of native arcobjects libraries environmental systems research institute 2019 instead of hard coded case specific implementations as in the original thereby providing more robust and seamless usage 2 updated georeferenced and non georeferenced databases such as inclusion of new spatiotemporal variables 3 migration from the commercial matlabtm based ann module to the scikit learn license free machine learning package pedregosa et al 2011 4 use of the significantly updated modflow models employing the sfr2 package as well as use of a more extensive simulation period and 5 combining datasets from both the usr and dsr of the larb as the source data for developing the dnn as opposed to previous usr only implementation of river geodss triana et al 2010b 2010a 5 compute efficient deep learning surrogate of regional scale models 5 1 anns and deep learning anns are a type of machine learning paradigm comprised of numerous combinations of simple processor units or neurons joined through interconnection links called synapses that result in massively parallel interconnected networks that allow application of connectionist learning procedures the synapses are assigned connection strengths or synaptic weights within which the acquired knowledge is stored the weights are used in the calculation of input activation for each neuron node in an ann layer where the weighted sum input signals from all feeder neurons to that node are essentially summed a feedforward algorithm is utilized where the activation function in each neuron processes the summed weighted inputs and passes neuron activation function output to the outgoing connected neurons in the next layer bias neurons can also be included in the input and hidden layers which along with the weighted sum of the inputs to a neuron provide an additional parameter similar to a constant intercept included in a linear equation which can provide additional flexibility by shifting the activation function in a desirable way to enhance learning in a supervisory learning mode the anns are trained by determining the weights that essentially result in a close match between measured or target outputs and the computed outputs of the trained ann where closeness can be defined in several ways the learning process usually employs the backpropagation algorithm where after information passes from the input layer to the final output layer of nodes the ann computed output values are then compared to the actual values the discrepancies are then transferred backward by progressing from the output layer back to the input layers to update the synapses connection weights that produce improved ann outputs at the end of the training process the final weight values attributed to the synapses are essentially the ann acquired knowledge from a dataset the canonical procedure after a machine learning training is then to test the learned model with an unseen data subset to validate its generalizability readers are referred to abu mostafa et al 2012 for in depth descriptions of ann methodologies advancements in computing power and affordability have propelled the development and widespread use of anns and further inspired the birth of the field of deep learning throughout the last half century significant research has been done to find accurate representations of complex data structures using the most efficient methodologies possible this research particularly blossomed in the field of computer vision where image classification was found to require highly multilayered or deep anns or dnns to circumvent this complexity a feature extraction approach can be taken which is essentially creating higher level abstractions e g lines and shapes of lower level features e g pixels that are then input to the machine learning algorithm this approach however is tedious and requires significant human intervention to the learning process particularly in the creation of higher level abstractions deep learning aims to better address the challenge its main idea being that of capturing multiple levels of knowledge representation from raw data with minimal manual interference alpaydin 2014 2016 lecun et al 2015 a dnn is a specific tool in the deep learning family which exploits multiple layers of representation to model complex relationships for supervised or unsupervised learning deng et al 2014 the aim is to allow a machine to be fed with raw data and automatically discover multiple levels of representation for regression and classification the key ingredient of deep learning is its raw data input and multilayered hidden units and in the case of dnn its employment of multiple layers of calculation nodes neurons similar to its shallow version feed forward and backpropagation algorithms are often employed in the training of dnns alpaydin 2014 2016 deng et al 2014 lecun et al 2015 5 2 deep learning model development the following steps for ann and dnn model development were suggested by wu et al 2014 1 input feature selection 2 data splitting 3 model architecture selection 4 model structure selection 5 model calibration and 6 model validation note that there is a distinction between model architecture and structure where the former relates to how information moves across the neural network such as the selection of feed forward recurrent jordan or recurrent elman architectures in contrast model structure focuses on the properties of the network itself e g the number of parallel layers of neuron nodes or processing elements numbers of nodes per layer and the activation function selected for the processing elements the development of dnn surrogates of the regional scale modflow models for this study deviates slightly from the protocol of wu et al 2014 with the steps taken here including 1 network architecture and feature selection 2 model structure selection 3 model calibration and 4 model validation since the model architecture step is combined with feature or input layer selection and this study only utilizes the feedforward neural network architecture discussion of this modeling step deals only with feature selection the data splitting step is also merged with model structure selection which is explained subsequently the model calibration step focuses on training and testing of the neural network where for example 10 of the entire input output dataset for supervisory learning may be utilized for performing the model validation step with the remaining 90 further sub divided into training and testing datasets the validated neural network model is then incorporated into river geodss as a compute efficient emulator of the modflow sfr2 stream aquifer system model 5 3 neural network architecture and feature selection fig 4 displays the input output structure of the feedforward dnn developed as a surrogate of modflow sfr2 for supervised learning neuron nodes in the input layer of the neural network represent measurable explanatory variables categorized as spatial temporal and scenario based inputs with the latter reflecting the wide range of water bmp combinations and intensities as modeled by modflow sfr2 the dnn output variables are groundwater and overland return flows to streams resulting from a large number of modflow sfr2 simulations for a wide range of bmps to predict those output variables spatial temporal and scenario based explanatory variables are selected using an ad hoc approach where the variables are expected to have a significant hydrologic impact on the behavior of the return flow output variables fff the spatial input variables are measured using gis spatial analysis operations on georeferenced maps with the temporal input variables including precipitation measurements and groundwater pumping rates the scenario based input variables are of two types management scenario dependent and geomodsim dependent where the latter are river flows and canal diversions calculated by geomodsim based on bmp impacts water right priorities and other administrative mechanisms all temporal explanatory variables i e precipitation pumping streamflow and average diversion are in weekly time increments ranging over the historical period 31 december 1998 through 30 december 2009 and spatially aggregated in buffer zones the buffer zones are defined as valley areas parallel to the main river channel with a longitudinal length of 15 km and incremental width of 3 km on both the north and south sides of the river the methodology for buffer zone aggregation is discussed in detail in triana et al 2010 maier et al 2010 stress the importance of ensuring input variable independence in the input data selection process where improperly accounting for input variable redundancy can result in increases in the number of neural network connection weights requiring training leading to multi modal fitting error surfaces and increasing the likelihood of overfitting extensive linear correlation analyses were conducted between all possible pairs of the original 15 explanatory variables with the results summarized in fig 5 three variables i e canal elevation stream elevation and buffer zone elevation are seen to be strongly correlated with each other requiring removal of two of them from the set of explanatory variables i e stream elevation and canal elevation 5 4 model structure selection for this study the model selection process includes selecting data splitting methods network architecture solver selection activation functions and the regularization value data splitting generally separates data into training testing and validation subsets the training subset is applied to training of the neural network whereas the testing subset determines if overfitting has occurred i e when training should be terminated the validation subset is used to assess the generalization capability of the trained neural network maier et al 2010 wu et al 2014 emphasize the importance of justifying the data splitting method by comparison with alternative methods for this study two types of data sampling methods were considered randomized and sequential sampling with each method having 10 of the data saved for the validation subset along with nine variations of training and testing percentage pairs of the remaining 90 data subset 10 90 20 80 30 70 40 60 50 50 60 40 70 30 80 20 and 90 10 where the first percentage applies to the training subset and the second to the testing subset for the randomized sampling method the 10 validation subset was sampled in advance in a randomized fashion with the remaining 90 sampled for the training subset also in a randomized manner leaving the remainder as the testing subset for sequential sampling the earliest 10 of the data were saved as the validation subset and the latest 90 portion serving as the training subset with the remaining datasets applied to model testing the decision to compare these two sampling groups i e randomized or sequential is based on the popularity of applying these two methods as seen in the literature he et al 2014 and gong et al 2016 applied sequential data sampling for training whereas triana et al 2010 and wu et al 2015 employed randomized data sampling wu et al 2014 also mention the importance of developing a well described and justified neural network architecture by comparing alternative architectures here the architecture selection is based on how many hidden layers are required as well as the number of neurons or nodes in each layer in this study up to 2000 nodes per layer were utilized with various types of solvers activation functions and regularization values also evaluated the hyperbolic tangent tanh rectified linear unit relu logistic and identity functions were considered as activation function alternatives with the following eight regularization values considered 0 0 00001 0 0001 0 001 0 01 0 1 1 and 10 the limited memory broyden fletcher goldfarb shanno algorithm lbfgs andrew and gao 2007 stochastic gradient descent sgd robbins and monro 1985 and adam kingma and ba 2015 were the solver alternatives considered 5 5 model testing and validation more than 40 000 neural network configurations were evaluated in this study out of this large number of trained networks only the best performing model was employed in the application step although a metamodeling approach could have been employed to find the best performing neural network broad et al 2015 a simple brute force approach was utilized with consideration of the available computing resources and the decision to avoid an extra layer of computing due to the large scale nature of the neural networks the replicative and predictive validation criteria were assessed which is consistent with two of the three validation methods suggested by humphrey et al 2017 replicative validity of a model can be confirmed using scatter plots of the predicted versus observed data where a good result indicates that the model captures the underlying characteristics in the data used for model training calibration predictive validation on the other hand is applied to determine the model generalization capability over the range of the calibration data where the validation of the trained neural network can be used for confirmation to avoid overfitting metrics used to assess performance of the trained neural networks are 1 the akaike information criterion aic akaike 1974 and 2 the amari number amari et al 1997 while r2 and rmse are the most commonly used performance metrics aic introduces more depth to the metrics aside from measuring model goodness of fit aic also penalizes model complexity this parsimony favoring nature of aic is useful for selecting the minimal model that best explains the observed data the amari number further explores model parsimony with overfitting assumed as linked to the ratio of the number of training samples to the number of connection weights where it has been shown that overfitting does not occur when the ratio exceeds 30 in this study the best performing neural network is evaluated with respect to predictive validity based on the lowest aic value while satisfying the condition of having an amari number larger than 30 the coefficient of determination r2 is also used to present the performance of the neural networks without penalization of the network complexity 6 dnn modeling results 6 1 deep neural network configuration training and testing of the many neural network configurations for the larb system required more than 5 days of computing with eleven desktop computers and servers mimicking parallel processing where the cpu specifications were multi core intel cpus at 3 4 4 0 ghz at 100 utilization each of the various run configurations required an average of 170 s per run which also represents the average cpu time required for simulating the impacts on arkansas river flows of a selected bmp scenario in geomodsim using the trained dnn for that scenario this is in contrast with the aforementioned linkage of riverware with modflow requiring up to 4 5 days for a single scenario valerio 2008 the best performing network was selected based on the lowest aic value while satisfying the amari number criteria fig 6 shows test aic values vs the number of hidden nodes per layer and network complexity for various numbers of layers and sampling methods four charts are shown random sampling with a single layer random sampling with two layers sequential sampling with one layer and sequential sampling with two layers a distinction is also made between neural networks prone to overfitting with amari numbers less than or equal to 30 shown in yellow and those considered to be safe from overfitting or having amari numbers greater than 30 shown in blue it is clear in this study that randomized sampling outperforms sequential sampling and that the two layer dnns generally outperformed the one layer shallow anns where the aic values of the two layer networks are lower at the boundary between the overfitting prone and overfitting safe points i e yellow and blue dots respectively this may indicate that the shallow anns are unable to capture the complexity and high nonlinearity of the stream aquifer system interactions being modeled in the same figure when comparing aic values against network complexity i e the secondary abscissa in fig 6 with network complexity defined as the number of connection weights estimated in the training the dnns outperform shallow anns at the same complexity this further reinforces the assertion that in this case the dnns outperform shallow anns by better capturing the system nonlinearity and complexity also evaluated were impacts on the testing aic values of changing other neural network properties although the results are not shown here it was found that randomly sampled neural networks with minimal regularization values are superior moreover test results indicated that the adam solver dominates training performance by requiring the lowest average number of iterations and average training time as compared with other solvers in comparing the various activation functions i e identity logistic relu and tanh the latter two methods outperformed the former with relu slightly leading although the identity function averaged the least amount of computation time and fastest convergence it yielded a relatively higher test aic value the logistic function required similar average computation times as relu and tanh but displayed greater variability and a higher average number of iterations along with poorer performance in terms of average and minimum test aic values 6 2 dnn training and testing another important parameter selection is the training testing percentage or data division governing the way data are divided between the training and testing datasets maier et al 2010 stress that this can have a significant impact on model performance and close attention should be given to it as indicated previously nine pairs of data division options were considered ranging from 10 training 90 testing to 90 training 10 testing with 10 increments although a higher training percentage might be selected since it produces well performing neural networks this can raise the specter of redundancy issues not related to structural configuration of the neural network due to over training since maier and dandy 2000 stress the importance of training and testing sets being representative of the same population a comparison of the nine data splitting methods was conducted using principal component analysis pearson 1901 to compare the data splitting options presented in fig 7 are density plots of the first principal component for the training and testing portions of the data as well as the nine training testing percentage options the chart generally indicates that most of the data splitting options having between a 40 and 80 training portion produce visually similar training and testing data density plot evaluating the merits of using dnns fig 8 provides a comparison of the r2 statistic and training time as affected by the number of hidden layers in this comparison all neural networks were trained with 50 nodes per hidden layer 50 training portion adam solver relaxed regularization relu activation function and the randomized sampling method although fig 8 shows a significant increase in performance using dnns instead of a shallow ann performance gained by increasing the depth of a neural network beyond 3 layers does not appear to be proportional to the increase in required training time moreover violation of the amari number criterion becomes apparent starting from a four layered dnn upwards therefore it is reasonable to conclude that two or three layered dnns are satisfactory for modeling stream aquifer interactions in the larb with deeper neural networks providing only minimal improvement but at the expense of increased training time and overfitting concerns based on the results presented herein the best performing dnn configuration was selected as randomized sampling method 50 training subset 3 layer 50 node hidden layers adam solver 0 00001 regularization value and relu activation function fig 9 compares the dnn vs modflow calculations for various return flow components i e target variables and data subsets including the validation subset the trained dnn generalizes well to the reserved validation subset with an r2 value of 0 89 as compared with performance of the dnns in the training and testing subsets i e with r2 values of 0 91 and 0 90 respectively as categorized according to the various return flow components fig 9 reveals that the dnn performs well in predicting the overland return flow component while slightly lower in the mainstream return flow component for the training testing and validation subsets respectively as well as the tributary return flow components it is concluded therefore that the trained tested and validated dnn reasonably emulates the compute intensive modflow stream aquifer system model for the larb and is suitable as a compute efficient emulator of modflow in river geodss for finding river basin management strates that accommodate implementation of water quality improvement bmps without violating basin water rights and the colorado kansas interstate compact 6 3 river geodss with dnn generated return flows to assess bmp impacts on streamflow the best performing dnns are employed in river geodss for accurate stream aquifer system analysis to calculate realistic spatially distributed return flows to the mainstem river and tributaries along the entire valley region within the larb return flows calculated by the neural networks are automatically assigned to the appropriate return flow nodes in river geodss geomodsim routes flows in the larb hydrologic network while ensuring full satisfaction of the prior appropriation water rights system where senior water rights are entitled to take an adjudicated flow rate for beneficial uses i e agricultural industrial or household prior to junior water right holders as shown subsequently the model also accounts for the goal of meeting the stateline flow requirements of the arkansas river compact river geodss was applied to simulating the impacts of 75 water related bmps on return flows and instream flow conditions along the entire extent of the larb these bmps were earlier assessed by morway et al 2013 and shultz et al 2018b to estimate their impacts on water table depth return flows water quality and related variables in the usr and dsr shultz et al 2018b modeled stand alone bmps involving not only improved water management but also land management i e enhancing the riparian buffer adjacent to the river and tributaries and reducing fertilizer applications as well as combinations of these water and land management bmps it should be noted that morway et al 2013 focused primarily on the impacts of selected bmps on groundwater levels which affect waterlogging and salinization groundwater return flows to the river system and non beneficial consumptive use fig 10 shows the effects of the 75 alternative bmp s simulated by river geodss on weekly water right shortages where shortages are defined as the average simulated delivered flow rate subtracted from the average demand and then divided by the average demand and expressed as a percentage the weekly demands are calculated based on actual historical diversion records it is assumed that whatever diversions an irrigator actually received during the historical period 1999 2009 were consistent with that irrigator s water right priority whether a junior or senior water right holder shortages due to bmp implementation are based on the difference between the actual flow delivery during 1999 2009 adjusted by the reduction in demand due to increased efficiency and what the irrigator would receive with implementation of a selected bmp scenario as an example a 30 reduced irrigation bmp ri30 would amount to 30 less water required at the field level thereby reducing the assigned flow demands for each canal moreover it is assumed that shortages occurring under a bmp implementation are sufficiently small so as to not significantly affect return flows computed by the dnn for that bmp these results indicate that water bmp implementations which reduce return flows and alter instream flow patterns could lead to shortages in the fulfillment of water rights along the river these shortages primarily occur during the dry period between 2002 and 2005 smaller shortages occur in the year 1999 which is associated with the priming of the simulation where numerical errors are present during the initial simulation timesteps fig 10 displays the shortages by dividing the water rights into those located upstream or downstream of john martin reservoir fig 2 as well as senior or junior water rights the division by seniority was made by sorting the rights in ascending order from the earliest 1 april 1861 to the most recent 31 july 2007 and then dividing them roughly in half at the date 1 march 1887 based on this division there are 68 senior rights and 39 junior rights upstream of john martin reservoir while downstream of the reservoir there are 5 senior and 14 junior rights shortages in meeting water rights associated with implementation of alternative bmps are negligible for the upstream and downstream senior water rights which include most of the oldest water rights along the river however shortages are predicted to occur for the upstream and downstream junior water rights albeit relatively small evaluating flows at the stateline simulated bmp alternatives were predicted to alter the pattern of flow delivery to kansas in the form of surpluses and deficits with more aggressive bmps introducing larger magnitudes of alteration fig 11 plots the simulated flows at the stateline resulting from implementation of the ri30 lf30 cs80 bmp scenario in relation to the baseline bl scenario where streamflow and diversions are based on data for the historical period 1999 2009 when colorado was in full compliance with the compact without a source of replacement water such as releases from a new storage account in john martin reservoir substantial flow deficits can occur at the stateline as indicated by negative flows on the plot without storage account resulting in potential violation of the compact agreement between colorado and kansas fig 12 shows average discrepancies from baseline stateline flows across all of the modeled bmps during periods of surplus as well as during periods of deficit the relatively large flows during periods of surplus are due in part to decreased non beneficial consumptive use of water derived from increased d wt and decreased water table upflux under non cultivated larb areas brought about by bmp efficiency improvements morway et al 2013 6 4 reservoir storage account for compliance with the interstate compact agreement one option to address the issue of compact violation is to store excess streamflow generated by efficient water use i e water remaining in the river due to reduced canal diversions resulting from bmp efficiency improvements in a new storage account in john martin reservoir this would allow timed releases to be made from the account to sustain compliance with the compact during later periods when return flows from the irrigated valley have diminished this possible augmentation plan has been simulated in river geodss whereby excess river flows specifically resulting from bmp implementation are captured and stored in the new storage account the establishment of such a storage account in john martin reservoir dedicated to providing replacement flows for compliance with the compact is currently under consideration by the arkansas river compact administration fig 12 displays flows at the stateline as simulated by river geodss for the ri30 lf30 cs80 bmp scenario relative to the bl flows with creation of a new storage account in john martin reservoir as compared with the current without storage account case results indicate that timed releases from the new storage account are capable of maintaining flows at the stateline at or above the bl scenario fig 11 compares the simulated flows at the stateline from implementation of the ri30 lf30 cs80 bmp scenario for the without storage account with assumed implementation of a new storage account in john martin reservoir it can be seen that with the assumed storage account full compliance with the compact is achieved another key finding is that use of a new storage account in john martin reservoir would completely eliminate shortages in fulfilling water rights downstream of the reservoir although negligible shortages upstream of john martin reservoir still exist this could be remedied by altering the operation of pueblo reservoir at the upstream end of the larb as shown by triana et al 2010 table 1 summarizes the potential beneficial impacts of some selected combined bmps namely cs20 rf10 cs40 rf20 cs60 rf30 and ri10 cs40 rf10 which are highlighted here since they have positive impacts on all studied pollutants the nomenclature for the bmps is defined as follows rix indicates a reduction in applied irrigation water over the region by x percent from current baseline bl conditions csx denotes canal sealing to reduce seepage losses by x percent from baseline conditions lfx denotes lease fallowing of x percent of the baseline irrigated land in concentrations shown are from simulations of long term conditions reported in shultz et al 2018b where the available dataset was repeated four times resulting in over 40 years of extended simulation data the estimated percent reductions in cropped area underlain by shallow saline water tables with d wt 1 m with 1 m d wt 2 m and with 2 m d wt 3 m are based on morway et al 2013 although studies of bmp impacts on salinization are still underway field data presented in morway and gates 2012 indicate that increased d wt corresponds to decreased soil salinity only the water bmp components of these combinations lead to altered irrigation return flow patterns the last two rows of table 1 summarize the storage account volumes in john martin reservoir required to offset return flow depletions from the bl resulting from implementation of the selected bmps so as to comply with the water right system and the compact as calculated using a linear reservoir method chow et al 1988 as indicated here the more aggressive bmps which reduce pollution more substantially were also found to require a larger storage account to maintain compact compliance nevertheless in the case of all modeled bmps the required capacity of the new storage account is only a small fraction 5 of the total available storage capacity in john martin reservoir further examination of water and land management bmps in addressing the lingering sustainability and productivity problems in the larb as well as formulation of optimal operating rules for a new storage account in john martin reservoir to mitigate side effects of the bmps are subjects of future research that can build upon this study future work also will include consideration of how altered releases from pueblo reservoir could potentially redress junior water rights shortages upstream of john martin reservoir 7 summary and conclusions the lower arkansas river basin larb of colorado similar to many irrigated alluvial river basins around the world is experiencing degradation of water quality and diminished crop yields due to inefficient irrigation water management a number of water and land best management practices bmps have been identified for alleviating these serious agro environmental impacts including increased irrigation efficiency canal sealing to reduce seepage lease fallowing programs reduced fertilizer applications and enhancing vegetated riparian buffers to promote chemical reduction and volatilization of pollutants the socio economic ramifications of attempting to implement these bmps require serious consideration but in many irrigated river basins governed by a prior appropriation doctrine of water rights and impacted by interstate compact agreements it is the political legal and institutional restrictions that can seriously inhibit implementation to explore ways to overcome these issues the gis based river basin decision support system river geodss created previously by triana et al 2010 but substantially updated in this work is applied to accurately model the implementation of bmp strategies in the larb with strict adherence to colorado water law and an interstate compact agreement a key requirement of river geodss is the accurate simulation of the complex spatiotemporal characteristics of the stream aquifer system extensive field data collection activities and modeling studies using modflow sfr2 coupled with the solute transport model rt3d otis have been carried out in upstream and downstream study regions of the larb for predicting the quantity and quality of return flows to the mainstem river and tributaries since the compute intensive nature of modflow sfr2 prevents its direct coupling with river geodss for modeling the entire larb valley deep neural networks dnns are developed to emulate modflow sfr2 for direct integration into river geodss utilizing large input output datasets resulting from numerous modflow sfr2 model executions for a wide range of bmp implementations detailed deep learning based modeling training and validation procedures have been conducted that yield modflow surrogate dnns with moderate complexity and low regularization values with the application of the adam solver and relu activation functions the dnns are shown to exhibit excellent generalization capability and are extrapolated over the entire larb valley area results of utilizing the river geodss dnn linkage show that use of a new account in john martin reservoir for storing replacement water from flows remaining in the river due to bmp efficiency improvements would enable judicious releases to curtail water right violations and augment depleted flows at the stateline a likely added benefit to bmp implementation inferred from the work of shultz et al 2018 is improvement of water quality at the stateline which deserves further investigation another important next step is to apply river geodss to developing integrated operating rules for both john martin reservoir and pueblo reservoir upstream for the timely intake of replacement water and release of augmentation flows that maintain water rights and assure compliance with the compact for a wide range of bmps acknowledgements the authors appreciate the financial support and cooperation for this study provided by the indonesia endowment fund for education and by the national integrated water quality program of the usda national institute of food and agriculture 2014 51130 22491 
26118,the denitrification decomposition dndc model was developed to better simulate alfalfa medicago sativa l production with winterkill effects in eastern canada the pre development dndc produced fair simulations of alfalfa yield and biomass index of agreement d 0 7 nash sutcliffe efficiency nse 0 but normalized root mean square error nrmse was 30 whereas the improved model indicated good to excellent performance nrmse 3 9 29 d 0 93 0 99 nse 0 76 0 99 under future rcp4 5 and 8 5 scenarios average annual yields increased by 60 3 and 81 8 respectively due to earlier planting spring regrowth with additional cuttings increased c assimilation and reduced water stress under higher co2 concentrations for locations at ottawa and quebec city there could be an increased incidence of winterkill under future climate due to reduced snow cover which leads to colder soil crown temperatures as well as reduced fall hardening this study indicated that the use of winter hardy cultivars could mitigate winterkill effects and increase production keywords dndc model alfalfa growth soil temperature moisture climate change winterkill 1 introduction alfalfa medicago sativa l is the most widely grown forage legume worldwide and plays a key role in the development of sustainable cropping and livestock production systems crews and peoples 2004 castonguay et al 2006 thivierge et al 2016 alfalfa contributes to sustainable agricultural systems by improving soil quality increasing water infiltration and soil water storage reducing soil erosion and reducing nutrient losses e g n2o fluxes and no3 leaching sarrantonio and gallandt 2003 confalonieri and bechini 2004 blackshaw et al 2010 there is considerable interest in maximizing economic return and minimizing environmental cost for cropping systems used in dairy livestock production where the alfalfa crop is often an intergral component confalonieri and bechihi 2004 brown 2009 in eastern canada perennial forage crops account for about 40 of total agricultural land 2 1 million ha with an annual estimated farm value of 1 3 billion can statistics canada 2001 bélanger et al 2006 it has been shown that the yield and n content of wheat following alfalfa was increased compared to wheat following a canola control bullied et al 2002 compared to monocultural corn alfalfa in rotation with corn increased soil organic carbon increased corn yields reduced n fertilizer rate and decreased n2o emissions from agricultural fields mackenzie et al 1998 gregorich et al 2001 in addition jarecki et al 2018 noted that for corn rotations that included alfalfa an increase in soil carbon and corn yields was observed as compared to monoculture corn and that these increases continued under future climate scenarios whereas in the monoculture system no further change was realized a major challenge for perennial crops in northern regions is the harsh overwintering conditions that contribute to frequent stand losses and yield reduction in canada suzuki 1972 ouellet 1976 bélanger et al 2006 several regions of the united states barnhart et al 2004 castonguay et al 2006 and northern europe eagles et al 1997 the winterkill of perennial crops is largely affected by climatic conditions including subfreezing temperatures lack of snow cover and ice encasement ouellet 1976 schwab et al 1996 bélanger et al 2006 cold tolerance of cultivars is the capacity of plants to tolerate subfreezing temperatures which can significantly influence the winter survival of perennial crops bélanger et al 2002 castonguay et al 2011 previous studies assessed the effects of environmental factors on the survival of winter sensitive forage species including alfalfa under both natural and controlled conditions paquin and mehuys 1980 mckenzie and mclean 1984 schwab et al 1996 bélanger et al 2014 the reported correlations between climate variables and winter damages were used to explore optimal climatic conditions e g temperature precipitation for winter survival ouellet 1976 bélanger et al 2002 castonguay et al 2011 and to develop appropriate algorithms for modelling winter hardiness and injury kanneganti et al 1998a b changes in temperature precipitation and atmospheric co2 concentration influence crop growth by impacting the rate of photosynthesis respiration and crop water use efficiency wue along with influencing soil biological and chemical transformations of c and n guo et al 2010 wang et al 2014 long et al 2015 alfalfa a c3 legume species benefits from elevated atmospheric co2 due to increased photosynthesis morgan et al 2004 and improved water use efficiency leakey et al 2009 barton et al 2011 many forage species including alfalfa are sensitive to drought and would be affected by future changes in precipitation rate and pattern aranjuelo et al 2007 thivierge et al 2016 the positive response to increased co2 concentration might be offset by an increased temperature increased evapotranspiration or reduced precipitation hatfield et al 2011 lee et al 2013 piva et al 2013 climate change may therefore have greatly varying effects on alfalfa production depending on the selected cultivars management and soil and climatic conditions izaurralde et al 2011 ipcc 2012 thivierge et al 2016 process based models have been widely employed across diverse agroecosystems to estimate crop production hydrologic processes and nutrient cycling jones et al 2003 williams et al 2008 li et al 2012 holzworth et al 2014 the denitrification decomposition dndc model is an example of a well established tool used to simulate crop growth and development ghg emissions and water quality in crop livestock and forest systems li et al 1992a b smith et al 2013 congreves et al 2016 the dndc model li et al 1992a b was initially developed to predict greenhouse gas ghg emissions from agricultural soils and was expanded to simulate soil c n cycling and nitrate leaching li et al 2006 and more recently to simulate nutrient cycling for full farm facility and livestock systems li et al 2012 the simulation of crop growth has advanced over the last 25 years zhang and niu 2016 and certain versions of dndc use detailed approaches for simulating physiological processes such as crop dndc zhang et al 2002 several countries have developed regional versions of dndc to improve the predictions of their agricultural systems brown et al 2002 leip et al 2011 smith et al 2013 gilhespy et al 2014 li et al 2017 including a canadian version which has been used to simulate crop soil and plant processes under cool climate conditions kröbel et al 2011 smith et al 2013 dutta et al 2016 he et al 2018 currently the model algorithms have been calibrated and validated to characterize canadian growing conditions including sub models for crop growth kröbel et al 2011 soil organic c dynamics smith et al 2012 n2o emission uzoma et al 2015 evapotranspiration dutta et al 2016 nh3 volatilization congreves et al 2016 and soil temperature dutta et al 2018 much of the development work has been conducted through comparison with other modeling frameworks in particular with daycent dssat and rzwqm2 grant et al 2016 dutta et al 2017 guest et al 2017 smith et al 2019 along with the cooperation with the primary us developers of dndc the current dndc model however is limited in its ability to simulate perennial crop growth especially regrowth after cutting and in subsequent seasons considering winterkill impacts this limits the application of the model for exploring scenarios under livestock based systems that include forages therefore the objectives of this study were to 1 improve alfalfa growth and development in the dndc model using measured biomass and yield data under various soil and climate conditions 2 compare performance of the revised and default versions for simulating long term alfalfa production soil temperature and moisture and 3 assess projected climate change impacts on alfalfa production including winterkill in eastern canada 2 materials and methods 2 1 research site and experiment measurements of aboveground biomass during the growth cycles at one location in eastern canada were used independently to develop a new phenological growth curve and then validate this new growth curve alfalfa was seeded in 2014 at the canadian food inspection agency cfia greenbelt farm southwest of ottawa 45 18 n 75 45 w fig 1 aboveground biomass samples were taken between 2 and 8 times in each of three growth cycles in the seeding year and four growth cycles in the first 2015 and second 2016 post seeding years the soil is a sandy loam clay 14 and the average annual precipitation is 950 mm 450 mm from may to september from an on site aafc weather station the soil clay fraction ph bulk density and soc of the topsoil 0 20 cm were 14 6 8 1 26 mg m 3 and 18 0 g kg 1 respectively the daily soil temperature c for four soil depths 0 05 0 10 0 20 and 0 50 m and volumetric soil water content m3 m 3 for two soil layers 0 0 05 and 0 05 0 10 m were monitored from 2014 to 2016 using soil thermocouple array 0 05 m and thermistors other depths and water content reflectometers respectively campbell scientific edmonton ab additional aboveground alfalfa biomass measurements were taken on a near weekly basis every 7 10 days from three experiments in 2014 near the cfia site in ottawa fig 1 during biomass collection a 0 25 m2 quadrant frame was randomly placed in four locations within each field to make a composite 1 m2 sample those measurements were made in the seeding year ottawa o2 45 10 n 75 16 w and in the first post seeding year ottawa o1 45 15 n 75 49 w and ottawa o3 45 05 n 75 10 w and they were used for the validation of the newly calibrated biomass curve measurements of alfalfa dm yields at the normal harvest times from two other sites in eastern canada were also used a long term experiment established in 1981 at the elora research station 43 38 n 80 25 w in southern ontario with corn corn alfalfa alfalfa ccaa and aacc rotations was used fig 1 measurements of alfalfa dm yield were made in the seeding year and the first post seeding year the mean annual precipitation was about 900 mm with seasonal rainfall of 425 mm may to september and average monthly temperature ranged from 7 c in january to 20 c in july the soil was a silt loam clay 17 with an average ph of 7 3 dry bulk density of 1 3 mg m 3 total n of 1 8 g kg 1 and soc of 21 2 g kg 1 in the 0 20 cm soil layer the final research site site 3 used for validation is located in normandin quebec 48 50 n 72 31 w with an average annual precipitation of 1100 mm and seasonal rainfall of 470 mm where alfalfa was planted in three experiments n1 1994 1996 n2 1995 1997 and n3 2008 2011 no measurements in 1994 1995 and 2011 for n1 n2 and n3 respectively a strip 5 m long and 0 6 m wide was harvested for different growth cycles to determine each cut yield with the majority of measurements in the early flowering stage the soil is a silt clay 45 clay and the bulk density was 1 32 mg m 3 detailed soil information and management practices for all sites are shown in table 1 and a 1 2 2 dndc model 2 2 1 model description the dndc model is a process based model describing c and n dynamics in agroecosystems on a daily time step li et al 1992a b it has been used worldwide to simulate greenhouse gas emissions crop growth soil c and n dynamics and water balance under different management practices and weather conditions li et al 2012 smith et al 2013 abdalla et al 2014 gilhespy et al 2014 brilli et al 2017 ehrhardt et al 2018 dndc is a complex two component model that incorporates many deterministic equations based on the classic laws of physics chemistry and biology along with empirical equations derived from research studies qin et al 2013 he et al 2018 the first component consists of the soil climate crop growth and decomposition sub models which are driven by four ecological drivers climate soil vegetation and management practices the second major component consists of the nitrification denitrification and fermentation sub models which are driven by simulated soil environmental factors to predict trace gas fluxes from the plant soil system li et al 2012 uzoma et al 2015 deng et al 2016 2 2 2 canadian version of dndc dndc v can a canadian specific version of the dndc model https github com brianbgrant dndcv can dndc v can has been developed over the past decade to adjust the simulation under canadian climate soil types and cropping systems the new additions in the canadian version of dndc include 1 the improvements of empirical growth curves for corn wheat and soybean kröbel et al 2011 grant et al 2016 2 the incorporation of the effects of atmospheric co2 concentration on water and n use efficiency revised temperature stress based on cardinal temperatures and temperature stress during anthesis smith et al 2013 3 the adjustment of the holding capacity for the deep water pool based on soil properties grant et al 2016 4 the improvement of et prediction based on penman monteith and fao crop coefficients dutta et al 2016 5 an improved nh3 volatilization routine for surface applied manure slurries congreves et al 2016 6 the incorporation of a pedo transfer function to recalculate changes in soil properties based on changes in soc jarecki et al 2018 and a new soil temperature submodel which includes effects of snow insulation surface residue live biomass and soil texture dutta et al 2018 2 2 3 development of an improved alfalfa growth curve 2 2 3 1 pre development dndc default dndc the dndc model is based on a gdd growing degree day approach for crop production coupled with phenological growth curves for specific cultivars which determine the crop n water and temperature requirements during the different growth phases the growth index 0 1 in dndc is conceived to represent the crop from germination to maturity the optimal rate of growth is expressed as a nitrogen demand curve over the full range of this index and is reduced by incurred crop stresses temperature water nutrient winterkill biomass c accumulation is derived from this growth curve using the parameterized c n ratios of the various biomass fractions and is segmented into vegetative and reproductive growth phases to characterize specific biomass fraction growth the growth index is advanced based on the calculated daily tdd thermal degree days which is synonymous with gdd calculated at a base degree threshold of 0 c the timing of phenology is determined by tdd and is not altered by any stress however the timing of incurred stresses does influence the degree of impact from water and temperature stress on the harvest index for many crops not alfalfa aboveground alfalfa live c was assumed to shift to standing dead c when daily air temperatures reached 2 c for more than 24 h by default the potential n demand for perennial crops is the daily change in pgi multiplied by the total n demand of the crop determined by crop input parameters 1 d ndemand totcrpn δ pgi where dndemand kg n ha 1 is the daily potential n demand of the crop totcrpn kg n ha 1 is the total crop n demand based on crop parameterization and δ pgi is the change in daily pgi 2 2 3 2 post development dndc improved dndc measurements of aboveground biomass means of four quadrat samples during the growth cycles from the ottawa cfia location 2014 2016 were used to construct a new alfalfa growth routine in dndc dry aboveground biomass measurements across the three years n 54 were pooled into first growth in the season and subsequent regrowth and converted to c equivalents absolute biomass c was then calculated as a rate of biomass c accumulation between measured dates growth rates per unit of tdd were slower at the first growth cycle after planting than for periods after cutting events likely due to root establishment based on this observation two functions were fit to each distinct growth period alfalfa growth in the beginning of the season 2 d ndemand 27 41 pgi totcrpn ccrpn alfalfa regrowth after cutting 3 d ndemand 12 43 pgi 2 totcrpn ccrpn where ccrpn kg n ha 1 represents the cumulative total n demand calculated for entire seasonal crop tdd as a total area under the curve as a means to scale the dndemand kg n ha 1 to the target crop n demand the equations represent a relative index along the growth curve using the actual to date nitrogen uptake totcrpn seasonal total potential n demand the value of ccrpn equalsthe area under the curve from the empirically derived measurements for plant n uptake over specific measured years and the modelled years need to be scaled to equal this measured period the crop gdd in the model is reset based on the average gdd realized from one cut to the next cut in the measured data similar to the recommended practices in thivierge et al 2016 other assumptions included that when a killing frost temperature occurs any remaining above ground live c is shifted to the standing dead residue pool and is subject to decomposition into the litter soil pools biomass c accumulation is directly calculated from the modelled n uptake amount and the c n ratio in the crop input parameters 2 2 4 development of cold tolerance and winterkill integration of alfacold model the process of cold hardening and dehardening based on the alfacold model kanneganti et al 1998a b were integrated into the dndc model to predict winterkill impacts on alfalfa growth alfacold model is a alfalfa simulation model which developed and incorporated numerical functions of cold tolerance fall dormancy and freezing injury to simulate alfalfa growth based on previous studies in canada mckenzie and mclean 1980 1984 paquin and pelletier 1980 mckenzie et al 1988 daily rate of increase in cold hardening hri c d 1 was computed as follows 4 hri chrmx ethri where ethri dimensionless 1 to 1 represents the effect of average daily crown temperature crtmp c on the potential rate of hardening chrmx c d 1 values of ethri are a function of soil temperature in the crown region at 3 cm depth kanneganti et al 1998a the daily rate of dehardening dri c d 1 which results in a decrease in cold tolerance is calculated as follows 5 dri cdrmx etdri where etdri dimensionless 0 to 1 represents the effect of average daily crown temperature on the potential rate of dehardening cdrmx c d 1 the values of etdri were based on the previous experiments paquin and pelletier 1980 mckenzie et al 1988 kanneganti et al 1998a the state of cold tolerance in a cultivar is quantified by ctt c which is equivalent to the subzero temperature that a crop can tolerate without being killed it is calculated as follows 6 ctt t max ctmx min 0 ctt t 1 hri dri δt the difference between hri and dri represents the daily rate of net increase or decrease in cold tolerance the δt is calculated on a daily time step note that ctt c and ctmx maximum temperature of cold tolerance c are negative values indicating subzero temperatures ctt is initialized to 0 c at the start of a simulation kanneganti et al 1998a plant death caused by winterkill is quantified by a plant death factor pdf d l which represents a fraction of plant population plants m 2 dead in a day as follows 7 pdf pdfmx max 0 ctt crtmp where crtmp is the average daily crown temperature which was estimated using soil temperature in the crown region at 3 cm depth and pdfmx d l c 1 below ctt represents cultivar specific potential rate of plant death the dndc model calculates soil temperature over each 1 cm increment and thus the 3 cm value was used in this equation in this study the default value of pdfmx was set as 0 12 d l c 1 based on previous studies in canada mckenzie and mclean 1980 1984 the calculation and incidence of a pdf event is applied as a cumulative scalar pdfcummulative that acts to reduce the potential crop n demand c biomass growth in alfalfa in order to represent the reduction in plant population due to winterkill over time e g 0 0 no winterkill effect and 1 0 representing 100 loss in plant population this approach is necessary as dndc does not explicitly represent whole plant populations as it is a 1d model thus the reduction of plant populations needed to be applied on a per plant basis in the model the winterkill effect we on alfalfa yield for each overwinter period was modelled based on potential yield reduction of the subsequent year the cumulative winterkill effect cwe is calculated based on the initial potential yield and total yield reduction due to winterkill as follows 8 we spyr spy 100 9 cwe tpyr ipy 100 where spyr kg ha 1 is potential yield reduction of subsequent year spy kg ha 1 is the potential alfalfa yield of subsequent year tpyr kg ha 1 represents the total potential yield reduction for all growing years because of plant stand loss and ipy kg ha 1 is the initial potential yield of alfalfa effect of fall dormancy on growth is simulated as a function of a cultivar s fall growth score fgs which ranged from 1 0 for highly dormant cultivars to 9 0 for the least dormant types it is cacluated based on crop height after approximately 40 days of regrowth following a cutting in the fall schwab et al 1996 kanneganti et al 1998a alfalfa winterkill could be highly affected by the fgs specified cultivar characteristics including hri and dri ctmx as well as pdfmx based on previous studies the soil temperature in alfalfa field conditions rarely goes below 15 c in eastern canada paquin and mehuys 1980 bélanger et al 2002 hence 15 c was chosen as the general ctmx of alfalfa cultivars fgs 5 to calibrate the model in this study kanneganti et al 1998a additionally based on the survival temperature in mckenzie and mclean 1980 a ctmx sensitivity of 20 and 10 c was also conducted to explore climate change and winterkill impacts on alfalfa yields table a 2 2 2 5 inputs initialization calibration and validation of dndc the dndc model requires the following input information 1 agricultural management data e g planting harvest dates tillage fertilizer rates and types residue 2 soil data e g soil bulk density soil ph texture initial soil organic c and soil nitrate and ammonium n content soil hydraulic parameters 3 daily weather maximum and minimum air temperature tmax and tmin c solar radiation mj m 2 daily precipitation cm wind speed m s 1 and humidity and 4 crop parameters including alfalfa potential yield kg c ha 1 yr 1 biomass fraction ratio of grain leaf stem and root biomass c n ratio tdd water demand g water g 1 dry matter and optimal temperature c zhang and niu 2016 he et al 2018 jarecki et al 2018 the local meteorological data during the experimental periods were acquired from environment canada weather stations located at each of the experimental sites the soil and crop management inputs were based on information gathered during the course of each field study and are listed in table 1 and a 1 a spin up period was simulated for 10 years prior to the experimental period of interest at each experimental site to stabilize c and n pools the aboveground alfalfa biomass data from the ottawa cfia site in 2014 2016 cultivar c2014 were selected for calibrating crop parameters annual dm yields from 1981 to 1984 1992 1994 2000 2002 and 2006 2009 were used to calibrate the cultivars e1981 1981 1991 e1992 1992 1999 e2000 2000 2005 and e2006 2006 2015 respectively for the long term study at elora the cultivar apica was calibrated using yields from each cut at normandin for the first 1995 and second 1996 post seeding years n1 table a 3 in this study the model was calibrated using trial and error during the first stage which was useful to quickly see the output changes affected by input parameters after initial trial and error we optimized rmse for each variable in a stepwise manner through r language then repeated the process until all parameters were reasonably optimized the calibration process generally resulted in similar crop biomass fractions across the sites however thermal degree days and water demand sometimes varied table a 3 the remaining biomass datasets from ottawa o1 o2 and o3 and yields data from elora 1985 1991 1995 1999 2003 2005 2010 2015 and normandin n2 and n3 were then used to validate the model performance 2 3 climate change scenarios climate scenarios of the representative concentration pathways rcps were used from the fifth assessment report of ipcc ipcc 2014 to make projections based on four different 21st century pathways of ghg emissions and atmospheric concentrations air pollutant emissions and land use moss et al 2010 van vuuren et al 2011 a new regional climate model canrcm4 was developed at the canadian centre for climate modelling and analysis cccma of environment canada scinocca et al 2016 in our study daily outputs from canrcm4 were obtained from cccma for the historical baseline period 1971 2000 and for two future periods of 2041 2070 and 2071 2100 under the forcing scenarios rcp4 5 and rcp8 5 which represent medium low and high emission scenarios with a radiative forcing of 4 5 and 8 5 w m 2 at the end of the 21st century respectively van vuuren et al 2011 ipcc 2014 in this study the cropping system used in future scenarios was corn c corn alfalfa a alfalfa alfalfa with five rotation phases simulated e g aaacc aacca accaa caaac ccaaa to ensure each year of the alfalfa stand was present for each year of weather the managements were set the same as the field experiments at each location three locations including elora ottawa and quebec city which represent different winter climate conditions were selected to explore the climate change impacts on alfalfa yield and related winterkill the comparison of historical and projected climate normals at the three locations are shown in table 2 and table a 4 2 4 adaptation measures employed under future climate to simulate the adaptation that farmers may employ in response to changes in growing season length that are expected to occur under climate change several adjustments were made to alfalfa management planting date and harvest date of each cut were adjusted during each year based on weather planting dates were evaluated based on the criteria in bootsma and de jong 1998 and qian et al 2018 which include a snow cover 10 mm for the day b daily precipitation 2 5 mm c 0 75tmax 0 25tmin 7 c d the sw1 soil water at the top 5 of the soil profile 90 available water holding capacity awhc e sw2 soil water at the next 7 5 of the profile 0 95 awhc therefore potential planting dates were automatically taken into consideration in the dndc simulations for both baseline and future climate scenarios additionally the timing and number of cuts was considered based on accumulated growing degree days during the alfalfa growth period in the improved dndc model automated harvesting was included and occurred when the alfalfa pgi see section 2 2 3 reached 0 85 along with ensuring that no cuts occurred less than 30 days before the first fall frost 2 5 model performance statistics to evaluate dndc model performance several statistics were employed to compare simulations against measurements for alfalfa aboveground biomass and dm yield soil temperature and soil moisture see eqs 10 14 below these included mean bias error mbe root mean square error rmse normalized root mean square error nrmse nash sutcliffe efficiency nse and index of agreement d nash and sutcliffe 1970 willmott 1985 yang et al 2000 moriasi et al 2007 he et al 2018 10 mbe i 1 n s i m i n 11 rmse i 1 n s i m i 2 n 12 nrmse i 1 n s i m i 2 n m 100 13 nse 1 i 1 n s i m i 2 i 1 n m i m 2 14 d 1 i 1 n s i m i 2 i 1 n s i m m i m 2 where si is the simulated value mi is the measured value i 1 n is the number of measured values and m is the mean of the measured values the mbe nrmse nse and d statistics evaluate various aspects of model performance the mbe can be used to illustrate whether the model showed underestimation mbe 0 or overestimation mbe 0 compared to the measured values willmott et al 1985 yang et al 2000 based on the recommendations in previous studies li et al 2015 he et al 2018 we consider an excellent model performance when the nrmse 10 good performance when 10 nrmse 20 fair performance when 20 nrmse 30 and poor performance when the nrmse 30 a nse to 1 value 1 0 indicates perfect agreement 0 50 nse 1 0 indicates good agreement 0 0 nse 0 50 indicates fair agreement and a nse 0 0 indicates poor agreement between simulated and measured data moriasi et al 2007 schaefli and gupta 2007 a value of d 0 9 illustrates excellent match 0 8 d 0 9 illustrates good match 0 7 d 0 8 illustrates fair match and d 0 7 illustrates poor match when comparing the simulated and measured values he et al 2018 3 results and discussion 3 1 alfalfa growth 3 1 1 aboveground biomass during growth cycles the calibration of the default dndc model resulted in good or fair agreements between the simulated and measured alfalfa aboveground biomass based on the values of nse 0 and d index 0 8 but the alfalfa aboveground biomass of the spring regrowth was underestimated in 2015 first post seeding year and 2016 second post seeding year fig 2 and a 1 table 3 the improved dndc model performed very well in simulating alfalfa aboveground biomass for all years with nse 0 9 d 0 9 and nrmse 20 table 3 the results overall indicated that the improved dndc model produced significantly better simulations of alfalfa biomass compared to the default version for validation the dndc model performance was evaluated using the simulated and measured biomass in 2014 first post seeding year at ottawa o1 in 2014 seeding year at ottawa o2 and in 2014 first post seeding year at ottawa o3 fig 2 table 3 for the default dndc model the statistical values of nse 0 5 and d values 0 8 indicated fair to good agreements between the simulated and measured alfalfa aboveground biomass for the three experiments but the model underestimated the biomass mbe 0 53 and 0 55 mg dm ha 1 with a high rmse 1 03 and 0 98 mg dm ha 1 at ottawa o1 and o3 which was consistent with the statistics in the calibration phase of the post seeding years at cfia fig 2 table 3 in contrast the improved dndc model showed good to excellent performances based on the statistical evaluations between the simulated and measured biomass the values of nse d nrmse ranged from 0 76 to 0 92 0 93 0 98 and 16 4 29 0 respectively table 3 the significantly better performance in predicting alfalfa aboveground biomass was due solely to the inclusion of the revised growth curve equations 2 and 3 which captured the rate of spring growth regrowth after cutting and regrowth in subsequent years 3 1 2 alfalfa yield the simulated alfalfa dm yield by the default and improved dndc models was compared to the annual dm yield at the long term experiment in elora as well as to the dm yield for each cut at normandin figs 3 and 4 table 3 in the calibration years the default dndc model only showed poor to good performance at normandin and elora based on the values of nse d and nrmse ranging from 1 1 0 60 0 52 0 84 and 23 9 24 0 respectively this was because the default dndc model underestimated the first cut dm yield of the first 1995 and second 1996 post seeding years at normandin and the annual dm yields of the first post seeding year in 1981 at elora fig 3 the improved dndc model produced markedly better simulations with good to excellent performance in predicting alfalfa annual cut yield with statistical values of nse of 0 88 0 95 d of 0 95 0 99 and nrmse of 3 9 17 9 table 3 fig 4 in the validation years the default dndc model overestimated alfalfa annual yields for the long term study at elora mbe 0 55 mg ha 1 due to the lack of sensitivity to dry conditions and a significant underestimation of the first cut dm yield in all post seeding years at normandin table 3 fig 3 the improved dndc model produced statistical values that were similar to the calibration phase e g d index ranged 0 94 0 99 nse ranged from 0 82 to 0 99 and nrmse ranged from 7 9 to 18 0 indicating good to excellent agreements between simulated and measured alfalfa yield the yield response to water parameters e g water demand field capacity and wilting point could be well captured during dry years e g 2016 in ottawa based on the sensitivity analysis in the revised dndc model fig a2 the impact of winterkill on alfalfa dm yield in the improved dndc model averaged less than 2 at elora but it was 18 6 in the overwinter 1996 1997 and 27 1 in the overwinter 2009 2010 at normandin the improved dndc model with the new alfalfa growth algorithm including winterkill effects performed better in estimating alfalfa yield than in a previous study at the same location conducted using ifsm integrated farm system model jégo et al 2015 which did not include the impact of winterkill and showed a poor to good performance based on rmse 960 kg ha 1 nrmse 30 and nse 0 51 moreover for other perennial crops e g timothy the stics simulateur multidisciplinaire pour les cultures standard and catimo canadian timothy model performed good in simulating aboveground biomass for primary growth with nrmse of 16 and 21 respectively but both models showed worse performances nrmse 27 89 during regrowth cycles compared to improved regrowth simulations in our revised dndc model jégo et al 2013 jing et al 2013 3 2 soil temperature soil temperatures were measured and simulated across different soil depths from 2014 to 2016 at cfia fig 5 table 4 the simulated daily soil temperature dynamics from both versions of dndc in the soil depths 0 05 0 10 and 0 20 and 0 5 m corresponded very well with the measured values with no statistically significant differences being found based on paired t test p 0 05 the nrmse 15 nse 0 60 and d 0 90 also indicated good to excellent agreements but the simulations were better for the improved dndc model table 4 the default model showed increasing overestimations in soil temperature with soil depths which was mitigated in the improved version by adjusting solar radiation impacts on soil temperature for example the mbe values were 0 78 and 0 29 c at the deeper 0 50 m soil layer for the default and improved dndc models respectively table 4 the dndc model was previously improved for estimating soil temperature dutta et al 2018 but the effects of cumulative insulation of surface crop residues and plant shading may be underestimated since alfalfa can establish a very dense canopy residue cover and root system the overestimations of soil temperature in deep layers were also found in other dynamic models such as dssat decision support system for agrotechnology transfer and vsmb versatile soil moisture budget because of deficiencies of physical processes that modulate soil temperature at depth liu et al 2013 akinseloyin 2015 3 3 soil moisture the simulated soil water dynamics in the 0 0 05 m and 0 05 0 10 m soil layers were evaluated using measured values from 2014 to 2016 at cfia site fig a 2 table 4 both models overall demonstrated poor to good performance in simulating daily soil moisture for both depths with the average statistics of nrmse 9 7 31 8 d 0 64 0 78 and nse 0 11 0 36 however both versions showed fair to good agreements between the simulated and measured soil water contents in the seeding year 2014 according to the average values of nrmse 20 nse 0 and d 0 70 fig a 2 additionally the model performed well in simulating growing season soil water content which helped to explain why the model was still able to simulate alfalfa biomass relatively well in spite of poorly simulated depth specific water contents fig a 3 also in this study we can only compare simulated verses observed soil water content in the top 0 1 m but alfalfa roots often extend to 1 5 m depth or more the poor to fair daily simulations in post seeding years with less seasonal rainfall e g 380 mm in 2016 were possibly due to the underestimated interception of rainfall by the alfalfa canopy and rapid water uptake by a dense rooting system this may limit soil water content simulation under dry conditions which need to be better characterized in dndc brady and weil 2002 uzoma et al 2015 another possibility is that the root density determines the weighted water uptake demand over depth but the generalized root density function does not fully represent a perennial stand that builds a dense rooting structure as stand age increases further adding to this issue is that dndc only simulates detailed soil water dynamics to 50 cm with an additional deep water pool for the 50 100 cm depth this can limit the potential water uptake for deep rooted crops such as alfalfa which might explain the rapid deviation in simulated water content near the surface layers relative to observations we suggest the model be improved in future development by extending the profile depth and including an alfalfa specific root density growth function 3 4 climate change impacts on alfalfa yield and winterkill the simulated annual alfalfa dm yields under four future climate scenarios increased markedly relative to the 1971 2000 baseline periods at all locations fig 6 table 5 when simulating a moderately cold tolerant cultivar with a maximum temperature of cold tolerance ctmx of 15 c under four climate scenarios the average annual alfalfa dm yields across the seeding year and the first two post seeding years increased by 45 0 83 6 at elora 57 7 84 9 at ottawa and by 66 9 119 4 at quebec city compared to the baseline period of 1971 2000 table 5 when a more cold tolerant cultivar ctmx 20 c was simulated the annual alfalfa dm yield notably increased especially for ottawa and quebec city colder conditions compared to a moderately cold tolerant cultivar as expected the baseline annual alfalfa dm yield significantly decreased at all three locations due to winterkill damage especially in the second post seeding year when using a more sensitive cultivar ctmx 10 c less hardiness fig 6 however the average annual dm yields under both future climate scenarios significantly increased compared to the baseline being attributed to the longer growing season with earlier planting spring regrowth more cuts and more precipitation table 5 the average planting dates under future climate scenarios were 10 18 9 15 and 6 13 days earlier than the baseline scenarios for elora ottawa and quebec city respectively for baseline scenarios the average cut numbers for the seeding year and post seeding year were 1 6 and 2 7 in elora 1 9 and 2 9 in ottawa and 1 3 and 2 4 in quebec city respectively in contrast the average cut numbers increased up to 2 2 3 1 and 3 5 4 1 in elora 2 5 3 1 and 3 8 4 2 in ottawa and 2 1 2 7 and 3 1 4 0 in quebec city for the seeding year and post seeding year cuts depending on the future climate scenarios table 5 our results were consistent with thivierge et al 2016 and jing et al 2014 who indicated that the forage crops yields including alfalfa and timothy would increase under future climate scenarios with adaptations more cuts earlier planting spring regrowth but the yield increases under future climate scenarios were higher in our study probably due to the reduced crop water stress by increasing water use efficiency under elevated co2 and more precipitation this is also reported in studies for other crops which indicated that crop water stress might be reduced in ontario under future climate smith et al 2013 he et al 2018 winterkill damage had significant impacts on alfalfa yield under baseline and future climate scenarios fig 7 table 5 the winterkill effects under baseline scenarios could range from 0 to 23 3 in elora 2 8 50 9 in ottawa and 0 35 0 in quebec city ctmx 20 to 10 c table 5 the modelled impacts of winterkill on alfalfa yields under future climate change were marginally reduced at elora which has warmer winters than the other two locations the impacts of winterkill at ottawa and quebec city remained consistently high under conditions with increased temperature fig 7 note that the impact of winterkill tended to be lower under rcp4 5 1941 2070 scenarios than the baseline condition this was because the regional climate model canrcm4 estimated a reduced number of cold days below ctmx in this period yet the snow cover was not greatly impacted although there is less extreme cold temperature events under rcp4 5 2071 2100 rcp8 5 2041 2070 and rcp8 5 2071 2100 there is still high projected winterkill injury on alfalfa which was caused by two factors 1 the warmer fall temperatures resulted in loss of hardening and 2 there was less insulation from snow cover resulting in the crown temperature dropping below the cold tolerance threshold ctmx even though the average air temperature was higher bélanger et al 2002 thus it is projected that winter hardy cultivars of alfalfa will continue to be necessary under future climate conditions 3 5 caveats and future improvements there are uncertainties in projecting the impacts of climate change on alfalfa production due to several factors including the inherent structural uncertainty in crop and climate models uncertainty associated with model calibration parameterization and evaluation as well as the selected socio economic emission scenarios asseng et al 2013 whitfield et al 2013 in this study effort was placed on evaluating the improved dndc model at locations of contrasting climate where the site data was not employed for development the model performed well in simulating alfalfa biomass during growth cycles and alfalfa dm yield at normal harvest times however we expect that further calibration would be necessary under warmer climates where different cultivars are employed in this case calibration of alfalfa parameters such as gdd plant biomass fractions biomass c n ratios optimum temperature hardening and dehardening rates may be needed if field measurements of alfalfa biomass over time are available then the growth curves describing phenology can also be adjusted for other crop types simulated in warmer regions and where the model has performed well relative to detailed physiological crop models we have found that this is seldom necessary ehrhardt et al 2018 smith et al 2019 the dndc model generally provides a better estimate of overall soil water storage than it does of water content in specific soil layers root growth down the soil profile is simulated to be linear hence water uptake near the soil surface may be underestimated whereas it may be over estimated deeper in the profile smith et al 2019 this could particularly be true as the alfalfa stand matures as it stands a mass balance of water is considered and general water deficit stress on crop production is simulated but the model could benefit from the inclusion of root density functions to improve soil water simulation at specific depths further dndc does include rainfall interception by crop canopies however interception may be underestimated uzoma et al 2015 additionally the dndc model uses a cascade water flow approach which is common in many agricultural models where each soil layer tips to field capacity every hour this can sometimes result in the underestimation of soil water contents when observations are above field capacity the uncertainty associated with canrcm4 generated climate scenarios could be large as noted in comparisons between global climate models or regional climate models means et al 2012 scinocca et al 2016 however this could be tested in the future by driving the dndc model with a range of future climate scenarios from various climate models the alfalfa growth can be affected when the hardiness increases as ctmx decreases hardening could result in decreased water potential in tissues and conformational changes in either membrane and or proteins and or an aba hormone receptor which can in turn result in the regulation of genes involved in cold acclimation a strong association exists between the degree of vernalization and the degree of cold tolerance that can be controlled by major genes related to freezing tolerance and plant growth sãulescu and braun 2001 thus the objective of breeding is to develop cultivars with the minimum cold hardiness necessary for avoiding winterkill and not to affect plant yield due to delayed spring growth or small cells the model currently does not include this function which can be developed in the future providing measurements are available in addition some soil parameters were assumed to remain constant in the future but certain soil physical properties e g hydraulic conductivity water holding capacity may change under different management practices e g fertilizer tillage rotation which could further impact winterkill effects and alfalfa yield bélanger et al 2002 castonguay et al 2006 the dndc model does simulate impacts of soil c change on water holding capacity however it does not include all positive and negative impacts on soil hydraulic properties cold tolerance is the most important factor for alfalfa winterkill which has been integrated into the dndc model but there are other factors affecting alfalfa winter survival including ice encasement soil heaving frost heaving diseases fusarium infected roots which were not captured in the dndc model due to insufficient field measurements to support model development bélanger et al 2002 2006 castonguay et al 2006 2009 thus a better expression of winterkill impacts could possibly be implemented in dndc once more experimental information is available 4 conclusions the dndc model was modified to improve alfalfa growth simulation using measured biomass and yield data in eastern canada the results demonstrated that the improved model with enhanced alfalfa growth algorithms showed statistically better performance compared to the default model in most biophysical variables the simulated soil temperature had good agreements with the measured values with better simulation for improved dndc in addition the dndc model predicted soil water contents well in the seeding year but it only showed poor to fair simulations in soil water contents in the alfalfa post seeding years likely attributed to root development uptake inaccuracies compared to the 1971 2000 baseline scenarios the average annual yields increased significantly under all future climate scenarios which was attributed to the longer growing season with earlier planting more cuts and increased water use efficiency lower water stress under elevated co2 winterkill had notable effects on alfalfa production under both baseline and future scenarios depending on the cultivar cold tolerances thus it is still necessary to develop winter hardy cultivars for future adaptation additionally more winterkill factors e g ice encasement soil heaving need to be monitored in the field experiments so that their effects can be implemented in mechanistic models to enable more accurate evaluation of winterkill effects under future climate declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors acknowledge the financial support of science and technology branch of agriculture and agri food canada we also gratefully thank ray desjardins for guidance and devon worth for the technical support appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104540 
26118,the denitrification decomposition dndc model was developed to better simulate alfalfa medicago sativa l production with winterkill effects in eastern canada the pre development dndc produced fair simulations of alfalfa yield and biomass index of agreement d 0 7 nash sutcliffe efficiency nse 0 but normalized root mean square error nrmse was 30 whereas the improved model indicated good to excellent performance nrmse 3 9 29 d 0 93 0 99 nse 0 76 0 99 under future rcp4 5 and 8 5 scenarios average annual yields increased by 60 3 and 81 8 respectively due to earlier planting spring regrowth with additional cuttings increased c assimilation and reduced water stress under higher co2 concentrations for locations at ottawa and quebec city there could be an increased incidence of winterkill under future climate due to reduced snow cover which leads to colder soil crown temperatures as well as reduced fall hardening this study indicated that the use of winter hardy cultivars could mitigate winterkill effects and increase production keywords dndc model alfalfa growth soil temperature moisture climate change winterkill 1 introduction alfalfa medicago sativa l is the most widely grown forage legume worldwide and plays a key role in the development of sustainable cropping and livestock production systems crews and peoples 2004 castonguay et al 2006 thivierge et al 2016 alfalfa contributes to sustainable agricultural systems by improving soil quality increasing water infiltration and soil water storage reducing soil erosion and reducing nutrient losses e g n2o fluxes and no3 leaching sarrantonio and gallandt 2003 confalonieri and bechini 2004 blackshaw et al 2010 there is considerable interest in maximizing economic return and minimizing environmental cost for cropping systems used in dairy livestock production where the alfalfa crop is often an intergral component confalonieri and bechihi 2004 brown 2009 in eastern canada perennial forage crops account for about 40 of total agricultural land 2 1 million ha with an annual estimated farm value of 1 3 billion can statistics canada 2001 bélanger et al 2006 it has been shown that the yield and n content of wheat following alfalfa was increased compared to wheat following a canola control bullied et al 2002 compared to monocultural corn alfalfa in rotation with corn increased soil organic carbon increased corn yields reduced n fertilizer rate and decreased n2o emissions from agricultural fields mackenzie et al 1998 gregorich et al 2001 in addition jarecki et al 2018 noted that for corn rotations that included alfalfa an increase in soil carbon and corn yields was observed as compared to monoculture corn and that these increases continued under future climate scenarios whereas in the monoculture system no further change was realized a major challenge for perennial crops in northern regions is the harsh overwintering conditions that contribute to frequent stand losses and yield reduction in canada suzuki 1972 ouellet 1976 bélanger et al 2006 several regions of the united states barnhart et al 2004 castonguay et al 2006 and northern europe eagles et al 1997 the winterkill of perennial crops is largely affected by climatic conditions including subfreezing temperatures lack of snow cover and ice encasement ouellet 1976 schwab et al 1996 bélanger et al 2006 cold tolerance of cultivars is the capacity of plants to tolerate subfreezing temperatures which can significantly influence the winter survival of perennial crops bélanger et al 2002 castonguay et al 2011 previous studies assessed the effects of environmental factors on the survival of winter sensitive forage species including alfalfa under both natural and controlled conditions paquin and mehuys 1980 mckenzie and mclean 1984 schwab et al 1996 bélanger et al 2014 the reported correlations between climate variables and winter damages were used to explore optimal climatic conditions e g temperature precipitation for winter survival ouellet 1976 bélanger et al 2002 castonguay et al 2011 and to develop appropriate algorithms for modelling winter hardiness and injury kanneganti et al 1998a b changes in temperature precipitation and atmospheric co2 concentration influence crop growth by impacting the rate of photosynthesis respiration and crop water use efficiency wue along with influencing soil biological and chemical transformations of c and n guo et al 2010 wang et al 2014 long et al 2015 alfalfa a c3 legume species benefits from elevated atmospheric co2 due to increased photosynthesis morgan et al 2004 and improved water use efficiency leakey et al 2009 barton et al 2011 many forage species including alfalfa are sensitive to drought and would be affected by future changes in precipitation rate and pattern aranjuelo et al 2007 thivierge et al 2016 the positive response to increased co2 concentration might be offset by an increased temperature increased evapotranspiration or reduced precipitation hatfield et al 2011 lee et al 2013 piva et al 2013 climate change may therefore have greatly varying effects on alfalfa production depending on the selected cultivars management and soil and climatic conditions izaurralde et al 2011 ipcc 2012 thivierge et al 2016 process based models have been widely employed across diverse agroecosystems to estimate crop production hydrologic processes and nutrient cycling jones et al 2003 williams et al 2008 li et al 2012 holzworth et al 2014 the denitrification decomposition dndc model is an example of a well established tool used to simulate crop growth and development ghg emissions and water quality in crop livestock and forest systems li et al 1992a b smith et al 2013 congreves et al 2016 the dndc model li et al 1992a b was initially developed to predict greenhouse gas ghg emissions from agricultural soils and was expanded to simulate soil c n cycling and nitrate leaching li et al 2006 and more recently to simulate nutrient cycling for full farm facility and livestock systems li et al 2012 the simulation of crop growth has advanced over the last 25 years zhang and niu 2016 and certain versions of dndc use detailed approaches for simulating physiological processes such as crop dndc zhang et al 2002 several countries have developed regional versions of dndc to improve the predictions of their agricultural systems brown et al 2002 leip et al 2011 smith et al 2013 gilhespy et al 2014 li et al 2017 including a canadian version which has been used to simulate crop soil and plant processes under cool climate conditions kröbel et al 2011 smith et al 2013 dutta et al 2016 he et al 2018 currently the model algorithms have been calibrated and validated to characterize canadian growing conditions including sub models for crop growth kröbel et al 2011 soil organic c dynamics smith et al 2012 n2o emission uzoma et al 2015 evapotranspiration dutta et al 2016 nh3 volatilization congreves et al 2016 and soil temperature dutta et al 2018 much of the development work has been conducted through comparison with other modeling frameworks in particular with daycent dssat and rzwqm2 grant et al 2016 dutta et al 2017 guest et al 2017 smith et al 2019 along with the cooperation with the primary us developers of dndc the current dndc model however is limited in its ability to simulate perennial crop growth especially regrowth after cutting and in subsequent seasons considering winterkill impacts this limits the application of the model for exploring scenarios under livestock based systems that include forages therefore the objectives of this study were to 1 improve alfalfa growth and development in the dndc model using measured biomass and yield data under various soil and climate conditions 2 compare performance of the revised and default versions for simulating long term alfalfa production soil temperature and moisture and 3 assess projected climate change impacts on alfalfa production including winterkill in eastern canada 2 materials and methods 2 1 research site and experiment measurements of aboveground biomass during the growth cycles at one location in eastern canada were used independently to develop a new phenological growth curve and then validate this new growth curve alfalfa was seeded in 2014 at the canadian food inspection agency cfia greenbelt farm southwest of ottawa 45 18 n 75 45 w fig 1 aboveground biomass samples were taken between 2 and 8 times in each of three growth cycles in the seeding year and four growth cycles in the first 2015 and second 2016 post seeding years the soil is a sandy loam clay 14 and the average annual precipitation is 950 mm 450 mm from may to september from an on site aafc weather station the soil clay fraction ph bulk density and soc of the topsoil 0 20 cm were 14 6 8 1 26 mg m 3 and 18 0 g kg 1 respectively the daily soil temperature c for four soil depths 0 05 0 10 0 20 and 0 50 m and volumetric soil water content m3 m 3 for two soil layers 0 0 05 and 0 05 0 10 m were monitored from 2014 to 2016 using soil thermocouple array 0 05 m and thermistors other depths and water content reflectometers respectively campbell scientific edmonton ab additional aboveground alfalfa biomass measurements were taken on a near weekly basis every 7 10 days from three experiments in 2014 near the cfia site in ottawa fig 1 during biomass collection a 0 25 m2 quadrant frame was randomly placed in four locations within each field to make a composite 1 m2 sample those measurements were made in the seeding year ottawa o2 45 10 n 75 16 w and in the first post seeding year ottawa o1 45 15 n 75 49 w and ottawa o3 45 05 n 75 10 w and they were used for the validation of the newly calibrated biomass curve measurements of alfalfa dm yields at the normal harvest times from two other sites in eastern canada were also used a long term experiment established in 1981 at the elora research station 43 38 n 80 25 w in southern ontario with corn corn alfalfa alfalfa ccaa and aacc rotations was used fig 1 measurements of alfalfa dm yield were made in the seeding year and the first post seeding year the mean annual precipitation was about 900 mm with seasonal rainfall of 425 mm may to september and average monthly temperature ranged from 7 c in january to 20 c in july the soil was a silt loam clay 17 with an average ph of 7 3 dry bulk density of 1 3 mg m 3 total n of 1 8 g kg 1 and soc of 21 2 g kg 1 in the 0 20 cm soil layer the final research site site 3 used for validation is located in normandin quebec 48 50 n 72 31 w with an average annual precipitation of 1100 mm and seasonal rainfall of 470 mm where alfalfa was planted in three experiments n1 1994 1996 n2 1995 1997 and n3 2008 2011 no measurements in 1994 1995 and 2011 for n1 n2 and n3 respectively a strip 5 m long and 0 6 m wide was harvested for different growth cycles to determine each cut yield with the majority of measurements in the early flowering stage the soil is a silt clay 45 clay and the bulk density was 1 32 mg m 3 detailed soil information and management practices for all sites are shown in table 1 and a 1 2 2 dndc model 2 2 1 model description the dndc model is a process based model describing c and n dynamics in agroecosystems on a daily time step li et al 1992a b it has been used worldwide to simulate greenhouse gas emissions crop growth soil c and n dynamics and water balance under different management practices and weather conditions li et al 2012 smith et al 2013 abdalla et al 2014 gilhespy et al 2014 brilli et al 2017 ehrhardt et al 2018 dndc is a complex two component model that incorporates many deterministic equations based on the classic laws of physics chemistry and biology along with empirical equations derived from research studies qin et al 2013 he et al 2018 the first component consists of the soil climate crop growth and decomposition sub models which are driven by four ecological drivers climate soil vegetation and management practices the second major component consists of the nitrification denitrification and fermentation sub models which are driven by simulated soil environmental factors to predict trace gas fluxes from the plant soil system li et al 2012 uzoma et al 2015 deng et al 2016 2 2 2 canadian version of dndc dndc v can a canadian specific version of the dndc model https github com brianbgrant dndcv can dndc v can has been developed over the past decade to adjust the simulation under canadian climate soil types and cropping systems the new additions in the canadian version of dndc include 1 the improvements of empirical growth curves for corn wheat and soybean kröbel et al 2011 grant et al 2016 2 the incorporation of the effects of atmospheric co2 concentration on water and n use efficiency revised temperature stress based on cardinal temperatures and temperature stress during anthesis smith et al 2013 3 the adjustment of the holding capacity for the deep water pool based on soil properties grant et al 2016 4 the improvement of et prediction based on penman monteith and fao crop coefficients dutta et al 2016 5 an improved nh3 volatilization routine for surface applied manure slurries congreves et al 2016 6 the incorporation of a pedo transfer function to recalculate changes in soil properties based on changes in soc jarecki et al 2018 and a new soil temperature submodel which includes effects of snow insulation surface residue live biomass and soil texture dutta et al 2018 2 2 3 development of an improved alfalfa growth curve 2 2 3 1 pre development dndc default dndc the dndc model is based on a gdd growing degree day approach for crop production coupled with phenological growth curves for specific cultivars which determine the crop n water and temperature requirements during the different growth phases the growth index 0 1 in dndc is conceived to represent the crop from germination to maturity the optimal rate of growth is expressed as a nitrogen demand curve over the full range of this index and is reduced by incurred crop stresses temperature water nutrient winterkill biomass c accumulation is derived from this growth curve using the parameterized c n ratios of the various biomass fractions and is segmented into vegetative and reproductive growth phases to characterize specific biomass fraction growth the growth index is advanced based on the calculated daily tdd thermal degree days which is synonymous with gdd calculated at a base degree threshold of 0 c the timing of phenology is determined by tdd and is not altered by any stress however the timing of incurred stresses does influence the degree of impact from water and temperature stress on the harvest index for many crops not alfalfa aboveground alfalfa live c was assumed to shift to standing dead c when daily air temperatures reached 2 c for more than 24 h by default the potential n demand for perennial crops is the daily change in pgi multiplied by the total n demand of the crop determined by crop input parameters 1 d ndemand totcrpn δ pgi where dndemand kg n ha 1 is the daily potential n demand of the crop totcrpn kg n ha 1 is the total crop n demand based on crop parameterization and δ pgi is the change in daily pgi 2 2 3 2 post development dndc improved dndc measurements of aboveground biomass means of four quadrat samples during the growth cycles from the ottawa cfia location 2014 2016 were used to construct a new alfalfa growth routine in dndc dry aboveground biomass measurements across the three years n 54 were pooled into first growth in the season and subsequent regrowth and converted to c equivalents absolute biomass c was then calculated as a rate of biomass c accumulation between measured dates growth rates per unit of tdd were slower at the first growth cycle after planting than for periods after cutting events likely due to root establishment based on this observation two functions were fit to each distinct growth period alfalfa growth in the beginning of the season 2 d ndemand 27 41 pgi totcrpn ccrpn alfalfa regrowth after cutting 3 d ndemand 12 43 pgi 2 totcrpn ccrpn where ccrpn kg n ha 1 represents the cumulative total n demand calculated for entire seasonal crop tdd as a total area under the curve as a means to scale the dndemand kg n ha 1 to the target crop n demand the equations represent a relative index along the growth curve using the actual to date nitrogen uptake totcrpn seasonal total potential n demand the value of ccrpn equalsthe area under the curve from the empirically derived measurements for plant n uptake over specific measured years and the modelled years need to be scaled to equal this measured period the crop gdd in the model is reset based on the average gdd realized from one cut to the next cut in the measured data similar to the recommended practices in thivierge et al 2016 other assumptions included that when a killing frost temperature occurs any remaining above ground live c is shifted to the standing dead residue pool and is subject to decomposition into the litter soil pools biomass c accumulation is directly calculated from the modelled n uptake amount and the c n ratio in the crop input parameters 2 2 4 development of cold tolerance and winterkill integration of alfacold model the process of cold hardening and dehardening based on the alfacold model kanneganti et al 1998a b were integrated into the dndc model to predict winterkill impacts on alfalfa growth alfacold model is a alfalfa simulation model which developed and incorporated numerical functions of cold tolerance fall dormancy and freezing injury to simulate alfalfa growth based on previous studies in canada mckenzie and mclean 1980 1984 paquin and pelletier 1980 mckenzie et al 1988 daily rate of increase in cold hardening hri c d 1 was computed as follows 4 hri chrmx ethri where ethri dimensionless 1 to 1 represents the effect of average daily crown temperature crtmp c on the potential rate of hardening chrmx c d 1 values of ethri are a function of soil temperature in the crown region at 3 cm depth kanneganti et al 1998a the daily rate of dehardening dri c d 1 which results in a decrease in cold tolerance is calculated as follows 5 dri cdrmx etdri where etdri dimensionless 0 to 1 represents the effect of average daily crown temperature on the potential rate of dehardening cdrmx c d 1 the values of etdri were based on the previous experiments paquin and pelletier 1980 mckenzie et al 1988 kanneganti et al 1998a the state of cold tolerance in a cultivar is quantified by ctt c which is equivalent to the subzero temperature that a crop can tolerate without being killed it is calculated as follows 6 ctt t max ctmx min 0 ctt t 1 hri dri δt the difference between hri and dri represents the daily rate of net increase or decrease in cold tolerance the δt is calculated on a daily time step note that ctt c and ctmx maximum temperature of cold tolerance c are negative values indicating subzero temperatures ctt is initialized to 0 c at the start of a simulation kanneganti et al 1998a plant death caused by winterkill is quantified by a plant death factor pdf d l which represents a fraction of plant population plants m 2 dead in a day as follows 7 pdf pdfmx max 0 ctt crtmp where crtmp is the average daily crown temperature which was estimated using soil temperature in the crown region at 3 cm depth and pdfmx d l c 1 below ctt represents cultivar specific potential rate of plant death the dndc model calculates soil temperature over each 1 cm increment and thus the 3 cm value was used in this equation in this study the default value of pdfmx was set as 0 12 d l c 1 based on previous studies in canada mckenzie and mclean 1980 1984 the calculation and incidence of a pdf event is applied as a cumulative scalar pdfcummulative that acts to reduce the potential crop n demand c biomass growth in alfalfa in order to represent the reduction in plant population due to winterkill over time e g 0 0 no winterkill effect and 1 0 representing 100 loss in plant population this approach is necessary as dndc does not explicitly represent whole plant populations as it is a 1d model thus the reduction of plant populations needed to be applied on a per plant basis in the model the winterkill effect we on alfalfa yield for each overwinter period was modelled based on potential yield reduction of the subsequent year the cumulative winterkill effect cwe is calculated based on the initial potential yield and total yield reduction due to winterkill as follows 8 we spyr spy 100 9 cwe tpyr ipy 100 where spyr kg ha 1 is potential yield reduction of subsequent year spy kg ha 1 is the potential alfalfa yield of subsequent year tpyr kg ha 1 represents the total potential yield reduction for all growing years because of plant stand loss and ipy kg ha 1 is the initial potential yield of alfalfa effect of fall dormancy on growth is simulated as a function of a cultivar s fall growth score fgs which ranged from 1 0 for highly dormant cultivars to 9 0 for the least dormant types it is cacluated based on crop height after approximately 40 days of regrowth following a cutting in the fall schwab et al 1996 kanneganti et al 1998a alfalfa winterkill could be highly affected by the fgs specified cultivar characteristics including hri and dri ctmx as well as pdfmx based on previous studies the soil temperature in alfalfa field conditions rarely goes below 15 c in eastern canada paquin and mehuys 1980 bélanger et al 2002 hence 15 c was chosen as the general ctmx of alfalfa cultivars fgs 5 to calibrate the model in this study kanneganti et al 1998a additionally based on the survival temperature in mckenzie and mclean 1980 a ctmx sensitivity of 20 and 10 c was also conducted to explore climate change and winterkill impacts on alfalfa yields table a 2 2 2 5 inputs initialization calibration and validation of dndc the dndc model requires the following input information 1 agricultural management data e g planting harvest dates tillage fertilizer rates and types residue 2 soil data e g soil bulk density soil ph texture initial soil organic c and soil nitrate and ammonium n content soil hydraulic parameters 3 daily weather maximum and minimum air temperature tmax and tmin c solar radiation mj m 2 daily precipitation cm wind speed m s 1 and humidity and 4 crop parameters including alfalfa potential yield kg c ha 1 yr 1 biomass fraction ratio of grain leaf stem and root biomass c n ratio tdd water demand g water g 1 dry matter and optimal temperature c zhang and niu 2016 he et al 2018 jarecki et al 2018 the local meteorological data during the experimental periods were acquired from environment canada weather stations located at each of the experimental sites the soil and crop management inputs were based on information gathered during the course of each field study and are listed in table 1 and a 1 a spin up period was simulated for 10 years prior to the experimental period of interest at each experimental site to stabilize c and n pools the aboveground alfalfa biomass data from the ottawa cfia site in 2014 2016 cultivar c2014 were selected for calibrating crop parameters annual dm yields from 1981 to 1984 1992 1994 2000 2002 and 2006 2009 were used to calibrate the cultivars e1981 1981 1991 e1992 1992 1999 e2000 2000 2005 and e2006 2006 2015 respectively for the long term study at elora the cultivar apica was calibrated using yields from each cut at normandin for the first 1995 and second 1996 post seeding years n1 table a 3 in this study the model was calibrated using trial and error during the first stage which was useful to quickly see the output changes affected by input parameters after initial trial and error we optimized rmse for each variable in a stepwise manner through r language then repeated the process until all parameters were reasonably optimized the calibration process generally resulted in similar crop biomass fractions across the sites however thermal degree days and water demand sometimes varied table a 3 the remaining biomass datasets from ottawa o1 o2 and o3 and yields data from elora 1985 1991 1995 1999 2003 2005 2010 2015 and normandin n2 and n3 were then used to validate the model performance 2 3 climate change scenarios climate scenarios of the representative concentration pathways rcps were used from the fifth assessment report of ipcc ipcc 2014 to make projections based on four different 21st century pathways of ghg emissions and atmospheric concentrations air pollutant emissions and land use moss et al 2010 van vuuren et al 2011 a new regional climate model canrcm4 was developed at the canadian centre for climate modelling and analysis cccma of environment canada scinocca et al 2016 in our study daily outputs from canrcm4 were obtained from cccma for the historical baseline period 1971 2000 and for two future periods of 2041 2070 and 2071 2100 under the forcing scenarios rcp4 5 and rcp8 5 which represent medium low and high emission scenarios with a radiative forcing of 4 5 and 8 5 w m 2 at the end of the 21st century respectively van vuuren et al 2011 ipcc 2014 in this study the cropping system used in future scenarios was corn c corn alfalfa a alfalfa alfalfa with five rotation phases simulated e g aaacc aacca accaa caaac ccaaa to ensure each year of the alfalfa stand was present for each year of weather the managements were set the same as the field experiments at each location three locations including elora ottawa and quebec city which represent different winter climate conditions were selected to explore the climate change impacts on alfalfa yield and related winterkill the comparison of historical and projected climate normals at the three locations are shown in table 2 and table a 4 2 4 adaptation measures employed under future climate to simulate the adaptation that farmers may employ in response to changes in growing season length that are expected to occur under climate change several adjustments were made to alfalfa management planting date and harvest date of each cut were adjusted during each year based on weather planting dates were evaluated based on the criteria in bootsma and de jong 1998 and qian et al 2018 which include a snow cover 10 mm for the day b daily precipitation 2 5 mm c 0 75tmax 0 25tmin 7 c d the sw1 soil water at the top 5 of the soil profile 90 available water holding capacity awhc e sw2 soil water at the next 7 5 of the profile 0 95 awhc therefore potential planting dates were automatically taken into consideration in the dndc simulations for both baseline and future climate scenarios additionally the timing and number of cuts was considered based on accumulated growing degree days during the alfalfa growth period in the improved dndc model automated harvesting was included and occurred when the alfalfa pgi see section 2 2 3 reached 0 85 along with ensuring that no cuts occurred less than 30 days before the first fall frost 2 5 model performance statistics to evaluate dndc model performance several statistics were employed to compare simulations against measurements for alfalfa aboveground biomass and dm yield soil temperature and soil moisture see eqs 10 14 below these included mean bias error mbe root mean square error rmse normalized root mean square error nrmse nash sutcliffe efficiency nse and index of agreement d nash and sutcliffe 1970 willmott 1985 yang et al 2000 moriasi et al 2007 he et al 2018 10 mbe i 1 n s i m i n 11 rmse i 1 n s i m i 2 n 12 nrmse i 1 n s i m i 2 n m 100 13 nse 1 i 1 n s i m i 2 i 1 n m i m 2 14 d 1 i 1 n s i m i 2 i 1 n s i m m i m 2 where si is the simulated value mi is the measured value i 1 n is the number of measured values and m is the mean of the measured values the mbe nrmse nse and d statistics evaluate various aspects of model performance the mbe can be used to illustrate whether the model showed underestimation mbe 0 or overestimation mbe 0 compared to the measured values willmott et al 1985 yang et al 2000 based on the recommendations in previous studies li et al 2015 he et al 2018 we consider an excellent model performance when the nrmse 10 good performance when 10 nrmse 20 fair performance when 20 nrmse 30 and poor performance when the nrmse 30 a nse to 1 value 1 0 indicates perfect agreement 0 50 nse 1 0 indicates good agreement 0 0 nse 0 50 indicates fair agreement and a nse 0 0 indicates poor agreement between simulated and measured data moriasi et al 2007 schaefli and gupta 2007 a value of d 0 9 illustrates excellent match 0 8 d 0 9 illustrates good match 0 7 d 0 8 illustrates fair match and d 0 7 illustrates poor match when comparing the simulated and measured values he et al 2018 3 results and discussion 3 1 alfalfa growth 3 1 1 aboveground biomass during growth cycles the calibration of the default dndc model resulted in good or fair agreements between the simulated and measured alfalfa aboveground biomass based on the values of nse 0 and d index 0 8 but the alfalfa aboveground biomass of the spring regrowth was underestimated in 2015 first post seeding year and 2016 second post seeding year fig 2 and a 1 table 3 the improved dndc model performed very well in simulating alfalfa aboveground biomass for all years with nse 0 9 d 0 9 and nrmse 20 table 3 the results overall indicated that the improved dndc model produced significantly better simulations of alfalfa biomass compared to the default version for validation the dndc model performance was evaluated using the simulated and measured biomass in 2014 first post seeding year at ottawa o1 in 2014 seeding year at ottawa o2 and in 2014 first post seeding year at ottawa o3 fig 2 table 3 for the default dndc model the statistical values of nse 0 5 and d values 0 8 indicated fair to good agreements between the simulated and measured alfalfa aboveground biomass for the three experiments but the model underestimated the biomass mbe 0 53 and 0 55 mg dm ha 1 with a high rmse 1 03 and 0 98 mg dm ha 1 at ottawa o1 and o3 which was consistent with the statistics in the calibration phase of the post seeding years at cfia fig 2 table 3 in contrast the improved dndc model showed good to excellent performances based on the statistical evaluations between the simulated and measured biomass the values of nse d nrmse ranged from 0 76 to 0 92 0 93 0 98 and 16 4 29 0 respectively table 3 the significantly better performance in predicting alfalfa aboveground biomass was due solely to the inclusion of the revised growth curve equations 2 and 3 which captured the rate of spring growth regrowth after cutting and regrowth in subsequent years 3 1 2 alfalfa yield the simulated alfalfa dm yield by the default and improved dndc models was compared to the annual dm yield at the long term experiment in elora as well as to the dm yield for each cut at normandin figs 3 and 4 table 3 in the calibration years the default dndc model only showed poor to good performance at normandin and elora based on the values of nse d and nrmse ranging from 1 1 0 60 0 52 0 84 and 23 9 24 0 respectively this was because the default dndc model underestimated the first cut dm yield of the first 1995 and second 1996 post seeding years at normandin and the annual dm yields of the first post seeding year in 1981 at elora fig 3 the improved dndc model produced markedly better simulations with good to excellent performance in predicting alfalfa annual cut yield with statistical values of nse of 0 88 0 95 d of 0 95 0 99 and nrmse of 3 9 17 9 table 3 fig 4 in the validation years the default dndc model overestimated alfalfa annual yields for the long term study at elora mbe 0 55 mg ha 1 due to the lack of sensitivity to dry conditions and a significant underestimation of the first cut dm yield in all post seeding years at normandin table 3 fig 3 the improved dndc model produced statistical values that were similar to the calibration phase e g d index ranged 0 94 0 99 nse ranged from 0 82 to 0 99 and nrmse ranged from 7 9 to 18 0 indicating good to excellent agreements between simulated and measured alfalfa yield the yield response to water parameters e g water demand field capacity and wilting point could be well captured during dry years e g 2016 in ottawa based on the sensitivity analysis in the revised dndc model fig a2 the impact of winterkill on alfalfa dm yield in the improved dndc model averaged less than 2 at elora but it was 18 6 in the overwinter 1996 1997 and 27 1 in the overwinter 2009 2010 at normandin the improved dndc model with the new alfalfa growth algorithm including winterkill effects performed better in estimating alfalfa yield than in a previous study at the same location conducted using ifsm integrated farm system model jégo et al 2015 which did not include the impact of winterkill and showed a poor to good performance based on rmse 960 kg ha 1 nrmse 30 and nse 0 51 moreover for other perennial crops e g timothy the stics simulateur multidisciplinaire pour les cultures standard and catimo canadian timothy model performed good in simulating aboveground biomass for primary growth with nrmse of 16 and 21 respectively but both models showed worse performances nrmse 27 89 during regrowth cycles compared to improved regrowth simulations in our revised dndc model jégo et al 2013 jing et al 2013 3 2 soil temperature soil temperatures were measured and simulated across different soil depths from 2014 to 2016 at cfia fig 5 table 4 the simulated daily soil temperature dynamics from both versions of dndc in the soil depths 0 05 0 10 and 0 20 and 0 5 m corresponded very well with the measured values with no statistically significant differences being found based on paired t test p 0 05 the nrmse 15 nse 0 60 and d 0 90 also indicated good to excellent agreements but the simulations were better for the improved dndc model table 4 the default model showed increasing overestimations in soil temperature with soil depths which was mitigated in the improved version by adjusting solar radiation impacts on soil temperature for example the mbe values were 0 78 and 0 29 c at the deeper 0 50 m soil layer for the default and improved dndc models respectively table 4 the dndc model was previously improved for estimating soil temperature dutta et al 2018 but the effects of cumulative insulation of surface crop residues and plant shading may be underestimated since alfalfa can establish a very dense canopy residue cover and root system the overestimations of soil temperature in deep layers were also found in other dynamic models such as dssat decision support system for agrotechnology transfer and vsmb versatile soil moisture budget because of deficiencies of physical processes that modulate soil temperature at depth liu et al 2013 akinseloyin 2015 3 3 soil moisture the simulated soil water dynamics in the 0 0 05 m and 0 05 0 10 m soil layers were evaluated using measured values from 2014 to 2016 at cfia site fig a 2 table 4 both models overall demonstrated poor to good performance in simulating daily soil moisture for both depths with the average statistics of nrmse 9 7 31 8 d 0 64 0 78 and nse 0 11 0 36 however both versions showed fair to good agreements between the simulated and measured soil water contents in the seeding year 2014 according to the average values of nrmse 20 nse 0 and d 0 70 fig a 2 additionally the model performed well in simulating growing season soil water content which helped to explain why the model was still able to simulate alfalfa biomass relatively well in spite of poorly simulated depth specific water contents fig a 3 also in this study we can only compare simulated verses observed soil water content in the top 0 1 m but alfalfa roots often extend to 1 5 m depth or more the poor to fair daily simulations in post seeding years with less seasonal rainfall e g 380 mm in 2016 were possibly due to the underestimated interception of rainfall by the alfalfa canopy and rapid water uptake by a dense rooting system this may limit soil water content simulation under dry conditions which need to be better characterized in dndc brady and weil 2002 uzoma et al 2015 another possibility is that the root density determines the weighted water uptake demand over depth but the generalized root density function does not fully represent a perennial stand that builds a dense rooting structure as stand age increases further adding to this issue is that dndc only simulates detailed soil water dynamics to 50 cm with an additional deep water pool for the 50 100 cm depth this can limit the potential water uptake for deep rooted crops such as alfalfa which might explain the rapid deviation in simulated water content near the surface layers relative to observations we suggest the model be improved in future development by extending the profile depth and including an alfalfa specific root density growth function 3 4 climate change impacts on alfalfa yield and winterkill the simulated annual alfalfa dm yields under four future climate scenarios increased markedly relative to the 1971 2000 baseline periods at all locations fig 6 table 5 when simulating a moderately cold tolerant cultivar with a maximum temperature of cold tolerance ctmx of 15 c under four climate scenarios the average annual alfalfa dm yields across the seeding year and the first two post seeding years increased by 45 0 83 6 at elora 57 7 84 9 at ottawa and by 66 9 119 4 at quebec city compared to the baseline period of 1971 2000 table 5 when a more cold tolerant cultivar ctmx 20 c was simulated the annual alfalfa dm yield notably increased especially for ottawa and quebec city colder conditions compared to a moderately cold tolerant cultivar as expected the baseline annual alfalfa dm yield significantly decreased at all three locations due to winterkill damage especially in the second post seeding year when using a more sensitive cultivar ctmx 10 c less hardiness fig 6 however the average annual dm yields under both future climate scenarios significantly increased compared to the baseline being attributed to the longer growing season with earlier planting spring regrowth more cuts and more precipitation table 5 the average planting dates under future climate scenarios were 10 18 9 15 and 6 13 days earlier than the baseline scenarios for elora ottawa and quebec city respectively for baseline scenarios the average cut numbers for the seeding year and post seeding year were 1 6 and 2 7 in elora 1 9 and 2 9 in ottawa and 1 3 and 2 4 in quebec city respectively in contrast the average cut numbers increased up to 2 2 3 1 and 3 5 4 1 in elora 2 5 3 1 and 3 8 4 2 in ottawa and 2 1 2 7 and 3 1 4 0 in quebec city for the seeding year and post seeding year cuts depending on the future climate scenarios table 5 our results were consistent with thivierge et al 2016 and jing et al 2014 who indicated that the forage crops yields including alfalfa and timothy would increase under future climate scenarios with adaptations more cuts earlier planting spring regrowth but the yield increases under future climate scenarios were higher in our study probably due to the reduced crop water stress by increasing water use efficiency under elevated co2 and more precipitation this is also reported in studies for other crops which indicated that crop water stress might be reduced in ontario under future climate smith et al 2013 he et al 2018 winterkill damage had significant impacts on alfalfa yield under baseline and future climate scenarios fig 7 table 5 the winterkill effects under baseline scenarios could range from 0 to 23 3 in elora 2 8 50 9 in ottawa and 0 35 0 in quebec city ctmx 20 to 10 c table 5 the modelled impacts of winterkill on alfalfa yields under future climate change were marginally reduced at elora which has warmer winters than the other two locations the impacts of winterkill at ottawa and quebec city remained consistently high under conditions with increased temperature fig 7 note that the impact of winterkill tended to be lower under rcp4 5 1941 2070 scenarios than the baseline condition this was because the regional climate model canrcm4 estimated a reduced number of cold days below ctmx in this period yet the snow cover was not greatly impacted although there is less extreme cold temperature events under rcp4 5 2071 2100 rcp8 5 2041 2070 and rcp8 5 2071 2100 there is still high projected winterkill injury on alfalfa which was caused by two factors 1 the warmer fall temperatures resulted in loss of hardening and 2 there was less insulation from snow cover resulting in the crown temperature dropping below the cold tolerance threshold ctmx even though the average air temperature was higher bélanger et al 2002 thus it is projected that winter hardy cultivars of alfalfa will continue to be necessary under future climate conditions 3 5 caveats and future improvements there are uncertainties in projecting the impacts of climate change on alfalfa production due to several factors including the inherent structural uncertainty in crop and climate models uncertainty associated with model calibration parameterization and evaluation as well as the selected socio economic emission scenarios asseng et al 2013 whitfield et al 2013 in this study effort was placed on evaluating the improved dndc model at locations of contrasting climate where the site data was not employed for development the model performed well in simulating alfalfa biomass during growth cycles and alfalfa dm yield at normal harvest times however we expect that further calibration would be necessary under warmer climates where different cultivars are employed in this case calibration of alfalfa parameters such as gdd plant biomass fractions biomass c n ratios optimum temperature hardening and dehardening rates may be needed if field measurements of alfalfa biomass over time are available then the growth curves describing phenology can also be adjusted for other crop types simulated in warmer regions and where the model has performed well relative to detailed physiological crop models we have found that this is seldom necessary ehrhardt et al 2018 smith et al 2019 the dndc model generally provides a better estimate of overall soil water storage than it does of water content in specific soil layers root growth down the soil profile is simulated to be linear hence water uptake near the soil surface may be underestimated whereas it may be over estimated deeper in the profile smith et al 2019 this could particularly be true as the alfalfa stand matures as it stands a mass balance of water is considered and general water deficit stress on crop production is simulated but the model could benefit from the inclusion of root density functions to improve soil water simulation at specific depths further dndc does include rainfall interception by crop canopies however interception may be underestimated uzoma et al 2015 additionally the dndc model uses a cascade water flow approach which is common in many agricultural models where each soil layer tips to field capacity every hour this can sometimes result in the underestimation of soil water contents when observations are above field capacity the uncertainty associated with canrcm4 generated climate scenarios could be large as noted in comparisons between global climate models or regional climate models means et al 2012 scinocca et al 2016 however this could be tested in the future by driving the dndc model with a range of future climate scenarios from various climate models the alfalfa growth can be affected when the hardiness increases as ctmx decreases hardening could result in decreased water potential in tissues and conformational changes in either membrane and or proteins and or an aba hormone receptor which can in turn result in the regulation of genes involved in cold acclimation a strong association exists between the degree of vernalization and the degree of cold tolerance that can be controlled by major genes related to freezing tolerance and plant growth sãulescu and braun 2001 thus the objective of breeding is to develop cultivars with the minimum cold hardiness necessary for avoiding winterkill and not to affect plant yield due to delayed spring growth or small cells the model currently does not include this function which can be developed in the future providing measurements are available in addition some soil parameters were assumed to remain constant in the future but certain soil physical properties e g hydraulic conductivity water holding capacity may change under different management practices e g fertilizer tillage rotation which could further impact winterkill effects and alfalfa yield bélanger et al 2002 castonguay et al 2006 the dndc model does simulate impacts of soil c change on water holding capacity however it does not include all positive and negative impacts on soil hydraulic properties cold tolerance is the most important factor for alfalfa winterkill which has been integrated into the dndc model but there are other factors affecting alfalfa winter survival including ice encasement soil heaving frost heaving diseases fusarium infected roots which were not captured in the dndc model due to insufficient field measurements to support model development bélanger et al 2002 2006 castonguay et al 2006 2009 thus a better expression of winterkill impacts could possibly be implemented in dndc once more experimental information is available 4 conclusions the dndc model was modified to improve alfalfa growth simulation using measured biomass and yield data in eastern canada the results demonstrated that the improved model with enhanced alfalfa growth algorithms showed statistically better performance compared to the default model in most biophysical variables the simulated soil temperature had good agreements with the measured values with better simulation for improved dndc in addition the dndc model predicted soil water contents well in the seeding year but it only showed poor to fair simulations in soil water contents in the alfalfa post seeding years likely attributed to root development uptake inaccuracies compared to the 1971 2000 baseline scenarios the average annual yields increased significantly under all future climate scenarios which was attributed to the longer growing season with earlier planting more cuts and increased water use efficiency lower water stress under elevated co2 winterkill had notable effects on alfalfa production under both baseline and future scenarios depending on the cultivar cold tolerances thus it is still necessary to develop winter hardy cultivars for future adaptation additionally more winterkill factors e g ice encasement soil heaving need to be monitored in the field experiments so that their effects can be implemented in mechanistic models to enable more accurate evaluation of winterkill effects under future climate declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors acknowledge the financial support of science and technology branch of agriculture and agri food canada we also gratefully thank ray desjardins for guidance and devon worth for the technical support appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104540 
26119,this article introduces an agent based modeling laboratory for investigating how evolving hazard information propagated through forecaster media public official and peer information networks affects patterns of public protective action decisions during hurricane threats the model called chime abm provides a platform for integrating atmospheric science social science and computer and information science knowledge and data to explore the complex socio ecological dynamics of modern hazard information and decision systems from a new perspective first the model s interdisciplinary conceptualization and implementation is described results are then presented from experiments demonstrating the model s behaviors and comparing patterns of evacuation decisions when key agent parameters and the geographical population distribution forecast skill and storm are varied the article illustrates how this type of theoretically and empirically informed digital laboratory can be used to develop new insights into the interactions among environmental hazards information flow protective decisions and societal outcomes keywords hurricanes evacuation decisions agent based modeling natural hazards information networks risk communication 1 introduction as hurricanes such as harvey irma maria florence and michael demonstrated during the 2017 and 2018 atlantic hurricane seasons improving hazard risk communication and decision making is critical for scientists and society nasem 2017a b nws 2019 most research on societal information flow and decision making for environmental risks is observational using data from surveys interviews and other empirical methods in the context of hurricanes tropical cyclones such research has developed a broad base of empirical understanding about how at risk members of the public access and use risk information and make protective decisions see e g reviews in baker 1991 dash and gladwin 2007 lazo et al 2015 huang et al 2016a however because this work typically focuses on specific populations or situations findings can vary widely across studies e g huang et al 2016a and it is difficult to identify broader spatial and temporal patterns many of these existing studies also utilize data at the individual or household levels which limits their ability to elucidate how the socially interactive processes underlying hazard risk communication and response scale up to influence outcomes e g drabek 1999 dash and gladwin 2007 taylor et al 2009 moreover in today s world forecast and warning information and its communication evolve rapidly as a hazard approaches in conjunction with interacting individuals information behaviors risk perceptions and decisions lee et al 2009 morss and hayden 2010 sherman morris et al 2011 morss et al 2017 bica et al 2019 collecting the empirical data needed to analyze and understand these intersecting social spatial and temporal dynamics is challenging meyer et al 2014 morss et al 2017 demuth et al 2018 computational modeling provides an opportunity to investigate hazard information flow and decisions using a fundamentally different approach one that complements and builds on empirical studies as discussed in morss et al 2017 weather prediction communication and decision making are interconnected components of a dynamic coupled natural human system when hazardous weather threatens multiple types of actors interact to create communicate interpret and use forecasts warnings and other risk information gladwin et al 2007 demuth et al 2012 morss et al 2015 bostrom et al 2016 bica et al 2019 within this system the risk information available and associated uncertainties change across space and through time further in the modern hyper connected environment information can be widely exchanged almost instantaneously little is known about how new hazard information propagates and influences protective decisions agent based modeling provides a useful toolkit for exploring the dynamics of this type of system this article introduces a new agent based modeling platform for investigating relationships among environmental hazards hazard information information flow and patterns in people s protective decisions the communicating hazard information in the modern environment chime abm was developed through collaboration among physical scientists social scientists computer and information scientists and agent based modelers as part of a larger multi method project studying the dynamic interconnected processes that characterize the modern hazard prediction communication and decision making system morss et al 2017 chime abm was created to provide a platform for integrating knowledge and data across disciplines to build understanding about the dynamical system of interest connecting concurrent geophysical predictive modeling and empirical social and information science research e g anderson et al 2016 bica et al 2019 demuth et al 2018 fossell et al 2017 kogan et al 2015 kogan and palen 2018 morss et al 2017 wilenski 1999 in doing so we aimed to develop an interdisciplinary digital laboratory for running controlled experiments with different configurations of hazard information and social interactions under various dynamic scenarios see e g waldrop 2017 verburg et al 2016 rovere et al 2016 magliocca and ellis 2016 the modeling platform was designed to represent key aspects of the real world system of interest given our research goals while remaining sufficiently simple to allow meaningful exploration and knowledge building sun et al 2016 buchmann et al 2016 allison et al 2018 it is currently implemented for a hurricane threatening the us coastline over a five day period but it was designed to represent general features of weather hazard forecasting warning and response in order to enable a variety of future extensions a core component of chime abm is a model of hazard information flow and protective decisions with heterogeneous agents who interact via peer and media networks the agent types represent five types of key actors in hazard forecast and warning systems forecasters public officials media broadcasters other media aggregators and communicators and citizens with varying individual characteristics the agents interact with each other and with a virtual world with ocean and land areas a hurricane that moves across the landscape and forecast and other risk information that evolves as the storm approaches although agent based modeling has previously been used in hazards research the research discussed here differs from this previous work in several ways one body of related work uses agent based modeling to study evacuation planning for hurricanes e g chen et al 2006 zhang et al 2009 yin et al 2014 ukkusuri et al 2017 or other hazards e g dawson et al 2011 wang et al 2016 bernardini et al 2017 such work focuses primarily on how collective evacuation behaviors interact with built infrastructure in order to explore issues such as traffic demand evacuation routing and strategies for improving evacuation effectiveness the emphasis of the agent based modeling in these studies is therefore not on who decides to take protective action when and why but on how people and vehicles move and interact after they decide to evacuate although evacuation logistics are important they are not examined here in chime abm v1 version 1 the agents do not physically move instead we focus on how inter agent dynamics and human information environment interactions influence patterns in evacuation decisions other studies have used agent based models to examine how hazard information diffusion or protective decision making is influenced by a population s characteristics such as social networks or inter individual heterogeneity e g widener et al 2013 rand et al 2015 dixon et al 2017 again the research presented in this article has a complementary but different emphasis although our work also focuses primarily on members of the public we design and utilize a model with multiple types of interacting agents representing key roles in the warning communication and response system in addition we add a new type of dynamical interaction as a central theme by modeling different types of evolving hazard information especially forecast information and associated uncertainty interacting with the dynamical human behaviors simulated by an agent based model another previous application of agent based modeling for hazards investigates hazard risk management decisions on longer multi year time scales e g haer et al 2016 reilly et al 2017 tonn and guikema 2018 the modeling presented here expands on this type of work and much of the agent based modeling work on human environment interactions e g parker et al 2003 boone et al 2011 an 2012 rounsevell et al 2012 filatova et al 2013 barton et al 2016 groeneveld et al 2017 schulze et al 2017 by examining interactions on much shorter time scales where different types of information and decisions are important the research presented in this article focuses on the time scale of a single hurricane threat although chime abm could be adapted to investigate social learning and other longer term issues building on and extending previous related work here we describe the conceptualization and implementation of a new agent based modeling laboratory for investigating how interactions among geospatially distributed heterogeneous agents influence and are influenced by hazard information flow decisions and outcomes we then present results from a set of experiments illustrating how large scale patterns of hazard risk management decisions can emerge from the decisions of many simplified heterogeneous agents as they interact with each other and with their evolving physical and informational environment for the initial model development and research shown here we used a case study approach performing simulations for the state of florida us and two hurricanes that previously made landfall in the state the model was designed to be flexible however and thus the modeling framework can readily be modified to study other regions agent configurations hazards and hazard information section 2 provides an overview of chime abm v1 and its key components section 3 describes the experimental design and implementation and the analysis of the experimental output section 4 presents results beginning with the spatial and temporal patterns in evacuation decisions exhibited by the model it then investigates the sensitivity of the model s behavior to changes in three key parameter sets the citizen agents weighting of different types of information the timing of public official agents evacuation orders and the geographic distribution of the citizen agent population and compares results for forecasts with two different levels of skill for two different hurricanes section 5 summarizes key results and discusses implications of this work for hurricane risk communication and future research 2 chime abm conceptual model and implementation chime abm was developed and all reported experiments were run in the netlogo 5 3 modeling environment wilenski 1999 see fig 1 this section describes the major components of the model and key elements of their design for further details the commented code an odd specification a formal detailed model description and supporting input files are available for download at the comses model library https www comses net codebases 5504 releases 1 4 0 1 1 version 1 4 0 is a netlogo 6 0 2 version of the code which has been peer reviewed through the comses network this version of the code also includes supporting files for model simulations in the texas region of the us gulf coast tested for hurricane harvey in 2017 chime abm v1 includes a spatially explicit modeled world representing a geographical area of interest described in section 2 1 a dynamic hazard i e a hurricane in this implementation that moves through that world section 2 2 evolving forecast information about that hazard section 2 3 and a multi agent model in which different types of hazard related information are accessed and interpreted by different agents circulated among them and used to assess risk and make protective decisions section 2 4 these components are conceptually and numerically interconnected as shown in fig 2 the simulations shown here use a one hour time step and run for 120 time steps representing 5 days the time step geography and representations of the hazard forecasts and agents in the current implementation of the model were designed to simulate decision making in response to a tropical cyclone threat however the model was designed to represent fundamental features of hazard information and decision systems and thus to be adaptable to other geophysical hazards to design and implement the model we integrated expertise in agent based modeling with knowledge and data from research and operational meteorology emergency management risk communication information science social vulnerabilities and protective decision making as in any modeling effort of this type many aspects of chime abm v1 are abstracted or simplified from the real world and some real world features and processes are not represented decisions about what to include in the model and how to represent it were based on our research questions cross disciplinary discussions among our research team interactions with the larger research project discussed in morss et al 2017 and review of relevant literature elements of the model version presented here could readily be modified or expanded to address additional research questions and several features of the model were designed to facilitate such future work 2 1 modeled world the modeled world is a cellular representation of a geographic area of interest which can be real or imagined and includes land and ocean surface in the simulations shown here the world is the state of florida us and surrounding oceans derived from a gis digital elevation model dem in esri ascii raster format with a spatial resolution of 0 5 km we used a realistic rather than abstract geographical domain in order to provide a starting point for more complex future experiments we selected florida for initial model development and testing because it is an area of the mainland us that is highly susceptible to hurricanes although it had not experienced a major hurricane landfall in several years when we began our study to create the modeled world used here the dem is imported into netlogo and interpolated to model environment cells with a spatial resolution of 5 91 km 3 67 mi elevation is not used in the current version of the model algorithms but is included for potential use in future experiments e g for estimating a cell s level of flood risk data for florida counties county seats and population density derived from us census data for the year 2000 are also imported into netlogo and applied to the model cells in chime abm v1 none of the cellular landscape characteristics is dynamic nor are features of the built environment such as buildings or roads represented the modeled world does include hazard zones representing geographical areas at different levels of risk that abstractly simulate hurricane evacuation zones in the simulations reported in this article a single evacuation zone is defined as all locations within 1 5 cells approximately 9 km from the coast as with other components of the model these aspects of the modeled world can be revised in future experiments 2 2 modeled storm the chime abm v1 modeled world also includes a simulated tropical cyclone that approaches and then potentially affects the model domain the storm can be real or synthetic here we simulate historical storms using the us national hurricane center s nhc s tropical cyclone best track data https www nhc noaa gov data hurdat which include a sequence of locations of the storm s eye and other characteristics table 1 future versions of the model could include more complex representations of the storm and associated hazards and impacts e g areas experiencing storm surge or inland flooding transportation disruptions or power outages which could then also influence information flow and decision making in this article we perform simulations for hurricanes charley and wilma which made landfall in florida in 2004 and 2005 respectively these were selected to represent different types of storms e g charley had a small wind field for a tropical cyclone and wilma a large wind field with different tracks and forecast errors 2 3 forecast information along with an evolving storm chime abm simulates forecasts of the future state of the storm which again can be real or synthetic in chime abm v1 the information in these forecasts table 1 is based on the format of the forecasts provided in nhc s tropical cyclone forecast advisory products fig 3 each forecast typically provides predictive information that is valid at 12 or 24 h intervals in the future 2 2 the difference between the forecast valid time and the issuance time is referred to as lead time for example in the nhc forecast product shown in fig 3 forecasts are provided at 9 21 33 45 and 69 hour lead times as well as 93 and 117 hour lead times not shown in the figure with new forecasts typically issued every 6 hours this means that the most current forecast information available in the model evolves with time although older forecast information may still be circulating in the information network of the multi agent model as discussed in the introduction and morss et al 2017 this is an important dynamical element of real world modern weather forecast and warning systems the experiments reported here use two types of forecast information historical and ideal the historical forecasts were simulated using data from the nhc tropical cyclone forecast advisory products that were available in real time as the storm being studied approached e g fig 3 the ideal forecasts simulate perfect forecast information available at all lead times and were generated from the model s representation of the evolution of the actual storm along with the forecasts the model also includes information about forecast uncertainty in v1 this is represented by the nhc cone of uncertainty which estimates uncertainty in the track forecast at different forecast lead times based on average errors in recent track forecasts see http www nhc noaa gov aboutcone shtml on average tropical cyclone track forecast errors decrease as a storm approaches and so the track forecast uncertainty increases with lead time a location is defined as in the cone of uncertainty if its distance from the eye of the storm is less than the track uncertainty for that forecast lead time fig 1 here we use historical track uncertainty data from 2005 obtained from the nhc note that for the experiments in this article the same cone of uncertainty is used for both historical and ideal forecast configurations future versions of the model could include more complex representations of hurricane forecast information including more detailed spatial representations of the forecast of the storm forecasts of storm related hazards and impacts e g areas at risk from storm surge or inland flooding or power outages and associated uncertainties in the multi agent model agents extract several variables from the forecast information for use in the risk assessments discussed in section 2 4 they search the forecast information to identify their anticipated closest distance to storm track smallest distance between their location and the forecasted trajectory of the storm s eye not considering the cone of uncertainty they identify the anticipated time of storm arrival the forecast time corresponding to the anticipated closest distance to storm track and they use this to calculate the anticipated time until storm arrival time until the storm s eye is anticipated to arrive at their location they also extract the forecasted maximum sustained winds at the anticipated time of storm arrival which is referred to as the anticipated storm intensity at arrival 2 4 multi agent model of hazard information flow and decision making the multi agent model of hazard information flow and decision making was designed to represent key elements of modern us weather forecast warning and response systems specifically for hurricanes e g gladwin et al 2007 demuth et al 2012 bostrom et al 2016 with features that are sufficiently general that the model could readily be adapted for other types of hazardous weather e g parker and fordham 1996 brotzge and donner 2013 morss et al 2015 it includes five types or breeds in netlogo of agents weather forecasters who initiate forecast information media broadcasters who adapt and communicate information other media aggregators and communicators public officials who provide protective action recommendations and citizens members of the public who collect and share information assess risks and make protective decisions given the goals of our research the citizen agents and their decision making processes are more complex than the other agents the other agent breeds are purposefully simple in chime abm v1 but their roles can be expanded in the future an overview of each of the agent breeds is provided in table 2 and each is described further below more detailed descriptions of the agents rules and algorithms can be found in the supporting documentation for the model 2 4 1 forecaster agent forecaster agents function as weather forecasters modeled after e g the us national weather service s nhc who provide forecast information about the hazard in chime abm v1 there is one forecaster and its only job is to publish new forecast information including uncertainty estimates as it is updated for use by other agents tables 2 and 3 in the simulations shown here the forecaster runs every time step although the forecaster has no need for a physical presence in the current implementation it is placed at a random location in the model s populated geographic domain represented by a small green circle e g in fig 1 forecaster agents were given a location to provide a starting point for potential future versions of the model in which different forecasters provide forecast information for different regions as in the real world 2 4 2 broadcaster agents broadcaster agents simulate the role of traditional media such as television in communicating hazard information to a broad audience demuth et al 2012 in chime abm v1 their primary role is to convey forecast information to the public official and citizen agents without changing the content of the information broadcasters reorganize the forecasts published by the forecaster into a format that can be more readily used by the public official and citizen agents tables 2 and 3 they also linearly interpolate the forecasts from the temporal resolution available in the nhc provided information e g 12 or 24 h time steps as shown in fig 3 to 1 h time steps which facilitates interpretation by public official and citizen agents in the simulations shown here there are 10 broadcasters and each broadcaster runs at each time step although broadcasters have no need for a physical presence in the current implementation they are placed at random locations in the populated domain represented by small yellow circles broadcasters were given locations for potential use in future versions e g to represent major media markets in order to simplify possible influences on evacuation patterns for the experiments reported here the broadcasters were constrained not to modify the forecast content nor to potentially introduce delays in providing updated forecast information to other agents this can also be modified in future experiments 2 4 3 information aggregator agents aggregator agents are intended to simulate the roles of new media information actors who access process and redistribute hazard information e g on the internet or mobile devices these agents were included in the model to provide the ability to represent the ways in which many people currently obtain and combine information from multiple types of sources a key feature of the modern information environment that we aim to explore dow and cutter 2000 gladwin et al 2007 morss et al 2017 in the simulations shown here aggregators function identically to broadcasters with two exceptions they do not run at every time step table 2 and they do not provide information to public official agents the random activation was designed to simulate internet based sources who may aggregate and communicate information intermittently compared to traditional media actors who tend to communicate on a more regular schedule in future versions of chime abm aggregators can be revised to play more complex information roles similar to those of real world internet based information sources and experiments can be run to investigate the influence of these different types of sources creating and conveying information in different ways the simulations shown here have 10 aggregators placed at random locations in the populated model domain and depicted as small pink circles as with the broadcasters aggregators were given a physical location to allow agents to have location based preferences for aggregator information sources in future experiments but this location does not influence the model s behavior in the current version 2 4 4 public official agents public official agents also referred to as officials simulate government personnel who help protect the public and inform people about protective actions demuth et al 2012 in v1 officials only role is to decide whether to issue evacuation orders which are conveyed to citizen agents for use in their risk assessment section 2 4 5 for the simulations shown here one official is located in each county at the county seat and only officials located in coastal counties those exposed directly to the ocean can issue evacuation orders this is modeled after real world hurricane evacuation orders which are typically issued for areas at risk of inundation from storm surge i e areas near the coast as a simulation evolves the modeled officials decide whether and when to issue evacuation orders based on their assessment of the risk that the hurricane poses to coastal locations in their county using updated forecast information obtained from broadcasters at each time step officials assess risk by obtaining and processing forecast information and comparing it to three global parameters whose values are set at initialization earliest latest and wind threshold table 3 at each coastal cell three criteria are evaluated 1 is the anticipated closest distance to storm track within the cone of uncertainty in other words less than the track forecast uncertainty corresponding to that lead time 2 is the anticipated time until storm arrival within the lead time window defined by earliest and latest and 3 is the anticipated storm intensity at arrival greater than wind threshold if all three criteria are met at any of its coastal cells an official will decide to issue an evacuation order set orders from 0 to 1 which becomes active at that time step and remains active for the remainder of the simulation the track cone of uncertainty is used here as a proxy for areas that warrant evacuation orders because at lead times of more than a day or two location specific storm surge predictions have low skill fossell et al 2017 thus the storm track and associated uncertainty is a reasonable first order approximation of coastal areas at risk of significant impacts future versions of the model could include more complex estimates of storm surge risk at different locations based e g on more complex representations of coastal geography and topography and or translation of the atmospheric hurricane forecasts into forecasts of surge inundation in all of the simulations shown here latest is set to 0 hours which means that officials can issue evacuation orders up until the storm s eye arrives at its county s coastline the values of wind threshold used in these simulations are equivalent to high category 2 low category 4 winds on the saffir simpson hurricane winds scale https www nhc noaa gov aboutsshws php officials are depicted in fig 1 as small red stars which turn white if that official issues an evacuation order as with the broadcasters and aggregators we chose to make the officials behaviors relatively simple here to simplify interpreting the results of these initial experiments the officials algorithms could be made more complex or their roles expanded in the future 2 4 5 citizen agents citizen agents cit ags model members of the public or public households who dynamically collect and process information about a potential hazard assess the risk posed by the hazard and decide whether the risk is sufficient to warrant changing their behaviors fig 2 cit ags information sources include other agents connected through their social and information network described in section 2 4 6 and the model s physical environment the cit ags also serve as disseminators by relaying information to other cit ags the design of the cit ag algorithms was adapted from conceptual models of protective decision making for hazards such as the protective action decision model padm lindell and perry 2004 2012 as well as findings from empirical research by members of our project team and other scholars on information flow and protective decision making for hurricanes e g baker 1991 dow and cutter 2000 gladwin et al 2001 gladwin et al 2007 dash and gladwin 2007 morss and hayden 2010 lazo et al 2015 huang et al 2016a morss et al 2016a demuth et al 2016 cuite et al 2017 bostrom et al 2018 demuth et al 2018 and references therein and for other weather related hazards e g mileti and sorensen 1990 sorensen 2000 brotzge and donner 2013 ruin et al 2014 lazrus et al 2016 morss et al 2016b one challenge of this project was to translate parsimonious theoretical models such as that provided by padm and the more detailed but often incomplete information available from empirical analyses into simple yet sufficiently specific instructions for agents to do so we synthesized the relevant literature to identify key behavioral features to implement given our research goals informed by the cross disciplinary expertise within our research team existing research indicates that in general people evacuate when they believe that an approaching hurricane poses a risk to their own or their family s safety and that different people can both perceive risk differently and have different evacuation barriers or constraints e g baker 1991 gladwin et al 2001 dash and gladwin 2007 lazo et al 2015 thus we formulated the cit ag module in terms of combining and translating information obtained from multiple sources into a risk assessment which is then compared with decision thresholds that vary across the cit ag population at model initialization chime abm v1 has two options for geographically distributing cit ags random and realistic the realistic population simulations distribute cit ags according to the real world population density based on census data here from the year 2000 along with location each cit ag is assigned multiple variables that influence its hazard information collection information processing risk assessment and decisions table 4 these variables were designed to capture the real world heterogeneity among members of the public in factors such as interest in and access to hazard information social and information connectedness trust in information sources risk perceptions and interest in and capacity for evacuating and taking other protective actions values for these variables are assigned individually to each cit ag at initialization and do not change during a simulation except for the variable that controls scheduling when the cit ag is active feedback1 this parameter can change if during an active time step a cit ag s risk assessment is sufficiently high or low that it decides to increase or decrease its information collection frequency cit ags check for information on average every 12 hours towards the beginning of a simulation until they assess that they may be at risk at which point they begin to run their algorithms more frequently but never more often than the hourly time step of the model if they assess that they are at low risk they increase the interval between active time steps this dynamic scheduling for individual cit ags was designed to represent the ways that real people may become more or less attuned to their physical and informational environment as a hazard threat evolves mileti and sorensen 1990 lee et al 2009 sherman morris et al 2011 lindell and perry 2012 morss et al 2017 demuth et al 2018 at each time step when a cit ag is active it executes four algorithms 1 collect hazard related information 2 sort and process that information 3 use the information to assess risk 4 decide whether to change its information collection frequency evacuate or take other e g property protective action fig 2 an overview of each of these algorithms is below additional details can be found in the supporting documentation a cit ag collects information by querying a the official agent in its network about whether an evacuation order has been issued b its environment about whether its location is currently experiencing winds of 34 knots or greater the minimum threshold for a tropical storm c broadcasters and aggregators in its media network for their forecast information d other cit ags in its peer network for their forecast information 3 3 cit ags make information available to other cit ags at all time steps not only when they are active and e its memory for its own most recent forecast interpretation the first four a d represent several of the major external sources of information that members of the public use in hurricane risk assessments and decision making evacuation orders physical environmental cues and forecasts from professional and social sources e g dow and cutter 1998 dash and gladwin 2007 morss and hayden 2010 petrolia and bhattacharjee 2010 lindell and perry 2012 demuth et al 2018 the last e represents people s tendencies to use new information such as a newly accessed forecast to update previous interpretations the cit ag then selects a random subset of the collected forecast information to process and combines that information weighted by its trust in each of the information sources to generate its own updated forecast interpretation this forecast interpretation is saved into the cit ag s memory for use the next time it seeks information and it is also now available to other linked cit ags in the social and information network next the cit ag assesses its risk table 5 it evaluates its risk based on forecast information using the equation 1 risk function h e t i m e c 2 2 σ 2 where time is time until storm arrival h is the peak height of the curve and c and σ are random variables defined in table 4 an example risk function is shown in fig 4 h is a function of the cit ag s representation of whether it lives in an evacuation zone its interpretation of the forecast and the track forecast uncertainty see full model description for details functionally h increases when the cit ag thinks that it is in an evacuation zone that the storm intensity at arrival will be higher and that it is in the cone of uncertainty or if not in the cone of uncertainty that its closest distance to storm track is greater relative to the track forecast uncertainty at time until arrival this formulation was developed based on previous and concurrent work indicating that these are important factors influencing how people interpret hurricane forecasts to evaluate risk e g dow and cutter 1998 2000 gladwin et al 2001 zhang et al 2007 dash and gladwin 2007 morss and hayden 2010 petrolia and bhattacharjee 2010 huang et al 2016a morss et al 2016a bostrom et al 2018 the value of eq 1 at the cit ag s anticipated time until storm arrival then becomes the cit ag s risk assessment based on forecast information as shown in fig 4 a cit ag assesses higher risk when h is greater and when the anticipated time until arrival is closer to c modulated by σ σ this formulation and the values of c and σ were selected to abstractly represent heterogeneous public preferences for evacuation timing with most members of the public evacuating between 12 and 72 hours in advance of landfall depending on the situation lindell et al 2005 gudishala and wilmot 2010 czajkowski 2011 wu et al 2012 huang et al 2016b 4 4 note that chime abm v1 does not simulate the fact that in the real world people tend to evacuate during daylight hours this diurnal cycle in evacuation timing would be important to add if the model were used to examine issues such as travel demand and evacuation routing the cit ag s final risk assessment risk estimate is calculated by adding the risk assessment based on forecast information a small amount of random error and factors based on evacuation orders and environmental cues if present as summarized in table 5 evacuation orders are given greater weight if the cit ag has greater trust in officials and if it believes it is in an evacuation zone e g mileti and sorensen 1990 gladwin et al 2001 cuite et al 2017 thompson et al 2017 in the current formulation the presence of environmental cues adds a constant value to the risk assessment finally the cit ag decides whether and how to modify its behaviors by comparing its final risk assessment with the risk thresholds in table 4 specifically risk estimate values greater than risk life risk property and info up trigger the cit ag to decide to evacuate take other protective action 5 5 this is modeled as a proxy for non evacuation protective actions such as boarding windows protecting other property or gathering supplies which people typically engage in at lower risk thresholds than evacuation and increase its information collection frequency decrease feedback1 respectively if risk estimate is less than info down the cit ag decreases its information collection frequency increases feedback1 if the cit ag decides to evacuate it will no longer run its risk assessment and decision algorithms at subsequent time steps however it will continue to collect and interpret forecast information according to its schedule and to make its evolving forecast interpretation available to other cit ags in its peer network at initialization each cit ag is visually depicted by a small blue circle fig 1 if a cit ag decides to take a non evacuation protective action its color changes to green and then to orange if it decides to evacuate one important simplification of the cit ags algorithms in chime abm v1 compared to the real world is that cit ags do not share or remember information other than their interpretation of the forecast another is that they do not consider social cues such as observations of others protective behaviors although these processes are known to influence people s hazard related risk assessments and behaviors e g mileti and sorensen 1990 dash and gladwin 2007 taylor et al 2009 lindell and perry 2012 demuth et al 2018 we chose not to include them in v1 because doing so would add free parameters and dynamics complicating interpretation of results from the experiments such features could be added in future model versions to explore additional system dynamics 2 4 6 citizen agent social and information network information sharing among agents in chime abm is implemented through a social and information network that connects cit ags with each other and with other agent breeds to support interpretation of other aspects of the system s dynamics the network structure used here was designed to capture some aspects of real social and information networks while also being relatively simple the network changes from simulation to simulation but in the current model formulation it is static during a simulation in addition all links between cit ags are non directional such that information can flow both ways this network formulation can be extended in complexity in the future or dynamic elements added in the experiments conducted here each cit ag selects a random sized randomly selected subset of broadcasters and aggregators to include in its network each cit ag is also connected with the geographically closest official the peer network builds connections between cit ags using two algorithms first a standard preferential attachment routine is run that creates a relatively small number of highly connected nodes in the network in other words a scale free social network this type of network is typically sparser than real social networks and real social networks often exhibit transitivity thus a second algorithm is then run which makes additional connections among cit ags to complete triads an example peer social and information network created using these algorithms is depicted in fig 5 additional detail about the network building algorithms can be found in the supporting model documentation 3 experimental methodology and data analysis 3 1 initializing the model and running simulations when chime abm v1 is initialized the inputs needed to run a simulation are loaded including map layers storm information and forecasts next the population of agents is distributed in the modeled world and the agents variables are initialized running a simulation starts the clock and triggers the storm and forecasts to evolve and every agent to run breed specific instructions according to its schedule as the model runs key variables are stored and the depiction on the model interface is updated at the end of each simulation relevant data are output for further analysis to run large numbers of simulations in parallel we used the netlogo behaviorspace tool 3 2 experimental design the experiments shown in this article begin with a set of model parameters that produced quasi realistic behaviors of interest and then systematically modify those parameters to explore key model behaviors and sensitivities because chime abm includes a number of stochastic elements it can exhibit significant run to run variability thus we ran multiple repetitions for each model configuration and aggregated results across these simulations table 6 provides an overview of the different sets of experiments reported in this article the first set of results shown in section 4 1 is from 100 simulations for hurricane charley with random geographical distribution of cit ags and ideal forecasts this configuration was selected as a starting point for the experiments to minimize the complicating effects of non uniform population distribution forecast errors and evolving forecast information when interpreting the results we ran these initial experiments with charley because unlike wilma the full diameter of charley s 34 knot winds remained over land as the storm crossed florida and so the model s land domain encompasses more of the highest impact zones discussed in section 3 3 the next two sets of experiments shown in sections 4 2 and 4 3 investigated the sensitivity of the model s behavior to key parameters in the cit ag and official agent algorithms respectively for the experiments varying the cit ags information weightings we ran 100 simulations for each combination of the information weightings shown in the second settings column in table 6 729 configurations for a total of 72 900 simulations 6 6 the values of the weightings for the different types of information are relative scales and do not have any independent meaning the maximum weightings used in the sensitivity tests were selected based on the increase in weighting of each type of information that was required to significantly affect evacuation rates for the experiments varying the timing of officials evacuation orders we shifted the earliest lead time at which officials could issue evacuation orders between 54 hours and 0 hours prior to anticipated storm arrival as shown in table 6 and ran 100 simulations for each of the 10 configurations we then examined the effects of changing the geographical distribution of cit ags from random to realistic holding all other settings constant table 6 for this set of experiments with only two model configurations we ran 1000 simulations per configuration section 4 4 finally we explored the evacuation patterns produced by the model when the ideal forecasts are changed to historical forecasts and when the storm is changed from charley to wilma section 4 5 as shown in table 6 all other settings were kept constant as in previous experiments except that wind threshold was adjusted so that each of the configurations had on average similar numbers of officials issuing evacuation orders evacuation orders are an important driver of evacuation decisions in the model and so this modification removes some of the variability between otherwise parallel scenarios while still allowing the officials to respond to forecast information 7 7 note that because the storm tracks and forecasts are different in the four configurations different sets of officials may issue evacuation orders for each of the four configurations we ran 1000 simulations 3 3 data analysis to facilitate comparing evacuation patterns quantitatively across simulations chime abm v1 tracks cit ag decisions within multiple impact zones designed as first order approximations of areas likely to experience different levels of impacts from the storm here we use six impact zones defined by whether a location a is coastal in an evacuation zone 1 5 grid cells or less from the ocean or inland and b experiences maximum storm winds during the simulation that are greater than 64 knots hurricane force between 34 and 64 knots tropical storm force or less than 34 knots the storm tracks and six impact zones for the hurricane charley and wilma simulations are shown in fig 6 the primary model output data analyzed here are the percent of cit ags in each impact zone that decided to evacuate in each simulation these were analyzed across the simulations run for each model configuration by examining statistics such as the mean and inter simulation variability we also examined officials evacuation order decisions more detailed spatial and temporal patterns in cit ags decisions and other aspects of the model s behavior and outcomes most of the results are presented in summary tables or figures to provide a compact summary of broad patterns together with graphics depicting more detailed aspects of the model s behavior to support more in depth interpretations discussed in the text 4 results 4 1 spatial and temporal patterns of cit ag evacuation decisions first we examine results from simulations with the model configuration shown in the first settings column in table 6 these results provide a first order assessment that the agents in chime abm are behaving in a structurally valid way based on the processes included in the model they also illustrate several key aspects of the model s behavior which provides a starting point for interpreting subsequent results the top row of table 7 summarizes cit ags evacuation decisions in the six impact zones averaged across the 100 simulations to illustrate spatial patterns in the model s output in greater detail fig 7 presents a map of cit ag evacuation decisions for a single randomly selected completed simulation note that random placement of the cit ags combined with charley s small size results in few cit ags within the coastal 64 knot impact zone these results show several patterns that are similar to real hurricane evacuation behaviors first cit ags evacuation rates are higher near the coasts in evacuation zones than inland they are also higher in areas closer to the storm s track this pattern arises because with the ideal forecasts in this simulation the officials in coastal counties that will experience strong winds issue evacuation orders shortly after their evacuation order window opens and cit ags in these areas receive forecasts that the storm will track near their region throughout the simulation cit ags in coastal areas are much more likely to believe they are in an evacuation zone which increases their sensitivity to both evacuation orders and forecast information thus between the evacuation orders the forecast information and the environmental cues that they experience as the storm approaches many of the cit ags in coastal 34 knot impact zones decide to evacuate as expected cit ag evacuation rates are greater in the 34 knot zones than in the lower impact 34 knot zones counterintuitively however modeled evacuation rates are greater in the 34 64 knot zones than in the higher impact 64 knot zones a more in depth investigation indicates that this occurs because the statistics presented here average across the western coast of florida where the storm makes landfall and a higher percentage cit ags evacuate and the eastern coast of florida where a lower percentage of cit ags evacuate as shown in fig 6 the 64 knot wind area expands as the storm crosses florida evacuation statistics in the 64 knot impact zones are therefore more heavily weighted towards the lower evacuation rates in eastern florida at the same time the model s land domain fully encompasses the 34 64 knot and 34 knot wind zones in the western part of florida but not in the eastern part of florida evacuation statistics in the 34 64 knot impact zones are therefore more heavily weighted towards the higher evacuation rates in western florida together this decreases evacuation rates in the 64 knot zones compared to those in the 34 64 knot zones as explained by baker 1991 evacuation rates vary from place to place in the same hurricane and from storm to storm in the same place p 291 which complicates comparing the model results with real world evacuation rates despite these limitations this general pattern evacuation rates that are highest in the highest risk areas along the coast near the storm s track and that decrease as one moves inland and away from the storm is broadly similar to that found in real world hurricane evacuations e g baker 1991 lindell et al 2005 morrow and gladwin 2005 huang et al 2012 2016b one major difference is that the evacuation rate in the model decreases much more rapidly as one moves inland than it typically does in the real world due in part to the simplified formulation of the influence of coastal proximity on risk assessments and decisions this could be modified in future versions a second pattern illustrated by table 7 and fig 7 is the inter agent variability in cit ag evacuation decisions although many cit ags in the highest impact zones decide to evacuate some do not and a small percentage of cit ags who are located in low risk areas decide to evacuate this is consistent with real world hurricane evacuations and more generally with the heterogeneity exhibited by real world u s individuals and households in hurricane evacuation decisions e g hasan et al 2011 dixon et al 2017 in the model this variability arises from the individual cit ags different values for the randomly generated variables in table 4 for example a cit ag in a high risk area may decide not to evacuate because it has a very high value of the risk threshold risk life or because it has very low values of the trust authority parameter or erroneously thinks it is not in an evacuation zone in these simulations none of the agents misinterpret the forecasts and cit ags do not consider evacuation orders from distant officials thus cit ags who evacuate from very low risk areas such as the one represented by the white dot in northern florida in fig 7 do so primarily because they have low values of the risk threshold for evacuation although preferences for evacuating early when the cone of uncertainty covers a larger area and other factors can play a role variability in cit ag behaviors can also result from more complex interactions among components of the model s dynamics for example a cit ag in a high risk area may not evacuate because the times at which it collects information and thus receives information indicating that it is at high risk do not coincide with the timing of peaks c in its risk function curve this is more likely if a cit ag collects information infrequently feedback1 is large and or info up and info down are high and has a narrow risk function curve σ is small when it runs the risk assessment algorithm to illustrate temporal patterns in the model s behavior fig 8 depicts the timing of cit ag evacuation decisions in each of the six impact zones averaged across the 100 simulations these results show two peaks in evacuation timing one at 30 54 hours before anticipated storm arrival and a smaller bump at 0 6 hours before arrival a small percentage of cit ags decide to evacuate prior to the issuance of evacuation orders prior to approximately 54 hours based on their interpretations of forecast information this then leads into the first peak in evacuation decisions which results from cit ags use of evacuation orders combined with forecast information the second peak in evacuation decisions occurs as the storm approaches close enough to provide cit ags with environmental cues along with evacuation orders and forecasts the first peak in evacuation timing is much larger in coastal impact zones than inland because cit ags in coastal zones are more likely to both receive evacuation orders and believe they live in an evacuation zone the latter of which makes them more sensitive to both forecasts and evacuation orders the second peak occurs only in the 64 knot and 34 64 knot impact zones because only cit ags in those impact zones receive environmental cues the variability in when cit ags decide to evacuate arises for reasons similar to those discussed above these include differences in cit ags scheduling and the timing of their risk function peak as well more complex interactions such as a higher risk life threshold that leads some cit ags to need to accumulate more information indicating that they are at high risk before they decide to evacuate 4 2 varying citizen agents weighting of different types of information building on the results in section 4 1 next we investigate the effects of modifying cit ags weightings of the three main types of hazard information they use to assess risk in chime abm v1 forecast information evacuation orders and environmental cues table 6 these results further elucidate key aspects of the modeled system s dynamics and they provide additional insight into the roles of different types of information in cig ags risk assessments and decisions table 7 summarizes cit ags evacuation decisions in the six impact zones for a subset of the perturbed information weightings to examine aspects of these results in greater depth fig 9 depicts evacuation rates and peak evacuation timing for cit ags in the highest impact zone across all 729 model configurations and fig 10 compares more detailed evacuation timing results for the highest impact zone for several of the information weightings the results in table 7 and the overall pattern of symbol sizes in fig 9 indicates that in the model s current formulation the cit ags risk assessments and decisions are more sensitive to forecast information than they are to evacuation orders and environmental cues for example when cit ags use only forecast information at its standard weighting forecasts only row in table 7 approximately 16 of cit ags in coastal impact zones evacuate in contrast when cit ags use only evacuation orders and environmental cues at their standard weightings alone or together very few cit ags evacuate in the absence of forecast information table 7 and fig 9 indicate that increasing the weighting of evacuation orders and environmental cues by a factor of 4 or more is needed to motivate a substantial percentage of high impact cit ags to evacuate aspects of these model behaviors can be modified by changing the information weightings however cit ags evacuation decisions are also more sensitive to forecast information due to features of the different types of information and the model s formulation first forecast information is available to all cit ags throughout the simulation while evacuation orders and environmental cues only become available to a subset of cit ags those whose closest official issues evacuation orders or who experience 34 knot winds respectively later in the simulation second forecast information is more influential because cit ags can modify their information behaviors in response to the hazard risk more specifically when cit ags receive forecast information that begins to signal risk they may decide to collect information and assess risk more frequently they are then more likely to obtain information from evacuation orders and or environmental cues soon after it becomes available at times that are closer to the peak of their risk function without forecast information playing this role many cit ags wait 12 or more hours between active times which may lead to significant delays before they obtain and process evacuation orders and environmental cues in this way the forecast information in the model helps cit ags become more attuned to the risk and access risk information more frequently by the time evacuation orders are issued and environmental cues are felt increasing their likelihood of evacuating similar behaviors occur in the real world in which risk information available at earlier stages of a hazard threat primes people to obtain and understand subsequent information e g mileti and sorensen 1990 dash and gladwin 2007 morss et al 2017 demuth et al 2018 however relatively little is known about how these types of behaviors influence system level patterns this illustrates how modeling laboratories such as chime abm v1 can provide a toolkit for studying such interactions in a simplified context to build understanding that can be used to interpret real world information and decision dynamics morss et al 2017 the results in table 7 also illustrate how the influence of different types of information in chime abm v1 varies spatially based on how the spatial attributes of the information intersect with the model algorithms as indicated by the forecasts only and orders only maximum weighting rows both forecasts and evacuation orders affect cit ag evacuations in all six impact zones however their influence is much larger in coastal zones where cit ags are more likely to receive evacuation orders and believe they are in an evacuation zone within coastal areas forecasts and evacuation orders have less influence farther from the storm s track in the 34 knot zone 8 8 as discussed in section 4 1 evacuation rates are higher in the 34 64 knot zones than in the 64 knot zones because of the asymmetric evolving nature of the storm which leads to an asymmetric distribution of these zones across the populated model domain fig 6 combined with the limited model domain and the averaging of evacuation rates across the west and east portions of florida as indicated by the env cues only maximum weighting row on the other hand environmental cues only affect evacuations in the 34 64 knot and 64 knot zones they have no effect in the 34 knot zones where cit ags do not receive any environmental cues and they are most influential in the 64 knot zones where winds are typically greater than 34 knots for a longer period of time than in the 34 64 knot zones unlike forecasts and evacuation orders environmental cues have similar effects on evacuations in coastal and inland zones the effects of varying the cit ag information weightings on the timing of cit ag evacuations is depicted by the colors of the symbols in fig 9 and the timing histograms in fig 10 when cig ags weight forecast information highly right side of fig 9 second plot from left in fig 10 evacuations peak between approximately 78 30 hours before storm arrival when evacuation orders are cit ags primary source of risk information and weighted highly third plot from left in fig 10 the evacuation peak shifts to 24 48 hours before arrival this occurs because cit ags have no information available to signal risk until after officials evacuation order window opens and because cit ags are not primed by the forecasts to quickly obtain and assess this risk information when environmental cues are cit ags primary information source top left of fig 9 right plot in fig 10 evacuations peak only a few hours before the storm s arrival because this is when environmental cues manifest and again because cit ags have not been primed by earlier information these results further illustrate the important role of forecasts and evacuation orders for motivating timely protective behaviors in the modeled system as in the real world 4 3 varying potential timing of evacuation orders issued by public officials now we investigate the effects of modifying a key component of the public official agent algorithms the timing of their evacuation orders table 6 these experiments build on the results examined in sections 4 1 and 4 2 related to the model s behavior and the roles of different types of information in the system they also begin to explore interactions between the evolving forecast uncertainty and the dynamics within the multi agent model the results are summarized in table 8 which presents cit ag evacuation rates in the six impact zones for the full set of experiments to examine aspects of these results in greater detail fig 11 depicts the timing of officials evacuation orders black dots and cit ag evacuation decisions histograms for a subset of the experiments the results for the officials evacuation orders in fig 11 show that as expected given the ideal forecasts and the model s formulation shifting the opening of officials evacuation order window later closer to storm arrival leads to later issuance of evacuation orders it also typically leads to fewer officials issuing evacuation orders this occurs primarily because as the time until storm arrival decreases the track forecast uncertainty decreases which means that fewer coastal counties intersect with the forecast cone of uncertainty fewer officials therefore decide that evacuation orders are needed the results in table 8 show that shifting the potential timing of evacuation orders later also leads to fewer cit ags deciding to evacuate this effect is most prominent in the coastal zones where as discussed in section 4 2 evacuation orders have the largest influence on evacuations fig 11 indicates that as one would expect shifting the timing of evacuation orders changes the timing of cit ag evacuations together table 8 and fig 11 show that shifting evacuation orders later decreases earlier evacuation decisions and shifts some but not all of those evacuations later closer to storm arrival building on section 4 2 this set of experiments further elucidates the influence of different types of information on evacuations for example for the coastal 64 knot zone results with earliest 24 hours in fig 11 3 peaks in cit ag evacuation timing are evident at 30 54 hours 18 24 hours and 0 6 hours prior to storm arrival the first peak is due to forecast information and the latter 2 peaks correspond to the times at which evacuation orders and environmental cues become available to signal risk in addition these results depict how the model simulates the tradeoffs between officials issuing evacuation orders earlier when the track uncertainty is greater versus waiting until closer to the storm s arrival when the forecast uncertainty is reduced the former leads to more officials issuing evacuation orders which increases the percentage of cit ags evacuating from both high impact areas and areas that end up not experiencing significant impacts from the storm the latter leads to more geographically targeted evacuation orders but a lower percentage of cit ags evacuating from high impact areas it also leads to later evacuations giving cit ags less time to complete the evacuation process this illustrates how this type of model can be used to explore in a simplified context the effects of different communication and decision strategies by professionals during hazardous weather threats 4 4 varying geographical distribution of citizen agent populations next we relax one of the idealizations in the experiments in sections 4 1 4 3 and investigate the model s behavior when the random geographical distribution of cit ags is changed to a realistic geographical distribution table 6 we present these results to explore the effects of using a more realistic less idealized model setup and to help interpret the results from subsequent experiments with realistic population distributions fig 12 summarizes cit ag evacuation decisions for the two model configurations to examine the spatial patterns in greater detail fig 13 depicts a map of cit ag evacuation decisions for a single completed simulation with a realistic cit ag geographical distribution overall the simulations with a realistic population distribution exhibit some patterns similar to those discussed above for a random population distribution for example cit ag evacuation rates remain higher in coastal than in inland impact zones however on average a much larger percentage of cit ags decide to evacuate in the realistic population distribution simulations there is also a shift in the spatial pattern of evacuations including a decreased evacuation rate in the coastal 64 knot zone and an increased evacuation rate in the coastal 34 knot zone more in depth investigation reveals that this counter intuitive pattern arises from how the geographical distribution of florida s population intersects with the forecasts the forecast uncertainty and the storm s eventual track and impact zones for example in the realistic population distribution simulations a large number of cit ags are located in the miami area in southeast florida fig 13 this region is in the forecast cone of uncertainty for the idealized charley forecasts for much of the simulation period and so evacuation orders are issued in this area and many coastal cit ags decide to evacuate blue circles in southeast florida in fig 13 based on the storm s track and size however southeast florida ends up not experiencing 34 knot winds thus in the realistic population distribution simulations a significant number of cit ags located in evacuation zones in the miami area make decisions to evacuate that in retrospect were unnecessary in fig 12 this appears as a larger evacuation rate in the coastal 34 knot impact zone as these results illustrate the uneven distribution of population in the real world complicates analyzing spatial patterns of evacuation decisions using aggregated metrics more generally the intersection between specific hazard tracks or forecasts complex coastal geography and clustered populations can lead to decision patterns that are difficult to understand and attribute even in this simplified model world in the real world which has many additional complexities understanding patterns in protective decision making is even more challenging this further indicates the potential value of this type of modeling laboratory where different components can be simplified or modified systematically to run a suite of experiments 4 5 varying the storm and forecast skill historical and ideal forecasts for hurricanes charley and wilma finally we explore the effects of using historical rather than idealized forecasts and of modifying the storm all experiments shown in this section use a realistic rather than random geographical distribution of cit ags table 6 using realistic forecast information adds a further complexity to the model simulations by adding a new dynamical component evolving imperfect forecast information that is present in real weather forecast information and decision systems these experiments also begin to investigate scenarios of interest to stakeholders such as meteorologists or emergency managers by exploring how differences in storm characteristics and forecast information can propagate through the multi agent system and translate into different patterns in public evacuations the top panel of fig 14 shows results for hurricane charley the storm used in the experiments in section 4 1 4 4 with historical forecasts comparing these results with those for parallel simulations with ideal forecasts lower panel of fig 12 we see that the mean evacuation rate is lower in the historical forecast simulations especially in the coastal 34 knot impact zone in other words as one might expect given the less consistent risk information imperfect forecasts lead to fewer cit ags deciding to evacuate the middle and lower panels in fig 14 show results for simulations with hurricane wilma for ideal and historical forecasts like charley wilma made landfall in southwestern florida but farther south and with a more west to east track fig 6 wilma was also a much larger storm than charley and so its 64 knot and 34 64 knot winds cover a much larger portion of the model domain comparing the wilma and charley results for ideal forecasts middle panel of fig 14 and lower panel of fig 12 the overall pattern of evacuation rates is similar except that the wilma evacuation rates are much lower in the coastal 34 knot zone in other words a much lower percentage of cit ags in the coastal 34 knot zone decide to evacuate unnecessarily in the ideal forecast simulations for wilma than for charley this occurs because unlike charley wilma is a large enough storm that most of the areas that are within the cone of uncertainty several days before landfall including the miami area end up experiencing 34 knot winds thus few cit ags in the 34 knot impact zones for wilma receive information indicating that they are at high risk and decide to evacuate this illustrates how and why given similar forecast track uncertainty and decision algorithms more people are likely to make evacuation decisions that turn out to be unnecessary for a smaller hurricane fewer people end up experiencing strong winds and other impacts similar to the charley results discussed above running wilma simulations with historical forecasts produces lower evacuation rates than simulations with ideal forecasts middle and lower panels of fig 14 in fact the wilma simulations with historical forecasts produce an overall cit ag spatial evacuation pattern similar to that which hurricane forecast and evacuation professionals might wish to see with higher evacuation rates in higher impact zones as the other results presented show however this pattern is not an inherent outcome of the model s dynamics instead it is produced by how the evolving imperfect and uncertain forecasts and the storm s track and winds overlay onto the unevenly distributed population this illustrates the challenges of understanding the dynamics that lead to different outcomes in realistic hurricane situations by enabling systematically perturbed experiments in more simplified contexts this type of modeling laboratory can help build new understanding about the interactions among evolving environmental hazards hazard information information flow and protective decisions given the goals of the work presented here we did not try to adjust the model to match real world evacuation rates and robust empirical data on spatially distributed evacuation rates for these two storms is not publicly available however the available empirical data indicates that the model is to first order generating reasonable evacuation rates in coastal high impact zones for example a survey conducted by smith and mccarty 2009 after charley made landfall found that 36 of the sample in charlotte county florida the coastal county where charley made landfall with 64 knot winds reported evacuating another post storm study of charley by baker 2005 found that 22 53 of the sample in areas similar to the model s coastal 34 knot zones and 12 33 of the sample in areas similar to the model s inland 34 knot zones reported evacuating for wilma a post storm survey conducted by solis et al 2010 in 3 southeastern florida counties all of which experienced 64 knot winds found that 32 of the sample evacuated comparision with fig 14 indicates that these evacuation rates are similar to those generated by the model in our coastal 34 64 knot and 64 knot zones for historical forecasts for the 2 storms as noted in section 4 1 however the evacuation rates in inland 34 knot zones produced by the model in its current configuration are lower than those in the real world 5 summary and discussion this article conceptualizes and implements an agent based model for studying the modern hazard information and decision system in the context of hurricanes approaching the us coastline the model includes multiple types of agents who interact with each other and with their physical and informational environments to access interpret and decide how to respond to evolving hazard information in a theoretically and empirically informed way the resulting digital laboratory provides opportunities to study this complex dynamic system from a new perspective complementing recent related work using other methods e g lee et al 2009 gudishala and wilmot 2010 meyer et al 2013 2014 ruin et al 2014 morss et al 2015 2017 lazrus et al 2016 bostrom et al 2018 demuth et al 2018 we use the modeling laboratory to ask how are the spatial and temporal patterns of protective decisions during hazardous weather threats affected when heterogeneous agents with semi realistic decision rules access share and interpret evolving forecasts and other hazard information specifically we perform experiments investigating how the model s behavior and outcomes change when key agent parameters and the geographical population distribution hurricane evolution and forecast skill are varied the results provide insight into how and why evacuation patterns can arise when interacting agents exchange and respond to evolving uncertain information from different environmental and social sources they also illustrate how interactions among evolving information uncertainty and decisions can produce complex emergent dynamics for example as agents respond to information indicating decreasing uncertainty about the potential threat feedback loops can lead to rapidly increasing risk assessments as a storm approaches as the experiments further show including factors that add complexity and realism to the model such as the coastal geography of a region such as florida the impacts of an asymmetric evolving storm non uniform geographical population distributions and evolving imperfect forecasts can complicate interpreting the model output this demonstrates the value of this type of modeling laboratory for building in depth understanding about hazard information and decision dynamics by enabling systematic manipulation of factors that cannot easily be controlled in the real world it also underscores the potential of this type of interdisciplinary modeling for addressing questions of interest to forecasters emergency managers and other stakeholders as well as researchers by allowing experiments in a wide range of scenarios the research described here advances scientific capabilities and knowledge in several ways first by adapting existing theoretical models and empirical understanding of hazard information flow and decision making for use in computational agent based models the work provides a new approach for exploring how evolving hazard information and decisions interact to create broader patterns of interest second the modeling framework discussed here provides a unique toolkit for exploring the effects of different hazard forecast information including timing and uncertainty risk related decision making and information network topologies on patterns in social decisions experiments that are impossible to perform in the real world further this research demonstrates how agent based modeling can be used to study systems in which coupling with evolving environmental and social information contributes to the system dynamics along with coupling between the natural and human system in these ways this study aims to extend model based hazards research toward work with theoretically and empirically informed agent based modeling in complex dynamic information contexts based on the research reported here we propose several areas for future related work first chime abm can be used to address additional research questions related to hurricane forecasting information communication and evacuations in particular the forecast information currently represented in the model is much simpler than that typically available in the real world today thus one possible extension for future experiments is incorporating additional or more complex forecast information and or representations of forecast uncertainty this might include coupling the agent based components of the model with more complex hazard and forecast information inputs e g from numerical modeling of hurricanes and their impacts to develop a more complete coupled physical social modeling laboratory another possible extension is to simulate different interpretations of forecast information content e g by having agents modify their representations of the forecast to explore how more complex aspects of information flow and interpretation interact to influence decisions the model could also be extended to study sequences of hurricanes during one or multiple years in order to explore the dynamics of how hurricane related experiences influence attitudes and behaviors in subsequent storms or other longer term aspects of hazard risks and resilience another area for future research is revising the model structure and its components including the agent algorithms hazard information social and information networks and evaluation of impacts to address idealizations in the current version in doing so it is important to consider the potential trade offs of adding different forms of realism and complexity in the context of the research goals we designed chime abm to be capable of using quasi realistic geography populations storms and forecasts imported using real data nevertheless as the results in section 4 show abstractions and simplifications can enhance the interpretability of the model output and the value of the model in elucidating key dynamics of the system of interest the model can also be adapted to study other hazards for which the evolution of different types of information and its exchange among multiple types of actors play important roles for example floods wildfires and volcanic eruptions take place at temporal and spatial scales similar to those of hurricanes the chime modeling environment could readily be adapted to investigate decision making in these contexts tsunamis tornados and flash floods typically unfold more quickly and affect smaller regions chime abm would require more extensive modification to explore information flow and decision making for such hazards an additional potential area for future work is comparing quasi realistic model simulations with observed social data from specific historical hurricanes to refine the model structure and parameters in its current form chime abm is well suited for exploring aspects of the system s behavior by comparing results across sets of simulations qualitative comparisons with prior related research and empirical evacuation data suggest that the model produces reasonable evacuation patterns but it does not attempt to realistically represent the multitude of factors that influence real world individual and household evacuation decisions in specific situations more in depth quantitative comparisons with real world data may therefore help improve the model s capabilities to address practical questions of interest however given the limited availability of the types of comprehensive empirical data required to perform such comparisons new data sets may need to be collected or compiled real world decisions are influenced by many complicated factors that must be simplified in any modeling approach since little is known about the emergent dynamics of the type of system being studied here our aim was to begin developing fundamental understanding that could form building blocks to be expanded on in future related modeling work given these goals chime abm is purposefully abstracted from the real world in multiple ways and so it has many limitations if evaluated from the perspective of simulating actual hurricane decisions and outcomes the model was developed however using theory research findings expertise and data from several relevant disciplines the modeling effort is also intersecting with ongoing research on hurricane hazard predictability information flow and decision making being conducted as part of a larger multi method research project morss et al 2017 thus the model is both informed by and feeds back into empirical research interpreted in conjunction with other work investigating real world hazard information flow and decision making we propose that modeling research such as that conducted here has significant potential to develop new understanding identify strengths and weaknesses in hazard forecasting and risk communication and recommend areas for improvement software data availability chime abm was implemented in the freeware agent based modeling platform netlogo version 5 3 1 and later updated to netlogo version 6 0 the model code supporting documentation and input files are archived on the comses net library openabm com at the url https www comses net codebases 5504 releases 1 4 0 declarations of interest none acknowledgements the authors acknowledge sean bergin wendy cegielski nicholas gauthier and grant snitker for their assistance with the development and testing of the agent based model we also thank our collaborators on the larger project especially heather lazrus olga wilhelmi christopher davis kathryn fossell david ahijevych chris snyder leysia palen and kenneth anderson for their contributions to this research the work reported here was supported by national science foundation award ags 1331490 the national center for atmospheric research is sponsored by the national science foundation 
26119,this article introduces an agent based modeling laboratory for investigating how evolving hazard information propagated through forecaster media public official and peer information networks affects patterns of public protective action decisions during hurricane threats the model called chime abm provides a platform for integrating atmospheric science social science and computer and information science knowledge and data to explore the complex socio ecological dynamics of modern hazard information and decision systems from a new perspective first the model s interdisciplinary conceptualization and implementation is described results are then presented from experiments demonstrating the model s behaviors and comparing patterns of evacuation decisions when key agent parameters and the geographical population distribution forecast skill and storm are varied the article illustrates how this type of theoretically and empirically informed digital laboratory can be used to develop new insights into the interactions among environmental hazards information flow protective decisions and societal outcomes keywords hurricanes evacuation decisions agent based modeling natural hazards information networks risk communication 1 introduction as hurricanes such as harvey irma maria florence and michael demonstrated during the 2017 and 2018 atlantic hurricane seasons improving hazard risk communication and decision making is critical for scientists and society nasem 2017a b nws 2019 most research on societal information flow and decision making for environmental risks is observational using data from surveys interviews and other empirical methods in the context of hurricanes tropical cyclones such research has developed a broad base of empirical understanding about how at risk members of the public access and use risk information and make protective decisions see e g reviews in baker 1991 dash and gladwin 2007 lazo et al 2015 huang et al 2016a however because this work typically focuses on specific populations or situations findings can vary widely across studies e g huang et al 2016a and it is difficult to identify broader spatial and temporal patterns many of these existing studies also utilize data at the individual or household levels which limits their ability to elucidate how the socially interactive processes underlying hazard risk communication and response scale up to influence outcomes e g drabek 1999 dash and gladwin 2007 taylor et al 2009 moreover in today s world forecast and warning information and its communication evolve rapidly as a hazard approaches in conjunction with interacting individuals information behaviors risk perceptions and decisions lee et al 2009 morss and hayden 2010 sherman morris et al 2011 morss et al 2017 bica et al 2019 collecting the empirical data needed to analyze and understand these intersecting social spatial and temporal dynamics is challenging meyer et al 2014 morss et al 2017 demuth et al 2018 computational modeling provides an opportunity to investigate hazard information flow and decisions using a fundamentally different approach one that complements and builds on empirical studies as discussed in morss et al 2017 weather prediction communication and decision making are interconnected components of a dynamic coupled natural human system when hazardous weather threatens multiple types of actors interact to create communicate interpret and use forecasts warnings and other risk information gladwin et al 2007 demuth et al 2012 morss et al 2015 bostrom et al 2016 bica et al 2019 within this system the risk information available and associated uncertainties change across space and through time further in the modern hyper connected environment information can be widely exchanged almost instantaneously little is known about how new hazard information propagates and influences protective decisions agent based modeling provides a useful toolkit for exploring the dynamics of this type of system this article introduces a new agent based modeling platform for investigating relationships among environmental hazards hazard information information flow and patterns in people s protective decisions the communicating hazard information in the modern environment chime abm was developed through collaboration among physical scientists social scientists computer and information scientists and agent based modelers as part of a larger multi method project studying the dynamic interconnected processes that characterize the modern hazard prediction communication and decision making system morss et al 2017 chime abm was created to provide a platform for integrating knowledge and data across disciplines to build understanding about the dynamical system of interest connecting concurrent geophysical predictive modeling and empirical social and information science research e g anderson et al 2016 bica et al 2019 demuth et al 2018 fossell et al 2017 kogan et al 2015 kogan and palen 2018 morss et al 2017 wilenski 1999 in doing so we aimed to develop an interdisciplinary digital laboratory for running controlled experiments with different configurations of hazard information and social interactions under various dynamic scenarios see e g waldrop 2017 verburg et al 2016 rovere et al 2016 magliocca and ellis 2016 the modeling platform was designed to represent key aspects of the real world system of interest given our research goals while remaining sufficiently simple to allow meaningful exploration and knowledge building sun et al 2016 buchmann et al 2016 allison et al 2018 it is currently implemented for a hurricane threatening the us coastline over a five day period but it was designed to represent general features of weather hazard forecasting warning and response in order to enable a variety of future extensions a core component of chime abm is a model of hazard information flow and protective decisions with heterogeneous agents who interact via peer and media networks the agent types represent five types of key actors in hazard forecast and warning systems forecasters public officials media broadcasters other media aggregators and communicators and citizens with varying individual characteristics the agents interact with each other and with a virtual world with ocean and land areas a hurricane that moves across the landscape and forecast and other risk information that evolves as the storm approaches although agent based modeling has previously been used in hazards research the research discussed here differs from this previous work in several ways one body of related work uses agent based modeling to study evacuation planning for hurricanes e g chen et al 2006 zhang et al 2009 yin et al 2014 ukkusuri et al 2017 or other hazards e g dawson et al 2011 wang et al 2016 bernardini et al 2017 such work focuses primarily on how collective evacuation behaviors interact with built infrastructure in order to explore issues such as traffic demand evacuation routing and strategies for improving evacuation effectiveness the emphasis of the agent based modeling in these studies is therefore not on who decides to take protective action when and why but on how people and vehicles move and interact after they decide to evacuate although evacuation logistics are important they are not examined here in chime abm v1 version 1 the agents do not physically move instead we focus on how inter agent dynamics and human information environment interactions influence patterns in evacuation decisions other studies have used agent based models to examine how hazard information diffusion or protective decision making is influenced by a population s characteristics such as social networks or inter individual heterogeneity e g widener et al 2013 rand et al 2015 dixon et al 2017 again the research presented in this article has a complementary but different emphasis although our work also focuses primarily on members of the public we design and utilize a model with multiple types of interacting agents representing key roles in the warning communication and response system in addition we add a new type of dynamical interaction as a central theme by modeling different types of evolving hazard information especially forecast information and associated uncertainty interacting with the dynamical human behaviors simulated by an agent based model another previous application of agent based modeling for hazards investigates hazard risk management decisions on longer multi year time scales e g haer et al 2016 reilly et al 2017 tonn and guikema 2018 the modeling presented here expands on this type of work and much of the agent based modeling work on human environment interactions e g parker et al 2003 boone et al 2011 an 2012 rounsevell et al 2012 filatova et al 2013 barton et al 2016 groeneveld et al 2017 schulze et al 2017 by examining interactions on much shorter time scales where different types of information and decisions are important the research presented in this article focuses on the time scale of a single hurricane threat although chime abm could be adapted to investigate social learning and other longer term issues building on and extending previous related work here we describe the conceptualization and implementation of a new agent based modeling laboratory for investigating how interactions among geospatially distributed heterogeneous agents influence and are influenced by hazard information flow decisions and outcomes we then present results from a set of experiments illustrating how large scale patterns of hazard risk management decisions can emerge from the decisions of many simplified heterogeneous agents as they interact with each other and with their evolving physical and informational environment for the initial model development and research shown here we used a case study approach performing simulations for the state of florida us and two hurricanes that previously made landfall in the state the model was designed to be flexible however and thus the modeling framework can readily be modified to study other regions agent configurations hazards and hazard information section 2 provides an overview of chime abm v1 and its key components section 3 describes the experimental design and implementation and the analysis of the experimental output section 4 presents results beginning with the spatial and temporal patterns in evacuation decisions exhibited by the model it then investigates the sensitivity of the model s behavior to changes in three key parameter sets the citizen agents weighting of different types of information the timing of public official agents evacuation orders and the geographic distribution of the citizen agent population and compares results for forecasts with two different levels of skill for two different hurricanes section 5 summarizes key results and discusses implications of this work for hurricane risk communication and future research 2 chime abm conceptual model and implementation chime abm was developed and all reported experiments were run in the netlogo 5 3 modeling environment wilenski 1999 see fig 1 this section describes the major components of the model and key elements of their design for further details the commented code an odd specification a formal detailed model description and supporting input files are available for download at the comses model library https www comses net codebases 5504 releases 1 4 0 1 1 version 1 4 0 is a netlogo 6 0 2 version of the code which has been peer reviewed through the comses network this version of the code also includes supporting files for model simulations in the texas region of the us gulf coast tested for hurricane harvey in 2017 chime abm v1 includes a spatially explicit modeled world representing a geographical area of interest described in section 2 1 a dynamic hazard i e a hurricane in this implementation that moves through that world section 2 2 evolving forecast information about that hazard section 2 3 and a multi agent model in which different types of hazard related information are accessed and interpreted by different agents circulated among them and used to assess risk and make protective decisions section 2 4 these components are conceptually and numerically interconnected as shown in fig 2 the simulations shown here use a one hour time step and run for 120 time steps representing 5 days the time step geography and representations of the hazard forecasts and agents in the current implementation of the model were designed to simulate decision making in response to a tropical cyclone threat however the model was designed to represent fundamental features of hazard information and decision systems and thus to be adaptable to other geophysical hazards to design and implement the model we integrated expertise in agent based modeling with knowledge and data from research and operational meteorology emergency management risk communication information science social vulnerabilities and protective decision making as in any modeling effort of this type many aspects of chime abm v1 are abstracted or simplified from the real world and some real world features and processes are not represented decisions about what to include in the model and how to represent it were based on our research questions cross disciplinary discussions among our research team interactions with the larger research project discussed in morss et al 2017 and review of relevant literature elements of the model version presented here could readily be modified or expanded to address additional research questions and several features of the model were designed to facilitate such future work 2 1 modeled world the modeled world is a cellular representation of a geographic area of interest which can be real or imagined and includes land and ocean surface in the simulations shown here the world is the state of florida us and surrounding oceans derived from a gis digital elevation model dem in esri ascii raster format with a spatial resolution of 0 5 km we used a realistic rather than abstract geographical domain in order to provide a starting point for more complex future experiments we selected florida for initial model development and testing because it is an area of the mainland us that is highly susceptible to hurricanes although it had not experienced a major hurricane landfall in several years when we began our study to create the modeled world used here the dem is imported into netlogo and interpolated to model environment cells with a spatial resolution of 5 91 km 3 67 mi elevation is not used in the current version of the model algorithms but is included for potential use in future experiments e g for estimating a cell s level of flood risk data for florida counties county seats and population density derived from us census data for the year 2000 are also imported into netlogo and applied to the model cells in chime abm v1 none of the cellular landscape characteristics is dynamic nor are features of the built environment such as buildings or roads represented the modeled world does include hazard zones representing geographical areas at different levels of risk that abstractly simulate hurricane evacuation zones in the simulations reported in this article a single evacuation zone is defined as all locations within 1 5 cells approximately 9 km from the coast as with other components of the model these aspects of the modeled world can be revised in future experiments 2 2 modeled storm the chime abm v1 modeled world also includes a simulated tropical cyclone that approaches and then potentially affects the model domain the storm can be real or synthetic here we simulate historical storms using the us national hurricane center s nhc s tropical cyclone best track data https www nhc noaa gov data hurdat which include a sequence of locations of the storm s eye and other characteristics table 1 future versions of the model could include more complex representations of the storm and associated hazards and impacts e g areas experiencing storm surge or inland flooding transportation disruptions or power outages which could then also influence information flow and decision making in this article we perform simulations for hurricanes charley and wilma which made landfall in florida in 2004 and 2005 respectively these were selected to represent different types of storms e g charley had a small wind field for a tropical cyclone and wilma a large wind field with different tracks and forecast errors 2 3 forecast information along with an evolving storm chime abm simulates forecasts of the future state of the storm which again can be real or synthetic in chime abm v1 the information in these forecasts table 1 is based on the format of the forecasts provided in nhc s tropical cyclone forecast advisory products fig 3 each forecast typically provides predictive information that is valid at 12 or 24 h intervals in the future 2 2 the difference between the forecast valid time and the issuance time is referred to as lead time for example in the nhc forecast product shown in fig 3 forecasts are provided at 9 21 33 45 and 69 hour lead times as well as 93 and 117 hour lead times not shown in the figure with new forecasts typically issued every 6 hours this means that the most current forecast information available in the model evolves with time although older forecast information may still be circulating in the information network of the multi agent model as discussed in the introduction and morss et al 2017 this is an important dynamical element of real world modern weather forecast and warning systems the experiments reported here use two types of forecast information historical and ideal the historical forecasts were simulated using data from the nhc tropical cyclone forecast advisory products that were available in real time as the storm being studied approached e g fig 3 the ideal forecasts simulate perfect forecast information available at all lead times and were generated from the model s representation of the evolution of the actual storm along with the forecasts the model also includes information about forecast uncertainty in v1 this is represented by the nhc cone of uncertainty which estimates uncertainty in the track forecast at different forecast lead times based on average errors in recent track forecasts see http www nhc noaa gov aboutcone shtml on average tropical cyclone track forecast errors decrease as a storm approaches and so the track forecast uncertainty increases with lead time a location is defined as in the cone of uncertainty if its distance from the eye of the storm is less than the track uncertainty for that forecast lead time fig 1 here we use historical track uncertainty data from 2005 obtained from the nhc note that for the experiments in this article the same cone of uncertainty is used for both historical and ideal forecast configurations future versions of the model could include more complex representations of hurricane forecast information including more detailed spatial representations of the forecast of the storm forecasts of storm related hazards and impacts e g areas at risk from storm surge or inland flooding or power outages and associated uncertainties in the multi agent model agents extract several variables from the forecast information for use in the risk assessments discussed in section 2 4 they search the forecast information to identify their anticipated closest distance to storm track smallest distance between their location and the forecasted trajectory of the storm s eye not considering the cone of uncertainty they identify the anticipated time of storm arrival the forecast time corresponding to the anticipated closest distance to storm track and they use this to calculate the anticipated time until storm arrival time until the storm s eye is anticipated to arrive at their location they also extract the forecasted maximum sustained winds at the anticipated time of storm arrival which is referred to as the anticipated storm intensity at arrival 2 4 multi agent model of hazard information flow and decision making the multi agent model of hazard information flow and decision making was designed to represent key elements of modern us weather forecast warning and response systems specifically for hurricanes e g gladwin et al 2007 demuth et al 2012 bostrom et al 2016 with features that are sufficiently general that the model could readily be adapted for other types of hazardous weather e g parker and fordham 1996 brotzge and donner 2013 morss et al 2015 it includes five types or breeds in netlogo of agents weather forecasters who initiate forecast information media broadcasters who adapt and communicate information other media aggregators and communicators public officials who provide protective action recommendations and citizens members of the public who collect and share information assess risks and make protective decisions given the goals of our research the citizen agents and their decision making processes are more complex than the other agents the other agent breeds are purposefully simple in chime abm v1 but their roles can be expanded in the future an overview of each of the agent breeds is provided in table 2 and each is described further below more detailed descriptions of the agents rules and algorithms can be found in the supporting documentation for the model 2 4 1 forecaster agent forecaster agents function as weather forecasters modeled after e g the us national weather service s nhc who provide forecast information about the hazard in chime abm v1 there is one forecaster and its only job is to publish new forecast information including uncertainty estimates as it is updated for use by other agents tables 2 and 3 in the simulations shown here the forecaster runs every time step although the forecaster has no need for a physical presence in the current implementation it is placed at a random location in the model s populated geographic domain represented by a small green circle e g in fig 1 forecaster agents were given a location to provide a starting point for potential future versions of the model in which different forecasters provide forecast information for different regions as in the real world 2 4 2 broadcaster agents broadcaster agents simulate the role of traditional media such as television in communicating hazard information to a broad audience demuth et al 2012 in chime abm v1 their primary role is to convey forecast information to the public official and citizen agents without changing the content of the information broadcasters reorganize the forecasts published by the forecaster into a format that can be more readily used by the public official and citizen agents tables 2 and 3 they also linearly interpolate the forecasts from the temporal resolution available in the nhc provided information e g 12 or 24 h time steps as shown in fig 3 to 1 h time steps which facilitates interpretation by public official and citizen agents in the simulations shown here there are 10 broadcasters and each broadcaster runs at each time step although broadcasters have no need for a physical presence in the current implementation they are placed at random locations in the populated domain represented by small yellow circles broadcasters were given locations for potential use in future versions e g to represent major media markets in order to simplify possible influences on evacuation patterns for the experiments reported here the broadcasters were constrained not to modify the forecast content nor to potentially introduce delays in providing updated forecast information to other agents this can also be modified in future experiments 2 4 3 information aggregator agents aggregator agents are intended to simulate the roles of new media information actors who access process and redistribute hazard information e g on the internet or mobile devices these agents were included in the model to provide the ability to represent the ways in which many people currently obtain and combine information from multiple types of sources a key feature of the modern information environment that we aim to explore dow and cutter 2000 gladwin et al 2007 morss et al 2017 in the simulations shown here aggregators function identically to broadcasters with two exceptions they do not run at every time step table 2 and they do not provide information to public official agents the random activation was designed to simulate internet based sources who may aggregate and communicate information intermittently compared to traditional media actors who tend to communicate on a more regular schedule in future versions of chime abm aggregators can be revised to play more complex information roles similar to those of real world internet based information sources and experiments can be run to investigate the influence of these different types of sources creating and conveying information in different ways the simulations shown here have 10 aggregators placed at random locations in the populated model domain and depicted as small pink circles as with the broadcasters aggregators were given a physical location to allow agents to have location based preferences for aggregator information sources in future experiments but this location does not influence the model s behavior in the current version 2 4 4 public official agents public official agents also referred to as officials simulate government personnel who help protect the public and inform people about protective actions demuth et al 2012 in v1 officials only role is to decide whether to issue evacuation orders which are conveyed to citizen agents for use in their risk assessment section 2 4 5 for the simulations shown here one official is located in each county at the county seat and only officials located in coastal counties those exposed directly to the ocean can issue evacuation orders this is modeled after real world hurricane evacuation orders which are typically issued for areas at risk of inundation from storm surge i e areas near the coast as a simulation evolves the modeled officials decide whether and when to issue evacuation orders based on their assessment of the risk that the hurricane poses to coastal locations in their county using updated forecast information obtained from broadcasters at each time step officials assess risk by obtaining and processing forecast information and comparing it to three global parameters whose values are set at initialization earliest latest and wind threshold table 3 at each coastal cell three criteria are evaluated 1 is the anticipated closest distance to storm track within the cone of uncertainty in other words less than the track forecast uncertainty corresponding to that lead time 2 is the anticipated time until storm arrival within the lead time window defined by earliest and latest and 3 is the anticipated storm intensity at arrival greater than wind threshold if all three criteria are met at any of its coastal cells an official will decide to issue an evacuation order set orders from 0 to 1 which becomes active at that time step and remains active for the remainder of the simulation the track cone of uncertainty is used here as a proxy for areas that warrant evacuation orders because at lead times of more than a day or two location specific storm surge predictions have low skill fossell et al 2017 thus the storm track and associated uncertainty is a reasonable first order approximation of coastal areas at risk of significant impacts future versions of the model could include more complex estimates of storm surge risk at different locations based e g on more complex representations of coastal geography and topography and or translation of the atmospheric hurricane forecasts into forecasts of surge inundation in all of the simulations shown here latest is set to 0 hours which means that officials can issue evacuation orders up until the storm s eye arrives at its county s coastline the values of wind threshold used in these simulations are equivalent to high category 2 low category 4 winds on the saffir simpson hurricane winds scale https www nhc noaa gov aboutsshws php officials are depicted in fig 1 as small red stars which turn white if that official issues an evacuation order as with the broadcasters and aggregators we chose to make the officials behaviors relatively simple here to simplify interpreting the results of these initial experiments the officials algorithms could be made more complex or their roles expanded in the future 2 4 5 citizen agents citizen agents cit ags model members of the public or public households who dynamically collect and process information about a potential hazard assess the risk posed by the hazard and decide whether the risk is sufficient to warrant changing their behaviors fig 2 cit ags information sources include other agents connected through their social and information network described in section 2 4 6 and the model s physical environment the cit ags also serve as disseminators by relaying information to other cit ags the design of the cit ag algorithms was adapted from conceptual models of protective decision making for hazards such as the protective action decision model padm lindell and perry 2004 2012 as well as findings from empirical research by members of our project team and other scholars on information flow and protective decision making for hurricanes e g baker 1991 dow and cutter 2000 gladwin et al 2001 gladwin et al 2007 dash and gladwin 2007 morss and hayden 2010 lazo et al 2015 huang et al 2016a morss et al 2016a demuth et al 2016 cuite et al 2017 bostrom et al 2018 demuth et al 2018 and references therein and for other weather related hazards e g mileti and sorensen 1990 sorensen 2000 brotzge and donner 2013 ruin et al 2014 lazrus et al 2016 morss et al 2016b one challenge of this project was to translate parsimonious theoretical models such as that provided by padm and the more detailed but often incomplete information available from empirical analyses into simple yet sufficiently specific instructions for agents to do so we synthesized the relevant literature to identify key behavioral features to implement given our research goals informed by the cross disciplinary expertise within our research team existing research indicates that in general people evacuate when they believe that an approaching hurricane poses a risk to their own or their family s safety and that different people can both perceive risk differently and have different evacuation barriers or constraints e g baker 1991 gladwin et al 2001 dash and gladwin 2007 lazo et al 2015 thus we formulated the cit ag module in terms of combining and translating information obtained from multiple sources into a risk assessment which is then compared with decision thresholds that vary across the cit ag population at model initialization chime abm v1 has two options for geographically distributing cit ags random and realistic the realistic population simulations distribute cit ags according to the real world population density based on census data here from the year 2000 along with location each cit ag is assigned multiple variables that influence its hazard information collection information processing risk assessment and decisions table 4 these variables were designed to capture the real world heterogeneity among members of the public in factors such as interest in and access to hazard information social and information connectedness trust in information sources risk perceptions and interest in and capacity for evacuating and taking other protective actions values for these variables are assigned individually to each cit ag at initialization and do not change during a simulation except for the variable that controls scheduling when the cit ag is active feedback1 this parameter can change if during an active time step a cit ag s risk assessment is sufficiently high or low that it decides to increase or decrease its information collection frequency cit ags check for information on average every 12 hours towards the beginning of a simulation until they assess that they may be at risk at which point they begin to run their algorithms more frequently but never more often than the hourly time step of the model if they assess that they are at low risk they increase the interval between active time steps this dynamic scheduling for individual cit ags was designed to represent the ways that real people may become more or less attuned to their physical and informational environment as a hazard threat evolves mileti and sorensen 1990 lee et al 2009 sherman morris et al 2011 lindell and perry 2012 morss et al 2017 demuth et al 2018 at each time step when a cit ag is active it executes four algorithms 1 collect hazard related information 2 sort and process that information 3 use the information to assess risk 4 decide whether to change its information collection frequency evacuate or take other e g property protective action fig 2 an overview of each of these algorithms is below additional details can be found in the supporting documentation a cit ag collects information by querying a the official agent in its network about whether an evacuation order has been issued b its environment about whether its location is currently experiencing winds of 34 knots or greater the minimum threshold for a tropical storm c broadcasters and aggregators in its media network for their forecast information d other cit ags in its peer network for their forecast information 3 3 cit ags make information available to other cit ags at all time steps not only when they are active and e its memory for its own most recent forecast interpretation the first four a d represent several of the major external sources of information that members of the public use in hurricane risk assessments and decision making evacuation orders physical environmental cues and forecasts from professional and social sources e g dow and cutter 1998 dash and gladwin 2007 morss and hayden 2010 petrolia and bhattacharjee 2010 lindell and perry 2012 demuth et al 2018 the last e represents people s tendencies to use new information such as a newly accessed forecast to update previous interpretations the cit ag then selects a random subset of the collected forecast information to process and combines that information weighted by its trust in each of the information sources to generate its own updated forecast interpretation this forecast interpretation is saved into the cit ag s memory for use the next time it seeks information and it is also now available to other linked cit ags in the social and information network next the cit ag assesses its risk table 5 it evaluates its risk based on forecast information using the equation 1 risk function h e t i m e c 2 2 σ 2 where time is time until storm arrival h is the peak height of the curve and c and σ are random variables defined in table 4 an example risk function is shown in fig 4 h is a function of the cit ag s representation of whether it lives in an evacuation zone its interpretation of the forecast and the track forecast uncertainty see full model description for details functionally h increases when the cit ag thinks that it is in an evacuation zone that the storm intensity at arrival will be higher and that it is in the cone of uncertainty or if not in the cone of uncertainty that its closest distance to storm track is greater relative to the track forecast uncertainty at time until arrival this formulation was developed based on previous and concurrent work indicating that these are important factors influencing how people interpret hurricane forecasts to evaluate risk e g dow and cutter 1998 2000 gladwin et al 2001 zhang et al 2007 dash and gladwin 2007 morss and hayden 2010 petrolia and bhattacharjee 2010 huang et al 2016a morss et al 2016a bostrom et al 2018 the value of eq 1 at the cit ag s anticipated time until storm arrival then becomes the cit ag s risk assessment based on forecast information as shown in fig 4 a cit ag assesses higher risk when h is greater and when the anticipated time until arrival is closer to c modulated by σ σ this formulation and the values of c and σ were selected to abstractly represent heterogeneous public preferences for evacuation timing with most members of the public evacuating between 12 and 72 hours in advance of landfall depending on the situation lindell et al 2005 gudishala and wilmot 2010 czajkowski 2011 wu et al 2012 huang et al 2016b 4 4 note that chime abm v1 does not simulate the fact that in the real world people tend to evacuate during daylight hours this diurnal cycle in evacuation timing would be important to add if the model were used to examine issues such as travel demand and evacuation routing the cit ag s final risk assessment risk estimate is calculated by adding the risk assessment based on forecast information a small amount of random error and factors based on evacuation orders and environmental cues if present as summarized in table 5 evacuation orders are given greater weight if the cit ag has greater trust in officials and if it believes it is in an evacuation zone e g mileti and sorensen 1990 gladwin et al 2001 cuite et al 2017 thompson et al 2017 in the current formulation the presence of environmental cues adds a constant value to the risk assessment finally the cit ag decides whether and how to modify its behaviors by comparing its final risk assessment with the risk thresholds in table 4 specifically risk estimate values greater than risk life risk property and info up trigger the cit ag to decide to evacuate take other protective action 5 5 this is modeled as a proxy for non evacuation protective actions such as boarding windows protecting other property or gathering supplies which people typically engage in at lower risk thresholds than evacuation and increase its information collection frequency decrease feedback1 respectively if risk estimate is less than info down the cit ag decreases its information collection frequency increases feedback1 if the cit ag decides to evacuate it will no longer run its risk assessment and decision algorithms at subsequent time steps however it will continue to collect and interpret forecast information according to its schedule and to make its evolving forecast interpretation available to other cit ags in its peer network at initialization each cit ag is visually depicted by a small blue circle fig 1 if a cit ag decides to take a non evacuation protective action its color changes to green and then to orange if it decides to evacuate one important simplification of the cit ags algorithms in chime abm v1 compared to the real world is that cit ags do not share or remember information other than their interpretation of the forecast another is that they do not consider social cues such as observations of others protective behaviors although these processes are known to influence people s hazard related risk assessments and behaviors e g mileti and sorensen 1990 dash and gladwin 2007 taylor et al 2009 lindell and perry 2012 demuth et al 2018 we chose not to include them in v1 because doing so would add free parameters and dynamics complicating interpretation of results from the experiments such features could be added in future model versions to explore additional system dynamics 2 4 6 citizen agent social and information network information sharing among agents in chime abm is implemented through a social and information network that connects cit ags with each other and with other agent breeds to support interpretation of other aspects of the system s dynamics the network structure used here was designed to capture some aspects of real social and information networks while also being relatively simple the network changes from simulation to simulation but in the current model formulation it is static during a simulation in addition all links between cit ags are non directional such that information can flow both ways this network formulation can be extended in complexity in the future or dynamic elements added in the experiments conducted here each cit ag selects a random sized randomly selected subset of broadcasters and aggregators to include in its network each cit ag is also connected with the geographically closest official the peer network builds connections between cit ags using two algorithms first a standard preferential attachment routine is run that creates a relatively small number of highly connected nodes in the network in other words a scale free social network this type of network is typically sparser than real social networks and real social networks often exhibit transitivity thus a second algorithm is then run which makes additional connections among cit ags to complete triads an example peer social and information network created using these algorithms is depicted in fig 5 additional detail about the network building algorithms can be found in the supporting model documentation 3 experimental methodology and data analysis 3 1 initializing the model and running simulations when chime abm v1 is initialized the inputs needed to run a simulation are loaded including map layers storm information and forecasts next the population of agents is distributed in the modeled world and the agents variables are initialized running a simulation starts the clock and triggers the storm and forecasts to evolve and every agent to run breed specific instructions according to its schedule as the model runs key variables are stored and the depiction on the model interface is updated at the end of each simulation relevant data are output for further analysis to run large numbers of simulations in parallel we used the netlogo behaviorspace tool 3 2 experimental design the experiments shown in this article begin with a set of model parameters that produced quasi realistic behaviors of interest and then systematically modify those parameters to explore key model behaviors and sensitivities because chime abm includes a number of stochastic elements it can exhibit significant run to run variability thus we ran multiple repetitions for each model configuration and aggregated results across these simulations table 6 provides an overview of the different sets of experiments reported in this article the first set of results shown in section 4 1 is from 100 simulations for hurricane charley with random geographical distribution of cit ags and ideal forecasts this configuration was selected as a starting point for the experiments to minimize the complicating effects of non uniform population distribution forecast errors and evolving forecast information when interpreting the results we ran these initial experiments with charley because unlike wilma the full diameter of charley s 34 knot winds remained over land as the storm crossed florida and so the model s land domain encompasses more of the highest impact zones discussed in section 3 3 the next two sets of experiments shown in sections 4 2 and 4 3 investigated the sensitivity of the model s behavior to key parameters in the cit ag and official agent algorithms respectively for the experiments varying the cit ags information weightings we ran 100 simulations for each combination of the information weightings shown in the second settings column in table 6 729 configurations for a total of 72 900 simulations 6 6 the values of the weightings for the different types of information are relative scales and do not have any independent meaning the maximum weightings used in the sensitivity tests were selected based on the increase in weighting of each type of information that was required to significantly affect evacuation rates for the experiments varying the timing of officials evacuation orders we shifted the earliest lead time at which officials could issue evacuation orders between 54 hours and 0 hours prior to anticipated storm arrival as shown in table 6 and ran 100 simulations for each of the 10 configurations we then examined the effects of changing the geographical distribution of cit ags from random to realistic holding all other settings constant table 6 for this set of experiments with only two model configurations we ran 1000 simulations per configuration section 4 4 finally we explored the evacuation patterns produced by the model when the ideal forecasts are changed to historical forecasts and when the storm is changed from charley to wilma section 4 5 as shown in table 6 all other settings were kept constant as in previous experiments except that wind threshold was adjusted so that each of the configurations had on average similar numbers of officials issuing evacuation orders evacuation orders are an important driver of evacuation decisions in the model and so this modification removes some of the variability between otherwise parallel scenarios while still allowing the officials to respond to forecast information 7 7 note that because the storm tracks and forecasts are different in the four configurations different sets of officials may issue evacuation orders for each of the four configurations we ran 1000 simulations 3 3 data analysis to facilitate comparing evacuation patterns quantitatively across simulations chime abm v1 tracks cit ag decisions within multiple impact zones designed as first order approximations of areas likely to experience different levels of impacts from the storm here we use six impact zones defined by whether a location a is coastal in an evacuation zone 1 5 grid cells or less from the ocean or inland and b experiences maximum storm winds during the simulation that are greater than 64 knots hurricane force between 34 and 64 knots tropical storm force or less than 34 knots the storm tracks and six impact zones for the hurricane charley and wilma simulations are shown in fig 6 the primary model output data analyzed here are the percent of cit ags in each impact zone that decided to evacuate in each simulation these were analyzed across the simulations run for each model configuration by examining statistics such as the mean and inter simulation variability we also examined officials evacuation order decisions more detailed spatial and temporal patterns in cit ags decisions and other aspects of the model s behavior and outcomes most of the results are presented in summary tables or figures to provide a compact summary of broad patterns together with graphics depicting more detailed aspects of the model s behavior to support more in depth interpretations discussed in the text 4 results 4 1 spatial and temporal patterns of cit ag evacuation decisions first we examine results from simulations with the model configuration shown in the first settings column in table 6 these results provide a first order assessment that the agents in chime abm are behaving in a structurally valid way based on the processes included in the model they also illustrate several key aspects of the model s behavior which provides a starting point for interpreting subsequent results the top row of table 7 summarizes cit ags evacuation decisions in the six impact zones averaged across the 100 simulations to illustrate spatial patterns in the model s output in greater detail fig 7 presents a map of cit ag evacuation decisions for a single randomly selected completed simulation note that random placement of the cit ags combined with charley s small size results in few cit ags within the coastal 64 knot impact zone these results show several patterns that are similar to real hurricane evacuation behaviors first cit ags evacuation rates are higher near the coasts in evacuation zones than inland they are also higher in areas closer to the storm s track this pattern arises because with the ideal forecasts in this simulation the officials in coastal counties that will experience strong winds issue evacuation orders shortly after their evacuation order window opens and cit ags in these areas receive forecasts that the storm will track near their region throughout the simulation cit ags in coastal areas are much more likely to believe they are in an evacuation zone which increases their sensitivity to both evacuation orders and forecast information thus between the evacuation orders the forecast information and the environmental cues that they experience as the storm approaches many of the cit ags in coastal 34 knot impact zones decide to evacuate as expected cit ag evacuation rates are greater in the 34 knot zones than in the lower impact 34 knot zones counterintuitively however modeled evacuation rates are greater in the 34 64 knot zones than in the higher impact 64 knot zones a more in depth investigation indicates that this occurs because the statistics presented here average across the western coast of florida where the storm makes landfall and a higher percentage cit ags evacuate and the eastern coast of florida where a lower percentage of cit ags evacuate as shown in fig 6 the 64 knot wind area expands as the storm crosses florida evacuation statistics in the 64 knot impact zones are therefore more heavily weighted towards the lower evacuation rates in eastern florida at the same time the model s land domain fully encompasses the 34 64 knot and 34 knot wind zones in the western part of florida but not in the eastern part of florida evacuation statistics in the 34 64 knot impact zones are therefore more heavily weighted towards the higher evacuation rates in western florida together this decreases evacuation rates in the 64 knot zones compared to those in the 34 64 knot zones as explained by baker 1991 evacuation rates vary from place to place in the same hurricane and from storm to storm in the same place p 291 which complicates comparing the model results with real world evacuation rates despite these limitations this general pattern evacuation rates that are highest in the highest risk areas along the coast near the storm s track and that decrease as one moves inland and away from the storm is broadly similar to that found in real world hurricane evacuations e g baker 1991 lindell et al 2005 morrow and gladwin 2005 huang et al 2012 2016b one major difference is that the evacuation rate in the model decreases much more rapidly as one moves inland than it typically does in the real world due in part to the simplified formulation of the influence of coastal proximity on risk assessments and decisions this could be modified in future versions a second pattern illustrated by table 7 and fig 7 is the inter agent variability in cit ag evacuation decisions although many cit ags in the highest impact zones decide to evacuate some do not and a small percentage of cit ags who are located in low risk areas decide to evacuate this is consistent with real world hurricane evacuations and more generally with the heterogeneity exhibited by real world u s individuals and households in hurricane evacuation decisions e g hasan et al 2011 dixon et al 2017 in the model this variability arises from the individual cit ags different values for the randomly generated variables in table 4 for example a cit ag in a high risk area may decide not to evacuate because it has a very high value of the risk threshold risk life or because it has very low values of the trust authority parameter or erroneously thinks it is not in an evacuation zone in these simulations none of the agents misinterpret the forecasts and cit ags do not consider evacuation orders from distant officials thus cit ags who evacuate from very low risk areas such as the one represented by the white dot in northern florida in fig 7 do so primarily because they have low values of the risk threshold for evacuation although preferences for evacuating early when the cone of uncertainty covers a larger area and other factors can play a role variability in cit ag behaviors can also result from more complex interactions among components of the model s dynamics for example a cit ag in a high risk area may not evacuate because the times at which it collects information and thus receives information indicating that it is at high risk do not coincide with the timing of peaks c in its risk function curve this is more likely if a cit ag collects information infrequently feedback1 is large and or info up and info down are high and has a narrow risk function curve σ is small when it runs the risk assessment algorithm to illustrate temporal patterns in the model s behavior fig 8 depicts the timing of cit ag evacuation decisions in each of the six impact zones averaged across the 100 simulations these results show two peaks in evacuation timing one at 30 54 hours before anticipated storm arrival and a smaller bump at 0 6 hours before arrival a small percentage of cit ags decide to evacuate prior to the issuance of evacuation orders prior to approximately 54 hours based on their interpretations of forecast information this then leads into the first peak in evacuation decisions which results from cit ags use of evacuation orders combined with forecast information the second peak in evacuation decisions occurs as the storm approaches close enough to provide cit ags with environmental cues along with evacuation orders and forecasts the first peak in evacuation timing is much larger in coastal impact zones than inland because cit ags in coastal zones are more likely to both receive evacuation orders and believe they live in an evacuation zone the latter of which makes them more sensitive to both forecasts and evacuation orders the second peak occurs only in the 64 knot and 34 64 knot impact zones because only cit ags in those impact zones receive environmental cues the variability in when cit ags decide to evacuate arises for reasons similar to those discussed above these include differences in cit ags scheduling and the timing of their risk function peak as well more complex interactions such as a higher risk life threshold that leads some cit ags to need to accumulate more information indicating that they are at high risk before they decide to evacuate 4 2 varying citizen agents weighting of different types of information building on the results in section 4 1 next we investigate the effects of modifying cit ags weightings of the three main types of hazard information they use to assess risk in chime abm v1 forecast information evacuation orders and environmental cues table 6 these results further elucidate key aspects of the modeled system s dynamics and they provide additional insight into the roles of different types of information in cig ags risk assessments and decisions table 7 summarizes cit ags evacuation decisions in the six impact zones for a subset of the perturbed information weightings to examine aspects of these results in greater depth fig 9 depicts evacuation rates and peak evacuation timing for cit ags in the highest impact zone across all 729 model configurations and fig 10 compares more detailed evacuation timing results for the highest impact zone for several of the information weightings the results in table 7 and the overall pattern of symbol sizes in fig 9 indicates that in the model s current formulation the cit ags risk assessments and decisions are more sensitive to forecast information than they are to evacuation orders and environmental cues for example when cit ags use only forecast information at its standard weighting forecasts only row in table 7 approximately 16 of cit ags in coastal impact zones evacuate in contrast when cit ags use only evacuation orders and environmental cues at their standard weightings alone or together very few cit ags evacuate in the absence of forecast information table 7 and fig 9 indicate that increasing the weighting of evacuation orders and environmental cues by a factor of 4 or more is needed to motivate a substantial percentage of high impact cit ags to evacuate aspects of these model behaviors can be modified by changing the information weightings however cit ags evacuation decisions are also more sensitive to forecast information due to features of the different types of information and the model s formulation first forecast information is available to all cit ags throughout the simulation while evacuation orders and environmental cues only become available to a subset of cit ags those whose closest official issues evacuation orders or who experience 34 knot winds respectively later in the simulation second forecast information is more influential because cit ags can modify their information behaviors in response to the hazard risk more specifically when cit ags receive forecast information that begins to signal risk they may decide to collect information and assess risk more frequently they are then more likely to obtain information from evacuation orders and or environmental cues soon after it becomes available at times that are closer to the peak of their risk function without forecast information playing this role many cit ags wait 12 or more hours between active times which may lead to significant delays before they obtain and process evacuation orders and environmental cues in this way the forecast information in the model helps cit ags become more attuned to the risk and access risk information more frequently by the time evacuation orders are issued and environmental cues are felt increasing their likelihood of evacuating similar behaviors occur in the real world in which risk information available at earlier stages of a hazard threat primes people to obtain and understand subsequent information e g mileti and sorensen 1990 dash and gladwin 2007 morss et al 2017 demuth et al 2018 however relatively little is known about how these types of behaviors influence system level patterns this illustrates how modeling laboratories such as chime abm v1 can provide a toolkit for studying such interactions in a simplified context to build understanding that can be used to interpret real world information and decision dynamics morss et al 2017 the results in table 7 also illustrate how the influence of different types of information in chime abm v1 varies spatially based on how the spatial attributes of the information intersect with the model algorithms as indicated by the forecasts only and orders only maximum weighting rows both forecasts and evacuation orders affect cit ag evacuations in all six impact zones however their influence is much larger in coastal zones where cit ags are more likely to receive evacuation orders and believe they are in an evacuation zone within coastal areas forecasts and evacuation orders have less influence farther from the storm s track in the 34 knot zone 8 8 as discussed in section 4 1 evacuation rates are higher in the 34 64 knot zones than in the 64 knot zones because of the asymmetric evolving nature of the storm which leads to an asymmetric distribution of these zones across the populated model domain fig 6 combined with the limited model domain and the averaging of evacuation rates across the west and east portions of florida as indicated by the env cues only maximum weighting row on the other hand environmental cues only affect evacuations in the 34 64 knot and 64 knot zones they have no effect in the 34 knot zones where cit ags do not receive any environmental cues and they are most influential in the 64 knot zones where winds are typically greater than 34 knots for a longer period of time than in the 34 64 knot zones unlike forecasts and evacuation orders environmental cues have similar effects on evacuations in coastal and inland zones the effects of varying the cit ag information weightings on the timing of cit ag evacuations is depicted by the colors of the symbols in fig 9 and the timing histograms in fig 10 when cig ags weight forecast information highly right side of fig 9 second plot from left in fig 10 evacuations peak between approximately 78 30 hours before storm arrival when evacuation orders are cit ags primary source of risk information and weighted highly third plot from left in fig 10 the evacuation peak shifts to 24 48 hours before arrival this occurs because cit ags have no information available to signal risk until after officials evacuation order window opens and because cit ags are not primed by the forecasts to quickly obtain and assess this risk information when environmental cues are cit ags primary information source top left of fig 9 right plot in fig 10 evacuations peak only a few hours before the storm s arrival because this is when environmental cues manifest and again because cit ags have not been primed by earlier information these results further illustrate the important role of forecasts and evacuation orders for motivating timely protective behaviors in the modeled system as in the real world 4 3 varying potential timing of evacuation orders issued by public officials now we investigate the effects of modifying a key component of the public official agent algorithms the timing of their evacuation orders table 6 these experiments build on the results examined in sections 4 1 and 4 2 related to the model s behavior and the roles of different types of information in the system they also begin to explore interactions between the evolving forecast uncertainty and the dynamics within the multi agent model the results are summarized in table 8 which presents cit ag evacuation rates in the six impact zones for the full set of experiments to examine aspects of these results in greater detail fig 11 depicts the timing of officials evacuation orders black dots and cit ag evacuation decisions histograms for a subset of the experiments the results for the officials evacuation orders in fig 11 show that as expected given the ideal forecasts and the model s formulation shifting the opening of officials evacuation order window later closer to storm arrival leads to later issuance of evacuation orders it also typically leads to fewer officials issuing evacuation orders this occurs primarily because as the time until storm arrival decreases the track forecast uncertainty decreases which means that fewer coastal counties intersect with the forecast cone of uncertainty fewer officials therefore decide that evacuation orders are needed the results in table 8 show that shifting the potential timing of evacuation orders later also leads to fewer cit ags deciding to evacuate this effect is most prominent in the coastal zones where as discussed in section 4 2 evacuation orders have the largest influence on evacuations fig 11 indicates that as one would expect shifting the timing of evacuation orders changes the timing of cit ag evacuations together table 8 and fig 11 show that shifting evacuation orders later decreases earlier evacuation decisions and shifts some but not all of those evacuations later closer to storm arrival building on section 4 2 this set of experiments further elucidates the influence of different types of information on evacuations for example for the coastal 64 knot zone results with earliest 24 hours in fig 11 3 peaks in cit ag evacuation timing are evident at 30 54 hours 18 24 hours and 0 6 hours prior to storm arrival the first peak is due to forecast information and the latter 2 peaks correspond to the times at which evacuation orders and environmental cues become available to signal risk in addition these results depict how the model simulates the tradeoffs between officials issuing evacuation orders earlier when the track uncertainty is greater versus waiting until closer to the storm s arrival when the forecast uncertainty is reduced the former leads to more officials issuing evacuation orders which increases the percentage of cit ags evacuating from both high impact areas and areas that end up not experiencing significant impacts from the storm the latter leads to more geographically targeted evacuation orders but a lower percentage of cit ags evacuating from high impact areas it also leads to later evacuations giving cit ags less time to complete the evacuation process this illustrates how this type of model can be used to explore in a simplified context the effects of different communication and decision strategies by professionals during hazardous weather threats 4 4 varying geographical distribution of citizen agent populations next we relax one of the idealizations in the experiments in sections 4 1 4 3 and investigate the model s behavior when the random geographical distribution of cit ags is changed to a realistic geographical distribution table 6 we present these results to explore the effects of using a more realistic less idealized model setup and to help interpret the results from subsequent experiments with realistic population distributions fig 12 summarizes cit ag evacuation decisions for the two model configurations to examine the spatial patterns in greater detail fig 13 depicts a map of cit ag evacuation decisions for a single completed simulation with a realistic cit ag geographical distribution overall the simulations with a realistic population distribution exhibit some patterns similar to those discussed above for a random population distribution for example cit ag evacuation rates remain higher in coastal than in inland impact zones however on average a much larger percentage of cit ags decide to evacuate in the realistic population distribution simulations there is also a shift in the spatial pattern of evacuations including a decreased evacuation rate in the coastal 64 knot zone and an increased evacuation rate in the coastal 34 knot zone more in depth investigation reveals that this counter intuitive pattern arises from how the geographical distribution of florida s population intersects with the forecasts the forecast uncertainty and the storm s eventual track and impact zones for example in the realistic population distribution simulations a large number of cit ags are located in the miami area in southeast florida fig 13 this region is in the forecast cone of uncertainty for the idealized charley forecasts for much of the simulation period and so evacuation orders are issued in this area and many coastal cit ags decide to evacuate blue circles in southeast florida in fig 13 based on the storm s track and size however southeast florida ends up not experiencing 34 knot winds thus in the realistic population distribution simulations a significant number of cit ags located in evacuation zones in the miami area make decisions to evacuate that in retrospect were unnecessary in fig 12 this appears as a larger evacuation rate in the coastal 34 knot impact zone as these results illustrate the uneven distribution of population in the real world complicates analyzing spatial patterns of evacuation decisions using aggregated metrics more generally the intersection between specific hazard tracks or forecasts complex coastal geography and clustered populations can lead to decision patterns that are difficult to understand and attribute even in this simplified model world in the real world which has many additional complexities understanding patterns in protective decision making is even more challenging this further indicates the potential value of this type of modeling laboratory where different components can be simplified or modified systematically to run a suite of experiments 4 5 varying the storm and forecast skill historical and ideal forecasts for hurricanes charley and wilma finally we explore the effects of using historical rather than idealized forecasts and of modifying the storm all experiments shown in this section use a realistic rather than random geographical distribution of cit ags table 6 using realistic forecast information adds a further complexity to the model simulations by adding a new dynamical component evolving imperfect forecast information that is present in real weather forecast information and decision systems these experiments also begin to investigate scenarios of interest to stakeholders such as meteorologists or emergency managers by exploring how differences in storm characteristics and forecast information can propagate through the multi agent system and translate into different patterns in public evacuations the top panel of fig 14 shows results for hurricane charley the storm used in the experiments in section 4 1 4 4 with historical forecasts comparing these results with those for parallel simulations with ideal forecasts lower panel of fig 12 we see that the mean evacuation rate is lower in the historical forecast simulations especially in the coastal 34 knot impact zone in other words as one might expect given the less consistent risk information imperfect forecasts lead to fewer cit ags deciding to evacuate the middle and lower panels in fig 14 show results for simulations with hurricane wilma for ideal and historical forecasts like charley wilma made landfall in southwestern florida but farther south and with a more west to east track fig 6 wilma was also a much larger storm than charley and so its 64 knot and 34 64 knot winds cover a much larger portion of the model domain comparing the wilma and charley results for ideal forecasts middle panel of fig 14 and lower panel of fig 12 the overall pattern of evacuation rates is similar except that the wilma evacuation rates are much lower in the coastal 34 knot zone in other words a much lower percentage of cit ags in the coastal 34 knot zone decide to evacuate unnecessarily in the ideal forecast simulations for wilma than for charley this occurs because unlike charley wilma is a large enough storm that most of the areas that are within the cone of uncertainty several days before landfall including the miami area end up experiencing 34 knot winds thus few cit ags in the 34 knot impact zones for wilma receive information indicating that they are at high risk and decide to evacuate this illustrates how and why given similar forecast track uncertainty and decision algorithms more people are likely to make evacuation decisions that turn out to be unnecessary for a smaller hurricane fewer people end up experiencing strong winds and other impacts similar to the charley results discussed above running wilma simulations with historical forecasts produces lower evacuation rates than simulations with ideal forecasts middle and lower panels of fig 14 in fact the wilma simulations with historical forecasts produce an overall cit ag spatial evacuation pattern similar to that which hurricane forecast and evacuation professionals might wish to see with higher evacuation rates in higher impact zones as the other results presented show however this pattern is not an inherent outcome of the model s dynamics instead it is produced by how the evolving imperfect and uncertain forecasts and the storm s track and winds overlay onto the unevenly distributed population this illustrates the challenges of understanding the dynamics that lead to different outcomes in realistic hurricane situations by enabling systematically perturbed experiments in more simplified contexts this type of modeling laboratory can help build new understanding about the interactions among evolving environmental hazards hazard information information flow and protective decisions given the goals of the work presented here we did not try to adjust the model to match real world evacuation rates and robust empirical data on spatially distributed evacuation rates for these two storms is not publicly available however the available empirical data indicates that the model is to first order generating reasonable evacuation rates in coastal high impact zones for example a survey conducted by smith and mccarty 2009 after charley made landfall found that 36 of the sample in charlotte county florida the coastal county where charley made landfall with 64 knot winds reported evacuating another post storm study of charley by baker 2005 found that 22 53 of the sample in areas similar to the model s coastal 34 knot zones and 12 33 of the sample in areas similar to the model s inland 34 knot zones reported evacuating for wilma a post storm survey conducted by solis et al 2010 in 3 southeastern florida counties all of which experienced 64 knot winds found that 32 of the sample evacuated comparision with fig 14 indicates that these evacuation rates are similar to those generated by the model in our coastal 34 64 knot and 64 knot zones for historical forecasts for the 2 storms as noted in section 4 1 however the evacuation rates in inland 34 knot zones produced by the model in its current configuration are lower than those in the real world 5 summary and discussion this article conceptualizes and implements an agent based model for studying the modern hazard information and decision system in the context of hurricanes approaching the us coastline the model includes multiple types of agents who interact with each other and with their physical and informational environments to access interpret and decide how to respond to evolving hazard information in a theoretically and empirically informed way the resulting digital laboratory provides opportunities to study this complex dynamic system from a new perspective complementing recent related work using other methods e g lee et al 2009 gudishala and wilmot 2010 meyer et al 2013 2014 ruin et al 2014 morss et al 2015 2017 lazrus et al 2016 bostrom et al 2018 demuth et al 2018 we use the modeling laboratory to ask how are the spatial and temporal patterns of protective decisions during hazardous weather threats affected when heterogeneous agents with semi realistic decision rules access share and interpret evolving forecasts and other hazard information specifically we perform experiments investigating how the model s behavior and outcomes change when key agent parameters and the geographical population distribution hurricane evolution and forecast skill are varied the results provide insight into how and why evacuation patterns can arise when interacting agents exchange and respond to evolving uncertain information from different environmental and social sources they also illustrate how interactions among evolving information uncertainty and decisions can produce complex emergent dynamics for example as agents respond to information indicating decreasing uncertainty about the potential threat feedback loops can lead to rapidly increasing risk assessments as a storm approaches as the experiments further show including factors that add complexity and realism to the model such as the coastal geography of a region such as florida the impacts of an asymmetric evolving storm non uniform geographical population distributions and evolving imperfect forecasts can complicate interpreting the model output this demonstrates the value of this type of modeling laboratory for building in depth understanding about hazard information and decision dynamics by enabling systematic manipulation of factors that cannot easily be controlled in the real world it also underscores the potential of this type of interdisciplinary modeling for addressing questions of interest to forecasters emergency managers and other stakeholders as well as researchers by allowing experiments in a wide range of scenarios the research described here advances scientific capabilities and knowledge in several ways first by adapting existing theoretical models and empirical understanding of hazard information flow and decision making for use in computational agent based models the work provides a new approach for exploring how evolving hazard information and decisions interact to create broader patterns of interest second the modeling framework discussed here provides a unique toolkit for exploring the effects of different hazard forecast information including timing and uncertainty risk related decision making and information network topologies on patterns in social decisions experiments that are impossible to perform in the real world further this research demonstrates how agent based modeling can be used to study systems in which coupling with evolving environmental and social information contributes to the system dynamics along with coupling between the natural and human system in these ways this study aims to extend model based hazards research toward work with theoretically and empirically informed agent based modeling in complex dynamic information contexts based on the research reported here we propose several areas for future related work first chime abm can be used to address additional research questions related to hurricane forecasting information communication and evacuations in particular the forecast information currently represented in the model is much simpler than that typically available in the real world today thus one possible extension for future experiments is incorporating additional or more complex forecast information and or representations of forecast uncertainty this might include coupling the agent based components of the model with more complex hazard and forecast information inputs e g from numerical modeling of hurricanes and their impacts to develop a more complete coupled physical social modeling laboratory another possible extension is to simulate different interpretations of forecast information content e g by having agents modify their representations of the forecast to explore how more complex aspects of information flow and interpretation interact to influence decisions the model could also be extended to study sequences of hurricanes during one or multiple years in order to explore the dynamics of how hurricane related experiences influence attitudes and behaviors in subsequent storms or other longer term aspects of hazard risks and resilience another area for future research is revising the model structure and its components including the agent algorithms hazard information social and information networks and evaluation of impacts to address idealizations in the current version in doing so it is important to consider the potential trade offs of adding different forms of realism and complexity in the context of the research goals we designed chime abm to be capable of using quasi realistic geography populations storms and forecasts imported using real data nevertheless as the results in section 4 show abstractions and simplifications can enhance the interpretability of the model output and the value of the model in elucidating key dynamics of the system of interest the model can also be adapted to study other hazards for which the evolution of different types of information and its exchange among multiple types of actors play important roles for example floods wildfires and volcanic eruptions take place at temporal and spatial scales similar to those of hurricanes the chime modeling environment could readily be adapted to investigate decision making in these contexts tsunamis tornados and flash floods typically unfold more quickly and affect smaller regions chime abm would require more extensive modification to explore information flow and decision making for such hazards an additional potential area for future work is comparing quasi realistic model simulations with observed social data from specific historical hurricanes to refine the model structure and parameters in its current form chime abm is well suited for exploring aspects of the system s behavior by comparing results across sets of simulations qualitative comparisons with prior related research and empirical evacuation data suggest that the model produces reasonable evacuation patterns but it does not attempt to realistically represent the multitude of factors that influence real world individual and household evacuation decisions in specific situations more in depth quantitative comparisons with real world data may therefore help improve the model s capabilities to address practical questions of interest however given the limited availability of the types of comprehensive empirical data required to perform such comparisons new data sets may need to be collected or compiled real world decisions are influenced by many complicated factors that must be simplified in any modeling approach since little is known about the emergent dynamics of the type of system being studied here our aim was to begin developing fundamental understanding that could form building blocks to be expanded on in future related modeling work given these goals chime abm is purposefully abstracted from the real world in multiple ways and so it has many limitations if evaluated from the perspective of simulating actual hurricane decisions and outcomes the model was developed however using theory research findings expertise and data from several relevant disciplines the modeling effort is also intersecting with ongoing research on hurricane hazard predictability information flow and decision making being conducted as part of a larger multi method research project morss et al 2017 thus the model is both informed by and feeds back into empirical research interpreted in conjunction with other work investigating real world hazard information flow and decision making we propose that modeling research such as that conducted here has significant potential to develop new understanding identify strengths and weaknesses in hazard forecasting and risk communication and recommend areas for improvement software data availability chime abm was implemented in the freeware agent based modeling platform netlogo version 5 3 1 and later updated to netlogo version 6 0 the model code supporting documentation and input files are archived on the comses net library openabm com at the url https www comses net codebases 5504 releases 1 4 0 declarations of interest none acknowledgements the authors acknowledge sean bergin wendy cegielski nicholas gauthier and grant snitker for their assistance with the development and testing of the agent based model we also thank our collaborators on the larger project especially heather lazrus olga wilhelmi christopher davis kathryn fossell david ahijevych chris snyder leysia palen and kenneth anderson for their contributions to this research the work reported here was supported by national science foundation award ags 1331490 the national center for atmospheric research is sponsored by the national science foundation 
