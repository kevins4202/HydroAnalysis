index,text
25790,integrated assessment models iams are important tools to analyse cross sectoral interdependencies and the use of global resources most current tools are highly detailed and require expert knowledge and proprietary software to generate scenarios and analyse their insights in this paper the complementary global least cost user friendly clews open source exploratory glucose model is presented as a highly aggregated global iam open and accessible from source to solver and using the osemosys tool and the clews framework the model enables the exploration of policy measures on the future development of the integrated resource system thanks to its relatively simple structure it requires low computational resources allowing for the generation of a large number of scenarios or to quickly conduct preliminary investigations glucose is targeted towards education and training purposes by a range of interested parties from students to stakeholders and decision makers to explore possible future pathways towards the sustainable management of global resources keywords clews osemosys integrated assessment modelling sustainable development education software availability name of the software open source energy modelling system osemosys short code contact information email address osemosys gmail com year first available 2008 hardware required windows macos linux 8 gb ram software required gnu linear programming kit glpk availability open source and open access program language and size gnu mathprog 92 kb the latest release of the open source energy modelling system osemosys is available in the dedicated github repository https github com osemosys osemosys gnu mathprog releases tag v1 0 data availability a detailed list of original data sources and related references is available as part of the electronic supplementary material submitted with this paper the data files used as input to the glucose model are provided as follows names dataglucose baseline txt dataglucose 2degree txt dataglucose food txt dataglucose materials txt dataglucose total txt form of repository txt files size 2 mb access form github repository https github com kth desa glucose 1 introduction the integrated analysis and planning of energy land and water resource systems are of utmost importance for reaching sustainable development goals traditional management of these systems as separate entities cannot address the many trade offs and synergies between actions within each bazilian et al 2011 integrated assessment models iams have been used to facilitate the analysis of policies and strategies on the environment and development of the global economy and society gambhir et al 2019 hamilton et al 2015 the history of integrated assessment modelling can be dated back to the early 1970s when the world 3 model meadows et al 1972 was developed as a global systems dynamics model coupling social and physical elements of the world system rotmans 1998 nordhaus 1979 linear programming energy model combined energy conversion emissions and co2 concentrations to investigate the global effects of the energy system on the environment this model later evolved to become the more economy centred dynamic integrated climate economy dice model nordhaus 1992 in the 1980s the rains model alcamo et al 1991 focused on the representation of acid rain in europe from their emissions to their final impact on the environment the rains model was the first of its kind in promoting and adopting a close dialogue with policymakers in order to better understand how to shape the model to make it more useful for the policy making process from 1990 the need to better understand climate change pushed for a new class of models focusing on the representation of climate from the sources of emissions to their impacts on the global environment of which image stehfest et al 2014 was the first example weyant et al 1996 in 1996 iams were first featured in the second assessment report of the international panel on climate change ipcc ipcc 1996 and they have been explicitly recognised and used to analyse climate change mitigation at the global level ever since gambhir et al 2019 schwanitz 2013 over the past years iams have received criticism for their complexity opaqueness high entry barriers and computational intensity gambhir et al 2019 parker et al 2002 rotmans and van asselt 2001 stanton et al 2009 van der sluijs 2002 so far iams have also largely escaped the increasing interest towards open source collaborative modelling efforts huppmann et al 2019 pfenninger et al 2017 today s open source modelling frameworks are mature tools that can provide good alternatives to similar well established proprietary or commercial tools groissb√∂ck 2019 pfenninger et al 2017 well documented why there is a need for models and data to be increasingly open in particular open source tools allow quality assurance efforts to be shared among a wider range of interested people including governments thus also enhancing policy science interaction and the active engagement of decision makers towards a more transparent and collaborative policy making process pfenninger et al 2017 acknowledging the increasing polarisation within society regarding climate policy chinn et al 2020 harvey et al 2018 and the urgency required to mitigate climate disaster there is a need to improve communication and education around iams to increase transparency and lower the barrier to entry for those interested in integrated resource systems modelling it is believed this will increase trust in scenarios improve the quality of science and public and scientific debate around the use and application of integrated assessment models the global least cost user friendly clews open source exploratory glucose model presented in this paper provides an open source transparent lightweight complement to the well known iams with a focus on communication and education an initial version of the model was featured as part of the prototype global sustainable development report un 2014 glucose intends to provide a balance between a biophysical oriented and techno economic aggregated approach as defined by rotmans 1998 and edmonds et al 2012 the model is based on the climate land energy and water systems clews framework ramos et al 2021 with the addition of a simplified representation of key materials sectors the clews framework was first used to investigate multiple resource systems and their relation with the climate system bazilian et al 2011 iaea 2009 the framework has been applied in several case studies at regional national and urban levels ramos et al 2021 glucose presents the world as a single global region to allow the user to focus on understanding cross sectoral interdependencies and links it aims at educating and training decision makers and stakeholders in understanding the implications of sectoral policies on different resource systems it also intends to allow them to generate multiple scenarios providing the basis for transparent and informed discussions on a wide range of global issues glucose aims at demonstrating how highly aggregated and simplified models can be used to understand the key concepts of the interdependencies of resource systems and the related need for integrated planning and policy development over the long term in addition it aims at informing on sustainable development pathways and the need for enhanced policy coherence in line with goal 17 partnership for the goals target 14 of the sustainable development goals sdgs oecd 2017 tosun and lang 2017 tosun and leininger 2017 un 2019 this paper provides the detailed methodology of the glucose model it validates the model against statistical data and compares its behaviour with results from the ssp2 or middle of the road pathway fricko et al 2017 rcp 6 0 it presents and compares four different scenarios looking at current policies and the impact of different additional measures including greenhouse gas ghg emissions reduction a shift towards a more sustainable and healthy diet a decrease in materials demand due to design efficiency improvements and a combination of such measures 2 the global least cost user friendly clews open source exploratory glucose model 2 1 the glucose model background glucose was first developed in close collaboration with the united nations un as a global integrated assessment model based on the clews framework bazilian et al 2011 that could complement the discussion on the formulation of the sustainable development goals and help in bridging the gap between science and policy un 2014 this work aimed at creating a transparent tool to act as a simplified testing ground for policies and allow the visualisation and assessment of different policy pathways in regard to sustainable development on a global scale potential trade offs and synergies between climate land energy water and material sectors were identified with the analysis the model was developed using the open source energy modelling system osemosys howells et al 2011 allowing for transparency accessibility and easy expansion upon completion of the different stages of model development osemosys is a long term modelling framework based on linear optimization that computes the least cost optimal solution for the modelled system the tool is open and accessible from source to solver and it has a large international community of users and developers that support and assist each other via an online forum gardumi et al 2018 the initial version of glucose was featured as part of the prototype global sustainable development report un 2014 where it provided a results comparison between the full model outputs and the results of its energy module only this comparison showed how the interlinkages between the different modules energy land and materials allowed for a better understanding of the dynamics between different resource systems un 2014 additionally results from this initial version of glucose were used to develop interactive results visualisation and training materials as part of the un modelling tools for sustainable development initiative launched in 2016 united nations n d 2 2 the glucose structure and main characteristics the glucose model consists of one single global region and three interlinked modules representing three main sectors of the global socio economic system with direct connections to the environmental system energy sector land and food sector and materials production sector the model so far does not yet include a hydrological module but accounts for the water consumed across the system by the different technologies and sectors represented the different sectors of the model are coupled within one single framework rendering manual or iterative data input between modules unnecessary fig 1 illustrates the conceptual model design where all sectors are coupled with water and emission tracking as well as other cross sectoral linkages the rationale and model structure selected resembles that of the targets model rotmans et al 1997 the energy module represents key techno economic parameters that characterize different supply technologies including renewable ones and carbon capture and sequestration ccs equipped generation the land and food module consisting of land resources and food transformation technologies represent the food and biomass production and conversion chain from land availability to final food demand the material module represents the aluminium cement iron and steel pulp and paper chemicals and petrochemicals and fertilizers material production sectors for which reasonable input data were available the water consumption estimated in the model accounts for the amount of water needed by the different modules to perform the transformation processes represented no constraint on water availability is considered as the water resource balance is not represented yet in the model emissions are accounted for over the entire model at the resource level in terms of greenhouse gas ghg emissions and can be constrained annually the modelling period considered for the analysis spans from 2010 to 2050 with yearly time steps and three time slices per year representing average day night and peak hours of energy demand respectively in table 1 to clarify the scope of glucose a comparison of the structure main characteristics and features of glucose against other similar biophysical oriented iams namely message globiom image and magpie is provided glucose presents the global resource systems in a highly aggregated geographical way using one global region this can facilitate the data collection process as less data are needed to characterize the system however the downside is that major data sources and statistics are provided at a national level therefore the challenge is to collect or derive values that are representative of the global average data having one single global region represented in the model also makes it easier for less experienced modelers or analysts to better understand the structure of the model and the cross sectoral interlinkages this further emphasises the suitability of glucose for educational and training purposes finally the lower level of detail and the simplified structure of the model mean glucose has low computational requirements and a fast solution time thus speeding up the results generation process and facilitating multiple scenarios generation and stochastic analysis 2 2 1 the reference resource system fig 2 shows the reference resource system rrs diagram of glucose the diagram provides a simplified overview of the model structure with each module representing a different resource system and interlinkages made explicit between them primary resources are represented on the left hand side of the rrs which indicates the resource level of the system the boxes in the middle represent the transformation technologies that populate the primary and secondary system s levels and produce the final products defined as demands on the right hand side of the rrs each arrow in fig 2 illustrates the flow of resources between different levels and across sectors in the system ghg emissions and respective emission sources as considered in the model are indicated by the black cloud icons blue oval shapes represent the accounting of water consumption in the system and the related level in which it occurs the dotted lines identify each of the modules of the glucose system a more detailed schematic representation of each module is provided in appendix a 2 2 2 the baseline scenario the glucose baseline scenario was initially based on current and expected policy measures and global techno economic trends as reflected in the etp 2012 6 c scenario from the energy technology perspectives report iea 2012 in addition as part of the sim4nexus project european commission 2020 funded by the european union s horizon 2020 research and innovation programme the glucose baseline scenario has been adjusted to be in line with the major socio economic assumptions behind the definition of the ssp2 rcp6 0 scenario fricko et al 2017 this means that long term demographic gross domestic product and related demand projections have been aligned to the ones defined for the ssp2 rcp6 0 scenario 2 2 3 the energy module in the energy module 1 1 see appendix a figure a 1 the final energy demand has been divided into electricity heat and transport 2 2 see appendix b a for details on the energy module demands heat industrial and residential demand are treated separately to facilitate the linkage to the materials production module the transport sector considers available projections for maritime and aviation railway and roadway travel demand data used to characterize the energy module technology specifications and energy demand projections were primarily taken from international energy agency iea publications 3 3 a complete list of glucose input data sources by sector is available in tables ii 1 supplementary material ii the power generation sector includes 21 technological options while the heat generation sector includes 15 technological options in both instances centralised and decentralised alternatives are available the model also considers 5 combined heat and power technologies 4 4 the list of technologies names and related description as defined in the glucose model is available in tables i 1 supplementary material i additionally the module is constructed to assess future investment potential in unconventional infrastructure and technological shifts in primary energy supply such as coal or biomass gasification coal to liquids and gas to liquids and in generation such as ccs lastly in the transport sector the model can assess the potential for market penetration of technologies using biofuels or electricity as fuel however at this point competition only occurs between technologies within each transport mode i e the share of each transport mode is defined exogenously this is the most detailed module within glucose in terms of technological options this can be attributed to the fact that the provision of energy services is responsible for the majority of ghg emissions worldwide 73 2 of total ghg emissions ge and friedrich 2020 ritchie and roser 2020 thus it is reasonable at this point of model development to emphasise on this sector 2 2 4 the land and food module the principal purpose of the land and food module is to establish linkages between agricultural production its associated land use land degradation and energy use and biomass production for energy and food purposes and materials production also this module provides the only natural source of carbon sequestration in the model by characterising the forest land with a negative ghg emission factor land is represented through six principal land categories 5 5 see appendix a figure a 2 which are aggregated by different uses from agriculture to forest to other i e land not classified as agriculture or forest according to fao definition fao 2019 all the land categories apart from other produce biomass as output which can either be processed and transformed to satisfy the demand for crops and livestock distinctively 6 6 see appendix b b for details on the land and food module demands or be used for energy and industrial purposes respectively besides the these links with between the energy module and the food sector the land module is linked with the materials module via two key industries the fertilizer production that is that is assumed to be used in the agriculture land for farming and the pulp and paper industry that uses the forest biomass produced from the forest land as input material demand for food is coupled with population projections from the image model data pbl netherlands environmental assessment agency 2019 stehfest et al 2014 provided as part of the ssps database fricko et al 2017 iiasa energy program 2018 riahi et al 2017 3 2 2 5 the materials module even though material production was not separately mentioned in the clews modelling framework bazilian et al 2011 iaea 2009 the energy intensity of this sector and its environmental impact as well as the potential for material and energy efficiency improvements in this sector are worthy of consideration allwood et al 2012 iea 2007 sustainable use of materials implies a reduction in material consumption and associated energy flows such a reduction can be achieved by addressing the supply side such as through efficiency improvements in manufacturing processes and products design or the demand side for instance by means of altered consumer behaviour this can be implemented to a considerable extent through adaptation in lifestyles and societal attitudes improved system design cooperation between industries for a decrease in waste heat and material losses and policy frameworks that facilitate such changes allwood et al 2012 this sector is interconnected with the land and energy modules on several levels extraction of raw materials results in land degradation ghg emissions and requires energy input the transformation of raw materials into consumer products is an energy intensive process while market globalisation means that products need to be shipped across great distances from the supply source to the manufacturing location and finally to the point of demand simultaneously equipment requirements in energy and agricultural production processes affect the demand for certain materials such as aluminium cement iron and steel including all these aspects in a modelling framework can help guide decision making however at the current phase of glucose model development the materials module acts as an energy consumer 7 7 see appendix a figure a 3 inspiration for the choice of materials originated from allwood and cullen s work allwood et al 2012 the material industries considered i e pulp and paper iron and steel aluminium cement fertilizers and petrochemicals industries take in energy in various forms i e heat electricity fuels and use it either to drive conversion processes or as feedstock weirich 2013 nevertheless efficiency improvements have been assumed based on existing projections iea 2012 a more detailed description of each industry type is given by weirich 2013 2 2 6 input data and sources to ensure transparency and retrievability of the information and data used to characterize the glucose model and to define its baseline scenario an effort was made to use open access data as much as possible and to correctly document the corresponding reference sources 3 one of the main issues encountered in the data collection was to find representative data for the aggregated global region represented in glucose in some cases it was necessary to extrapolate data from country or region specific sources and apply them at the global level some databases provided estimates of aggregated global values based on country related data collection and estimations conducted in collaboration with national governments and institutions that inform international statistical data as for the case of the faostat database fao 2019 also between different sources some discrepancies were identified for the same data and in the definition of the type of data considered for instance when comparing figures of the land areas allocated to different uses various aggregated global data sources disagree on the actual value of land area and in the definition of the land use considered 8 8 see supplementary material iii tables iii 1 for land use data and tables iii 2 for ghg emission data in addition the discrepancies between global aggregated values and national or local figures indicate that there are significant uncertainties among the different sources for this reason for the characterization of glucose global aggregated data were used when available when global data where not retrievable average values from a selection of either country or region specific data sources were used instead 2 3 glucose base year validation this section validates the base modelling year of glucose against statistical data to show that the model has been correctly defined in its starting year to be in line with the relevant sources as the energy module is the most detailed in glucose emphasis is put on this module only due to the lack of more aggregated and consistent statistical data sources for the materials and land and food modules as currently represented it is challenging to establish meaningful comparisons for other model results fig 3 shows the share of total primary energy supply tpes by energy source as obtained from the base year of the glucose model compared to the statistical data from the iea world energy balance 2020 for the base year 2010 the energy mix extracted from the glucose results correctly reproduces the historical energy mix as retrievable from iea s statistical data the major difference between the two energy mixes is in the share of biofuels and waste this can be explained by the fact that in glucose the use of waste as fuel is not considered whereas it is accounted for in the iea data fig 4 compares the share of ghg emissions by sector in the glucose results against the statistics provided by the world resource institute wri climate data explorer wri 2015 for the base year 2010 in this case the share of energy related emissions is approximately 6 higher in the glucose results than in the wri data whereas the share of industry and land and food related emissions are approximately 4 and 3 lower this can be explained by how glucose is structured where the fuel related emissions are accounted for at the primary resource level and the energy conversion and production processes are represented in the energy module therefore some of the emissions linked to the production of heat or electricity to be used in the materials or the land and food modules are accounted for in the energy module where the heat and electricity are produced 2 4 comparing glucose baseline scenario results and long term trends with other iams in this section the glucose baseline scenario results are compared with the iams that provided quantitative projections to define the ssp2 rcp6 0 scenario fricko et al 2017 iiasa energy program 2018 this comparison ensures that the glucose modelling is in line with the key long term trends as provided by the models featured in the ssps database iiasa energy program 2018 riahi et al 2017 kling et al 2017 and rising 2020 describe how iams can support the nexus analysis of resource systems management and related infrastructure development by providing quantitative insights on the long term impact of different decisions however models are always subject to prediction errors that might affect outputs and associated insights therefore it is beneficial to conduct model comparison exercises to explore the uncertainties and likelihood of different outcomes kling et al 2017 in an attempt to harmonise the modelling efforts and pursue integration across different climate change research communities several efforts of comparing and assessing modelling inputs and results have been recorded over the past decades the integrated assessment modelling consortium iamc founded in 2007 has been focusing on the development of four common scenarios known as the representative concentration pathways rcps van vuuren et al 2014 to be used in climate modelling as the basis for providing insights to the ipcc fifth assessment report on the potential impact of climate change over the short and long term iamc 2020a iamc 2020b iiasa 2019 the shared socio economic pathways ssps instead have been developed to facilitate the integrated analysis of future climate impacts vulnerabilities adaptation and mitigation riahi et al 2017 the ssps consist of five narratives that focus on different global socio economic development pathways these narratives in combination with the rcps have been translated into modelled scenarios by an initial group of six iams van vuuren et al 2014 the selected models span across different iams types from high resolution biophysical oriented and policy evaluation models namely image message globiom gcam to macroeconomic oriented and policy optimization models namely aim witch remind these models generated projections for different resource sectors linked to the ssps riahi et al 2017 within the ssps context the ssp2 narrative scenarios are characterized by a middle of the road development in mitigation and adaptation challenges space fricko et al 2017 in practice this means that the social economic and technological trends described in the ssp2 narrative are expected to follow historical trajectories in the future leading to a moderate development trend that does not foresee major political instabilities or disruptive events fricko et al 2017 within the ssp2 narrative the ssp2 rcp6 0 scenario combines the ssp2 global socioeconomic development trajectory with the climate development trend as described in the rcp 6 0 this trend envisions a mitigation pathway that will reach by 2100 a radiative forcing target of 6 0 w m2 corresponding to keeping the global mean temperature increase below 4 c in relation to pre industrial levels fricko et al 2017 ipcc 2015 moss et al 2010 van vuuren et al 2014 for the comparison between glucose and other iams with regards to the baseline results versus the results from the ssp2 rcp6 0 scenario the energy sector was taken as the main reference for this sector detailed results are provided in the ssps database which allows for a closer understanding of how the sector is expected to evolve in the different models considered in addition thanks to its more detailed representation the energy module of glucose allows for a closer look into the reasons behind similarities and differences in model behaviour looking at the results for the global installed electrical capacity fig 5 the glucose model results follow a similar trend to the ssp2 rcp6 0 scenario results by falling within the range provided by other models this is also valid for the global secondary fig 6 left and final fig 6 right electricity production levels as shown in fig 6 the glucose results seem to follow the overall trends projected by the other ssp2 rcp6 0 models outputs looking at the primary energy production level in fig 7 left instead glucose results present a lower increase in their trajectory than the average trends presented by the ssp2 rcp6 0 models with a difference of approximately 15 in 2050 however for what concerns the total final energy production in fig 7 right again glucose results seem to be in line with the trends coming from ssp2 rcp6 0 models this can be explained by the use of different assumptions for the technology costs and efficiencies projections considered in the model the expected availability of variable renewable energy sources and the detail of technological representation such assumptions are at the core of each modelling effort they strongly depend on the data sources used to characterize the resource level in a model and affect its behaviour particularly in the case of a least cost optimization tool from the comparison exercise presented above it can be deduced that the glucose model represents the long term evolution of the integrated system producing results that are in line with the main trends identified by the iams tools featured in the ssp database 3 model application investigating global policy scenarios in glucose from the glucose baseline scenario already described in 2 2 2 four different global policy scenarios were developed the 2degree food materials and total scenarios each of the scenarios shows how the model can capture key inter sectoral dynamics and provide insights on the impact of selected policies on the global resource systems modelled in glucose 3 1 scenarios description the 2degree scenario provides an understanding of the impact of selected techno socio economic policy measures to limit the total amount of ghg emissions in the atmosphere and a related average global mean temperature increase of less than 2 c relative to pre industrial levels by 2100 it is derived from the baseline scenario by lowering the yearly ghg emissions limit and by changing some of the exogenous demands defined in the model namely demand for electricity heat road and maritime and aviation transportation simulating efficiency improvements on the end use side the 2degree scenario is aligned with the etp 2012 2 c scenario and the ssp2 rcp2 6 scenario which combines the ssp2 development trajectory with the rcp 2 6 assumption of reaching a radiative forcing target of 2 6 w m2 by 2100 this corresponds to keeping the global mean temperature increase below 2 c in relation to pre industrial levels fricko et al 2017 iea 2012 moss et al 2010 van vuuren et al 2014 the food scenario investigates the impact of eradicating hunger while ensuring a global shift toward a healthy and sustainable diet and more efficient food production systems it consists of reducing consumption of animal based proteins and dairy products in favour of more plant based proteins vegetables and cereals in addition it assumes a 50 reduction in the waste generated by crops harvesting by 2050 this scenario has been derived from the baseline by changing the final demand for crops and livestock between 2020 and 2050 to simulate a global shift toward the healthy reference diet composition as defined by the eat lancet commission by 2050 willett et al 2019 also the conversion efficiency of crops harvesting technologies has been assumed to linearly increase by 50 by 2050 to simulate a global halving of food losses at the crops harvesting level as a result of a transition towards more sustainable food production systems the materials scenario analyses the impact of optimized products design on the final demands for aluminium cement paper and steel and consequently on the overall resource systems based on allwood et al 2012 best estimates of possible materials design efficiency finally the total scenario combines the policies applied in the 2degree the food and the materials scenarios to investigate the implications of integrated policy planning on the long term evolution of the modelled sectors table 2 provides a summary of the main assumptions characterising the different glucose scenarios the variations in demands across the scenarios are shown in appendix b 3 2 scenarios results key results are compared for the years 2010 2030 and 2050 the results showcase the impact of above mentioned policy scenarios on the systems represented in glucose and the intersectoral dynamics that the model can capture fig 8 shows the absolute values upper graph and relative shares lower graph of total primary energy supply tpes by different resources across scenarios and along the modelled period similarly fig 9 shows the composition of each scenario installed electrical capacity in absolute upper graph and relative lower graph values fig 10 provides an overview of the land cover change fig 11 shows the change in final energy used by the materials sector across the different scenarios in comparison to the baseline finally fig 12 shows the projections of the total ghg emissions from the system for each scenario in the 2degree scenario the overall tpes is reduced by 40 as shown in fig 8 this is partially due to the overall expected reduction in exogenous energy demand linked to the annual ghg emissions limit imposed and the decarbonisation policies modelled in addition in the 2degree scenario there is an increase in investments in low emission energy supply technologies such as nuclear and renewables at the expense of fossil fuel based technologies to reduce the total ghg emissions accounted for in the model this technological change is driven by the need to meet the ambitious emissions reduction target of the 2degree scenario see fig 12 as a result nuclear technology tpes share in 2050 increases from 13 in the baseline to 24 in the 2degree scenario similarly in 2050 the share of primary energy supplied by renewable sources meaning hydro biomass solar wind geothermal and marine resources increases from 17 in the baseline to 39 in the 2degree scenario fig 8 lower graph also the share of electrical capacity available in the system and linked to nuclear and renewable technologies in 2050 increases from 13 to 24 for nuclear and from 17 to 29 for renewables between the baseline and 2degree scenario fig 9 lower graph at the same time the fossil fuel related tpes is reduced by 68 between the baseline and 2degree scenario the corresponding electrical capacity is reduced by 76 largely due to the phase out of coal based energy generation as shown in fig 9 a small amount of biomass with ccs capacity is also installed by 2050 in the 2degree scenario when comparing results between the baseline and the food scenarios one first notable outcome is the increase of tpes from biomass resources as shown in fig 8 by 2050 the amount of biomass tpes increases from 29 3 ej corresponding to 4 of the total in the baseline to 59 ej corresponding to 8 5 of the total in the food scenario similarly as shown in fig 9 in 2050 the biomass based electrical capacity installed in the system is 358 gw corresponding to 2 6 of the total in the baseline scenario and it increases to 759 gw equal to 5 7 of the total in the food scenario such increase comes from the reduced need for agricultural land for food production which in the food scenario is expected to decrease thanks to the shift towards a healthy and sustainable global diet starting in 2020 this is also supported by the increased efficiency of the crops transformation sector fig 10 shows for the food scenario a reduction in the land covered by cropland and pasture of almost 50 by 2050 compared to the baseline consequently by 2050 the amount of land covered by forest double from 3000 million ha in the baseline to approximately 6000 million h in the food scenario therefore also the ghg emission sequestration effect provided by the forest land is increased as a result by 2050 in the food scenario the tpes from fossil fuels increases by 7 compared to the baseline when comparing results between the baseline and the materials scenarios it is interesting to notice that by 2050 the materials design improvements contribute to reducing by 15 the final energy used by the materials sector in relation to the baseline scenario as shown in fig 11 as a result by 2050 the tpes fig 8 is slightly reduced by 4 2 for the materials scenario the mix of primary energy sources changes slightly in favour of more fossil fuels 6 ej to the expenses of nuclear 34 ej in comparison to the baseline while ghg emissions remain constant finally when comparing the results between the total and the baseline scenarios the combined energy food agriculture and materials policies from the 2degree food and materials scenarios respectively act together resulting in a combined positive impact on the modelled system tpes results in fig 8 indicate by 2050 an overall 45 reduction in the total scenario compared to the baseline slightly lower than in the 2degree scenario similarly by 2050 the total installed electrical capacity fig 9 upper graph is reduced by 14 in the total scenario compared to the baseline the share tpes from renewables in 2050 fig 8 lower graph increases from 17 in the baseline to 39 in the total scenario mainly due to a higher share of biomass one key difference between the 2degree and the total scenario results is that for the latter the tpes from nuclear resources is reduced compared to the baseline as a result more fossil fuel resources mainly coal are used in the total scenario compared to the 2degree as shown in fig 8 these results can be explained by the change in land cover presented in fig 10 and the reduction in final energy used by the materials sector shown in fig 11 in fig 10 the total scenario results resemble the ones from the food scenario showing an increase in forest land of approximately 3000 million ha by 2050 in comparison to the baseline thus more biomass is available in the total scenario to the energy sector at the same time the additional ghg emission sequestration effect of the increased forest land allows the model some flexibility for investments in traditional fossil fuel technologies to supply both heat and electricity to the system while fulfilling the emissions reduction target in addition fig 11 shows a 23 reduction in final energy used by the materials sector which gives the model additional flexibility in the use of the energy resources available to meet the related demands while respecting the ghg emission reduction goal the total scenario results show how combined sectoral policies allow the model to opt for a more diverse mix of energy sources and ensure effective and efficient use of resources in the transition towards a highly decarbonized system 4 discussion and conclusion this paper has described the development of a lightweight reliable transparent tool for education and training of broader audiences on the importance of the integrated systems planning approach and cross sectoral policy measures for the achievement of sustainable development pathways glucose unlike other iams glucose is fully open source and accessible from source to solver although lacking a regional representation that would allow a more detailed analysis of the integrated global system the model is able to provide a reasonable understanding of the modelled systems and inform on the interdependencies among global resources similarly to other iams glucose operates by computing the least cost optimal mix of technologies and resources needed to meet exogenously and endogenously defined demands thus providing one optimal solution to the investigated problem and potentially missing to provide an overview of possible future scenarios lund et al 2017 however glucose s aggregated structure facilitates the development of a large number of scenarios that can help testing different boundary conditions provide insights on a wide range of cross sectoral linkages across resource systems and explore solutions to integrated development challenges as an open and documented tool glucose allows users to investigate issues linked to the achievement of possible sustainable futures its aggregated structure also facilitates the communication of key clews framework concepts and aims at stimulating discussions around cross systems implications for the long term management of resources the scenarios presented in this paper intend to showcase the inter sectoral dynamics currently captured by the model and illustrate the key insights the scenarios can provide they assume ambitious policy measures to be applied at the global level therefore they intend to give an overview of the variety of globally relevant issues that the model can investigate through the glucose base year validation presented in section 2 3 it is shown that the energy module and the ghg emission projections of glucose are in line with reference statistical data sources similarly in section 2 4 the comparison of glucose baseline results with the outputs from the iams featured in the ssp2 rcp6 0 scenario runs show that glucose can provide comparable long term trends this confirms the validity of the glucose model as a reliable complement to other iams despite its different structure and granularity this work has highlighted difficulties in collecting reliable data that are consistent across sources to accurately characterize global aggregated models such as glucose global statistical data from well established international sources such as faostat world bank our world in data ipcc provide different figures for the same data or different data classifications this makes it difficult for a modeller to select representative data and to validate it against statistical information since the latter differ among themselves baj≈æelj et al 2013 also presented a similar issue highlighting two main reasons for uncertainties in global emissions data one related to the fact that emissions data are typically estimated and not measured both at the national and international level and the other one that it is difficult to allocate emissions correctly when looking at various emission inventories linked to different sectors baj≈æelj et al 2013 another example of such an issue is presented by larsen et al 2019 who highlighted the difficulty in collecting reliable data for characterising the water energy nexus both at regional and global level this work draws attention to the need for exploring the influence and relevance of the data uncertainty on model outcomes using a simple and aggregated model such as glucose can facilitate such assessment however it raises the question whether working at an aggregated scale might introduce more uncertainty than modelling individual countries separately and then linking them together which further raises the question of how other iams deal with the data uncertainty issue in terms of future work a few improvements can be mentioned one refers to the current representation of the materials module which is still minimal limiting the possibility of exploring sectoral targeted policies and technology changes according to the latest development trends expanding the model with technology and price alternatives similarly to what described in the iea tracking industry 2020 report iea 2020b would allow the model to opt for investments combining a range of transformation technologies characterized by various conversion efficiencies and emission factors the representation of the water sector in the model is also limited currently glucose accounts only for water consumption linked to the operation of technologies in the model by adding a simplified representation of the global hydrological cycle the model could provide further insights on the use of water resources and their exploitation or depletion along the time finally for what concerns the representation of the transport sector in the energy module it could be interesting to consider a further split of passenger transport and freight into short and long distance travel such an update would allow to better represent possible technology market penetration rates e g shift towards using public transport i e railway instead of private i e roadway for long distance passenger travel acknowledgments the authors of this paper would like to thank viktoria martin for her valuable inputs during the initial drafting of the paper mark howells acknowledges supplementary post project support from the climate compatible growth program ccg of the uk s foreign development and commonwealth office fcdo the views expressed in this paper do not necessarily reflect the uk government s official policies funding this work was supported by the european union s horizon 2020 research and innovation programme under grant agreement no 689150 sim4nexus credit authorship contribution statement agnese beltramo conceptualization methodology validation formal analysis data curation visualization writing original draft writing review editing eunice pereira ramos conceptualization supervision writing review editing constantinos taliotis methodology data curation writing review editing mark howells supervision writing review editing will usher conceptualization supervision writing original draft writing review editing appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105091 appendix a appendix schematic representation of the different modules in glucose figure a 1 energy module structure as defined in the glucose model figure a 2 land and food module structure as defined in the glucose model figure a 3 material module structure as defined in the glucose model b appendix glucose final demands a energy demands figure b 1 global final demand for electricity as defined in glucose figure b 2 global final demand for industrial heat as defined in glucose figure b 3 global final demand for residential heat as defined in glucose figure b 4 global final demand for roadway transport as defined in glucose figure b 5 global final demand for railway transport as defined in glucose figure b 6 global final demand for maritime and aviation transport as defined in glucose b food demands figure b 7 global final demand for crops as defined in glucose figure b 8 global final demand for livestock as defined in glucose c materials demands figure b 9 global final demand for aluminium as defined in glucose figure b 10 global final demand for cement as defined in glucose figure b 11 global final demand for pulp and paper as defined in glucose figure b 12 global final demand for petrochemicals as defined in glucose figure b 13 global final demand for steel and iron as defined in glucose 
25790,integrated assessment models iams are important tools to analyse cross sectoral interdependencies and the use of global resources most current tools are highly detailed and require expert knowledge and proprietary software to generate scenarios and analyse their insights in this paper the complementary global least cost user friendly clews open source exploratory glucose model is presented as a highly aggregated global iam open and accessible from source to solver and using the osemosys tool and the clews framework the model enables the exploration of policy measures on the future development of the integrated resource system thanks to its relatively simple structure it requires low computational resources allowing for the generation of a large number of scenarios or to quickly conduct preliminary investigations glucose is targeted towards education and training purposes by a range of interested parties from students to stakeholders and decision makers to explore possible future pathways towards the sustainable management of global resources keywords clews osemosys integrated assessment modelling sustainable development education software availability name of the software open source energy modelling system osemosys short code contact information email address osemosys gmail com year first available 2008 hardware required windows macos linux 8 gb ram software required gnu linear programming kit glpk availability open source and open access program language and size gnu mathprog 92 kb the latest release of the open source energy modelling system osemosys is available in the dedicated github repository https github com osemosys osemosys gnu mathprog releases tag v1 0 data availability a detailed list of original data sources and related references is available as part of the electronic supplementary material submitted with this paper the data files used as input to the glucose model are provided as follows names dataglucose baseline txt dataglucose 2degree txt dataglucose food txt dataglucose materials txt dataglucose total txt form of repository txt files size 2 mb access form github repository https github com kth desa glucose 1 introduction the integrated analysis and planning of energy land and water resource systems are of utmost importance for reaching sustainable development goals traditional management of these systems as separate entities cannot address the many trade offs and synergies between actions within each bazilian et al 2011 integrated assessment models iams have been used to facilitate the analysis of policies and strategies on the environment and development of the global economy and society gambhir et al 2019 hamilton et al 2015 the history of integrated assessment modelling can be dated back to the early 1970s when the world 3 model meadows et al 1972 was developed as a global systems dynamics model coupling social and physical elements of the world system rotmans 1998 nordhaus 1979 linear programming energy model combined energy conversion emissions and co2 concentrations to investigate the global effects of the energy system on the environment this model later evolved to become the more economy centred dynamic integrated climate economy dice model nordhaus 1992 in the 1980s the rains model alcamo et al 1991 focused on the representation of acid rain in europe from their emissions to their final impact on the environment the rains model was the first of its kind in promoting and adopting a close dialogue with policymakers in order to better understand how to shape the model to make it more useful for the policy making process from 1990 the need to better understand climate change pushed for a new class of models focusing on the representation of climate from the sources of emissions to their impacts on the global environment of which image stehfest et al 2014 was the first example weyant et al 1996 in 1996 iams were first featured in the second assessment report of the international panel on climate change ipcc ipcc 1996 and they have been explicitly recognised and used to analyse climate change mitigation at the global level ever since gambhir et al 2019 schwanitz 2013 over the past years iams have received criticism for their complexity opaqueness high entry barriers and computational intensity gambhir et al 2019 parker et al 2002 rotmans and van asselt 2001 stanton et al 2009 van der sluijs 2002 so far iams have also largely escaped the increasing interest towards open source collaborative modelling efforts huppmann et al 2019 pfenninger et al 2017 today s open source modelling frameworks are mature tools that can provide good alternatives to similar well established proprietary or commercial tools groissb√∂ck 2019 pfenninger et al 2017 well documented why there is a need for models and data to be increasingly open in particular open source tools allow quality assurance efforts to be shared among a wider range of interested people including governments thus also enhancing policy science interaction and the active engagement of decision makers towards a more transparent and collaborative policy making process pfenninger et al 2017 acknowledging the increasing polarisation within society regarding climate policy chinn et al 2020 harvey et al 2018 and the urgency required to mitigate climate disaster there is a need to improve communication and education around iams to increase transparency and lower the barrier to entry for those interested in integrated resource systems modelling it is believed this will increase trust in scenarios improve the quality of science and public and scientific debate around the use and application of integrated assessment models the global least cost user friendly clews open source exploratory glucose model presented in this paper provides an open source transparent lightweight complement to the well known iams with a focus on communication and education an initial version of the model was featured as part of the prototype global sustainable development report un 2014 glucose intends to provide a balance between a biophysical oriented and techno economic aggregated approach as defined by rotmans 1998 and edmonds et al 2012 the model is based on the climate land energy and water systems clews framework ramos et al 2021 with the addition of a simplified representation of key materials sectors the clews framework was first used to investigate multiple resource systems and their relation with the climate system bazilian et al 2011 iaea 2009 the framework has been applied in several case studies at regional national and urban levels ramos et al 2021 glucose presents the world as a single global region to allow the user to focus on understanding cross sectoral interdependencies and links it aims at educating and training decision makers and stakeholders in understanding the implications of sectoral policies on different resource systems it also intends to allow them to generate multiple scenarios providing the basis for transparent and informed discussions on a wide range of global issues glucose aims at demonstrating how highly aggregated and simplified models can be used to understand the key concepts of the interdependencies of resource systems and the related need for integrated planning and policy development over the long term in addition it aims at informing on sustainable development pathways and the need for enhanced policy coherence in line with goal 17 partnership for the goals target 14 of the sustainable development goals sdgs oecd 2017 tosun and lang 2017 tosun and leininger 2017 un 2019 this paper provides the detailed methodology of the glucose model it validates the model against statistical data and compares its behaviour with results from the ssp2 or middle of the road pathway fricko et al 2017 rcp 6 0 it presents and compares four different scenarios looking at current policies and the impact of different additional measures including greenhouse gas ghg emissions reduction a shift towards a more sustainable and healthy diet a decrease in materials demand due to design efficiency improvements and a combination of such measures 2 the global least cost user friendly clews open source exploratory glucose model 2 1 the glucose model background glucose was first developed in close collaboration with the united nations un as a global integrated assessment model based on the clews framework bazilian et al 2011 that could complement the discussion on the formulation of the sustainable development goals and help in bridging the gap between science and policy un 2014 this work aimed at creating a transparent tool to act as a simplified testing ground for policies and allow the visualisation and assessment of different policy pathways in regard to sustainable development on a global scale potential trade offs and synergies between climate land energy water and material sectors were identified with the analysis the model was developed using the open source energy modelling system osemosys howells et al 2011 allowing for transparency accessibility and easy expansion upon completion of the different stages of model development osemosys is a long term modelling framework based on linear optimization that computes the least cost optimal solution for the modelled system the tool is open and accessible from source to solver and it has a large international community of users and developers that support and assist each other via an online forum gardumi et al 2018 the initial version of glucose was featured as part of the prototype global sustainable development report un 2014 where it provided a results comparison between the full model outputs and the results of its energy module only this comparison showed how the interlinkages between the different modules energy land and materials allowed for a better understanding of the dynamics between different resource systems un 2014 additionally results from this initial version of glucose were used to develop interactive results visualisation and training materials as part of the un modelling tools for sustainable development initiative launched in 2016 united nations n d 2 2 the glucose structure and main characteristics the glucose model consists of one single global region and three interlinked modules representing three main sectors of the global socio economic system with direct connections to the environmental system energy sector land and food sector and materials production sector the model so far does not yet include a hydrological module but accounts for the water consumed across the system by the different technologies and sectors represented the different sectors of the model are coupled within one single framework rendering manual or iterative data input between modules unnecessary fig 1 illustrates the conceptual model design where all sectors are coupled with water and emission tracking as well as other cross sectoral linkages the rationale and model structure selected resembles that of the targets model rotmans et al 1997 the energy module represents key techno economic parameters that characterize different supply technologies including renewable ones and carbon capture and sequestration ccs equipped generation the land and food module consisting of land resources and food transformation technologies represent the food and biomass production and conversion chain from land availability to final food demand the material module represents the aluminium cement iron and steel pulp and paper chemicals and petrochemicals and fertilizers material production sectors for which reasonable input data were available the water consumption estimated in the model accounts for the amount of water needed by the different modules to perform the transformation processes represented no constraint on water availability is considered as the water resource balance is not represented yet in the model emissions are accounted for over the entire model at the resource level in terms of greenhouse gas ghg emissions and can be constrained annually the modelling period considered for the analysis spans from 2010 to 2050 with yearly time steps and three time slices per year representing average day night and peak hours of energy demand respectively in table 1 to clarify the scope of glucose a comparison of the structure main characteristics and features of glucose against other similar biophysical oriented iams namely message globiom image and magpie is provided glucose presents the global resource systems in a highly aggregated geographical way using one global region this can facilitate the data collection process as less data are needed to characterize the system however the downside is that major data sources and statistics are provided at a national level therefore the challenge is to collect or derive values that are representative of the global average data having one single global region represented in the model also makes it easier for less experienced modelers or analysts to better understand the structure of the model and the cross sectoral interlinkages this further emphasises the suitability of glucose for educational and training purposes finally the lower level of detail and the simplified structure of the model mean glucose has low computational requirements and a fast solution time thus speeding up the results generation process and facilitating multiple scenarios generation and stochastic analysis 2 2 1 the reference resource system fig 2 shows the reference resource system rrs diagram of glucose the diagram provides a simplified overview of the model structure with each module representing a different resource system and interlinkages made explicit between them primary resources are represented on the left hand side of the rrs which indicates the resource level of the system the boxes in the middle represent the transformation technologies that populate the primary and secondary system s levels and produce the final products defined as demands on the right hand side of the rrs each arrow in fig 2 illustrates the flow of resources between different levels and across sectors in the system ghg emissions and respective emission sources as considered in the model are indicated by the black cloud icons blue oval shapes represent the accounting of water consumption in the system and the related level in which it occurs the dotted lines identify each of the modules of the glucose system a more detailed schematic representation of each module is provided in appendix a 2 2 2 the baseline scenario the glucose baseline scenario was initially based on current and expected policy measures and global techno economic trends as reflected in the etp 2012 6 c scenario from the energy technology perspectives report iea 2012 in addition as part of the sim4nexus project european commission 2020 funded by the european union s horizon 2020 research and innovation programme the glucose baseline scenario has been adjusted to be in line with the major socio economic assumptions behind the definition of the ssp2 rcp6 0 scenario fricko et al 2017 this means that long term demographic gross domestic product and related demand projections have been aligned to the ones defined for the ssp2 rcp6 0 scenario 2 2 3 the energy module in the energy module 1 1 see appendix a figure a 1 the final energy demand has been divided into electricity heat and transport 2 2 see appendix b a for details on the energy module demands heat industrial and residential demand are treated separately to facilitate the linkage to the materials production module the transport sector considers available projections for maritime and aviation railway and roadway travel demand data used to characterize the energy module technology specifications and energy demand projections were primarily taken from international energy agency iea publications 3 3 a complete list of glucose input data sources by sector is available in tables ii 1 supplementary material ii the power generation sector includes 21 technological options while the heat generation sector includes 15 technological options in both instances centralised and decentralised alternatives are available the model also considers 5 combined heat and power technologies 4 4 the list of technologies names and related description as defined in the glucose model is available in tables i 1 supplementary material i additionally the module is constructed to assess future investment potential in unconventional infrastructure and technological shifts in primary energy supply such as coal or biomass gasification coal to liquids and gas to liquids and in generation such as ccs lastly in the transport sector the model can assess the potential for market penetration of technologies using biofuels or electricity as fuel however at this point competition only occurs between technologies within each transport mode i e the share of each transport mode is defined exogenously this is the most detailed module within glucose in terms of technological options this can be attributed to the fact that the provision of energy services is responsible for the majority of ghg emissions worldwide 73 2 of total ghg emissions ge and friedrich 2020 ritchie and roser 2020 thus it is reasonable at this point of model development to emphasise on this sector 2 2 4 the land and food module the principal purpose of the land and food module is to establish linkages between agricultural production its associated land use land degradation and energy use and biomass production for energy and food purposes and materials production also this module provides the only natural source of carbon sequestration in the model by characterising the forest land with a negative ghg emission factor land is represented through six principal land categories 5 5 see appendix a figure a 2 which are aggregated by different uses from agriculture to forest to other i e land not classified as agriculture or forest according to fao definition fao 2019 all the land categories apart from other produce biomass as output which can either be processed and transformed to satisfy the demand for crops and livestock distinctively 6 6 see appendix b b for details on the land and food module demands or be used for energy and industrial purposes respectively besides the these links with between the energy module and the food sector the land module is linked with the materials module via two key industries the fertilizer production that is that is assumed to be used in the agriculture land for farming and the pulp and paper industry that uses the forest biomass produced from the forest land as input material demand for food is coupled with population projections from the image model data pbl netherlands environmental assessment agency 2019 stehfest et al 2014 provided as part of the ssps database fricko et al 2017 iiasa energy program 2018 riahi et al 2017 3 2 2 5 the materials module even though material production was not separately mentioned in the clews modelling framework bazilian et al 2011 iaea 2009 the energy intensity of this sector and its environmental impact as well as the potential for material and energy efficiency improvements in this sector are worthy of consideration allwood et al 2012 iea 2007 sustainable use of materials implies a reduction in material consumption and associated energy flows such a reduction can be achieved by addressing the supply side such as through efficiency improvements in manufacturing processes and products design or the demand side for instance by means of altered consumer behaviour this can be implemented to a considerable extent through adaptation in lifestyles and societal attitudes improved system design cooperation between industries for a decrease in waste heat and material losses and policy frameworks that facilitate such changes allwood et al 2012 this sector is interconnected with the land and energy modules on several levels extraction of raw materials results in land degradation ghg emissions and requires energy input the transformation of raw materials into consumer products is an energy intensive process while market globalisation means that products need to be shipped across great distances from the supply source to the manufacturing location and finally to the point of demand simultaneously equipment requirements in energy and agricultural production processes affect the demand for certain materials such as aluminium cement iron and steel including all these aspects in a modelling framework can help guide decision making however at the current phase of glucose model development the materials module acts as an energy consumer 7 7 see appendix a figure a 3 inspiration for the choice of materials originated from allwood and cullen s work allwood et al 2012 the material industries considered i e pulp and paper iron and steel aluminium cement fertilizers and petrochemicals industries take in energy in various forms i e heat electricity fuels and use it either to drive conversion processes or as feedstock weirich 2013 nevertheless efficiency improvements have been assumed based on existing projections iea 2012 a more detailed description of each industry type is given by weirich 2013 2 2 6 input data and sources to ensure transparency and retrievability of the information and data used to characterize the glucose model and to define its baseline scenario an effort was made to use open access data as much as possible and to correctly document the corresponding reference sources 3 one of the main issues encountered in the data collection was to find representative data for the aggregated global region represented in glucose in some cases it was necessary to extrapolate data from country or region specific sources and apply them at the global level some databases provided estimates of aggregated global values based on country related data collection and estimations conducted in collaboration with national governments and institutions that inform international statistical data as for the case of the faostat database fao 2019 also between different sources some discrepancies were identified for the same data and in the definition of the type of data considered for instance when comparing figures of the land areas allocated to different uses various aggregated global data sources disagree on the actual value of land area and in the definition of the land use considered 8 8 see supplementary material iii tables iii 1 for land use data and tables iii 2 for ghg emission data in addition the discrepancies between global aggregated values and national or local figures indicate that there are significant uncertainties among the different sources for this reason for the characterization of glucose global aggregated data were used when available when global data where not retrievable average values from a selection of either country or region specific data sources were used instead 2 3 glucose base year validation this section validates the base modelling year of glucose against statistical data to show that the model has been correctly defined in its starting year to be in line with the relevant sources as the energy module is the most detailed in glucose emphasis is put on this module only due to the lack of more aggregated and consistent statistical data sources for the materials and land and food modules as currently represented it is challenging to establish meaningful comparisons for other model results fig 3 shows the share of total primary energy supply tpes by energy source as obtained from the base year of the glucose model compared to the statistical data from the iea world energy balance 2020 for the base year 2010 the energy mix extracted from the glucose results correctly reproduces the historical energy mix as retrievable from iea s statistical data the major difference between the two energy mixes is in the share of biofuels and waste this can be explained by the fact that in glucose the use of waste as fuel is not considered whereas it is accounted for in the iea data fig 4 compares the share of ghg emissions by sector in the glucose results against the statistics provided by the world resource institute wri climate data explorer wri 2015 for the base year 2010 in this case the share of energy related emissions is approximately 6 higher in the glucose results than in the wri data whereas the share of industry and land and food related emissions are approximately 4 and 3 lower this can be explained by how glucose is structured where the fuel related emissions are accounted for at the primary resource level and the energy conversion and production processes are represented in the energy module therefore some of the emissions linked to the production of heat or electricity to be used in the materials or the land and food modules are accounted for in the energy module where the heat and electricity are produced 2 4 comparing glucose baseline scenario results and long term trends with other iams in this section the glucose baseline scenario results are compared with the iams that provided quantitative projections to define the ssp2 rcp6 0 scenario fricko et al 2017 iiasa energy program 2018 this comparison ensures that the glucose modelling is in line with the key long term trends as provided by the models featured in the ssps database iiasa energy program 2018 riahi et al 2017 kling et al 2017 and rising 2020 describe how iams can support the nexus analysis of resource systems management and related infrastructure development by providing quantitative insights on the long term impact of different decisions however models are always subject to prediction errors that might affect outputs and associated insights therefore it is beneficial to conduct model comparison exercises to explore the uncertainties and likelihood of different outcomes kling et al 2017 in an attempt to harmonise the modelling efforts and pursue integration across different climate change research communities several efforts of comparing and assessing modelling inputs and results have been recorded over the past decades the integrated assessment modelling consortium iamc founded in 2007 has been focusing on the development of four common scenarios known as the representative concentration pathways rcps van vuuren et al 2014 to be used in climate modelling as the basis for providing insights to the ipcc fifth assessment report on the potential impact of climate change over the short and long term iamc 2020a iamc 2020b iiasa 2019 the shared socio economic pathways ssps instead have been developed to facilitate the integrated analysis of future climate impacts vulnerabilities adaptation and mitigation riahi et al 2017 the ssps consist of five narratives that focus on different global socio economic development pathways these narratives in combination with the rcps have been translated into modelled scenarios by an initial group of six iams van vuuren et al 2014 the selected models span across different iams types from high resolution biophysical oriented and policy evaluation models namely image message globiom gcam to macroeconomic oriented and policy optimization models namely aim witch remind these models generated projections for different resource sectors linked to the ssps riahi et al 2017 within the ssps context the ssp2 narrative scenarios are characterized by a middle of the road development in mitigation and adaptation challenges space fricko et al 2017 in practice this means that the social economic and technological trends described in the ssp2 narrative are expected to follow historical trajectories in the future leading to a moderate development trend that does not foresee major political instabilities or disruptive events fricko et al 2017 within the ssp2 narrative the ssp2 rcp6 0 scenario combines the ssp2 global socioeconomic development trajectory with the climate development trend as described in the rcp 6 0 this trend envisions a mitigation pathway that will reach by 2100 a radiative forcing target of 6 0 w m2 corresponding to keeping the global mean temperature increase below 4 c in relation to pre industrial levels fricko et al 2017 ipcc 2015 moss et al 2010 van vuuren et al 2014 for the comparison between glucose and other iams with regards to the baseline results versus the results from the ssp2 rcp6 0 scenario the energy sector was taken as the main reference for this sector detailed results are provided in the ssps database which allows for a closer understanding of how the sector is expected to evolve in the different models considered in addition thanks to its more detailed representation the energy module of glucose allows for a closer look into the reasons behind similarities and differences in model behaviour looking at the results for the global installed electrical capacity fig 5 the glucose model results follow a similar trend to the ssp2 rcp6 0 scenario results by falling within the range provided by other models this is also valid for the global secondary fig 6 left and final fig 6 right electricity production levels as shown in fig 6 the glucose results seem to follow the overall trends projected by the other ssp2 rcp6 0 models outputs looking at the primary energy production level in fig 7 left instead glucose results present a lower increase in their trajectory than the average trends presented by the ssp2 rcp6 0 models with a difference of approximately 15 in 2050 however for what concerns the total final energy production in fig 7 right again glucose results seem to be in line with the trends coming from ssp2 rcp6 0 models this can be explained by the use of different assumptions for the technology costs and efficiencies projections considered in the model the expected availability of variable renewable energy sources and the detail of technological representation such assumptions are at the core of each modelling effort they strongly depend on the data sources used to characterize the resource level in a model and affect its behaviour particularly in the case of a least cost optimization tool from the comparison exercise presented above it can be deduced that the glucose model represents the long term evolution of the integrated system producing results that are in line with the main trends identified by the iams tools featured in the ssp database 3 model application investigating global policy scenarios in glucose from the glucose baseline scenario already described in 2 2 2 four different global policy scenarios were developed the 2degree food materials and total scenarios each of the scenarios shows how the model can capture key inter sectoral dynamics and provide insights on the impact of selected policies on the global resource systems modelled in glucose 3 1 scenarios description the 2degree scenario provides an understanding of the impact of selected techno socio economic policy measures to limit the total amount of ghg emissions in the atmosphere and a related average global mean temperature increase of less than 2 c relative to pre industrial levels by 2100 it is derived from the baseline scenario by lowering the yearly ghg emissions limit and by changing some of the exogenous demands defined in the model namely demand for electricity heat road and maritime and aviation transportation simulating efficiency improvements on the end use side the 2degree scenario is aligned with the etp 2012 2 c scenario and the ssp2 rcp2 6 scenario which combines the ssp2 development trajectory with the rcp 2 6 assumption of reaching a radiative forcing target of 2 6 w m2 by 2100 this corresponds to keeping the global mean temperature increase below 2 c in relation to pre industrial levels fricko et al 2017 iea 2012 moss et al 2010 van vuuren et al 2014 the food scenario investigates the impact of eradicating hunger while ensuring a global shift toward a healthy and sustainable diet and more efficient food production systems it consists of reducing consumption of animal based proteins and dairy products in favour of more plant based proteins vegetables and cereals in addition it assumes a 50 reduction in the waste generated by crops harvesting by 2050 this scenario has been derived from the baseline by changing the final demand for crops and livestock between 2020 and 2050 to simulate a global shift toward the healthy reference diet composition as defined by the eat lancet commission by 2050 willett et al 2019 also the conversion efficiency of crops harvesting technologies has been assumed to linearly increase by 50 by 2050 to simulate a global halving of food losses at the crops harvesting level as a result of a transition towards more sustainable food production systems the materials scenario analyses the impact of optimized products design on the final demands for aluminium cement paper and steel and consequently on the overall resource systems based on allwood et al 2012 best estimates of possible materials design efficiency finally the total scenario combines the policies applied in the 2degree the food and the materials scenarios to investigate the implications of integrated policy planning on the long term evolution of the modelled sectors table 2 provides a summary of the main assumptions characterising the different glucose scenarios the variations in demands across the scenarios are shown in appendix b 3 2 scenarios results key results are compared for the years 2010 2030 and 2050 the results showcase the impact of above mentioned policy scenarios on the systems represented in glucose and the intersectoral dynamics that the model can capture fig 8 shows the absolute values upper graph and relative shares lower graph of total primary energy supply tpes by different resources across scenarios and along the modelled period similarly fig 9 shows the composition of each scenario installed electrical capacity in absolute upper graph and relative lower graph values fig 10 provides an overview of the land cover change fig 11 shows the change in final energy used by the materials sector across the different scenarios in comparison to the baseline finally fig 12 shows the projections of the total ghg emissions from the system for each scenario in the 2degree scenario the overall tpes is reduced by 40 as shown in fig 8 this is partially due to the overall expected reduction in exogenous energy demand linked to the annual ghg emissions limit imposed and the decarbonisation policies modelled in addition in the 2degree scenario there is an increase in investments in low emission energy supply technologies such as nuclear and renewables at the expense of fossil fuel based technologies to reduce the total ghg emissions accounted for in the model this technological change is driven by the need to meet the ambitious emissions reduction target of the 2degree scenario see fig 12 as a result nuclear technology tpes share in 2050 increases from 13 in the baseline to 24 in the 2degree scenario similarly in 2050 the share of primary energy supplied by renewable sources meaning hydro biomass solar wind geothermal and marine resources increases from 17 in the baseline to 39 in the 2degree scenario fig 8 lower graph also the share of electrical capacity available in the system and linked to nuclear and renewable technologies in 2050 increases from 13 to 24 for nuclear and from 17 to 29 for renewables between the baseline and 2degree scenario fig 9 lower graph at the same time the fossil fuel related tpes is reduced by 68 between the baseline and 2degree scenario the corresponding electrical capacity is reduced by 76 largely due to the phase out of coal based energy generation as shown in fig 9 a small amount of biomass with ccs capacity is also installed by 2050 in the 2degree scenario when comparing results between the baseline and the food scenarios one first notable outcome is the increase of tpes from biomass resources as shown in fig 8 by 2050 the amount of biomass tpes increases from 29 3 ej corresponding to 4 of the total in the baseline to 59 ej corresponding to 8 5 of the total in the food scenario similarly as shown in fig 9 in 2050 the biomass based electrical capacity installed in the system is 358 gw corresponding to 2 6 of the total in the baseline scenario and it increases to 759 gw equal to 5 7 of the total in the food scenario such increase comes from the reduced need for agricultural land for food production which in the food scenario is expected to decrease thanks to the shift towards a healthy and sustainable global diet starting in 2020 this is also supported by the increased efficiency of the crops transformation sector fig 10 shows for the food scenario a reduction in the land covered by cropland and pasture of almost 50 by 2050 compared to the baseline consequently by 2050 the amount of land covered by forest double from 3000 million ha in the baseline to approximately 6000 million h in the food scenario therefore also the ghg emission sequestration effect provided by the forest land is increased as a result by 2050 in the food scenario the tpes from fossil fuels increases by 7 compared to the baseline when comparing results between the baseline and the materials scenarios it is interesting to notice that by 2050 the materials design improvements contribute to reducing by 15 the final energy used by the materials sector in relation to the baseline scenario as shown in fig 11 as a result by 2050 the tpes fig 8 is slightly reduced by 4 2 for the materials scenario the mix of primary energy sources changes slightly in favour of more fossil fuels 6 ej to the expenses of nuclear 34 ej in comparison to the baseline while ghg emissions remain constant finally when comparing the results between the total and the baseline scenarios the combined energy food agriculture and materials policies from the 2degree food and materials scenarios respectively act together resulting in a combined positive impact on the modelled system tpes results in fig 8 indicate by 2050 an overall 45 reduction in the total scenario compared to the baseline slightly lower than in the 2degree scenario similarly by 2050 the total installed electrical capacity fig 9 upper graph is reduced by 14 in the total scenario compared to the baseline the share tpes from renewables in 2050 fig 8 lower graph increases from 17 in the baseline to 39 in the total scenario mainly due to a higher share of biomass one key difference between the 2degree and the total scenario results is that for the latter the tpes from nuclear resources is reduced compared to the baseline as a result more fossil fuel resources mainly coal are used in the total scenario compared to the 2degree as shown in fig 8 these results can be explained by the change in land cover presented in fig 10 and the reduction in final energy used by the materials sector shown in fig 11 in fig 10 the total scenario results resemble the ones from the food scenario showing an increase in forest land of approximately 3000 million ha by 2050 in comparison to the baseline thus more biomass is available in the total scenario to the energy sector at the same time the additional ghg emission sequestration effect of the increased forest land allows the model some flexibility for investments in traditional fossil fuel technologies to supply both heat and electricity to the system while fulfilling the emissions reduction target in addition fig 11 shows a 23 reduction in final energy used by the materials sector which gives the model additional flexibility in the use of the energy resources available to meet the related demands while respecting the ghg emission reduction goal the total scenario results show how combined sectoral policies allow the model to opt for a more diverse mix of energy sources and ensure effective and efficient use of resources in the transition towards a highly decarbonized system 4 discussion and conclusion this paper has described the development of a lightweight reliable transparent tool for education and training of broader audiences on the importance of the integrated systems planning approach and cross sectoral policy measures for the achievement of sustainable development pathways glucose unlike other iams glucose is fully open source and accessible from source to solver although lacking a regional representation that would allow a more detailed analysis of the integrated global system the model is able to provide a reasonable understanding of the modelled systems and inform on the interdependencies among global resources similarly to other iams glucose operates by computing the least cost optimal mix of technologies and resources needed to meet exogenously and endogenously defined demands thus providing one optimal solution to the investigated problem and potentially missing to provide an overview of possible future scenarios lund et al 2017 however glucose s aggregated structure facilitates the development of a large number of scenarios that can help testing different boundary conditions provide insights on a wide range of cross sectoral linkages across resource systems and explore solutions to integrated development challenges as an open and documented tool glucose allows users to investigate issues linked to the achievement of possible sustainable futures its aggregated structure also facilitates the communication of key clews framework concepts and aims at stimulating discussions around cross systems implications for the long term management of resources the scenarios presented in this paper intend to showcase the inter sectoral dynamics currently captured by the model and illustrate the key insights the scenarios can provide they assume ambitious policy measures to be applied at the global level therefore they intend to give an overview of the variety of globally relevant issues that the model can investigate through the glucose base year validation presented in section 2 3 it is shown that the energy module and the ghg emission projections of glucose are in line with reference statistical data sources similarly in section 2 4 the comparison of glucose baseline results with the outputs from the iams featured in the ssp2 rcp6 0 scenario runs show that glucose can provide comparable long term trends this confirms the validity of the glucose model as a reliable complement to other iams despite its different structure and granularity this work has highlighted difficulties in collecting reliable data that are consistent across sources to accurately characterize global aggregated models such as glucose global statistical data from well established international sources such as faostat world bank our world in data ipcc provide different figures for the same data or different data classifications this makes it difficult for a modeller to select representative data and to validate it against statistical information since the latter differ among themselves baj≈æelj et al 2013 also presented a similar issue highlighting two main reasons for uncertainties in global emissions data one related to the fact that emissions data are typically estimated and not measured both at the national and international level and the other one that it is difficult to allocate emissions correctly when looking at various emission inventories linked to different sectors baj≈æelj et al 2013 another example of such an issue is presented by larsen et al 2019 who highlighted the difficulty in collecting reliable data for characterising the water energy nexus both at regional and global level this work draws attention to the need for exploring the influence and relevance of the data uncertainty on model outcomes using a simple and aggregated model such as glucose can facilitate such assessment however it raises the question whether working at an aggregated scale might introduce more uncertainty than modelling individual countries separately and then linking them together which further raises the question of how other iams deal with the data uncertainty issue in terms of future work a few improvements can be mentioned one refers to the current representation of the materials module which is still minimal limiting the possibility of exploring sectoral targeted policies and technology changes according to the latest development trends expanding the model with technology and price alternatives similarly to what described in the iea tracking industry 2020 report iea 2020b would allow the model to opt for investments combining a range of transformation technologies characterized by various conversion efficiencies and emission factors the representation of the water sector in the model is also limited currently glucose accounts only for water consumption linked to the operation of technologies in the model by adding a simplified representation of the global hydrological cycle the model could provide further insights on the use of water resources and their exploitation or depletion along the time finally for what concerns the representation of the transport sector in the energy module it could be interesting to consider a further split of passenger transport and freight into short and long distance travel such an update would allow to better represent possible technology market penetration rates e g shift towards using public transport i e railway instead of private i e roadway for long distance passenger travel acknowledgments the authors of this paper would like to thank viktoria martin for her valuable inputs during the initial drafting of the paper mark howells acknowledges supplementary post project support from the climate compatible growth program ccg of the uk s foreign development and commonwealth office fcdo the views expressed in this paper do not necessarily reflect the uk government s official policies funding this work was supported by the european union s horizon 2020 research and innovation programme under grant agreement no 689150 sim4nexus credit authorship contribution statement agnese beltramo conceptualization methodology validation formal analysis data curation visualization writing original draft writing review editing eunice pereira ramos conceptualization supervision writing review editing constantinos taliotis methodology data curation writing review editing mark howells supervision writing review editing will usher conceptualization supervision writing original draft writing review editing appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105091 appendix a appendix schematic representation of the different modules in glucose figure a 1 energy module structure as defined in the glucose model figure a 2 land and food module structure as defined in the glucose model figure a 3 material module structure as defined in the glucose model b appendix glucose final demands a energy demands figure b 1 global final demand for electricity as defined in glucose figure b 2 global final demand for industrial heat as defined in glucose figure b 3 global final demand for residential heat as defined in glucose figure b 4 global final demand for roadway transport as defined in glucose figure b 5 global final demand for railway transport as defined in glucose figure b 6 global final demand for maritime and aviation transport as defined in glucose b food demands figure b 7 global final demand for crops as defined in glucose figure b 8 global final demand for livestock as defined in glucose c materials demands figure b 9 global final demand for aluminium as defined in glucose figure b 10 global final demand for cement as defined in glucose figure b 11 global final demand for pulp and paper as defined in glucose figure b 12 global final demand for petrochemicals as defined in glucose figure b 13 global final demand for steel and iron as defined in glucose 
25791,the ecosystem modelling complex ecopath with ecosim has been implemented extensively in the field of marine science however despite its widespread application descriptions of its functionality remain arcane in the literature this study conducts an evaluation of the software s prediction capacity using eight published ecopath models response of six ecosystem status indicators to four basic input variables to which imprecision had been added was investigated kempton s q index and total system throughput emerged as the most consistently responsive parameters moreover input biomass was identified as a high leverage parameter its influence on outputs being greater than that exerted by any other input variable this study constitutes one of the first comprehensive investigations of the response of selected outputs to imprecise input values and provides sufficient basis to warrant a sensitivity assessment of the software as well as introduction of a dedicated tool to perform such a task within ecopath with ecosim keywords ecosystem modelling ecosystem based management model performance ewe uncertainty monte carlo abbreviations ewe ecopath with ecosim sa sensitivity analysis lsa local sensitivity analysis gsa global sensitivity analysis oat one at a time ecoind ecological indicators b biomass ee ecotrophic efficiency p b production to biomass ratio q b consumption to biomass ratio mtlco mean trophic level of the community tst total system throughput ci confidence interval cv coefficient of variation rmse root mean square error si sensitivity index tl trophic level 1 introduction ecopath with ecosim hereinafter ewe is a freely available ecosystem modelling complex for analysis of anthropogenically exploited aquatic ecosystems ewe affords a quantitative and species based representation of any aquatic system of interest enabling construction and analysis of food web models through a suite of ecosystem level simulations christensen and walters 2004 the ecopath approach was advanced originally by polovina 1984a b to estimate the biomass of distinct components i e species or groups of species of the french frigate shoals system in the north western hawaiian islands heymans et al 2016 ecopath was integrated thereafter with diverse approaches in theoretical ecology in particular those advanced by ulanowicz 1986 regarding analysis of flows amidst separate components of a given ecosystem the ecopath system has undergone extensive development since 1990 christensen and pauly 1992 culminating ultimately in the integrated software package ecopath with ecosim christensen and walters 2004 the ecopath approach has long been utilised within the field of marine science from analysis of fisheries management aquaculture and general marine ecology to assessments of climate change pollution and offshore platform impacts piroddi et al 2011 niiranen et al 2013 coll√©ter et al 2015 serpetti et al 2017 in the context of fisheries for example ewe allows scientists to i envision alterations in exploited fish communities as a result of management measures ii substantiate accuracy of such predictions and ultimately iii inform decision makers with regard to probable outcomes and consequences of distinct management actions munro 2011 in this regard ewe affords two complementary uses exploration and prediction essington 2007 when utilised as an exploratory tool ecopath enables construction of heuristic models to explore scenarios of unforeseen alterations to trophic interactions that may derive from enforcement of management strategies essington 2007 in exploratory applications the focus is on discernment of prospective outcomes and uncertainties through the process of policy gaming i e implementation of gaming simulation to assist policy exploration decision making and strategic change geurts et al 2007 wherein potential unanticipated consequences of management decisions can be determined walters 1986 when utilised as a predictive tool however ewe may play a significant role in the actual formulation of policy decisions e g de mutsert et al 2015 stips et al 2015 the focus being on the exact estimation of policy relevant parameters such as population biomass fishing mortality rates and maximum sustainable yield essington 2007 however in order for ecopath to be applied in this context a formal assessment of the predictive capacity of the model is required usefulness of any model is contingent on the reliability and accuracy of its output yet as all models represent imperfect abstractions of reality and accurate input data are seldom obtainable the entirety of output values are de facto subject to imprecision loucks et al 2005 this is particularly true of multi species modelling approaches as complexity of such models is augmented to reflect biological realism a concurrent increase in scientific uncertainty due to scarce knowledge of functional relationships is unavoidable plag√°nyi and butterworth 2004 input data inaccuracies and modelling uncertainties are not independent of one another and can interact in several ways often leading to uncertainty becoming associated with model outputs loucks et al 2005 the incidence of such an event can be reduced however through execution of inter alia a sensitivity analysis sa saltelli 2002 defined as the assessment of how variations in a model outputs can be apportioned to different sources of variations in the model inputs saltelli et al 2004 landsberg and sands 2011 this method is intended to quantify impact of imprecisions in input data on predicted model outputs and overall model performance zheng et al 2014 as the relationship between the latter and a state variable is seldom linear determination of sensitivity at two or more levels of parameter alteration is often required zheng et al 2014 importantly sensitivity analysis enables discernment of high leverage parameters whose values exert a significant impact on model behaviour versus low leverage parameters whose values exert a rather negligible effect on the model and its functioning landsberg and sands 2011 a vast array of approaches to analysing sensitivity are available at present however these can be grouped broadly into two main typologies namely local sensitivity analysis lsa and global sensitivity analysis gsa lsa also referred to as one factor at a time oat consists in the assessment of sensitivity around a sole nominal point in the parameter space saltelli et al 2008 razavi et al 2020 meaning that lsa assesses impact of one uncertain input parameter at a time on the model outputs despite its simple and intuitive nature lsa is appropriate mostly in very specific contexts e g simple models characterised by linear input output relationships gupta and razavi 2017 however at times the approach has been implemented loosely in the literature causing it to be criticised for affording a rather localised and biased picture of model sensitivities saltelli and annoni 2010 saltelli et al 2019 a generally more appropriate and rigorous approach is that of gsa saltelli et al 2000 which investigates how uncertainty in outputs can be apportioned to uncertainty in each input parameter over their entire range zhou et al 2008 gsa is useful for identifying input parameters that are key to model outputs some approaches also quantifying the contribution of each input parameter and interactions amongst parameters to output uncertainty the theoretically superior approaches to sa such as gsa however tend to be computationally demanding and time consuming ye and hill 2017 the increasing computational expense associated with more complex models results often in simpler ad hoc strategies being favoured over more rigorous approaches to sa gupta and razavi 2017 with practitioners often recommending implementation of simple methods before considering time consuming gsa ye and hill 2017 despite the growing prominence that sa is achieving in the literature ewe currently fails to include a formal assessment for parameter sensitivity unlike earlier versions the built in resampling routine ecoranger consisting in a monte carlo like approach to identify the best fitting ecopath parameter in accordance with user defined criteria christensen and pauly 1996 elicited extensive criticism for presenting a rather simplistic solution to faulty models and was thus eliminated with the release of ewe version 6 0 christensen and lai 2007 however a number of alternative routines are available within the software enabling model users to investigate at least partly model performance following introduction of imprecision christensen and walters 2004 amongst these is the built in monte carlo engine which is a random data sampling technique that allows determination of multiple model solutions metropolis and ulam 1949 and the ecological indicators ecoind plug in which is used for assessing impact of input uncertainty on a range of ecological indicators coll and steenbeek 2017 indicators of ecosystem status constitute a critical component of ewe outputs as well as being integral to ecosystem based management playing a pivotal role in communication and in supporting objectives of conservation strategies rogers and greenaway 2005 when forecasting ecological benefits or costs of alternative marine ecosystem management scenarios a vast range of indicators can be of use with several now incorporated into the ewe modelling complex ainsworth and pitcher 2006 indicators fall into separate categories depending on their role in ecosystem monitoring including inter alia i biodiversity ii structure and functioning iii stability and resistance iv resource potential and v anthropogenic pressure bundy et al 2017 indicators tend to be utilised collectively as part of global blanchard et al 2010 bundy et al 2010 coll et al 2010 regional uusitalo et al 2016 or ecosystem level shackell et al 2012 assessments of marine system status and trends affording a detailed and most importantly multifaceted snapshot of ecosystem health and resilience consequently the present paper focuses on output indicators describing distinct components of a given ecosystem considering the limitations of lsa and the computational and time expenses associated with gsa this paper conducts a preliminary ad hoc assessment of the ability of ecopath models to retrieve precisely nominal output values in the instance of imprecision being present in the inputs 1 1 objectives the present study conducts a practical assessment of the ewe modelling complex investigating model response to imprecision in individual input parameters and simultaneous alteration of separate parameters at respectively different levels of imprecision analyses focus on simulated predictions generated by the ecopath component of the software simulation testing was executed to address two principal questions concerning implementation of ecopath models i the extent to which the model is responsive to imprecision in particular input variables and ii the extent to which the subjective process of model balancing originating from an identical initial dataset a practice herein simulated through the monte carlo routine may cause predictions to diverge from their nominal original values the present analyses focus on the capacity of ecopath to predict precisely not accurately and consistently both stock biomass b an indicator of resource potential and ecotrophic efficiency ee an indicator of anthropogenic pressure that describes the fraction of species production accounted for as predation and fishing mortality what the present paper attempts to address is the extent to which the ecopath modelling complex is capable of precisely retrieving the original output values produced by an unaltered and here certain model as a result of introduction of imprecision into model input parameters with methods adapted from essington 2007 b estimation represents a pivotal constituent of fisheries management whilst ee allows comprehension of the connection strength between species production and mortality essington 2007 lastly this paper aims to investigate influence of imprecision in input data on four ecosystem metrics belonging to three separate categories of ecosystem level indicators i kempton s q index a biodiversity indicator ii predatory biomass and iii mean trophic level of the community mtlco both describing ecosystem structure and functioning and iv total system throughput tst describing ecosystem stability and resistance rochet and trenkel 2003 ainsworth and pitcher 2006 a brief overview of the nature of these indicators is presented in table 1 2 methods 2 1 the ecopath model a detailed description of ecopath fundamentals and master equations underlying its functioning can be found in christensen and walters 2004 the software comprises three focal components i ecopath a static snapshot of trophic flows and biomasses within the system ii ecosim a time dynamic simulation module and iii ecospace a temporo spatial dynamic module for the purpose of this paper the ecopath static component alone is considered the ecopath module is constructed on the key principle of mass balance according to which when defining a functional group i e set of species sharing alike characteristics within a community total energy extracted from said group e g through fishing or predation must be balanced by energy expended i e consumption a schematic illustration of the approach adopted in the present study explored in detail in the following sections is presented in fig 1 2 2 model selection eight pre existing ewe models table 2 were accrued from the ecobase models repository using ewe version 6 6 ewe 2019 models were selected to be representative of four distinct marine ecosystem types coral reef continental shelf upwelling and open ocean within both subtropical and tropical regions fig 2 the eight models were chosen to provide a wide array of model structures because of intrinsic properties associated with distinct ecosystem types that were expected to be reflected in the model makeup compared to open ocean where production is patchy continental shelf systems upwelling and coral reef ecosystems are characterised by higher production rates a dissimilitude likely to be reflected in the productivity related values of the models belonging to high and low lower productivity systems in fact production is greater generally closer to land owing to elevated nutrient input from inter alia rivers and upwelling systems kaiser et al 2020 although coral reef systems per se are characterised by oligotrophic waters i e low in nutrients nutrients are extracted from upwelling intrusions e g andrews and gentien 1982 furnas 2011 lastly existence of vertical currents and mixing processes in upwelling systems is a phenomenon likely to cause differences amongst productivity values of upwelling and the other systems kaiser et al 2020 models were selected also based on their ecosystem trophic complexity favouring those when possible with comparable numbers of functional groups to reduce potential bias arising from disparate food web configurations in fact for these to be comparable rigorously the number of functional groups and thus of input values or sample size and individual pedigrees explained in following sections must be if not identical close morissette 2005 this however was limited by model availability one of the models included in the study baja california was characterised by a number of functional groups lower than that of the other models however having equal sample size for each ecosystem type was deemed more important and this choice was not expected to affect significantly the output of the study see essington 2007 models were utilised as hypothetical realities to investigate response of marine ecosystems to alterations in input data this was accomplished through consistent introduction of imprecision or uncertainty into four basic ecopath input parameters biomass b production to biomass ratio p b consumption to biomass ratio q b and ecotrophic efficiency ee for simplicity diet compositions and fishing yields were assumed to be known without error prior to analysis and according to the general pattern in tropical marine ecosystems lercari and arregu√≠n s√°nchez 2009 three broad trophic categories were identified for all investigated models i apex predators seabirds marine mammals and sharks ii smaller consumers all other faunal biomass and iii primary producers phytoplankton and algal material such trophic categories were used in the determination of uncertainty scenarios explained in the following sections 2 3 prediction precision assessment to explore robustness of model outputs to input imprecision 61 uncertainty scenarios entailing both individual and paired parameter perturbations were constructed and are presented in the supplementary material section 1 1 the terminology uncertainty scenario is intended to describe a hypothetical reality in which selected input parameters are altered through addition of differing levels of imprecision under 61 separate conditions each scenario entailed simultaneous allocation of one of four quality scores to each of the input variables i e b p b q b ee scores ranging from very certain to more certain relatively certain and completely uncertain each score was allocated a specific and default confidence interval ci 10 90 ci with b and ee assigned identical ci values within each scenario owing to their complementary relationship more information in essington 2007 as a result b and ee were always altered together in each single and paired parameter scenario table 1 supplementary material section 1 1 e g b ee 80 ci b ee 80 ci pb 90 ci default cis were obtained from pre defined pedigree tables provided in christensen et al 2000 and were utilised to generate parameter specific pedigree indices or coefficients of variation cvs for b table 3 and p b and q b table 4 with scores allocated based on the level of imprecision required by the 61 different uncertainty scenarios the pedigree of an ecopath model is a coded statement that computes the uncertainty or pedigree index cv associated to each input value within a given model morissette 2005 the built in pedigree routine has been described in detail by morissette 2005 therefore it will not be discussed further in this paper default cis function as a restriction on the extent to which the monte carlo routine is permitted to alter parameters from their nominal values as part of the balancing process christensen and walters 2004 in instances where impact of a single uncertain altered parameter was assessed remaining variables were regarded as well known and assigned a narrow ci e g 10 ci thus ensuring minimal changes were produced compared to parameters with wide cis i e uncertain the greater the ci assigned to a selected input parameter s the lower the confidence in the accuracy of said parameter assignment of different cis was conducted to retain a certain amount of control over the well known parameters during the monte carlo routine as no parameters were missing in the selected ewe models the missing parameter quality score was excluded from the 61 uncertainty scenarios considered 2 4 ecosystem indicators through implementation of the built in monte carlo routine values from all selected parameter sets i e b p b q b and ee were randomly substituted with alternative values describing the individual and joint uncertainty in each said values computed automatically by the pedigree routine originating from normal distributions in the range of mean i e nominal value 2 coefficient of variation cv steenbeek et al 2018 cv were loaded separately for each model input from the pedigree tables compiled a priori according to the 61 uncertainty scenarios the modelled system was simulated subsequently to obtain estimates of selected performance indices namely predatory biomass kempton s q index and mtlco all contributing ultimately to a quantitative assessment of ecosystem degradation kempton and taylor 1976 rochet and trenkel 2003 steenbeek et al 2018 further details on the calculation of ecosystem indicators are included in the supplementary material section 1 2 2 5 b ee tst for the second stage of output collection entailing acquisition of b and ee values together with tst manual perturbation of input parameters was required the monte carlo engine has been devised to allow for quantification of sensitivity of ecosim outputs to ecopath parameterisation and was therefore of no use here following the same 61 uncertainty scenarios utilised throughout the study the original ecopath b p b q b and ee were replaced manually with the lower parameter limits generated by the import of cv values into the monte carlo routine however as ecopath model solutions are constrained to satisfy the conservation of mass and basic input estimates for any given system failed frequently to produce ee values constrained between 0 and 1 manual balancing of perturbed ecopath models was also required further details on mass balancing are included in the supplementary material section 1 3 2 6 data analysis all statistical analyses were performed using r version 3 6 2 r core team 2019 residuals prediction errors and percentage residuals were determined for each generated output as a basic proxy of prediction precision and a minimum divergence of 10 from nominal values was selected as a significance threshold residuals were calculated as follows 1 residual b where indicates the residual difference between altered output and the corresponding baseline value b prediction precision was further determined through calculation of a sensitivity index si 2 si dmax dmin dmax where dmax and dmin represent maximum and minimum output values respectively resulting from variation of the input parameter over its entire range hoffman and gardner 1983 like residuals sis were calculated as a proxy of prediction precision identifying the extent to which the output of a model with imprecise inputs had deviated from nominal output values depending on the output being investigated sensitivity indices were calculated for each functional group separately e g sharks coastal demersals etc thus affording a good indication of group variability across an array of uncertainty scenarios root mean square error rmse a standard metric for model errors was executed to determine average model prediction error in the units of the parameter of interest with an arbitrary reference cutoff value c of 0 05 browne and cudeck 1993 rmse affords an absolute measure of model performance with lower values indicating better fit i e a small discrepancy between observed data points and values predicted by the model in the present paper a good fit i e rmse c corresponds to rmse 0 05 owing to the data failing to meet normality criteria kolmogorov smirnov test a kruskal wallis test was conducted to explore differences in percentage residuals across uncertainty scenarios 61 within separate models the test was executed for each parameter of interest significance level Œ± was set at 0 05 for all statistical analyses 3 results 3 1 ecological indicators the first set of simulations evaluated responses of ecosystem indicators to imprecisions in b p b q b and ee input parameters investigated scenarios were divided into b p b and q b perturbed scenarios each comprising both individual and paired conditions residual si and rmse values are presented for each indicator in the supplementary material section 2 1 alongside results of pairwise comparison tests 3 1 1 predatory biomass when b and ee were perturbed predatory biomass derived sis reached minimum 11 87 and maximum 60 21 values for the two continental shelf models i e baja california and gulf of california respectively distinct levels of imprecision appeared to cause model specific variations in prediction precision with a ci of 60 resulting for example in the highest si for the gulf of california and lowest si for the south benguela model four models exhibited rmse 0 05 significance threshold analogously to si different uncertainty scenarios failed to engender consistent trends in rmse across distinct models significant differences in percentage residuals were found in the gal√°pagos kruskal wallis h 74 78 df 2 n 1 94 n 2 997 n 3 987 n 4 991 p 0 001 gulf of california kruskal wallis h 19 46 df 2 n 1 100 n 2 1005 n 3 1007 n 4 1005 p 0 001 humboldt current kruskal wallis h 200 64 df 2 n 1 98 n 2 997 n 3 999 n 4 1051 p 0 001 south benguela kruskal wallis h 11 25 df 2 n 1 99 n 2 992 n 3 1046 n 4 1094 p 0 011 lesser antilles kruskal wallis h 10 02 df 2 n 1 100 n 2 1003 n 3 1001 n 4 1004 p 0 018 and northwest africa models kruskal wallis h 265 01 df 2 n 1 100 n 2 1594 n 3 1542 n 4 1596 p 0 001 fig 3 with h representing the kruskal wallis test statistics df the degrees of freedom and n 1 4 the different samples i e different cis considered in this case 10 80 ci no differences were determined in the danajon bank and baja california models when p b was subject to imprecision a similar trend in sis to that observed above was found no consistent pattern was observed in the relationship between cis and sis rmse was highest for gal√°pagos gulf of california south benguela and lesser antilles models all exceeding the 0 05 threshold significant differences in percentage residuals were determined in the gal√°pagos kruskal wallis h 27 92 df 2 n 1 685 n 2 691 n 3 691 p 0 001 and northwest africa models kruskal wallis h 3417 df 2 n 1 1291 n 2 1293 n 3 1880 p 0 001 fig 4 albeit failing to attain statistical significance in the remaining models lastly under q b scenarios a similar range of sis was identified across the models rmse was highest for gal√°pagos gulf of california south benguela and lesser antilles these representing the sole models exceeding the rmse significance threshold 0 05 significant differences in percentage residuals were found in the gal√°pagos kruskal wallis h 7 23 df 2 n 1 689 n 2 684 n 3 690 p 0 027 humboldt current kruskal wallis h 72 81 df 2 n 1 698 n 2 699 n 3 699 p 0 001 and northwest africa models kruskal wallis h 240 46 df 2 n 1 1926 n 2 1880 n 3 705 p 0 001 fig 5 no differences were found in the remaining models 3 1 2 kempton s q index when input b and ee were subject to imprecision kempton s q derived sis ranged from 9 54 for baja california to 43 02 for danajon bank all models save baja california had rmse exceeding the significance threshold of 0 05 significant differences in percentage residuals were found in all models except for lesser antilles gal√°pagos kruskal wallis h 44 57 df 3 n 1 94 n 2 997 n 3 987 n 4 991 p 0 001 danajon bank kruskal wallis h 137 12 df 3 n 1 99 n 2 1006 n 3 1002 n 4 1003 p 0 001 baja california kruskal wallis h 25 94 df 3 n 1 100 n 2 998 n 3 998 n 4 995 p 0 001 gulf of california kruskal wallis h 15 36 df 3 n 1 100 n 2 1005 n 3 1007 n 4 1005 p 0 001 humboldt current kruskal wallis h 403 96 df 3 n 1 98 n 2 997 n 3 999 n 4 1051 p 0 001 south benguela kruskal wallis h 37 22 df 3 n 1 99 n 2 992 n 3 1046 n 4 1094 p 0 001 and northwest africa kruskal wallis h 85 25 df 3 n 1 100 n 2 1594 n 3 1542 n 4 1596 p 0 001 fig 6 under p b perturbed scenarios sis ranged from 9 54 for baja california to 69 36 for northwest africa all models except for baja california had rmse exceeding 0 05 significant differences in percentage residuals were identified in the gal√°pagos kruskal wallis h 26 82 df 2 n 1 685 n 2 691 n 3 691 p 0 001 danajon bank kruskal wallis h 9 04 df 2 n 1 703 n 2 701 n 3 701 p 0 011 lesser antilles kruskal wallis h 9 4 df 2 n 1 696 n 2 697 n 3 800 p 0 009 and northwest africa models kruskal wallis h 3425 7 df 2 n 1 1291 n 2 1293 n 3 1880 p 0 001 fig 7 lastly when input q b was subject to imprecision sis varied from 10 77 for baja california to 69 28 for northwest africa all models save baja california exhibited rmse exceeding the 0 05 significance threshold significant differences in percentage residuals were found in the danajon bank kruskal wallis h 12 24 df 2 n 1 702 n 2 704 n 3 702 p 0 002 and humboldt current models kruskal wallis h 183 03 df 2 n 1 698 n 2 699 n 3 699 p 0 001 fig 8 albeit failing to attain statistical significance in the remaining models 3 1 3 mtlco under b and ee perturbed scenarios mtlco derived sis varied from 2 20 gal√°pagos to 40 43 for south benguela the latter representing the sole model to exceed 12 south benguela was the sole model to exhibit rmse exceeding the significance threshold of 0 05 significant differences in percentage residuals were identified in the gal√°pagos kruskal wallis h 96 23 df 3 n 1 94 n 2 997 n 3 987 n 4 991 p 0 001 danajon bank kruskal wallis h 134 27 df 3 n 1 99 n 2 1006 n 3 1002 n 4 1003 p 0 001 baja california kruskal wallis h 21 54 df 3 n 1 100 n 2 998 n 3 998 n 4 995 p 0 001 humboldt current kruskal wallis h 78 28 df 3 n 1 99 n 2 997 n 3 999 n 4 1051 p 0 001 and northwest africa models kruskal wallis h 176 89 df 3 n 1 100 n 2 1594 n 3 1542 n 4 1596 p 0 001 fig 9 under p b perturbed scenarios sis attained minimum 3 59 and maximum 19 12 values for gal√°pagos gal√°pagos south benguela and lesser antilles models exhibited rmse consistently greater than 0 05 whilst the remaining models never exceeded the significance threshold significant differences in percentage residuals were found in the humboldt current kruskal wallis h 7 08 df 2 n 1 702 n 2 697 n 3 744 p 0 029 south benguela kruskal wallis h 7 66 df 2 n 1 690 n 2 688 n 3 688 p 0 022 and northwest africa models kruskal wallis h 30 56 df 2 n 1 1291 n 2 1293 n 3 1880 p 0 001 fig 10 lastly under q b perturbed scenarios sis varied from 3 40 for gal√°pagos to 13 47 for south benguela exhibiting a narrow range of estimates gal√°pagos south benguela and lesser antilles models exhibited rmse consistently 0 05 significant differences in percentage residuals were found in baja california kruskal wallis h 104 86 df 2 n 1 698 n 2 698 n 3 698 p 0 001 humboldt current kruskal wallis h 22 39 df 2 n 1 698 n 2 699 n 3 699 p 0 001 and northwest africa models kruskal wallis h 130 79 df 2 n 1 1926 n 2 1880 n 3 705 p 0 001 fig 11 3 2 b ee tst the present set of simulations evaluated the response of b ee and tst to imprecisions in b p b q b and ee input parameters the focus is on differences in percentage residuals across functional groups nomenclature from original models used rather than uncertainty scenarios and therefore different cis in this section sensitivity refers to si values sis and rmse are presented for each functional group in the supplementary material section 2 2 3 2 1 northwest africa under b and ee perturbed scenarios b estimates exhibited substantially low sensitivity for most functional groups si 5 whilst bathydemersal predators i e demersal fish leaving at depths 200 m exhibited twice as high sensitivity si 13 24 ee estimates tended to exhibit greater sensitivity than b for all functional groups si 10 excluding outliers seven groups exhibited significant discrepancies between observed b data points and values predicted by the model rmse 0 05 analogously eight groups exhibited ee derived rmse 0 05 under both p b and q b perturbed scenarios sensitivity remained low for all functional groups ca 47 of functional groups exhibited significant discrepancies between observed b data points and values predicted by the model rmse 0 05 under both p b and q b perturbed scenarios under p b scenarios 15 of groups presented ee derived rmse 0 05 whilst 37 had rmse 0 05 under q b scenarios when paired uncertainty scenarios were considered both si and rmse estimates exhibited a pattern similar to that observed under p b and q b scenarios although sensitivity of b outputs equalled zero under p b q b scenarios except for bathydemersal predators significant differences in percentage residuals were found for b outputs across the 61 uncertainty scenarios kruskal wallis h 516 21 df 6 n 1 27 n 2 484 n 3 484 n 4 484 n 5 484 n 6 484 n 7 484 p 0 001 fig 12 a with a pairwise comparison test revealing existence of such differences between each functional group supplementary material section 2 2 1 significant differences in percentage residuals were further identified for ee outputs across the entirety of explored scenarios kruskal wallis h 774 43 df 6 n 1 27 n 2 27 n 3 27 n 4 27 n 5 27 n 6 27 n 7 27 p 0 001 fig 12b a pairwise comparison test for the latter however failed to be produced because of insufficient x observations caused by the absence of ee estimates for seabirds and all cetaceans except for dolphins with said groups failing to be represented in the respective graph tst exhibited variable sensitivity to input errors under investigated scenarios table 5 owing to the elevated nature of tst derived prediction errors rmse significance threshold was no longer meaningful and mean percentage error was provided to facilitate understanding magnitude of mean percentage errors was reflected into that of rmse as a single tst output was generated per uncertainty scenario no statistical assessment could be performed 3 2 2 gal√°pagos both b and ee outputs exhibited high inter group variability in sensitivity estimates when imprecision was incorporated into a single input parameter i e b and ee p b or q b however ca 50 of functional groups exhibited similar sensitivity responses to input error behaviour of b and ee output metrics diverged when input b and p b were subject to error simultaneously under b p b perturbed scenarios b output exhibited low inter group variability and overall low sensitivity conversely the same scenario engendered high inter group variability in the sensitivity response of ee outputs p b q b scenarios engendered highest sensitivity in biomass outputs for most functional groups although 36 of groups exhibited sensitivity equal to zero inter group variability remained however relatively low for both b and ee outputs lastly under q b b scenarios both b and ee outputs exhibited low inter group variability in sensitivity eighty five of groups exhibited significant discrepancies between observed data points and values predicted by the model for b rmse 0 05 with five groups 15 characterised by rmse 10 highly significant error ee output estimates exhibited rmse 0 05 for 19 groups 57 ee derived rmse was highest under b p b scenarios significant differences in percentage residuals were found for b outputs across investigated scenarios kruskal wallis h 642 81 df 6 n 1 33 n 2 33 n 3 33 n 4 33 n 5 33 n 6 33 n 7 33 p 0 001 fig 13 no significant differences were identified for ee outputs tst exhibited variable sensitivity to input errors under investigated scenarios table 6 a maximum mean percentage error of 30 60 was achieved under q b scenarios whilst the latter perturbed in combination with p b exerted a minor effect 4 12 except for p b q b and q b b scenarios rmse estimates appeared to reflect the magnitude of mean percentage error 3 2 3 baja california under b ee perturbed scenarios b derived sis ranged from 9 23 marlin to 27 60 mesopelagic fish inter group variability in sensitivity however was low moreover b sensitivity was found to remain virtually unaltered under all uncertainty scenarios ee outputs exhibited greater sensitivity overall a further reduction in inter group variability was observed for b sis under b p b scenarios with 83 of groups exhibiting estimates comprised between 6 10 and 6 54 a similarly low inter group variability was found for ee related sis under the same scenario sensitivity ranging from 9 75 to 12 34 p b q b scenarios also exhibited low inter group variability for b sensitivity conversely under the same scenario ee derived sis were found to be more varied ranging from 10 20 small sharks to 33 33 mesopelagic fish a similar pattern was observed under q b b scenarios seven groups exhibited b derived rmse 0 05 under b perturbed scenarios with such estimates remaining virtually unaltered across alternative conditions both individual and paired ten groups presented ee derived rmse 0 05 under b perturbed scenarios such groups increasing to 15 and 14 under p b and q b scenarios respectively when paired scenarios were applied the number of ee derived rmse exceeding 0 05 ranged from 15 b p b p b q b to 17 q b b significant differences in percentage residuals across the tested scenarios were identified for both b kruskal wallis h 404 44 df 6 n 1 18 n 2 147 n 3 147 n 4 147 n 5 147 n 6 147 n 7 147 p 0 001 fig 14 a and ee outputs kruskal wallis h 520 38 df 6 n 1 18 n 2 147 n 3 147 n 4 147 n 5 147 n 6 147 n 7 147 p 0 001 fig 14b tst was characterised by moderate sensitivity this ranging from 7 51 p b q b scenarios to 17 97 p b scenarios table 7 mean percentage errors were similar across different scenarios with a maximum value of 29 40 obtained under b scenarios this was reflected in rmses with b scenarios generating the highest rmse in a list of similar estimates 4 discussion the simulations presented herein suggest a parameter specific response to input imprecision with kempton s q index constituting the most consistently responsive of the investigated ecosystem metrics irrespective of model perturbed input parameter and degree of alteration by considering a cutoff point of 0 05 the index exhibited low to moderate prediction error occasionally reaching considerable imprecision consistently with previous research e g steenbeek et al 2018 predatory biomass follows whilst mtlco qualifies as the least responsive when considering graphs presented for ecosystem indicators presence of numerous extreme values is evidenced this generated issues with the computation of said graphs the latter often requiring elimination of outliers to become readable removal of extreme values however caused at times severe disruption of statistical analysis resulting often in substantially different outputs it is suggested that utilisation of the built in monte carlo routine for generation of alternative balanced models may produce excessive out of range estimates and investigation into causes of this phenomenon is therefore encouraged in mainstream statistics outliers are regarded often as unrepresentative observations distorting statistics about the underlying population of interest tabachnick and fidell 2013 proudlove et al 2019 outlier detection methodologies for monte carlo computations include statistical assessments such as pearson and hartley s significance test this enabling identification of multiple outliers in random samples with n 30 pearson and hartley 1966 and robust chauvenet rejection rcr this allowing for detection of unreliable data points based on the chauvenet s criterion lin and sherman 2007 maples et al 2018 additional statistical approaches frequentist include mahalanobis distance gnanadesikan 2011 and creation of funnel plots bird et al 2005 since the monte carlo routine constitutes an important and useful component of ewe strategies to minimise data disruption and outlier generation should be investigated an exiguous unequal distribution of leverage amongst input parameters is also suggested regarding explored metrics b p b and q b both individually and paired impacted predictions precision in a comparable fashion across operational models b however resulting more consistently influential overall see essington 2007 in the instance of kempton s q index this can be apportioned to its biomass based nature the index expresses biomass diversity of upper trophic level tl taxa tl 3 within a system kempton and taylor 1976 cited in ainsworth and pitcher 2006 and error in input biomass translates therefore into error in the predicted diversity metric the slight dominance of input biomass on predatory biomass sensitivity can be ascribed to the same biomass based nature output b and ee prediction errors were also unequally affected by data types input b being again most consistently influential this can be explicated with reference to the mass balance requirements of ecopath whereby total production and total losses must be at equilibrium christensen and walters 2004 as the former corresponds to the product of b and production to biomass ratio p b imprecision in such variables likely propagates to biomass based measures suggesting that the mass balance constraint of ecopath models may be insufficient to avert inaccurate predictions in addition to input b p b was identified as the second most influential parameter on the precision of tst outputs also consistently responsive across all investigated scenarios tst is estimated as the sum of all flows within a system these consisting in total consumption i e q b total export total respiration and total flows to detritus the relative failure of q b to exert the greatest impact may thus seem surprising however when the second master equation on which the ecopath module is based christensen and walters 2004 is recollected and consumption deconstructed into individual components the role of the production term is evidenced 3 consumption production respiration unassimilated food by expressing the equation in formal terms equation 4 is obtained 4 bi q b i bi p b i ri ui considering the abovementioned calculation of total production the dominant role of b and p b in the determination of tst response can be comprehended although analysed functional groups were model specific the broad category of predatory fish which include bathydemersal and pelagic fish appeared to be more responsive to input imprecision overall than any other group however all functional groups exhibited some degree of deviation from nominal values as well as significant inter group differences in their response to imprecision supplementary material section 2 2 however the model specific nature of most functional groups and their attribution to often broad taxonomic categories e g predatory fish rather than fish species listed individually hinders comparison between models and identification of species specific response to imprecision within the model becomes challenging the present simulations further indicate the apparent absence of linearity in the relationship between confidence intervals and model predictions with the greatest source of data imprecision failing to exert consistently the greatest impact on the model outputs considered essington 2007 input uncertainty is suggested to impact both within and cross model parameter precision differentially with distinct levels of parameter imprecision engendering effects of differing magnitude for a given variable this suggests that the subjective process of model balancing originating from an identical initial dataset herein simulated through the monte carlo routine is likely to generate markedly different values across separate simulations thus contributing to predictions diverging from their nominal values 4 1 implications future research ecosystem based models and ewe in particular are implemented increasingly in a vast array of scientific disciplines and decision making processes a more comprehensive understanding of model variance and imprecision propagation is thus crucial to evaluate and construe ewe model predictions with a greater degree of certainty to this end this paper contributes to the available body of literature elucidating the behaviour of six ewe commonly predicted parameters under a diverse set of software specific imprecision conditions information on the input variables to which the model outputs appear at a preliminary stage to be most responsive and the predicted parameters outputs whose behaviour is most greatly affected are herein presented consequently this work aims to provide the basis for identification of areas towards which further research efforts should be directed the utility of the results presented herein is that to facilitate anticipation of the likely prediction error in investigated parameters when complete confidence in input data is missing this is particularly true for the kempton s q index whose behaviour remained rather constant within and across all investigated models under distinct software specific imprecision scenarios when utilised in conjunction with other ecosystem indicators kempton s q affords an effective measure to i assess impacts of alternative fisheries management strategies ii trace effect of climate variations on biodiversity iii estimate non consumptive value of ecosystems and ultimately iv inform ecosystem based management approaches ainsworth and pitcher 2006 irrespective of their nature ecosystem indicators are of particular scientific interest biodiversity measures holding however a special appeal to the public owing to their synoptic nature which also augments the parameter s policy relevance ainsworth and pitcher 2006 in this regard by identifying biomass as a possible high leverage parameter measures to ensure the highest possible level of certainty can be implemented to increase the reliability of important output metrics as a result given that biomass estimates are highly susceptible to inaccuracy christensen et al 2000 achieving greater exactness of such estimates becomes paramount integration of controlling mechanisms to halt imprecision propagation from input to output e g inclusion of equilibrium constants orr et al 2018 in addition to the existing mass balance constraint could enhance possibly ewe error screening capacities by investigating output behaviour under a vast array of software specific imprecision and uncertainty conditions this paper has attempted to cover most of the possible scenarios in which one or more input parameters were inaccurate in their entirety or for a subset of functional groups to assess the extent to which the model outputs would diverge from nominal values by consulting this paper and identifying the most relevant uncertainty scenario users may be able to anticipate the most likely degree of imprecision in their output metrics despite a general trend of little and negligible variations because of imprecision in inputs two ecological indicators kempton s q and tst were observed to experience sufficient shifts in values to warrant further and thorough investigation the present paper should encourage scientists to subject ewe to the most appropriate gsa approach to identify the extent to which high leverage input parameters influence observed imprecision in outputs due to the unique nature of ewe this paper also demonstrates the importance of providing users with access to a dedicated tool within the software itself to conduct a thorough assessment of individual models that accounts for the specific nature of the software and its different components static and mass balanced time dynamic and temporo spatial dynamic this would reduce computational expense associated with sa especially gsa and allow even practitioners unfamiliar with sa to assess reliability of their models the question of ewe precision would also greatly benefit from larger scale research evaluating consequences of diverse typologies of uncertainty an example of which is structural uncertainty deriving from omission of important yet poorly understood species mcallister and kirchner 2002 and imprecision on a vaster array of predicted measures ultimately comparing such results with those generated by identical assessments conducted on alternative ecosystem modelling software 5 conclusions analyses aimed at the determination of imprecision susceptible parameters identified kempton s q index and tst as the most consistently responsive of the six investigated parameters irrespective of model perturbed input parameter and degree of alteration except for relatively sporadic instances of considerable imprecision kempton s q exhibited low to moderate prediction error and a rather constant behaviour under the entirety of explored uncertainty scenarios tst was characterised by significantly higher mean prediction error instead the usefulness of such results lies in the behavioural predictability itself of the two parameters contributing to modellers ability to anticipate likely prediction errors when confidence in the quality of input data is deficient an uneven distribution of leverage amongst input parameters was identified with biomass constituting the most consistently influential overall this is likely a by product of the mass balance constraint of ecopath models whereby total production estimated as the product of b and p b and total losses must be at equilibrium propagation of imprecision from input biomass to biomass based outputs suggests therefore a potential inadequacy of the mass balance constraint in averting unprecise predictions in this regard identification of biomass as a high leverage parameter may be viewed as an incentive to enhance confidence in input data and augment reliability of pivotal parameters as a result due to the intrinsic susceptibility of biomass estimates to inaccuracy attainment of greater exactness becomes fundamental author contributions i s conceived and designed the research i s performed the research and analysed the data i s wrote the manuscript and v l g t provided comments on the manuscript funding this research did not receive any specific grant from funding agencies in the public commercial or not for profit sectors declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we would like to thank the ecopath with ecosim community for the operational models and for technical support the first author would also like to thank dr helene burningham associate professor in the department of geography at university college london for her supervision and support during the initial development of this research appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105098 
25791,the ecosystem modelling complex ecopath with ecosim has been implemented extensively in the field of marine science however despite its widespread application descriptions of its functionality remain arcane in the literature this study conducts an evaluation of the software s prediction capacity using eight published ecopath models response of six ecosystem status indicators to four basic input variables to which imprecision had been added was investigated kempton s q index and total system throughput emerged as the most consistently responsive parameters moreover input biomass was identified as a high leverage parameter its influence on outputs being greater than that exerted by any other input variable this study constitutes one of the first comprehensive investigations of the response of selected outputs to imprecise input values and provides sufficient basis to warrant a sensitivity assessment of the software as well as introduction of a dedicated tool to perform such a task within ecopath with ecosim keywords ecosystem modelling ecosystem based management model performance ewe uncertainty monte carlo abbreviations ewe ecopath with ecosim sa sensitivity analysis lsa local sensitivity analysis gsa global sensitivity analysis oat one at a time ecoind ecological indicators b biomass ee ecotrophic efficiency p b production to biomass ratio q b consumption to biomass ratio mtlco mean trophic level of the community tst total system throughput ci confidence interval cv coefficient of variation rmse root mean square error si sensitivity index tl trophic level 1 introduction ecopath with ecosim hereinafter ewe is a freely available ecosystem modelling complex for analysis of anthropogenically exploited aquatic ecosystems ewe affords a quantitative and species based representation of any aquatic system of interest enabling construction and analysis of food web models through a suite of ecosystem level simulations christensen and walters 2004 the ecopath approach was advanced originally by polovina 1984a b to estimate the biomass of distinct components i e species or groups of species of the french frigate shoals system in the north western hawaiian islands heymans et al 2016 ecopath was integrated thereafter with diverse approaches in theoretical ecology in particular those advanced by ulanowicz 1986 regarding analysis of flows amidst separate components of a given ecosystem the ecopath system has undergone extensive development since 1990 christensen and pauly 1992 culminating ultimately in the integrated software package ecopath with ecosim christensen and walters 2004 the ecopath approach has long been utilised within the field of marine science from analysis of fisheries management aquaculture and general marine ecology to assessments of climate change pollution and offshore platform impacts piroddi et al 2011 niiranen et al 2013 coll√©ter et al 2015 serpetti et al 2017 in the context of fisheries for example ewe allows scientists to i envision alterations in exploited fish communities as a result of management measures ii substantiate accuracy of such predictions and ultimately iii inform decision makers with regard to probable outcomes and consequences of distinct management actions munro 2011 in this regard ewe affords two complementary uses exploration and prediction essington 2007 when utilised as an exploratory tool ecopath enables construction of heuristic models to explore scenarios of unforeseen alterations to trophic interactions that may derive from enforcement of management strategies essington 2007 in exploratory applications the focus is on discernment of prospective outcomes and uncertainties through the process of policy gaming i e implementation of gaming simulation to assist policy exploration decision making and strategic change geurts et al 2007 wherein potential unanticipated consequences of management decisions can be determined walters 1986 when utilised as a predictive tool however ewe may play a significant role in the actual formulation of policy decisions e g de mutsert et al 2015 stips et al 2015 the focus being on the exact estimation of policy relevant parameters such as population biomass fishing mortality rates and maximum sustainable yield essington 2007 however in order for ecopath to be applied in this context a formal assessment of the predictive capacity of the model is required usefulness of any model is contingent on the reliability and accuracy of its output yet as all models represent imperfect abstractions of reality and accurate input data are seldom obtainable the entirety of output values are de facto subject to imprecision loucks et al 2005 this is particularly true of multi species modelling approaches as complexity of such models is augmented to reflect biological realism a concurrent increase in scientific uncertainty due to scarce knowledge of functional relationships is unavoidable plag√°nyi and butterworth 2004 input data inaccuracies and modelling uncertainties are not independent of one another and can interact in several ways often leading to uncertainty becoming associated with model outputs loucks et al 2005 the incidence of such an event can be reduced however through execution of inter alia a sensitivity analysis sa saltelli 2002 defined as the assessment of how variations in a model outputs can be apportioned to different sources of variations in the model inputs saltelli et al 2004 landsberg and sands 2011 this method is intended to quantify impact of imprecisions in input data on predicted model outputs and overall model performance zheng et al 2014 as the relationship between the latter and a state variable is seldom linear determination of sensitivity at two or more levels of parameter alteration is often required zheng et al 2014 importantly sensitivity analysis enables discernment of high leverage parameters whose values exert a significant impact on model behaviour versus low leverage parameters whose values exert a rather negligible effect on the model and its functioning landsberg and sands 2011 a vast array of approaches to analysing sensitivity are available at present however these can be grouped broadly into two main typologies namely local sensitivity analysis lsa and global sensitivity analysis gsa lsa also referred to as one factor at a time oat consists in the assessment of sensitivity around a sole nominal point in the parameter space saltelli et al 2008 razavi et al 2020 meaning that lsa assesses impact of one uncertain input parameter at a time on the model outputs despite its simple and intuitive nature lsa is appropriate mostly in very specific contexts e g simple models characterised by linear input output relationships gupta and razavi 2017 however at times the approach has been implemented loosely in the literature causing it to be criticised for affording a rather localised and biased picture of model sensitivities saltelli and annoni 2010 saltelli et al 2019 a generally more appropriate and rigorous approach is that of gsa saltelli et al 2000 which investigates how uncertainty in outputs can be apportioned to uncertainty in each input parameter over their entire range zhou et al 2008 gsa is useful for identifying input parameters that are key to model outputs some approaches also quantifying the contribution of each input parameter and interactions amongst parameters to output uncertainty the theoretically superior approaches to sa such as gsa however tend to be computationally demanding and time consuming ye and hill 2017 the increasing computational expense associated with more complex models results often in simpler ad hoc strategies being favoured over more rigorous approaches to sa gupta and razavi 2017 with practitioners often recommending implementation of simple methods before considering time consuming gsa ye and hill 2017 despite the growing prominence that sa is achieving in the literature ewe currently fails to include a formal assessment for parameter sensitivity unlike earlier versions the built in resampling routine ecoranger consisting in a monte carlo like approach to identify the best fitting ecopath parameter in accordance with user defined criteria christensen and pauly 1996 elicited extensive criticism for presenting a rather simplistic solution to faulty models and was thus eliminated with the release of ewe version 6 0 christensen and lai 2007 however a number of alternative routines are available within the software enabling model users to investigate at least partly model performance following introduction of imprecision christensen and walters 2004 amongst these is the built in monte carlo engine which is a random data sampling technique that allows determination of multiple model solutions metropolis and ulam 1949 and the ecological indicators ecoind plug in which is used for assessing impact of input uncertainty on a range of ecological indicators coll and steenbeek 2017 indicators of ecosystem status constitute a critical component of ewe outputs as well as being integral to ecosystem based management playing a pivotal role in communication and in supporting objectives of conservation strategies rogers and greenaway 2005 when forecasting ecological benefits or costs of alternative marine ecosystem management scenarios a vast range of indicators can be of use with several now incorporated into the ewe modelling complex ainsworth and pitcher 2006 indicators fall into separate categories depending on their role in ecosystem monitoring including inter alia i biodiversity ii structure and functioning iii stability and resistance iv resource potential and v anthropogenic pressure bundy et al 2017 indicators tend to be utilised collectively as part of global blanchard et al 2010 bundy et al 2010 coll et al 2010 regional uusitalo et al 2016 or ecosystem level shackell et al 2012 assessments of marine system status and trends affording a detailed and most importantly multifaceted snapshot of ecosystem health and resilience consequently the present paper focuses on output indicators describing distinct components of a given ecosystem considering the limitations of lsa and the computational and time expenses associated with gsa this paper conducts a preliminary ad hoc assessment of the ability of ecopath models to retrieve precisely nominal output values in the instance of imprecision being present in the inputs 1 1 objectives the present study conducts a practical assessment of the ewe modelling complex investigating model response to imprecision in individual input parameters and simultaneous alteration of separate parameters at respectively different levels of imprecision analyses focus on simulated predictions generated by the ecopath component of the software simulation testing was executed to address two principal questions concerning implementation of ecopath models i the extent to which the model is responsive to imprecision in particular input variables and ii the extent to which the subjective process of model balancing originating from an identical initial dataset a practice herein simulated through the monte carlo routine may cause predictions to diverge from their nominal original values the present analyses focus on the capacity of ecopath to predict precisely not accurately and consistently both stock biomass b an indicator of resource potential and ecotrophic efficiency ee an indicator of anthropogenic pressure that describes the fraction of species production accounted for as predation and fishing mortality what the present paper attempts to address is the extent to which the ecopath modelling complex is capable of precisely retrieving the original output values produced by an unaltered and here certain model as a result of introduction of imprecision into model input parameters with methods adapted from essington 2007 b estimation represents a pivotal constituent of fisheries management whilst ee allows comprehension of the connection strength between species production and mortality essington 2007 lastly this paper aims to investigate influence of imprecision in input data on four ecosystem metrics belonging to three separate categories of ecosystem level indicators i kempton s q index a biodiversity indicator ii predatory biomass and iii mean trophic level of the community mtlco both describing ecosystem structure and functioning and iv total system throughput tst describing ecosystem stability and resistance rochet and trenkel 2003 ainsworth and pitcher 2006 a brief overview of the nature of these indicators is presented in table 1 2 methods 2 1 the ecopath model a detailed description of ecopath fundamentals and master equations underlying its functioning can be found in christensen and walters 2004 the software comprises three focal components i ecopath a static snapshot of trophic flows and biomasses within the system ii ecosim a time dynamic simulation module and iii ecospace a temporo spatial dynamic module for the purpose of this paper the ecopath static component alone is considered the ecopath module is constructed on the key principle of mass balance according to which when defining a functional group i e set of species sharing alike characteristics within a community total energy extracted from said group e g through fishing or predation must be balanced by energy expended i e consumption a schematic illustration of the approach adopted in the present study explored in detail in the following sections is presented in fig 1 2 2 model selection eight pre existing ewe models table 2 were accrued from the ecobase models repository using ewe version 6 6 ewe 2019 models were selected to be representative of four distinct marine ecosystem types coral reef continental shelf upwelling and open ocean within both subtropical and tropical regions fig 2 the eight models were chosen to provide a wide array of model structures because of intrinsic properties associated with distinct ecosystem types that were expected to be reflected in the model makeup compared to open ocean where production is patchy continental shelf systems upwelling and coral reef ecosystems are characterised by higher production rates a dissimilitude likely to be reflected in the productivity related values of the models belonging to high and low lower productivity systems in fact production is greater generally closer to land owing to elevated nutrient input from inter alia rivers and upwelling systems kaiser et al 2020 although coral reef systems per se are characterised by oligotrophic waters i e low in nutrients nutrients are extracted from upwelling intrusions e g andrews and gentien 1982 furnas 2011 lastly existence of vertical currents and mixing processes in upwelling systems is a phenomenon likely to cause differences amongst productivity values of upwelling and the other systems kaiser et al 2020 models were selected also based on their ecosystem trophic complexity favouring those when possible with comparable numbers of functional groups to reduce potential bias arising from disparate food web configurations in fact for these to be comparable rigorously the number of functional groups and thus of input values or sample size and individual pedigrees explained in following sections must be if not identical close morissette 2005 this however was limited by model availability one of the models included in the study baja california was characterised by a number of functional groups lower than that of the other models however having equal sample size for each ecosystem type was deemed more important and this choice was not expected to affect significantly the output of the study see essington 2007 models were utilised as hypothetical realities to investigate response of marine ecosystems to alterations in input data this was accomplished through consistent introduction of imprecision or uncertainty into four basic ecopath input parameters biomass b production to biomass ratio p b consumption to biomass ratio q b and ecotrophic efficiency ee for simplicity diet compositions and fishing yields were assumed to be known without error prior to analysis and according to the general pattern in tropical marine ecosystems lercari and arregu√≠n s√°nchez 2009 three broad trophic categories were identified for all investigated models i apex predators seabirds marine mammals and sharks ii smaller consumers all other faunal biomass and iii primary producers phytoplankton and algal material such trophic categories were used in the determination of uncertainty scenarios explained in the following sections 2 3 prediction precision assessment to explore robustness of model outputs to input imprecision 61 uncertainty scenarios entailing both individual and paired parameter perturbations were constructed and are presented in the supplementary material section 1 1 the terminology uncertainty scenario is intended to describe a hypothetical reality in which selected input parameters are altered through addition of differing levels of imprecision under 61 separate conditions each scenario entailed simultaneous allocation of one of four quality scores to each of the input variables i e b p b q b ee scores ranging from very certain to more certain relatively certain and completely uncertain each score was allocated a specific and default confidence interval ci 10 90 ci with b and ee assigned identical ci values within each scenario owing to their complementary relationship more information in essington 2007 as a result b and ee were always altered together in each single and paired parameter scenario table 1 supplementary material section 1 1 e g b ee 80 ci b ee 80 ci pb 90 ci default cis were obtained from pre defined pedigree tables provided in christensen et al 2000 and were utilised to generate parameter specific pedigree indices or coefficients of variation cvs for b table 3 and p b and q b table 4 with scores allocated based on the level of imprecision required by the 61 different uncertainty scenarios the pedigree of an ecopath model is a coded statement that computes the uncertainty or pedigree index cv associated to each input value within a given model morissette 2005 the built in pedigree routine has been described in detail by morissette 2005 therefore it will not be discussed further in this paper default cis function as a restriction on the extent to which the monte carlo routine is permitted to alter parameters from their nominal values as part of the balancing process christensen and walters 2004 in instances where impact of a single uncertain altered parameter was assessed remaining variables were regarded as well known and assigned a narrow ci e g 10 ci thus ensuring minimal changes were produced compared to parameters with wide cis i e uncertain the greater the ci assigned to a selected input parameter s the lower the confidence in the accuracy of said parameter assignment of different cis was conducted to retain a certain amount of control over the well known parameters during the monte carlo routine as no parameters were missing in the selected ewe models the missing parameter quality score was excluded from the 61 uncertainty scenarios considered 2 4 ecosystem indicators through implementation of the built in monte carlo routine values from all selected parameter sets i e b p b q b and ee were randomly substituted with alternative values describing the individual and joint uncertainty in each said values computed automatically by the pedigree routine originating from normal distributions in the range of mean i e nominal value 2 coefficient of variation cv steenbeek et al 2018 cv were loaded separately for each model input from the pedigree tables compiled a priori according to the 61 uncertainty scenarios the modelled system was simulated subsequently to obtain estimates of selected performance indices namely predatory biomass kempton s q index and mtlco all contributing ultimately to a quantitative assessment of ecosystem degradation kempton and taylor 1976 rochet and trenkel 2003 steenbeek et al 2018 further details on the calculation of ecosystem indicators are included in the supplementary material section 1 2 2 5 b ee tst for the second stage of output collection entailing acquisition of b and ee values together with tst manual perturbation of input parameters was required the monte carlo engine has been devised to allow for quantification of sensitivity of ecosim outputs to ecopath parameterisation and was therefore of no use here following the same 61 uncertainty scenarios utilised throughout the study the original ecopath b p b q b and ee were replaced manually with the lower parameter limits generated by the import of cv values into the monte carlo routine however as ecopath model solutions are constrained to satisfy the conservation of mass and basic input estimates for any given system failed frequently to produce ee values constrained between 0 and 1 manual balancing of perturbed ecopath models was also required further details on mass balancing are included in the supplementary material section 1 3 2 6 data analysis all statistical analyses were performed using r version 3 6 2 r core team 2019 residuals prediction errors and percentage residuals were determined for each generated output as a basic proxy of prediction precision and a minimum divergence of 10 from nominal values was selected as a significance threshold residuals were calculated as follows 1 residual b where indicates the residual difference between altered output and the corresponding baseline value b prediction precision was further determined through calculation of a sensitivity index si 2 si dmax dmin dmax where dmax and dmin represent maximum and minimum output values respectively resulting from variation of the input parameter over its entire range hoffman and gardner 1983 like residuals sis were calculated as a proxy of prediction precision identifying the extent to which the output of a model with imprecise inputs had deviated from nominal output values depending on the output being investigated sensitivity indices were calculated for each functional group separately e g sharks coastal demersals etc thus affording a good indication of group variability across an array of uncertainty scenarios root mean square error rmse a standard metric for model errors was executed to determine average model prediction error in the units of the parameter of interest with an arbitrary reference cutoff value c of 0 05 browne and cudeck 1993 rmse affords an absolute measure of model performance with lower values indicating better fit i e a small discrepancy between observed data points and values predicted by the model in the present paper a good fit i e rmse c corresponds to rmse 0 05 owing to the data failing to meet normality criteria kolmogorov smirnov test a kruskal wallis test was conducted to explore differences in percentage residuals across uncertainty scenarios 61 within separate models the test was executed for each parameter of interest significance level Œ± was set at 0 05 for all statistical analyses 3 results 3 1 ecological indicators the first set of simulations evaluated responses of ecosystem indicators to imprecisions in b p b q b and ee input parameters investigated scenarios were divided into b p b and q b perturbed scenarios each comprising both individual and paired conditions residual si and rmse values are presented for each indicator in the supplementary material section 2 1 alongside results of pairwise comparison tests 3 1 1 predatory biomass when b and ee were perturbed predatory biomass derived sis reached minimum 11 87 and maximum 60 21 values for the two continental shelf models i e baja california and gulf of california respectively distinct levels of imprecision appeared to cause model specific variations in prediction precision with a ci of 60 resulting for example in the highest si for the gulf of california and lowest si for the south benguela model four models exhibited rmse 0 05 significance threshold analogously to si different uncertainty scenarios failed to engender consistent trends in rmse across distinct models significant differences in percentage residuals were found in the gal√°pagos kruskal wallis h 74 78 df 2 n 1 94 n 2 997 n 3 987 n 4 991 p 0 001 gulf of california kruskal wallis h 19 46 df 2 n 1 100 n 2 1005 n 3 1007 n 4 1005 p 0 001 humboldt current kruskal wallis h 200 64 df 2 n 1 98 n 2 997 n 3 999 n 4 1051 p 0 001 south benguela kruskal wallis h 11 25 df 2 n 1 99 n 2 992 n 3 1046 n 4 1094 p 0 011 lesser antilles kruskal wallis h 10 02 df 2 n 1 100 n 2 1003 n 3 1001 n 4 1004 p 0 018 and northwest africa models kruskal wallis h 265 01 df 2 n 1 100 n 2 1594 n 3 1542 n 4 1596 p 0 001 fig 3 with h representing the kruskal wallis test statistics df the degrees of freedom and n 1 4 the different samples i e different cis considered in this case 10 80 ci no differences were determined in the danajon bank and baja california models when p b was subject to imprecision a similar trend in sis to that observed above was found no consistent pattern was observed in the relationship between cis and sis rmse was highest for gal√°pagos gulf of california south benguela and lesser antilles models all exceeding the 0 05 threshold significant differences in percentage residuals were determined in the gal√°pagos kruskal wallis h 27 92 df 2 n 1 685 n 2 691 n 3 691 p 0 001 and northwest africa models kruskal wallis h 3417 df 2 n 1 1291 n 2 1293 n 3 1880 p 0 001 fig 4 albeit failing to attain statistical significance in the remaining models lastly under q b scenarios a similar range of sis was identified across the models rmse was highest for gal√°pagos gulf of california south benguela and lesser antilles these representing the sole models exceeding the rmse significance threshold 0 05 significant differences in percentage residuals were found in the gal√°pagos kruskal wallis h 7 23 df 2 n 1 689 n 2 684 n 3 690 p 0 027 humboldt current kruskal wallis h 72 81 df 2 n 1 698 n 2 699 n 3 699 p 0 001 and northwest africa models kruskal wallis h 240 46 df 2 n 1 1926 n 2 1880 n 3 705 p 0 001 fig 5 no differences were found in the remaining models 3 1 2 kempton s q index when input b and ee were subject to imprecision kempton s q derived sis ranged from 9 54 for baja california to 43 02 for danajon bank all models save baja california had rmse exceeding the significance threshold of 0 05 significant differences in percentage residuals were found in all models except for lesser antilles gal√°pagos kruskal wallis h 44 57 df 3 n 1 94 n 2 997 n 3 987 n 4 991 p 0 001 danajon bank kruskal wallis h 137 12 df 3 n 1 99 n 2 1006 n 3 1002 n 4 1003 p 0 001 baja california kruskal wallis h 25 94 df 3 n 1 100 n 2 998 n 3 998 n 4 995 p 0 001 gulf of california kruskal wallis h 15 36 df 3 n 1 100 n 2 1005 n 3 1007 n 4 1005 p 0 001 humboldt current kruskal wallis h 403 96 df 3 n 1 98 n 2 997 n 3 999 n 4 1051 p 0 001 south benguela kruskal wallis h 37 22 df 3 n 1 99 n 2 992 n 3 1046 n 4 1094 p 0 001 and northwest africa kruskal wallis h 85 25 df 3 n 1 100 n 2 1594 n 3 1542 n 4 1596 p 0 001 fig 6 under p b perturbed scenarios sis ranged from 9 54 for baja california to 69 36 for northwest africa all models except for baja california had rmse exceeding 0 05 significant differences in percentage residuals were identified in the gal√°pagos kruskal wallis h 26 82 df 2 n 1 685 n 2 691 n 3 691 p 0 001 danajon bank kruskal wallis h 9 04 df 2 n 1 703 n 2 701 n 3 701 p 0 011 lesser antilles kruskal wallis h 9 4 df 2 n 1 696 n 2 697 n 3 800 p 0 009 and northwest africa models kruskal wallis h 3425 7 df 2 n 1 1291 n 2 1293 n 3 1880 p 0 001 fig 7 lastly when input q b was subject to imprecision sis varied from 10 77 for baja california to 69 28 for northwest africa all models save baja california exhibited rmse exceeding the 0 05 significance threshold significant differences in percentage residuals were found in the danajon bank kruskal wallis h 12 24 df 2 n 1 702 n 2 704 n 3 702 p 0 002 and humboldt current models kruskal wallis h 183 03 df 2 n 1 698 n 2 699 n 3 699 p 0 001 fig 8 albeit failing to attain statistical significance in the remaining models 3 1 3 mtlco under b and ee perturbed scenarios mtlco derived sis varied from 2 20 gal√°pagos to 40 43 for south benguela the latter representing the sole model to exceed 12 south benguela was the sole model to exhibit rmse exceeding the significance threshold of 0 05 significant differences in percentage residuals were identified in the gal√°pagos kruskal wallis h 96 23 df 3 n 1 94 n 2 997 n 3 987 n 4 991 p 0 001 danajon bank kruskal wallis h 134 27 df 3 n 1 99 n 2 1006 n 3 1002 n 4 1003 p 0 001 baja california kruskal wallis h 21 54 df 3 n 1 100 n 2 998 n 3 998 n 4 995 p 0 001 humboldt current kruskal wallis h 78 28 df 3 n 1 99 n 2 997 n 3 999 n 4 1051 p 0 001 and northwest africa models kruskal wallis h 176 89 df 3 n 1 100 n 2 1594 n 3 1542 n 4 1596 p 0 001 fig 9 under p b perturbed scenarios sis attained minimum 3 59 and maximum 19 12 values for gal√°pagos gal√°pagos south benguela and lesser antilles models exhibited rmse consistently greater than 0 05 whilst the remaining models never exceeded the significance threshold significant differences in percentage residuals were found in the humboldt current kruskal wallis h 7 08 df 2 n 1 702 n 2 697 n 3 744 p 0 029 south benguela kruskal wallis h 7 66 df 2 n 1 690 n 2 688 n 3 688 p 0 022 and northwest africa models kruskal wallis h 30 56 df 2 n 1 1291 n 2 1293 n 3 1880 p 0 001 fig 10 lastly under q b perturbed scenarios sis varied from 3 40 for gal√°pagos to 13 47 for south benguela exhibiting a narrow range of estimates gal√°pagos south benguela and lesser antilles models exhibited rmse consistently 0 05 significant differences in percentage residuals were found in baja california kruskal wallis h 104 86 df 2 n 1 698 n 2 698 n 3 698 p 0 001 humboldt current kruskal wallis h 22 39 df 2 n 1 698 n 2 699 n 3 699 p 0 001 and northwest africa models kruskal wallis h 130 79 df 2 n 1 1926 n 2 1880 n 3 705 p 0 001 fig 11 3 2 b ee tst the present set of simulations evaluated the response of b ee and tst to imprecisions in b p b q b and ee input parameters the focus is on differences in percentage residuals across functional groups nomenclature from original models used rather than uncertainty scenarios and therefore different cis in this section sensitivity refers to si values sis and rmse are presented for each functional group in the supplementary material section 2 2 3 2 1 northwest africa under b and ee perturbed scenarios b estimates exhibited substantially low sensitivity for most functional groups si 5 whilst bathydemersal predators i e demersal fish leaving at depths 200 m exhibited twice as high sensitivity si 13 24 ee estimates tended to exhibit greater sensitivity than b for all functional groups si 10 excluding outliers seven groups exhibited significant discrepancies between observed b data points and values predicted by the model rmse 0 05 analogously eight groups exhibited ee derived rmse 0 05 under both p b and q b perturbed scenarios sensitivity remained low for all functional groups ca 47 of functional groups exhibited significant discrepancies between observed b data points and values predicted by the model rmse 0 05 under both p b and q b perturbed scenarios under p b scenarios 15 of groups presented ee derived rmse 0 05 whilst 37 had rmse 0 05 under q b scenarios when paired uncertainty scenarios were considered both si and rmse estimates exhibited a pattern similar to that observed under p b and q b scenarios although sensitivity of b outputs equalled zero under p b q b scenarios except for bathydemersal predators significant differences in percentage residuals were found for b outputs across the 61 uncertainty scenarios kruskal wallis h 516 21 df 6 n 1 27 n 2 484 n 3 484 n 4 484 n 5 484 n 6 484 n 7 484 p 0 001 fig 12 a with a pairwise comparison test revealing existence of such differences between each functional group supplementary material section 2 2 1 significant differences in percentage residuals were further identified for ee outputs across the entirety of explored scenarios kruskal wallis h 774 43 df 6 n 1 27 n 2 27 n 3 27 n 4 27 n 5 27 n 6 27 n 7 27 p 0 001 fig 12b a pairwise comparison test for the latter however failed to be produced because of insufficient x observations caused by the absence of ee estimates for seabirds and all cetaceans except for dolphins with said groups failing to be represented in the respective graph tst exhibited variable sensitivity to input errors under investigated scenarios table 5 owing to the elevated nature of tst derived prediction errors rmse significance threshold was no longer meaningful and mean percentage error was provided to facilitate understanding magnitude of mean percentage errors was reflected into that of rmse as a single tst output was generated per uncertainty scenario no statistical assessment could be performed 3 2 2 gal√°pagos both b and ee outputs exhibited high inter group variability in sensitivity estimates when imprecision was incorporated into a single input parameter i e b and ee p b or q b however ca 50 of functional groups exhibited similar sensitivity responses to input error behaviour of b and ee output metrics diverged when input b and p b were subject to error simultaneously under b p b perturbed scenarios b output exhibited low inter group variability and overall low sensitivity conversely the same scenario engendered high inter group variability in the sensitivity response of ee outputs p b q b scenarios engendered highest sensitivity in biomass outputs for most functional groups although 36 of groups exhibited sensitivity equal to zero inter group variability remained however relatively low for both b and ee outputs lastly under q b b scenarios both b and ee outputs exhibited low inter group variability in sensitivity eighty five of groups exhibited significant discrepancies between observed data points and values predicted by the model for b rmse 0 05 with five groups 15 characterised by rmse 10 highly significant error ee output estimates exhibited rmse 0 05 for 19 groups 57 ee derived rmse was highest under b p b scenarios significant differences in percentage residuals were found for b outputs across investigated scenarios kruskal wallis h 642 81 df 6 n 1 33 n 2 33 n 3 33 n 4 33 n 5 33 n 6 33 n 7 33 p 0 001 fig 13 no significant differences were identified for ee outputs tst exhibited variable sensitivity to input errors under investigated scenarios table 6 a maximum mean percentage error of 30 60 was achieved under q b scenarios whilst the latter perturbed in combination with p b exerted a minor effect 4 12 except for p b q b and q b b scenarios rmse estimates appeared to reflect the magnitude of mean percentage error 3 2 3 baja california under b ee perturbed scenarios b derived sis ranged from 9 23 marlin to 27 60 mesopelagic fish inter group variability in sensitivity however was low moreover b sensitivity was found to remain virtually unaltered under all uncertainty scenarios ee outputs exhibited greater sensitivity overall a further reduction in inter group variability was observed for b sis under b p b scenarios with 83 of groups exhibiting estimates comprised between 6 10 and 6 54 a similarly low inter group variability was found for ee related sis under the same scenario sensitivity ranging from 9 75 to 12 34 p b q b scenarios also exhibited low inter group variability for b sensitivity conversely under the same scenario ee derived sis were found to be more varied ranging from 10 20 small sharks to 33 33 mesopelagic fish a similar pattern was observed under q b b scenarios seven groups exhibited b derived rmse 0 05 under b perturbed scenarios with such estimates remaining virtually unaltered across alternative conditions both individual and paired ten groups presented ee derived rmse 0 05 under b perturbed scenarios such groups increasing to 15 and 14 under p b and q b scenarios respectively when paired scenarios were applied the number of ee derived rmse exceeding 0 05 ranged from 15 b p b p b q b to 17 q b b significant differences in percentage residuals across the tested scenarios were identified for both b kruskal wallis h 404 44 df 6 n 1 18 n 2 147 n 3 147 n 4 147 n 5 147 n 6 147 n 7 147 p 0 001 fig 14 a and ee outputs kruskal wallis h 520 38 df 6 n 1 18 n 2 147 n 3 147 n 4 147 n 5 147 n 6 147 n 7 147 p 0 001 fig 14b tst was characterised by moderate sensitivity this ranging from 7 51 p b q b scenarios to 17 97 p b scenarios table 7 mean percentage errors were similar across different scenarios with a maximum value of 29 40 obtained under b scenarios this was reflected in rmses with b scenarios generating the highest rmse in a list of similar estimates 4 discussion the simulations presented herein suggest a parameter specific response to input imprecision with kempton s q index constituting the most consistently responsive of the investigated ecosystem metrics irrespective of model perturbed input parameter and degree of alteration by considering a cutoff point of 0 05 the index exhibited low to moderate prediction error occasionally reaching considerable imprecision consistently with previous research e g steenbeek et al 2018 predatory biomass follows whilst mtlco qualifies as the least responsive when considering graphs presented for ecosystem indicators presence of numerous extreme values is evidenced this generated issues with the computation of said graphs the latter often requiring elimination of outliers to become readable removal of extreme values however caused at times severe disruption of statistical analysis resulting often in substantially different outputs it is suggested that utilisation of the built in monte carlo routine for generation of alternative balanced models may produce excessive out of range estimates and investigation into causes of this phenomenon is therefore encouraged in mainstream statistics outliers are regarded often as unrepresentative observations distorting statistics about the underlying population of interest tabachnick and fidell 2013 proudlove et al 2019 outlier detection methodologies for monte carlo computations include statistical assessments such as pearson and hartley s significance test this enabling identification of multiple outliers in random samples with n 30 pearson and hartley 1966 and robust chauvenet rejection rcr this allowing for detection of unreliable data points based on the chauvenet s criterion lin and sherman 2007 maples et al 2018 additional statistical approaches frequentist include mahalanobis distance gnanadesikan 2011 and creation of funnel plots bird et al 2005 since the monte carlo routine constitutes an important and useful component of ewe strategies to minimise data disruption and outlier generation should be investigated an exiguous unequal distribution of leverage amongst input parameters is also suggested regarding explored metrics b p b and q b both individually and paired impacted predictions precision in a comparable fashion across operational models b however resulting more consistently influential overall see essington 2007 in the instance of kempton s q index this can be apportioned to its biomass based nature the index expresses biomass diversity of upper trophic level tl taxa tl 3 within a system kempton and taylor 1976 cited in ainsworth and pitcher 2006 and error in input biomass translates therefore into error in the predicted diversity metric the slight dominance of input biomass on predatory biomass sensitivity can be ascribed to the same biomass based nature output b and ee prediction errors were also unequally affected by data types input b being again most consistently influential this can be explicated with reference to the mass balance requirements of ecopath whereby total production and total losses must be at equilibrium christensen and walters 2004 as the former corresponds to the product of b and production to biomass ratio p b imprecision in such variables likely propagates to biomass based measures suggesting that the mass balance constraint of ecopath models may be insufficient to avert inaccurate predictions in addition to input b p b was identified as the second most influential parameter on the precision of tst outputs also consistently responsive across all investigated scenarios tst is estimated as the sum of all flows within a system these consisting in total consumption i e q b total export total respiration and total flows to detritus the relative failure of q b to exert the greatest impact may thus seem surprising however when the second master equation on which the ecopath module is based christensen and walters 2004 is recollected and consumption deconstructed into individual components the role of the production term is evidenced 3 consumption production respiration unassimilated food by expressing the equation in formal terms equation 4 is obtained 4 bi q b i bi p b i ri ui considering the abovementioned calculation of total production the dominant role of b and p b in the determination of tst response can be comprehended although analysed functional groups were model specific the broad category of predatory fish which include bathydemersal and pelagic fish appeared to be more responsive to input imprecision overall than any other group however all functional groups exhibited some degree of deviation from nominal values as well as significant inter group differences in their response to imprecision supplementary material section 2 2 however the model specific nature of most functional groups and their attribution to often broad taxonomic categories e g predatory fish rather than fish species listed individually hinders comparison between models and identification of species specific response to imprecision within the model becomes challenging the present simulations further indicate the apparent absence of linearity in the relationship between confidence intervals and model predictions with the greatest source of data imprecision failing to exert consistently the greatest impact on the model outputs considered essington 2007 input uncertainty is suggested to impact both within and cross model parameter precision differentially with distinct levels of parameter imprecision engendering effects of differing magnitude for a given variable this suggests that the subjective process of model balancing originating from an identical initial dataset herein simulated through the monte carlo routine is likely to generate markedly different values across separate simulations thus contributing to predictions diverging from their nominal values 4 1 implications future research ecosystem based models and ewe in particular are implemented increasingly in a vast array of scientific disciplines and decision making processes a more comprehensive understanding of model variance and imprecision propagation is thus crucial to evaluate and construe ewe model predictions with a greater degree of certainty to this end this paper contributes to the available body of literature elucidating the behaviour of six ewe commonly predicted parameters under a diverse set of software specific imprecision conditions information on the input variables to which the model outputs appear at a preliminary stage to be most responsive and the predicted parameters outputs whose behaviour is most greatly affected are herein presented consequently this work aims to provide the basis for identification of areas towards which further research efforts should be directed the utility of the results presented herein is that to facilitate anticipation of the likely prediction error in investigated parameters when complete confidence in input data is missing this is particularly true for the kempton s q index whose behaviour remained rather constant within and across all investigated models under distinct software specific imprecision scenarios when utilised in conjunction with other ecosystem indicators kempton s q affords an effective measure to i assess impacts of alternative fisheries management strategies ii trace effect of climate variations on biodiversity iii estimate non consumptive value of ecosystems and ultimately iv inform ecosystem based management approaches ainsworth and pitcher 2006 irrespective of their nature ecosystem indicators are of particular scientific interest biodiversity measures holding however a special appeal to the public owing to their synoptic nature which also augments the parameter s policy relevance ainsworth and pitcher 2006 in this regard by identifying biomass as a possible high leverage parameter measures to ensure the highest possible level of certainty can be implemented to increase the reliability of important output metrics as a result given that biomass estimates are highly susceptible to inaccuracy christensen et al 2000 achieving greater exactness of such estimates becomes paramount integration of controlling mechanisms to halt imprecision propagation from input to output e g inclusion of equilibrium constants orr et al 2018 in addition to the existing mass balance constraint could enhance possibly ewe error screening capacities by investigating output behaviour under a vast array of software specific imprecision and uncertainty conditions this paper has attempted to cover most of the possible scenarios in which one or more input parameters were inaccurate in their entirety or for a subset of functional groups to assess the extent to which the model outputs would diverge from nominal values by consulting this paper and identifying the most relevant uncertainty scenario users may be able to anticipate the most likely degree of imprecision in their output metrics despite a general trend of little and negligible variations because of imprecision in inputs two ecological indicators kempton s q and tst were observed to experience sufficient shifts in values to warrant further and thorough investigation the present paper should encourage scientists to subject ewe to the most appropriate gsa approach to identify the extent to which high leverage input parameters influence observed imprecision in outputs due to the unique nature of ewe this paper also demonstrates the importance of providing users with access to a dedicated tool within the software itself to conduct a thorough assessment of individual models that accounts for the specific nature of the software and its different components static and mass balanced time dynamic and temporo spatial dynamic this would reduce computational expense associated with sa especially gsa and allow even practitioners unfamiliar with sa to assess reliability of their models the question of ewe precision would also greatly benefit from larger scale research evaluating consequences of diverse typologies of uncertainty an example of which is structural uncertainty deriving from omission of important yet poorly understood species mcallister and kirchner 2002 and imprecision on a vaster array of predicted measures ultimately comparing such results with those generated by identical assessments conducted on alternative ecosystem modelling software 5 conclusions analyses aimed at the determination of imprecision susceptible parameters identified kempton s q index and tst as the most consistently responsive of the six investigated parameters irrespective of model perturbed input parameter and degree of alteration except for relatively sporadic instances of considerable imprecision kempton s q exhibited low to moderate prediction error and a rather constant behaviour under the entirety of explored uncertainty scenarios tst was characterised by significantly higher mean prediction error instead the usefulness of such results lies in the behavioural predictability itself of the two parameters contributing to modellers ability to anticipate likely prediction errors when confidence in the quality of input data is deficient an uneven distribution of leverage amongst input parameters was identified with biomass constituting the most consistently influential overall this is likely a by product of the mass balance constraint of ecopath models whereby total production estimated as the product of b and p b and total losses must be at equilibrium propagation of imprecision from input biomass to biomass based outputs suggests therefore a potential inadequacy of the mass balance constraint in averting unprecise predictions in this regard identification of biomass as a high leverage parameter may be viewed as an incentive to enhance confidence in input data and augment reliability of pivotal parameters as a result due to the intrinsic susceptibility of biomass estimates to inaccuracy attainment of greater exactness becomes fundamental author contributions i s conceived and designed the research i s performed the research and analysed the data i s wrote the manuscript and v l g t provided comments on the manuscript funding this research did not receive any specific grant from funding agencies in the public commercial or not for profit sectors declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we would like to thank the ecopath with ecosim community for the operational models and for technical support the first author would also like to thank dr helene burningham associate professor in the department of geography at university college london for her supervision and support during the initial development of this research appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105098 
25792,a novel ensemble based conceptual data driven approach for improved streamflow simulations anna e sikorska senoner a john m quilty b a department of geography university of zurich winterthurerstrasse 190 8057 z√ºrich switzerland department of geography university of zurich winterthurerstrasse 190 z√ºrich 8057 switzerland department of geography university of zurich winterthurerstrasse 190 8057 zurich switzerland b department of civil and environmental engineering university of waterloo 200 university avenue west waterloo on n2l 3g1 canada department of civil and environmental engineering university of waterloo 200 university avenue west waterloo on n2l 3g1 canada department of civil and environmental engineering university of waterloo 200 university avenue west waterloo on n2l 3g1 canada corresponding author a novel ensemble based conceptual data driven approach cdda is developed where a data driven model ddm is used to correct the residuals from an ensemble of hydrological model hm simulations the cdda respects hydrological processes via the hm and it benefits from the ddm s ability to simulate the complex relationship between residuals and input variables the cdda can accomodate any hm and ddm allowing for different configurations to be tested the cdda is tested for ensemble streamflow simulation in three swiss catchments where the hm hbv hydrologiska byr√•ns vattenbalansavdelning is coupled with eight different ddms multiple linear regression k nearest neighbours regression second order volterra series model artificial neural networks and two variants of extreme gradient boosting xgb and random forests rf the proposed cdda was able to improve the mean continuous ranked probability score by 16 29 over the standalone hm since xgb and rf demonstrated the best performance they are recommended for simulating the hm residuals graphical abstract image 1 keywords ensemble streamflow simulation data driven model hydrological model residuals uncertainty 1 introduction the hydrology and water resources domains have witnessed growing interest in using data driven models ddms to improve the simulation of hydrological variables streamflow rainfall groundwater levels etc shen 2018 nearing et al 2020 xu and liang 2021 whether by directly replacing e g kratzert et al 2019 or being used in conjunction with e g boucher et al 2020 hydrological models hms in this study a novel framework is proposed where ddms are used for improving ensemble based hm simulations where the ddms act as residual models in other words ddms are used to correct the model residuals errors stemming from an ensemble of hm simulations while the proposed framework is general to any type of hm water quantity or quality this study focuses on ensemble streamflow simulation due to its importance to the hydrology and water resources communities bourdin et al 2012 hm errors impair estimates of design floods risk assessment and in general water resources management farmer and vogel 2016 hm errors result from numerous sources of uncertainty such as model structure input and output variables parameters initial conditions etc kuczera et al 2010 renard et al 2011 mcmillan et al 2012 sikorska and renard 2017 although hm errors have been traditionally assumed to be independent and normally distributed it is now widely accepted that this assumption does not hold since hm residuals are highly auto correlated sorooshian and dracup 1980 yang et al 2007 sikorska et al 2012 hence they cannot be modelled as a normally distributed random variable as a result alternative methods that do not rely on these limiting assumptions have gained widespread attention in hydrology available methods for modelling hm errors are typically classified into two major groups ni et al 2020 the first group uses various tools for hm error identification and estimating related uncertainties from methodologically simple monte carlo experiments with different hm parameter sets e g padiyedath gopalan et al 2019 to more advanced likelihood based methods see examples in the paragraph below the second group uses data driven approaches to directly link the hm simulations to its input variables within a hybrid modelling approach althoff et al 2021 among the first group of methods bayesian based methods have received the most attention since they are able to simulate the auto correlation of model residuals and quantify contributions from different uncertainty sources mantovan and todini 2006 renard et al 2011 yet they require making an explicit assumption on model error properties with introducing additional parameters for the model inference and a definition of the likelihood function to sample from the posterior distribution montanari and koutsoyiannis 2012 sikorska et al 2012 2015 smith et al 2015 alternative methods condition model errors on input and or output variables del giudice et al 2013 mcinerney et al 2017 link their parameters i e of the error distributions to flow conditions schaefli et al 2007 or introduce their time dependent description pianosi and raso 2012 alternatively several methodologically simpler likelihood free approaches are frequently applied that rely on metaheuristic search algorithms maier et al 2014 piotrowski et al 2017 such as genetic algorithm particle swarm optimization where model deficiencies are accounted for via using multiple generated parameter sets instead of making assumptions on model errors the second group of methods relies on ddms that establish statistical links between target and input variables using a training dataset but do so without including a hm bowden et al 2005 solomatine and ostfeld 2008 thus ddms are very powerful for modelling complex relationships between input and output variables or for gaining insights on governing physical processes tongal and booij 2018 without the need to explicitly consider the physical laws governing such processes hence they are constantly increasing in popularity within hydrology and have been applied for many different hydrological and water resources applications precipitation runoff modelling rajurkar et al 2004 shortridge et al 2016 tongal and booij 2018 streamflow forecasting solomatine and xue 2004 boucher et al 2011 papacharalampous and tyralis 2018 ni et al 2020 tyralis et al 2020 groundwater level forecasting suryanarayana et al 2014 rahman et al 2020 water quality modeling fatehi et al 2015 bhagat et al 2021 urban water demand forecasting quilty et al 2019 and many others however ddms require the pre selection of input variables as potential predictors which greatly impacts their accuracy galelli et al 2014 in addition since hydrological variables often display auto and cross correlation it is important to consider time lagged versions of the input variables e g precipitation air temperature when simulating a target hydrological variable e g streamflow gauch et al 2021 and to be able to identify the maximum time lag memory length this maximum time lag defines the time lag after which the impact of the input variable on the target variable is marginal bowden et al 2005 a correct pre selection of input variables enables the exclusion of redundant and or irrelevant input variables thereby reducing the complexity and increasing the interpretability of the resulting ddm galelli et al 2014 most previous data driven approaches for streamflow simulation have used ddms to explicitly simulate streamflow and in this way they provide an alternative to a hm yet only very few works have attempted to characterize or directly simulate residuals of a hm with a data driven approach some examples include montanari and koutsoyiannis 2012 sikorska et al 2015 wani et al 2017 and ehlers et al 2019 who simulate model errors via resampling from a query dataset based on streamflow properties and other hydro meteorological data in the case of ehlers et al 2019 however each of these studies relied on a specific statistical method for resampling model errors the meta gaussian approach in montanari and koutsoyiannis 2012 and k nearest neighbours in the others and therefore their approaches are not generalized to any ddm as is the case in this study approaches for coupling hms with ddms into a hybrid model are very rare in application to streamflow modelling wang et al 2021 for example tongal and booij 2018 decomposed streamflow into base and surface flows and used hm simulations at the previous time step along with meteorological variables as input to a ddm to simulate streamflow at the current time step indirectly linking model residuals to flow conditions senent aparicio et al 2019 have proposed an indirect coupled approach in which streamflow simulations from a ddm are corrected using additional information on maximum mean daily flow that is taken from a hm yang et al 2020a have proposed a different indirect coupled approach that used a hm to simulate pseudo observed data for training a ddm to perform streamflow simulation hybrid streamflow simulation models where a ddm is used to model hm residuals have to the best of the authors knowledge only been explored by wu et al 2019 and konapala et al 2020 both studies used ddms for simulating the residuals of hms but did not consider ensemble streamflow simulation ignoring the uncertainty in estimating hm parameters in addition wu et al 2019 required the transformation of hm residuals prior to modelling via ddms the determination of stationary time windows as identified by the hilbert huang transform in order to simulate the hm residuals and assumed that the simulated hm residuals follow a student s t distribution to permit the construction of uncertainty intervals more recently wang et al 2021 proposed a hybrid model that uses the output of the xinanjiang hm along with wavelet decomposed sub series of previous streamflow observations as input to random forests rf for simulating a single realization of streamflow without considering any sources of uncertainty althoff et al 2021 proposed an alternative hybrid model that consisted of a simplified version of a hm soil moisture component and a ddm to simulate streamflow however they also generate only a single realization of the streamflow and do not consider any sources of uncertainty as can be seen from the above literature review despite several recently proposed hybrid modelling approaches none of the above mentioned studies developed a fully coupled framework for an ensemble based streamflow simulation where any ddm can be used to simulate an ensemble of residuals stemming from a hm conditioned on input variables e g precipitation air temperature thus in this study a novel conceptual data driven modelling framework is developed that pairs an ensemble of conceptual deterministic hms with an ensemble of ddms that simulate the hm residuals for improved ensemble streamflow simulation this novel framework is called an ensemble based conceptual data driven approach cdda while the hm simulates the precipitation runoff process at the catchment scale respecting to an extent the physical hydrological processes a ddm added on top of the hm mimics the hm residuals enabling the streamflow simulations to be improved thus the cdda combines the advantages of a hm that seeks to respect the physical relationships between hydrological processes with the capabilities of the ddm that can model complex nonlinear relationships between input target variables enabling the accurate estimation of correlated residuals stemming from the hm the output of the cdda framework is an ensemble of streamflow simulations instead of a single streamflow realization conceptually the hm generated ensemble passes information about the observed and simulated streamflow gained through the identification of the hm parameters i e calibrated using observed data to the ddm which is relied upon to extract additional information not captured by the hm to simulate the hm residuals and improve the overall ensemble streamflow simulation thus the cdda can overcome certain issues of the above mentioned hybrid models since it provides an ensemble of hm residuals accounting for hm parameter uncertainty and does not require any transformation of hm residuals relaxing all assumptions on the residual distribution these last two strengths of the proposed cdda also make it more attractive than the bayesian method described above that requires specification of a likelihood function and the distribution of the model residuals thus the novel cdda can be seen as a less restrictive and more flexible data driven method for simulating hm residuals and generating an ensemble of streamflow simulations the focus of this paper is primarily on developing the cdda framework that enables for any case study where data required by the hm is available the identification of the best ddm to simulate hm residuals and the selection of the most useful input variables to use in the ddm since any ddm can be used within the cdda eight different ddms that have either been explored in detail or shown to be promising in recent studies in the hydrological and water resources modelling literature are explored multiple linear regression mlr k nearest neighbours regression knn second order volterra series model sov artificial neural networks ann as well as two variants of extreme gradient boosting xgb and random forests rf the ddms consider time lagged versions of observed streamflow precipitation and air temperature as potential predictors and are coupled with a bucket type hm hydrologiska byr√•ns vattenbalansavdelning or hbv bergstr√∂m and forsman 1973 and tested using three swiss catchments furthermore several input variable selection ivs methods are considered for selecting the most important candidate input variables to use in the ddms thus the major objectives of this study are to introduce the ensemble based cdda framework and evaluate its performance against a standalone ensemble based hm model in addition the proposed cdda enables one to answer questions specific to individual case studies such as a which ddms are most suitable for simulating the residuals of the hm within cdda b which input variables are the most important to consider when simulating hm residuals via the explored ddms thus these questions are also answered for the case study explored herein the framework developed in this study is timely as coupled hm ddm approaches are only beginning to consider uncertainty in resulting streamflow simulations e g papacharalampous et al 2019a tyralis et al 2019a boucher et al 2020 teweldebrhan et al 2020 although no such framework exists that uses ddms to simulate the residuals from an ensemble of hm simulations the remainder of this paper is organized as follows section 2 describes the methods by introducing the theoretical cdda and providing details on candidate ddms selected for simulating the hm residuals section 3 includes the experimental settings an overview of the case study as well as the hm and ddm development details section 4 highlights the main results section 5 discusses the significance of the results describes the current limitations of cdda and suggests future research avenues and section 6 concludes the paper 2 methods 2 1 overview of the conceptual data driven approach cdda a data driven model is developed that simulates the residuals of a hydrological model using input explanatory variables which in the case of streamflow simulation are usually current and time lagged precipitation and air temperature as well as time lagged streamflow if available the ddm is attached on top of a single hm simulation e g streamflow based on a given or calibrated parameter set thus the target response variable for the ddm is the residual between the observed streamflow and the hm simulated streamflow at time step t 0 when multiple parameter sets are available for the hm i e an ensemble of hm streamflow simulations a ddm is attached to each hm simulation i e there is a single ddm trained for each set of residuals this latter models setup i e an ensemble hm ddm is what is referred to as the ensemble based conceptual data driven approach or simply cdda please see fig 1 for an overview of the cdda the residuals of the hm are simulated using different ddms see section 2 3 for details and different input variable combinations are tested section 2 4 to explore which set of inputs best simulate the residuals of the hm in this study the observed streamflow is assumed to be available at a regular time interval at a given site in case observed streamflow is unavailable both the hm and ddm cannot be calibrated and other indirect approaches would be required see section 5 2 to evaluate the usefulness of the cdda the ensemble of simulations from the hm without any ddm is used as the benchmark section 2 6 the eight different variants of the cdda are compared against each other and the benchmark to determine the best ddm and identify useful input combinations 2 2 ensemble based cdda the streamflow simulation for the ensemble member y i of the cdda output is the sum of the hm simulation y i and the simulation of its residual using the ddm r i y i y i r i 1 y i p t q Œ∏ c d d a y i p t Œ∏ h m r i p t q Œ∏ d d m where p t and q are observed precipitation air temperature and streamflow Œ∏ c d d a is a parameter set of the cdda Œ∏ h m and Œ∏ d d m are the parameter sets of the hm and ddm models respectively with Œ∏ c d d a Œ∏ h m Œ∏ d d m note that each ensemble member is associated with a single Œ∏ c d d a by applying eq 1 to each ensemble member an ensemble of streamflow simulations can be obtained through cdda finally while the hm requires p and t as input to simulate streamflow for a given day the ddm can additionally use p t and q from previous days i e time lagged copies of these variables in order to improve predictive performance this idea is discussed in more detail in section 2 4 the hm is run at a daily time step as is the ddm depending on the chosen hm structure other input variables may be also required for the hm e g potential evapotranspiration and thus could also be explored within the ddm similarly using a different time step for the hm e g hourly may require the ddm to be conditioned on input variables of a different resolution and different time lag length 2 3 data driven models ddm in this study six different ddm and eight variants in total were explored for simulating the residuals of the hm since this section is intended to provide only a brief explanation of each ddm references to appropriate literature are included for the reader interested in a more detailed treatment of the various methods however due to the ubiquity of multiple linear regression mlr which is one of the adopted ddms no details are provided for this model 2 3 1 k nearest neighbours regression knn k nearest neighbours regression introduced first by fix and hodges 1951 is one of the simplest nonlinear methods to apply altman 1992 mitchell 1998 liu et al 2004 hastie et al 2009 knn generates predictions for a query vector i e an input variable vector by applying two simple steps 1 searching for a set of k neighbours i e input variable vectors in a training dataset that are closest to the query according to a predefined distance metric e g euclidean distance and 2 taking the average of the target response variable associated with each of the k closest neighbours thus knn is a local regression technique despite its simplicity one of the main drawbacks of knn is that it cannot extrapolate beyond the range of the target in the training set nonetheless knn has a rich history in hydrology karlsson and yakowitz 1987 and is still frequently used for simulating and predicting streamflow lee et al 2017 ebrahimi et al 2020 among other hydrological applications sun and tevor 2017 jiang et al 2020 for more information on knn the interested reader may refer to chapter 2 3 of hastie et al 2009 2 3 2 second order volterra series model sov similar to knn the second order volterra series model sov is another simple nonlinear regression technique sov was initially proposed for estimating non linear interactions between variables commonly considered in hydrologic systems amorocho 1963 and subsequently applied for rainfall runoff simulation amorocho and brandstetter 1971 diskin et al 1984 and more recently for forecasting streamflow maheswaran and khosa 2012 groundwater levels maheswaran and khosa 2013 urban water demand quilty and adamowski 2018 and soil moisture prasad et al 2018 as well as downscaling climatic variables lakhanpal et al 2017 sehgal et al 2018 sov is essentially a polynomial regression where the design matrix is created by considering all zero first and second order interactions amongst the input variables the coefficient attached to the zero order term is the bias and the remaining coefficients attached to first and second order terms are considered kernel coefficients wu and kareem 2014 after they have been estimated the coefficients can be used to generate predictions of the target variable for a given input variable vector the reader may consult the supplemental material of quilty and adamowski 2020 for the mathematical formulation of the sov adopted in this study 2 3 3 artificial neural networks ann artificial neural networks have been applied to streamflow prediction since the 1990 s abrahart et al 2012 and represent one of the most popular data driven models used for hydrological prediction fahimi et al 2017 ann generates predictions by passing a vector of input variables through a network comprising of a series of weights that connect to neurons which contain a bias and potentially a nonlinear activation function that transforms the weighted inputs and directs them to the next layer of the network the output of the network is the sum of weighted and nonlinearly transformed inputs the reason why ann is so powerful is due to its universal approximation capabilities which allow it to approximate any nonlinear function i e mapping from input variables to the target provided nonlinear infinitely differentiable activation functions are used in the hidden layer and enough training iterations epochs as well as model parameters weights and biases are considered hornik et al 1989 the type of ann used in this study is a feed forward backpropagation ffbp network with a single hidden layer along with the usual input and output layers representing the input variables and target respectively the ffbp ann operates by initializing all network parameters i e weights and biases to small random values and iteratively updates these parameters by back propagating the error signal through the network for a fixed number of epochs or until sequential evaluations of the error function do not appreciably decrease with respect to a predefined tolerance after it has been trained ann generates predictions by passing input variable vectors through the network where the resulting output is the sum of weighted and nonlinearly transformed inputs for additional information on ann the reader can refer to chapter 5 of ripley 1996 or many of the references included in review articles in the domains of statistics ching and titterington 1994 or hydrology abrahart et al 2012 wu et al 2014 2 3 4 random forests rf random forests introduced by breiman 2001 are a class of ensemble decision trees that include random sampling of both training instances often referred to as bagging and input variables reducing predictive variance without increasing the bias of the model breiman 1996 while also providing robust performance in the presence of noisy input variables biau 2012 a very useful feature of rf is that it implicitly provides an estimate of input variable importance gr√∂mping 2009 which can be used for input variable selection genuer et al 2010 see for example tyralis and papacharalampous 2017 rf generates predictions by taking the average across all predictions produced by each ensemble member despite its widespread success in numerous domains such as genomics chen and ishwaran 2012 remote sensing belgiu and drƒÉgu 2016 rf is only beginning to grow in popularity within hydrology tyralis et al 2019b although rf is still relatively new to the hydrology domain it has been used for streamflow simulation shortridge et al 2016 forecasting papacharalampous and tyralis 2018 and reconstruction li et al 2019 groundwater level forecasting rahman et al 2020 and downscaling extreme rainfall pham et al 2019 among other applications the interested reader may refer to chapter 15 of hastie et al 2009 for details on rf as well as fawagreh et al 2014 for a review of different rf variants and applications 2 3 5 extreme gradient boosting xgb the extreme gradient boosting model is a very recent method that combines decision trees and boosting chen and guestrin 2016 while bagging as used in rf operates by developing an ensemble of independent models on each resampled dataset boosting instead builds an ensemble of models where each member is built on top of the residuals generated by the previous member with the objective to reduce the errors made in the previous iteration in other words bagging reduces the variance of the predictors while boosting reduces bias vanschoren et al 2012 both addressing opposite aspects of the bias variance trade off and using different strategies for ensembling decision trees mehta et al 2019 while recent research has shown that combining bagging and boosting can lead to better performing ensemble decision trees ghosal and hooker 2020 such methods are not within the scope of this study xgb is an improved version of the gradient boosting machine that is more computationally efficient and less prone to overfitting due to l2 norm regularization chen and guestrin 2016 similar to rf xgb inherently estimates the importance of input variables and can be used for input variable selection e g chen et al 2020 rahman et al 2020 zhang et al 2020 xgb has only recently been considered within the hydrological domain but has shown promise for streamflow simulation hadi et al 2019 gauch et al 2021 and forecasting ni et al 2020 tyralis et al 2020 predicting daily reference evapotranspiration fan et al 2018 water quality index prediction abba et al 2020 water table depth forecasting br√©dy et al 2020 and imputing missing sub hourly precipitation records chivers et al 2020 among other applications additional details on the theory and main innovations behind xgb can be found in chen and guestrin 2016 2 4 input variable selection ivs since input variable selection is an important step in the development of any ddm galelli et al 2014 for each of the six ddms mlr knn sov ann rf and xgb ivs was carried out to identify the input variables that may be useful for simulating the hm residuals different ivs approaches were considered for the ddm the linear partial correlation input selection pcis algorithm may et al 2008 was paired with mlr to serve as a fully linear benchmark the edgeworth approximations based approach ea was used to carry out conditional mutual information cmi based ivs which is a nonlinear analogue to pcis quilty et al 2016 the ea approach was coupled with knn sov and ann since both rf and xgb implicitly perform ivs an external model free ivs method e g pcis ea was not necessary the pcis method was selected as it is one of the most popular linear ivs methods galelli et al 2014 while the ea method was selected as it has been shown to provide similar ivs accuracy as competing cmi based methods e g based on kernel density estimation k nearest neighbours at a fraction of the computational cost quilty et al 2016 a short description of the different ivs methods is provided in the sub sections below 2 4 1 partial correlation input selection pcis pcis is an iterative greedy ivs method whereby candidate input variables i e time lagged copies of observed streamflow precipitation and air temperature are selected one at a time based on their partial correlation with the target variable i e hm residuals at t 0 conditioned on previously selected inputs at each iteration the candidate input variable with the highest partial correlation i e the best candidate input variable is selected and a predefined ivs stopping condition is checked the version of pcis adopted in this study uses the bayesian information criterion bic as a stopping condition to halt the ivs procedure see galelli et al 2014 at each iteration of the ivs procedure an mlr model is built that predicts the target variable using all previously selected inputs and the best candidate input variable afterwards the bic is measured if the bic increases for the current iteration compared to the previous one the ivs procedure is halted and all input variables selected before the current iteration are returned otherwise the ivs procedure continues the variable importance measure for pcis is the partial correlation coefficient pc which ranges between 1 and 1 with 0 representing independence while 1 and 1 represent perfect correlation additional details on pcis can be found in may et al 2008 and galelli et al 2014 2 4 2 edgeworth approximations based conditional mutual information ea the ea approach to cmi based ivs was introduced and discussed in detail in quilty et al 2016 thus only the essential features of this method are described here similar to pcis ea is an iterative greedy ivs method however in ea the cmi is estimated between candidate input variables and the target conditioned on previously selected inputs instead of partial correlation the stopping condition used for ea is based on a modified version of the tolerance based approach from vlachos and kugiumtzis 2010 where at each iteration the cmi between the best candidate input variable and the target variable conditioned on all previously selected inputs is compared to the mutual information between the target variable and all previously selected input variables including the best candidate input variable if the ratio between these two quantities drops below the tolerance ranging between 0 and 1 the ivs procedure halts and all input variables selected before the current iteration are returned by increasing the tolerance the number of selected input variables can be decreased the only difference between the ea method adopted here and in quilty et al 2016 is in the choice of stopping criterion it was found during earlier experimentation that the tolerance based method provided greater control over the ivs process facilitating an improved balance between computational run time and model accuracy compared to the method used in quilty et al 2016 trial and error led to the selection of 0 05 as a suitable threshold to balance these two objectives typically for cmi based ivs adopting the tolerance threshold stopping criterion values of 0 01 0 03 0 05 and 0 15 are common vlachos and kugiumtzis 2010 tsimpiris et al 2012 note that these tolerance values are obtained by subtracting the tolerance values mentioned in the above studies from 1 the reason for this is due to our modified formulation of the stopping criterion presented in vlachos and kugiumtzis 2010 the variable importance measure for the ea method is the partial informational correlation pic which is a nonlinearly scaled version of cmi sharma and mehrotra 2014 such that the pic ranges between 0 and 1 with 0 representing independence between variables and 1 indicating perfect correlation 2 4 3 variable importance decision tree based methods the decision tree based methods rf and xgb generate internal measures of variable importance that are useful in identifying the most important inputs to the model pathy et al 2020 wang et al 2015 in contrast to the pcis and ea methods which are considered model free approaches rf and xgb are model based ivs approaches while the input variable importance measures generated by rf and xgb can also be used for ivs in other ddm see for instance chen et al 2018 hadi et al 2019 prasad et al 2019 chen et al 2020 tyralis et al 2020 bhagat et al 2021 this study only uses the variable importance measures to identify the most useful variables specific to these individual models for rf the variable importance score is the total decrease in node impurity that occurs by splitting on a particular input variable and averaged over all decision trees measured by the sum of squared errors lee et al 2017 wright and ziegler 2017 the variable importance scores generated by rf were normalized by dividing each input variable s score by the maximum variable score see eq 1 in deng and runger 2013 resulting in the normalized variable importance nvi the variable importance vi score adopted for xgb is the fractional contribution of each input variable to the model prediction averaged across all trees also known as gain lee et al 2017 where the model s predictive performance is measured by the sum of squared errors loss function chen et al 2019 for both rf and xgb higher importance scores represent variables that are more important than the others lee et al 2017 2 5 benchmark method as discussed in section 2 1 the cdda can generate an ensemble of streamflow simulations via eq 1 in order to evaluate the added value of using the cdda it must be compared against a benchmark in this case the benchmark is simply the ensemble streamflow simulations generated by the hm model for each Œ∏ h m i e without any model for simulating the hm residuals 2 6 performance metrics in order to compare the performance of cdda against the benchmark hm as well as to identify the best ddm to use within the cdda a number of statistical performance metrics were used the performance metrics were divided into two classes ensemble and deterministic metrics the ensemble metrics make use of all ensemble members when evaluating the simulations performance while the deterministic metrics are computed using the mean ensemble member i e the mean simulation at each simulation time step since the metrics adopted in this study are well known in the hydrology and water resources communities the formulae used to calculate these scores are not provided although the cited sources include the necessary information to permit their calculation the ensemble metrics consist of the mean continuous ranked probability score crps br√∂cker 2012 the mean interval score mis gneiting and raftery 2007 and the average width aw xiong et al 2009 both the mis and aw require the specification of a confidence level which is taken to be 0 05 in this study for a particular confidence level upper and lower uncertainty intervals can be computed either by assuming the ensemble members follow a particular distribution e g gaussian or by empirical means such as estimating the quantiles associated with the confidence level the latter approach is followed in this study thus the upper and lower uncertainty intervals are estimated at the 0 975 and 0 025 quantiles respectively and they together define the 95 uncertainty intervals the crps was calculated using the scoringrules r package jordan et al 2019 while the mis and aw were calculated using custom r functions the crps is a useful metric that evaluates a simulation s reliability and sharpness reducing to the mean absolute error mae for deterministic simulations permitting the comparison between ensemble and deterministic forecast quality boucher et al 2011 the mis also evaluates a simulation s reliability and sharpness but additionally includes a penalty for simulations that fall outside the upper and lower uncertainty intervals papacharalampous et al 2020 the aw solely measures the simulation s sharpness i e the narrowness of its uncertainty intervals in general a high quality ensemble simulation should have crps mis and aw as low as possible however preference in this study is given to simulations with lower crps and mis scores since a low aw score is not highly informative if the simulations are unreliable finally since the cdda seeks to improve the predictive performance of the ensemble hm simulations which themselves only consider parametric uncertainty through ddm the term uncertainty intervals as adopted here is more closely related to confidence intervals cis than prediction intervals thus cis are referred to when the model results are discussed in general prediction intervals take into account the uncertainty in the model output which is not done here since we use the ddm to predict the expectation of the individual hm errors not their distribution the deterministic metrics adopted for evaluating the ensemble models include the mean absolute error mae root mean square error rmse nash sutcliffe efficiency nse and the kling gupta efficiency kge the advantages of these metrics are described in detail within the supplemental material of papacharalampous et al 2019b the deterministic metrics were calculated using the hydrogof r package zambrano bigiarini 2017 3 experimental settings 3 1 study catchments three swiss mid size mid altitude catchments were chosen for this study fig 2 all three catchments have an insignificant areal glacier percentage 5 and are without any significant human direct impacts documented in the observation period 1981 2015 according to sikorska senoner and seibert 2020 the catchments represent two different types of dominant flood processes i e rainfall driven d√ºnnern and a mixed contribution of rainfall and snowmelt floods kleine emme and muota see also table 1 3 2 observed data the available observed data consists of continuous records of precipitation depth mm day minimum mean and maximum daily air temperature c daily evaporation rates mm day and streamflow at catchment outlets mm day all variables were available for the period 1981 2014 at a daily resolution the meteorological data were made available from the meteoswiss and the hydrological observations from the swiss federal office for the environment foen all variables were averaged to mean catchment values using the thiessen polygon method 3 3 hydrological model in this study a conceptual hbv model in particular hbv light seibert and vis 2012 was used for streamflow simulations this bucket type model consists of four major routines 1 precipitation excess and snowmelt 2 soil moisture 3 groundwater and runoff streamflow response and 4 routing in the stream the snow component is important for catchments with significant snow processes i e for two out of three studied here hbv has been frequently applied to rainfall and snow dominated catchments e g breinl 2016 griessinger et al 2016 sikorska and seibert 2018 westerberg et al 2020 the hbv model used in this study has 15 parameters and is run at a daily time step model inputs are precipitation and air temperature time series as well as long term averaged values of daily potential evapotranspiration and air temperature the model output is a single deterministic realization of continuous streamflow time series at the catchment outlet due to its simplicity hbv is used as an example in this study but can be easily interchanged with another hm without any difficulties 3 4 calibration of the hydrological model the hbv model was calibrated via the genetic algorithm and powell gap optimization method seibert 2000 using the kling gupta efficiency kge as the objective function gupta et al 2009 calibration involved 1000 independent runs with randomly selected initial values for the 15 parameters resulting in 1000 optimized and equally plausible parameter sets representative of model parameter uncertainty this uncertainty mainly results from parameter equifinality beven and freer 2001 and to a lesser degree the randomization involved in initializing the parameter sets running multiple independent model calibration runs with randomly chosen initial values has been proposed by sikorska senoner et al 2020 as a heuristic approach to deal with the parameter equifinality problem such an optimization approach should ensure that the parameter space is fully explored and should minimize the possibility of being trapped in the same local optimum during different calibration runs such an approach is prioritized over the likelihood based methods since it does not require making any assumption about the model errors which are simulated with a dedicated ddm in the cdda the hbv model was calibrated using data from 1985 2004 with a preceding 4 year warm up period of 1981 1984 and validated using years 2005 2014 at a daily time step in the three study catchments the kge score achieved in the validation period table 2 for the mean ensemble varied from 0 80 in the d√ºnnern catchment to 0 87 in the muota and kleine emme catchments these 1000 optimized parameter sets are afterwards used within hbv to generate an ensemble of streamflow simulations and subtracted from the observed streamflow to generate an ensemble of hm residuals finally the inputs are determined through ivs and the ddms are calibrated using the selected inputs for each of the 1000 hm residuals 3 5 input variable selection and calibration of data driven models prior to the calibration of the ddms input variable selection was required input variables for the ddm included observed streamflow precipitation and air temperature at the current and or previous nine days for streamflow inputs included observations from the nine days preceding the simulation day t 1 t 9 for precipitation and air temperature inputs included observations from the day of the simulation t 0 as well as the previous nine days t 0 t 9 in total 29 different input variables were considered as potential predictors of the hm residuals at t 0 the maximum time lag d for each input variable was determined using the conditional mutual information brown et al 2012 between the hm residuals at t 0 and each explanatory variable from t 0 to t d with the exception of streamflow which considered time lags t 1 to t d by locating the lag at which the cmi reached a local minimum the goal was to identify a sufficient number of time lags to accurately simulate the hm residuals while also attempting to keep the input variable set of a reasonable size given that precipitation has the largest effect on modifying the streamflow m√ºft√ºoƒülu 1991 it was given a higher priority in identifying the maximum time lag this approach resulted in a maximum time lag of d 9 section 2 4 2 outlines the method used to estimate cmi the different ddms were calibrated trained using the target residuals of the hm simulated streamflow and input variables time lagged versions of the observed precipitation air temperature and streamflow for the same calibration period as the hbv model i e years 1985 2004 the remaining data years 2005 2014 was used for validating the ddms to ensure consistency with the hm in the sub sections below the different ddm parameters and hyper parameters are described along with the strategy used to train the various ddms all ddms were developed on an intel r core tm i7 8750h cpu 2 20 ghz laptop with 32 0 gb ram the ann rf and xgb models were run in parallel on 10 cpu cores 3 5 1 k nearest neighbours regression knn as noted in section 2 3 1 the knn model does not require any explicit training strategy since model predictions are generated by computing the distance here the euclidean distance of a given input variable vector e g from the validation set with those from the training data locating its k nearest neighbours and taking the mean of the targets associated with each of the k neighbours while knn does not require training it requires the selection of the hyper parameter k with lower k values leading to predictions with high variance and low bias and vice versa for high k values hastie et al 2009 previous research by one of the authors found that a wide variety of k values 5 10 20 30 50 and 100 lead to similar results when sampling conditional model errors for streamflow simulation sikorska et al 2015 thus this study used k 5 seeking to strike a balance in the bias variance tradeoff related to the selection of k other common choices for k include n where n is the number of samples in the training set lall and sharma 1996 to ensure input variables with higher ranges are given equal weight or importance as input variables with smaller ranges prior to searching for nearest neighbours all inputs were individually normalized i e scaled between 0 and 1 using the maximum and minimum of each variable over the training set using normalized inputs in knn has been shown to significantly improve model performance compared to using unnormalized inputs piryonesi and el diraby 2020 and in earlier experiments was also found to have the same effects for the catchments explored here the fnn package in r beygelzimer et al 2019 was used for developing the knn models which uses the fast k nearest neighbours method beygelzimer et al 2006 and the kd tree approach friedman et al 1977 when searching for nearest neighbours 3 5 2 multiple linear regression mlr and second order volterra series model sov the parameters in mlr are the slope and bias coefficients associated with the design matrix input variables while the parameters in sov are the bias zero order and kernel coefficients associated with the first and second order interactions amongst the input variables the parameters in mlr and sov were solved via ordinary least squares wu and kareem 2014 note that there are no tunable hyper parameters for the mlr and sov models the mlr and sov models were developed using custom functions in r 3 5 3 artificial neural networks ann the parameters in ann include the input hidden and output layer weights as well as the hidden and output layer biases the particular application of ann used in this study is based on the avnnet function in the caret r package kuhn et al 2019 which makes use of the nnet r package venables and ripley 2002 this implementation of ann individually trains several networks with different randomly initialized parameters via the broyden fletcher goldfarb shanno bfgs algorithm fletcher 1987 and averages their predictions the hyper parameters include the number of networks set by default to 5 the number of training epochs or iterations set by default to 100 the number of hidden nodes since only a single hidden layer is used and the decay rate a grid search over the decay rate 1e 3 0 01 0 1 and number of hidden neurons 1 2 n i 1 where n i is the number of inputs in the ann hecht nielsen 1989 fatehi et al 2015 was carried out using 5 fold cross validation and the rmse as the objective function to determine optimal values for these hyper parameters the ann models adopted linear activation functions in the output layer and sigmoid logistic activation functions in the hidden layer all model inputs were normalized before training the ann i e using the same approach as knn 3 5 4 random forests rf the parameters in rf are the splitting variables and split points at each node of the decision trees while the hyper parameters include the number of trees b the number of variables selected randomly at each split m and the minimum node size n m i n hastie et al 2009 in practice it is generally found that once a sufficient number of trees have been considered in the forest including additional trees beyond this point does not substantially increase performance for example see the large scale study by probst and boulesteix 2018 and comes at an increased computational cost hastie et al 2009 further default values for n m i n set to 5 in the ranger r package wright and ziegler 2017 used to develop the rf models in this study have also been shown to provide high performance d√≠az uriarte and alvarez de andr√©s 2006 while probst et al 2019 confirm that n m i n is less important to tune than m they show that at an increased computational cost it can be jointly optimized with m to improve rf performance however in order to strike a balance between predictive accuracy and computational efficiency n m i n is fixed in this study at its default value 5 and a two stage approach is used to identify suitable values for b and m as described below at first a preliminary analysis was undertaken revealing that 300 trees b 300 and a minimum node size of 5 n m i n 5 led to stable performance when m was within the range of 1 5 thus b and n m i n were held constant at 300 and 5 respectively and m was optimized through a grid search m 1 2 5 using 5 fold cross validation where the rmse was used as the objective function the rf models were developed using a combination of the caret and ranger r packages kuhn et al 2019 wright and ziegler 2017 3 5 5 extreme gradient boosting xgb the parameters in xgb similar to rf are the variables and values used when node splitting the xgb hyper parameters are described in detail in chen and guestrin 2016 and chen et al 2019 and given below along with their ranges considered during optimization integer values are presented with an l nrounds i e the number of trees 1l 150l eta 0 001 0 5 gamma 0 10 max depth 2l 12l min child weight 1l 10l subsample 0 5 1 colsample bytree 0 1 1 lambda 0 1 alpha 0 1 the xgb hyper parameters were optimized using the bayesian optimization approach of snoek et al 2012 which is based on gaussian processes the bayesian optimization routine adopted the expected improvement method for updating the estimates of the best model parameters ribeiro et al 2020 and was run for 20 iterations after generating 5 initial starting points for the model parameters details on applying bayesian optimization using the expected improvement method for training ddms is described in zuo et al 2020 the xgb models were built using the parbayesianoptimization wilson 2019 and xgboost chen et al 2019 r packages 4 results 4 1 uncertainty intervals cdda vs benchmark hm through the cdda uncertainty intervals were generated for the ensemble streamflow simulations using six different ddms mlr knn sov ann rf xgb in addition to these six ddms one extra variant was considered for both rf and xgb models that considered only the six most important input variables as input to the ddm by assessing the variable importance scores of rf and xgb these model variants are referred to as rf 6 and xgb 6 these additional models were created for two reasons 1 to see if using a smaller number of inputs in these models leads to significantly poorer performance compared to the case when all inputs are considered and 2 to enable a comparison with the other nonlinear methods knn sov and ann that on average used six input variables as selected by the ea input variable selection method figs 3 5 present the 95 uncertainty intervals of these cdda variants for three study catchments these uncertainty intervals result from the parametric uncertainty of the hm model only i e via using multiple 1000 optimized parameter sets for the hbv model and thus represent only the 95 confidence intervals 95 cis since a ddm is trained for each set of hm residuals there are also 1000 ddms the combination of both hm and ddm result in the cdda based ensemble streamflow simulations for better visibility only a short simulation period of 30 days is displayed in figs 3 5 while a longer simulation period is provided in the supplementary data the cdda based ensemble streamflow simulations are compared to the benchmark i e hm based ensemble simulated streamflow which is simply the deterministic output of the hbv model for the 1000 optimized parameter sets as can be seen from figs 3 5 and those in the supplementary data the uncertainty intervals of all cdda and the hbv model are narrow for the three study catchments the cis for most of the cdda variants and hbv at the study catchments do well at covering low flows moderately well at medium flows but perform quite poorly at high flows in addition the 95 cis for all cdda closely overlap with the intervals of the benchmark model hbv hence based only on the visual assessment it is difficult to identify which of cdda variants performs best to support this analysis performance metrics were evaluated and are presented in section 4 2 the narrow uncertainty intervals for all cdda and the hbv model result from the fact that only the parametric uncertainty of the hbv model was considered whereas other sources of uncertainty input data model structure model output etc were excluded which if considered may prove useful in improving the quality of the uncertainty intervals this issue is further discussed in section 5 2 4 2 quantitative assessment of the cdda performance deterministic performance metrics for the mean ensemble simulation i e the mean over all 1000 ensemble members are presented for all cdda variants in table 2 whereas table 3 illustrates the ensemble performance metrics that consider all 1000 ensemble members these values are compared to the performance criteria computed for the benchmark which can be used to determine if and by how much the performance of the ensemble streamflow simulation is improved when using the cdda instead of the hm only the performance criteria are provided for all three study catchments four deterministic criteria were analyzed the mean absolute error root mean square error nash sutcliffe efficiency and the kling gupta efficiency please refer to the supplementary data for additional metrics optimal performance for nse and kge occurs at a value of 1 whereas for rmse and mae optimal performance occurs at a value of 0 thus if any cdda variants lead to an improvement in reference to hbv nse and kge will increase while rmse and mae will decrease from table 2 it can be noticed that most of the cdda variants lead to an improvement in most metrics over the standalone hbv based simulations namely nse rmse and mae were all improved for all cdda variants in comparison to the hbv in all three catchments regarding kge it was improved in the kleine emme catchment for models knn ann rf rf 6 xgb and xgb 6 and in the muota catchment for models sov ann rf rf 6 xgb and xgb 6 opposite to that in the d√ºnnern catchment none of the cdda variants led to an improvement of the kge values despite the kge values computed for the cdda being slightly lower they were still very close to the benchmark value obtained with the hbv model in this catchment by reviewing the additional performance metrics in the supplementary data it can be seen for the d√ºnnern catchment that the cdda variants have slightly higher bias than hbv which is the likely reason why the kge is slightly lower for the cdda among all tested cdda variants rf and xgb led to the largest improvement in the deterministic performance metrics followed by their variants rf 6 and xgb 6 as compared to the standalone hbv model for example rf rf 6 and xgb xgb 6 led to improvements in mae of 20 22 17 19 24 25 18 20 and 15 17 9 10 for d√ºnnern kleine emme and muota catchments respectively while the cdda based on mlr led to a marginal improvement only in these criteria e g improvements in mae of 1 6 across the three catchments to summarize the effect of the cdda in terms of its mean ensemble deterministic performance refer also to the supplementary data most ddms are very effective at significantly reducing variance in the resulting simulations especially rf rf 6 xgb and xgb 6 and in some cases rf rf 6 xgb and or xgb 6 also improve bias albeit to a lesser degree e g at d√ºnnern and muota catchments given that cdda generates ensemble streamflow simulations it is also very important to assess ensemble performance when comparing cdda to the standalone hbv to assess the properties of the uncertainty intervals of the cdda simulations versus the hbv based simulations benchmark three ensemble performance metrics were considered the mean continuous ranked probability score the mean interval score and the average width of uncertainty intervals which are presented in table 3 optimal values for crps mis and aw should be as small as possible thus if any of these criteria are lower for cdda than hbv then it is indicative that cdda provides superior ensemble performance with the caveat that lower crps and mis scores are preferred over lower aw scores by evaluating the ensemble performance metrics it can be noticed that for all three catchments crps was decreased for all cdda variants with the exception of mlr for the d√ºnnern catchment mis was decreased for all cdda variants in the d√ºnnern and in the muota catchments and for most of cdda variants in the kleine emme catchment apart from mlr regarding aw this criterion was decreased for most cdda variants in kleine emme apart from xgb 6 but only for three models in the other two catchments the analysis of aw is however not straightforward as the optimal uncertainty intervals should prioritize the smallest values for the other two criteria i e crps and mis over the aw thus a slightly larger value for aw obtained for a cdda variant with reference to the benchmark does not necessarily indicate poorer ensemble performance similar to the deterministic performance criteria when comparing all tested cdda variants xgb and rf provided the largest improvement in ensemble performance metrics followed by their variants rf 6 and xgb 6 with reference to the standalone hbv model for example rf rf 6 and xgb xgb 6 led to improvements in crps of 23 27 19 21 24 29 20 21 and 16 22 11 13 for d√ºnnern kleine emme and muota catchments respectively the cdda based on mlr led to marginal improvements in these criteria e g crps was increased by 6 and 4 for d√ºnnern and muota catchments respectively and no change was observed for the kleine emme in general based on both deterministic and ensemble performance criteria it can be concluded that all cdda variants led to an improvement in the ensemble streamflow simulations with reference to the hm based simulations among all tested cdda variants the cdda based on xgb led to the largest improvement in the ensemble streamflow simulations the second best model was the one based on rf the third and fourth best models were xgb 6 and rf 6 followed by ann sov and knn the worst simulation performance was achieved for the cdda adopting mlr which was only slightly better than the benchmark thus it may be argued that linear ddms are inappropriate for fitting hm residuals in the study catchments see further section 5 3 4 3 importance of input variables in ddms to simulate the hm residuals the different ddms considered several input variables a timeframe of up to 9 days preceding the day of simulation was considered for all three candidate input variables precipitation air temperature and streamflow as described in section 2 4 hence the previous 9 days of streamflow t 1 t 9 as well as the current and previous 9 days t 0 t 9 of precipitation and air temperature were considered as input variables in total 29 different input variables were considered as potential predictors of the hm residuals ivs was performed for each of the 1000 ensemble members considering the hm residuals as the target variable in order to determine the best predictors to use for each individual residual series since pcis and ea are both model free ivs methods they were run independent of the ddms i e the inputs and model parameters were determined separately in contrast rf and xgb are model based ivs approaches thus the inputs selected by these methods are related to the model parameters determined during training the importance scores of the input variables selected by the different ivs methods associated with the cdda variants are illustrated in figs 6 8 for the three study catchments which summarize the importance scores across all 1000 ensemble members using box plots it is assumed that the higher the importance score the stronger the impact the particular input variable has on the simulated residuals since the knn sov and ann models use the exact same inputs as determined by the ea ivs method only a single plot is considered for these methods since pcis is a linear ivs method it is solely coupled with mlr note that for xgb 6 and rf 6 only six input variables are plotted as these model variants considered only the six most important input variables determined by their base method xgb and rf it is important to note that all variables presented in these plots with importance scores above 0 were not necessarily selected for each of the 1000 ensemble members but any inputs with importance scores higher than 0 were selected at least once in the 1000 ensemble members from figs 6 8 it can be seen that generally the importance of the input variables increases with decreasing the lag time i e a higher importance is given to input variables whose lag time directly precedes the simulation day for all cdda variants among the three considered input variables precipitation was the strongest predictor of the hm residuals followed by streamflow the air temperature was the weakest predictor among all ddms apart from the mlr model where it was a stronger predictor than the streamflow in two out of three catchments looking at different models in detail it appears that for knn sov and ann which use the ea method based on conditional mutual information for ivs the air temperature does not play any significant role while the importance of the streamflow from the three preceding days t 1 t 2 t 3 and the precipitation from the three preceding days including the day of simulation t 0 t 3 were the most important for simulating hm residuals in all three catchments for mlr the precipitation t 0 t 1 t 2 was very important in all catchments whereas the importance of the air temperature and streamflow varied depending on the catchment in rf and xgb the precipitation t 0 t 3 the streamflow t 1 t 2 t 3 and the temperature t 0 t 3 were all very important in all three catchments with precipitation t 0 t 1 t 2 and streamflow t 1 being the most important yet it can be noticed that all input variables have above 0 importance in rf and xgb meaning that all variables contribute to the overall ensemble hm residual simulations for rf 6 and xgb 6 that consider only the six most important variables from their respective base model the selected variables included streamflow at t 1 and t 2 and precipitation at t 0 t 1 and t 2 in all three catchments the sixth most important variable varied depending on the catchment and it was either air temperature t 2 d√ºnnern air temperature t 0 kleine emme or streamflow t 3 muota the order of importance for these six variables varied depending on the catchment however streamflow at t 1 and precipitation at t 0 and t 1 were always found to be very important an interesting result was obtained by rf 6 for the muota catchment when only the six most important inputs identified by rf were considered in rf 6 it was found that precipitation at t 2 did not add any information that was useful when simulating the hm residuals this suggests that when all 29 inputs are considered there is another input outside of the other five selected inputs that when combined with precipitation at t 2 adds information that is useful for simulating the hm residuals while it is outside the scope of this research to identify the other inter dependent input s the interested reader may refer to galelli et al 2014 whom discuss the inter dependency of inputs in ivs 4 4 effect of the ensemble size simulations of both cdda and the benchmark hm were based upon 1000 ensemble members that originate from the 1000 optimized parameter sets of the hbv model yet it is not clear whether use of all 1000 ensemble members is of a value for the cdda as it appears from figs 3 5 the uncertainty intervals are rather narrow which may indicate that some members may be redundant this issue was investigated here by exploring the effect of the ensemble size on the simulation performance for this purpose fig 9 can be used to analyze how the crps changes as the ensemble member size grows for all cdda variants and the benchmark hm hbv in the three study catchments based on fig 9 it can be seen that as few as 100 ensemble members provide roughly the same ensemble performance as 1000 ensemble members for both the hbv based simulations benchmark and for the cdda based simulations when considering the crps using less than 100 ensemble members leads to a visible drop in the crps whereas using more than 100 ensemble members does not significantly improve the model performance the strongest improvement is noticed between one and 10 ensemble members which seems logical when moving from the deterministic approach only one ensemble member towards a ensemble approach several ensemble members this effect of growing the ensemble size on the model performance was visible for all tested cdda variants as well as for the hbv model in all three study catchments 5 discussion in this work an ensemble based conceptual data driven approach cdda was developed that consists of a hydrological model to simulate the precipitation streamflow process and a data driven model to simulate the hm residuals the goal of the cdda is to improve the predictive capability of the ensemble streamflow simulation compared to the standalone hm results from three study catchments in switzerland have demonstrated that coupling ddms with a simple hydrological model hbv is capable of improving the predictive performance of the hm assessed by both deterministic and ensemble performance criteria as compared to the standalone hm benchmark below the major findings of this work are explored in detail its limitations are discussed and some recommendations for future research are given 5 1 different ddm and importance of input variables six different ddms mlr knn sov ann rf and xgb and two additional variants rf 6 and xgb 6 were tested for simulating hm residuals these ddms link the hm residuals on the simulation day t 0 to observed precipitation and air temperature on the simulation day and those preceding it t 0 to t 9 as well as streamflow on preceding days t 1 to t 9 the maximal time lag of nine days was determined by studying the nonlinear correlation between explanatory variables and the hm residuals see section 3 5 these results demonstrated that generally all ddms coupled with the hm within the cdda led to an improvement in the ensemble streamflow simulations with reference to the hm based simulations section 4 2 this increase in the streamflow simulation performance measured by deterministic and ensemble performance metrics was however not the same for all models and for the simplest ddm i e mlr it was only marginal the largest increase in the model performance was achieved with xgb and then with rf models the xgb and rf models consistently had the best performance across all three catchments analysis of the input variable importance section 4 3 revealed that the importance of the input variables increases with decreasing lag time i e input variables from days directly preceding the simulation day were of higher importance than those from the distant past for all ddms the observed streamflow t 1 t 2 t 3 and precipitation t 0 t 3 from the current and preceding three days had the highest impact on the hm simulated residuals while for certain methods rf and xgb the air temperature from the current and three preceding days t 0 t 3 were also deemed important inputs however for xgb and rf observations farther in the past t 4 t 9 also had an impact on the simulated residuals hence it appears that despite the maximal time lag of nine days the effective memory length which determines the lag length beyond which input variables have only marginal or null effect on the simulated output appears to be about three days directly preceding the simulation day this memory lag length bowden et al 2005 may be further explained by the catchment memory to past inputs that determines how long water is retained in the catchment in different forms such as aquifers snowpack or groundwater storage m√ºft√ºoƒülu 1984 rajurkar et al 2004 this catchment memory is often identified as precipitation influence history m√ºft√ºoƒülu 1991 which is the strongest predictor of the streamflow the length of the catchment memory varies between catchments and may be from several hours to several days or longer as the results indicate not only the simulated streamflow but also residuals of the hydrological model may be linked to the catchment memory from these findings it appears that the more complex ddms that link the residuals to additional input variables i e use longer time lags have better predictive skill in correctly modelling the residuals of the hm hence although the most important input variables seem to be observations from the previous three days using longer lag times in more complex models further improves the model performance note however that using longer lag times with simpler models does not improve the model performance next among the three considered input variables the observed precipitation was the strongest predictor of the hm residuals followed by the observed streamflow while the observed air temperature was the weakest predictor of residuals this seems reasonable as the precipitation is usually also the strongest predictor of streamflow in any hm that relies on the precipitation streamflow generation concept the air temperature is usually used within many hms to determine the form of the precipitation liquid or solid and to determine whether snow melt occurs seibert and vis 2012 thus it seems reasonable that its impact is smaller than that of the precipitation which determines the amount of water entering the catchment the importance of the observed streamflow as a predictor of model residuals can be explained by the auto correlation effect present for most hydrological models i e when streamflow residuals at t 0 are correlated with residuals at preceding time steps t 1 t 2 etc sorooshian and dracup 1980 yang et al 2007 sikorska et al 2012 the strength of this auto correlation likely depends on the precipitation and flow conditions and for wet or high flow periods it is expected to be higher than for dry or low flow periods this concept has been explored by del giudice et al 2013 who linked the residuals of a hm to the precipitation amount or streamflow via bayesian inference the authors found that if conditioned on one variable the streamflow dependent description of model residuals was better performing than the precipitation dependent error however this seems opposite to the findings from this study although ddms that rely on only a single input variable were not investigated here instead ddms always included several input variables 5 2 applicability of the ensemble based cdda approach and limitations the results demonstrated that the ensemble based cdda framework is very promising for simulating hm residuals and generating improved ensemble streamflow simulations when compared to the hm benchmark although in this study only a single conceptual model hbv was applied to simulate the precipitation streamflow process the cdda is not limited to this model and any other hm can be coupled with the proposed framework it should be noted that using any other hm or even an hbv model with different parameters would require the ddms to be recalibrated since the hm will generate different residuals in addition since different hms may use other input variables e g potential evapotranspiration the importance of such variables should also be explored to develop the best possible ddm for the given dataset the same holds for an application of the proposed approach to other sites where the hm residuals may exhibit different properties than those obtained in this study hence the ddms should be re calibrated trained based on local observations yet the cdda as a general framework for improving ensemble based hm simulations remains essentially the same for different hms or study sites in general the cdda can be used to identify the best ddm and select the most suitable input variables for a given location using a ddm for simulating hm residuals is highly advantageous since it does not require any explicit assumptions on the characteristics of the residuals as is the case in all bayesian based methods thus there is no need to assume the residuals independence normality or auto correlation structure moreover with methods such as rf and xgb it appears there is little need to perform pre selection of input variables as important variables tend to be included in the modelling framework as corroborated by model free ivs methods such as ea allowing the model to distinguish on its own relevant from irrelevant and or redundant inputs however the use of inputs that have unjustifiable use in the model is not advocated as it is rational to only include in the ddms input variables that have an impact on the hm model residuals and in this case streamflow even for methods such as rf and xgb performing ivs may lead to improved performance and models with lower complexity tyralis and papacharalampous 2017 hadi et al 2019 indeed for methods such as mlr knn sov and ann input variable selection is a necessity since the models on their own lack the ability to filter out non useful inputs galelli et al 2014 although recent research has made progress in addressing this short coming by placing tunable weights on the input variables used in the ddm which may be tuned simultaneously along with the ddm parameters to provide insights on how the input variables impact model predictions yang et al 2020b further the use of ivs can also help reduce the computational burden of model development which is substantial for large ensembles e g 1000 members although running the models is extremely quick in an operational setting thus by reducing the computational burden ivs may also increase the exploration of alternative ddms additionally ivs may also prove useful in reducing the number of ensemble members in the cdda by identifying a reduced number of members that maintain a similar level of performance as the initial ensemble size for example it was found that 100 ensemble members i e the first 100 out of 1000 randomly generated members appeared to include approximately the same level of information as the entire 1000 member ensemble see section 4 4 the major limitation of this study is that only parametric uncertainty of the hydrological model here hbv was investigated and represented with multiple optimized parameter sets 1000 without explicitly considering any other uncertainty sources of the hm such as uncertainties in the inputs or model structure renard et al 2011 sikorska and renard 2017 hence only confidence intervals surrounding the hm parameters can be computed this can also explain why the uncertainty intervals are very narrow for all simulations i e for the standalone hm as well as for different cdda variants note that residuals simulated with a ddm represent the remaining uncertainty of the hm that is not explicitly considered yet as the output of the cdda is conditioned on the simulations from the hm narrow uncertainty intervals resulting from the hm translate to narrow uncertainties resulting from the cdda however in order to focus on the exploring the difference in performance across several ddms different uncertainty sources i e other than parameter uncertainty were explicitly not considered here the effect of other uncertainty sources should be however investigated in future studies at present the proposed ensemble based cdda framework has only been tested in three gauged catchments yet it would be very interesting to test the approach in ungauged catchments without streamflow observations this would enable for improving streamflow simulations at sites where performance is the lowest due to a lack of recorded data to calibrate a hm as a direct calibration of the cdda and its components i e the hm or ddm is not possible at ungauged locations other methods to inform both models should be considered including regionalization approaches to inform the hm model parameters while training in a large set of catchments of different properties could help to constrain information on simulated residuals and transfer these residuals to the ungauged catchments that have similar properties gauch et al 2021 found that using data from a large set of catchments to train a single ddm yields better streamflow simulation results as the number of catchments increases also in poorly gauged regions suggesting potential towards generalization of ddms wu et al 2019 have also demonstrated that a ddm used to simulate residuals of an un calibrated hm may improve its predictive performance finally it is important to note that in an operational context if there are no streamflow observations available then lagged measurements of the streamflow cannot be used as an input variable for simulating streamflow via the cdda this is relevant as there may be cases where streamflow gauging stations may be temporarily offline or are no longer in operation due to decommissioning damage from a flooding event etc tencaliec et al 2015 villalba et al 2021 thus obtaining updated information on recent streamflow is not possible in these cases while streamflow data may be used for calibrating the cdda i e using streamflow measurements from a limited historical database as the target variable other input variables may be required instead of streamflow to improve its predictive performance 5 3 recommendations to summarize the results indicate for the study catchments that it is generally better to include almost any nonlinear ddm to simulate the residuals of a hydrological model than using a standalone hm however in the case of the simplest data driven model mlr the improvement in ensemble streamflow simulation performance was only marginal whereas more complex nonlinear models xgb and rf led to a significant improvement in the ensemble streamflow simulation performance however using a ddm to simulate hm residuals is linked with some additional computational efforts thus the selection of appropriate ddms is very important when computational resources are limited since it was found that mlr barely improved the original hm simulations this approach is not recommended unless of course there is reason to believe the relationship between hm residuals and input variables is linear which did not appear to be the case for the study catchments however it was found that xgb and rf both led to significant gains in deterministic and ensemble performance over the standalone hm given that both methods inherently perform ivs and thus require little user intervention while providing impressive performance are recommended for further study additionally since it was found that only 100 ensemble members provided a similar level of performance as the initial 1000 member ensemble it is plausible although it was not verified that a smaller and carefully selected set of ensemble members identified via ivs may provide a similar level of performance as the initial ensemble size this could be explored in two ways by performing ivs 1 on the hm ensemble and then developing ddms for this reduced set or 2 on the cdda ensemble speculation as to which of the two approaches would result in better performance is out of the scope of this paper but it is recommended as an interesting line of future research additionally it is recommended to include other sources of uncertainty in the ensemble based cdda such as those related to inputs input variable selection parameters and model output in order to improve uncertainty estimation and the overall utility of the ensemble simulations finally other potentially useful input variables such as potential evapotranspiration soil moisture relative humidity wind speed are recommended to be explored in the ddm even if such methods are unable to be included in a particular hm 6 conclusions a novel ensemble based conceptual data driven approach cdda has been proposed it consists of an ensemble of hydrological models hms used to simulate the hydrological process es of interest and an ensemble of data driven models ddms to simulate the resulting ensemble of hm residuals while the cdda can be used to simulate any hydrological process it is applied for ensemble streamflow simulation due to its significance to the hydrology and water resources communities here the ddm uses as input time lagged precipitation air temperature and streamflow to simulate the hm residuals the cdda combines the advantages of a hm respecting hydrological processes with the ability of the ddm to simulate complex nonlinear relationships between input target explanatory response variables by tackling auto correlated hm residuals the cdda does not require any statistical assumptions about the hm residuals and it provides a framework for identifying suitable ddms and input variables moreover the ensemble based cdda is very flexible as it can be coupled with any hm and any ddm the selection of potential input variables can also be adjusted based on available data as well as user needs and specific conditions e g hydrological model or type of runoff generation using three swiss catchments as a case study the cdda was shown to be a very promising approach for improving ensemble streamflow simulations among eight variants of different ddms extreme gradient boosting xgb and random forests rf were found to be the most accurate predictors of the hm residuals and also required less user intervention in terms of selecting appropriate inputs to the ddm it was found that precipitation and streamflow from the three days directly preceding the simulation day had the largest impact on the simulated hm residuals since xgb and rf demonstrated the best performance for the study catchments improving the continuous ranked probability score by 16 29 both models are recommended to simulate the residuals of the hm within the ensemble based cdda framework it was also found in this study that the number of ensemble members may be substantially reduced i e from 1000 to 100 without significantly affecting model performance this is especially important for cases where computational resources are limited declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors wish to thank the three anonymous reviewers who provided constructive feedback that has helped to improve the quality of this paper this research did not receive funding from any agency and was carried out according to the authors curiosity the swiss federal office for the environment foen is acknowledged for providing the streamflow data used in this study that were available from the project no 15 0054 pj o503 1381 calibration of the hbv model was run using the sciencecloud provided by s3it at the university of zurich appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105094 data availability the observed discharge data for calibrating the hydrologic model can be ordered from the foen https www bafu admin ch last access december 11 2020 the observed meteorological data from meteoswiss http www meteoswiss ch last access december 11 2020 the latest version of the hbv model is available at https www geo uzh ch en units h2k services hbv model html funding this research did not receive any specific grant from funding agencies in the public commercial or not for profit sectors 
25792,a novel ensemble based conceptual data driven approach for improved streamflow simulations anna e sikorska senoner a john m quilty b a department of geography university of zurich winterthurerstrasse 190 8057 z√ºrich switzerland department of geography university of zurich winterthurerstrasse 190 z√ºrich 8057 switzerland department of geography university of zurich winterthurerstrasse 190 8057 zurich switzerland b department of civil and environmental engineering university of waterloo 200 university avenue west waterloo on n2l 3g1 canada department of civil and environmental engineering university of waterloo 200 university avenue west waterloo on n2l 3g1 canada department of civil and environmental engineering university of waterloo 200 university avenue west waterloo on n2l 3g1 canada corresponding author a novel ensemble based conceptual data driven approach cdda is developed where a data driven model ddm is used to correct the residuals from an ensemble of hydrological model hm simulations the cdda respects hydrological processes via the hm and it benefits from the ddm s ability to simulate the complex relationship between residuals and input variables the cdda can accomodate any hm and ddm allowing for different configurations to be tested the cdda is tested for ensemble streamflow simulation in three swiss catchments where the hm hbv hydrologiska byr√•ns vattenbalansavdelning is coupled with eight different ddms multiple linear regression k nearest neighbours regression second order volterra series model artificial neural networks and two variants of extreme gradient boosting xgb and random forests rf the proposed cdda was able to improve the mean continuous ranked probability score by 16 29 over the standalone hm since xgb and rf demonstrated the best performance they are recommended for simulating the hm residuals graphical abstract image 1 keywords ensemble streamflow simulation data driven model hydrological model residuals uncertainty 1 introduction the hydrology and water resources domains have witnessed growing interest in using data driven models ddms to improve the simulation of hydrological variables streamflow rainfall groundwater levels etc shen 2018 nearing et al 2020 xu and liang 2021 whether by directly replacing e g kratzert et al 2019 or being used in conjunction with e g boucher et al 2020 hydrological models hms in this study a novel framework is proposed where ddms are used for improving ensemble based hm simulations where the ddms act as residual models in other words ddms are used to correct the model residuals errors stemming from an ensemble of hm simulations while the proposed framework is general to any type of hm water quantity or quality this study focuses on ensemble streamflow simulation due to its importance to the hydrology and water resources communities bourdin et al 2012 hm errors impair estimates of design floods risk assessment and in general water resources management farmer and vogel 2016 hm errors result from numerous sources of uncertainty such as model structure input and output variables parameters initial conditions etc kuczera et al 2010 renard et al 2011 mcmillan et al 2012 sikorska and renard 2017 although hm errors have been traditionally assumed to be independent and normally distributed it is now widely accepted that this assumption does not hold since hm residuals are highly auto correlated sorooshian and dracup 1980 yang et al 2007 sikorska et al 2012 hence they cannot be modelled as a normally distributed random variable as a result alternative methods that do not rely on these limiting assumptions have gained widespread attention in hydrology available methods for modelling hm errors are typically classified into two major groups ni et al 2020 the first group uses various tools for hm error identification and estimating related uncertainties from methodologically simple monte carlo experiments with different hm parameter sets e g padiyedath gopalan et al 2019 to more advanced likelihood based methods see examples in the paragraph below the second group uses data driven approaches to directly link the hm simulations to its input variables within a hybrid modelling approach althoff et al 2021 among the first group of methods bayesian based methods have received the most attention since they are able to simulate the auto correlation of model residuals and quantify contributions from different uncertainty sources mantovan and todini 2006 renard et al 2011 yet they require making an explicit assumption on model error properties with introducing additional parameters for the model inference and a definition of the likelihood function to sample from the posterior distribution montanari and koutsoyiannis 2012 sikorska et al 2012 2015 smith et al 2015 alternative methods condition model errors on input and or output variables del giudice et al 2013 mcinerney et al 2017 link their parameters i e of the error distributions to flow conditions schaefli et al 2007 or introduce their time dependent description pianosi and raso 2012 alternatively several methodologically simpler likelihood free approaches are frequently applied that rely on metaheuristic search algorithms maier et al 2014 piotrowski et al 2017 such as genetic algorithm particle swarm optimization where model deficiencies are accounted for via using multiple generated parameter sets instead of making assumptions on model errors the second group of methods relies on ddms that establish statistical links between target and input variables using a training dataset but do so without including a hm bowden et al 2005 solomatine and ostfeld 2008 thus ddms are very powerful for modelling complex relationships between input and output variables or for gaining insights on governing physical processes tongal and booij 2018 without the need to explicitly consider the physical laws governing such processes hence they are constantly increasing in popularity within hydrology and have been applied for many different hydrological and water resources applications precipitation runoff modelling rajurkar et al 2004 shortridge et al 2016 tongal and booij 2018 streamflow forecasting solomatine and xue 2004 boucher et al 2011 papacharalampous and tyralis 2018 ni et al 2020 tyralis et al 2020 groundwater level forecasting suryanarayana et al 2014 rahman et al 2020 water quality modeling fatehi et al 2015 bhagat et al 2021 urban water demand forecasting quilty et al 2019 and many others however ddms require the pre selection of input variables as potential predictors which greatly impacts their accuracy galelli et al 2014 in addition since hydrological variables often display auto and cross correlation it is important to consider time lagged versions of the input variables e g precipitation air temperature when simulating a target hydrological variable e g streamflow gauch et al 2021 and to be able to identify the maximum time lag memory length this maximum time lag defines the time lag after which the impact of the input variable on the target variable is marginal bowden et al 2005 a correct pre selection of input variables enables the exclusion of redundant and or irrelevant input variables thereby reducing the complexity and increasing the interpretability of the resulting ddm galelli et al 2014 most previous data driven approaches for streamflow simulation have used ddms to explicitly simulate streamflow and in this way they provide an alternative to a hm yet only very few works have attempted to characterize or directly simulate residuals of a hm with a data driven approach some examples include montanari and koutsoyiannis 2012 sikorska et al 2015 wani et al 2017 and ehlers et al 2019 who simulate model errors via resampling from a query dataset based on streamflow properties and other hydro meteorological data in the case of ehlers et al 2019 however each of these studies relied on a specific statistical method for resampling model errors the meta gaussian approach in montanari and koutsoyiannis 2012 and k nearest neighbours in the others and therefore their approaches are not generalized to any ddm as is the case in this study approaches for coupling hms with ddms into a hybrid model are very rare in application to streamflow modelling wang et al 2021 for example tongal and booij 2018 decomposed streamflow into base and surface flows and used hm simulations at the previous time step along with meteorological variables as input to a ddm to simulate streamflow at the current time step indirectly linking model residuals to flow conditions senent aparicio et al 2019 have proposed an indirect coupled approach in which streamflow simulations from a ddm are corrected using additional information on maximum mean daily flow that is taken from a hm yang et al 2020a have proposed a different indirect coupled approach that used a hm to simulate pseudo observed data for training a ddm to perform streamflow simulation hybrid streamflow simulation models where a ddm is used to model hm residuals have to the best of the authors knowledge only been explored by wu et al 2019 and konapala et al 2020 both studies used ddms for simulating the residuals of hms but did not consider ensemble streamflow simulation ignoring the uncertainty in estimating hm parameters in addition wu et al 2019 required the transformation of hm residuals prior to modelling via ddms the determination of stationary time windows as identified by the hilbert huang transform in order to simulate the hm residuals and assumed that the simulated hm residuals follow a student s t distribution to permit the construction of uncertainty intervals more recently wang et al 2021 proposed a hybrid model that uses the output of the xinanjiang hm along with wavelet decomposed sub series of previous streamflow observations as input to random forests rf for simulating a single realization of streamflow without considering any sources of uncertainty althoff et al 2021 proposed an alternative hybrid model that consisted of a simplified version of a hm soil moisture component and a ddm to simulate streamflow however they also generate only a single realization of the streamflow and do not consider any sources of uncertainty as can be seen from the above literature review despite several recently proposed hybrid modelling approaches none of the above mentioned studies developed a fully coupled framework for an ensemble based streamflow simulation where any ddm can be used to simulate an ensemble of residuals stemming from a hm conditioned on input variables e g precipitation air temperature thus in this study a novel conceptual data driven modelling framework is developed that pairs an ensemble of conceptual deterministic hms with an ensemble of ddms that simulate the hm residuals for improved ensemble streamflow simulation this novel framework is called an ensemble based conceptual data driven approach cdda while the hm simulates the precipitation runoff process at the catchment scale respecting to an extent the physical hydrological processes a ddm added on top of the hm mimics the hm residuals enabling the streamflow simulations to be improved thus the cdda combines the advantages of a hm that seeks to respect the physical relationships between hydrological processes with the capabilities of the ddm that can model complex nonlinear relationships between input target variables enabling the accurate estimation of correlated residuals stemming from the hm the output of the cdda framework is an ensemble of streamflow simulations instead of a single streamflow realization conceptually the hm generated ensemble passes information about the observed and simulated streamflow gained through the identification of the hm parameters i e calibrated using observed data to the ddm which is relied upon to extract additional information not captured by the hm to simulate the hm residuals and improve the overall ensemble streamflow simulation thus the cdda can overcome certain issues of the above mentioned hybrid models since it provides an ensemble of hm residuals accounting for hm parameter uncertainty and does not require any transformation of hm residuals relaxing all assumptions on the residual distribution these last two strengths of the proposed cdda also make it more attractive than the bayesian method described above that requires specification of a likelihood function and the distribution of the model residuals thus the novel cdda can be seen as a less restrictive and more flexible data driven method for simulating hm residuals and generating an ensemble of streamflow simulations the focus of this paper is primarily on developing the cdda framework that enables for any case study where data required by the hm is available the identification of the best ddm to simulate hm residuals and the selection of the most useful input variables to use in the ddm since any ddm can be used within the cdda eight different ddms that have either been explored in detail or shown to be promising in recent studies in the hydrological and water resources modelling literature are explored multiple linear regression mlr k nearest neighbours regression knn second order volterra series model sov artificial neural networks ann as well as two variants of extreme gradient boosting xgb and random forests rf the ddms consider time lagged versions of observed streamflow precipitation and air temperature as potential predictors and are coupled with a bucket type hm hydrologiska byr√•ns vattenbalansavdelning or hbv bergstr√∂m and forsman 1973 and tested using three swiss catchments furthermore several input variable selection ivs methods are considered for selecting the most important candidate input variables to use in the ddms thus the major objectives of this study are to introduce the ensemble based cdda framework and evaluate its performance against a standalone ensemble based hm model in addition the proposed cdda enables one to answer questions specific to individual case studies such as a which ddms are most suitable for simulating the residuals of the hm within cdda b which input variables are the most important to consider when simulating hm residuals via the explored ddms thus these questions are also answered for the case study explored herein the framework developed in this study is timely as coupled hm ddm approaches are only beginning to consider uncertainty in resulting streamflow simulations e g papacharalampous et al 2019a tyralis et al 2019a boucher et al 2020 teweldebrhan et al 2020 although no such framework exists that uses ddms to simulate the residuals from an ensemble of hm simulations the remainder of this paper is organized as follows section 2 describes the methods by introducing the theoretical cdda and providing details on candidate ddms selected for simulating the hm residuals section 3 includes the experimental settings an overview of the case study as well as the hm and ddm development details section 4 highlights the main results section 5 discusses the significance of the results describes the current limitations of cdda and suggests future research avenues and section 6 concludes the paper 2 methods 2 1 overview of the conceptual data driven approach cdda a data driven model is developed that simulates the residuals of a hydrological model using input explanatory variables which in the case of streamflow simulation are usually current and time lagged precipitation and air temperature as well as time lagged streamflow if available the ddm is attached on top of a single hm simulation e g streamflow based on a given or calibrated parameter set thus the target response variable for the ddm is the residual between the observed streamflow and the hm simulated streamflow at time step t 0 when multiple parameter sets are available for the hm i e an ensemble of hm streamflow simulations a ddm is attached to each hm simulation i e there is a single ddm trained for each set of residuals this latter models setup i e an ensemble hm ddm is what is referred to as the ensemble based conceptual data driven approach or simply cdda please see fig 1 for an overview of the cdda the residuals of the hm are simulated using different ddms see section 2 3 for details and different input variable combinations are tested section 2 4 to explore which set of inputs best simulate the residuals of the hm in this study the observed streamflow is assumed to be available at a regular time interval at a given site in case observed streamflow is unavailable both the hm and ddm cannot be calibrated and other indirect approaches would be required see section 5 2 to evaluate the usefulness of the cdda the ensemble of simulations from the hm without any ddm is used as the benchmark section 2 6 the eight different variants of the cdda are compared against each other and the benchmark to determine the best ddm and identify useful input combinations 2 2 ensemble based cdda the streamflow simulation for the ensemble member y i of the cdda output is the sum of the hm simulation y i and the simulation of its residual using the ddm r i y i y i r i 1 y i p t q Œ∏ c d d a y i p t Œ∏ h m r i p t q Œ∏ d d m where p t and q are observed precipitation air temperature and streamflow Œ∏ c d d a is a parameter set of the cdda Œ∏ h m and Œ∏ d d m are the parameter sets of the hm and ddm models respectively with Œ∏ c d d a Œ∏ h m Œ∏ d d m note that each ensemble member is associated with a single Œ∏ c d d a by applying eq 1 to each ensemble member an ensemble of streamflow simulations can be obtained through cdda finally while the hm requires p and t as input to simulate streamflow for a given day the ddm can additionally use p t and q from previous days i e time lagged copies of these variables in order to improve predictive performance this idea is discussed in more detail in section 2 4 the hm is run at a daily time step as is the ddm depending on the chosen hm structure other input variables may be also required for the hm e g potential evapotranspiration and thus could also be explored within the ddm similarly using a different time step for the hm e g hourly may require the ddm to be conditioned on input variables of a different resolution and different time lag length 2 3 data driven models ddm in this study six different ddm and eight variants in total were explored for simulating the residuals of the hm since this section is intended to provide only a brief explanation of each ddm references to appropriate literature are included for the reader interested in a more detailed treatment of the various methods however due to the ubiquity of multiple linear regression mlr which is one of the adopted ddms no details are provided for this model 2 3 1 k nearest neighbours regression knn k nearest neighbours regression introduced first by fix and hodges 1951 is one of the simplest nonlinear methods to apply altman 1992 mitchell 1998 liu et al 2004 hastie et al 2009 knn generates predictions for a query vector i e an input variable vector by applying two simple steps 1 searching for a set of k neighbours i e input variable vectors in a training dataset that are closest to the query according to a predefined distance metric e g euclidean distance and 2 taking the average of the target response variable associated with each of the k closest neighbours thus knn is a local regression technique despite its simplicity one of the main drawbacks of knn is that it cannot extrapolate beyond the range of the target in the training set nonetheless knn has a rich history in hydrology karlsson and yakowitz 1987 and is still frequently used for simulating and predicting streamflow lee et al 2017 ebrahimi et al 2020 among other hydrological applications sun and tevor 2017 jiang et al 2020 for more information on knn the interested reader may refer to chapter 2 3 of hastie et al 2009 2 3 2 second order volterra series model sov similar to knn the second order volterra series model sov is another simple nonlinear regression technique sov was initially proposed for estimating non linear interactions between variables commonly considered in hydrologic systems amorocho 1963 and subsequently applied for rainfall runoff simulation amorocho and brandstetter 1971 diskin et al 1984 and more recently for forecasting streamflow maheswaran and khosa 2012 groundwater levels maheswaran and khosa 2013 urban water demand quilty and adamowski 2018 and soil moisture prasad et al 2018 as well as downscaling climatic variables lakhanpal et al 2017 sehgal et al 2018 sov is essentially a polynomial regression where the design matrix is created by considering all zero first and second order interactions amongst the input variables the coefficient attached to the zero order term is the bias and the remaining coefficients attached to first and second order terms are considered kernel coefficients wu and kareem 2014 after they have been estimated the coefficients can be used to generate predictions of the target variable for a given input variable vector the reader may consult the supplemental material of quilty and adamowski 2020 for the mathematical formulation of the sov adopted in this study 2 3 3 artificial neural networks ann artificial neural networks have been applied to streamflow prediction since the 1990 s abrahart et al 2012 and represent one of the most popular data driven models used for hydrological prediction fahimi et al 2017 ann generates predictions by passing a vector of input variables through a network comprising of a series of weights that connect to neurons which contain a bias and potentially a nonlinear activation function that transforms the weighted inputs and directs them to the next layer of the network the output of the network is the sum of weighted and nonlinearly transformed inputs the reason why ann is so powerful is due to its universal approximation capabilities which allow it to approximate any nonlinear function i e mapping from input variables to the target provided nonlinear infinitely differentiable activation functions are used in the hidden layer and enough training iterations epochs as well as model parameters weights and biases are considered hornik et al 1989 the type of ann used in this study is a feed forward backpropagation ffbp network with a single hidden layer along with the usual input and output layers representing the input variables and target respectively the ffbp ann operates by initializing all network parameters i e weights and biases to small random values and iteratively updates these parameters by back propagating the error signal through the network for a fixed number of epochs or until sequential evaluations of the error function do not appreciably decrease with respect to a predefined tolerance after it has been trained ann generates predictions by passing input variable vectors through the network where the resulting output is the sum of weighted and nonlinearly transformed inputs for additional information on ann the reader can refer to chapter 5 of ripley 1996 or many of the references included in review articles in the domains of statistics ching and titterington 1994 or hydrology abrahart et al 2012 wu et al 2014 2 3 4 random forests rf random forests introduced by breiman 2001 are a class of ensemble decision trees that include random sampling of both training instances often referred to as bagging and input variables reducing predictive variance without increasing the bias of the model breiman 1996 while also providing robust performance in the presence of noisy input variables biau 2012 a very useful feature of rf is that it implicitly provides an estimate of input variable importance gr√∂mping 2009 which can be used for input variable selection genuer et al 2010 see for example tyralis and papacharalampous 2017 rf generates predictions by taking the average across all predictions produced by each ensemble member despite its widespread success in numerous domains such as genomics chen and ishwaran 2012 remote sensing belgiu and drƒÉgu 2016 rf is only beginning to grow in popularity within hydrology tyralis et al 2019b although rf is still relatively new to the hydrology domain it has been used for streamflow simulation shortridge et al 2016 forecasting papacharalampous and tyralis 2018 and reconstruction li et al 2019 groundwater level forecasting rahman et al 2020 and downscaling extreme rainfall pham et al 2019 among other applications the interested reader may refer to chapter 15 of hastie et al 2009 for details on rf as well as fawagreh et al 2014 for a review of different rf variants and applications 2 3 5 extreme gradient boosting xgb the extreme gradient boosting model is a very recent method that combines decision trees and boosting chen and guestrin 2016 while bagging as used in rf operates by developing an ensemble of independent models on each resampled dataset boosting instead builds an ensemble of models where each member is built on top of the residuals generated by the previous member with the objective to reduce the errors made in the previous iteration in other words bagging reduces the variance of the predictors while boosting reduces bias vanschoren et al 2012 both addressing opposite aspects of the bias variance trade off and using different strategies for ensembling decision trees mehta et al 2019 while recent research has shown that combining bagging and boosting can lead to better performing ensemble decision trees ghosal and hooker 2020 such methods are not within the scope of this study xgb is an improved version of the gradient boosting machine that is more computationally efficient and less prone to overfitting due to l2 norm regularization chen and guestrin 2016 similar to rf xgb inherently estimates the importance of input variables and can be used for input variable selection e g chen et al 2020 rahman et al 2020 zhang et al 2020 xgb has only recently been considered within the hydrological domain but has shown promise for streamflow simulation hadi et al 2019 gauch et al 2021 and forecasting ni et al 2020 tyralis et al 2020 predicting daily reference evapotranspiration fan et al 2018 water quality index prediction abba et al 2020 water table depth forecasting br√©dy et al 2020 and imputing missing sub hourly precipitation records chivers et al 2020 among other applications additional details on the theory and main innovations behind xgb can be found in chen and guestrin 2016 2 4 input variable selection ivs since input variable selection is an important step in the development of any ddm galelli et al 2014 for each of the six ddms mlr knn sov ann rf and xgb ivs was carried out to identify the input variables that may be useful for simulating the hm residuals different ivs approaches were considered for the ddm the linear partial correlation input selection pcis algorithm may et al 2008 was paired with mlr to serve as a fully linear benchmark the edgeworth approximations based approach ea was used to carry out conditional mutual information cmi based ivs which is a nonlinear analogue to pcis quilty et al 2016 the ea approach was coupled with knn sov and ann since both rf and xgb implicitly perform ivs an external model free ivs method e g pcis ea was not necessary the pcis method was selected as it is one of the most popular linear ivs methods galelli et al 2014 while the ea method was selected as it has been shown to provide similar ivs accuracy as competing cmi based methods e g based on kernel density estimation k nearest neighbours at a fraction of the computational cost quilty et al 2016 a short description of the different ivs methods is provided in the sub sections below 2 4 1 partial correlation input selection pcis pcis is an iterative greedy ivs method whereby candidate input variables i e time lagged copies of observed streamflow precipitation and air temperature are selected one at a time based on their partial correlation with the target variable i e hm residuals at t 0 conditioned on previously selected inputs at each iteration the candidate input variable with the highest partial correlation i e the best candidate input variable is selected and a predefined ivs stopping condition is checked the version of pcis adopted in this study uses the bayesian information criterion bic as a stopping condition to halt the ivs procedure see galelli et al 2014 at each iteration of the ivs procedure an mlr model is built that predicts the target variable using all previously selected inputs and the best candidate input variable afterwards the bic is measured if the bic increases for the current iteration compared to the previous one the ivs procedure is halted and all input variables selected before the current iteration are returned otherwise the ivs procedure continues the variable importance measure for pcis is the partial correlation coefficient pc which ranges between 1 and 1 with 0 representing independence while 1 and 1 represent perfect correlation additional details on pcis can be found in may et al 2008 and galelli et al 2014 2 4 2 edgeworth approximations based conditional mutual information ea the ea approach to cmi based ivs was introduced and discussed in detail in quilty et al 2016 thus only the essential features of this method are described here similar to pcis ea is an iterative greedy ivs method however in ea the cmi is estimated between candidate input variables and the target conditioned on previously selected inputs instead of partial correlation the stopping condition used for ea is based on a modified version of the tolerance based approach from vlachos and kugiumtzis 2010 where at each iteration the cmi between the best candidate input variable and the target variable conditioned on all previously selected inputs is compared to the mutual information between the target variable and all previously selected input variables including the best candidate input variable if the ratio between these two quantities drops below the tolerance ranging between 0 and 1 the ivs procedure halts and all input variables selected before the current iteration are returned by increasing the tolerance the number of selected input variables can be decreased the only difference between the ea method adopted here and in quilty et al 2016 is in the choice of stopping criterion it was found during earlier experimentation that the tolerance based method provided greater control over the ivs process facilitating an improved balance between computational run time and model accuracy compared to the method used in quilty et al 2016 trial and error led to the selection of 0 05 as a suitable threshold to balance these two objectives typically for cmi based ivs adopting the tolerance threshold stopping criterion values of 0 01 0 03 0 05 and 0 15 are common vlachos and kugiumtzis 2010 tsimpiris et al 2012 note that these tolerance values are obtained by subtracting the tolerance values mentioned in the above studies from 1 the reason for this is due to our modified formulation of the stopping criterion presented in vlachos and kugiumtzis 2010 the variable importance measure for the ea method is the partial informational correlation pic which is a nonlinearly scaled version of cmi sharma and mehrotra 2014 such that the pic ranges between 0 and 1 with 0 representing independence between variables and 1 indicating perfect correlation 2 4 3 variable importance decision tree based methods the decision tree based methods rf and xgb generate internal measures of variable importance that are useful in identifying the most important inputs to the model pathy et al 2020 wang et al 2015 in contrast to the pcis and ea methods which are considered model free approaches rf and xgb are model based ivs approaches while the input variable importance measures generated by rf and xgb can also be used for ivs in other ddm see for instance chen et al 2018 hadi et al 2019 prasad et al 2019 chen et al 2020 tyralis et al 2020 bhagat et al 2021 this study only uses the variable importance measures to identify the most useful variables specific to these individual models for rf the variable importance score is the total decrease in node impurity that occurs by splitting on a particular input variable and averaged over all decision trees measured by the sum of squared errors lee et al 2017 wright and ziegler 2017 the variable importance scores generated by rf were normalized by dividing each input variable s score by the maximum variable score see eq 1 in deng and runger 2013 resulting in the normalized variable importance nvi the variable importance vi score adopted for xgb is the fractional contribution of each input variable to the model prediction averaged across all trees also known as gain lee et al 2017 where the model s predictive performance is measured by the sum of squared errors loss function chen et al 2019 for both rf and xgb higher importance scores represent variables that are more important than the others lee et al 2017 2 5 benchmark method as discussed in section 2 1 the cdda can generate an ensemble of streamflow simulations via eq 1 in order to evaluate the added value of using the cdda it must be compared against a benchmark in this case the benchmark is simply the ensemble streamflow simulations generated by the hm model for each Œ∏ h m i e without any model for simulating the hm residuals 2 6 performance metrics in order to compare the performance of cdda against the benchmark hm as well as to identify the best ddm to use within the cdda a number of statistical performance metrics were used the performance metrics were divided into two classes ensemble and deterministic metrics the ensemble metrics make use of all ensemble members when evaluating the simulations performance while the deterministic metrics are computed using the mean ensemble member i e the mean simulation at each simulation time step since the metrics adopted in this study are well known in the hydrology and water resources communities the formulae used to calculate these scores are not provided although the cited sources include the necessary information to permit their calculation the ensemble metrics consist of the mean continuous ranked probability score crps br√∂cker 2012 the mean interval score mis gneiting and raftery 2007 and the average width aw xiong et al 2009 both the mis and aw require the specification of a confidence level which is taken to be 0 05 in this study for a particular confidence level upper and lower uncertainty intervals can be computed either by assuming the ensemble members follow a particular distribution e g gaussian or by empirical means such as estimating the quantiles associated with the confidence level the latter approach is followed in this study thus the upper and lower uncertainty intervals are estimated at the 0 975 and 0 025 quantiles respectively and they together define the 95 uncertainty intervals the crps was calculated using the scoringrules r package jordan et al 2019 while the mis and aw were calculated using custom r functions the crps is a useful metric that evaluates a simulation s reliability and sharpness reducing to the mean absolute error mae for deterministic simulations permitting the comparison between ensemble and deterministic forecast quality boucher et al 2011 the mis also evaluates a simulation s reliability and sharpness but additionally includes a penalty for simulations that fall outside the upper and lower uncertainty intervals papacharalampous et al 2020 the aw solely measures the simulation s sharpness i e the narrowness of its uncertainty intervals in general a high quality ensemble simulation should have crps mis and aw as low as possible however preference in this study is given to simulations with lower crps and mis scores since a low aw score is not highly informative if the simulations are unreliable finally since the cdda seeks to improve the predictive performance of the ensemble hm simulations which themselves only consider parametric uncertainty through ddm the term uncertainty intervals as adopted here is more closely related to confidence intervals cis than prediction intervals thus cis are referred to when the model results are discussed in general prediction intervals take into account the uncertainty in the model output which is not done here since we use the ddm to predict the expectation of the individual hm errors not their distribution the deterministic metrics adopted for evaluating the ensemble models include the mean absolute error mae root mean square error rmse nash sutcliffe efficiency nse and the kling gupta efficiency kge the advantages of these metrics are described in detail within the supplemental material of papacharalampous et al 2019b the deterministic metrics were calculated using the hydrogof r package zambrano bigiarini 2017 3 experimental settings 3 1 study catchments three swiss mid size mid altitude catchments were chosen for this study fig 2 all three catchments have an insignificant areal glacier percentage 5 and are without any significant human direct impacts documented in the observation period 1981 2015 according to sikorska senoner and seibert 2020 the catchments represent two different types of dominant flood processes i e rainfall driven d√ºnnern and a mixed contribution of rainfall and snowmelt floods kleine emme and muota see also table 1 3 2 observed data the available observed data consists of continuous records of precipitation depth mm day minimum mean and maximum daily air temperature c daily evaporation rates mm day and streamflow at catchment outlets mm day all variables were available for the period 1981 2014 at a daily resolution the meteorological data were made available from the meteoswiss and the hydrological observations from the swiss federal office for the environment foen all variables were averaged to mean catchment values using the thiessen polygon method 3 3 hydrological model in this study a conceptual hbv model in particular hbv light seibert and vis 2012 was used for streamflow simulations this bucket type model consists of four major routines 1 precipitation excess and snowmelt 2 soil moisture 3 groundwater and runoff streamflow response and 4 routing in the stream the snow component is important for catchments with significant snow processes i e for two out of three studied here hbv has been frequently applied to rainfall and snow dominated catchments e g breinl 2016 griessinger et al 2016 sikorska and seibert 2018 westerberg et al 2020 the hbv model used in this study has 15 parameters and is run at a daily time step model inputs are precipitation and air temperature time series as well as long term averaged values of daily potential evapotranspiration and air temperature the model output is a single deterministic realization of continuous streamflow time series at the catchment outlet due to its simplicity hbv is used as an example in this study but can be easily interchanged with another hm without any difficulties 3 4 calibration of the hydrological model the hbv model was calibrated via the genetic algorithm and powell gap optimization method seibert 2000 using the kling gupta efficiency kge as the objective function gupta et al 2009 calibration involved 1000 independent runs with randomly selected initial values for the 15 parameters resulting in 1000 optimized and equally plausible parameter sets representative of model parameter uncertainty this uncertainty mainly results from parameter equifinality beven and freer 2001 and to a lesser degree the randomization involved in initializing the parameter sets running multiple independent model calibration runs with randomly chosen initial values has been proposed by sikorska senoner et al 2020 as a heuristic approach to deal with the parameter equifinality problem such an optimization approach should ensure that the parameter space is fully explored and should minimize the possibility of being trapped in the same local optimum during different calibration runs such an approach is prioritized over the likelihood based methods since it does not require making any assumption about the model errors which are simulated with a dedicated ddm in the cdda the hbv model was calibrated using data from 1985 2004 with a preceding 4 year warm up period of 1981 1984 and validated using years 2005 2014 at a daily time step in the three study catchments the kge score achieved in the validation period table 2 for the mean ensemble varied from 0 80 in the d√ºnnern catchment to 0 87 in the muota and kleine emme catchments these 1000 optimized parameter sets are afterwards used within hbv to generate an ensemble of streamflow simulations and subtracted from the observed streamflow to generate an ensemble of hm residuals finally the inputs are determined through ivs and the ddms are calibrated using the selected inputs for each of the 1000 hm residuals 3 5 input variable selection and calibration of data driven models prior to the calibration of the ddms input variable selection was required input variables for the ddm included observed streamflow precipitation and air temperature at the current and or previous nine days for streamflow inputs included observations from the nine days preceding the simulation day t 1 t 9 for precipitation and air temperature inputs included observations from the day of the simulation t 0 as well as the previous nine days t 0 t 9 in total 29 different input variables were considered as potential predictors of the hm residuals at t 0 the maximum time lag d for each input variable was determined using the conditional mutual information brown et al 2012 between the hm residuals at t 0 and each explanatory variable from t 0 to t d with the exception of streamflow which considered time lags t 1 to t d by locating the lag at which the cmi reached a local minimum the goal was to identify a sufficient number of time lags to accurately simulate the hm residuals while also attempting to keep the input variable set of a reasonable size given that precipitation has the largest effect on modifying the streamflow m√ºft√ºoƒülu 1991 it was given a higher priority in identifying the maximum time lag this approach resulted in a maximum time lag of d 9 section 2 4 2 outlines the method used to estimate cmi the different ddms were calibrated trained using the target residuals of the hm simulated streamflow and input variables time lagged versions of the observed precipitation air temperature and streamflow for the same calibration period as the hbv model i e years 1985 2004 the remaining data years 2005 2014 was used for validating the ddms to ensure consistency with the hm in the sub sections below the different ddm parameters and hyper parameters are described along with the strategy used to train the various ddms all ddms were developed on an intel r core tm i7 8750h cpu 2 20 ghz laptop with 32 0 gb ram the ann rf and xgb models were run in parallel on 10 cpu cores 3 5 1 k nearest neighbours regression knn as noted in section 2 3 1 the knn model does not require any explicit training strategy since model predictions are generated by computing the distance here the euclidean distance of a given input variable vector e g from the validation set with those from the training data locating its k nearest neighbours and taking the mean of the targets associated with each of the k neighbours while knn does not require training it requires the selection of the hyper parameter k with lower k values leading to predictions with high variance and low bias and vice versa for high k values hastie et al 2009 previous research by one of the authors found that a wide variety of k values 5 10 20 30 50 and 100 lead to similar results when sampling conditional model errors for streamflow simulation sikorska et al 2015 thus this study used k 5 seeking to strike a balance in the bias variance tradeoff related to the selection of k other common choices for k include n where n is the number of samples in the training set lall and sharma 1996 to ensure input variables with higher ranges are given equal weight or importance as input variables with smaller ranges prior to searching for nearest neighbours all inputs were individually normalized i e scaled between 0 and 1 using the maximum and minimum of each variable over the training set using normalized inputs in knn has been shown to significantly improve model performance compared to using unnormalized inputs piryonesi and el diraby 2020 and in earlier experiments was also found to have the same effects for the catchments explored here the fnn package in r beygelzimer et al 2019 was used for developing the knn models which uses the fast k nearest neighbours method beygelzimer et al 2006 and the kd tree approach friedman et al 1977 when searching for nearest neighbours 3 5 2 multiple linear regression mlr and second order volterra series model sov the parameters in mlr are the slope and bias coefficients associated with the design matrix input variables while the parameters in sov are the bias zero order and kernel coefficients associated with the first and second order interactions amongst the input variables the parameters in mlr and sov were solved via ordinary least squares wu and kareem 2014 note that there are no tunable hyper parameters for the mlr and sov models the mlr and sov models were developed using custom functions in r 3 5 3 artificial neural networks ann the parameters in ann include the input hidden and output layer weights as well as the hidden and output layer biases the particular application of ann used in this study is based on the avnnet function in the caret r package kuhn et al 2019 which makes use of the nnet r package venables and ripley 2002 this implementation of ann individually trains several networks with different randomly initialized parameters via the broyden fletcher goldfarb shanno bfgs algorithm fletcher 1987 and averages their predictions the hyper parameters include the number of networks set by default to 5 the number of training epochs or iterations set by default to 100 the number of hidden nodes since only a single hidden layer is used and the decay rate a grid search over the decay rate 1e 3 0 01 0 1 and number of hidden neurons 1 2 n i 1 where n i is the number of inputs in the ann hecht nielsen 1989 fatehi et al 2015 was carried out using 5 fold cross validation and the rmse as the objective function to determine optimal values for these hyper parameters the ann models adopted linear activation functions in the output layer and sigmoid logistic activation functions in the hidden layer all model inputs were normalized before training the ann i e using the same approach as knn 3 5 4 random forests rf the parameters in rf are the splitting variables and split points at each node of the decision trees while the hyper parameters include the number of trees b the number of variables selected randomly at each split m and the minimum node size n m i n hastie et al 2009 in practice it is generally found that once a sufficient number of trees have been considered in the forest including additional trees beyond this point does not substantially increase performance for example see the large scale study by probst and boulesteix 2018 and comes at an increased computational cost hastie et al 2009 further default values for n m i n set to 5 in the ranger r package wright and ziegler 2017 used to develop the rf models in this study have also been shown to provide high performance d√≠az uriarte and alvarez de andr√©s 2006 while probst et al 2019 confirm that n m i n is less important to tune than m they show that at an increased computational cost it can be jointly optimized with m to improve rf performance however in order to strike a balance between predictive accuracy and computational efficiency n m i n is fixed in this study at its default value 5 and a two stage approach is used to identify suitable values for b and m as described below at first a preliminary analysis was undertaken revealing that 300 trees b 300 and a minimum node size of 5 n m i n 5 led to stable performance when m was within the range of 1 5 thus b and n m i n were held constant at 300 and 5 respectively and m was optimized through a grid search m 1 2 5 using 5 fold cross validation where the rmse was used as the objective function the rf models were developed using a combination of the caret and ranger r packages kuhn et al 2019 wright and ziegler 2017 3 5 5 extreme gradient boosting xgb the parameters in xgb similar to rf are the variables and values used when node splitting the xgb hyper parameters are described in detail in chen and guestrin 2016 and chen et al 2019 and given below along with their ranges considered during optimization integer values are presented with an l nrounds i e the number of trees 1l 150l eta 0 001 0 5 gamma 0 10 max depth 2l 12l min child weight 1l 10l subsample 0 5 1 colsample bytree 0 1 1 lambda 0 1 alpha 0 1 the xgb hyper parameters were optimized using the bayesian optimization approach of snoek et al 2012 which is based on gaussian processes the bayesian optimization routine adopted the expected improvement method for updating the estimates of the best model parameters ribeiro et al 2020 and was run for 20 iterations after generating 5 initial starting points for the model parameters details on applying bayesian optimization using the expected improvement method for training ddms is described in zuo et al 2020 the xgb models were built using the parbayesianoptimization wilson 2019 and xgboost chen et al 2019 r packages 4 results 4 1 uncertainty intervals cdda vs benchmark hm through the cdda uncertainty intervals were generated for the ensemble streamflow simulations using six different ddms mlr knn sov ann rf xgb in addition to these six ddms one extra variant was considered for both rf and xgb models that considered only the six most important input variables as input to the ddm by assessing the variable importance scores of rf and xgb these model variants are referred to as rf 6 and xgb 6 these additional models were created for two reasons 1 to see if using a smaller number of inputs in these models leads to significantly poorer performance compared to the case when all inputs are considered and 2 to enable a comparison with the other nonlinear methods knn sov and ann that on average used six input variables as selected by the ea input variable selection method figs 3 5 present the 95 uncertainty intervals of these cdda variants for three study catchments these uncertainty intervals result from the parametric uncertainty of the hm model only i e via using multiple 1000 optimized parameter sets for the hbv model and thus represent only the 95 confidence intervals 95 cis since a ddm is trained for each set of hm residuals there are also 1000 ddms the combination of both hm and ddm result in the cdda based ensemble streamflow simulations for better visibility only a short simulation period of 30 days is displayed in figs 3 5 while a longer simulation period is provided in the supplementary data the cdda based ensemble streamflow simulations are compared to the benchmark i e hm based ensemble simulated streamflow which is simply the deterministic output of the hbv model for the 1000 optimized parameter sets as can be seen from figs 3 5 and those in the supplementary data the uncertainty intervals of all cdda and the hbv model are narrow for the three study catchments the cis for most of the cdda variants and hbv at the study catchments do well at covering low flows moderately well at medium flows but perform quite poorly at high flows in addition the 95 cis for all cdda closely overlap with the intervals of the benchmark model hbv hence based only on the visual assessment it is difficult to identify which of cdda variants performs best to support this analysis performance metrics were evaluated and are presented in section 4 2 the narrow uncertainty intervals for all cdda and the hbv model result from the fact that only the parametric uncertainty of the hbv model was considered whereas other sources of uncertainty input data model structure model output etc were excluded which if considered may prove useful in improving the quality of the uncertainty intervals this issue is further discussed in section 5 2 4 2 quantitative assessment of the cdda performance deterministic performance metrics for the mean ensemble simulation i e the mean over all 1000 ensemble members are presented for all cdda variants in table 2 whereas table 3 illustrates the ensemble performance metrics that consider all 1000 ensemble members these values are compared to the performance criteria computed for the benchmark which can be used to determine if and by how much the performance of the ensemble streamflow simulation is improved when using the cdda instead of the hm only the performance criteria are provided for all three study catchments four deterministic criteria were analyzed the mean absolute error root mean square error nash sutcliffe efficiency and the kling gupta efficiency please refer to the supplementary data for additional metrics optimal performance for nse and kge occurs at a value of 1 whereas for rmse and mae optimal performance occurs at a value of 0 thus if any cdda variants lead to an improvement in reference to hbv nse and kge will increase while rmse and mae will decrease from table 2 it can be noticed that most of the cdda variants lead to an improvement in most metrics over the standalone hbv based simulations namely nse rmse and mae were all improved for all cdda variants in comparison to the hbv in all three catchments regarding kge it was improved in the kleine emme catchment for models knn ann rf rf 6 xgb and xgb 6 and in the muota catchment for models sov ann rf rf 6 xgb and xgb 6 opposite to that in the d√ºnnern catchment none of the cdda variants led to an improvement of the kge values despite the kge values computed for the cdda being slightly lower they were still very close to the benchmark value obtained with the hbv model in this catchment by reviewing the additional performance metrics in the supplementary data it can be seen for the d√ºnnern catchment that the cdda variants have slightly higher bias than hbv which is the likely reason why the kge is slightly lower for the cdda among all tested cdda variants rf and xgb led to the largest improvement in the deterministic performance metrics followed by their variants rf 6 and xgb 6 as compared to the standalone hbv model for example rf rf 6 and xgb xgb 6 led to improvements in mae of 20 22 17 19 24 25 18 20 and 15 17 9 10 for d√ºnnern kleine emme and muota catchments respectively while the cdda based on mlr led to a marginal improvement only in these criteria e g improvements in mae of 1 6 across the three catchments to summarize the effect of the cdda in terms of its mean ensemble deterministic performance refer also to the supplementary data most ddms are very effective at significantly reducing variance in the resulting simulations especially rf rf 6 xgb and xgb 6 and in some cases rf rf 6 xgb and or xgb 6 also improve bias albeit to a lesser degree e g at d√ºnnern and muota catchments given that cdda generates ensemble streamflow simulations it is also very important to assess ensemble performance when comparing cdda to the standalone hbv to assess the properties of the uncertainty intervals of the cdda simulations versus the hbv based simulations benchmark three ensemble performance metrics were considered the mean continuous ranked probability score the mean interval score and the average width of uncertainty intervals which are presented in table 3 optimal values for crps mis and aw should be as small as possible thus if any of these criteria are lower for cdda than hbv then it is indicative that cdda provides superior ensemble performance with the caveat that lower crps and mis scores are preferred over lower aw scores by evaluating the ensemble performance metrics it can be noticed that for all three catchments crps was decreased for all cdda variants with the exception of mlr for the d√ºnnern catchment mis was decreased for all cdda variants in the d√ºnnern and in the muota catchments and for most of cdda variants in the kleine emme catchment apart from mlr regarding aw this criterion was decreased for most cdda variants in kleine emme apart from xgb 6 but only for three models in the other two catchments the analysis of aw is however not straightforward as the optimal uncertainty intervals should prioritize the smallest values for the other two criteria i e crps and mis over the aw thus a slightly larger value for aw obtained for a cdda variant with reference to the benchmark does not necessarily indicate poorer ensemble performance similar to the deterministic performance criteria when comparing all tested cdda variants xgb and rf provided the largest improvement in ensemble performance metrics followed by their variants rf 6 and xgb 6 with reference to the standalone hbv model for example rf rf 6 and xgb xgb 6 led to improvements in crps of 23 27 19 21 24 29 20 21 and 16 22 11 13 for d√ºnnern kleine emme and muota catchments respectively the cdda based on mlr led to marginal improvements in these criteria e g crps was increased by 6 and 4 for d√ºnnern and muota catchments respectively and no change was observed for the kleine emme in general based on both deterministic and ensemble performance criteria it can be concluded that all cdda variants led to an improvement in the ensemble streamflow simulations with reference to the hm based simulations among all tested cdda variants the cdda based on xgb led to the largest improvement in the ensemble streamflow simulations the second best model was the one based on rf the third and fourth best models were xgb 6 and rf 6 followed by ann sov and knn the worst simulation performance was achieved for the cdda adopting mlr which was only slightly better than the benchmark thus it may be argued that linear ddms are inappropriate for fitting hm residuals in the study catchments see further section 5 3 4 3 importance of input variables in ddms to simulate the hm residuals the different ddms considered several input variables a timeframe of up to 9 days preceding the day of simulation was considered for all three candidate input variables precipitation air temperature and streamflow as described in section 2 4 hence the previous 9 days of streamflow t 1 t 9 as well as the current and previous 9 days t 0 t 9 of precipitation and air temperature were considered as input variables in total 29 different input variables were considered as potential predictors of the hm residuals ivs was performed for each of the 1000 ensemble members considering the hm residuals as the target variable in order to determine the best predictors to use for each individual residual series since pcis and ea are both model free ivs methods they were run independent of the ddms i e the inputs and model parameters were determined separately in contrast rf and xgb are model based ivs approaches thus the inputs selected by these methods are related to the model parameters determined during training the importance scores of the input variables selected by the different ivs methods associated with the cdda variants are illustrated in figs 6 8 for the three study catchments which summarize the importance scores across all 1000 ensemble members using box plots it is assumed that the higher the importance score the stronger the impact the particular input variable has on the simulated residuals since the knn sov and ann models use the exact same inputs as determined by the ea ivs method only a single plot is considered for these methods since pcis is a linear ivs method it is solely coupled with mlr note that for xgb 6 and rf 6 only six input variables are plotted as these model variants considered only the six most important input variables determined by their base method xgb and rf it is important to note that all variables presented in these plots with importance scores above 0 were not necessarily selected for each of the 1000 ensemble members but any inputs with importance scores higher than 0 were selected at least once in the 1000 ensemble members from figs 6 8 it can be seen that generally the importance of the input variables increases with decreasing the lag time i e a higher importance is given to input variables whose lag time directly precedes the simulation day for all cdda variants among the three considered input variables precipitation was the strongest predictor of the hm residuals followed by streamflow the air temperature was the weakest predictor among all ddms apart from the mlr model where it was a stronger predictor than the streamflow in two out of three catchments looking at different models in detail it appears that for knn sov and ann which use the ea method based on conditional mutual information for ivs the air temperature does not play any significant role while the importance of the streamflow from the three preceding days t 1 t 2 t 3 and the precipitation from the three preceding days including the day of simulation t 0 t 3 were the most important for simulating hm residuals in all three catchments for mlr the precipitation t 0 t 1 t 2 was very important in all catchments whereas the importance of the air temperature and streamflow varied depending on the catchment in rf and xgb the precipitation t 0 t 3 the streamflow t 1 t 2 t 3 and the temperature t 0 t 3 were all very important in all three catchments with precipitation t 0 t 1 t 2 and streamflow t 1 being the most important yet it can be noticed that all input variables have above 0 importance in rf and xgb meaning that all variables contribute to the overall ensemble hm residual simulations for rf 6 and xgb 6 that consider only the six most important variables from their respective base model the selected variables included streamflow at t 1 and t 2 and precipitation at t 0 t 1 and t 2 in all three catchments the sixth most important variable varied depending on the catchment and it was either air temperature t 2 d√ºnnern air temperature t 0 kleine emme or streamflow t 3 muota the order of importance for these six variables varied depending on the catchment however streamflow at t 1 and precipitation at t 0 and t 1 were always found to be very important an interesting result was obtained by rf 6 for the muota catchment when only the six most important inputs identified by rf were considered in rf 6 it was found that precipitation at t 2 did not add any information that was useful when simulating the hm residuals this suggests that when all 29 inputs are considered there is another input outside of the other five selected inputs that when combined with precipitation at t 2 adds information that is useful for simulating the hm residuals while it is outside the scope of this research to identify the other inter dependent input s the interested reader may refer to galelli et al 2014 whom discuss the inter dependency of inputs in ivs 4 4 effect of the ensemble size simulations of both cdda and the benchmark hm were based upon 1000 ensemble members that originate from the 1000 optimized parameter sets of the hbv model yet it is not clear whether use of all 1000 ensemble members is of a value for the cdda as it appears from figs 3 5 the uncertainty intervals are rather narrow which may indicate that some members may be redundant this issue was investigated here by exploring the effect of the ensemble size on the simulation performance for this purpose fig 9 can be used to analyze how the crps changes as the ensemble member size grows for all cdda variants and the benchmark hm hbv in the three study catchments based on fig 9 it can be seen that as few as 100 ensemble members provide roughly the same ensemble performance as 1000 ensemble members for both the hbv based simulations benchmark and for the cdda based simulations when considering the crps using less than 100 ensemble members leads to a visible drop in the crps whereas using more than 100 ensemble members does not significantly improve the model performance the strongest improvement is noticed between one and 10 ensemble members which seems logical when moving from the deterministic approach only one ensemble member towards a ensemble approach several ensemble members this effect of growing the ensemble size on the model performance was visible for all tested cdda variants as well as for the hbv model in all three study catchments 5 discussion in this work an ensemble based conceptual data driven approach cdda was developed that consists of a hydrological model to simulate the precipitation streamflow process and a data driven model to simulate the hm residuals the goal of the cdda is to improve the predictive capability of the ensemble streamflow simulation compared to the standalone hm results from three study catchments in switzerland have demonstrated that coupling ddms with a simple hydrological model hbv is capable of improving the predictive performance of the hm assessed by both deterministic and ensemble performance criteria as compared to the standalone hm benchmark below the major findings of this work are explored in detail its limitations are discussed and some recommendations for future research are given 5 1 different ddm and importance of input variables six different ddms mlr knn sov ann rf and xgb and two additional variants rf 6 and xgb 6 were tested for simulating hm residuals these ddms link the hm residuals on the simulation day t 0 to observed precipitation and air temperature on the simulation day and those preceding it t 0 to t 9 as well as streamflow on preceding days t 1 to t 9 the maximal time lag of nine days was determined by studying the nonlinear correlation between explanatory variables and the hm residuals see section 3 5 these results demonstrated that generally all ddms coupled with the hm within the cdda led to an improvement in the ensemble streamflow simulations with reference to the hm based simulations section 4 2 this increase in the streamflow simulation performance measured by deterministic and ensemble performance metrics was however not the same for all models and for the simplest ddm i e mlr it was only marginal the largest increase in the model performance was achieved with xgb and then with rf models the xgb and rf models consistently had the best performance across all three catchments analysis of the input variable importance section 4 3 revealed that the importance of the input variables increases with decreasing lag time i e input variables from days directly preceding the simulation day were of higher importance than those from the distant past for all ddms the observed streamflow t 1 t 2 t 3 and precipitation t 0 t 3 from the current and preceding three days had the highest impact on the hm simulated residuals while for certain methods rf and xgb the air temperature from the current and three preceding days t 0 t 3 were also deemed important inputs however for xgb and rf observations farther in the past t 4 t 9 also had an impact on the simulated residuals hence it appears that despite the maximal time lag of nine days the effective memory length which determines the lag length beyond which input variables have only marginal or null effect on the simulated output appears to be about three days directly preceding the simulation day this memory lag length bowden et al 2005 may be further explained by the catchment memory to past inputs that determines how long water is retained in the catchment in different forms such as aquifers snowpack or groundwater storage m√ºft√ºoƒülu 1984 rajurkar et al 2004 this catchment memory is often identified as precipitation influence history m√ºft√ºoƒülu 1991 which is the strongest predictor of the streamflow the length of the catchment memory varies between catchments and may be from several hours to several days or longer as the results indicate not only the simulated streamflow but also residuals of the hydrological model may be linked to the catchment memory from these findings it appears that the more complex ddms that link the residuals to additional input variables i e use longer time lags have better predictive skill in correctly modelling the residuals of the hm hence although the most important input variables seem to be observations from the previous three days using longer lag times in more complex models further improves the model performance note however that using longer lag times with simpler models does not improve the model performance next among the three considered input variables the observed precipitation was the strongest predictor of the hm residuals followed by the observed streamflow while the observed air temperature was the weakest predictor of residuals this seems reasonable as the precipitation is usually also the strongest predictor of streamflow in any hm that relies on the precipitation streamflow generation concept the air temperature is usually used within many hms to determine the form of the precipitation liquid or solid and to determine whether snow melt occurs seibert and vis 2012 thus it seems reasonable that its impact is smaller than that of the precipitation which determines the amount of water entering the catchment the importance of the observed streamflow as a predictor of model residuals can be explained by the auto correlation effect present for most hydrological models i e when streamflow residuals at t 0 are correlated with residuals at preceding time steps t 1 t 2 etc sorooshian and dracup 1980 yang et al 2007 sikorska et al 2012 the strength of this auto correlation likely depends on the precipitation and flow conditions and for wet or high flow periods it is expected to be higher than for dry or low flow periods this concept has been explored by del giudice et al 2013 who linked the residuals of a hm to the precipitation amount or streamflow via bayesian inference the authors found that if conditioned on one variable the streamflow dependent description of model residuals was better performing than the precipitation dependent error however this seems opposite to the findings from this study although ddms that rely on only a single input variable were not investigated here instead ddms always included several input variables 5 2 applicability of the ensemble based cdda approach and limitations the results demonstrated that the ensemble based cdda framework is very promising for simulating hm residuals and generating improved ensemble streamflow simulations when compared to the hm benchmark although in this study only a single conceptual model hbv was applied to simulate the precipitation streamflow process the cdda is not limited to this model and any other hm can be coupled with the proposed framework it should be noted that using any other hm or even an hbv model with different parameters would require the ddms to be recalibrated since the hm will generate different residuals in addition since different hms may use other input variables e g potential evapotranspiration the importance of such variables should also be explored to develop the best possible ddm for the given dataset the same holds for an application of the proposed approach to other sites where the hm residuals may exhibit different properties than those obtained in this study hence the ddms should be re calibrated trained based on local observations yet the cdda as a general framework for improving ensemble based hm simulations remains essentially the same for different hms or study sites in general the cdda can be used to identify the best ddm and select the most suitable input variables for a given location using a ddm for simulating hm residuals is highly advantageous since it does not require any explicit assumptions on the characteristics of the residuals as is the case in all bayesian based methods thus there is no need to assume the residuals independence normality or auto correlation structure moreover with methods such as rf and xgb it appears there is little need to perform pre selection of input variables as important variables tend to be included in the modelling framework as corroborated by model free ivs methods such as ea allowing the model to distinguish on its own relevant from irrelevant and or redundant inputs however the use of inputs that have unjustifiable use in the model is not advocated as it is rational to only include in the ddms input variables that have an impact on the hm model residuals and in this case streamflow even for methods such as rf and xgb performing ivs may lead to improved performance and models with lower complexity tyralis and papacharalampous 2017 hadi et al 2019 indeed for methods such as mlr knn sov and ann input variable selection is a necessity since the models on their own lack the ability to filter out non useful inputs galelli et al 2014 although recent research has made progress in addressing this short coming by placing tunable weights on the input variables used in the ddm which may be tuned simultaneously along with the ddm parameters to provide insights on how the input variables impact model predictions yang et al 2020b further the use of ivs can also help reduce the computational burden of model development which is substantial for large ensembles e g 1000 members although running the models is extremely quick in an operational setting thus by reducing the computational burden ivs may also increase the exploration of alternative ddms additionally ivs may also prove useful in reducing the number of ensemble members in the cdda by identifying a reduced number of members that maintain a similar level of performance as the initial ensemble size for example it was found that 100 ensemble members i e the first 100 out of 1000 randomly generated members appeared to include approximately the same level of information as the entire 1000 member ensemble see section 4 4 the major limitation of this study is that only parametric uncertainty of the hydrological model here hbv was investigated and represented with multiple optimized parameter sets 1000 without explicitly considering any other uncertainty sources of the hm such as uncertainties in the inputs or model structure renard et al 2011 sikorska and renard 2017 hence only confidence intervals surrounding the hm parameters can be computed this can also explain why the uncertainty intervals are very narrow for all simulations i e for the standalone hm as well as for different cdda variants note that residuals simulated with a ddm represent the remaining uncertainty of the hm that is not explicitly considered yet as the output of the cdda is conditioned on the simulations from the hm narrow uncertainty intervals resulting from the hm translate to narrow uncertainties resulting from the cdda however in order to focus on the exploring the difference in performance across several ddms different uncertainty sources i e other than parameter uncertainty were explicitly not considered here the effect of other uncertainty sources should be however investigated in future studies at present the proposed ensemble based cdda framework has only been tested in three gauged catchments yet it would be very interesting to test the approach in ungauged catchments without streamflow observations this would enable for improving streamflow simulations at sites where performance is the lowest due to a lack of recorded data to calibrate a hm as a direct calibration of the cdda and its components i e the hm or ddm is not possible at ungauged locations other methods to inform both models should be considered including regionalization approaches to inform the hm model parameters while training in a large set of catchments of different properties could help to constrain information on simulated residuals and transfer these residuals to the ungauged catchments that have similar properties gauch et al 2021 found that using data from a large set of catchments to train a single ddm yields better streamflow simulation results as the number of catchments increases also in poorly gauged regions suggesting potential towards generalization of ddms wu et al 2019 have also demonstrated that a ddm used to simulate residuals of an un calibrated hm may improve its predictive performance finally it is important to note that in an operational context if there are no streamflow observations available then lagged measurements of the streamflow cannot be used as an input variable for simulating streamflow via the cdda this is relevant as there may be cases where streamflow gauging stations may be temporarily offline or are no longer in operation due to decommissioning damage from a flooding event etc tencaliec et al 2015 villalba et al 2021 thus obtaining updated information on recent streamflow is not possible in these cases while streamflow data may be used for calibrating the cdda i e using streamflow measurements from a limited historical database as the target variable other input variables may be required instead of streamflow to improve its predictive performance 5 3 recommendations to summarize the results indicate for the study catchments that it is generally better to include almost any nonlinear ddm to simulate the residuals of a hydrological model than using a standalone hm however in the case of the simplest data driven model mlr the improvement in ensemble streamflow simulation performance was only marginal whereas more complex nonlinear models xgb and rf led to a significant improvement in the ensemble streamflow simulation performance however using a ddm to simulate hm residuals is linked with some additional computational efforts thus the selection of appropriate ddms is very important when computational resources are limited since it was found that mlr barely improved the original hm simulations this approach is not recommended unless of course there is reason to believe the relationship between hm residuals and input variables is linear which did not appear to be the case for the study catchments however it was found that xgb and rf both led to significant gains in deterministic and ensemble performance over the standalone hm given that both methods inherently perform ivs and thus require little user intervention while providing impressive performance are recommended for further study additionally since it was found that only 100 ensemble members provided a similar level of performance as the initial 1000 member ensemble it is plausible although it was not verified that a smaller and carefully selected set of ensemble members identified via ivs may provide a similar level of performance as the initial ensemble size this could be explored in two ways by performing ivs 1 on the hm ensemble and then developing ddms for this reduced set or 2 on the cdda ensemble speculation as to which of the two approaches would result in better performance is out of the scope of this paper but it is recommended as an interesting line of future research additionally it is recommended to include other sources of uncertainty in the ensemble based cdda such as those related to inputs input variable selection parameters and model output in order to improve uncertainty estimation and the overall utility of the ensemble simulations finally other potentially useful input variables such as potential evapotranspiration soil moisture relative humidity wind speed are recommended to be explored in the ddm even if such methods are unable to be included in a particular hm 6 conclusions a novel ensemble based conceptual data driven approach cdda has been proposed it consists of an ensemble of hydrological models hms used to simulate the hydrological process es of interest and an ensemble of data driven models ddms to simulate the resulting ensemble of hm residuals while the cdda can be used to simulate any hydrological process it is applied for ensemble streamflow simulation due to its significance to the hydrology and water resources communities here the ddm uses as input time lagged precipitation air temperature and streamflow to simulate the hm residuals the cdda combines the advantages of a hm respecting hydrological processes with the ability of the ddm to simulate complex nonlinear relationships between input target explanatory response variables by tackling auto correlated hm residuals the cdda does not require any statistical assumptions about the hm residuals and it provides a framework for identifying suitable ddms and input variables moreover the ensemble based cdda is very flexible as it can be coupled with any hm and any ddm the selection of potential input variables can also be adjusted based on available data as well as user needs and specific conditions e g hydrological model or type of runoff generation using three swiss catchments as a case study the cdda was shown to be a very promising approach for improving ensemble streamflow simulations among eight variants of different ddms extreme gradient boosting xgb and random forests rf were found to be the most accurate predictors of the hm residuals and also required less user intervention in terms of selecting appropriate inputs to the ddm it was found that precipitation and streamflow from the three days directly preceding the simulation day had the largest impact on the simulated hm residuals since xgb and rf demonstrated the best performance for the study catchments improving the continuous ranked probability score by 16 29 both models are recommended to simulate the residuals of the hm within the ensemble based cdda framework it was also found in this study that the number of ensemble members may be substantially reduced i e from 1000 to 100 without significantly affecting model performance this is especially important for cases where computational resources are limited declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors wish to thank the three anonymous reviewers who provided constructive feedback that has helped to improve the quality of this paper this research did not receive funding from any agency and was carried out according to the authors curiosity the swiss federal office for the environment foen is acknowledged for providing the streamflow data used in this study that were available from the project no 15 0054 pj o503 1381 calibration of the hbv model was run using the sciencecloud provided by s3it at the university of zurich appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105094 data availability the observed discharge data for calibrating the hydrologic model can be ordered from the foen https www bafu admin ch last access december 11 2020 the observed meteorological data from meteoswiss http www meteoswiss ch last access december 11 2020 the latest version of the hbv model is available at https www geo uzh ch en units h2k services hbv model html funding this research did not receive any specific grant from funding agencies in the public commercial or not for profit sectors 
25793,worldwide billions of people rely on fresh groundwater reserves for their domestic agricultural and industrial water use extreme droughts and excessive groundwater pumping put pressure on water authorities in maintaining sustainable water usage high resolution integrated models are valuable assets in supporting them the netherlands hydrological instrument nhi provides the dutch water authorities with open source modeling software and data however nhi integrated groundwater models often require long run times and large memory usage therefore strongly limiting their application as a solution we present a distributed memory parallelization focusing on the national hydrological model depending on the level of integration we show that significant speedups can be obtained up to two orders of magnitude as far as we know this is the first reported integrated groundwater parallelization of an operational hydrological model used for national scale integrated water management and policy making the parallel model code and data are freely available keywords parallel computing distributed memory netherlands hydrological instrument groundwater numerical modeling integrated modeling 1 introduction worldwide groundwater reserves being of vital importance for more than 7 billion of people for drinking water agriculture and industry wada et al 2014 are threatened under changing climate conditions and increasing population threats such as extreme droughts and excessive groundwater pumping are putting strains on national and regional water authorities to come up with adequate long term plans for investments and adaptive measures leading to a sustainable and robust water management for the decennia to come the netherlands with a long history in water management huisman 1998 experienced a severe drought in 1976 which led to the development of several nationwide model applications ranging from nested systems based models of the complete water system pulles and sprong 1985 national scale finite element groundwater models kovar et al 1992 and national scale analytical element models de lange 1996 a number of high resolution regional groundwater model applications were developed from 2000 2013 to support groundwater management by water boards and drinking water companies and eventually the model applications covered most of the netherlands under these developments in 2005 the national and regional water authorities joined forces in unifying their modeling software and data together with several national research institutes former alterra now wageningen environmental research former tno bgs and wl delft hydraulics now deltares former mnp now pbl this initiative led to the release of the consensus based national hydrological instrument in 2013 nhi de lange et al 2014 for more than a decade the nhi provides the dutch water authorities and consultancies with a modeling environment used for answering actual water related questions where the continuity of management and maintenance along with new developments of the modeling software and data is secured by the nhi consortium driven by a large amount of available data in the netherlands e g due to the many boreholes van der meulen et al 2013 and the dense surface water network covering the netherlands these models typically have a high spatial 250 m and temporal 1 day resolution and inherently involve many computational cells and timesteps for the national model application of the nhi here briefly referred to as the netherlands hydrological model nhm computational resources required for running this integrated model as a single thread on a single core are significant ignoring surface water flow and transport model component one simulation year takes 9 h computing time 45 gb gigabyte of ram and 30 gb of disc storage this severely limits the practical application of the nhm e g in the dutch national water model prinsen et al 2015 for evaluating future climate change scenarios with proposed adaptation measures which could require simulation time scales up to 100 years this means that assuming constant computing time during simulation 100 years of simulation would roughly take 900 computation hours or 37 5 days and 3 tb terabyte of storage with a single threaded run on a single core such long run times are highly undesirable since typically a large number of simulations are required for model calibration and scenario analysis furthermore such long run times require that servers are stable for long periods of time which in practice is difficult to accomplish distributed memory parallel computing see e g eijkhout et al 2015 r√ºnger and rauber 2013 is a method to significantly reduce computational times and memory typically following a non uniform memory access architecture numa in numa the entire computational grid memory is first partitioned into multiple subdomains and one or more subdomains is assigned distributed to a node each having local main memory ram and one or more multi core cpus processors then the processor cores solve the problem simultaneously while exchanging necessary data between the nodes through a fast interconnection network using the message passing interface mpi forum 1994 in the remainder we refer to a mpi process as the program that uniquely runs on an associated single processor core in this paper we focus on distributed memory parallelization of two of the five hydrological model codes that have been combined in the nhi the model code for saturated groundwater and the model code for soil vegetation water transfers in the unsaturated zone svat see de lange et al 2014 the reason for doing this is that the groundwater and svat model components are most time consuming and memory intensive for groundwater we parallelize the model code modflow harbaugh 2005 the most widely used groundwater flow modeling program in the world developed by the united states geological survey usgs modflow has a large open source community of users and developers from many governments consultancies and research institutes for the svat model component we parallelize the model code metaswap transol van walsum and veldhuizen 2011 metaswap is a fast richards equation emulator that uses a database of steady state soil moisture profiles for soil physical units and transol an emulator of the advection dispersion equation metaswap is implicitly connected to modflow through memory at the outer picard iteration level since parallelization of metaswap transol is done in a relatively straightforward way without requiring communication between processors hence embarrassingly parallel the focus in this paper is primarily on parallelizing modflow parallelization of the nhi surface water hydrological model codes is beyond the scope of this research although this paper focusses on parallelization of nhi model codes used in the national scale coupled nhm all the presented methods and software can be used as standalone applications and used at other spatial domains our parallel implementation successfully went through the four phases of dtap development testing acceptance and production https en wikipedia org wiki development testing acceptance and production and is operational in the national water model prinsen et al 2015 running on eight processor cores as far as we are aware this is the first time that such integrated groundwater parallelization is applied in such a setting furthermore our open source software is readily available to a wide range of hydrogeological modelers at regional water authorities and consultancies 2 methods 2 1 general nhm parallelization strategy here the nhm de lange et al 2014 is defined as five coupled nhi hydrological model components see fig 1 the groundwater gw model component consisting of 7 confined model layers the soil vegetation atmosphere for the transfer of quantitative water in the unsaturated zone svat model component the unsaturated zone salt transport uzst model component the surface water for sub catchments swsc model component and the surface water for optimized distributing swod model component this definition differs from de lange et al 2014 in a way that in our study we exclude the surface water flow and transport model component and include the uzst model component model component details are summarized in table 1 see de lange et al 2014 for a more comprehensive description and details on the coupling connectors in this paper besides the fully coupled nhm or fnhm including all five models components we also consider the reduced nhm of rnhm that only includes the coupled gw and svat model components the reason for doing this is that gw svat models are commonly used as regional application of the nhi by a large number of dutch consortia of provinces water boards drinking water companies and municipalities see e g snepvangers et al 2007 a timing experiment for the fully coupled nhm on the nhi windows server see fig 2 shows that the gw svat and uzst model components are most time consuming and account for 52 16 and 26 of the total simulation run time respectively hence this motivates parallelizing these model components for sake of simplifying coding our parallelization assumes that each vertical column of cells is assigned to the same subdomain including the coupled gw svat uzst cells hence our partitioning of the computational grid is in lateral horizontal direction only this seems a valid assumption since in groundwater models the number of lateral cells is generally much larger than the number of model layers and therefore our approach naturally minimizes the subdomain interface surface area and mpi communication fig 1 illustrates the partitioning of nhi for the case of two subdomains where the left subdomain is assigned to parent process p0 and the right subdomain to worker process p1 in our parallelization we always assume that each subdomain is uniquely assigned to a single processor core corresponding to a single mpi process furthermore the parent process is always responsible for gathering all necessary data from the worker processes and coupling towards the surface water sub catchments except for the off line file based coupling connectors swsc svat and swsc gw parallelization of the other nhm connectors is done in a straightforward manner for swsc svat and swsc gw all processes read and clip the necessary data from the output files of the swsc model component in parallel e g sub catchment river stages for the groundwater model component and groundwater sprinkling for the svat model component for svat swsc and gw swsc each process aggregates all necessary fluxes in parallel for the surface water sub catchments e g drainage discharge from the groundwater model and sends them to the parent process that writes the input file for the swsc model component since subdomain boundaries may divide surface water sub catchments e g see the light blue eastern sub catchment in fig 1 this means that computing and communicating of partial sums is involved parallelization for the svat and uzst model components is straightforward since both models apply a one dimensional discretization in model layer vertical direction and therefore no lateral mpi communication is needed 2 2 parallel performance evaluation a commonly used indicator for measuring parallel performance is speedup s p t 1 t p where t 1 is the serial run time using a single processor core and t p is the parallel run time using p processer cores see e g eijkhout et al 2015 r√ºnger and rauber 2013 in practice evaluating strong scaling means that the problem size e g defined by the number of grid cells is kept fixed while the number of processor cores is increased from a modeling point of view this matches best how users evaluate the parallel performance for their existing models when they have access to multiple processors in our evaluation we measure speedup using actual measured wall clock time instead of cpu time and use the same solver and solver settings for all serial and parallel runs in our study we have not attempted to determine the optimal performing serial solver or to determine the optimal solver settings although for the ideal case s p ideal p in real world applications this is difficult to obtain first from an algorithmic and programming point of view this depends on the portion of work load that could not or has not been parallelized if we denote this serial fraction as f then the well known ahmdahl s law amdahl 1967 states that s p f 1 f p 1 and therefore the asymptotic theoretical speedup s 1 f when assuming unlimited computer resources for the fully coupled nhm model f is estimated 0 06 which only accounts for the non parallelized surface water models fig 2 and 6 swsc swod thus the maximum theoretical speedup is bounded by s 16 7 since other models are also likely to contain serial fractions e g due to solver limitations we should therefore be realistic about our expectations second assuming that the processors used are connected through a fast interconnection network low latency and high bandwidth such as a infiniband interconnect achieving parallel performance is typically hampered by communication overhead in the form of wait time b√∂hme 2013 work load imbalance is defined as i p p max p l p l where p is the total number of cores being used l p is the work load for core p and l the total work load 2 3 parallelization for groundwater model component 2 3 1 relationship to other work the groundwater model component for the nhm is based on the model code modflow harbaugh 2005 a numerical groundwater flow simulation code using control volumes cells for solving the discretized groundwater flow equation this typically results in consecutively solving large and sparse linear systems of equations accounting for most of the simulation run time therefore parallelization of the modflow linear solver has been subject of considerable research see table 2 our distributed memory parallelization has similarity with some of the work from table 2 especially with schreuder 2005 and naff 2008 regarding linear solver preconditioning we also apply the additive schwarz preconditioner for solving the linear system in parallel however there are notable differences compared to those efforts first our approach is fully distributed memory including input and output data second for load balancing models with irregular model boundaries e g due to administrative boundaries or geology we support a robust orthogonal recursive bisection method berger and bokhari 1987 boman et al 2012 fox 1988 to divide partition the groundwater cells into equally loaded blocks given an arbitrary number of processors schreuder 2005 also addressed this problem and developed a partitioning method that iteratively merges cell weighted blocks while shifting subdomain interfaces however this method was concluded not to be robust enough for general purpose third our parallel software only depends on the mpi software library and is therefore relatively easy to compile on multiple platforms this makes our software accessible to users on a wide range of operating systems contrasting with other parallelization efforts that use parallel solver libraries primarily developed for the linux unix operating system and have low to no support for windows machines e g the petsc solver library balay et al 2014 fourth our parallel software is open source and actively maintained as part of imod vermeulen et al 2019 imod is an easy to use graphical user interface for the windows operating system that integrates our accelerated modflow 2005 version with fast subdomain modeling techniques and is extensively used by the nhm user community fifth we add a new modular unstructured parallel solver to modflow 2005 to which we refer to as the parallel krylov solver pks verkaik et al 2016 2015 that is largely based on the upcg linear solvers hughes and white 2013 besides assuring minimal dependency on third party software another reason for developing the pks is the ease in reproducing the stopping criteria of the commonly used pcg solver hill 1990 and the flexibility of adding advanced parallel multi level preconditioners in the near future 2 3 2 general description the main components in our parallelization for groundwater models are a partitioning of grid cells into subdomains blocks b setting up communication between subdomains c reading and writing model input and output files in parallel and d parallelization of the linear solver in this section we highlight the basic concepts of a c referring to appendix a 1 for more details for the technical details on solver parallelization d the reader is referred to appendix a 2 2 3 2 1 subdomain partitioning in general parallelization aims to minimize processor idle times in which a processor does nothing but wait for other processors to finish see e g r√ºnger and rauber 2013 reduced idle times can be obtained by load balancing and minimizing communication overhead between the processors load balancing means that work is equally assigned to the processor cores and in our application directly relates to distributing partitioning cells of the computational grid to minimize communication overhead we partition the horizontal plane as there is only a few vertical model layers that gives the smallest interface surface area between subdomains and hence the amount of data communicated two non overlapping partitioning methods for partitioning the grid in the horizontal plane are considered a straightforward method for obtaining equally sized rectangles here referred to as uniform partitioning and orthogonal recursive bisection orb berger and bokhari 1987 fig 3 illustrates these methods for an example grid having an irregularly shaped model boundary considering four partitions p1 p4 see appendix a 1 for details the red boxes in fig 3a show the partitions obtained by straightforward uniform partitioning clearly showing that the number of active cells vary strongly for each partition the red boxes in fig 3b show the partitions obtained by the orb partitioning here using cell weights of value one as illustration these user defined cells are used within the orb partitioning to balance the partitions targeting equal sum of cell weights for each partition typically these cell weights could be chosen to be equal to the number of model layers it should be noted that our orb partitioning also includes the dirichlet boundary conditions or constant value active cells as in example fig 3 denoted by index 1 since these cells are eliminated in the linear solver this means that a load imbalance may occur in the solving process although the orb partitioning might be optimized for these boundary conditions this was not a subject for this research in our approach the grid is partitioned only once prior to simulation hence corresponding to static load balancing therefore we neglect the spatio temporal variation in computing time that might occur during simulation due to changing boundary conditions causing load imbalance for the nhm this is a reasonable assumption since the number of active cells does not vary in time and the input output data size and frequency remains constant 2 3 2 2 overlap and communication discretization of the groundwater flow equation typically results in evaluating a 7 point computation stencil 5 point in the horizontal plane 3 point in the vertical plane meaning that the unknown head in a cell implicitly depends on the heads of at most six neighboring cells in a parallel setting this means that evaluating the discretization of a cell near the subdomain boundary for instance which is necessary for computing a matrix vector product within a linear iteration data from the adjacent subdomain is required and therefore needs to be communicated to support such local point to point communication the non overlapping partitions are expanded to overlapping partitions see pink boxes for p1 for the example in fig 3 by adding one row of cells so called halo or ghost cells see the dark greens cells for p1 in fig 3 for example in fig 3a with the uniform partitioning processor p1 needs to communicate with p2 and p3 and each processor has exactly two neighbors however as can be seen in fig 3b for the orb partitioning p2 has three neighbors p1 p3 and p4 and hence there is an additional interface between p2 and p3 this additional communication for orb is the trade off for obtaining optimal load balance for our application this does not seem to be an issue since the amount of data communicated remains low and local communication overhead is secondary compared to global communication overhead i e communication involving all processors requiring synchronization and load imbalance the large benefit of using overlapping partitions is that no additional data e g inter cell transmissivity need to be explicitly specified by the user or communicated for evaluating the discretization scheme at the subdomain interfaces moreover this physical overlap facilitates the usage of advanced computational schemes with relative ease such as for applying full tensor anisotropy that requires the evaluation of cross terms in our approach halo cells are treated in a similar way as computational cells regarding input output and matrix assembly but are different during computation each processor is responsible for updating computing groundwater heads for its non overlapping subdomain and halo cells are used to store data received copies from adjacent processors due to the symmetry of the subdomain overlap local communication is two sided meaning that in addition to receiving data each processor sends data to the neighbor processor see for the example in fig 3 the light green cells 2 3 2 3 input and output our parallelization supports independent parallel input output where each process reads its subdomain data from files that are defined for the entire computational domain and writes its subdomain results to separate files parallel input is done from files supporting raster data point data and line data for reading raster data we use the binary geo referenced imod data format idf imod python development team 2017 vermeulen et al 2019 since this file format supports fast unformatted binary direct access read and can be easily visualized with the imod graphical user interface idf files allow us to efficiently read subdomain data in parallel while keeping memory usage locally besides pumping well and geological fault data that are read as point and line data respectively all static module and dynamic package data e g for rivers and drains are read from idf raster files this means that a significant amount of redundant no value data might be read for sparse raster files e g for modeling drainage systems in semi arid areas since in the netherlands the surface water network is dense the nhm raster data is also dense and therefore the expected redundancy of using idfs is low parallel output is straightforward where each process writes its separate idf files or standard modflow ascii binary files for its non overlapping partition post processing these subdomain results might require additional tools such as imod for merging these data into a single dataset for the total computational grid 3 test cases to evaluate the parallel performance we consider three test cases with increasing complexity a hypothetical steady state regional scale groundwater model the reduced nhm excluding modeling of salt transport in the unsaturated zone and dynamic surface water simulation for 2006 the fully coupled nhm with the same initial set up as the reduced nhm 3 1 hypothetical steady state groundwater model this hypothetical test case simulates steady state regional scale groundwater flow in a heterogeneous aquifer for a square computational domain applying uniform partitioning see fig 4 for 12 12 144 subdomains the model area is 1000 m 1000 m and two model layers are used from 10 m to 15 m and 15 m to 30 m respectively each having 8000 cells in both x and y direction hence the model has 128 million active cells each having a resolution of 0 125 m 0 125 m a hydraulic gradient of 0 01 m m is specified in the west to east direction no flow boundary conditions are specified along the north and south edges of the model and four equal pumping wells are located in the center of the domain withdrawing a total of 1000 m 3 d from the lowest model layer this test case has great similarity with the problem considered by hughes and white 2013 our model uses the same simulated multivariate gaussian 10log hydraulic conductivity field for both model layers with an average of 4 81 m d a 10log variance of 1 23 and an effective range of 750 m and with the same top and bottom of the aquifer and boundary conditions since we have more cells in the x and y direction the hydraulic conductivity of hughes and white 2013 is downscaled using nearest neighbor interpolation our model only considers a single discretization and we assume that the transmissivity of the aquifer is constant confined option which makes the problem linear a linear model gives better insight into the parallel solver performance and is consistent with the assumptions applied in the nhm model convergence with the pks is obtained for the test case where the stopping criteria are Œµ hclose 0 001 m maximum absolute groundwater head difference and Œµ rclose 1 5625 10 5 m 3 d cell size squared times Œµ hclose the simulation starts with initial heads of 0 0 m across the entire domain see appendix a 2 for more details since the domain is square and all cells are active the uniform partitioning is used and results in optimal load balance where the number blocks of in x direction column direction is always equal to the number of blocks in y direction row direction 3 2 reduced and fully coupled nhm simulating 2006 for both test cases nhm v3 1 is taken hoogewoud et al 2015 see www nhi nu the default groundwater solver settings are used corresponding to Œµ hclose 0 001 m Œµ rclose 100 m 3 d and a maximum of 30 inner iterations for the pks see maxinner in fig 12 of appendix a 2 the simulation period is 2006 for both the nhm test cases we apply orb partitioning since the model boundary is irregular see fig 5 for the example of 24 subdomains for obtaining a reasonably well load balance for the combined groundwater and svat cells a trial and error method is used to obtain the orb cell weights see section 2 3 2 starting from a uniform cell weight of one for both the groundwater and svat cells the cell weights are obtained by simply increasing the weights for the svats cells from 1 to 10 timing results show that the value of 5 seems to give the overall lowest computing times 3 3 hardware and compiler the parallel performance is evaluated on the dutch national supercomputer cartesius surfsara 2014 for the hypothetical test case and the reduced nhm test case and on the nhi server for the fully coupled nhm cartesius consists of 1900 computing nodes running on linux that are tightly connected using a fast interconnection network for a total of 48 thousand intel xeon cores and 128 terabytes of memory all scaling experiments on the cartesius are carried out on so called thin nodes where each node consists of two haswell 12 core cpus e5 2690 v3 with a total of 64 gb memory the single node nhi server runs on microsoft windows and consists of two intel haswell 16 core cpus e5 2698 v3 with a total of 128 gb memory at the time of evaluating the test cases the nhi server is a dedicated resource for running nhm simulations that are used for national scale long term planning the reason for not evaluating the fully coupled nhm test case on cartesius is that the surface water for sub catchments model code mozart and surface water for the surface water for optimized distributing model code dm are not supported to run on linux we compiled the coupled modflow metaswap transol model code as part of imod v4 0 using the intel fortran compiler v15 0 with high level o3 optimization on cartesius the compiled code was linked with the intel mpi library v5 0 update 3 and for running on the nhi server the code was linked with the mpich library v1 4 1 on cartesius we use a maximum of four cores per node hence two cores per cpu based on trial and error testing that has indicated that run times are shortest using this number of cores using a maximum of four cores per node results in 20 idle cores during computation and a relatively low core utilization of 17 the reason why using more cores in our trial and error analyis results in higher run times can likely be explained by large memory requirements for our model applications and the competition of processor cores within a multi core cpu for the main memory tudor et al 2011 since we expect this is a hardware related issue inherent to multi core architectures this issue is recommended for future research and left outside the scope of this study 4 results 4 1 hypothetical steady state groundwater model fig 6 shows the measured speedups a and total memory usage b for the hypothetical test case see section 3 1 for our strong scaling experiments on cartesius up to 144 processor cores the serial run requires 4 h and 48 min computing time to converge and 45 gb main memory the computing time is reduced to 2 min and 40 s using 144 cores 36 nodes resulting in a speed up of 108 the absolute groundwater head difference is less than the specified Œµ hclose 0 001 m in each cell in the serial and parallel simulations 4 2 reduced and fully coupled nhm simulating 2006 fig 7 shows the measured speedups a and total memory usage b for the reduced nhm test case on cartesius and the fully coupled nhm test case on the nhi server up to a maximum of 64 and 24 cores respectively maximum speedups of 21 6 and 4 6 are obtained for the nhm test cases respectively besides the ideal linear speedup fig 7a also shows the maximum speedup for the fully coupled nhm according to amdahl s law when accounting for 6 serial surface water computation 10 for 24 cores see section 2 2 regarding accuracy transient results for the serial and 24 core parallel simulations are evaluated the root mean squared error values for the entire period of 2006 and considering all model layers are 3 0 10 4 m and 1 1 10 3 m for the reduced nhm test case and the fully coupled nhm test case respectively for the reduced nhm the maximum absolute head difference greater than 0 001 m and 0 01 m is exceeded for 3 and 0 1 of the total number of cells during simulation respectively for the fully coupled nhm these values are 7 and 0 4 respectively 5 discussion figs 6a and 7a show that significant speedups are obtained ranging from two orders of magnitude for groundwater only modflow hypothetical test case one order of magnitude for the reduced nhm excluding dynamic surface water and unsaturated zone transport and less than one order of magnitude for the fully coupled nhm however the speedup curves flatten as the number of cores increases this can be explained by hardware related issues non scalable algorithms methods and non scalable components in implementation regarding hardware the memory competition issue see section 3 3 is likely to contribute to the flattening of the fully coupled nhm speedups since a maximum of 12 out of 16 cores per cpu is used compared to 4 out of 12 cores per cpu for the reduced nhm however due to the lack of scheduling control options on the nhi windows server we are not able to quantify this effect concerning non scalable algorithms a more important explanation for the flattening of the fully coupled nhm speedups can be found in the non parallelized surface water model components which account for 6 of the total run time using amdahl s law see section 2 2 and a serial fraction of 6 a significant flattening of the nhm speedup is expected see fig 7a where the maximum theoretical speedup using 24 cores is 10 and the measured speedup is 4 6 furthermore parallel linear solver iterations might increase with the number of subdomains as a result of low frequency eigen modes that can hamper the linear solver convergence dolean et al 2015 smith et al 1996 which can require an additional multi level preconditioner to improve convergence for our test cases however the maximum observed linear iteration increase is 15 see fig 8 and suggests that low frequency eigen modes have a relatively limited effect for that reason we did not find any need to apply such preconditioner with respect to the non scalable components the run time behavior is analyzed by cost analysis for the hypothetical test case and the reduced nhm test case on cartesius using the scalasca profiling and tracing tool geimer et al 2010 fig 9 shows the most significant cost components where c p c denotes the cost of component c for using p cores defined as the cumulative sum of time spent on c accounting for all processor cores the total cost c p of a parallel program is defined as c p p t p and perfect scalability or cost optimality is obtained when this cost remains constant for increasing number of cores hence in the ideal case for each component the relative value c p c c 1 c p c t 1 as shown in figs 9a and c should remain constant the ratio c p c c p as shown in figs 9b and d expresses the contribution of a component to the total parallel run time for the hypothetical test case fig 9a gws pks shows that majority of the run time is spent in the linear solver and this component has near perfect scalability however load imbalance is manifested in global mpi communication wait times mpi global in fig 9a since the subdomain partitioning is uniform we suspect that this imbalance is caused by the physical overlap of one row whereas we do not account for this overlap in the load balancing see section 2 3 2 for the reduced nhm test case scalability of the pks for the groundwater model component and computations for the svat model component are also nearly perfect see fig 9c gw pks and svat respectively however a strong load imbalance is observed where 45 of the run time is spent on waiting when using 64 cores mpi global in fig 9d this load imbalance is very likely related to the groundwater model component and svat model components sharing the same partition for groundwater active cells exist across the dutch land border in model layers 2 to 7 whereas svat cells exist only within the dutch land border using more subdomains enhances this discrepancy near the border and the orb partitioning becomes less effective this is illustrated in fig 10 for the example with 48 subdomains considering different orb cell weights in fig 10a the same cell weights are used as for our nhm test cases see fig 5 where in fig 10b cell weights are used that are equal to the number of active groundwater model layers tracing analysis shows that subdomains p36 and p42 are responsible for most of the delay which is mainly caused by global mpi communication groundwater cells for subdomains p36 and p42 are not connected to any svat cells fig 10b this results in a significant load imbalance for the svat model component although for groundwater load is well balanced and a total delay time of 75 relative to the total parallel cost however as can be seen in fig 10a by using different cell weights subdomains p36 and p42 now have connections to svats and by this we improve the svat model component load on the other hand we introduce a load imbalance for the groundwater model component however this has an overall positive effect on the total delay time that is reduced to 36 relative to the total parallel cost this illustrates that orb partitioning for the nhm is complicated by the coupled svat gw models a better approach for load balancing might be to decouple the groundwater and svat partitions however this would require a significant programming effort and is therefore beyond the scope of this current research other components that contribute to the flattening of the speedup curved for the reduced nhm fig 7a are related to input data reading see fig 9c the component gw init read is related to redundant file information required by the operating system the component svat read meteo is related to the non scalable ascii reading of 1 km precipitation and evaporation input grids and the component svat read database related to reading the database with steady states of soil moisture profiles for the 72 soil physical units since the svat model component is a metamodel that strongly relies on this pre compiled database see van walsum and veldhuizen 2011 in a worst case scenario when all soil physical units are entirely heterogeneous this means that each processor needs to read the entire database and keep all data in memory regarding ram memory usage the svat database significantly increases memory usage by 2 5 gb for the nhm test cases fig 7b for each processor core added in practice this means that sufficient memory should be available however for the reduced nhm test case on cartesius the 64 gb ram per node does not put any constraint on the processor core usage on the other hand for the fully coupled nhm test case on the nhi server fig 7b the memory increase together with the large memory usage of 40 gb for the unsaturated zone salt transport model component limits the core usage to not exceed the maximum of 128 gb ram together with the increase of read time this would advocate minimizing the number of soil physical units per subdomain as part of future research on the other hand the groundwater model seems to satisfy the distributed memory approach as illustrated in the hypothetical test case fig 6b although a slight memory increase is observed one reason for this increase is the inaccurate memory measurement on cartesius where we might overestimate the total memory usage by simply multiplying the measured peak amount of memory during simulation maxrss maximum resident set size with the number of processes another reason might be that the physical overlap of partitions introduces a slight memory increase for increasing number of processes regarding accuracy small absolute groundwater head differences between the serial and 24 core parallel nhm simulations see fig 11 red lines were observed that are larger than the linear solver head change stopping criterion of Œµ hclose 0 001 m the largest difference occurred in a small number of cells with 99 9 and 99 6 of the cells having mean absolute differences between 0 001 m 0 01 m in the reduced nhm and fully coupled nhm respectively errors greater than 1 m were observed during summer 2006 are occur in only 5 cells 0 08 and 10 cells 0 16 in the reduced nhm and fully coupled nhm respectively and might be related to local convergence issues for the gw svat coupling scheme the root mean squared error fig 11 blue lines and the mean absolute root error fig 11 green lines are lower than or equal to 0 001 m indicating a good match errors greater than Œµ hclose might be caused by the parallel preconditioner for the groundwater model component that differs for each core configuration resulting in different convergence behavior including the couplings furthermore errors greater then Œµ hclose might be the result of rounding errors caused by single precision accuracy of the model component connectors explaining why the fully coupled nhm seems generally slightly less accurate than the reduced nhm model or by the parallel aggregation of budgets for the surface water sub catchments that is non associative regarding floating point arithmetic in the parent worker mechanism see section 2 1 although small differences may occur they are found to be acceptable regarding the dtap software development and therefore we do not find the need to do a more extensive accuracy analysis 6 conclusions we have presented the results of an integrated groundwater parallelization as part of the nhi focusing on the nhm application significant speedups were obtained ranging from two orders of magnitude for the non integrated groundwater model considering a hypothetical test case speedup 108 using 144 processor cores to one order of magnitude for the reduced nhm test case excluding the surface water model components and the unsaturated zone transport model component speedup 22 using 64 processor cores to less than one order of magnitude for the fully coupled nhm speedup 5 using 24 processor cores this clearly shows that coupling more models results in a decrease in speedup a result that is mostly related to our chosen parallelization strategy first we focused on parallelizing the groundwater and svat uzst model components exclusively that are most dominant in computing time and memory usage therefore ignoring run time due to surface water computations second to parallelize the gw svat uzst connector in the current model codes with relative ease we assumed that the groundwater and svat uzst model components share the same partitions resulting in insoluble load imbalance when using many processor cores possible improvements of the current parallelization approach would be to parallelize the surface water components and decouple the groundwater and svat uzst partitions to improve load balancing furthermore our analysis showed that parallel data input can be further fine tuned regarding memory usage we conclude that our parallelization distributes memory sufficiently for the groundwater model component exclusively but not for the nhm where the memory might exceed the total available memory approximately 2 5 gb ram per additional processor core is needed for the nhm since the svat model component requires that a same large database with soil moisture profiles for spatially varying soil physical units is being read into memory by each process this suggests the parallelization could be further refined to account partitioning heterogeneity and reduce overall memory usage another possibility for reducing memory could be the usage of a ram disk where instead of reading the database in memory at initialization all processes read the necessary data dynamically from a virtual storage created by local memory we conclude that for the nhm parallel model results are sufficiently accurate supported by the measured root mean squared errors for a parallel run comparing to the maximum absolute groundwater head change stopping criterion differences greater than the stopping criterion were observed that are likely caused by differences in parallel convergence behavior and rounding errors in the serial and parallel model connectors however for most cells these differences are too small to prohibit use of the parallel model application as a result we believe our parallelization is suitable for national policy analyses and operational management as far as we know this is the first accomplished parallelization and speedup of a large scale integrated hydrological model using modflow our parallelization is open source as part of imod and nhi and ready to use for applications that would benefit from reduced computing time and memory usage we have shown that parallelizing integrated hydrological model codes is more challenging as the number of model codes increase each having their own characteristics and model application as integrated models improve and evolve in time by adding new model components or replacing model components with more sophisticated ones a redesign of the parallelization approach may be needed in worst case scenario this means that many model components proportionally contribute to the run time inevitably requiring huge parallelization efforts to obtain speedup zhang et al 2020 for the fully coupled nhm there are plans to revise the surface water components and to increase the spatial resolution for groundwater since run times for the current surface water model components are relatively small we expect that revising these components will quickly result in surface water run times that are dominant and therefore require parallelization with the current parallelization strategy we expect that increasing the groundwater spatial resolution will result in improved speedup when using more cores since the serial fraction of 6 for the surface water model components will then likely be smaller we also expect that in the future multi core cpus having more and more cores will become more efficient for memory bound problems such as the latest generation amd epyc zen cpus and result in improved speedup and better core utilization for the fully coupled nhm however using more cores efficiently means more memory usage for the svat model component and therefore more urgency to reduce memory usage and to equip newest servers with sufficient memory authorship contribution jv performed the conceptualization methodology parallelization of modflow and metaswap transol as integral part of nhi and imod pre and postprocessing of the test cases simulations and analysis of the results jdh wrote the serial code for pks and assisted on setting up the hypothetical test case pvw assisted on parallelization of the metaswap transol mfpb hxl and ghpoe supervised this research and helped with its conceptualization jv prepared the manuscript with contributions from all authors software availability the model codes that are used in this paper are mostly open source and partly in transition to the open domain as part of imod https oss deltares nl web imod home the nhi model data are freely available at http www nhi nu declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we thank deltares in special timo kroon as project leader of nhi for making this research possible we also thank peter vermeulen the imod code architect for his help on incorporating pks in imod moreover we thank joachim hunink liduin burgering and janneke pouwels for their support in running testing the nhm in parallel on the nhi server furthermore we thank john donners edwin sutanudjaja and martijn russcher for their support on running jobs on cartesius this work was carried out on the dutch national e infrastructure with the support of the surf cooperative appendices a parallelization details a 1 subdomain partitioning to illustrate the grid partitioning consider the example of fig 3 showing an irregular domain consisting of n c 16 columns and n r 14 rows that is partitioned into four partitions p 4 uniform partitioning fig 3a shows an example of a uniform partitioning the blocks are evenly distributed in the row and column direction targeting equally shaped rectangles without accounting for the irregular domain this method aims at minimizing the edge cuts hence the number of connections at the interface between the partitions ignoring the work load that is typically defined by the active cells let n be the minimum of n c and n r from all possible combinations p p c p r where p c and p r are the number of blocks in column and row direction respectively that combination is selected such that p i equals p i max 1 n n c n r p 1 where i c when n n c or i r otherwise assuming equal weight of each cell the load imbalance can be defined as i max l p n where l p is the sum of load for the non overlapping partition p for this example processor 1 clearly has the largest number of cells 42 resulting in a load imbalance of i 1 68 orthogonal recursive bisection partitioning the orthogonal recursive bisection recursively bisects intermediate partitions perpendicular to their longest dimension k 0 times until p 2 k partitions each of approximately the same load are obtained fig 3b shows an example of four partitions k 2 where each active cell both variable as constant value dirichlet cells is assumed to have equal weight the first intermediate partition is determined by applying a minimum bounding box enclosing cells a and b since the longest dimension 15 is along the columns a vertical cut black dashed line is being made by bisecting the column sum of weights 4 6 10 12 10 12 8 11 9 6 5 5 6 5 2 1 with a total sum of 100 resulting in vertical line between column 6 and 7 such that each new intermediate partition has exactly load 50 then two new intermediate partitions are determined enclosing a and c and enclosing d and e both having the row direction as the longest dimension hence rows sum of weights are used to determine the horizontal cuts dotted black lines since in this example all the partitions have the same load 25 the load imbalance is i 1 which is optimal a 2 parallelization of linear solver finite volume discretization of the flow equation results in after picard linearization and eliminating the dirichlet boundary constant value conditions in solving the linear system of equations 1 a h b where h l is the vector of unknown heads a l 2 t 1 a square symmetric positive definite coefficient matrix with the hydraulic cell by cell conductivity and b l 3 t 1 the vector with groundwater sink source and storage terms the corresponding computational stencil is 7 point hence a has 7 bands for solving the linear systems 1 in modflow we use krylov subspace acceleration and apply this preconditioner in the preconditioned conjugate gradient pcg method barrett et al 1995 instead of solving 1 directly the symmetrized preconditioned system 2 m 1 2 a m 1 2 m 1 2 h m 1 2 b m 1 2 m 1 2 m 1 is solved where the matrix m is called the preconditioner barrett et al 1995 golub and van loan 1996 using block wise natural node ordering as illustrated by the positive numbering in fig 3 the matrix a can be written as a block matrix of the form 3 a 1 1 a 1 2 a 1 p a 2 1 a 2 2 a p 1 a p p where a i i correspond to the interior node coefficients and a i j i j to the coupling coefficients between the subdomains considering a 7 point computational stencil and a single band for the uniform partitioning example in fig 3a the block matrix 3 has 4 4 blocks p 4 and for the first subdomain p1 the interior coefficient sub matrix a 1 1 has dimension 37 37 local coupling sub matrix a 1 2 contains two non zero entries 33 38 37 39 and a 1 3 four non zero entries 34 44 35 45 36 46 37 47 and a 1 4 note that in a distributed memory parallel setting the global matrix 3 is never formed explicitly since each processor only has local coefficients corresponding to a block row taking m as the block diagonal matrix of a results in the non overlapping additive schwarz preconditioner dolean et al 2015 smith et al 1996 denoted by m as 4 m as a 1 1 a 2 2 a p p in each pcg iteration called inner iteration the preconditioner is being applied once and the system of the form Œº as y z has to be solved where y and z are denoted as typical search directions this can be done entirely in parallel each processor solves the local subdomain problem a i i y i z i local solve in parallel and inaccurately brakkee et al 1998 in our parallelization the local solve is done using an incomplete lu factorization with zero fill in ilu 0 similar to petsc balay et al 2014 convergence at the i th inner iteration is reached for pcg when the stopping criteria x i x i 1 Œµ hclose and b a x i Œµ rclose are satisfied where the infinity norm is defined as y max i y i the additive schwarz preconditioned pcg algorithm in pseudo code is given by fig 12 parallelization of this method involves a local mpi point to point communication of vectors between subdomains prior to sparse matrix vector multiplication b global collective mpi communication to determine global sums for inner products and global maxima for stopping criteria fig 12 additive schwarz preconditioned conjugate gradient linear solver algorithm for the parallel krylov solver the symbol denotes that the left hand side is assigned to the value of the right hand side according to smith et al 1996 maxinner is the maximum of inner iterations for further notation see fig 2 5 of barrett et al 1995 the numbers denote the mpi communication points fig 12 
25793,worldwide billions of people rely on fresh groundwater reserves for their domestic agricultural and industrial water use extreme droughts and excessive groundwater pumping put pressure on water authorities in maintaining sustainable water usage high resolution integrated models are valuable assets in supporting them the netherlands hydrological instrument nhi provides the dutch water authorities with open source modeling software and data however nhi integrated groundwater models often require long run times and large memory usage therefore strongly limiting their application as a solution we present a distributed memory parallelization focusing on the national hydrological model depending on the level of integration we show that significant speedups can be obtained up to two orders of magnitude as far as we know this is the first reported integrated groundwater parallelization of an operational hydrological model used for national scale integrated water management and policy making the parallel model code and data are freely available keywords parallel computing distributed memory netherlands hydrological instrument groundwater numerical modeling integrated modeling 1 introduction worldwide groundwater reserves being of vital importance for more than 7 billion of people for drinking water agriculture and industry wada et al 2014 are threatened under changing climate conditions and increasing population threats such as extreme droughts and excessive groundwater pumping are putting strains on national and regional water authorities to come up with adequate long term plans for investments and adaptive measures leading to a sustainable and robust water management for the decennia to come the netherlands with a long history in water management huisman 1998 experienced a severe drought in 1976 which led to the development of several nationwide model applications ranging from nested systems based models of the complete water system pulles and sprong 1985 national scale finite element groundwater models kovar et al 1992 and national scale analytical element models de lange 1996 a number of high resolution regional groundwater model applications were developed from 2000 2013 to support groundwater management by water boards and drinking water companies and eventually the model applications covered most of the netherlands under these developments in 2005 the national and regional water authorities joined forces in unifying their modeling software and data together with several national research institutes former alterra now wageningen environmental research former tno bgs and wl delft hydraulics now deltares former mnp now pbl this initiative led to the release of the consensus based national hydrological instrument in 2013 nhi de lange et al 2014 for more than a decade the nhi provides the dutch water authorities and consultancies with a modeling environment used for answering actual water related questions where the continuity of management and maintenance along with new developments of the modeling software and data is secured by the nhi consortium driven by a large amount of available data in the netherlands e g due to the many boreholes van der meulen et al 2013 and the dense surface water network covering the netherlands these models typically have a high spatial 250 m and temporal 1 day resolution and inherently involve many computational cells and timesteps for the national model application of the nhi here briefly referred to as the netherlands hydrological model nhm computational resources required for running this integrated model as a single thread on a single core are significant ignoring surface water flow and transport model component one simulation year takes 9 h computing time 45 gb gigabyte of ram and 30 gb of disc storage this severely limits the practical application of the nhm e g in the dutch national water model prinsen et al 2015 for evaluating future climate change scenarios with proposed adaptation measures which could require simulation time scales up to 100 years this means that assuming constant computing time during simulation 100 years of simulation would roughly take 900 computation hours or 37 5 days and 3 tb terabyte of storage with a single threaded run on a single core such long run times are highly undesirable since typically a large number of simulations are required for model calibration and scenario analysis furthermore such long run times require that servers are stable for long periods of time which in practice is difficult to accomplish distributed memory parallel computing see e g eijkhout et al 2015 r√ºnger and rauber 2013 is a method to significantly reduce computational times and memory typically following a non uniform memory access architecture numa in numa the entire computational grid memory is first partitioned into multiple subdomains and one or more subdomains is assigned distributed to a node each having local main memory ram and one or more multi core cpus processors then the processor cores solve the problem simultaneously while exchanging necessary data between the nodes through a fast interconnection network using the message passing interface mpi forum 1994 in the remainder we refer to a mpi process as the program that uniquely runs on an associated single processor core in this paper we focus on distributed memory parallelization of two of the five hydrological model codes that have been combined in the nhi the model code for saturated groundwater and the model code for soil vegetation water transfers in the unsaturated zone svat see de lange et al 2014 the reason for doing this is that the groundwater and svat model components are most time consuming and memory intensive for groundwater we parallelize the model code modflow harbaugh 2005 the most widely used groundwater flow modeling program in the world developed by the united states geological survey usgs modflow has a large open source community of users and developers from many governments consultancies and research institutes for the svat model component we parallelize the model code metaswap transol van walsum and veldhuizen 2011 metaswap is a fast richards equation emulator that uses a database of steady state soil moisture profiles for soil physical units and transol an emulator of the advection dispersion equation metaswap is implicitly connected to modflow through memory at the outer picard iteration level since parallelization of metaswap transol is done in a relatively straightforward way without requiring communication between processors hence embarrassingly parallel the focus in this paper is primarily on parallelizing modflow parallelization of the nhi surface water hydrological model codes is beyond the scope of this research although this paper focusses on parallelization of nhi model codes used in the national scale coupled nhm all the presented methods and software can be used as standalone applications and used at other spatial domains our parallel implementation successfully went through the four phases of dtap development testing acceptance and production https en wikipedia org wiki development testing acceptance and production and is operational in the national water model prinsen et al 2015 running on eight processor cores as far as we are aware this is the first time that such integrated groundwater parallelization is applied in such a setting furthermore our open source software is readily available to a wide range of hydrogeological modelers at regional water authorities and consultancies 2 methods 2 1 general nhm parallelization strategy here the nhm de lange et al 2014 is defined as five coupled nhi hydrological model components see fig 1 the groundwater gw model component consisting of 7 confined model layers the soil vegetation atmosphere for the transfer of quantitative water in the unsaturated zone svat model component the unsaturated zone salt transport uzst model component the surface water for sub catchments swsc model component and the surface water for optimized distributing swod model component this definition differs from de lange et al 2014 in a way that in our study we exclude the surface water flow and transport model component and include the uzst model component model component details are summarized in table 1 see de lange et al 2014 for a more comprehensive description and details on the coupling connectors in this paper besides the fully coupled nhm or fnhm including all five models components we also consider the reduced nhm of rnhm that only includes the coupled gw and svat model components the reason for doing this is that gw svat models are commonly used as regional application of the nhi by a large number of dutch consortia of provinces water boards drinking water companies and municipalities see e g snepvangers et al 2007 a timing experiment for the fully coupled nhm on the nhi windows server see fig 2 shows that the gw svat and uzst model components are most time consuming and account for 52 16 and 26 of the total simulation run time respectively hence this motivates parallelizing these model components for sake of simplifying coding our parallelization assumes that each vertical column of cells is assigned to the same subdomain including the coupled gw svat uzst cells hence our partitioning of the computational grid is in lateral horizontal direction only this seems a valid assumption since in groundwater models the number of lateral cells is generally much larger than the number of model layers and therefore our approach naturally minimizes the subdomain interface surface area and mpi communication fig 1 illustrates the partitioning of nhi for the case of two subdomains where the left subdomain is assigned to parent process p0 and the right subdomain to worker process p1 in our parallelization we always assume that each subdomain is uniquely assigned to a single processor core corresponding to a single mpi process furthermore the parent process is always responsible for gathering all necessary data from the worker processes and coupling towards the surface water sub catchments except for the off line file based coupling connectors swsc svat and swsc gw parallelization of the other nhm connectors is done in a straightforward manner for swsc svat and swsc gw all processes read and clip the necessary data from the output files of the swsc model component in parallel e g sub catchment river stages for the groundwater model component and groundwater sprinkling for the svat model component for svat swsc and gw swsc each process aggregates all necessary fluxes in parallel for the surface water sub catchments e g drainage discharge from the groundwater model and sends them to the parent process that writes the input file for the swsc model component since subdomain boundaries may divide surface water sub catchments e g see the light blue eastern sub catchment in fig 1 this means that computing and communicating of partial sums is involved parallelization for the svat and uzst model components is straightforward since both models apply a one dimensional discretization in model layer vertical direction and therefore no lateral mpi communication is needed 2 2 parallel performance evaluation a commonly used indicator for measuring parallel performance is speedup s p t 1 t p where t 1 is the serial run time using a single processor core and t p is the parallel run time using p processer cores see e g eijkhout et al 2015 r√ºnger and rauber 2013 in practice evaluating strong scaling means that the problem size e g defined by the number of grid cells is kept fixed while the number of processor cores is increased from a modeling point of view this matches best how users evaluate the parallel performance for their existing models when they have access to multiple processors in our evaluation we measure speedup using actual measured wall clock time instead of cpu time and use the same solver and solver settings for all serial and parallel runs in our study we have not attempted to determine the optimal performing serial solver or to determine the optimal solver settings although for the ideal case s p ideal p in real world applications this is difficult to obtain first from an algorithmic and programming point of view this depends on the portion of work load that could not or has not been parallelized if we denote this serial fraction as f then the well known ahmdahl s law amdahl 1967 states that s p f 1 f p 1 and therefore the asymptotic theoretical speedup s 1 f when assuming unlimited computer resources for the fully coupled nhm model f is estimated 0 06 which only accounts for the non parallelized surface water models fig 2 and 6 swsc swod thus the maximum theoretical speedup is bounded by s 16 7 since other models are also likely to contain serial fractions e g due to solver limitations we should therefore be realistic about our expectations second assuming that the processors used are connected through a fast interconnection network low latency and high bandwidth such as a infiniband interconnect achieving parallel performance is typically hampered by communication overhead in the form of wait time b√∂hme 2013 work load imbalance is defined as i p p max p l p l where p is the total number of cores being used l p is the work load for core p and l the total work load 2 3 parallelization for groundwater model component 2 3 1 relationship to other work the groundwater model component for the nhm is based on the model code modflow harbaugh 2005 a numerical groundwater flow simulation code using control volumes cells for solving the discretized groundwater flow equation this typically results in consecutively solving large and sparse linear systems of equations accounting for most of the simulation run time therefore parallelization of the modflow linear solver has been subject of considerable research see table 2 our distributed memory parallelization has similarity with some of the work from table 2 especially with schreuder 2005 and naff 2008 regarding linear solver preconditioning we also apply the additive schwarz preconditioner for solving the linear system in parallel however there are notable differences compared to those efforts first our approach is fully distributed memory including input and output data second for load balancing models with irregular model boundaries e g due to administrative boundaries or geology we support a robust orthogonal recursive bisection method berger and bokhari 1987 boman et al 2012 fox 1988 to divide partition the groundwater cells into equally loaded blocks given an arbitrary number of processors schreuder 2005 also addressed this problem and developed a partitioning method that iteratively merges cell weighted blocks while shifting subdomain interfaces however this method was concluded not to be robust enough for general purpose third our parallel software only depends on the mpi software library and is therefore relatively easy to compile on multiple platforms this makes our software accessible to users on a wide range of operating systems contrasting with other parallelization efforts that use parallel solver libraries primarily developed for the linux unix operating system and have low to no support for windows machines e g the petsc solver library balay et al 2014 fourth our parallel software is open source and actively maintained as part of imod vermeulen et al 2019 imod is an easy to use graphical user interface for the windows operating system that integrates our accelerated modflow 2005 version with fast subdomain modeling techniques and is extensively used by the nhm user community fifth we add a new modular unstructured parallel solver to modflow 2005 to which we refer to as the parallel krylov solver pks verkaik et al 2016 2015 that is largely based on the upcg linear solvers hughes and white 2013 besides assuring minimal dependency on third party software another reason for developing the pks is the ease in reproducing the stopping criteria of the commonly used pcg solver hill 1990 and the flexibility of adding advanced parallel multi level preconditioners in the near future 2 3 2 general description the main components in our parallelization for groundwater models are a partitioning of grid cells into subdomains blocks b setting up communication between subdomains c reading and writing model input and output files in parallel and d parallelization of the linear solver in this section we highlight the basic concepts of a c referring to appendix a 1 for more details for the technical details on solver parallelization d the reader is referred to appendix a 2 2 3 2 1 subdomain partitioning in general parallelization aims to minimize processor idle times in which a processor does nothing but wait for other processors to finish see e g r√ºnger and rauber 2013 reduced idle times can be obtained by load balancing and minimizing communication overhead between the processors load balancing means that work is equally assigned to the processor cores and in our application directly relates to distributing partitioning cells of the computational grid to minimize communication overhead we partition the horizontal plane as there is only a few vertical model layers that gives the smallest interface surface area between subdomains and hence the amount of data communicated two non overlapping partitioning methods for partitioning the grid in the horizontal plane are considered a straightforward method for obtaining equally sized rectangles here referred to as uniform partitioning and orthogonal recursive bisection orb berger and bokhari 1987 fig 3 illustrates these methods for an example grid having an irregularly shaped model boundary considering four partitions p1 p4 see appendix a 1 for details the red boxes in fig 3a show the partitions obtained by straightforward uniform partitioning clearly showing that the number of active cells vary strongly for each partition the red boxes in fig 3b show the partitions obtained by the orb partitioning here using cell weights of value one as illustration these user defined cells are used within the orb partitioning to balance the partitions targeting equal sum of cell weights for each partition typically these cell weights could be chosen to be equal to the number of model layers it should be noted that our orb partitioning also includes the dirichlet boundary conditions or constant value active cells as in example fig 3 denoted by index 1 since these cells are eliminated in the linear solver this means that a load imbalance may occur in the solving process although the orb partitioning might be optimized for these boundary conditions this was not a subject for this research in our approach the grid is partitioned only once prior to simulation hence corresponding to static load balancing therefore we neglect the spatio temporal variation in computing time that might occur during simulation due to changing boundary conditions causing load imbalance for the nhm this is a reasonable assumption since the number of active cells does not vary in time and the input output data size and frequency remains constant 2 3 2 2 overlap and communication discretization of the groundwater flow equation typically results in evaluating a 7 point computation stencil 5 point in the horizontal plane 3 point in the vertical plane meaning that the unknown head in a cell implicitly depends on the heads of at most six neighboring cells in a parallel setting this means that evaluating the discretization of a cell near the subdomain boundary for instance which is necessary for computing a matrix vector product within a linear iteration data from the adjacent subdomain is required and therefore needs to be communicated to support such local point to point communication the non overlapping partitions are expanded to overlapping partitions see pink boxes for p1 for the example in fig 3 by adding one row of cells so called halo or ghost cells see the dark greens cells for p1 in fig 3 for example in fig 3a with the uniform partitioning processor p1 needs to communicate with p2 and p3 and each processor has exactly two neighbors however as can be seen in fig 3b for the orb partitioning p2 has three neighbors p1 p3 and p4 and hence there is an additional interface between p2 and p3 this additional communication for orb is the trade off for obtaining optimal load balance for our application this does not seem to be an issue since the amount of data communicated remains low and local communication overhead is secondary compared to global communication overhead i e communication involving all processors requiring synchronization and load imbalance the large benefit of using overlapping partitions is that no additional data e g inter cell transmissivity need to be explicitly specified by the user or communicated for evaluating the discretization scheme at the subdomain interfaces moreover this physical overlap facilitates the usage of advanced computational schemes with relative ease such as for applying full tensor anisotropy that requires the evaluation of cross terms in our approach halo cells are treated in a similar way as computational cells regarding input output and matrix assembly but are different during computation each processor is responsible for updating computing groundwater heads for its non overlapping subdomain and halo cells are used to store data received copies from adjacent processors due to the symmetry of the subdomain overlap local communication is two sided meaning that in addition to receiving data each processor sends data to the neighbor processor see for the example in fig 3 the light green cells 2 3 2 3 input and output our parallelization supports independent parallel input output where each process reads its subdomain data from files that are defined for the entire computational domain and writes its subdomain results to separate files parallel input is done from files supporting raster data point data and line data for reading raster data we use the binary geo referenced imod data format idf imod python development team 2017 vermeulen et al 2019 since this file format supports fast unformatted binary direct access read and can be easily visualized with the imod graphical user interface idf files allow us to efficiently read subdomain data in parallel while keeping memory usage locally besides pumping well and geological fault data that are read as point and line data respectively all static module and dynamic package data e g for rivers and drains are read from idf raster files this means that a significant amount of redundant no value data might be read for sparse raster files e g for modeling drainage systems in semi arid areas since in the netherlands the surface water network is dense the nhm raster data is also dense and therefore the expected redundancy of using idfs is low parallel output is straightforward where each process writes its separate idf files or standard modflow ascii binary files for its non overlapping partition post processing these subdomain results might require additional tools such as imod for merging these data into a single dataset for the total computational grid 3 test cases to evaluate the parallel performance we consider three test cases with increasing complexity a hypothetical steady state regional scale groundwater model the reduced nhm excluding modeling of salt transport in the unsaturated zone and dynamic surface water simulation for 2006 the fully coupled nhm with the same initial set up as the reduced nhm 3 1 hypothetical steady state groundwater model this hypothetical test case simulates steady state regional scale groundwater flow in a heterogeneous aquifer for a square computational domain applying uniform partitioning see fig 4 for 12 12 144 subdomains the model area is 1000 m 1000 m and two model layers are used from 10 m to 15 m and 15 m to 30 m respectively each having 8000 cells in both x and y direction hence the model has 128 million active cells each having a resolution of 0 125 m 0 125 m a hydraulic gradient of 0 01 m m is specified in the west to east direction no flow boundary conditions are specified along the north and south edges of the model and four equal pumping wells are located in the center of the domain withdrawing a total of 1000 m 3 d from the lowest model layer this test case has great similarity with the problem considered by hughes and white 2013 our model uses the same simulated multivariate gaussian 10log hydraulic conductivity field for both model layers with an average of 4 81 m d a 10log variance of 1 23 and an effective range of 750 m and with the same top and bottom of the aquifer and boundary conditions since we have more cells in the x and y direction the hydraulic conductivity of hughes and white 2013 is downscaled using nearest neighbor interpolation our model only considers a single discretization and we assume that the transmissivity of the aquifer is constant confined option which makes the problem linear a linear model gives better insight into the parallel solver performance and is consistent with the assumptions applied in the nhm model convergence with the pks is obtained for the test case where the stopping criteria are Œµ hclose 0 001 m maximum absolute groundwater head difference and Œµ rclose 1 5625 10 5 m 3 d cell size squared times Œµ hclose the simulation starts with initial heads of 0 0 m across the entire domain see appendix a 2 for more details since the domain is square and all cells are active the uniform partitioning is used and results in optimal load balance where the number blocks of in x direction column direction is always equal to the number of blocks in y direction row direction 3 2 reduced and fully coupled nhm simulating 2006 for both test cases nhm v3 1 is taken hoogewoud et al 2015 see www nhi nu the default groundwater solver settings are used corresponding to Œµ hclose 0 001 m Œµ rclose 100 m 3 d and a maximum of 30 inner iterations for the pks see maxinner in fig 12 of appendix a 2 the simulation period is 2006 for both the nhm test cases we apply orb partitioning since the model boundary is irregular see fig 5 for the example of 24 subdomains for obtaining a reasonably well load balance for the combined groundwater and svat cells a trial and error method is used to obtain the orb cell weights see section 2 3 2 starting from a uniform cell weight of one for both the groundwater and svat cells the cell weights are obtained by simply increasing the weights for the svats cells from 1 to 10 timing results show that the value of 5 seems to give the overall lowest computing times 3 3 hardware and compiler the parallel performance is evaluated on the dutch national supercomputer cartesius surfsara 2014 for the hypothetical test case and the reduced nhm test case and on the nhi server for the fully coupled nhm cartesius consists of 1900 computing nodes running on linux that are tightly connected using a fast interconnection network for a total of 48 thousand intel xeon cores and 128 terabytes of memory all scaling experiments on the cartesius are carried out on so called thin nodes where each node consists of two haswell 12 core cpus e5 2690 v3 with a total of 64 gb memory the single node nhi server runs on microsoft windows and consists of two intel haswell 16 core cpus e5 2698 v3 with a total of 128 gb memory at the time of evaluating the test cases the nhi server is a dedicated resource for running nhm simulations that are used for national scale long term planning the reason for not evaluating the fully coupled nhm test case on cartesius is that the surface water for sub catchments model code mozart and surface water for the surface water for optimized distributing model code dm are not supported to run on linux we compiled the coupled modflow metaswap transol model code as part of imod v4 0 using the intel fortran compiler v15 0 with high level o3 optimization on cartesius the compiled code was linked with the intel mpi library v5 0 update 3 and for running on the nhi server the code was linked with the mpich library v1 4 1 on cartesius we use a maximum of four cores per node hence two cores per cpu based on trial and error testing that has indicated that run times are shortest using this number of cores using a maximum of four cores per node results in 20 idle cores during computation and a relatively low core utilization of 17 the reason why using more cores in our trial and error analyis results in higher run times can likely be explained by large memory requirements for our model applications and the competition of processor cores within a multi core cpu for the main memory tudor et al 2011 since we expect this is a hardware related issue inherent to multi core architectures this issue is recommended for future research and left outside the scope of this study 4 results 4 1 hypothetical steady state groundwater model fig 6 shows the measured speedups a and total memory usage b for the hypothetical test case see section 3 1 for our strong scaling experiments on cartesius up to 144 processor cores the serial run requires 4 h and 48 min computing time to converge and 45 gb main memory the computing time is reduced to 2 min and 40 s using 144 cores 36 nodes resulting in a speed up of 108 the absolute groundwater head difference is less than the specified Œµ hclose 0 001 m in each cell in the serial and parallel simulations 4 2 reduced and fully coupled nhm simulating 2006 fig 7 shows the measured speedups a and total memory usage b for the reduced nhm test case on cartesius and the fully coupled nhm test case on the nhi server up to a maximum of 64 and 24 cores respectively maximum speedups of 21 6 and 4 6 are obtained for the nhm test cases respectively besides the ideal linear speedup fig 7a also shows the maximum speedup for the fully coupled nhm according to amdahl s law when accounting for 6 serial surface water computation 10 for 24 cores see section 2 2 regarding accuracy transient results for the serial and 24 core parallel simulations are evaluated the root mean squared error values for the entire period of 2006 and considering all model layers are 3 0 10 4 m and 1 1 10 3 m for the reduced nhm test case and the fully coupled nhm test case respectively for the reduced nhm the maximum absolute head difference greater than 0 001 m and 0 01 m is exceeded for 3 and 0 1 of the total number of cells during simulation respectively for the fully coupled nhm these values are 7 and 0 4 respectively 5 discussion figs 6a and 7a show that significant speedups are obtained ranging from two orders of magnitude for groundwater only modflow hypothetical test case one order of magnitude for the reduced nhm excluding dynamic surface water and unsaturated zone transport and less than one order of magnitude for the fully coupled nhm however the speedup curves flatten as the number of cores increases this can be explained by hardware related issues non scalable algorithms methods and non scalable components in implementation regarding hardware the memory competition issue see section 3 3 is likely to contribute to the flattening of the fully coupled nhm speedups since a maximum of 12 out of 16 cores per cpu is used compared to 4 out of 12 cores per cpu for the reduced nhm however due to the lack of scheduling control options on the nhi windows server we are not able to quantify this effect concerning non scalable algorithms a more important explanation for the flattening of the fully coupled nhm speedups can be found in the non parallelized surface water model components which account for 6 of the total run time using amdahl s law see section 2 2 and a serial fraction of 6 a significant flattening of the nhm speedup is expected see fig 7a where the maximum theoretical speedup using 24 cores is 10 and the measured speedup is 4 6 furthermore parallel linear solver iterations might increase with the number of subdomains as a result of low frequency eigen modes that can hamper the linear solver convergence dolean et al 2015 smith et al 1996 which can require an additional multi level preconditioner to improve convergence for our test cases however the maximum observed linear iteration increase is 15 see fig 8 and suggests that low frequency eigen modes have a relatively limited effect for that reason we did not find any need to apply such preconditioner with respect to the non scalable components the run time behavior is analyzed by cost analysis for the hypothetical test case and the reduced nhm test case on cartesius using the scalasca profiling and tracing tool geimer et al 2010 fig 9 shows the most significant cost components where c p c denotes the cost of component c for using p cores defined as the cumulative sum of time spent on c accounting for all processor cores the total cost c p of a parallel program is defined as c p p t p and perfect scalability or cost optimality is obtained when this cost remains constant for increasing number of cores hence in the ideal case for each component the relative value c p c c 1 c p c t 1 as shown in figs 9a and c should remain constant the ratio c p c c p as shown in figs 9b and d expresses the contribution of a component to the total parallel run time for the hypothetical test case fig 9a gws pks shows that majority of the run time is spent in the linear solver and this component has near perfect scalability however load imbalance is manifested in global mpi communication wait times mpi global in fig 9a since the subdomain partitioning is uniform we suspect that this imbalance is caused by the physical overlap of one row whereas we do not account for this overlap in the load balancing see section 2 3 2 for the reduced nhm test case scalability of the pks for the groundwater model component and computations for the svat model component are also nearly perfect see fig 9c gw pks and svat respectively however a strong load imbalance is observed where 45 of the run time is spent on waiting when using 64 cores mpi global in fig 9d this load imbalance is very likely related to the groundwater model component and svat model components sharing the same partition for groundwater active cells exist across the dutch land border in model layers 2 to 7 whereas svat cells exist only within the dutch land border using more subdomains enhances this discrepancy near the border and the orb partitioning becomes less effective this is illustrated in fig 10 for the example with 48 subdomains considering different orb cell weights in fig 10a the same cell weights are used as for our nhm test cases see fig 5 where in fig 10b cell weights are used that are equal to the number of active groundwater model layers tracing analysis shows that subdomains p36 and p42 are responsible for most of the delay which is mainly caused by global mpi communication groundwater cells for subdomains p36 and p42 are not connected to any svat cells fig 10b this results in a significant load imbalance for the svat model component although for groundwater load is well balanced and a total delay time of 75 relative to the total parallel cost however as can be seen in fig 10a by using different cell weights subdomains p36 and p42 now have connections to svats and by this we improve the svat model component load on the other hand we introduce a load imbalance for the groundwater model component however this has an overall positive effect on the total delay time that is reduced to 36 relative to the total parallel cost this illustrates that orb partitioning for the nhm is complicated by the coupled svat gw models a better approach for load balancing might be to decouple the groundwater and svat partitions however this would require a significant programming effort and is therefore beyond the scope of this current research other components that contribute to the flattening of the speedup curved for the reduced nhm fig 7a are related to input data reading see fig 9c the component gw init read is related to redundant file information required by the operating system the component svat read meteo is related to the non scalable ascii reading of 1 km precipitation and evaporation input grids and the component svat read database related to reading the database with steady states of soil moisture profiles for the 72 soil physical units since the svat model component is a metamodel that strongly relies on this pre compiled database see van walsum and veldhuizen 2011 in a worst case scenario when all soil physical units are entirely heterogeneous this means that each processor needs to read the entire database and keep all data in memory regarding ram memory usage the svat database significantly increases memory usage by 2 5 gb for the nhm test cases fig 7b for each processor core added in practice this means that sufficient memory should be available however for the reduced nhm test case on cartesius the 64 gb ram per node does not put any constraint on the processor core usage on the other hand for the fully coupled nhm test case on the nhi server fig 7b the memory increase together with the large memory usage of 40 gb for the unsaturated zone salt transport model component limits the core usage to not exceed the maximum of 128 gb ram together with the increase of read time this would advocate minimizing the number of soil physical units per subdomain as part of future research on the other hand the groundwater model seems to satisfy the distributed memory approach as illustrated in the hypothetical test case fig 6b although a slight memory increase is observed one reason for this increase is the inaccurate memory measurement on cartesius where we might overestimate the total memory usage by simply multiplying the measured peak amount of memory during simulation maxrss maximum resident set size with the number of processes another reason might be that the physical overlap of partitions introduces a slight memory increase for increasing number of processes regarding accuracy small absolute groundwater head differences between the serial and 24 core parallel nhm simulations see fig 11 red lines were observed that are larger than the linear solver head change stopping criterion of Œµ hclose 0 001 m the largest difference occurred in a small number of cells with 99 9 and 99 6 of the cells having mean absolute differences between 0 001 m 0 01 m in the reduced nhm and fully coupled nhm respectively errors greater than 1 m were observed during summer 2006 are occur in only 5 cells 0 08 and 10 cells 0 16 in the reduced nhm and fully coupled nhm respectively and might be related to local convergence issues for the gw svat coupling scheme the root mean squared error fig 11 blue lines and the mean absolute root error fig 11 green lines are lower than or equal to 0 001 m indicating a good match errors greater than Œµ hclose might be caused by the parallel preconditioner for the groundwater model component that differs for each core configuration resulting in different convergence behavior including the couplings furthermore errors greater then Œµ hclose might be the result of rounding errors caused by single precision accuracy of the model component connectors explaining why the fully coupled nhm seems generally slightly less accurate than the reduced nhm model or by the parallel aggregation of budgets for the surface water sub catchments that is non associative regarding floating point arithmetic in the parent worker mechanism see section 2 1 although small differences may occur they are found to be acceptable regarding the dtap software development and therefore we do not find the need to do a more extensive accuracy analysis 6 conclusions we have presented the results of an integrated groundwater parallelization as part of the nhi focusing on the nhm application significant speedups were obtained ranging from two orders of magnitude for the non integrated groundwater model considering a hypothetical test case speedup 108 using 144 processor cores to one order of magnitude for the reduced nhm test case excluding the surface water model components and the unsaturated zone transport model component speedup 22 using 64 processor cores to less than one order of magnitude for the fully coupled nhm speedup 5 using 24 processor cores this clearly shows that coupling more models results in a decrease in speedup a result that is mostly related to our chosen parallelization strategy first we focused on parallelizing the groundwater and svat uzst model components exclusively that are most dominant in computing time and memory usage therefore ignoring run time due to surface water computations second to parallelize the gw svat uzst connector in the current model codes with relative ease we assumed that the groundwater and svat uzst model components share the same partitions resulting in insoluble load imbalance when using many processor cores possible improvements of the current parallelization approach would be to parallelize the surface water components and decouple the groundwater and svat uzst partitions to improve load balancing furthermore our analysis showed that parallel data input can be further fine tuned regarding memory usage we conclude that our parallelization distributes memory sufficiently for the groundwater model component exclusively but not for the nhm where the memory might exceed the total available memory approximately 2 5 gb ram per additional processor core is needed for the nhm since the svat model component requires that a same large database with soil moisture profiles for spatially varying soil physical units is being read into memory by each process this suggests the parallelization could be further refined to account partitioning heterogeneity and reduce overall memory usage another possibility for reducing memory could be the usage of a ram disk where instead of reading the database in memory at initialization all processes read the necessary data dynamically from a virtual storage created by local memory we conclude that for the nhm parallel model results are sufficiently accurate supported by the measured root mean squared errors for a parallel run comparing to the maximum absolute groundwater head change stopping criterion differences greater than the stopping criterion were observed that are likely caused by differences in parallel convergence behavior and rounding errors in the serial and parallel model connectors however for most cells these differences are too small to prohibit use of the parallel model application as a result we believe our parallelization is suitable for national policy analyses and operational management as far as we know this is the first accomplished parallelization and speedup of a large scale integrated hydrological model using modflow our parallelization is open source as part of imod and nhi and ready to use for applications that would benefit from reduced computing time and memory usage we have shown that parallelizing integrated hydrological model codes is more challenging as the number of model codes increase each having their own characteristics and model application as integrated models improve and evolve in time by adding new model components or replacing model components with more sophisticated ones a redesign of the parallelization approach may be needed in worst case scenario this means that many model components proportionally contribute to the run time inevitably requiring huge parallelization efforts to obtain speedup zhang et al 2020 for the fully coupled nhm there are plans to revise the surface water components and to increase the spatial resolution for groundwater since run times for the current surface water model components are relatively small we expect that revising these components will quickly result in surface water run times that are dominant and therefore require parallelization with the current parallelization strategy we expect that increasing the groundwater spatial resolution will result in improved speedup when using more cores since the serial fraction of 6 for the surface water model components will then likely be smaller we also expect that in the future multi core cpus having more and more cores will become more efficient for memory bound problems such as the latest generation amd epyc zen cpus and result in improved speedup and better core utilization for the fully coupled nhm however using more cores efficiently means more memory usage for the svat model component and therefore more urgency to reduce memory usage and to equip newest servers with sufficient memory authorship contribution jv performed the conceptualization methodology parallelization of modflow and metaswap transol as integral part of nhi and imod pre and postprocessing of the test cases simulations and analysis of the results jdh wrote the serial code for pks and assisted on setting up the hypothetical test case pvw assisted on parallelization of the metaswap transol mfpb hxl and ghpoe supervised this research and helped with its conceptualization jv prepared the manuscript with contributions from all authors software availability the model codes that are used in this paper are mostly open source and partly in transition to the open domain as part of imod https oss deltares nl web imod home the nhi model data are freely available at http www nhi nu declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we thank deltares in special timo kroon as project leader of nhi for making this research possible we also thank peter vermeulen the imod code architect for his help on incorporating pks in imod moreover we thank joachim hunink liduin burgering and janneke pouwels for their support in running testing the nhm in parallel on the nhi server furthermore we thank john donners edwin sutanudjaja and martijn russcher for their support on running jobs on cartesius this work was carried out on the dutch national e infrastructure with the support of the surf cooperative appendices a parallelization details a 1 subdomain partitioning to illustrate the grid partitioning consider the example of fig 3 showing an irregular domain consisting of n c 16 columns and n r 14 rows that is partitioned into four partitions p 4 uniform partitioning fig 3a shows an example of a uniform partitioning the blocks are evenly distributed in the row and column direction targeting equally shaped rectangles without accounting for the irregular domain this method aims at minimizing the edge cuts hence the number of connections at the interface between the partitions ignoring the work load that is typically defined by the active cells let n be the minimum of n c and n r from all possible combinations p p c p r where p c and p r are the number of blocks in column and row direction respectively that combination is selected such that p i equals p i max 1 n n c n r p 1 where i c when n n c or i r otherwise assuming equal weight of each cell the load imbalance can be defined as i max l p n where l p is the sum of load for the non overlapping partition p for this example processor 1 clearly has the largest number of cells 42 resulting in a load imbalance of i 1 68 orthogonal recursive bisection partitioning the orthogonal recursive bisection recursively bisects intermediate partitions perpendicular to their longest dimension k 0 times until p 2 k partitions each of approximately the same load are obtained fig 3b shows an example of four partitions k 2 where each active cell both variable as constant value dirichlet cells is assumed to have equal weight the first intermediate partition is determined by applying a minimum bounding box enclosing cells a and b since the longest dimension 15 is along the columns a vertical cut black dashed line is being made by bisecting the column sum of weights 4 6 10 12 10 12 8 11 9 6 5 5 6 5 2 1 with a total sum of 100 resulting in vertical line between column 6 and 7 such that each new intermediate partition has exactly load 50 then two new intermediate partitions are determined enclosing a and c and enclosing d and e both having the row direction as the longest dimension hence rows sum of weights are used to determine the horizontal cuts dotted black lines since in this example all the partitions have the same load 25 the load imbalance is i 1 which is optimal a 2 parallelization of linear solver finite volume discretization of the flow equation results in after picard linearization and eliminating the dirichlet boundary constant value conditions in solving the linear system of equations 1 a h b where h l is the vector of unknown heads a l 2 t 1 a square symmetric positive definite coefficient matrix with the hydraulic cell by cell conductivity and b l 3 t 1 the vector with groundwater sink source and storage terms the corresponding computational stencil is 7 point hence a has 7 bands for solving the linear systems 1 in modflow we use krylov subspace acceleration and apply this preconditioner in the preconditioned conjugate gradient pcg method barrett et al 1995 instead of solving 1 directly the symmetrized preconditioned system 2 m 1 2 a m 1 2 m 1 2 h m 1 2 b m 1 2 m 1 2 m 1 is solved where the matrix m is called the preconditioner barrett et al 1995 golub and van loan 1996 using block wise natural node ordering as illustrated by the positive numbering in fig 3 the matrix a can be written as a block matrix of the form 3 a 1 1 a 1 2 a 1 p a 2 1 a 2 2 a p 1 a p p where a i i correspond to the interior node coefficients and a i j i j to the coupling coefficients between the subdomains considering a 7 point computational stencil and a single band for the uniform partitioning example in fig 3a the block matrix 3 has 4 4 blocks p 4 and for the first subdomain p1 the interior coefficient sub matrix a 1 1 has dimension 37 37 local coupling sub matrix a 1 2 contains two non zero entries 33 38 37 39 and a 1 3 four non zero entries 34 44 35 45 36 46 37 47 and a 1 4 note that in a distributed memory parallel setting the global matrix 3 is never formed explicitly since each processor only has local coefficients corresponding to a block row taking m as the block diagonal matrix of a results in the non overlapping additive schwarz preconditioner dolean et al 2015 smith et al 1996 denoted by m as 4 m as a 1 1 a 2 2 a p p in each pcg iteration called inner iteration the preconditioner is being applied once and the system of the form Œº as y z has to be solved where y and z are denoted as typical search directions this can be done entirely in parallel each processor solves the local subdomain problem a i i y i z i local solve in parallel and inaccurately brakkee et al 1998 in our parallelization the local solve is done using an incomplete lu factorization with zero fill in ilu 0 similar to petsc balay et al 2014 convergence at the i th inner iteration is reached for pcg when the stopping criteria x i x i 1 Œµ hclose and b a x i Œµ rclose are satisfied where the infinity norm is defined as y max i y i the additive schwarz preconditioned pcg algorithm in pseudo code is given by fig 12 parallelization of this method involves a local mpi point to point communication of vectors between subdomains prior to sparse matrix vector multiplication b global collective mpi communication to determine global sums for inner products and global maxima for stopping criteria fig 12 additive schwarz preconditioned conjugate gradient linear solver algorithm for the parallel krylov solver the symbol denotes that the left hand side is assigned to the value of the right hand side according to smith et al 1996 maxinner is the maximum of inner iterations for further notation see fig 2 5 of barrett et al 1995 the numbers denote the mpi communication points fig 12 
25794,traditional approaches to inundation modelling are computationally intensive and thus not well suited to assessing the uncertainty involved in estimating flood inundation surfaces for planning design and forecasting purposes in this study a rapid flood inundation modelling framework is developed consisting of a novel spatial reduction and reconstruction srr approach and a deep learning dl modelling component the srr approach is developed to reduce computational cost by identifying representative locations of inundation surfaces where water levels are simulated using dl models and to efficiently reconstruct inundation surfaces based on simulated water level information the dl model includes a built in input selection layer to simplify the model development process and a long short term memory layer for time series modelling the accuracy and efficiency of the srr dl framework is assessed by application to a real world river system where the inundation of over 3 million grid cells can be simulated in 4 s keywords flood inundation modelling deep learning long short term memory spatial reduction 1 introduction flood inundation models are commonly used for assessing flood hazard in planning design and forecasting research in recent years has demonstrated an increasing interest in probabilistic flood inundation estimation using ensembles of inputs to account for uncertainty cooten et al 2011 georgas et al 2016 gomez et al 2019 wu et al 2020 this is a challenging task due to the high computational requirements of existing flood inundation models as a result fast flood inundation models are needed to simulate accurate flood inundation extent and depth at high spatial resolutions with small timesteps gomez et al 2019 teng et al 2017 commonly used flood inundation models can be divided into three major categories including empirical models hydrodynamic models and simplified conceptual models teng et al 2017 among these models 2 dimensional 2d hydrodynamic flood inundation models are generally considered accurate and therefore commonly used for engineering applications n√©elz and pender 2010 however they are computationally expensive and therefore not suitable for applications where a large number of model runs are required kalyanapu et al 2011 though simplified conceptual models are generally more computationally efficient they are not well suited to capturing the temporal dynamics of floods lhomme et al 2008 teng et al 2015 zheng 2018 to reduce the computational cost while maintaining desired accuracy data driven emulation models mostly artificial neural networks anns have also been developed for flood inundation modelling chang et al 2018 chu et al 2019 xie et al 2020 one drawback of traditional anns such as multilayer perceptron models is that they cannot take advantage of serial correlation that is present in the input time series data used to specify flow and water level boundary conditions chu et al 2019 kratzert et al 2018 this adds unnecessary computational complexity and uncertainty to the simulations which could be avoided if explicit account is given to the serial correlation contained in time series input data hsu et al 1997 kratzert et al 2018 kumar et al 2004 in addition current implementations of anns for simulating flood inundation typically involve developing models to simulate flood depth or water level at each grid cell within the modelling domain this often results in either a large number of ann models being developed or a very complex ann model which increases the difficulty of model development for example chu et al 2019 and xie et al 2020 developed as many as 19 448 ann models to model flood depth for an area of 7 8 km2 kabir et al 2020 modelled flood inundation depth for 581 061 grid cells using a single convolutional ann which has over 297 million parameters such approaches also present difficulties when applied to data sparse regions where there are very limited flood inundation data for model development xie et al 2020 since flood inundation models simulate continuous water surfaces water levels at one grid cell are spatially correlated to those in its neighbouring cells current ann based modelling approaches ignore the redundancy in flood inundation information associated with this spatial correlation leaving room for potential improvement of modelling efficiency in this study a new flood inundation modelling framework is developed to address the two key issues listed above first deep learning dl models are used to accommodate the serial correlation embedded in the multivariate time series inputs for flood inundation modelling among different dl models recurrent neural networks rnns were designed to handle sequential inputs and have been found to be effective for time series modelling connor et al 1994 hsu et al 1997 kumar et al 2004 among different rnns the long short term memory lstm architecture has been increasingly applied to discharge modelling in recent years as lstm has been found to out perform the traditional rnns for time series modelling in a number of studies gers et al 1999 kratzert et al 2018 sahoo et al 2019 therefore the lstm architecture is adopted for this study in addition to the traditional lstm layer a fully connected input selection layer is included in the lstm model architecture consequently input selection is incorporated into the model calibration process rather than being a separate step of the model development process and this significantly simplifies the model development process second in order to reduce the information redundancy in the current one model per grid approach used in traditional ann based flood inundation modelling approaches a spatial reduction and reconstruction srr method is developed to characterise the flood inundation surface in a more parsimonious manner this srr method allows the whole inundation surface across the model domain to be represented by water levels at selected representative locations this significantly reduces the number of models that need to be developed to represent the flood inundation surface thus reducing the computational cost and the number of parameters involved in the simulation the representative locations are selected to make the best use of flood information in data rich regions as this has been found to improve the accuracy of the model simulations xie et al 2020 finally the accuracy and efficiency of this framework which hereafter is referred to as srr dl are demonstrated via a real world case study in queensland australia the paper is structured as follows a detailed description of the srr dl framework is provided in section 2 including the srr method and the design of the dl model architecture then the application of the framework is presented in section 3 including the introduction of the case study and detailed modelling process applied section 4 includes results obtained and discussion of main findings and this is followed by conclusions in section 5 2 methodology the srr dl framework consists of two major components 1 the srr method is developed and used to select representative locations rls within the modelling domain that can be used to estimate flood inundation behaviour over the whole simulation period based on water levels simulated at these locations 2 dl models are used to simulate water levels at each of the selected representative locations based on the time series of model inputs e g mainstream and tributary inflows and ocean water levels influencing the lower model boundary the overall framework is illustrated in fig 1 the details of the two major components of the framework are introduced below 2 1 spatial reduction and reconstruction method for flood inundation modelling the first major component of the srr dl framework for flood inundation modelling is the spatial reduction and reconstruction srr method as shown in fig 1 there are two modules involved in this component namely srr rl srr representative location and srr reco srr reconstruction by finding locations where the water levels are representative of flood behaviour over the whole model domain the srr rl module aims to 1 reduce the number of locations where dl modelling is required and 2 focus dl modelling in data rich regions where there are sufficient data for model development the srr reco module is then used to reconstruct flood inundation surfaces based on water levels simulated at these representative locations the srr method is written in the python programming language the code is freely available online at https github com yuerongz srr method the details of the srr method are included in the methodsx article related to this paper a brief description of the srr method including its two modules is presented in the following two sub sections 2 1 1 representative location selection with srr rl the function of the srr rl module in the srr method is to identify the rls that best capture inundation behaviour over the whole modelling domain the rls are selected according to the following two criteria 1 the rls need to cover regions where the water level varies rapidly through time and 2 there need to be sufficient water level data at these locations two types of rls are selected to represent the different inundation characteristics along mainstream and lateral flow paths the mainstream rls i e rls cl are selected along the centroid line of the main river channel at intervals which allow the path of the channel thalweg to be represented by a series of straight lines as a result the more meandering a river channel is the more rls cl will be selected the lateral rls i e rls side are selected along both sides of the mainstream channel at locations where flood water exits or enters river channel a python function named srr search is developed as part of the srr rl module to identify both the mainstream and lateral flow path locations see the associated methodsx paper 2 1 2 flood surface reconstruction with srr reco after the rls are selected dl models are used to simulate water levels at these rls the dl models are discussed in section 2 2 below the srr reco module is developed to convert the simulated water level information into flood inundation surfaces srr reco is a simple and fast method for inundation surface reconstruction it applies a 2d linear interpolation method barber et al 1996 to reconstruct the inundation water surface based on water levels simulated at rls and at model boundaries where water levels are specified as an input boundary condition the interpolated water surfaces are then compared to the digital elevation model dem of the area so the water surfaces can be adjusted to take account of those areas where the dem is higher than the interpolated water surface elevation the resolution of the reconstructed inundation surfaces is determined by the resolution of the underlying dem and thus there is no restriction in the output resolution provided by srr reco details of the srr reco module are included in the associated methodsx paper 2 2 deep learning model 2 2 1 model architecture in the srr dl framework one popular deep learning model the long short term memory recurrent neural networks model lstm is used to model water levels at rls lstm models are well suited to simulating non linear relationships between inputs and outputs liu et al 2017 schmidhuber 2015 and as mentioned previously they are efficient at handling time series inputs gers et al 1999 kratzert et al 2018 sahoo et al 2019 to simplify the overall model development process and improve input selection a built in input selection layer is included in the lstm model to incorporate input selection into the model calibration process details of the built in input selection layer are presented in section 2 2 2 below the proposed lstm model architecture including the built in input selection layer is shown in fig 2 the first layer of the proposed lstm model architecture is an input layer which takes the multivariate time series inputs such as inflows at various locations along the river and water levels of downstream boundary conditions each variable in the input is represented as one neuron in the input layer which means each neuron in the input layer is a time series vector the second layer in the architecture is the built in input selection biis layer which is a fully connected layer designed to select inputs from all input variables considered the input and biis layers are connected through the weighted sum and rectified linear unit relu functions the third layer is the lstm layer which takes the information of selected input variables from the biis layer and converts the multivariate time series sequences to a data vector using the standard lstm memory cells gers et al 1999 kratzert et al 2018 the output from the lstm layer is transformed using a tanh function before being passed to the final output layer as shown in fig 2 the output layer in the proposed model architecture takes a weighted sum of the received data from the previous layer and converts them into the final outputs the water levels at specific times in order to produce time series outputs the input time series are successively shifted forward by one timestep and input to the lstm model to provide a time series of water level outputs the basic concepts and equations of the model are included in section a of the supplementary material 2 2 2 build in input selection the build in input selection biis layer in the proposed lstm model architecture is designed to simplify the model development process by including the traditionally separate input selection step into the model calibration process this way the appropriate input variables can be selected at the same time as the model parameter values are calibrated avoiding the potential impact of predetermined inputs on model calibration furthermore by eliminating the additional separate input selection step the different lstm models at different rls across the model domain can all take the same set of inputs this not only simplifies model development process but also reduces the effort required to prepare input data for real world applications the selection of input variables is determined by the values of the weights connecting the input layer and the biis layer the larger the absolute value of a weight the greater the level of information that is passed from the input variable to the lstm layer and thus the greater the impact of the input variable on the model outputs because the selection of inputs is hidden in the parameter values of the trained models the interpretation of selected inputs needs to be done through connection weight analysis a step that has been used previously to interpreted ann models andrews et al 1995 gevrey et al 2003 kingston et al 2005 2006 solomatine et al 2009 wilby et al 2003 in this study a simple indexing method is developed to quantify the importance of each input variable fed into the biis layer and therefore this indexing method can also be used to evaluate the selected input variables in trained models the indexing method makes use of the information inheritance rate ifr which represents the level of influence that an input variable has on the output the ifr for each variable can be calculated using the following equation 1 i f r v a r i a b l e j k 1 n b i i s a b s w k j where n b i i s is the number of neurons in the biis layer and a b s w k j is the absolute value of the weight from the connection between variable j and the kth neuron in the biis layer variables that have a greater influence on model results are associated with higher ifr indices and vice versa another important function of the biis layer is that it can reduce the total number of parameters that need to be trained in the lmst model lmst model have often been criticised for having a large number of parameters and therefore are prone to overfitting srivastava et al 2014 the addition of the biis layer in the lstm model can reduce the number of neurons connected to the lstm layer which includes the majority of the model parameters and hence can reduce the total number of model parameters while maintaining the flexibility of the model in fitting all important features in input data a detailed explanation on how the biis can reduce the total number of parameters in the lmst model is included in section b of the supplementary material 3 application 3 1 study area and hydrodynamic model available the application of the srr dl framework to flood inundation modelling is demonstrated using the burnett river system downstream of paradise dam in queensland australia fig 3 shows the location and model domain of the system the river reach in the study area is approximately 145 km long and ends at the coral sea at burnett heads a 2d hydrodynamic model was developed for the system using the tuflow modelling suite huxley and syme 2016 the modelling area extents from the paradise dam to the discharge point at burnett heads the 2d model domain contains 3 697 597 20 m by 20 m active model grid cells covering over 1479 km2 area the tuflow model has in total 15 inputs including the main inflow into the system released from paradise dam 13 concurrent tributary inflows at various locations along the mainstream and the lower boundary water levels at burnett heads all the input data are available at 15 min intervals 3 2 flood event datasets the dataset used in this study consists of 74 flood events simulated using the tuflow model these flood events include three historical events and 71 design events for the 71 design events the downstream tidal level conditions are generated based on sea level data generated using the regional ocean modelling system roms shchepetkin and mcwilliams 2005 the aris average recurrence intervals of these events range from 2 years to 500 years a summary of all 74 flood events is provided in section c of the supplementary material for the development of the lstm models all 15 input variables are included each single input for an individual lstm model at a specific rl is a multivariate time series with a length of 48 h and at 15 min time interval the 48 h input time period is determined based on estimated travel time of water from paradise dam to the discharge point into the coral sea 3 3 modelling process 3 3 1 selecting representative locations as illustrated in fig 1 the srr method presented in section 2 1 is first applied to find the representative locations rls in the modelling domain in total 125 rls including 46 rls cl and 79 rls side are identified the locations of these rls are shown in fig 4 3 3 2 data splitting the development of lstm models like any other models requires data for calibration and validation in this study 70 flood events are used for model calibration i e events 5 74 in section c of the supplementary material while the remaining four events including the three historical events are used for model validation i e events 1 4 in section c of the supplementary material referred to as validation events 1 2 3 and 4 respectively the calibration data are then randomly divided into 80 for training and 20 for testing the training data are used to calibrate the lstm model parameters while the testing data are used to calibrate hyperparameters including epoch number batch size and learning rate which are used for training model parameters 3 3 3 model calibration the lstm models include 15 neurons in the input layer corresponding to the 15 input variables 10 neurons in the biis layer 20 standard memory cells in the lstm layer and one neuron in the output layer the number of neurons in the biis and lstm memory layers are determined using trial and error as for most data driven models the inputs are normalised in this study inflows are normalised based on the highest discharge of 18 470 m3 s and the lower boundary water levels are normalised based on both the minimum and maximum sea levels i e 1 5 m and 2 1 m australian height datum respectively the hyperparameters used in the final lstm model training process include an epoch of 40 a batch size of 200 and a learning rate of 0 005 the mean square error is used as the loss function and adam is used as the optimiser which is commonly used for deep learning model training kingma and ba 2014 to account for the stochastic nature of random data splits and model training kratzert et al 2019 four different random number seeds are used in the model development process this results in an ensemble of four lstm models developed at each rl therefore a total of 500 lstm models are developed to model water levels at all 125 rls 3 3 4 model validation model validation is carried out on two different levels to evaluate 1 the performance of lstm models for flood level simulation at rls and 2 the performance of the srr dl framework for flood inundation simulation across the model domain the ensemble of four models developed at each rl is used for the evaluation of lstm model performance to reduce the impact of the stochastic nature of random data splits and model training while only one out of the four models is selected at each rl to reconstruct flood surfaces for the evaluation of the simulated flood surfaces see section 4 1 2 for justification the general process and evaluation metrics used are described as follows lstm model validation to validate the lstm models developed for modelling water levels at rls both the inputs selected using the built in input selection layer and the predictive performance of the developed lstm models are evaluated the selected inputs are evaluated using the ifr introduced in section 2 2 2 since the spatial distance along the main river channel between the inflow locations and the rls is one of the major factors determining the level of importance of the inputs the first step in input evaluation is to divide the developed lstm models into 29 groups based on the distance along the main river channel between the rls and the lower boundary of the modelling domain at 5 km intervals then the median of the ifr indices in each group is derived for each input variable the efficacy of the selected inputs is assessed by evaluating their impact on model responses at different locations in the modelling domain the predictive performance of the lstm models is evaluated using root mean squared errors rmses estimated for the four validation flood events to account for the stochastic nature of the lstm model development process models trained using all four random seeds are considered during model validation process as mentioned previously flood inundation validation the performance of the srr dl framework for flood inundation modelling is evaluated using a range of metrics for the maximum inundation extent the flood depth across the entire floodplain and the temporal evolution of flood events a threshold water depth of 0 05 m is adopted to differentiate between wet and dry cells for flood inundation evaluations two commonly used metrics namely the probability of detection pod and the rate of false alarm rfa are used to evaluate of the maximum flood inundation extent the pod represents the proportion of the inundated area detected using the srr dl framework compared to the inundated area originally simulated using the tuflow model the rfa represents the proportion of the inundated area that is falsely predicted over the total wet area simulated using the framework these two metrics can be calculated using the following equations 2 p o d a r e a d e t e c t e d a r e a d e t e c t e d a r e a m i s s e d 100 3 r f a a r e a f a l s e a l a r m a r e a d e t e c t e d a r e a f a l s e a l a r m 100 where a r e a d e t e c t e d is the area of inundation detected using both the srr dl framework and the tuflow model a r e a m i s s e d is the area of inundation simulated using the tuflow model but which is not detected with the srr dl framework and a r e a f a l s e a l a r m is the area of inundation simulated using the srr dl framework but which is dry in the tuflow model simulation results flood depth across the floodplain is evaluated using rmse and relative rmse rrmse estimated for the water depths at each grid cell the rrmse for each cell is calculated by dividing the rmse by the average water depth simulated using the tuflow model for the evaluation of flood inundation evolution over time the total inundation area simulated using the framework is compared to that obtained using the tuflow model over entire floodplain at 15 min intervals the occurrence time of the peak water level in each grid cell is also compared 4 results and discussion 4 1 lstm model performance at representative locations rls 4 1 1 input selection evaluation the efficacy of selected inputs at various rls is illustrated in fig 5 fig 5a shows the grouping of the 500 lstm models developed for the 125 rls based on their distance to the downstream boundary each group includes two to seven lstm models the 13 concurrent inflows cfs are distributed inputs and the middle point of each concurrent inflow location is indicated in the figure fig 5b shows the ifr indices for the 15 input variables including the main inflow q the lower boundary water levels at burnett heads sl and the 13 concurrent inflow variables cf 0 to cf 12 higher ifr values represent higher impact on model outputs as mentioned previously each row in fig 5b represents one input variable as labelled where the pink vertical line represents the distance between the location of cf input to the lower boundary of model domain it is evident from fig 5b that the impact of the main upstream inflow q diminishes as the modelling location moves downstream towards the lower boundary of the model domain conversely the lower boundary water levels at burnett heads sl have a higher impact on water levels at downstream locations compared to upstream locations in other words the impacts of both inputs diminish with distance which is expected the influence of the cf variables also diminishes with distance however as the cfs are distributed inputs along the river channel each cf variable influences water levels along both the upstream and downstream directions this is consistent with the results shown in fig 5 in addition the impact of each cf diminishes more quickly in an upstream direction than downstream which is to be expected considering the direction of flow out of all of the cf inputs the cf 9 is the only input variable that does not have a significant impact on water levels at its nearby locations moreover the ifrs for cf 9 are similar to that for cf 12 this is because cf 9 is of little importance to the outputs while its input hydrograph is similar to that of cf 12 in addition to the distance changes in elevation levels along the mainstream can also influence the importance of different inflow variables for example the impact of q increases and the impacts of variables cf 0 to cf 6 drop dramatically near locations of group 13 which is caused due to a significant drop in the elevation level at this location 4 1 2 predictive performance the predictive performance of lstm models is evaluated using the rmse estimated for the four validation flood events fig 6 shows the average rmses at all modelling locations i e rls the average rmse and the standard deviation of rmses of lstm models developed using the four different random seeds are presented in table 1 while rmse values for all lstm models developed are provided in section d of the supplementary material as shown in fig 6 the performance of lstm models varies slightly with locations models developed for the mainstream rls i e rl cl outperform those for the lateral rls i e rl side with the average rmses being 0 048 m and 0 061 m for the two types of rls respectively the rl with the maximum rmse of 0 54 m is located at an anabranch 10 km downstream of the paradise dam as shown in fig 6 the poor performance of the lstm models developed at this rl is due to the fact that water level remains constant for most of the simulation periods in the anabranch overall lstm models provide accurate simulations of the water levels at all rls the rmses for lstm models developed with different random number seeds are statistically similar as shown in table 1 given the overall high level of performance across all simulations regardless of the random number seed used there is no advantage to be gained by using an ensemble of lstm models accordingly only 125 lstm models developed with the first random number seed is used to reconstruct the flood inundation surface this also reduces the computational effort required without loss of accuracy 4 2 flood inundation modelling performance the analysis of flood inundation results is conducted only for areas upstream of the bundaberg city to avoid reporting on areas that might be of community concern as requested by the owner of the data this area covers about 78 of the model domain with 2 884 880 active grid cells as shown in fig 3 4 2 1 maximum inundation extent the maximum inundation extent simulated using the srr dl framework is compared with the tuflow model results table 2 summarises the probability of detection pod and the rate of false alarm rfa of the simulated maximum inundation extent for the four validation events it can be seen from the table that the srr dl framework performs well with the pod results being over 99 for all four validation events the rfa values vary with smaller events having higher rfa values this indicates that false alarms i e dry areas incorrectly classified as being inundated are more likely to occur in smaller floods compared to larger floods the maximum inundation extents for the largest validation flood event simulated using both the tuflow model and the srr dl framework are shown in fig 7 overall the inundation extent simulated using the srr dl framework matches well with the results obtained using the tuflow model there are only a very few areas shown by green shading in fig 7a where the srr dl framework misses areas of inundation the false alarm inundation area is larger than the missed areas but still accounts for a very small proportion 0 4 of the model domain fig 7b and 7c in addition false alarm mainly occurs at the outer edge of the floodplains and indicates overestimation of flood water levels is likely to occur at transitional regions between dry and wet conditions in general the srr dl framework is able to simulate the maximum inundation extent of all four validation flood events with a high level of accuracy with over 99 of the inundated area being detected 4 2 2 flood inundation depth the predicted flood depth obtained from the srr dl framework is evaluated for each grid cell in the model domain against those generated using the tuflow models for the four validation events the results in fig 8 are cumulative percentage of grid cells where the rmses and rrmses are below given values for the rising and receding periods of the four validation flood events the spatial distribution of rmses and rrmses for validation event 4 is shown in fig 9 the spatial distributions of rmses and rrmses for the other three validation events are included in section e of the supplementary material the results in fig 8 show that the srr dl framework leads to relatively low rmses and rrmses for most of the grid cells in the model domain with over 90 of grid cells having rmse values below 0 8 m and rrmse values below 0 4 the srr dl framework performs better during the rising period of flood events compared to the receding period of flood events this is evident from fig 8 where there are more grid cells with lower rmse and rrmse values in the rising period blue line than in the receding period orange line of flood events in addition the srr dl framework performs better near the rls where the water levels are directly estimated from input variables using the lstm models as shown in fig 9 it should be noted that areas with high rrmses in fig 9b tend to be on floodplains where the depths of inundation are small however the absolute error metrics rmse in these areas are generally less than 0 5 m 4 2 3 flood inundation evolution apart from the maximum inundation extent and the inundation depth the performance the srr dl framework is also evaluated considering flood evolution over time the changes in the total inundation area with time fig 10 are first investigated it can be seen from the figure that the performance during the rising period of flood events again is better than that during the receding period of flood events this result is more pronounced for larger events as shown in fig 10d the relatively poor performance of the srr dl framework over the receding period of floods is mainly related to how flood surfaces are reconstructed using the srr reco module as the water level recedes flood water is trapped in overbank elevation depressions and becomes disconnected from the river channel a phenomenon also known as dead storage the flood inundation at each time step is reconstructed separately using the srr reco module while this approach improves computational efficiency it means that inundation conditions from the previous time step are not considered and during a recession period any information on dead storage conditions cannot be used to inform estimates at subsequent time steps the estimated timing of the maximum total inundation area is summarised in table 3 the flood peak estimated using the srr dl framework arrives slightly earlier than that simulated using the tuflow model the minimum reported time difference is 0 25 h as this is the smallest output interval used and the largest time difference is 1 5 h which is approximately 0 9 of the time between the beginning and peak of the event the occurrence time of the peak water level in each grid cell is estimated for the results of the tuflow model the srr dl framework and the differences between the two results are evaluated fig 11 shows the cumulative percentage of grid cells where the occurrence time difference is below given values the time differences for all four events are less than 2 h for most of the grid cells in the modelling domain as the duration of these events are between 190 h and 430 h a difference of 2 h is only between 0 5 and 1 1 of the durations of the entire events 4 3 computational efficiency the computational speed of the srr dl framework is significantly faster than that of the tuflow model once the model is developed it takes less than 4s to simulate flood inundation at any time step over the whole study area on a computer with intel r core tm i7 8700k 3 70ghz cpu and a nvidia quadro p5000 gpu with dedicated 16 gb memory in the application the srr dl framework run time is estimated to be 101 min for the event with a real time duration of 430 h with 15 min output interval see table 4 in comparison the tuflow model accelerated with a gpu takes around 2180 min 36 h to simulate the same event therefore the srr dl framework is over 21 times faster than the tuflow model in run time flood inundation simulations using the srr dl framework solely relies on the dem and flood driving factors including discharge inflows and lower boundary water levels there is considerable potential for parallelisation of the scheme to model flood inundation surfaces at different times simultaneously it is also worth noting that the run time for the framework is proportional to the output time step which has very limited impact on the run time of the tuflow model for example if the output time step of the same event is 1 h instead of 15 min for both models the time required by the srr dl framework is reduced to 25 min which makes the srr dl framework 86 times faster than the tuflow model in run time 5 conclusion the srr dl spatial reduction and reconstruction deep learning framework proposed here is found to provide fast time series estimates of the flood inundation without appreciable loss of accuracy the framework comprises two main components one component srr reduces the computational burden by taking advantage of spatial correlations across the model domain to reduce the number of locations where models are required and the other component long short term memory lstm model is a deep learning model that takes advantage of temporal dependencies in the time series to map the relationship between the flood driving mechanisms such as streamflow discharge and tidal water level and water levels at the representative locations the special built in input selection layer in the lstm models facilitates selection of the most useful predictor variables during the model calibration process thus simplifying model development process applying the srr dl framework to flood data derived from a 2d hydrodynamic model provides water level estimates at representative locations with an average rmse of 0 056 m compared to the tuflow model the simulated flood inundation surfaces obtained using the srr dl framework detect over 99 of the areal extent of maximum inundation this framework takes less than 4 s to simulate the flood inundation surface at one time step in a model domain with over 3 million grid cells covering over 1479 km2 area the major limitation of using dl model or any other data driven modelling approach in real world application is that they cannot extrapolate beyond the range of data used for training however the dl models in this study are trained using results from a 2d hydrodynamic model which means that we are able to generate training data for a flood of any magnitude the srr dl framework can thus be used to model events of any magnitude as long as the equivalent 2d hydrodynamic model results are provided the srr dl framework can potentially be used for characterising uncertainty in operational real time forecasting or design applications where computationally efficient models are required software and data availability the spatial reduction and reconstruction method as presented in the associated methodsx paper is programmed using the python programming language version 3 7 and is freely available from https github com yuerongz srr method the development of dl models relies on the python programming language version 3 7 and open source libraries including pytorch numpy scipy fiona gdal geopandas and netcdf4 data and program used for dl model development are available from https doi org 10 26188 13122623 v1 and https github com yuerongz lstm model for srr dl burnett editorial conflict of interest statement given her role as associate editor of environmental modelling software wenyan wu was not involved in the peer review of this article and had no access to information regarding its peer review full responsibility for the editorial process for this article was delegated to editor in chief daniel p ames declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we thank hydrology and risk consulting harc for providing the tuflow 2d hydrodynamic model configuration and sunwater for their permission to use the burnett river as a case study we also thank bmt for providing the tuflow license to conduct the tuflow simulations this research was undertaken using the lief hpc gpgpu facility hosted at the university of melbourne this facility was established with the assistance of lief grant grant number le170100200 at last for a range of open source software and libraries used in this study including but not limited to python programming language version 3 7 pytorch numpy scipy fiona qgis netcdf4 geopandas and geospatial data abstraction library gdal we would like to thank the developers and the open source community for all contributions appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105112 
25794,traditional approaches to inundation modelling are computationally intensive and thus not well suited to assessing the uncertainty involved in estimating flood inundation surfaces for planning design and forecasting purposes in this study a rapid flood inundation modelling framework is developed consisting of a novel spatial reduction and reconstruction srr approach and a deep learning dl modelling component the srr approach is developed to reduce computational cost by identifying representative locations of inundation surfaces where water levels are simulated using dl models and to efficiently reconstruct inundation surfaces based on simulated water level information the dl model includes a built in input selection layer to simplify the model development process and a long short term memory layer for time series modelling the accuracy and efficiency of the srr dl framework is assessed by application to a real world river system where the inundation of over 3 million grid cells can be simulated in 4 s keywords flood inundation modelling deep learning long short term memory spatial reduction 1 introduction flood inundation models are commonly used for assessing flood hazard in planning design and forecasting research in recent years has demonstrated an increasing interest in probabilistic flood inundation estimation using ensembles of inputs to account for uncertainty cooten et al 2011 georgas et al 2016 gomez et al 2019 wu et al 2020 this is a challenging task due to the high computational requirements of existing flood inundation models as a result fast flood inundation models are needed to simulate accurate flood inundation extent and depth at high spatial resolutions with small timesteps gomez et al 2019 teng et al 2017 commonly used flood inundation models can be divided into three major categories including empirical models hydrodynamic models and simplified conceptual models teng et al 2017 among these models 2 dimensional 2d hydrodynamic flood inundation models are generally considered accurate and therefore commonly used for engineering applications n√©elz and pender 2010 however they are computationally expensive and therefore not suitable for applications where a large number of model runs are required kalyanapu et al 2011 though simplified conceptual models are generally more computationally efficient they are not well suited to capturing the temporal dynamics of floods lhomme et al 2008 teng et al 2015 zheng 2018 to reduce the computational cost while maintaining desired accuracy data driven emulation models mostly artificial neural networks anns have also been developed for flood inundation modelling chang et al 2018 chu et al 2019 xie et al 2020 one drawback of traditional anns such as multilayer perceptron models is that they cannot take advantage of serial correlation that is present in the input time series data used to specify flow and water level boundary conditions chu et al 2019 kratzert et al 2018 this adds unnecessary computational complexity and uncertainty to the simulations which could be avoided if explicit account is given to the serial correlation contained in time series input data hsu et al 1997 kratzert et al 2018 kumar et al 2004 in addition current implementations of anns for simulating flood inundation typically involve developing models to simulate flood depth or water level at each grid cell within the modelling domain this often results in either a large number of ann models being developed or a very complex ann model which increases the difficulty of model development for example chu et al 2019 and xie et al 2020 developed as many as 19 448 ann models to model flood depth for an area of 7 8 km2 kabir et al 2020 modelled flood inundation depth for 581 061 grid cells using a single convolutional ann which has over 297 million parameters such approaches also present difficulties when applied to data sparse regions where there are very limited flood inundation data for model development xie et al 2020 since flood inundation models simulate continuous water surfaces water levels at one grid cell are spatially correlated to those in its neighbouring cells current ann based modelling approaches ignore the redundancy in flood inundation information associated with this spatial correlation leaving room for potential improvement of modelling efficiency in this study a new flood inundation modelling framework is developed to address the two key issues listed above first deep learning dl models are used to accommodate the serial correlation embedded in the multivariate time series inputs for flood inundation modelling among different dl models recurrent neural networks rnns were designed to handle sequential inputs and have been found to be effective for time series modelling connor et al 1994 hsu et al 1997 kumar et al 2004 among different rnns the long short term memory lstm architecture has been increasingly applied to discharge modelling in recent years as lstm has been found to out perform the traditional rnns for time series modelling in a number of studies gers et al 1999 kratzert et al 2018 sahoo et al 2019 therefore the lstm architecture is adopted for this study in addition to the traditional lstm layer a fully connected input selection layer is included in the lstm model architecture consequently input selection is incorporated into the model calibration process rather than being a separate step of the model development process and this significantly simplifies the model development process second in order to reduce the information redundancy in the current one model per grid approach used in traditional ann based flood inundation modelling approaches a spatial reduction and reconstruction srr method is developed to characterise the flood inundation surface in a more parsimonious manner this srr method allows the whole inundation surface across the model domain to be represented by water levels at selected representative locations this significantly reduces the number of models that need to be developed to represent the flood inundation surface thus reducing the computational cost and the number of parameters involved in the simulation the representative locations are selected to make the best use of flood information in data rich regions as this has been found to improve the accuracy of the model simulations xie et al 2020 finally the accuracy and efficiency of this framework which hereafter is referred to as srr dl are demonstrated via a real world case study in queensland australia the paper is structured as follows a detailed description of the srr dl framework is provided in section 2 including the srr method and the design of the dl model architecture then the application of the framework is presented in section 3 including the introduction of the case study and detailed modelling process applied section 4 includes results obtained and discussion of main findings and this is followed by conclusions in section 5 2 methodology the srr dl framework consists of two major components 1 the srr method is developed and used to select representative locations rls within the modelling domain that can be used to estimate flood inundation behaviour over the whole simulation period based on water levels simulated at these locations 2 dl models are used to simulate water levels at each of the selected representative locations based on the time series of model inputs e g mainstream and tributary inflows and ocean water levels influencing the lower model boundary the overall framework is illustrated in fig 1 the details of the two major components of the framework are introduced below 2 1 spatial reduction and reconstruction method for flood inundation modelling the first major component of the srr dl framework for flood inundation modelling is the spatial reduction and reconstruction srr method as shown in fig 1 there are two modules involved in this component namely srr rl srr representative location and srr reco srr reconstruction by finding locations where the water levels are representative of flood behaviour over the whole model domain the srr rl module aims to 1 reduce the number of locations where dl modelling is required and 2 focus dl modelling in data rich regions where there are sufficient data for model development the srr reco module is then used to reconstruct flood inundation surfaces based on water levels simulated at these representative locations the srr method is written in the python programming language the code is freely available online at https github com yuerongz srr method the details of the srr method are included in the methodsx article related to this paper a brief description of the srr method including its two modules is presented in the following two sub sections 2 1 1 representative location selection with srr rl the function of the srr rl module in the srr method is to identify the rls that best capture inundation behaviour over the whole modelling domain the rls are selected according to the following two criteria 1 the rls need to cover regions where the water level varies rapidly through time and 2 there need to be sufficient water level data at these locations two types of rls are selected to represent the different inundation characteristics along mainstream and lateral flow paths the mainstream rls i e rls cl are selected along the centroid line of the main river channel at intervals which allow the path of the channel thalweg to be represented by a series of straight lines as a result the more meandering a river channel is the more rls cl will be selected the lateral rls i e rls side are selected along both sides of the mainstream channel at locations where flood water exits or enters river channel a python function named srr search is developed as part of the srr rl module to identify both the mainstream and lateral flow path locations see the associated methodsx paper 2 1 2 flood surface reconstruction with srr reco after the rls are selected dl models are used to simulate water levels at these rls the dl models are discussed in section 2 2 below the srr reco module is developed to convert the simulated water level information into flood inundation surfaces srr reco is a simple and fast method for inundation surface reconstruction it applies a 2d linear interpolation method barber et al 1996 to reconstruct the inundation water surface based on water levels simulated at rls and at model boundaries where water levels are specified as an input boundary condition the interpolated water surfaces are then compared to the digital elevation model dem of the area so the water surfaces can be adjusted to take account of those areas where the dem is higher than the interpolated water surface elevation the resolution of the reconstructed inundation surfaces is determined by the resolution of the underlying dem and thus there is no restriction in the output resolution provided by srr reco details of the srr reco module are included in the associated methodsx paper 2 2 deep learning model 2 2 1 model architecture in the srr dl framework one popular deep learning model the long short term memory recurrent neural networks model lstm is used to model water levels at rls lstm models are well suited to simulating non linear relationships between inputs and outputs liu et al 2017 schmidhuber 2015 and as mentioned previously they are efficient at handling time series inputs gers et al 1999 kratzert et al 2018 sahoo et al 2019 to simplify the overall model development process and improve input selection a built in input selection layer is included in the lstm model to incorporate input selection into the model calibration process details of the built in input selection layer are presented in section 2 2 2 below the proposed lstm model architecture including the built in input selection layer is shown in fig 2 the first layer of the proposed lstm model architecture is an input layer which takes the multivariate time series inputs such as inflows at various locations along the river and water levels of downstream boundary conditions each variable in the input is represented as one neuron in the input layer which means each neuron in the input layer is a time series vector the second layer in the architecture is the built in input selection biis layer which is a fully connected layer designed to select inputs from all input variables considered the input and biis layers are connected through the weighted sum and rectified linear unit relu functions the third layer is the lstm layer which takes the information of selected input variables from the biis layer and converts the multivariate time series sequences to a data vector using the standard lstm memory cells gers et al 1999 kratzert et al 2018 the output from the lstm layer is transformed using a tanh function before being passed to the final output layer as shown in fig 2 the output layer in the proposed model architecture takes a weighted sum of the received data from the previous layer and converts them into the final outputs the water levels at specific times in order to produce time series outputs the input time series are successively shifted forward by one timestep and input to the lstm model to provide a time series of water level outputs the basic concepts and equations of the model are included in section a of the supplementary material 2 2 2 build in input selection the build in input selection biis layer in the proposed lstm model architecture is designed to simplify the model development process by including the traditionally separate input selection step into the model calibration process this way the appropriate input variables can be selected at the same time as the model parameter values are calibrated avoiding the potential impact of predetermined inputs on model calibration furthermore by eliminating the additional separate input selection step the different lstm models at different rls across the model domain can all take the same set of inputs this not only simplifies model development process but also reduces the effort required to prepare input data for real world applications the selection of input variables is determined by the values of the weights connecting the input layer and the biis layer the larger the absolute value of a weight the greater the level of information that is passed from the input variable to the lstm layer and thus the greater the impact of the input variable on the model outputs because the selection of inputs is hidden in the parameter values of the trained models the interpretation of selected inputs needs to be done through connection weight analysis a step that has been used previously to interpreted ann models andrews et al 1995 gevrey et al 2003 kingston et al 2005 2006 solomatine et al 2009 wilby et al 2003 in this study a simple indexing method is developed to quantify the importance of each input variable fed into the biis layer and therefore this indexing method can also be used to evaluate the selected input variables in trained models the indexing method makes use of the information inheritance rate ifr which represents the level of influence that an input variable has on the output the ifr for each variable can be calculated using the following equation 1 i f r v a r i a b l e j k 1 n b i i s a b s w k j where n b i i s is the number of neurons in the biis layer and a b s w k j is the absolute value of the weight from the connection between variable j and the kth neuron in the biis layer variables that have a greater influence on model results are associated with higher ifr indices and vice versa another important function of the biis layer is that it can reduce the total number of parameters that need to be trained in the lmst model lmst model have often been criticised for having a large number of parameters and therefore are prone to overfitting srivastava et al 2014 the addition of the biis layer in the lstm model can reduce the number of neurons connected to the lstm layer which includes the majority of the model parameters and hence can reduce the total number of model parameters while maintaining the flexibility of the model in fitting all important features in input data a detailed explanation on how the biis can reduce the total number of parameters in the lmst model is included in section b of the supplementary material 3 application 3 1 study area and hydrodynamic model available the application of the srr dl framework to flood inundation modelling is demonstrated using the burnett river system downstream of paradise dam in queensland australia fig 3 shows the location and model domain of the system the river reach in the study area is approximately 145 km long and ends at the coral sea at burnett heads a 2d hydrodynamic model was developed for the system using the tuflow modelling suite huxley and syme 2016 the modelling area extents from the paradise dam to the discharge point at burnett heads the 2d model domain contains 3 697 597 20 m by 20 m active model grid cells covering over 1479 km2 area the tuflow model has in total 15 inputs including the main inflow into the system released from paradise dam 13 concurrent tributary inflows at various locations along the mainstream and the lower boundary water levels at burnett heads all the input data are available at 15 min intervals 3 2 flood event datasets the dataset used in this study consists of 74 flood events simulated using the tuflow model these flood events include three historical events and 71 design events for the 71 design events the downstream tidal level conditions are generated based on sea level data generated using the regional ocean modelling system roms shchepetkin and mcwilliams 2005 the aris average recurrence intervals of these events range from 2 years to 500 years a summary of all 74 flood events is provided in section c of the supplementary material for the development of the lstm models all 15 input variables are included each single input for an individual lstm model at a specific rl is a multivariate time series with a length of 48 h and at 15 min time interval the 48 h input time period is determined based on estimated travel time of water from paradise dam to the discharge point into the coral sea 3 3 modelling process 3 3 1 selecting representative locations as illustrated in fig 1 the srr method presented in section 2 1 is first applied to find the representative locations rls in the modelling domain in total 125 rls including 46 rls cl and 79 rls side are identified the locations of these rls are shown in fig 4 3 3 2 data splitting the development of lstm models like any other models requires data for calibration and validation in this study 70 flood events are used for model calibration i e events 5 74 in section c of the supplementary material while the remaining four events including the three historical events are used for model validation i e events 1 4 in section c of the supplementary material referred to as validation events 1 2 3 and 4 respectively the calibration data are then randomly divided into 80 for training and 20 for testing the training data are used to calibrate the lstm model parameters while the testing data are used to calibrate hyperparameters including epoch number batch size and learning rate which are used for training model parameters 3 3 3 model calibration the lstm models include 15 neurons in the input layer corresponding to the 15 input variables 10 neurons in the biis layer 20 standard memory cells in the lstm layer and one neuron in the output layer the number of neurons in the biis and lstm memory layers are determined using trial and error as for most data driven models the inputs are normalised in this study inflows are normalised based on the highest discharge of 18 470 m3 s and the lower boundary water levels are normalised based on both the minimum and maximum sea levels i e 1 5 m and 2 1 m australian height datum respectively the hyperparameters used in the final lstm model training process include an epoch of 40 a batch size of 200 and a learning rate of 0 005 the mean square error is used as the loss function and adam is used as the optimiser which is commonly used for deep learning model training kingma and ba 2014 to account for the stochastic nature of random data splits and model training kratzert et al 2019 four different random number seeds are used in the model development process this results in an ensemble of four lstm models developed at each rl therefore a total of 500 lstm models are developed to model water levels at all 125 rls 3 3 4 model validation model validation is carried out on two different levels to evaluate 1 the performance of lstm models for flood level simulation at rls and 2 the performance of the srr dl framework for flood inundation simulation across the model domain the ensemble of four models developed at each rl is used for the evaluation of lstm model performance to reduce the impact of the stochastic nature of random data splits and model training while only one out of the four models is selected at each rl to reconstruct flood surfaces for the evaluation of the simulated flood surfaces see section 4 1 2 for justification the general process and evaluation metrics used are described as follows lstm model validation to validate the lstm models developed for modelling water levels at rls both the inputs selected using the built in input selection layer and the predictive performance of the developed lstm models are evaluated the selected inputs are evaluated using the ifr introduced in section 2 2 2 since the spatial distance along the main river channel between the inflow locations and the rls is one of the major factors determining the level of importance of the inputs the first step in input evaluation is to divide the developed lstm models into 29 groups based on the distance along the main river channel between the rls and the lower boundary of the modelling domain at 5 km intervals then the median of the ifr indices in each group is derived for each input variable the efficacy of the selected inputs is assessed by evaluating their impact on model responses at different locations in the modelling domain the predictive performance of the lstm models is evaluated using root mean squared errors rmses estimated for the four validation flood events to account for the stochastic nature of the lstm model development process models trained using all four random seeds are considered during model validation process as mentioned previously flood inundation validation the performance of the srr dl framework for flood inundation modelling is evaluated using a range of metrics for the maximum inundation extent the flood depth across the entire floodplain and the temporal evolution of flood events a threshold water depth of 0 05 m is adopted to differentiate between wet and dry cells for flood inundation evaluations two commonly used metrics namely the probability of detection pod and the rate of false alarm rfa are used to evaluate of the maximum flood inundation extent the pod represents the proportion of the inundated area detected using the srr dl framework compared to the inundated area originally simulated using the tuflow model the rfa represents the proportion of the inundated area that is falsely predicted over the total wet area simulated using the framework these two metrics can be calculated using the following equations 2 p o d a r e a d e t e c t e d a r e a d e t e c t e d a r e a m i s s e d 100 3 r f a a r e a f a l s e a l a r m a r e a d e t e c t e d a r e a f a l s e a l a r m 100 where a r e a d e t e c t e d is the area of inundation detected using both the srr dl framework and the tuflow model a r e a m i s s e d is the area of inundation simulated using the tuflow model but which is not detected with the srr dl framework and a r e a f a l s e a l a r m is the area of inundation simulated using the srr dl framework but which is dry in the tuflow model simulation results flood depth across the floodplain is evaluated using rmse and relative rmse rrmse estimated for the water depths at each grid cell the rrmse for each cell is calculated by dividing the rmse by the average water depth simulated using the tuflow model for the evaluation of flood inundation evolution over time the total inundation area simulated using the framework is compared to that obtained using the tuflow model over entire floodplain at 15 min intervals the occurrence time of the peak water level in each grid cell is also compared 4 results and discussion 4 1 lstm model performance at representative locations rls 4 1 1 input selection evaluation the efficacy of selected inputs at various rls is illustrated in fig 5 fig 5a shows the grouping of the 500 lstm models developed for the 125 rls based on their distance to the downstream boundary each group includes two to seven lstm models the 13 concurrent inflows cfs are distributed inputs and the middle point of each concurrent inflow location is indicated in the figure fig 5b shows the ifr indices for the 15 input variables including the main inflow q the lower boundary water levels at burnett heads sl and the 13 concurrent inflow variables cf 0 to cf 12 higher ifr values represent higher impact on model outputs as mentioned previously each row in fig 5b represents one input variable as labelled where the pink vertical line represents the distance between the location of cf input to the lower boundary of model domain it is evident from fig 5b that the impact of the main upstream inflow q diminishes as the modelling location moves downstream towards the lower boundary of the model domain conversely the lower boundary water levels at burnett heads sl have a higher impact on water levels at downstream locations compared to upstream locations in other words the impacts of both inputs diminish with distance which is expected the influence of the cf variables also diminishes with distance however as the cfs are distributed inputs along the river channel each cf variable influences water levels along both the upstream and downstream directions this is consistent with the results shown in fig 5 in addition the impact of each cf diminishes more quickly in an upstream direction than downstream which is to be expected considering the direction of flow out of all of the cf inputs the cf 9 is the only input variable that does not have a significant impact on water levels at its nearby locations moreover the ifrs for cf 9 are similar to that for cf 12 this is because cf 9 is of little importance to the outputs while its input hydrograph is similar to that of cf 12 in addition to the distance changes in elevation levels along the mainstream can also influence the importance of different inflow variables for example the impact of q increases and the impacts of variables cf 0 to cf 6 drop dramatically near locations of group 13 which is caused due to a significant drop in the elevation level at this location 4 1 2 predictive performance the predictive performance of lstm models is evaluated using the rmse estimated for the four validation flood events fig 6 shows the average rmses at all modelling locations i e rls the average rmse and the standard deviation of rmses of lstm models developed using the four different random seeds are presented in table 1 while rmse values for all lstm models developed are provided in section d of the supplementary material as shown in fig 6 the performance of lstm models varies slightly with locations models developed for the mainstream rls i e rl cl outperform those for the lateral rls i e rl side with the average rmses being 0 048 m and 0 061 m for the two types of rls respectively the rl with the maximum rmse of 0 54 m is located at an anabranch 10 km downstream of the paradise dam as shown in fig 6 the poor performance of the lstm models developed at this rl is due to the fact that water level remains constant for most of the simulation periods in the anabranch overall lstm models provide accurate simulations of the water levels at all rls the rmses for lstm models developed with different random number seeds are statistically similar as shown in table 1 given the overall high level of performance across all simulations regardless of the random number seed used there is no advantage to be gained by using an ensemble of lstm models accordingly only 125 lstm models developed with the first random number seed is used to reconstruct the flood inundation surface this also reduces the computational effort required without loss of accuracy 4 2 flood inundation modelling performance the analysis of flood inundation results is conducted only for areas upstream of the bundaberg city to avoid reporting on areas that might be of community concern as requested by the owner of the data this area covers about 78 of the model domain with 2 884 880 active grid cells as shown in fig 3 4 2 1 maximum inundation extent the maximum inundation extent simulated using the srr dl framework is compared with the tuflow model results table 2 summarises the probability of detection pod and the rate of false alarm rfa of the simulated maximum inundation extent for the four validation events it can be seen from the table that the srr dl framework performs well with the pod results being over 99 for all four validation events the rfa values vary with smaller events having higher rfa values this indicates that false alarms i e dry areas incorrectly classified as being inundated are more likely to occur in smaller floods compared to larger floods the maximum inundation extents for the largest validation flood event simulated using both the tuflow model and the srr dl framework are shown in fig 7 overall the inundation extent simulated using the srr dl framework matches well with the results obtained using the tuflow model there are only a very few areas shown by green shading in fig 7a where the srr dl framework misses areas of inundation the false alarm inundation area is larger than the missed areas but still accounts for a very small proportion 0 4 of the model domain fig 7b and 7c in addition false alarm mainly occurs at the outer edge of the floodplains and indicates overestimation of flood water levels is likely to occur at transitional regions between dry and wet conditions in general the srr dl framework is able to simulate the maximum inundation extent of all four validation flood events with a high level of accuracy with over 99 of the inundated area being detected 4 2 2 flood inundation depth the predicted flood depth obtained from the srr dl framework is evaluated for each grid cell in the model domain against those generated using the tuflow models for the four validation events the results in fig 8 are cumulative percentage of grid cells where the rmses and rrmses are below given values for the rising and receding periods of the four validation flood events the spatial distribution of rmses and rrmses for validation event 4 is shown in fig 9 the spatial distributions of rmses and rrmses for the other three validation events are included in section e of the supplementary material the results in fig 8 show that the srr dl framework leads to relatively low rmses and rrmses for most of the grid cells in the model domain with over 90 of grid cells having rmse values below 0 8 m and rrmse values below 0 4 the srr dl framework performs better during the rising period of flood events compared to the receding period of flood events this is evident from fig 8 where there are more grid cells with lower rmse and rrmse values in the rising period blue line than in the receding period orange line of flood events in addition the srr dl framework performs better near the rls where the water levels are directly estimated from input variables using the lstm models as shown in fig 9 it should be noted that areas with high rrmses in fig 9b tend to be on floodplains where the depths of inundation are small however the absolute error metrics rmse in these areas are generally less than 0 5 m 4 2 3 flood inundation evolution apart from the maximum inundation extent and the inundation depth the performance the srr dl framework is also evaluated considering flood evolution over time the changes in the total inundation area with time fig 10 are first investigated it can be seen from the figure that the performance during the rising period of flood events again is better than that during the receding period of flood events this result is more pronounced for larger events as shown in fig 10d the relatively poor performance of the srr dl framework over the receding period of floods is mainly related to how flood surfaces are reconstructed using the srr reco module as the water level recedes flood water is trapped in overbank elevation depressions and becomes disconnected from the river channel a phenomenon also known as dead storage the flood inundation at each time step is reconstructed separately using the srr reco module while this approach improves computational efficiency it means that inundation conditions from the previous time step are not considered and during a recession period any information on dead storage conditions cannot be used to inform estimates at subsequent time steps the estimated timing of the maximum total inundation area is summarised in table 3 the flood peak estimated using the srr dl framework arrives slightly earlier than that simulated using the tuflow model the minimum reported time difference is 0 25 h as this is the smallest output interval used and the largest time difference is 1 5 h which is approximately 0 9 of the time between the beginning and peak of the event the occurrence time of the peak water level in each grid cell is estimated for the results of the tuflow model the srr dl framework and the differences between the two results are evaluated fig 11 shows the cumulative percentage of grid cells where the occurrence time difference is below given values the time differences for all four events are less than 2 h for most of the grid cells in the modelling domain as the duration of these events are between 190 h and 430 h a difference of 2 h is only between 0 5 and 1 1 of the durations of the entire events 4 3 computational efficiency the computational speed of the srr dl framework is significantly faster than that of the tuflow model once the model is developed it takes less than 4s to simulate flood inundation at any time step over the whole study area on a computer with intel r core tm i7 8700k 3 70ghz cpu and a nvidia quadro p5000 gpu with dedicated 16 gb memory in the application the srr dl framework run time is estimated to be 101 min for the event with a real time duration of 430 h with 15 min output interval see table 4 in comparison the tuflow model accelerated with a gpu takes around 2180 min 36 h to simulate the same event therefore the srr dl framework is over 21 times faster than the tuflow model in run time flood inundation simulations using the srr dl framework solely relies on the dem and flood driving factors including discharge inflows and lower boundary water levels there is considerable potential for parallelisation of the scheme to model flood inundation surfaces at different times simultaneously it is also worth noting that the run time for the framework is proportional to the output time step which has very limited impact on the run time of the tuflow model for example if the output time step of the same event is 1 h instead of 15 min for both models the time required by the srr dl framework is reduced to 25 min which makes the srr dl framework 86 times faster than the tuflow model in run time 5 conclusion the srr dl spatial reduction and reconstruction deep learning framework proposed here is found to provide fast time series estimates of the flood inundation without appreciable loss of accuracy the framework comprises two main components one component srr reduces the computational burden by taking advantage of spatial correlations across the model domain to reduce the number of locations where models are required and the other component long short term memory lstm model is a deep learning model that takes advantage of temporal dependencies in the time series to map the relationship between the flood driving mechanisms such as streamflow discharge and tidal water level and water levels at the representative locations the special built in input selection layer in the lstm models facilitates selection of the most useful predictor variables during the model calibration process thus simplifying model development process applying the srr dl framework to flood data derived from a 2d hydrodynamic model provides water level estimates at representative locations with an average rmse of 0 056 m compared to the tuflow model the simulated flood inundation surfaces obtained using the srr dl framework detect over 99 of the areal extent of maximum inundation this framework takes less than 4 s to simulate the flood inundation surface at one time step in a model domain with over 3 million grid cells covering over 1479 km2 area the major limitation of using dl model or any other data driven modelling approach in real world application is that they cannot extrapolate beyond the range of data used for training however the dl models in this study are trained using results from a 2d hydrodynamic model which means that we are able to generate training data for a flood of any magnitude the srr dl framework can thus be used to model events of any magnitude as long as the equivalent 2d hydrodynamic model results are provided the srr dl framework can potentially be used for characterising uncertainty in operational real time forecasting or design applications where computationally efficient models are required software and data availability the spatial reduction and reconstruction method as presented in the associated methodsx paper is programmed using the python programming language version 3 7 and is freely available from https github com yuerongz srr method the development of dl models relies on the python programming language version 3 7 and open source libraries including pytorch numpy scipy fiona gdal geopandas and netcdf4 data and program used for dl model development are available from https doi org 10 26188 13122623 v1 and https github com yuerongz lstm model for srr dl burnett editorial conflict of interest statement given her role as associate editor of environmental modelling software wenyan wu was not involved in the peer review of this article and had no access to information regarding its peer review full responsibility for the editorial process for this article was delegated to editor in chief daniel p ames declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we thank hydrology and risk consulting harc for providing the tuflow 2d hydrodynamic model configuration and sunwater for their permission to use the burnett river as a case study we also thank bmt for providing the tuflow license to conduct the tuflow simulations this research was undertaken using the lief hpc gpgpu facility hosted at the university of melbourne this facility was established with the assistance of lief grant grant number le170100200 at last for a range of open source software and libraries used in this study including but not limited to python programming language version 3 7 pytorch numpy scipy fiona qgis netcdf4 geopandas and geospatial data abstraction library gdal we would like to thank the developers and the open source community for all contributions appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105112 
