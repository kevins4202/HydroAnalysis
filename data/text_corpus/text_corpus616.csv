index,text
3080,in an open channel an emergent canopy of vegetation enhances local resistance producing a complicated velocity distribution within and around the canopy laboratory experiments were performed to understand the flow velocity variations in a partially vegetated channel revealing both streamwise and lateral velocity variations as the flow entered the canopy an analytical model was proposed to predict the lateral velocity profiles across the vegetated region and bare channel in both the flow adjustment region and the fully developed flow region the flow velocity variations in the streamwise and lateral directions were parameterized and included in the new proposed model allowing the model to capture both the streamwise and the lateral velocity variations twenty sets of experimental data from our experiments and data from published studies were used to verify the new model the predicted velocity profiles exhibited good agreement with the measurements under a wide range of vegetation and flow conditions suggesting that the proposed model is capable of accurately predicting the velocity profiles in a partially vegetated channel finally the proposed model was applied to evaluate the critical frontal area per canopy volume at which kelvin helmholtz vortices form along the emergent canopy side edge with the same spatially averaged velocity the critical frontal area exhibited no dependence on stem diameter keywords analytical model velocity profiles partially vegetated channel kelvin helmholtz vortices data availability data will be made available on request notation the following symbols are used in this paper a 1 a 2 a 3 and a 4 integration constants in eqs 14 and 15 a frontal area per canopy volume acri critical frontal area per canopy volume b half canopy width model canopy located along the flume centerline or canopy width model canopy located along the flume sidewall b channel width cd drag force coefficient d cylinder diameter dx dy longitudinal and lateral spacings between two neighboring cylinders respectively fr froude number f darcy weisbach friction factor fx drag force in the streamwise direction g gravitational acceleration h flow depth kveg kbare advection parameters in the vegetated region and bare channel respectively kx velocity variation in the longitudinal x direction i e the x component of the advection parameter ky velocity variation in the lateral y direction i e the y component of the advection parameter l length of the model canopy li flow adjustment distance in the vegetated region or bare channel li m li p flow adjustment distances determined by the measured velocities and predicted by eq 1 respectively n areal density of cylinder nc manning s parameter ncri critical cylinder density n number of measurements p fluid pressure r hydraulic radius re reynolds number rep stem reynolds number rmse root mean square error s slope of the water surface u v w velocity components in the x y and z directions respectively ud depth averaged velocity u 0 mean channel velocity uveg 0 mean velocity at the upstream edge of the vegetation canopy uveg f mean velocity in the fully developed flow region ud m ud p measured and predicted velocities respectively ud 1 ud 2 velocities predicted by the analytical model in the vegetated region and bare channel respectively uveg ubare mean velocities in the vegetated region and bare channel respectively u1 u2 steady velocities in the vegetated region and bare channel respectively u shear velocity x y z streamwise lateral and vertical directions respectively λ lateral dimensionless eddy viscosity ε local eddy viscosity ε depth averaged eddy viscosity ν kinematic viscosity κ von karman constant ϕ solid volume fraction τ reynolds shear stress τ b bed shear stress ρ flow density θ θp velocity differences estimated based on the measurements and predicted velocities respectively θc critical velocity difference for the formation of kh vortices 0 4 1 introduction aquatic vegetation is widely distributed in natural rivers wetlands and marsh regions and plays an important role in natural hydrologic systems thus influencing flow nutrients and sediment transport bouma et al 2007 chembolu et al 2019 huai et al 2021 liu et al 2016 2018a marjoribanks et al 2019 ortiz et al 2013 the lengths and widths of vegetation canopies in rivers are often on the order of meters biggs et al 2019 cornacchia et al 2018 licci et al 2019 liu et al 2018b sand jensen and pedersen 2008 schoelynck 2012 shan et al 2017 2018 siniscalchi et al 2012 temmerman et al 2005 as flow enters an emergent vegetation canopy the flow along the canopy leading edge is adjusted and the flow velocity is modified in both the streamwise and the lateral directions e g liu and shan 2019 zong and nepf 2010 the flow adjustment distance within the canopy li is defined as the distance between the canopy leading edge x 0 and the position in the streamwise stream parallel direction at which the flow velocity decreases to a constant on each cross stream transect orthogonal to the streamwise direction the velocity relative to the mean channel velocity is diminished inside the canopy but is enhanced in the adjacent bare channel e g dieter 1990 rominger and nepf 2011 liu et al 2020 as a consequence of the exchanges of mass and momentum between the fast flow in the bare channel and the slow flow inside the canopy the velocity profiles inside and outside such an emergent canopy are complicated which impacts sediment transport and in turn influences the evolution of vegetated landscapes shan et al 2020 vandenbruwaene et al 2011 bertagni et al 2018 therefore it is important to understand the complex velocity variations in partially vegetated channels previous studies have experimentally investigated the velocity distribution within and around an emergent canopy in an open channel huai et al 2016 liu et al 2020 liu and shan 2019 2022 maji et al 2016 rominger and nepf 2011 yan et al 2020 for an emergent array of circular cylinders centered in a channel rominger and nepf 2011 reported that the flow adjustment distance over which the velocity decreases to a constant in the streamwise direction li is related to the drag coefficient of the cylinders cd the frontal area per canopy volume a nd where n is the areal density of cylinders and d is the cylinder diameter and the half patch width b for a patch along the sidewall b becomes the full patch width fig 1 beyond x li the flow is fully developed and the velocity is constant in the downstream direction accordingly rominger and nepf 2011 proposed formulas to predict the flow adjustment distance li for low and high density canopies 1a for low density c d a b 1 l i 3 0 0 3 2 c d a 1 c d a b 2 1b for high density c d a b 1 l i 5 5 0 4 2 c d a 2 b 2 liu et al 2020 confirmed that the flow adjustment distance inside a canopy is the same as that in the adjacent bare channel see fig 4 in their paper thus eq 1 is valid for predicting the flow adjustment distance both inside a canopy and in the adjacent bare channel inside an emergent canopy the vegetated region in fig 1 liu et al 2020 proposed an exponential model to predict the longitudinal profile of the spatially averaged velocity 2 u veg u v e g f u veg 0 u v e g f e 3 x l i where u veg f 8 g s h 4 c d a h f is the spatially averaged velocity in the fully developed flow region x li in which f is the darcy weisbach friction factor s is the slope of water surface g is the gravitational acceleration and h is the flow depth u 0 8 g s h f is the mean channel velocity and uveg 0 is the mean velocity at the leading edge of the canopy for a canopy with cdab 0 to 9 uveg 0 can be estimated using the following equation proposed by liu and shan 2019 3 u veg 0 u 0 1 0 15 0 02 c d a b using eqs 2 and 3 the longitudinal profile of the spatially averaged velocity in the vegetated region can be predicted based on the flow continuity the longitudinal profile of the spatially averaged velocity in the bare channel u bare can also be predicted liu et al 2020 yan et al 2020 4 u bare b u 0 b u veg b b by ignoring the velocity variation in the lateral direction eqs 2 to 4 constitute a valid method for predicting the longitudinal velocity profiles in the vegetated region and bare channel however these equations cannot predict the lateral velocity profile at each x position moreover by ignoring the longitudinal velocity variation some scholars have proposed analytical models for predicting the lateral velocity profile only in the fully developed flow region while assuming that the flow velocity does not vary in the streamwise direction e g white and nepf 2007 huai et al 2008 chen et al 2010 zhang et al 2018 2021 nevertheless as flow enters a canopy the flow velocity varies in both the longitudinal and the lateral directions resulting in a complicated velocity distribution particularly in the flow adjustment region 0 x li hence a model considering the velocity variation in only the longitudinal direction or only the lateral direction cannot capture the velocity distribution and thus cannot predict the velocity distribution within and around a canopy in summary a model that is capable of predicting the velocity distribution in a partially vegetated channel has not yet to be developed to complement previous research and resolve this problem this study was to propose an analytical model to predict the lateral profiles of velocity at different streamwise positions in a partially vegetated channel specifically the new model parameterizes the velocity variations in both the longitudinal and the lateral directions 2 experimental methods two groups of experiments were performed fig 1 for the first group cases b1 to b3 the flume was 23 m long 2 m wide and 0 5 m high with a 15 m long test section located 3 m downstream from the flume inlet the vegetation canopy was placed in the middle of the test section and the streamwise canopy lengths in cases b1 b2 and b3 were 450 400 and 300 cm respectively the half canopy width was b 30 to 40 cm and the half flume width was b 100 cm leading to a channel blockage ratio of b b 0 3 to 0 4 which is consistent with the ratio b b 0 5 observed for natural rivers e g yamasaki et al 2019 two rulers were vertically fixed at distances of 0 5 m in front of and 0 2 m behind the canopy to measure the flow depths at those locations for all three cases the flow depth was h 18 cm and the mean channel velocity was u 0 18 cm s the flow was turbulent with a reynolds number of re u 0 r ν 28 000 where r is the hydraulic radius and ν 0 01 cm2 s is the kinematic viscosity the flow was subcritical based on a froude number of fr u 0 g h 0 14 for the second group cases d1 to d3 the vegetation canopy was situated along the sidewall of the channel and the canopy lengths in cases d1 d2 and d3 were 450 450 and 300 cm respectively the flume was 13 m long 1 m wide and 0 5 m high with a 7 m long test section located 3 m downstream from the flume inlet the full canopy width was b 33 cm and the full channel width was b 100 cm resulting in a channel blockage ratio of b b 0 33 because the channel blockage ratios of the two groups b1 to b3 and d1 to d3 were approximately the same the velocity differences between the two groups of cases i e centrally situated and side situated canopies can be compared in addition two rulers were again vertically fixed at distances of 0 5 m in front of and 0 2 m behind the canopy to measure the flow depths for all three cases the flow depth was h 20 cm and the mean channel velocity was u 0 17 20 cm s the flow was turbulent with the reynolds number ranging from re 23 000 27 000 and subcritical with the froude number ranging from fr 0 12 0 14 for both channels the distance between the flume inlet and the leading edge of the test section was 3 m preliminary experiments were conducted to measure the longitudinal velocity profile along the centerline of each flume the velocity was laterally uniform prior to entering the test section and remained constant in the longitudinal direction within the test section indicating that the flow entering each canopy was fully developed when an emergent canopy was placed in the channel the flow was adjusted along the canopy leading edge causing the flow depth within and around the canopy to change slightly according to our observations the flow depth inside and outside each canopy was altered by less than 5 mm this variation in flow depth 5 mm was then used to calculate the variation in water surface slope in the test section yielding a 1 water surface slope change in cases b1 b3 and d1 d3 which contributed to a 1 2 velocity change these findings indicate that the changes in the flow depth and velocity over the test section were negligible in all six cases b1 b3 and d1 d3 circular cylinders were deployed in a staggered arrangement to construct the model canopies fig 2 the cylinders were rigid and did not deform the characteristics of the cylinders are similar to those of cattails tree bases and mangrove roots abood 2011 shan et al 2019 yang et al 2007 coon et al 2000 the cylinder diameter d 0 8 cm was chosen to fall within the range d 0 2 1 2 cm of stem diameters of plants observed in wetlands and river floodplains lightbody and nepf 2006 leonard and luther 1995 yang et al 2007 liu et al 2013 in each case the longitudinal and lateral spacings between two neighboring cylinders were dx and dy respectively fig 1c and 1d and the values of which are summarized in table 1 for all six cases the cylinder density was n 0 03 0 15 cm 2 producing a frontal area per canopy volume of a nd 0 024 0 12 cm 1 and the solid volume fraction was ϕ π 4 n d 2 0 015 0 076 for the six cases see table 1 the canopy was emergent in all cases the longitudinal lateral and vertical directions were defined as x y and z respectively specifically in both flumes x 0 and z 0 indicate the upstream edge of the canopy and the channel bed surface respectively for the centrally situated canopy cases b1 to b3 y 0 indicates the flume centerline fig 1c for the side situated canopy cases d1 to d3 y 0 indicates the right sidewall of the flume fig 1d velocities were recorded using a sontek acoustic doppler velocimeter adv in cases b1 to b3 and a nortek adv in cases d1 to d3 first the vertical velocity profiles both inside and outside each canopy were measured to examine whether the depth averaged velocity was the same as the mid depth velocity for example in each side situated canopy case the vertical velocity profiles were measured at some positions inside the canopy x 100 cm and y 16 cm in case d1 x 120 and 380 cm and y 16 cm in case d2 and x 80 and 270 cm and y 16 cm in case d3 and outside the canopy x 100 cm and y 65 cm in case d1 x 120 and 380 cm and y 65 cm in case d2 and x 80 and 270 cm and y 65 cm in case d3 these profiles were used to calculate the depth averaged velocity ud the difference between ud and the mid depth z h 2 velocity u was 3 on average thus the flow velocities were measured at only the mid depth position in each case and the measured value was adopted as the depth averaged velocity in addition once ud was confirmed to be the same as the mid depth velocity the velocity was measured along lateral mid depth transects at different x positions inside each model canopy a characteristic region between two neighboring cylinders the dashed boxes in fig 1c and 1d was defined to account for the effect of spatial flow heterogeneity between cylinders on the measurements accordingly the flow velocities were measured at y dy 4 and dy 2 and the mean of these two velocities differed from the spatially averaged velocity in the characteristic region by less than 12 thus we measured the mid depth velocities at y dy 4 and dy 2 in each characteristic region and took the mean of the two velocities as the spatially averaged velocity at each y position across the canopy outside the canopy velocities were measured only once at each mid depth position measurement transects were established in both the flow adjustment region 0 x li and the fully developed flow region li x l specifically for the centrally situated canopy cases b1 to b3 lateral velocity profiles were measured at x 50 100 150 200 250 300 400 and 420 cm for case b1 and at x 50 100 150 200 250 and 300 cm for cases b2 and b3 for cases b1 b3 the lateral interval between two neighboring measurement points was 10 cm between y 0 and 90 cm on each transect note that for the centrally situated canopy cases the lateral velocity profiles were symmetric about the centerline y 0 which was confirmed by the preliminary experiment so the lateral velocity profiles were measured only on the left side of the flume 0 y b for the side situated canopy cases d1 to d3 lateral velocity profiles were measured at x 50 100 150 200 250 300 350 400 and 450 cm for cases d1 and d2 and at x 50 100 150 200 250 and 300 cm for case d3 for cases d1 d3 the lateral interval between two neighboring measurement points was 5 cm between y 25 and 60 cm and 10 cm between y 5 25 cm and y 60 90 cm on each transect at each position the measurement period was 150 s frequency of 50 hz the raw values with correlations below 70 and a signal to noise ratio smaller than 15 db were filtered and then despiked using the acceleration threshold method proposed by goring and nikora 2002 a matlab code was used to decompose the remaining instantaneous velocities into time averaged velocities u v and w the spatially averaged velocity in the vegetated region u veg 0 y b and that in the bare channel u bare b y b are defined as 5a u v e g 1 b 0 b u d 1 d y 5b u b a r e 1 b b b b u d 2 d y where u d 1 and u d 2 are the velocities in the vegetated region and bare channel respectively to compare the differences between the predicted and measured velocities the root mean square error rmse was computed as follows 6 rmse 1 n i 1 n u d m u d p 2 where n is the number of measurements and predictions in each case u d m is the measured velocity and u d p is the predicted velocity 3 experimental results fig 3 shows a comparison of the lateral velocity profiles at different streamwise positions in case d1 ϕ 0 025 and case d3 ϕ 0 076 the flow adjustment distance li was determined by taking measurements inside the canopy specifically li was defined as the distance between the canopy leading edge x 0 and the position at which the velocity became constant i e the velocity varied by less than 5 for each case li was also estimated using eq 1 among the six cases the difference between the measured and predicted values was less than 16 table 1 thus it is reasonable to use eq 1 to predict li in the following calculations the dense case d3 produced a shorter flow adjustment distance li 200 20 cm than did the sparse case d1 li 330 30 cm in the flow adjustment region 0 x li the velocity varied in both the longitudinal and the lateral directions fig 3a and 3b specifically as the distance from the canopy leading edge increased increasing x the velocity decreased inside the canopy but increased in the bare channel reflecting the velocity variation in the longitudinal direction likewise on each transect the velocity increased from the vegetated region to the bare channel displaying the velocity variation in the lateral direction fig 3c and 3d beyond x li the flow was fully developed and the velocity variation in the lateral direction existed only in the fully developed flow region the lateral velocity profiles remained the same as the distance from the canopy leading edge increased triangles in fig 3c and 3d these observations suggest that the velocity profile in the flow adjustment region is more complicated than that in the fully developed flow region because the velocities in the former vary both longitudinally and laterally white and nepf 2008 meftah and mossa 2016 to predict the velocity profiles through a partially vegetated channel then velocity variations in both the longitudinal and the lateral directions were considered in the newly proposed model see section 4 4 theory 4 1 analytical model for a channel with an emergent canopy the momentum equation is as follows 7 ρ f x τ xx x τ yx y τ zx z ρ u t u x u u y v u z w where ρ is the flow density τ xx is the normal stress τ yx and τ zx are the shear stresses and u v and w are the time averaged velocity components corresponding to the x y and z directions respectively for steady flows u t 0 f x is the vegetation drag force in the streamwise direction and is defined as 8 f x 1 2 c d a u 2 the normal stress τ xx is defined as 9 τ xx ρ g h 2 ρ ε u x where ε is the local eddy viscosity the flow continuity equation is 10 u x v y w z 0 by combining eqs 7 to 10 one can obtain 11 ρ g s x 2 ρ ε u x τ yx y τ zx z 1 2 ρ c d a u 2 ρ u 2 x u v y u w z integrating eq 11 over the flow depth can yield the depth averaged governing equation by combining the following four assumptions first the flow conditions in a vegetated channel are assumed to be uniform second the shear stress acting on the x y plane is expressed as 1 h 0 h τ yx d z ρ h ε u d y shiono and knight 1991 where ε λ h u is the depth averaged eddy viscosity λ is the dimensionless eddy viscosity u f 8 u d is the shear velocity and f is the darcy weisbach friction factor third the shear stress acting on the x z plane is τ zx 0 n cm2 at the water surface z h and that acting on the x z plane is equal to the bed shear stress τ b ρ u 2 at the channel bed z 0 i e 0 h τ zx d z τ b fourth at the water surface z h and channel bed z 0 the velocity in the vertical direction is w 0 cm s thus 0 h u w z d z 0 based on the above assumptions the depth averaged governing equation eq 12 is obtained by integrating eq 11 over the flow depth 12 gsh 1 2 λ h 2 f 8 2 u d 2 y 2 1 2 c d a h f 8 u d 2 k the advection parameter k in eq 12 is defined as 13 k h u d 2 x h 2 λ f 8 2 u d x 2 k x h u v d y k y where kveg and kbare are the spatially averaged advection parameters in the vegetated region and bare channel respectively at each x position kveg in the vegetated region and kbare in the bare channel were considered constant in each region in the flow adjustment region 0 x li eq 13 includes the velocity variation in the longitudinal direction k x h u d 2 x ρ h 2 λ f 8 2 u d x 2 and the velocity variation flow deflection in the lateral direction k y h u v d y specifically kx represents the velocity decrease in the vegetated region or the velocity increase in the adjacent bare channel and ky represents the intensity of the lateral flow deflection from the vegetated region to the bare channel see the discussion in section 9 2 some existing models consider only the longitudinal velocity variation thereby neglecting k y h u v d y e g liu et al 2020 yan et al 2020 in the fully developed flow region li x l eq 13 can be simplified to k h u v d y due to the absence of velocity variation in the longitudinal direction i e x 0 as in some previous models e g white and nepf 2007 huai et al 2008 chen et al 2010 zhang et al 2018 2021 we reiterate that our proposed model considers the velocity variations in both the longitudinal and the lateral directions allowing the model to capture the flow pattern and accurately predict the velocity distribution see section 6 in the vegetated region 0 x b the analytical solution for u d 1 is 14 u d 1 a 1 e r 1 y a 2 e r 2 y 8 h g s k veg 4 c d a h f where r 1 2 1 2 h 1 λ 8 f 1 2 4 c d a h f and a 1 and a 2 are unknown constants in the bare channel b x b the analytical solution for u d 2 is 15 u d 2 a 3 e r 3 y a 4 e r 4 y 8 h g s k bare f where r 3 4 1 2 h 1 λ 8 f 1 2 f and a 3 and a 4 are unknown constants 4 2 boundary conditions the four unknown constants a 1 a 2 a 3 and a 4 in eqs 14 and 15 are determined using four boundary conditions first the velocity and velocity gradient are continuous at the interface between the vegetated region and bare channel yielding two boundary conditions second for the centrally situated canopy the velocity gradient at the canopy centerline must be zero due to flow symmetry which provides the third boundary condition finally our measurements confirmed that the sidewall impacted the velocity only within 5 cm from the wall thus the velocity at y 0 95b with b 1 m reached a constant in the lateral direction fig 3 and the velocity gradient at y 0 95b was considered zero providing the fourth boundary condition these four conditions are summarized as follows 1 u d 1 u d 2 at y b 2 u d 1 y u d 2 y at y b 3 u d 1 y 0 at y 0 4 u d 2 y 0 at y 0 95b 4 3 determining the parameters the drag coefficient cd was estimated from the following equation of white 1991 16 c d 1 10 r e p 2 3 where r e p u d ν is the local stem reynolds number in the vegetated region based on eq 16 and the measured velocities the calculated rep from cases b1 to b3 and d1 to d3 the cd value ranges between 1 1 and 1 5 thus cd 1 and 1 5 were inserted into eq 1 to predict li when cd changed the other parameters in eq 1 were fixed among the six cases the average difference among the predicted li using cd 1 and 1 5 was 18 thus for simplicity it is reasonable to use cd 1 for these calculations the darcy weisbach friction factor f was estimated from f 8 g n c 2 r 1 3 knight et al 2007 huai et al 2008 where nc is manning s parameter yan et al 2020 reported nc 0 013 for pvc baseboards the dimensionless eddy viscosity coefficient λ was calculated from λ κ 6 where κ 0 4 is the von karman constant based on previous studies tang and knight 2008 abril and knight 2004 λ 0 067 was used in this study the advection parameters in the vegetated region kveg and bare channel kbare were determined via the following steps for each transect the spatially averaged velocities in the vegetated region and bare channel u veg and u bare were predicted from eqs 2 and 4 respectively the predicted u veg and u bare were then used to determine kveg and kbare that is kveg and kbare were determined by inserting eqs 14 and 15 into eq 5 5 published data the experimental data from this study and published studies devi et al 2019 zong and nepf 2010 caroppi et al 2020 were used to verify the proposed model over ranges of the solid volume fraction ϕ 0 001 0 02 canopy width to channel width ratio b b 0 3 0 5 and mean channel velocity u 0 5 0 71 4 cm s these data were chosen because velocity profiles were measured in the flow adjustment region and fully developed flow region the details of the published data were reported in each study for convenience the experimental parameters adopted in these previous studies are briefly introduced below and summarized in table 2 1 devi et al 2019 performed experiments in a 20 m long and 1 m wide flume with a 5 m long and 0 5 m wide canopy the canopy was constructed of natural rice stems o sativa with a stem diameter of d 0 2 0 1 cm and the frontal area was a 0 06 0 03 cm 1 yielding a solid volume fraction of ϕ 0 010 0 006 the flow depth was h 12 cm and the mean channel velocity was u 0 27 cm s velocity profiles were measured in the flow adjustment region x 0 and 250 cm and fully developed flow region x 500 cm 2 zong and nepf 2011 performed experiments in a 16 m long and 1 2 m wide flume with a 10 m long and 0 4 m wide model canopy the canopy was constructed of cylinders with a diameter of d 0 6 cm the flow depth was h 12 14 cm and the mean channel velocity was u 0 5 0 11 6 cm s two canopies with a 0 04 and 0 21 cm 1 were constructed corresponding to ϕ 0 02 and 0 1 respectively velocity profiles were measured in the fully developed flow region 3 caroppi et al 2020 performed experiments in a 0 4 m wide flume with an 8 m long and 0 16 m wide vegetation canopy the canopy was constructed of cylinders with a diameter of d 0 45 cm the flow depth was h 11 1 13 6 cm and the mean channel velocity was u 0 54 4 71 4 cm s canopies with a 0 0045 0 072 cm 1 corresponding to ϕ 0 001 0 025 respectively were constructed velocity profiles were measured in the fully developed flow region 6 model validation first the model was verified in the centrally situated canopy cases cases b1 to b3 u 0 18 cm s ϕ 0 015 to 0 045 s 0 6 10 4 f 0 025 and in the side situated canopy cases cases d1 to d3 u 0 17 to 20 cm s ϕ 0 025 to 0 076 s 0 4 10 4 to 0 6 10 4 f 0 025 at each position the velocity ud was normalized by the mean channel velocity u0 figs 4 and 5 confirm that the predicted velocity profiles black lines exhibit good agreement with the measurements symbols in both the flow adjustment region 0 x li and the fully developed flow region li x l this finding is supported by an rmse u0 10 6 14 9 next the data from devi et al 2019 were used to verify the new model for a channel with an o sativa canopy ϕ 0 01 the water surface slope and the darcy weisbach friction factor were s 0 0015 and f 0 15 respectively and the predicted velocity profiles black lines matched the measurements points in both the flow adjustment region x 0 and 250 cm and the fully developed flow region x 500 cm fig 6 with an rmse u0 of 20 9 finally the model was verified in the fully developed flow region using the experimental data from zong and nepf 2011 and caroppi et al 2020 the predicted velocity profiles of the new model showed excellent agreement with the measurements over wide ranges of mean channel velocities u 0 5 0 71 4 cm s solid volume fractions ϕ 0 002 0 1 and water surface slopes s 0 1 10 4 to 0 5 10 4 and a darcy weisbach friction factor of f 0 048 figs 7 and 8 the rmse u0 between the predicted and measured velocities varied from 10 to 21 the above comparison suggests that the proposed model is capable of predicting velocity profiles through a partially vegetated channel in particular the model accurately captured not only the longitudinal and lateral velocity variations in the flow adjustment region but also the lateral velocity variation in the fully developed flow region 7 model application the proposed model was applied to evaluate whether kelvin helmholtz kh vortices formed along the canopy side edge y b kh vortices started to form along the canopy side edge when the horizontal shear was sufficiently strong caroppi et al 2019 zong and nepf 2011 caroppi et al 2020 defined a nondimensional parameter θ to describe the horizontal shear intensity 17 θ u 2 u 1 u 2 u 1 where u 1 and u 2 are the steady flow velocities in the vegetated region and bare channel respectively see fig 8a for emergent canopies caroppi et al 2020 performed velocity spectrum analyses to confirm that kh vortices occur along the canopy side edge when θ surpasses a given threshold θc 0 4 this finding was supported by zong and nepf 2011 who observed kh vortices at θ 0 86 0 98 0 4 herein we discuss only the case with a canopy longer than the flow development distance l li so that the fully developed flow produces horizontal shear that is sufficiently strong to produce kh vortices by combining the new model with eq 17 the steady velocities in the vegetated region and bare channel u 1 and u 2 respectively can be predicted and used to determine the predicted θ denoted θp in the following text using the data from caroppi et al 2019 and zong and nepf 2011 the predicted value θp was compared to the measured values θ and the difference between θp and θ was less than 7 on average fig 9 this result suggests that the new model can accurately predict θp by comparing the predicted θp values with the selected threshold θc 0 4 the presence and absence of kh vortices can be evaluated based on θp θc and θp θc respectively many species of natural aquatic plants exist and each has a specific range of stem diameters liu et al 2021 the new model was further applied to determine whether the stem diameter impacts the horizontal shear intensity described by θp to verify this influence case 9 from caroppi et al 2020 was used as an example with a flow depth of h 12 cm and a channel mean velocity of u0 52 cm s four stem diameters d 0 2 0 45 0 6 and 1 2 cm were selected based on field observations d 0 2 1 2 cm representing four different plant species for each stem diameter the proposed model predicted θp values for plant densities of n 0 0005 1 cm 2 fig 10 a the horizontal dashed line is the threshold θc 0 4 for the formation of kh vortices fig 10a suggests that the critical density ncri for the formation of kh vortices increased as the stem diameter d decreased this result can be explained by the force balance within an emergent canopy eq 18 in the fully developed flow region the flow pressure balances the vegetation and bed drag forces 18 gsh f 8 u 1 2 1 2 c d n cri d h u 1 2 where ncri is the critical density of vegetation for the presence of kh vortices along the side edge of a canopy under the same flow conditions and bed roughness the flow pressure gsh and bed drag f 8 u 1 2 remained the same such that the drag force 1 2 c d n cri d h u 1 2 remained the same and ncrid was constant as a result a smaller stem diameter d corresponded to a higher critical density ncri fig 10a using the stem diameter d and the critical density ncri predicted by the new model the critical frontal area per canopy volume acri ncri d was determined for different stem diameters fig 10b fig 10b shows that acri was not dependent on stem diameter and the mean critical frontal area per canopy volume was acri 0 008 cm 1 under the conditions mentioned above h 12 cm and u0 52 cm s kh vortices formed at the frontal area per canopy volume a 0 008 cm 1 thus the critical frontal area required to form kh vortices depends on the flow conditions see eq 18 8 sensitivity analysis 8 1 drag coefficient cd the influence of the drag coefficient cd on the prediction was examined cd was estimated from eq 16 and u veg decreased over the flow adjustment region producing an increase in rep in this region based on the measured velocity inside the canopy in all six cases in table 1 eq 16 produced cd 1 1 1 5 case b1 u0 18 cm s h 18 cm and ϕ 0 015 was taken as an example to examine the influence of cd on the predicted velocity profiles in case b1 cd 1 1 1 4 then a wider range of cd 1 2 was considered and the other parameters were fixed as specified in tables 1 and 3 fig 11 shows that cd had a negligible influence on the predicted velocities suggesting that the proposed model is not sensitive to cd for simplicity it is reasonable to use cd 1 predicting flow velocities 8 2 advection parameters kveg and kbare furthermore the influences of the two advection parameters k veg and k bare on the predicted velocity profiles were investigated and case b1 was again taken as an example in the flow adjustment region x 200 cm the predicted velocity profiles were compared using different combinations of kveg and kbare fig 12 three combinations of k veg and kbare were considered while the other parameters remained fixed as specified in table 1 each combination kveg and kbare was determined using the method in section 4 3 fig 12 shows that the predicted velocity inside the canopy increased as kveg decreased from 0 0035 to 0 02 and similarly the velocity in the bare channel increased as kbare decreased from 0 0006 to 0 0013 due to the flow continuity constraint eq 4 the combinations of k veg and k bare produced the opposite tendency of predicted velocities between the vegetated region y b 0 1 and the bare channel y b 1 2 5 specifically an increase in the predicted velocity in the vegetated region corresponded to a decrease in the predicted velocity in the bare channel to further examine the trends of the two parameters inside and outside the canopy kveg and kbare were plotted against the x position and the values were normalized by the flow adjustment distance li fig 13 for each case the maximum absolute value of kveg and the minimum absolute value of kbare occurred at the leading canopy edge as the distance from the leading edge increased x li increased the absolute value of kveg decreased and the absolute value of kbare increased in the flow adjustment region 0 x li 1 in the fully developed flow region x li 1 kveg and kbare both became constant the values of kveg and kbare in the cases from devi et al 2019 exhibited a similar trend see table 4 kveg and kbare varied in the flow adjustment region because these parameters were influenced therein by the velocity variations in both the longitudinal and the lateral directions eq 13 this point reiterates the importance of considering the velocity variations in both the longitudinal and the lateral directions in the proposed new model 9 discussion 9 1 influences of the advection parameter on the predictions the advection parameter k includes both x direction kx and y direction ky components eq 13 i e k kx ky to examine the influence of either or both components kx and ky on the predicted velocities the predictions using four different k values were compared kveg and kbare represented the k value in the vegetated region and the bare channel respectively because the x direction component is negligible i e kx 0 in the fully developed flow region we took velocity measurements at x 50 cm from the flow adjustment region li 330 30 cm in case d1 as an example first we ignored both the x and the y components of the k value kx 0 and ky 0 so that k 0 the predictions were smaller than the measurements across both the vegetated region and the bare channel orange line in fig 14 indicating that the velocity variations in the longitudinal and lateral directions cannot be ignored second only the flow variation in the longitudinal direction was considered i e k kx inappropriate k values produced negative values in the square root of the analytical solution eqs 14 and 15 and thus the predicted velocities could not be obtained further verifying that the lateral velocity variation in the flow adjustment region cannot be ignored third only the flow variation in the lateral direction was considered i e k ky the longitudinal velocity variation was ignored kx 0 so the k values in the vegetated region and bare channel were the same as those in the fully developed flow region as expected compared to the predictions obtained using appropriate k values black line in fig 14 the predicted velocities were underestimated in the vegetated region and overestimated in the bare channel blue line in fig 14 suggesting that the longitudinal velocity variation kx cannot be ignored in the flow adjustment region in summary velocities can be accurately predicted only if the flow variations in both the longitudinal and the lateral directions are considered as the black line shown in fig 14 9 2 physical meanings of kx and ky for the advection parameter the x kx and y ky components were plotted against the x position that was normalized by the flow adjustment distance li fig 15 based on the measurements from cases b1 to b3 and d1 to d3 where the subscripts veg and bare of kx and ky indicate the parameters in the vegetated region and bare channel respectively x li 1 and 1 indicate the flow adjustment region and fully developed flow region respectively kx veg and kx bare were determined by eq 13 using the longitudinal profiles of measured velocities in the vegetated region and bare channel and for each case kveg and kbare were determined by the method in section 4 3 table 3 ultimately ky veg kveg kx veg and ky bare kbare kx bare were determined first the value of kx represents the velocity variation in the longitudinal direction and negative and positive signs of kx signify decreasing and increasing velocities respectively in the x direction for example in the vegetated region the flow velocity decreased exponentially eq 2 with increasing distance from the canopy leading edge x 0 in the flow adjustment region x li 1 thus the velocity gradient u veg x was negative and the largest velocity variation the largest negative kx veg occurred at the canopy leading edge open symbols in fig 15a the velocity became constant in the fully developed flow region x li 1 yielding u veg x 0 i e kx veg 0 in the bare channel because the flow was continuous the greatest velocity increase was observed close to the canopy leading edge resulting in the largest positive kx bare solid symbols in fig 15a in contrast with increasing distance from the canopy leading edge kx bare decreased in the flow adjustment region x li 1 and dropped to zero in the fully developed flow region x li 1 second the value of ky represents the velocity variation in the lateral direction which is associated with lateral flow deflection in the flow adjustment region x li 1 the flow was deflected laterally toward the bare channel by the drag induced by the vegetated region hence the lateral velocity gradient ky was negative in both regions the strongest lateral flow deflection occurred at the canopy leading edge x 0 where the largest negative ky was observed in the fully developed flow region x li 1 the lateral flow was not deflected yielding ky 0 in both the vegetated region and the bare channel fig 15b 10 conclusion herein laboratory experiments clarified that under unidirectional flows the flow velocities both inside and outside a canopy varied in both the longitudinal and the lateral directions accordingly this study proposed an analytical model for predicting the lateral flow velocity profiles through a partially vegetated channel the velocity variations in both the longitudinal and the lateral directions were parameterized and included in the new model which was then verified using twenty groups of experimental data covering a wide range of vegetation and flow conditions the predicted velocity distributions were in good agreement with the measurements indicating that our model is capable of accurately predicting the velocity distribution in a partially vegetated channel finally the model was applied to evaluate the critical frontal area per canopy volume at which kh vortices begin to form along the canopy side edge at the same spatially averaged velocity the critical frontal area required for the formation of kh vortices was independent of stem diameter credit authorship contribution statement chunhao yan investigation formal analysis software writing original draft yuqi shan validation writing review editing chao liu conceptualization methodology writing review editing supervision project administration xingnian liu supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study received financial support from the national natural science foundation of china nos 52022063 52179074 and u2040219 and the sichuan science and technology program 2021yfh0028 chunhao yan was supported by postdoctoral funding from chongqing jiaotong university 307696 the authors appreciate the editor s and reviewers constructive suggestions which significantly improved the quality of the paper 
3080,in an open channel an emergent canopy of vegetation enhances local resistance producing a complicated velocity distribution within and around the canopy laboratory experiments were performed to understand the flow velocity variations in a partially vegetated channel revealing both streamwise and lateral velocity variations as the flow entered the canopy an analytical model was proposed to predict the lateral velocity profiles across the vegetated region and bare channel in both the flow adjustment region and the fully developed flow region the flow velocity variations in the streamwise and lateral directions were parameterized and included in the new proposed model allowing the model to capture both the streamwise and the lateral velocity variations twenty sets of experimental data from our experiments and data from published studies were used to verify the new model the predicted velocity profiles exhibited good agreement with the measurements under a wide range of vegetation and flow conditions suggesting that the proposed model is capable of accurately predicting the velocity profiles in a partially vegetated channel finally the proposed model was applied to evaluate the critical frontal area per canopy volume at which kelvin helmholtz vortices form along the emergent canopy side edge with the same spatially averaged velocity the critical frontal area exhibited no dependence on stem diameter keywords analytical model velocity profiles partially vegetated channel kelvin helmholtz vortices data availability data will be made available on request notation the following symbols are used in this paper a 1 a 2 a 3 and a 4 integration constants in eqs 14 and 15 a frontal area per canopy volume acri critical frontal area per canopy volume b half canopy width model canopy located along the flume centerline or canopy width model canopy located along the flume sidewall b channel width cd drag force coefficient d cylinder diameter dx dy longitudinal and lateral spacings between two neighboring cylinders respectively fr froude number f darcy weisbach friction factor fx drag force in the streamwise direction g gravitational acceleration h flow depth kveg kbare advection parameters in the vegetated region and bare channel respectively kx velocity variation in the longitudinal x direction i e the x component of the advection parameter ky velocity variation in the lateral y direction i e the y component of the advection parameter l length of the model canopy li flow adjustment distance in the vegetated region or bare channel li m li p flow adjustment distances determined by the measured velocities and predicted by eq 1 respectively n areal density of cylinder nc manning s parameter ncri critical cylinder density n number of measurements p fluid pressure r hydraulic radius re reynolds number rep stem reynolds number rmse root mean square error s slope of the water surface u v w velocity components in the x y and z directions respectively ud depth averaged velocity u 0 mean channel velocity uveg 0 mean velocity at the upstream edge of the vegetation canopy uveg f mean velocity in the fully developed flow region ud m ud p measured and predicted velocities respectively ud 1 ud 2 velocities predicted by the analytical model in the vegetated region and bare channel respectively uveg ubare mean velocities in the vegetated region and bare channel respectively u1 u2 steady velocities in the vegetated region and bare channel respectively u shear velocity x y z streamwise lateral and vertical directions respectively λ lateral dimensionless eddy viscosity ε local eddy viscosity ε depth averaged eddy viscosity ν kinematic viscosity κ von karman constant ϕ solid volume fraction τ reynolds shear stress τ b bed shear stress ρ flow density θ θp velocity differences estimated based on the measurements and predicted velocities respectively θc critical velocity difference for the formation of kh vortices 0 4 1 introduction aquatic vegetation is widely distributed in natural rivers wetlands and marsh regions and plays an important role in natural hydrologic systems thus influencing flow nutrients and sediment transport bouma et al 2007 chembolu et al 2019 huai et al 2021 liu et al 2016 2018a marjoribanks et al 2019 ortiz et al 2013 the lengths and widths of vegetation canopies in rivers are often on the order of meters biggs et al 2019 cornacchia et al 2018 licci et al 2019 liu et al 2018b sand jensen and pedersen 2008 schoelynck 2012 shan et al 2017 2018 siniscalchi et al 2012 temmerman et al 2005 as flow enters an emergent vegetation canopy the flow along the canopy leading edge is adjusted and the flow velocity is modified in both the streamwise and the lateral directions e g liu and shan 2019 zong and nepf 2010 the flow adjustment distance within the canopy li is defined as the distance between the canopy leading edge x 0 and the position in the streamwise stream parallel direction at which the flow velocity decreases to a constant on each cross stream transect orthogonal to the streamwise direction the velocity relative to the mean channel velocity is diminished inside the canopy but is enhanced in the adjacent bare channel e g dieter 1990 rominger and nepf 2011 liu et al 2020 as a consequence of the exchanges of mass and momentum between the fast flow in the bare channel and the slow flow inside the canopy the velocity profiles inside and outside such an emergent canopy are complicated which impacts sediment transport and in turn influences the evolution of vegetated landscapes shan et al 2020 vandenbruwaene et al 2011 bertagni et al 2018 therefore it is important to understand the complex velocity variations in partially vegetated channels previous studies have experimentally investigated the velocity distribution within and around an emergent canopy in an open channel huai et al 2016 liu et al 2020 liu and shan 2019 2022 maji et al 2016 rominger and nepf 2011 yan et al 2020 for an emergent array of circular cylinders centered in a channel rominger and nepf 2011 reported that the flow adjustment distance over which the velocity decreases to a constant in the streamwise direction li is related to the drag coefficient of the cylinders cd the frontal area per canopy volume a nd where n is the areal density of cylinders and d is the cylinder diameter and the half patch width b for a patch along the sidewall b becomes the full patch width fig 1 beyond x li the flow is fully developed and the velocity is constant in the downstream direction accordingly rominger and nepf 2011 proposed formulas to predict the flow adjustment distance li for low and high density canopies 1a for low density c d a b 1 l i 3 0 0 3 2 c d a 1 c d a b 2 1b for high density c d a b 1 l i 5 5 0 4 2 c d a 2 b 2 liu et al 2020 confirmed that the flow adjustment distance inside a canopy is the same as that in the adjacent bare channel see fig 4 in their paper thus eq 1 is valid for predicting the flow adjustment distance both inside a canopy and in the adjacent bare channel inside an emergent canopy the vegetated region in fig 1 liu et al 2020 proposed an exponential model to predict the longitudinal profile of the spatially averaged velocity 2 u veg u v e g f u veg 0 u v e g f e 3 x l i where u veg f 8 g s h 4 c d a h f is the spatially averaged velocity in the fully developed flow region x li in which f is the darcy weisbach friction factor s is the slope of water surface g is the gravitational acceleration and h is the flow depth u 0 8 g s h f is the mean channel velocity and uveg 0 is the mean velocity at the leading edge of the canopy for a canopy with cdab 0 to 9 uveg 0 can be estimated using the following equation proposed by liu and shan 2019 3 u veg 0 u 0 1 0 15 0 02 c d a b using eqs 2 and 3 the longitudinal profile of the spatially averaged velocity in the vegetated region can be predicted based on the flow continuity the longitudinal profile of the spatially averaged velocity in the bare channel u bare can also be predicted liu et al 2020 yan et al 2020 4 u bare b u 0 b u veg b b by ignoring the velocity variation in the lateral direction eqs 2 to 4 constitute a valid method for predicting the longitudinal velocity profiles in the vegetated region and bare channel however these equations cannot predict the lateral velocity profile at each x position moreover by ignoring the longitudinal velocity variation some scholars have proposed analytical models for predicting the lateral velocity profile only in the fully developed flow region while assuming that the flow velocity does not vary in the streamwise direction e g white and nepf 2007 huai et al 2008 chen et al 2010 zhang et al 2018 2021 nevertheless as flow enters a canopy the flow velocity varies in both the longitudinal and the lateral directions resulting in a complicated velocity distribution particularly in the flow adjustment region 0 x li hence a model considering the velocity variation in only the longitudinal direction or only the lateral direction cannot capture the velocity distribution and thus cannot predict the velocity distribution within and around a canopy in summary a model that is capable of predicting the velocity distribution in a partially vegetated channel has not yet to be developed to complement previous research and resolve this problem this study was to propose an analytical model to predict the lateral profiles of velocity at different streamwise positions in a partially vegetated channel specifically the new model parameterizes the velocity variations in both the longitudinal and the lateral directions 2 experimental methods two groups of experiments were performed fig 1 for the first group cases b1 to b3 the flume was 23 m long 2 m wide and 0 5 m high with a 15 m long test section located 3 m downstream from the flume inlet the vegetation canopy was placed in the middle of the test section and the streamwise canopy lengths in cases b1 b2 and b3 were 450 400 and 300 cm respectively the half canopy width was b 30 to 40 cm and the half flume width was b 100 cm leading to a channel blockage ratio of b b 0 3 to 0 4 which is consistent with the ratio b b 0 5 observed for natural rivers e g yamasaki et al 2019 two rulers were vertically fixed at distances of 0 5 m in front of and 0 2 m behind the canopy to measure the flow depths at those locations for all three cases the flow depth was h 18 cm and the mean channel velocity was u 0 18 cm s the flow was turbulent with a reynolds number of re u 0 r ν 28 000 where r is the hydraulic radius and ν 0 01 cm2 s is the kinematic viscosity the flow was subcritical based on a froude number of fr u 0 g h 0 14 for the second group cases d1 to d3 the vegetation canopy was situated along the sidewall of the channel and the canopy lengths in cases d1 d2 and d3 were 450 450 and 300 cm respectively the flume was 13 m long 1 m wide and 0 5 m high with a 7 m long test section located 3 m downstream from the flume inlet the full canopy width was b 33 cm and the full channel width was b 100 cm resulting in a channel blockage ratio of b b 0 33 because the channel blockage ratios of the two groups b1 to b3 and d1 to d3 were approximately the same the velocity differences between the two groups of cases i e centrally situated and side situated canopies can be compared in addition two rulers were again vertically fixed at distances of 0 5 m in front of and 0 2 m behind the canopy to measure the flow depths for all three cases the flow depth was h 20 cm and the mean channel velocity was u 0 17 20 cm s the flow was turbulent with the reynolds number ranging from re 23 000 27 000 and subcritical with the froude number ranging from fr 0 12 0 14 for both channels the distance between the flume inlet and the leading edge of the test section was 3 m preliminary experiments were conducted to measure the longitudinal velocity profile along the centerline of each flume the velocity was laterally uniform prior to entering the test section and remained constant in the longitudinal direction within the test section indicating that the flow entering each canopy was fully developed when an emergent canopy was placed in the channel the flow was adjusted along the canopy leading edge causing the flow depth within and around the canopy to change slightly according to our observations the flow depth inside and outside each canopy was altered by less than 5 mm this variation in flow depth 5 mm was then used to calculate the variation in water surface slope in the test section yielding a 1 water surface slope change in cases b1 b3 and d1 d3 which contributed to a 1 2 velocity change these findings indicate that the changes in the flow depth and velocity over the test section were negligible in all six cases b1 b3 and d1 d3 circular cylinders were deployed in a staggered arrangement to construct the model canopies fig 2 the cylinders were rigid and did not deform the characteristics of the cylinders are similar to those of cattails tree bases and mangrove roots abood 2011 shan et al 2019 yang et al 2007 coon et al 2000 the cylinder diameter d 0 8 cm was chosen to fall within the range d 0 2 1 2 cm of stem diameters of plants observed in wetlands and river floodplains lightbody and nepf 2006 leonard and luther 1995 yang et al 2007 liu et al 2013 in each case the longitudinal and lateral spacings between two neighboring cylinders were dx and dy respectively fig 1c and 1d and the values of which are summarized in table 1 for all six cases the cylinder density was n 0 03 0 15 cm 2 producing a frontal area per canopy volume of a nd 0 024 0 12 cm 1 and the solid volume fraction was ϕ π 4 n d 2 0 015 0 076 for the six cases see table 1 the canopy was emergent in all cases the longitudinal lateral and vertical directions were defined as x y and z respectively specifically in both flumes x 0 and z 0 indicate the upstream edge of the canopy and the channel bed surface respectively for the centrally situated canopy cases b1 to b3 y 0 indicates the flume centerline fig 1c for the side situated canopy cases d1 to d3 y 0 indicates the right sidewall of the flume fig 1d velocities were recorded using a sontek acoustic doppler velocimeter adv in cases b1 to b3 and a nortek adv in cases d1 to d3 first the vertical velocity profiles both inside and outside each canopy were measured to examine whether the depth averaged velocity was the same as the mid depth velocity for example in each side situated canopy case the vertical velocity profiles were measured at some positions inside the canopy x 100 cm and y 16 cm in case d1 x 120 and 380 cm and y 16 cm in case d2 and x 80 and 270 cm and y 16 cm in case d3 and outside the canopy x 100 cm and y 65 cm in case d1 x 120 and 380 cm and y 65 cm in case d2 and x 80 and 270 cm and y 65 cm in case d3 these profiles were used to calculate the depth averaged velocity ud the difference between ud and the mid depth z h 2 velocity u was 3 on average thus the flow velocities were measured at only the mid depth position in each case and the measured value was adopted as the depth averaged velocity in addition once ud was confirmed to be the same as the mid depth velocity the velocity was measured along lateral mid depth transects at different x positions inside each model canopy a characteristic region between two neighboring cylinders the dashed boxes in fig 1c and 1d was defined to account for the effect of spatial flow heterogeneity between cylinders on the measurements accordingly the flow velocities were measured at y dy 4 and dy 2 and the mean of these two velocities differed from the spatially averaged velocity in the characteristic region by less than 12 thus we measured the mid depth velocities at y dy 4 and dy 2 in each characteristic region and took the mean of the two velocities as the spatially averaged velocity at each y position across the canopy outside the canopy velocities were measured only once at each mid depth position measurement transects were established in both the flow adjustment region 0 x li and the fully developed flow region li x l specifically for the centrally situated canopy cases b1 to b3 lateral velocity profiles were measured at x 50 100 150 200 250 300 400 and 420 cm for case b1 and at x 50 100 150 200 250 and 300 cm for cases b2 and b3 for cases b1 b3 the lateral interval between two neighboring measurement points was 10 cm between y 0 and 90 cm on each transect note that for the centrally situated canopy cases the lateral velocity profiles were symmetric about the centerline y 0 which was confirmed by the preliminary experiment so the lateral velocity profiles were measured only on the left side of the flume 0 y b for the side situated canopy cases d1 to d3 lateral velocity profiles were measured at x 50 100 150 200 250 300 350 400 and 450 cm for cases d1 and d2 and at x 50 100 150 200 250 and 300 cm for case d3 for cases d1 d3 the lateral interval between two neighboring measurement points was 5 cm between y 25 and 60 cm and 10 cm between y 5 25 cm and y 60 90 cm on each transect at each position the measurement period was 150 s frequency of 50 hz the raw values with correlations below 70 and a signal to noise ratio smaller than 15 db were filtered and then despiked using the acceleration threshold method proposed by goring and nikora 2002 a matlab code was used to decompose the remaining instantaneous velocities into time averaged velocities u v and w the spatially averaged velocity in the vegetated region u veg 0 y b and that in the bare channel u bare b y b are defined as 5a u v e g 1 b 0 b u d 1 d y 5b u b a r e 1 b b b b u d 2 d y where u d 1 and u d 2 are the velocities in the vegetated region and bare channel respectively to compare the differences between the predicted and measured velocities the root mean square error rmse was computed as follows 6 rmse 1 n i 1 n u d m u d p 2 where n is the number of measurements and predictions in each case u d m is the measured velocity and u d p is the predicted velocity 3 experimental results fig 3 shows a comparison of the lateral velocity profiles at different streamwise positions in case d1 ϕ 0 025 and case d3 ϕ 0 076 the flow adjustment distance li was determined by taking measurements inside the canopy specifically li was defined as the distance between the canopy leading edge x 0 and the position at which the velocity became constant i e the velocity varied by less than 5 for each case li was also estimated using eq 1 among the six cases the difference between the measured and predicted values was less than 16 table 1 thus it is reasonable to use eq 1 to predict li in the following calculations the dense case d3 produced a shorter flow adjustment distance li 200 20 cm than did the sparse case d1 li 330 30 cm in the flow adjustment region 0 x li the velocity varied in both the longitudinal and the lateral directions fig 3a and 3b specifically as the distance from the canopy leading edge increased increasing x the velocity decreased inside the canopy but increased in the bare channel reflecting the velocity variation in the longitudinal direction likewise on each transect the velocity increased from the vegetated region to the bare channel displaying the velocity variation in the lateral direction fig 3c and 3d beyond x li the flow was fully developed and the velocity variation in the lateral direction existed only in the fully developed flow region the lateral velocity profiles remained the same as the distance from the canopy leading edge increased triangles in fig 3c and 3d these observations suggest that the velocity profile in the flow adjustment region is more complicated than that in the fully developed flow region because the velocities in the former vary both longitudinally and laterally white and nepf 2008 meftah and mossa 2016 to predict the velocity profiles through a partially vegetated channel then velocity variations in both the longitudinal and the lateral directions were considered in the newly proposed model see section 4 4 theory 4 1 analytical model for a channel with an emergent canopy the momentum equation is as follows 7 ρ f x τ xx x τ yx y τ zx z ρ u t u x u u y v u z w where ρ is the flow density τ xx is the normal stress τ yx and τ zx are the shear stresses and u v and w are the time averaged velocity components corresponding to the x y and z directions respectively for steady flows u t 0 f x is the vegetation drag force in the streamwise direction and is defined as 8 f x 1 2 c d a u 2 the normal stress τ xx is defined as 9 τ xx ρ g h 2 ρ ε u x where ε is the local eddy viscosity the flow continuity equation is 10 u x v y w z 0 by combining eqs 7 to 10 one can obtain 11 ρ g s x 2 ρ ε u x τ yx y τ zx z 1 2 ρ c d a u 2 ρ u 2 x u v y u w z integrating eq 11 over the flow depth can yield the depth averaged governing equation by combining the following four assumptions first the flow conditions in a vegetated channel are assumed to be uniform second the shear stress acting on the x y plane is expressed as 1 h 0 h τ yx d z ρ h ε u d y shiono and knight 1991 where ε λ h u is the depth averaged eddy viscosity λ is the dimensionless eddy viscosity u f 8 u d is the shear velocity and f is the darcy weisbach friction factor third the shear stress acting on the x z plane is τ zx 0 n cm2 at the water surface z h and that acting on the x z plane is equal to the bed shear stress τ b ρ u 2 at the channel bed z 0 i e 0 h τ zx d z τ b fourth at the water surface z h and channel bed z 0 the velocity in the vertical direction is w 0 cm s thus 0 h u w z d z 0 based on the above assumptions the depth averaged governing equation eq 12 is obtained by integrating eq 11 over the flow depth 12 gsh 1 2 λ h 2 f 8 2 u d 2 y 2 1 2 c d a h f 8 u d 2 k the advection parameter k in eq 12 is defined as 13 k h u d 2 x h 2 λ f 8 2 u d x 2 k x h u v d y k y where kveg and kbare are the spatially averaged advection parameters in the vegetated region and bare channel respectively at each x position kveg in the vegetated region and kbare in the bare channel were considered constant in each region in the flow adjustment region 0 x li eq 13 includes the velocity variation in the longitudinal direction k x h u d 2 x ρ h 2 λ f 8 2 u d x 2 and the velocity variation flow deflection in the lateral direction k y h u v d y specifically kx represents the velocity decrease in the vegetated region or the velocity increase in the adjacent bare channel and ky represents the intensity of the lateral flow deflection from the vegetated region to the bare channel see the discussion in section 9 2 some existing models consider only the longitudinal velocity variation thereby neglecting k y h u v d y e g liu et al 2020 yan et al 2020 in the fully developed flow region li x l eq 13 can be simplified to k h u v d y due to the absence of velocity variation in the longitudinal direction i e x 0 as in some previous models e g white and nepf 2007 huai et al 2008 chen et al 2010 zhang et al 2018 2021 we reiterate that our proposed model considers the velocity variations in both the longitudinal and the lateral directions allowing the model to capture the flow pattern and accurately predict the velocity distribution see section 6 in the vegetated region 0 x b the analytical solution for u d 1 is 14 u d 1 a 1 e r 1 y a 2 e r 2 y 8 h g s k veg 4 c d a h f where r 1 2 1 2 h 1 λ 8 f 1 2 4 c d a h f and a 1 and a 2 are unknown constants in the bare channel b x b the analytical solution for u d 2 is 15 u d 2 a 3 e r 3 y a 4 e r 4 y 8 h g s k bare f where r 3 4 1 2 h 1 λ 8 f 1 2 f and a 3 and a 4 are unknown constants 4 2 boundary conditions the four unknown constants a 1 a 2 a 3 and a 4 in eqs 14 and 15 are determined using four boundary conditions first the velocity and velocity gradient are continuous at the interface between the vegetated region and bare channel yielding two boundary conditions second for the centrally situated canopy the velocity gradient at the canopy centerline must be zero due to flow symmetry which provides the third boundary condition finally our measurements confirmed that the sidewall impacted the velocity only within 5 cm from the wall thus the velocity at y 0 95b with b 1 m reached a constant in the lateral direction fig 3 and the velocity gradient at y 0 95b was considered zero providing the fourth boundary condition these four conditions are summarized as follows 1 u d 1 u d 2 at y b 2 u d 1 y u d 2 y at y b 3 u d 1 y 0 at y 0 4 u d 2 y 0 at y 0 95b 4 3 determining the parameters the drag coefficient cd was estimated from the following equation of white 1991 16 c d 1 10 r e p 2 3 where r e p u d ν is the local stem reynolds number in the vegetated region based on eq 16 and the measured velocities the calculated rep from cases b1 to b3 and d1 to d3 the cd value ranges between 1 1 and 1 5 thus cd 1 and 1 5 were inserted into eq 1 to predict li when cd changed the other parameters in eq 1 were fixed among the six cases the average difference among the predicted li using cd 1 and 1 5 was 18 thus for simplicity it is reasonable to use cd 1 for these calculations the darcy weisbach friction factor f was estimated from f 8 g n c 2 r 1 3 knight et al 2007 huai et al 2008 where nc is manning s parameter yan et al 2020 reported nc 0 013 for pvc baseboards the dimensionless eddy viscosity coefficient λ was calculated from λ κ 6 where κ 0 4 is the von karman constant based on previous studies tang and knight 2008 abril and knight 2004 λ 0 067 was used in this study the advection parameters in the vegetated region kveg and bare channel kbare were determined via the following steps for each transect the spatially averaged velocities in the vegetated region and bare channel u veg and u bare were predicted from eqs 2 and 4 respectively the predicted u veg and u bare were then used to determine kveg and kbare that is kveg and kbare were determined by inserting eqs 14 and 15 into eq 5 5 published data the experimental data from this study and published studies devi et al 2019 zong and nepf 2010 caroppi et al 2020 were used to verify the proposed model over ranges of the solid volume fraction ϕ 0 001 0 02 canopy width to channel width ratio b b 0 3 0 5 and mean channel velocity u 0 5 0 71 4 cm s these data were chosen because velocity profiles were measured in the flow adjustment region and fully developed flow region the details of the published data were reported in each study for convenience the experimental parameters adopted in these previous studies are briefly introduced below and summarized in table 2 1 devi et al 2019 performed experiments in a 20 m long and 1 m wide flume with a 5 m long and 0 5 m wide canopy the canopy was constructed of natural rice stems o sativa with a stem diameter of d 0 2 0 1 cm and the frontal area was a 0 06 0 03 cm 1 yielding a solid volume fraction of ϕ 0 010 0 006 the flow depth was h 12 cm and the mean channel velocity was u 0 27 cm s velocity profiles were measured in the flow adjustment region x 0 and 250 cm and fully developed flow region x 500 cm 2 zong and nepf 2011 performed experiments in a 16 m long and 1 2 m wide flume with a 10 m long and 0 4 m wide model canopy the canopy was constructed of cylinders with a diameter of d 0 6 cm the flow depth was h 12 14 cm and the mean channel velocity was u 0 5 0 11 6 cm s two canopies with a 0 04 and 0 21 cm 1 were constructed corresponding to ϕ 0 02 and 0 1 respectively velocity profiles were measured in the fully developed flow region 3 caroppi et al 2020 performed experiments in a 0 4 m wide flume with an 8 m long and 0 16 m wide vegetation canopy the canopy was constructed of cylinders with a diameter of d 0 45 cm the flow depth was h 11 1 13 6 cm and the mean channel velocity was u 0 54 4 71 4 cm s canopies with a 0 0045 0 072 cm 1 corresponding to ϕ 0 001 0 025 respectively were constructed velocity profiles were measured in the fully developed flow region 6 model validation first the model was verified in the centrally situated canopy cases cases b1 to b3 u 0 18 cm s ϕ 0 015 to 0 045 s 0 6 10 4 f 0 025 and in the side situated canopy cases cases d1 to d3 u 0 17 to 20 cm s ϕ 0 025 to 0 076 s 0 4 10 4 to 0 6 10 4 f 0 025 at each position the velocity ud was normalized by the mean channel velocity u0 figs 4 and 5 confirm that the predicted velocity profiles black lines exhibit good agreement with the measurements symbols in both the flow adjustment region 0 x li and the fully developed flow region li x l this finding is supported by an rmse u0 10 6 14 9 next the data from devi et al 2019 were used to verify the new model for a channel with an o sativa canopy ϕ 0 01 the water surface slope and the darcy weisbach friction factor were s 0 0015 and f 0 15 respectively and the predicted velocity profiles black lines matched the measurements points in both the flow adjustment region x 0 and 250 cm and the fully developed flow region x 500 cm fig 6 with an rmse u0 of 20 9 finally the model was verified in the fully developed flow region using the experimental data from zong and nepf 2011 and caroppi et al 2020 the predicted velocity profiles of the new model showed excellent agreement with the measurements over wide ranges of mean channel velocities u 0 5 0 71 4 cm s solid volume fractions ϕ 0 002 0 1 and water surface slopes s 0 1 10 4 to 0 5 10 4 and a darcy weisbach friction factor of f 0 048 figs 7 and 8 the rmse u0 between the predicted and measured velocities varied from 10 to 21 the above comparison suggests that the proposed model is capable of predicting velocity profiles through a partially vegetated channel in particular the model accurately captured not only the longitudinal and lateral velocity variations in the flow adjustment region but also the lateral velocity variation in the fully developed flow region 7 model application the proposed model was applied to evaluate whether kelvin helmholtz kh vortices formed along the canopy side edge y b kh vortices started to form along the canopy side edge when the horizontal shear was sufficiently strong caroppi et al 2019 zong and nepf 2011 caroppi et al 2020 defined a nondimensional parameter θ to describe the horizontal shear intensity 17 θ u 2 u 1 u 2 u 1 where u 1 and u 2 are the steady flow velocities in the vegetated region and bare channel respectively see fig 8a for emergent canopies caroppi et al 2020 performed velocity spectrum analyses to confirm that kh vortices occur along the canopy side edge when θ surpasses a given threshold θc 0 4 this finding was supported by zong and nepf 2011 who observed kh vortices at θ 0 86 0 98 0 4 herein we discuss only the case with a canopy longer than the flow development distance l li so that the fully developed flow produces horizontal shear that is sufficiently strong to produce kh vortices by combining the new model with eq 17 the steady velocities in the vegetated region and bare channel u 1 and u 2 respectively can be predicted and used to determine the predicted θ denoted θp in the following text using the data from caroppi et al 2019 and zong and nepf 2011 the predicted value θp was compared to the measured values θ and the difference between θp and θ was less than 7 on average fig 9 this result suggests that the new model can accurately predict θp by comparing the predicted θp values with the selected threshold θc 0 4 the presence and absence of kh vortices can be evaluated based on θp θc and θp θc respectively many species of natural aquatic plants exist and each has a specific range of stem diameters liu et al 2021 the new model was further applied to determine whether the stem diameter impacts the horizontal shear intensity described by θp to verify this influence case 9 from caroppi et al 2020 was used as an example with a flow depth of h 12 cm and a channel mean velocity of u0 52 cm s four stem diameters d 0 2 0 45 0 6 and 1 2 cm were selected based on field observations d 0 2 1 2 cm representing four different plant species for each stem diameter the proposed model predicted θp values for plant densities of n 0 0005 1 cm 2 fig 10 a the horizontal dashed line is the threshold θc 0 4 for the formation of kh vortices fig 10a suggests that the critical density ncri for the formation of kh vortices increased as the stem diameter d decreased this result can be explained by the force balance within an emergent canopy eq 18 in the fully developed flow region the flow pressure balances the vegetation and bed drag forces 18 gsh f 8 u 1 2 1 2 c d n cri d h u 1 2 where ncri is the critical density of vegetation for the presence of kh vortices along the side edge of a canopy under the same flow conditions and bed roughness the flow pressure gsh and bed drag f 8 u 1 2 remained the same such that the drag force 1 2 c d n cri d h u 1 2 remained the same and ncrid was constant as a result a smaller stem diameter d corresponded to a higher critical density ncri fig 10a using the stem diameter d and the critical density ncri predicted by the new model the critical frontal area per canopy volume acri ncri d was determined for different stem diameters fig 10b fig 10b shows that acri was not dependent on stem diameter and the mean critical frontal area per canopy volume was acri 0 008 cm 1 under the conditions mentioned above h 12 cm and u0 52 cm s kh vortices formed at the frontal area per canopy volume a 0 008 cm 1 thus the critical frontal area required to form kh vortices depends on the flow conditions see eq 18 8 sensitivity analysis 8 1 drag coefficient cd the influence of the drag coefficient cd on the prediction was examined cd was estimated from eq 16 and u veg decreased over the flow adjustment region producing an increase in rep in this region based on the measured velocity inside the canopy in all six cases in table 1 eq 16 produced cd 1 1 1 5 case b1 u0 18 cm s h 18 cm and ϕ 0 015 was taken as an example to examine the influence of cd on the predicted velocity profiles in case b1 cd 1 1 1 4 then a wider range of cd 1 2 was considered and the other parameters were fixed as specified in tables 1 and 3 fig 11 shows that cd had a negligible influence on the predicted velocities suggesting that the proposed model is not sensitive to cd for simplicity it is reasonable to use cd 1 predicting flow velocities 8 2 advection parameters kveg and kbare furthermore the influences of the two advection parameters k veg and k bare on the predicted velocity profiles were investigated and case b1 was again taken as an example in the flow adjustment region x 200 cm the predicted velocity profiles were compared using different combinations of kveg and kbare fig 12 three combinations of k veg and kbare were considered while the other parameters remained fixed as specified in table 1 each combination kveg and kbare was determined using the method in section 4 3 fig 12 shows that the predicted velocity inside the canopy increased as kveg decreased from 0 0035 to 0 02 and similarly the velocity in the bare channel increased as kbare decreased from 0 0006 to 0 0013 due to the flow continuity constraint eq 4 the combinations of k veg and k bare produced the opposite tendency of predicted velocities between the vegetated region y b 0 1 and the bare channel y b 1 2 5 specifically an increase in the predicted velocity in the vegetated region corresponded to a decrease in the predicted velocity in the bare channel to further examine the trends of the two parameters inside and outside the canopy kveg and kbare were plotted against the x position and the values were normalized by the flow adjustment distance li fig 13 for each case the maximum absolute value of kveg and the minimum absolute value of kbare occurred at the leading canopy edge as the distance from the leading edge increased x li increased the absolute value of kveg decreased and the absolute value of kbare increased in the flow adjustment region 0 x li 1 in the fully developed flow region x li 1 kveg and kbare both became constant the values of kveg and kbare in the cases from devi et al 2019 exhibited a similar trend see table 4 kveg and kbare varied in the flow adjustment region because these parameters were influenced therein by the velocity variations in both the longitudinal and the lateral directions eq 13 this point reiterates the importance of considering the velocity variations in both the longitudinal and the lateral directions in the proposed new model 9 discussion 9 1 influences of the advection parameter on the predictions the advection parameter k includes both x direction kx and y direction ky components eq 13 i e k kx ky to examine the influence of either or both components kx and ky on the predicted velocities the predictions using four different k values were compared kveg and kbare represented the k value in the vegetated region and the bare channel respectively because the x direction component is negligible i e kx 0 in the fully developed flow region we took velocity measurements at x 50 cm from the flow adjustment region li 330 30 cm in case d1 as an example first we ignored both the x and the y components of the k value kx 0 and ky 0 so that k 0 the predictions were smaller than the measurements across both the vegetated region and the bare channel orange line in fig 14 indicating that the velocity variations in the longitudinal and lateral directions cannot be ignored second only the flow variation in the longitudinal direction was considered i e k kx inappropriate k values produced negative values in the square root of the analytical solution eqs 14 and 15 and thus the predicted velocities could not be obtained further verifying that the lateral velocity variation in the flow adjustment region cannot be ignored third only the flow variation in the lateral direction was considered i e k ky the longitudinal velocity variation was ignored kx 0 so the k values in the vegetated region and bare channel were the same as those in the fully developed flow region as expected compared to the predictions obtained using appropriate k values black line in fig 14 the predicted velocities were underestimated in the vegetated region and overestimated in the bare channel blue line in fig 14 suggesting that the longitudinal velocity variation kx cannot be ignored in the flow adjustment region in summary velocities can be accurately predicted only if the flow variations in both the longitudinal and the lateral directions are considered as the black line shown in fig 14 9 2 physical meanings of kx and ky for the advection parameter the x kx and y ky components were plotted against the x position that was normalized by the flow adjustment distance li fig 15 based on the measurements from cases b1 to b3 and d1 to d3 where the subscripts veg and bare of kx and ky indicate the parameters in the vegetated region and bare channel respectively x li 1 and 1 indicate the flow adjustment region and fully developed flow region respectively kx veg and kx bare were determined by eq 13 using the longitudinal profiles of measured velocities in the vegetated region and bare channel and for each case kveg and kbare were determined by the method in section 4 3 table 3 ultimately ky veg kveg kx veg and ky bare kbare kx bare were determined first the value of kx represents the velocity variation in the longitudinal direction and negative and positive signs of kx signify decreasing and increasing velocities respectively in the x direction for example in the vegetated region the flow velocity decreased exponentially eq 2 with increasing distance from the canopy leading edge x 0 in the flow adjustment region x li 1 thus the velocity gradient u veg x was negative and the largest velocity variation the largest negative kx veg occurred at the canopy leading edge open symbols in fig 15a the velocity became constant in the fully developed flow region x li 1 yielding u veg x 0 i e kx veg 0 in the bare channel because the flow was continuous the greatest velocity increase was observed close to the canopy leading edge resulting in the largest positive kx bare solid symbols in fig 15a in contrast with increasing distance from the canopy leading edge kx bare decreased in the flow adjustment region x li 1 and dropped to zero in the fully developed flow region x li 1 second the value of ky represents the velocity variation in the lateral direction which is associated with lateral flow deflection in the flow adjustment region x li 1 the flow was deflected laterally toward the bare channel by the drag induced by the vegetated region hence the lateral velocity gradient ky was negative in both regions the strongest lateral flow deflection occurred at the canopy leading edge x 0 where the largest negative ky was observed in the fully developed flow region x li 1 the lateral flow was not deflected yielding ky 0 in both the vegetated region and the bare channel fig 15b 10 conclusion herein laboratory experiments clarified that under unidirectional flows the flow velocities both inside and outside a canopy varied in both the longitudinal and the lateral directions accordingly this study proposed an analytical model for predicting the lateral flow velocity profiles through a partially vegetated channel the velocity variations in both the longitudinal and the lateral directions were parameterized and included in the new model which was then verified using twenty groups of experimental data covering a wide range of vegetation and flow conditions the predicted velocity distributions were in good agreement with the measurements indicating that our model is capable of accurately predicting the velocity distribution in a partially vegetated channel finally the model was applied to evaluate the critical frontal area per canopy volume at which kh vortices begin to form along the canopy side edge at the same spatially averaged velocity the critical frontal area required for the formation of kh vortices was independent of stem diameter credit authorship contribution statement chunhao yan investigation formal analysis software writing original draft yuqi shan validation writing review editing chao liu conceptualization methodology writing review editing supervision project administration xingnian liu supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study received financial support from the national natural science foundation of china nos 52022063 52179074 and u2040219 and the sichuan science and technology program 2021yfh0028 chunhao yan was supported by postdoctoral funding from chongqing jiaotong university 307696 the authors appreciate the editor s and reviewers constructive suggestions which significantly improved the quality of the paper 
3081,fully distributed hydrological models are widely used in groundwater management but model speed and data requirements impede their use for decision support purposes metamodels provide a simpler and faster model which emulates the underlying complex model using machine learning techniques however metamodel predictions beyond the ranges in space and or time of training data are highly uncertain and thus it is important to assess the predictive model performance to ranges outside the training data i e model transferability we present a novel methodology for evaluating model transferability to areas not contained in the training data set based on various metrics that quantify the differences in covariate distributions between training and testing data the transferability method can be employed as a screening tool to assess the suitability of a metamodel for spatial prediction beyond its training domain we evaluated this transferability approach on a random forest metamodel of a 1000 km2 fully distributed coupled groundwater model for predicting drainage fraction the partitioning of infiltrating water between drains and groundwater we conducted spatial cross validation on 9 holdout sub basins to assess metamodel transferability beyond sampling locations and compared this estimate with a random split sample validation test using mappable covariates only the metamodel showed high performance r2 0 79 tested on a 20 randomly sampled holdout conversely metamodel performance significantly decreased for the 9 spatial holdouts r2 ranging from 0 13 to 0 61 we document that the proposed transferability metric correlates with metamodel predictive performance and demonstrate its use to assess model transferability to datasets outside the training data spatial domain keywords machine learning predictive modelling model portability model generalization histogram distance drain partitioning 1 introduction decision makers in water resources management largely rely on model forecasts of system responses to develop management strategies and to support the implementation of environmental legislation such as the european water framework directive wfd in this context process based models are powerful tools to simulate complex hydrological systems as they integrate knowledge of the physical system and calculate hydrological states and fluxes at each model grid meanwhile immense requirements in terms of data calibration and computation time are among the key limitations of process based hydrological models being used for decision support asher et al 2015 clark et al 2017 metamodels also called surrogate models or model emulators provide a simpler and hence faster alternative blanning 1975 machine learning techniques are commonly used to make metamodels which emulate the specified output of the underlying complex model as a function of its paired inputs and outputs advantages of metamodels for decision support are three fold britz and leip 2009 fienen et al 2013 villa vialaneix et al 2012 i increased simulation speed to estimate a specific output ii screening level predictions in areas where a full physical model is not available iii easier integration with other model platforms accordingly metamodels can be used for on the fly simulations in a decision making process e g during stakeholder meetings and the increased simulation speed offers higher flexibility for data integration and exploration of uncertainty furthermore screening level predictions are often needed in areas where process based modelling is not possible due to lack of data and funding finally integrating outputs from metamodels with other simulation platforms is useful when a decision support tool must incorporate many different processes that are not available in one integrated model conversely the major drawback of metamodels is that increased computation speed comes at the cost of accuracy however for decision making applications approximate but fast decisions can be more valuable than precise predictions e g for screening purposes where numerous scenarios must be evaluated and where the variations in impacts between the individual scenarios are larger than the uncertainty of the metamodel nevertheless predictions for input regimes outside the training test set may be highly uncertain hampering the use of metamodels thus screening level predictions can only be made where input variable values are obtained for a region of similar conditions where the metamodel is considered valid metamodels have been extensively applied to groundwater modelling asher et al 2015 including contaminant transport in soils and groundwater bouzaher et al 1993 van der heijden and haberlandt 2015 nolan et al 2015 2018 piñeros garcet et al 2006 villa vialaneix et al 2012 groundwater age and residence time fienen et al 2018 starn et al 2021 starn and belitz 2018 groundwater response to sea level rise fienen et al 2013 sources of water to wells fienen et al 2016 and saltwater intrusion roy and datta 2019 however metamodels that reproduce outputs of fully distributed groundwater models are underrepresented in literature despite fully distributed groundwater models being abundant in practice asher et al 2015 metamodel predictions and any other machine learning application are highly uncertain beyond the spatial and temporal bounds of their training data however this is not reflected in current evaluation measures the predictive performance of machine learning metamodels is commonly evaluated using random k fold cross validation where the data set is randomly partitioned into folds or by testing the model on a randomly sampled holdout bouzaher et al 1993 nolan et al 2015 2018 sajedi hosseini et al 2018 villa vialaneix et al 2012 when the training and test set are randomly sampled from the data set they can be assumed to have similar covariate distributions however this assumption is often violated in practice when making predictions into new spatial domains unsampled space when a metamodel model is applied to a new spatial domain e g another catchment it may contain local biases that are not present in the training data which will likely cause a decrease in model performance fienen et al 2016 2018 van der heijden and haberlandt 2015 meyer et al 2019 concluded that spatial cross validation which uses spatially independent data sets for validation is essential in preventing overoptimistic model performance thus while the standard validation methods give valuable insights on the model s ability to reproduce variability in the training data they give no information on the model s ability to make predictions outside the training data named model transferability currently expert or a priori knowledge is used to evaluate whether a metamodel can be applied on a different spatial region starn et al 2021 or potential differences are ignored recent efforts to evaluate model transferability include the area of applicability for spatial mapping of environmental variables meyer and pebesma 2021 and the concept of applicability domain in the field of chemical modelling e g mathea et al 2016 toplak et al 2014 but are generally overlooked in spatial modelling however these methods are computationally complex as they rely on a point based approach evaluating the distance between each individual new test data point and all training data points across all covariates furthermore the methods from chemical modelling do not transfer intuitively to spatial data which is important when model end users should be able to compute and interpret model transferability this study proposes a metric for assessing model transferability that can be used as a screening tool to evaluate model suitability for making predictions into new spatial domains we show that metamodel transferability can be approximated by quantifying the dissimilarity between training and test data sets of covariate distributions computed as histogram distances weighted by covariate importance in an example application we apply drainage fraction as the metamodel target variable defined as the partitioning of the water flux between drains and groundwater artificial drainage in agriculture significantly influences the hydrological cycle boland brien et al 2014 watershed discharge king et al 2014 and the leaching of nutrients ernstsen et al 2015 field measurements of tile drain contribution of annual no3 loads to surface water ranges from 62 williams et al 2015 up to 90 92 rozemeijer et al 2010 thus tile discharge is a critical hydrological variable for determining nutrient losses to surface water and groundwater drainage fraction can be used as an indicator of surface water groundwater vulnerability to nitrogen application in agricultural catchments and thus it is a relevant parameter for decision making regarding spatially targeted measures to reduce losses to surface or groundwater in the landscape in this study we i develop a transferability assessment method based on differences between training and test data distributions of different areas weighted by covariate importance ii develop a random forest metamodel of drainage fraction where the response is derived from a process based distributed groundwater model iii compare random vs spatial cross validation to assess the transferability of the model to new catchment areas and evaluate the transferability method 2 material and methods 2 1 study area and metamodel data the storaa catchment is situated in western denmark and covers an area of 1000 km2 fig 1 elevations range from sea level to the west at the outlet of the river storaa to nissum fjord and the north sea to about 100 m above sea level in the eastern headwater the primary land use in the storaa catchment is agriculture 63 the catchment is dominated by sandy soils with some clayey less permeable soils the gentle topography in combination with a temperate climate average yearly precipitation 1000 mm requires significant parts of the agricultural land to be artificially drained mainly by tile drains groundwater levels in denmark are generally shallow koch et al 2019 and approximately 50 of the national agricultural area is artificially drained møller et al 2018 we derive drainage fraction predictions based on a fully coupled surface water groundwater mike she model of the storaa catchment stisen et al 2018 the storaa catchment model is at 100 m resolution and calibrated against transient discharge and groundwater head measurements 2000 2006 in the model drains are implemented homogeneously throughout the catchment at 1 m below ground level mbgl drainage fraction ranging between 0 and 1 is calculated as the average partitioning of infiltrating water to the saturated zone between drains and groundwater in each model grid cell a drainage fraction of 1 means that all infiltrating water in the grid cell is transported via drains to surface water recipients datapoints with daily negative recharge upwelling and corresponding datapoints for drainage are set to zero because drainage fraction considers only the partitioning of infiltrating water positive recharge between tile drains and groundwater daily outputs of drainage and recharge to the saturated zone are extracted for the period 2000 2006 the drainage fraction is calculated for each 100 m grid as the temporal mean drainage divided by the temporal mean recharge to saturated zone lakes urban areas forests and streams are masked out out of the 77 333 datapoints each representing a 100 100 m cell 57 have drainage fraction 0 and consequently the distribution is skewed towards zero fig 2 a b the catchment is dominated by sandy soils and drainage mainly occurs in clayey soils as seen in the northern part and along streams fig 2e this promotes the storaa catchment as an excellent candidate to test metamodel transferability because the landscape and the drainage fraction are spatially dependent we apply drainage fraction as the metamodel target variable thus the study must assume that mike she correctly describes the physical processes which control drainage in the following we consider the drainage fraction from the mike she model as our reference and focus on the errors of the metamodel in comparison to that thus any errors in the formulation are not explicitly estimated in the metamodel the metamodel covariates table 1 span the physical factors that are expected to influence drainage flow and recharge to the saturated zone the covariates are mapped to the same resolution 100 m as the input to the groundwater model both mappable and non mappable covariates were considered as a first step to identify covariates mappable covariates refer to quantities that are readily available without requiring a model to derive them the metamodel using all covariates is considered a reference model for metamodels created with the goal of applying to a currently unmodeled area only the mappable predictor variables are relevant consequently only mappable parameters are included in the following metamodel transferability analysis 2 2 random forest regression the metamodel uses a random forest rf machine learning regressor that is trained to emulate the target variable drainage fraction rf breiman 2001 is an ensemble model applying bagging as the ensemble method and regression decision tree as the base model we chose rf because they are simple to train require little tuning and are robust to overfitting hastie et al 2009 we applied the scikit learn v 0 22 1 rf regressor package in python v 3 7 pedregosa et al 2011 the strength of rf relies on building an ensemble of decorrelated and unbiased decision trees and then reducing its variance by averaging the trees each decision tree is grown by recursively splitting the data into more homogeneous groups until the minimum node size is reached rf possesses two important elements of randomness to reduce the correlation between trees first a unique bootstrapped data sample is created for each tree where about two thirds of the data are randomly sampled with replacement and the remaining one third is left out these are known as the out of bag oob samples second a random subset of input covariates is selected for each split in the tree growing process hastie et al 2009 thereby each bootstrap tree will involve different covariates and training data and may have a different number of terminal nodes i e leaves the oob feature allows random forests to be fit with internal cross validation being performed simultaneously hastie et al 2009 the oob error is calculated on the oob samples excluded from the training for each tree the final bagged estimate can be derived as the average prediction across all trees where data was retained for training the oob error is also used for model validation purposes amare et al 2021 koch et al 2019 2 3 hyperparameter tuning and baseline models rf has five hyperparameters in the scikit learn implementation v 0 22 1 that control model complexity and regularization four of which we adjusted for tuning table 2 n estimators max depth min samples split min samples leaf and max features we used a fixed value of 0 33 for max features hastie et al 2009 p 592 for hyperparameter tuning we included all covariates and applied 5 fold cross validation on a randomly sampled data set 80 of the entire data set of 77 333 datapoints evaluating each permutation of tuning parameters table 2 resulted in 180 candidate models from which we selected the hyperparameter set that gave the best mean score coefficient of determination r2 across the five folds using the optimal hyperparameter set we trained two baseline metamodels one including all covariates and the second including mappable covariates table 1 both metamodels were trained on the same 80 randomly sampled datapoints and tested on the corresponding 20 holdout 2 4 transferability assessment the following paragraphs present the transferability assessment method the method approximates metamodel transferability by quantifying the differences in covariate distributions between training and test data sets i e how much the test spatial domain differs in its physical properties topography soil type etc compared to the training data we quantify differences as histogram distances between train and test covariate distributions not all covariates are equally important in the rf model thus the distances are weighted by covariate importance finally the correlation between weighted distance and model performance is used to predict model performance in a spatial holdout i e model transferability the transferability assessment proceeded in six steps fig 3 1 compute differences between training and test sets for all covariates expressed as histogram distances 2 compute individual covariate importances 3 compute weighted distance wd across all covariates by weighted linear multiplication using covariate importance as weighting factor this is the metric of transferability 4 conduct spatial cross validation to obtain predictive model performance 5 evaluate transferability metric by correlating wd with predictive model performance r2 6 predict model transferability using linear regression models and evaluate against spatial cross validation results 2 4 1 histogram distance metrics transferability between data sets is evaluated by quantifying the difference expressed as a distance between normalized histograms of covariates the training and test set of each covariate are standardized for the histogram comparison to be independent of specific units 1 x i j s x i j x j σ j x i j s is the scaled value of j th predictor for the i th observation x j and σ j are mean and standard deviation respectively of predictor j where mean and standard deviation are computed over the combined training and test data furthermore to make the histogram comparison independent of sample size the histograms are normalized so each bin contains the value of the probability density function at the bin such that the integral over the range is 1 for comparison of continuous covariate distributions we apply four histogram distance metrics d1 d4 cha and srihari 2002 the histograms hb a and hb b of data set a and b where b is the number of bins are treated as b dimensional vectors the more similar the two histograms are the smaller the value d a b is the histograms are normalized with equal size and width of bins the standard vector norms can be used as distances between the two histograms city block l1 norm 2 d 1 a b i 0 b 1 h i a h i b the city block distance represents the sum of the bin wise absolute differences between the two histograms euclidian l2 norm 3 d 2 a b i 0 b 1 h i a h i b 2 the euclidian distance is used to determine the bin to bin difference between two histograms it is the square root of the sum of the squared differences of the histograms minimum difference of pair assignments cha and srihari 2002 4 d 3 a b min a b i j 0 n 1 d a i b j given n elements ai a and n elements bj b and d is the paired absolute distance between two elements the minimum difference of pair assignments accounts for the shape of the non overlapping area of the histograms unlike d1 and d2 the distance between two histograms of n elements a and b is expressed as the minimum distance of pair assignments between the two sets the problem is to determine the best one to one assignment between two sets such that the sum of all distances between two individual elements in a pair is minimized intersection 5 d 4 a b i 0 b 1 m i n h i a h i b i 0 b 1 h i a 1 the fourth metric computes the intersection between two histograms intersection is multiplied with 1 to have the same interpretation across metrics i e high distance corresponding to low similarity for comparison of categorical covariates we compute the absolute difference between the relative size of each class of the categorical variable and sum across all classes c the distance between the train and test sets a and b of sample size n a and n b is computed as 6 d cat a b i 1 c a b s r a c r b c where c is number of classes of the categorical covariate r a and r b are the relative sizes of class c in a and b defined as the ratio between the number of observations of class c in data set a and the total number of observations in data set a 7 r a c n a c n a the covariate analysis assumes that all input covariates are available for the holdout set at the same resolution as the training set 2 4 2 weighted distance as transferability metric for each distance metric the covariate distances were aggregated linearly and weighted by their covariate importance 8 wd j 1 n w j d j where wd is weighted distance w is weight importance of covariate j and d is distance of covariate j consequently covariates with a higher importance have a larger influence on the wd we computed individual covariate importance as permutation importance defined as the fractional decrease in model score r2 when a single covariate is randomly permuted thus ranging between 0 and 1 breiman 2001 we used the rfpimp python package parr and turgutlu 2019 and not the scikit learn random forest in built feature importance function the latter is biased towards high cardinality covariates and is unreliable when covariates vary in their scale of measurement or their number of categories strobl et al 2007 however when two covariates are correlated and one of them is permuted it may show an alleged low importance because the model will still have access to the information through its correlated feature therefore we also permuted covariates by groups topographic soil texture precipitation and land use table 1 the groups were based on covariate physical relationships and supported by a correlation matrix to identify the degree to which individual covariates were correlated 2 4 3 spatial cross validation we divided the storaa catchment into sub basins using the watersheds basins module in the q gis saga 2 3 2 toolbox conrad et al 2015 we discarded one sub basin with 100 grid cells that occurred as an artefact of the confluence of multiple branches using the remaining nine sub basins fig 2c we trained nine separate metamodels where each of the sub basins was withheld in succession from the training and used for testing as a spatial holdout named sb1 9 and compared with the random 80 20 split sample named rdm only mappable covariates were included in the spatial cross validation for each of the 10 training and test data sets 9 spatially defined 1 randomly sampled we calculated r2 for the prediction on i the training set ii the holdout and iii the oob prediction 2 4 4 wd performance correlation and transferability from linear regression models we evaluated the correlation between metamodel predictive performance r2 and wd for the 10 metamodels using the pearson correlation coefficient ρ p which measures the strength of a linear correlation negative correlations are expected between wd and r2 since larger differences between the training and test set in principle should lead to poorer metamodel performance for our 10 fold spatial cross validation we tested if the metamodel performance can be predicted from the wd of a holdout based on linear regression on the remaining 9 holdouts thus for each holdout we produced a linear regression model fitted against metamodel r2 using wd as input the predicted model performance for each hold out was subsequently evaluated against the true model performance computed in the spatial cross validation 3 results 3 1 metamodel output we developed metamodels based on all available data and using mappable data only for transferability mappable data are key the mappable metamodel runtime for predicting drainage fraction over the entire storaa catchment was 3 s whereas the mike she model runtime was 9 h both metamodels perform well however the metamodel r2 decreases from 0 85 to 0 79 when only mappable covariates are included fig 4 a b this is primarily due to the information loss of the non mappable soil texture covariates fig 4c both metamodels capture reasonably well drainage fraction 0 7 but neither accurately represent drainage fraction 1 the topographic covariates have the highest grouped importance for both metamodels fig 4c topography is expected to influence surface runoff and infiltration the second most important group considering all covariates is soil texture soil texture is expected to largely influence drainage with most of the drainage taking place in clayey soils where the infiltration rate is lower in sandy soils the water can easily infiltrate and recharge groundwater resources however when the non mappable soil texture covariates are removed precipitation becomes the second most important covariate group 3 2 spatial cross validation the training skill is very high across all 10 metamodels training r2 0 89 meaning that the random forest model has enough capacity to nearly reproduce the training set the oob r2 ranges from 0 79 to 0 82 across all metamodels and the randomly sampled holdout shows an r2 of 0 79 fig 5 conversely the hold out performance is poorer in the spatial cross validation where metamodel performances drop to 0 15 0 60 fig 5 this significant decrease in performance is a result of local biases in the holdout sub basins sb1 9 not reflected in the training data furthermore the spatial holdouts differ in terms of the distribution of the largest errors between mike she derived drainage fraction and metamodel predicted drainage fraction fig 6 sb1 and sb3 resemble the randomly sampled metamodel the most in terms of performance and distribution sb7 and sb8 fail to predict drainage fraction 0 whereas sb9 reaches a reasonable average performance due to good performance in the mid range despite low performance at high drainage fraction generally the highest errors occur at high drainage fraction which is primarily observed close to streams and in clayey soils 3 3 model transferability assessment the transferability method combines a covariate distribution distance metric weighted by covariate importance and it correlates the weighted distance with metamodel performance through spatial cross validation selected intermediate results of this stepwise approach section 2 4 are shown in fig 7 for all training and test data sets sb1 9 and rdm histogram distances were computed across all covariates illustrated by relative topo r for sb4 and sb8 fig 7a and individual covariate importances were computed for weighting fig 7b for each covariate model performance r2 and distance was correlated across the 10 training and test data sets relative topo r fig 7c when the distances are weighted by covariate importance the linear correlation is strengthened seen as an increase in pearson correlation coefficient fig 7d finally the weighted covariate distances are aggregated linearly across covariates resulting in one correlation plot for each distance metric fig 8 we observe negative pearson correlation coefficients between r2 and wd across all distance metrics both weighted and unweighted with the city block metric resulting in the strongest correlation of 0 82 fig 8 and table 3 thereby we document that the metamodel performance decreases as the linear sum of weighted distances between the covariate distributions of the training domain and test domain increases in other words the more different the train and test sets are the lower the model performance is the rdm data set shows almost identical distributions for the train and test data whereas the spatial sub basins show different degrees of histogram overlap and similarity in shape due to local biases consequently the rdm data set shows the lowest distance along with the highest performance across all metrics fig 8 the wd performance correlation is strengthened for the city block min dif pair and euclidian metrics when the covariates are weighted by their importance table 3 this is because on average the stronger correlations are found for covariates with high importance e g relative topo and prec fall however for categorical covariates the correlation is weakened when weighted by importance pearson decreases from 0 57 to 0 35 because the soil class covariate with the highest importance has a positive correlation the predicted model performances i e model transferability obtained from the linear regression models are in good agreement with the spatial cross validation model performance fig 9 the higher the predicted r2 the more transferable the model is as expected the results attest highest transferability of the metamodel where train and test sets are randomly sampled and thereby similar in covariate distributions model transferability is overestimated for the two sub basins sb7 and sb8 as the predicted r2 is higher than the spatial cv derived r2 however for the remaining metamodels the predicted r2 gives an accurate and mostly conservative estimate of model transferability 4 discussion 4 1 transferability of rf metamodel for predicting drainage fraction this study presents a novel method for assessing model transferability to new spatial domains evaluated on a random forest metamodel for predicting drainage fraction the method uses a transferability metric wd computed from covariate histogram distance metrics weighted by covariate importance different histogram distance metrics were tested and city block was found to give the strongest correlation with r2 city block was only slightly better than min dif pair although it is a mathematically much simpler metric intersection neglects the non overlapping parts of the histogram thus it is not surprising that it is not the best metric the euclidian distance metric squares the difference and thereby more weight is put on bins with a large distance even though the remaining histograms are very similar our results demonstrate that the city block wd is a suited metric to assess differences between train and test sets and that wd can be used to quantify and assess model transferability when used as an input in linear regression models fitted to spatial holdouts of the training domain our findings confirm that random k fold cross validation leads to an overoptimistic view on model performance in the face of bias when applied to an unsampled region as illustrated by fienen et al 2018 and concluded by meyer et al 2018 we show that model performance can vary significantly between seemingly similar training and test data sets in this case sub basins within the same catchment this understanding is highly relevant for metamodels and machine learning models that are trained on a unique data set and applied to other independent data sets such as different catchments or geographical regions or used for temporal extrapolation the proposed transferability method provides a quantitative screening tool which can be used to identify suitable areas of model application the binned histogram analysis allows model end users to compare similarities of training data and target domain i e different catchments both visually and quantitatively the proposed method offers an objective and quantitative alternative to expert or a priori knowledge furthermore the transferability method may be able to guide the appropriate choice of training data for machine learning predictions at a target site by evaluating weighted histogram distance i e whether the added data lies in the relevant region histogram of the relevant parameters covariate importance nevertheless a priori knowledge should still be considered in combination with the transferability assessment in the case where the target domain is dominated by covariates not included in the training set e g in a different geological region with mountains and exposed bedrock the oob and randomly sampled holdout performance of the two metamodels mappable and all covariates show that a high proportion of the variance in drainage fraction predictions made using the numerical mike she model can be explained by the rf metamodel these results confirm that the metamodels capture a large amount of the system behaviour inherent in the distributed hydrological model they are trained on the metamodel obtains a performance of r2 0 79 for the randomly sampled and thereby most representative training data the metamodels have a good fit for drainage fraction 0 7 which can be attributed to an under representation of training data in the range from 0 7 to 1 the decrease in metamodel accuracy relative to the mike she model is traded for a significant improvement of the simulation runtime from 9 h to 3 s that promotes model application for decision support mapping drainage fraction based on catchment characteristics could potentially support decision making for spatially targeted n measures to reduce losses to surface or groundwater i e land cover modification and application of different land management options provided the short model runtime the effects of different management scenarios could be evaluated on the fly with local stakeholders the transferability method presented in this paper would allow model end users to assess whether the metamodel could be applied to a certain catchment transferability is not only relevant for metamodels it is relevant for any ml model applied to data which differs in distribution from the training data this also includes ml models for predictive mapping between point observations where normally field samples used as training data are unevenly distributed over the study area exemplified by naghibi et al 2020 and ransom et al 2017 increasing computational power has resulted in ml methods becoming more frequently used in water applications in the past decade tyralis et al 2019 and available for practical applications in decision making this calls for an improved understanding and quantification of model uncertainties and limitations to their application to which this paper contributes 4 2 limitations and future work a key limitation of metamodels is assuming that the underlying numerical model is an adequate representation of the physical system improving drainage representation in groundwater models is widely studied hansen et al 2013 2019 de schepper et al 2015 shafii et al 2019 thomas et al 2016 furthermore we are aware that the target variable distribution is skewed towards zero due to catchment characteristics which means we can expect some stability issues in the regression trees solutions on how to accommodate this are discussed in literature including beta forest method weinhold et al 2020 which could be a possible alternative in further studies as the presented rf models have reasonably high performances we have not addressed the issue further the transferability analysis is based on a single set of sub basins covered by the same underlying numerical model we do not know yet how it generalizes to other catchments or how the results extend to other types of metamodels and covariates covariate biases between different catchments are expected to be larger than the nine sub basin biases observed within the storaa catchment however we believe that the transferability methodology presented here provides useful insights on the transferability of metamodels to un modelled areas beyond the storaa catchment 5 conclusions we successfully develop a rf metamodel of a distributed integrated hydrological model using mappable parameters only which reliably predicts drainage fraction our findings confirm that random k fold cross validation cv can lead to an overoptimistic view on model results compared to spatial cross validation we document that metamodel performance decreases as the weighted histogram distance between the covariate distributions of the training domain and test domain increases the proposed distance metric and spatial cross validation can be used to assess transferability of models we finally conclude that it is essential to assess the transferability of a metamodel if the model is intended to be used beyond the training data for this purpose the proposed distance metric can be used to identify suitable areas of model application data availability the data and python code to estimate model transferability is available from the github repository https github com elisabjerre model transferability credit authorship contribution statement elisa bjerre conceptualization methodology software formal analysis visualization writing original draft writing review editing michael n fienen conceptualization methodology software formal analysis writing review editing raphael schneider methodology writing review editing julian koch methodology writing review editing anker l højberg conceptualization methodology writing review editing supervision funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work has been carried out under the waterprotect research project supported by the european union s horizon 2020 research and innovation programme under grant agreement no 727450 any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government 
3081,fully distributed hydrological models are widely used in groundwater management but model speed and data requirements impede their use for decision support purposes metamodels provide a simpler and faster model which emulates the underlying complex model using machine learning techniques however metamodel predictions beyond the ranges in space and or time of training data are highly uncertain and thus it is important to assess the predictive model performance to ranges outside the training data i e model transferability we present a novel methodology for evaluating model transferability to areas not contained in the training data set based on various metrics that quantify the differences in covariate distributions between training and testing data the transferability method can be employed as a screening tool to assess the suitability of a metamodel for spatial prediction beyond its training domain we evaluated this transferability approach on a random forest metamodel of a 1000 km2 fully distributed coupled groundwater model for predicting drainage fraction the partitioning of infiltrating water between drains and groundwater we conducted spatial cross validation on 9 holdout sub basins to assess metamodel transferability beyond sampling locations and compared this estimate with a random split sample validation test using mappable covariates only the metamodel showed high performance r2 0 79 tested on a 20 randomly sampled holdout conversely metamodel performance significantly decreased for the 9 spatial holdouts r2 ranging from 0 13 to 0 61 we document that the proposed transferability metric correlates with metamodel predictive performance and demonstrate its use to assess model transferability to datasets outside the training data spatial domain keywords machine learning predictive modelling model portability model generalization histogram distance drain partitioning 1 introduction decision makers in water resources management largely rely on model forecasts of system responses to develop management strategies and to support the implementation of environmental legislation such as the european water framework directive wfd in this context process based models are powerful tools to simulate complex hydrological systems as they integrate knowledge of the physical system and calculate hydrological states and fluxes at each model grid meanwhile immense requirements in terms of data calibration and computation time are among the key limitations of process based hydrological models being used for decision support asher et al 2015 clark et al 2017 metamodels also called surrogate models or model emulators provide a simpler and hence faster alternative blanning 1975 machine learning techniques are commonly used to make metamodels which emulate the specified output of the underlying complex model as a function of its paired inputs and outputs advantages of metamodels for decision support are three fold britz and leip 2009 fienen et al 2013 villa vialaneix et al 2012 i increased simulation speed to estimate a specific output ii screening level predictions in areas where a full physical model is not available iii easier integration with other model platforms accordingly metamodels can be used for on the fly simulations in a decision making process e g during stakeholder meetings and the increased simulation speed offers higher flexibility for data integration and exploration of uncertainty furthermore screening level predictions are often needed in areas where process based modelling is not possible due to lack of data and funding finally integrating outputs from metamodels with other simulation platforms is useful when a decision support tool must incorporate many different processes that are not available in one integrated model conversely the major drawback of metamodels is that increased computation speed comes at the cost of accuracy however for decision making applications approximate but fast decisions can be more valuable than precise predictions e g for screening purposes where numerous scenarios must be evaluated and where the variations in impacts between the individual scenarios are larger than the uncertainty of the metamodel nevertheless predictions for input regimes outside the training test set may be highly uncertain hampering the use of metamodels thus screening level predictions can only be made where input variable values are obtained for a region of similar conditions where the metamodel is considered valid metamodels have been extensively applied to groundwater modelling asher et al 2015 including contaminant transport in soils and groundwater bouzaher et al 1993 van der heijden and haberlandt 2015 nolan et al 2015 2018 piñeros garcet et al 2006 villa vialaneix et al 2012 groundwater age and residence time fienen et al 2018 starn et al 2021 starn and belitz 2018 groundwater response to sea level rise fienen et al 2013 sources of water to wells fienen et al 2016 and saltwater intrusion roy and datta 2019 however metamodels that reproduce outputs of fully distributed groundwater models are underrepresented in literature despite fully distributed groundwater models being abundant in practice asher et al 2015 metamodel predictions and any other machine learning application are highly uncertain beyond the spatial and temporal bounds of their training data however this is not reflected in current evaluation measures the predictive performance of machine learning metamodels is commonly evaluated using random k fold cross validation where the data set is randomly partitioned into folds or by testing the model on a randomly sampled holdout bouzaher et al 1993 nolan et al 2015 2018 sajedi hosseini et al 2018 villa vialaneix et al 2012 when the training and test set are randomly sampled from the data set they can be assumed to have similar covariate distributions however this assumption is often violated in practice when making predictions into new spatial domains unsampled space when a metamodel model is applied to a new spatial domain e g another catchment it may contain local biases that are not present in the training data which will likely cause a decrease in model performance fienen et al 2016 2018 van der heijden and haberlandt 2015 meyer et al 2019 concluded that spatial cross validation which uses spatially independent data sets for validation is essential in preventing overoptimistic model performance thus while the standard validation methods give valuable insights on the model s ability to reproduce variability in the training data they give no information on the model s ability to make predictions outside the training data named model transferability currently expert or a priori knowledge is used to evaluate whether a metamodel can be applied on a different spatial region starn et al 2021 or potential differences are ignored recent efforts to evaluate model transferability include the area of applicability for spatial mapping of environmental variables meyer and pebesma 2021 and the concept of applicability domain in the field of chemical modelling e g mathea et al 2016 toplak et al 2014 but are generally overlooked in spatial modelling however these methods are computationally complex as they rely on a point based approach evaluating the distance between each individual new test data point and all training data points across all covariates furthermore the methods from chemical modelling do not transfer intuitively to spatial data which is important when model end users should be able to compute and interpret model transferability this study proposes a metric for assessing model transferability that can be used as a screening tool to evaluate model suitability for making predictions into new spatial domains we show that metamodel transferability can be approximated by quantifying the dissimilarity between training and test data sets of covariate distributions computed as histogram distances weighted by covariate importance in an example application we apply drainage fraction as the metamodel target variable defined as the partitioning of the water flux between drains and groundwater artificial drainage in agriculture significantly influences the hydrological cycle boland brien et al 2014 watershed discharge king et al 2014 and the leaching of nutrients ernstsen et al 2015 field measurements of tile drain contribution of annual no3 loads to surface water ranges from 62 williams et al 2015 up to 90 92 rozemeijer et al 2010 thus tile discharge is a critical hydrological variable for determining nutrient losses to surface water and groundwater drainage fraction can be used as an indicator of surface water groundwater vulnerability to nitrogen application in agricultural catchments and thus it is a relevant parameter for decision making regarding spatially targeted measures to reduce losses to surface or groundwater in the landscape in this study we i develop a transferability assessment method based on differences between training and test data distributions of different areas weighted by covariate importance ii develop a random forest metamodel of drainage fraction where the response is derived from a process based distributed groundwater model iii compare random vs spatial cross validation to assess the transferability of the model to new catchment areas and evaluate the transferability method 2 material and methods 2 1 study area and metamodel data the storaa catchment is situated in western denmark and covers an area of 1000 km2 fig 1 elevations range from sea level to the west at the outlet of the river storaa to nissum fjord and the north sea to about 100 m above sea level in the eastern headwater the primary land use in the storaa catchment is agriculture 63 the catchment is dominated by sandy soils with some clayey less permeable soils the gentle topography in combination with a temperate climate average yearly precipitation 1000 mm requires significant parts of the agricultural land to be artificially drained mainly by tile drains groundwater levels in denmark are generally shallow koch et al 2019 and approximately 50 of the national agricultural area is artificially drained møller et al 2018 we derive drainage fraction predictions based on a fully coupled surface water groundwater mike she model of the storaa catchment stisen et al 2018 the storaa catchment model is at 100 m resolution and calibrated against transient discharge and groundwater head measurements 2000 2006 in the model drains are implemented homogeneously throughout the catchment at 1 m below ground level mbgl drainage fraction ranging between 0 and 1 is calculated as the average partitioning of infiltrating water to the saturated zone between drains and groundwater in each model grid cell a drainage fraction of 1 means that all infiltrating water in the grid cell is transported via drains to surface water recipients datapoints with daily negative recharge upwelling and corresponding datapoints for drainage are set to zero because drainage fraction considers only the partitioning of infiltrating water positive recharge between tile drains and groundwater daily outputs of drainage and recharge to the saturated zone are extracted for the period 2000 2006 the drainage fraction is calculated for each 100 m grid as the temporal mean drainage divided by the temporal mean recharge to saturated zone lakes urban areas forests and streams are masked out out of the 77 333 datapoints each representing a 100 100 m cell 57 have drainage fraction 0 and consequently the distribution is skewed towards zero fig 2 a b the catchment is dominated by sandy soils and drainage mainly occurs in clayey soils as seen in the northern part and along streams fig 2e this promotes the storaa catchment as an excellent candidate to test metamodel transferability because the landscape and the drainage fraction are spatially dependent we apply drainage fraction as the metamodel target variable thus the study must assume that mike she correctly describes the physical processes which control drainage in the following we consider the drainage fraction from the mike she model as our reference and focus on the errors of the metamodel in comparison to that thus any errors in the formulation are not explicitly estimated in the metamodel the metamodel covariates table 1 span the physical factors that are expected to influence drainage flow and recharge to the saturated zone the covariates are mapped to the same resolution 100 m as the input to the groundwater model both mappable and non mappable covariates were considered as a first step to identify covariates mappable covariates refer to quantities that are readily available without requiring a model to derive them the metamodel using all covariates is considered a reference model for metamodels created with the goal of applying to a currently unmodeled area only the mappable predictor variables are relevant consequently only mappable parameters are included in the following metamodel transferability analysis 2 2 random forest regression the metamodel uses a random forest rf machine learning regressor that is trained to emulate the target variable drainage fraction rf breiman 2001 is an ensemble model applying bagging as the ensemble method and regression decision tree as the base model we chose rf because they are simple to train require little tuning and are robust to overfitting hastie et al 2009 we applied the scikit learn v 0 22 1 rf regressor package in python v 3 7 pedregosa et al 2011 the strength of rf relies on building an ensemble of decorrelated and unbiased decision trees and then reducing its variance by averaging the trees each decision tree is grown by recursively splitting the data into more homogeneous groups until the minimum node size is reached rf possesses two important elements of randomness to reduce the correlation between trees first a unique bootstrapped data sample is created for each tree where about two thirds of the data are randomly sampled with replacement and the remaining one third is left out these are known as the out of bag oob samples second a random subset of input covariates is selected for each split in the tree growing process hastie et al 2009 thereby each bootstrap tree will involve different covariates and training data and may have a different number of terminal nodes i e leaves the oob feature allows random forests to be fit with internal cross validation being performed simultaneously hastie et al 2009 the oob error is calculated on the oob samples excluded from the training for each tree the final bagged estimate can be derived as the average prediction across all trees where data was retained for training the oob error is also used for model validation purposes amare et al 2021 koch et al 2019 2 3 hyperparameter tuning and baseline models rf has five hyperparameters in the scikit learn implementation v 0 22 1 that control model complexity and regularization four of which we adjusted for tuning table 2 n estimators max depth min samples split min samples leaf and max features we used a fixed value of 0 33 for max features hastie et al 2009 p 592 for hyperparameter tuning we included all covariates and applied 5 fold cross validation on a randomly sampled data set 80 of the entire data set of 77 333 datapoints evaluating each permutation of tuning parameters table 2 resulted in 180 candidate models from which we selected the hyperparameter set that gave the best mean score coefficient of determination r2 across the five folds using the optimal hyperparameter set we trained two baseline metamodels one including all covariates and the second including mappable covariates table 1 both metamodels were trained on the same 80 randomly sampled datapoints and tested on the corresponding 20 holdout 2 4 transferability assessment the following paragraphs present the transferability assessment method the method approximates metamodel transferability by quantifying the differences in covariate distributions between training and test data sets i e how much the test spatial domain differs in its physical properties topography soil type etc compared to the training data we quantify differences as histogram distances between train and test covariate distributions not all covariates are equally important in the rf model thus the distances are weighted by covariate importance finally the correlation between weighted distance and model performance is used to predict model performance in a spatial holdout i e model transferability the transferability assessment proceeded in six steps fig 3 1 compute differences between training and test sets for all covariates expressed as histogram distances 2 compute individual covariate importances 3 compute weighted distance wd across all covariates by weighted linear multiplication using covariate importance as weighting factor this is the metric of transferability 4 conduct spatial cross validation to obtain predictive model performance 5 evaluate transferability metric by correlating wd with predictive model performance r2 6 predict model transferability using linear regression models and evaluate against spatial cross validation results 2 4 1 histogram distance metrics transferability between data sets is evaluated by quantifying the difference expressed as a distance between normalized histograms of covariates the training and test set of each covariate are standardized for the histogram comparison to be independent of specific units 1 x i j s x i j x j σ j x i j s is the scaled value of j th predictor for the i th observation x j and σ j are mean and standard deviation respectively of predictor j where mean and standard deviation are computed over the combined training and test data furthermore to make the histogram comparison independent of sample size the histograms are normalized so each bin contains the value of the probability density function at the bin such that the integral over the range is 1 for comparison of continuous covariate distributions we apply four histogram distance metrics d1 d4 cha and srihari 2002 the histograms hb a and hb b of data set a and b where b is the number of bins are treated as b dimensional vectors the more similar the two histograms are the smaller the value d a b is the histograms are normalized with equal size and width of bins the standard vector norms can be used as distances between the two histograms city block l1 norm 2 d 1 a b i 0 b 1 h i a h i b the city block distance represents the sum of the bin wise absolute differences between the two histograms euclidian l2 norm 3 d 2 a b i 0 b 1 h i a h i b 2 the euclidian distance is used to determine the bin to bin difference between two histograms it is the square root of the sum of the squared differences of the histograms minimum difference of pair assignments cha and srihari 2002 4 d 3 a b min a b i j 0 n 1 d a i b j given n elements ai a and n elements bj b and d is the paired absolute distance between two elements the minimum difference of pair assignments accounts for the shape of the non overlapping area of the histograms unlike d1 and d2 the distance between two histograms of n elements a and b is expressed as the minimum distance of pair assignments between the two sets the problem is to determine the best one to one assignment between two sets such that the sum of all distances between two individual elements in a pair is minimized intersection 5 d 4 a b i 0 b 1 m i n h i a h i b i 0 b 1 h i a 1 the fourth metric computes the intersection between two histograms intersection is multiplied with 1 to have the same interpretation across metrics i e high distance corresponding to low similarity for comparison of categorical covariates we compute the absolute difference between the relative size of each class of the categorical variable and sum across all classes c the distance between the train and test sets a and b of sample size n a and n b is computed as 6 d cat a b i 1 c a b s r a c r b c where c is number of classes of the categorical covariate r a and r b are the relative sizes of class c in a and b defined as the ratio between the number of observations of class c in data set a and the total number of observations in data set a 7 r a c n a c n a the covariate analysis assumes that all input covariates are available for the holdout set at the same resolution as the training set 2 4 2 weighted distance as transferability metric for each distance metric the covariate distances were aggregated linearly and weighted by their covariate importance 8 wd j 1 n w j d j where wd is weighted distance w is weight importance of covariate j and d is distance of covariate j consequently covariates with a higher importance have a larger influence on the wd we computed individual covariate importance as permutation importance defined as the fractional decrease in model score r2 when a single covariate is randomly permuted thus ranging between 0 and 1 breiman 2001 we used the rfpimp python package parr and turgutlu 2019 and not the scikit learn random forest in built feature importance function the latter is biased towards high cardinality covariates and is unreliable when covariates vary in their scale of measurement or their number of categories strobl et al 2007 however when two covariates are correlated and one of them is permuted it may show an alleged low importance because the model will still have access to the information through its correlated feature therefore we also permuted covariates by groups topographic soil texture precipitation and land use table 1 the groups were based on covariate physical relationships and supported by a correlation matrix to identify the degree to which individual covariates were correlated 2 4 3 spatial cross validation we divided the storaa catchment into sub basins using the watersheds basins module in the q gis saga 2 3 2 toolbox conrad et al 2015 we discarded one sub basin with 100 grid cells that occurred as an artefact of the confluence of multiple branches using the remaining nine sub basins fig 2c we trained nine separate metamodels where each of the sub basins was withheld in succession from the training and used for testing as a spatial holdout named sb1 9 and compared with the random 80 20 split sample named rdm only mappable covariates were included in the spatial cross validation for each of the 10 training and test data sets 9 spatially defined 1 randomly sampled we calculated r2 for the prediction on i the training set ii the holdout and iii the oob prediction 2 4 4 wd performance correlation and transferability from linear regression models we evaluated the correlation between metamodel predictive performance r2 and wd for the 10 metamodels using the pearson correlation coefficient ρ p which measures the strength of a linear correlation negative correlations are expected between wd and r2 since larger differences between the training and test set in principle should lead to poorer metamodel performance for our 10 fold spatial cross validation we tested if the metamodel performance can be predicted from the wd of a holdout based on linear regression on the remaining 9 holdouts thus for each holdout we produced a linear regression model fitted against metamodel r2 using wd as input the predicted model performance for each hold out was subsequently evaluated against the true model performance computed in the spatial cross validation 3 results 3 1 metamodel output we developed metamodels based on all available data and using mappable data only for transferability mappable data are key the mappable metamodel runtime for predicting drainage fraction over the entire storaa catchment was 3 s whereas the mike she model runtime was 9 h both metamodels perform well however the metamodel r2 decreases from 0 85 to 0 79 when only mappable covariates are included fig 4 a b this is primarily due to the information loss of the non mappable soil texture covariates fig 4c both metamodels capture reasonably well drainage fraction 0 7 but neither accurately represent drainage fraction 1 the topographic covariates have the highest grouped importance for both metamodels fig 4c topography is expected to influence surface runoff and infiltration the second most important group considering all covariates is soil texture soil texture is expected to largely influence drainage with most of the drainage taking place in clayey soils where the infiltration rate is lower in sandy soils the water can easily infiltrate and recharge groundwater resources however when the non mappable soil texture covariates are removed precipitation becomes the second most important covariate group 3 2 spatial cross validation the training skill is very high across all 10 metamodels training r2 0 89 meaning that the random forest model has enough capacity to nearly reproduce the training set the oob r2 ranges from 0 79 to 0 82 across all metamodels and the randomly sampled holdout shows an r2 of 0 79 fig 5 conversely the hold out performance is poorer in the spatial cross validation where metamodel performances drop to 0 15 0 60 fig 5 this significant decrease in performance is a result of local biases in the holdout sub basins sb1 9 not reflected in the training data furthermore the spatial holdouts differ in terms of the distribution of the largest errors between mike she derived drainage fraction and metamodel predicted drainage fraction fig 6 sb1 and sb3 resemble the randomly sampled metamodel the most in terms of performance and distribution sb7 and sb8 fail to predict drainage fraction 0 whereas sb9 reaches a reasonable average performance due to good performance in the mid range despite low performance at high drainage fraction generally the highest errors occur at high drainage fraction which is primarily observed close to streams and in clayey soils 3 3 model transferability assessment the transferability method combines a covariate distribution distance metric weighted by covariate importance and it correlates the weighted distance with metamodel performance through spatial cross validation selected intermediate results of this stepwise approach section 2 4 are shown in fig 7 for all training and test data sets sb1 9 and rdm histogram distances were computed across all covariates illustrated by relative topo r for sb4 and sb8 fig 7a and individual covariate importances were computed for weighting fig 7b for each covariate model performance r2 and distance was correlated across the 10 training and test data sets relative topo r fig 7c when the distances are weighted by covariate importance the linear correlation is strengthened seen as an increase in pearson correlation coefficient fig 7d finally the weighted covariate distances are aggregated linearly across covariates resulting in one correlation plot for each distance metric fig 8 we observe negative pearson correlation coefficients between r2 and wd across all distance metrics both weighted and unweighted with the city block metric resulting in the strongest correlation of 0 82 fig 8 and table 3 thereby we document that the metamodel performance decreases as the linear sum of weighted distances between the covariate distributions of the training domain and test domain increases in other words the more different the train and test sets are the lower the model performance is the rdm data set shows almost identical distributions for the train and test data whereas the spatial sub basins show different degrees of histogram overlap and similarity in shape due to local biases consequently the rdm data set shows the lowest distance along with the highest performance across all metrics fig 8 the wd performance correlation is strengthened for the city block min dif pair and euclidian metrics when the covariates are weighted by their importance table 3 this is because on average the stronger correlations are found for covariates with high importance e g relative topo and prec fall however for categorical covariates the correlation is weakened when weighted by importance pearson decreases from 0 57 to 0 35 because the soil class covariate with the highest importance has a positive correlation the predicted model performances i e model transferability obtained from the linear regression models are in good agreement with the spatial cross validation model performance fig 9 the higher the predicted r2 the more transferable the model is as expected the results attest highest transferability of the metamodel where train and test sets are randomly sampled and thereby similar in covariate distributions model transferability is overestimated for the two sub basins sb7 and sb8 as the predicted r2 is higher than the spatial cv derived r2 however for the remaining metamodels the predicted r2 gives an accurate and mostly conservative estimate of model transferability 4 discussion 4 1 transferability of rf metamodel for predicting drainage fraction this study presents a novel method for assessing model transferability to new spatial domains evaluated on a random forest metamodel for predicting drainage fraction the method uses a transferability metric wd computed from covariate histogram distance metrics weighted by covariate importance different histogram distance metrics were tested and city block was found to give the strongest correlation with r2 city block was only slightly better than min dif pair although it is a mathematically much simpler metric intersection neglects the non overlapping parts of the histogram thus it is not surprising that it is not the best metric the euclidian distance metric squares the difference and thereby more weight is put on bins with a large distance even though the remaining histograms are very similar our results demonstrate that the city block wd is a suited metric to assess differences between train and test sets and that wd can be used to quantify and assess model transferability when used as an input in linear regression models fitted to spatial holdouts of the training domain our findings confirm that random k fold cross validation leads to an overoptimistic view on model performance in the face of bias when applied to an unsampled region as illustrated by fienen et al 2018 and concluded by meyer et al 2018 we show that model performance can vary significantly between seemingly similar training and test data sets in this case sub basins within the same catchment this understanding is highly relevant for metamodels and machine learning models that are trained on a unique data set and applied to other independent data sets such as different catchments or geographical regions or used for temporal extrapolation the proposed transferability method provides a quantitative screening tool which can be used to identify suitable areas of model application the binned histogram analysis allows model end users to compare similarities of training data and target domain i e different catchments both visually and quantitatively the proposed method offers an objective and quantitative alternative to expert or a priori knowledge furthermore the transferability method may be able to guide the appropriate choice of training data for machine learning predictions at a target site by evaluating weighted histogram distance i e whether the added data lies in the relevant region histogram of the relevant parameters covariate importance nevertheless a priori knowledge should still be considered in combination with the transferability assessment in the case where the target domain is dominated by covariates not included in the training set e g in a different geological region with mountains and exposed bedrock the oob and randomly sampled holdout performance of the two metamodels mappable and all covariates show that a high proportion of the variance in drainage fraction predictions made using the numerical mike she model can be explained by the rf metamodel these results confirm that the metamodels capture a large amount of the system behaviour inherent in the distributed hydrological model they are trained on the metamodel obtains a performance of r2 0 79 for the randomly sampled and thereby most representative training data the metamodels have a good fit for drainage fraction 0 7 which can be attributed to an under representation of training data in the range from 0 7 to 1 the decrease in metamodel accuracy relative to the mike she model is traded for a significant improvement of the simulation runtime from 9 h to 3 s that promotes model application for decision support mapping drainage fraction based on catchment characteristics could potentially support decision making for spatially targeted n measures to reduce losses to surface or groundwater i e land cover modification and application of different land management options provided the short model runtime the effects of different management scenarios could be evaluated on the fly with local stakeholders the transferability method presented in this paper would allow model end users to assess whether the metamodel could be applied to a certain catchment transferability is not only relevant for metamodels it is relevant for any ml model applied to data which differs in distribution from the training data this also includes ml models for predictive mapping between point observations where normally field samples used as training data are unevenly distributed over the study area exemplified by naghibi et al 2020 and ransom et al 2017 increasing computational power has resulted in ml methods becoming more frequently used in water applications in the past decade tyralis et al 2019 and available for practical applications in decision making this calls for an improved understanding and quantification of model uncertainties and limitations to their application to which this paper contributes 4 2 limitations and future work a key limitation of metamodels is assuming that the underlying numerical model is an adequate representation of the physical system improving drainage representation in groundwater models is widely studied hansen et al 2013 2019 de schepper et al 2015 shafii et al 2019 thomas et al 2016 furthermore we are aware that the target variable distribution is skewed towards zero due to catchment characteristics which means we can expect some stability issues in the regression trees solutions on how to accommodate this are discussed in literature including beta forest method weinhold et al 2020 which could be a possible alternative in further studies as the presented rf models have reasonably high performances we have not addressed the issue further the transferability analysis is based on a single set of sub basins covered by the same underlying numerical model we do not know yet how it generalizes to other catchments or how the results extend to other types of metamodels and covariates covariate biases between different catchments are expected to be larger than the nine sub basin biases observed within the storaa catchment however we believe that the transferability methodology presented here provides useful insights on the transferability of metamodels to un modelled areas beyond the storaa catchment 5 conclusions we successfully develop a rf metamodel of a distributed integrated hydrological model using mappable parameters only which reliably predicts drainage fraction our findings confirm that random k fold cross validation cv can lead to an overoptimistic view on model results compared to spatial cross validation we document that metamodel performance decreases as the weighted histogram distance between the covariate distributions of the training domain and test domain increases the proposed distance metric and spatial cross validation can be used to assess transferability of models we finally conclude that it is essential to assess the transferability of a metamodel if the model is intended to be used beyond the training data for this purpose the proposed distance metric can be used to identify suitable areas of model application data availability the data and python code to estimate model transferability is available from the github repository https github com elisabjerre model transferability credit authorship contribution statement elisa bjerre conceptualization methodology software formal analysis visualization writing original draft writing review editing michael n fienen conceptualization methodology software formal analysis writing review editing raphael schneider methodology writing review editing julian koch methodology writing review editing anker l højberg conceptualization methodology writing review editing supervision funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work has been carried out under the waterprotect research project supported by the european union s horizon 2020 research and innovation programme under grant agreement no 727450 any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government 
3082,as a significant hydrological parameter the water level of complex and shallow river lake wetland has important implications for a number of hydrological biological physical and chemical processes current in situ and satellite based methods both have disadvantages in capturing water levels over smaller surface water bodies with spatial heterogeneity or complexity interferometric imaging radar altimeter inira is a latest generation of ku band radar altimeter onboard tiangong 2 tg 2 the chinese space laboratory launched in 2016 similar to the design idea of nasa s swot it is expected to improve this situation through wide swath altimetry however the capacity of the tg 2 inira to measure water levels within small surface water bodies remains unexamined to explore these performances particularly their potential impact on hydrology research an assessment and comparison between in situ and tg 2 derived measurements was performed over the largest river lake wetland in china the results showed that the tg 2 inira with a broad range of coverage was able to effectively capture most of the surface water at its elevation for the focused shallow depressions a relatively good correlation was observed between the in situ and inira derived water surface elevations wses with an r2 of 0 97 rmse of 0 29 m and absolute difference ranging from 3 to 55 cm the accuracy of inira derived wses was highly controlled by the depression sizes and the absolute biases broadly increased in a power law fashion as the size of the depression decreased in addition surface water profiles along the ganjiang and xiushui river channels indicated that the tg 2 inira could also measure river wses with absolute differences below 0 43 m although differences were observed the results of the river gradient distribution were consistent with the practical situation and showed overall slopes of 18 72 cm km and 12 43 cm km for the xiushui and ganjiang rivers respectively such results demonstrated that tg 2 inira measurements can satisfactorily capture spatial patterns of the surface water gradient which is useful for accurately estimating river discharge this study will not only bridge the gap between in situ measurements and forthcoming swot products but also contribute to water resource management and hydrologic service assessments over data sparse regions of river lake systems within the study area keywords tangong 2 interferometric imaging radar altimeter small water bodies water level river gradient performance 1 introduction surface water refers to water retained in rivers streams lakes reservoirs and wetlands and accounts for less than 1 of the total amount of water on earth shiklomanov 1993 lehner and doll 2004 however it has a large impact on the hydrological and biogeochemical cycles strayer and dudgeon 2010 becker et al 2018 melo and getirana 2019 as a significant hydrological parameter the water level influences the transfer of water and energy between atmosphere and the land water surface vassiljev et al 1995 vincent 2009 quantifying the water level and its dynamics is highly relevant to a number of hydrological biological physical chemical processes and some industrial applications becker et al 2018 panagopoulos 2020 panagopoulos 2022 that are crucial for water resource management and other ecosystem services associated with surface water frappart et al 2012 wu and liu 2015 messager et al 2016 cael et al 2017 traditionally our knowledge of water level and its dynamics relies on in situ based measurements berg 1992 calmant et al 2008 but acquiring water level data across remote areas is particularly expensive and labor intensive smith 1997 alsdorf and lettenmaier 2003 over the past three decades the advent of earth observation technology has offered more opportunities to monitor water levels at global and regional scales several accepted satellite based methods have been developed to facilitate this purpose smith 1997 frappart et al 2006 schumann et al 2008 pan and nichols 2013 gao 2015 recently radar altimetry has emerged as a new and promising technique for directly measuring water levels from space bates et al 2013 tourian et al 2016 pham et al 2018 several altimeter satellites have also been launched cretaux et al 2015 birkett et al 2022 and widely used to derive water levels in large lakes and reservoirs troitskaya et al 2012 tong et al 2016 xu et al 2020a chen et al 2022 however traditional altimetry satellites only provide point based measurements with footprints from 3 to 5 km wide and spatial gaps greater than 100 km gao 2015 which are not ideal for capturing water levels over smaller water bodies smaller than 1 km2 or surface water with high spatial heterogeneity or complexity alsdorf et al 2007 calmant et al 2008 jiang et al 2020 thus quantifying water levels over these water bodies remains challenging papa and frappart 2021 cooley et al 2021 next generation three dimensionalimaging altimeter is expected to improve this situation through wide swath insar interferometric synthetic aperture radar prigent et al 2016 biancamaria et al 2016 currently to address this issue a proposed mission named swot surface water ocean topography based on the concept of wide swath imaging altimetry has been jointly developed by the united states and france which is expected to be launched in 2022 pavelsky et al 2014 biancamaria et al 2016 grippa et al 2019 similar to the design idea of swot through equipping a couple of near nadir interferometric imaging radar altimeters iniras china s tiangong 2 tg 2 space laboratory have acquired three dimensional imaging radar altimetry measurements globally from 2016 kong et al 2017 li et al 2018 zhang et al 2021 the tg 2 inira is the first spaceborne wide swath radar altimeter mainly aimed at generating spatially distributed measurements of water surface elevation which makes it possible to accurately measure much smaller water bodies than previously given this great potential it is imperative to determine the ability of the tg 2 inira to measure water levels in small water bodies which is currently unknown therefore the main objectives of this paper are to 1 quantify the water level from the tg 2 inira within small water bodies 2 assess the accuracy and influencing factors of the tg 2 inira derived water levels and 3 explore whether the accuracy of tg 2 observations is sufficient for capturing river gradients which are critical for characterizing the spatial hydraulics of regional river systems this paper is structured as follows section 2 introduces the study area and data used with an overview of the data collection and processing included in section 2 2 section 3 describes the methodology for estimating water levels from noisy tg 2 data and the validated analytical methods for quantifying water level section 4 presents the results section 5 discusses the conclusions 2 study area and data 2 1 study area the study area was chosen in the poyang lake wetland of china 28 22 29 45 n 115 47 116 45 e an internationally protected wetland that provides habitat for migratory birds and it is located in the middle reaches of the yangtze river the area receives seasonal water flows from five major tributary rivers namely xiushui ganjiang fuhe xinjiang and raohe and exchanges water with the yangtze river via the hukou fig 1 a which greatly contributes to the considerable variability of inundation extent and water level throughout a year wu and liu 2015 in the wet season april september the water levels reach a peak along with the inundation area exceeding 3000 km2 shankman et al 2006 in the dry season october march however the inundation area shrinks to less than 1000 km2 feng et al 2012 during this period the area is characterized by a wide floodplain hundreds of independent saucer shaped shallow depressions as small as 1 km2 and distributary channels with varying water levels and hydrologic connectivity from south to north wu and liu 2017 fig 1b which largely determine the habitat conditions for thousands of migratory birds barzen et al 2009 li et al 2019 given the complexity of the lake floodplain system and the importance of wetland conservation hydrological characteristics of the poyang lake have attracted wide attention from many scholars in recent years chen et al 2018 wang et al 2019 xu et al 2020b liu et al 2020 most previous studies primarily focused on water bodies of entire lake and none have comprehensively addressed the water level changes in small shallow depressions due to the limited measurements to assess the performance of the tg 2 wide swath imaging altimeter observations such a highly dynamic water regime and numerous independent smaller depressions make the poyang lake wetland an ideal test area in this study we chose to examine only the southwestern region of the poyang wetland that presents most saucer shaped shallow depressions within the study area fig 1b specifically eight typical depressions were selected as study sites from south to north these sites were chosen due to their varied size ranging from 2 to 80 km2 distinct morphometric characteristics and availability of in situ data for validation a list of the selected depressions and their geographic features are shown in table 1 fig 1d also displays a map that is magnified on the focused depressions in this subfigure some of the in situ sites used for validation are annotated with their ids additionally considering the accessibility of the data ganjiang river and xiushui river were also focused on for the purpose of assessing tg 2 s performance in characterizing the water surface slope fig 1d 2 2 data collection and preprocessing this study uses satellite imagery collected by tg 2 and sentinel 2 satellite platform in situ field measurements and other ancillary data 2 2 1 tg 2 inira data inira is a latest generation of ku band radar altimeter onboard tg 2 which launched on september 15 2016 and runs in a non sun synchronous orbit at an altitude of approximately 400 km it is dedicated to enabling three dimensional imaging measurement of the surface water height by integrating interferometry synthetic aperture and traditional altimeter height tracking techniques and it has a wide swath of 40 km li et al 2018 kang et al 2019 unlike traditional radar altimeters tg 2 inira adopts a short baseline 2 3 m and a near nadir incidence of 2 5 7 5 to acquire a high coherence interferometric phase from surface water which makes it possible to measure water surface height with wide swath and inland water bodies lakes rivers wetlands etc thus yielding a spatial resolution of 40 m kong et al 2017 zhang et al 2018 tg 2 inira data were produced by the technology and engineering center for space utilization chinese academy of sciences each inira dataset consists of four standard products in the form of an image radiometric correction product as a complex valued image level 1 a geometric correction product as a backscatter image level 2 an orthophoto correction product level 3 and a digital elevation model product with a universal transverse mercator utm projection level 4 kang et al 2019 in this study four level 4 products of the tg 2 inira as georeferenced raster images of elevation in meters above the world geodetic system 1984 wgs 84 ellipsoid were collected on 16 september 2017 over the focused shallow depressions the covered area was 30 km east to west and 110 km north to south as presented in fig 1b these data were acquired from the space application data promoting service platform for china manned space engineering https www msadc cn 2 2 2 in situ data for this study we evaluated the tg 2 inira s performance mainly through in situ measurements of elevation and position first eight sets of solinst 3001 self logging pressure transducers pts solinst ontario canada were fixed in the selected shallow depressions to record accurate measurements of surface water elevation fig 1c the reported accuracy for the transducers at a maximum depth of 10 m is 1 cm wu and liu 2017 water level measurements were continuously logged every 30 min from february 8 2015 and later aggregated to daily means additionally daily average water level records based on local wusong height datum from three gauging stations i e yongxiu wucheng and changyi stations were also collected for assessing the tg 2 inira retrieved river gradients fig 1d these records were acquired from the hydrological bureau of jiangxi province during the study period in fig 1d the location of each transducer is shown as a green dot while the location of the in situ gauging station is shown as a black circle with a cross inside in order to effectively evaluate the performance of the tg 2 inira measurements different types of water level measurements were transformed to the same vertical reference of the wusong height datum for this purpose several ground checkpoints along the focused depressions and water bodies of the main lake were surveyed using a huace a220gr real time kinematic rtk gps receiver huace ltd shanghai china on november 5 2015 fig 1c we used rtk gps surveys to convert the in situ measurements to absolute water levels by measuring the height difference between the water surface and the gps surveyed benchmarks the rtk system provides precise ground position and elevation with an accuracy as high as 1 cm bisnath et al 2004 tang et al 2010 and it can potentially be used to calibrate in situ measurements of water level to the same referenced datum of gauging stations 2 2 3 ancillary data two ancillary datasets namely sentinel 2 imagery and digitized topographic maps were also used in this study the selected sentinel 2 images bands 2 3 4 and 8 with 10 m spatial resolution were acquired on 18 september 2017 from the european space agency esa sentinel science hub https scihub copernicus eu these images are closest to the tg 2 inira observations with relatively less cloud coverage thus they were applied to delineate water masks the topographic digital map of the poyang lake wetland at a scale of 1 100000 hydrological bureau of jiangxi province china 2011 was also used in combination with sentinel 2 images to identify the depressions and streams all of the acquired imagery were projected onto the universal transverse mercator utm with the wgs 84 zone 50 n coordinate system 3 methodology 3 1 water mask derivation in this study we focus on the performance of the tg 2 inira in estimating the surface water level within complex river lake system to fulfill this goal we first generate a water mask from simultaneously acquired sentinel 2 images to isolate only open water pixels in the tg 2 inira data using a normalized difference water index ndwi which was calculated as follows mcfeeters 1996 1 ndwi d n g r e e n d n nir d n g r e e n d n nir where dngreen is the green band b3 and dnnir is the near infrared band b8 in the sentinel 2 data it has been proved to be one of the most effective index in detecting water surfaces jain et al 2005 liu and wu 2015 to enhance the detection accuracy a localized threshold segmentation algorithm was then used for each focused area otsu 1979 liu et al 2012 and the ndwi pixels were classified into water values greater than the threshold and non water values less than the threshold fig 2 3 2 tg 2 inira data filtering according to the derived water mask the tg 2 surface water elevation over the study sites can be extracted by multiplying the tg 2 inira images with the water mask to assess the ability of tg 2 to measure surface water elevations within complex and shallow wetlands several filtering and spatial averaging techniques are performed to refine the tg 2 inira images before comparing them with the in situ measurements first we employ a 2 km2 moving window to filter out extreme outliers 3 standard deviations and remove them from the mean tg 2 inira measurements which helps remove contaminated pixels of layover and improper height estimation from adjacent vegetation mudflat or misclassified water land pixels goldstein et al 1988 biancamaria et al 2016 second to quantify the water level differences between the tg 2 inira data and in situ measurements for each study site we also calculate the mean tg 2 elevation by spatially averaging within a square window of 10 10 pixels from the filtered tg 2 data which is commonly applied to reduce independent random errors rodriguez and martin 1992 altenau et al 2019 3 3 validation and uncertainty analysis once the averaged measurements were determined the performance of tg 2 derived wses was validated through comparing with simultaneously acquired in situ observations and the coefficient of determination r2 was then performed to assess the consistency between in situ measurements and tg 2 retrieved wses furthermore several statistical indicators including the root mean square error rmse absolute bias ab and mean absolute bias mab were calculated described as follows around each in situ site to quantify the performance of the tg 2 measurements 2 rmse i 1 n t i m i 2 n 3 ab t i m i 4 mab i 1 n t i m i n where t represents the tg 2 inira measurement m is the in situ measurement n is the in situ site number and i is the sequential order among these indicators the absolute bias was used to evaluate the magnitude of difference between inira derived wses and in situ measurements rmse describes the absolute accuracy of the inira derived water level accounting for all the effects of random and systematic errors 4 results 4 1 determination of tg 2 wses fig 3 shows the processing results of tg 2 wses over the study area as seen in fig 3c several contaminated pixels within shallow depressions are clearly shown which indicate improper estimations of wse likely resulting from a combination of wetland vegetation and more complex adjacent topography in fig 3d the areas affected by wse noise and outliers were successfully removed by standard deviation threshold filtering and spatially average processing the effect of the processing on the histogram of the data can be seen in fig 4 which illustrates the histogram distributions of inira wse pixels for the focused shallow depressions before blue and after orange filtering the histogram distributions of the original image present a relatively smooth variation of wse values after processing the histogram of the data showed a large improvement the image processing reallocates selected wse values to move them toward the center the frequency was also remarkably increased and a higher peak appeared in the processed histograms which indicates that the processing is effective to avoid large wse biases in the final results 4 2 tg 2 wse validation considering that the wgs 84 height datum inira derived measurements and the wusong height datum in situ measurements was about 0 4 m lower and 1 65 2 26 m higher than the china 1985 national height datum in the study area respectively li et al 2015 lin et al 2016 therefore the datum biases of all measurements were quantified and transformed into the wusong datum the validation of inira based wses with coincident pt measurements within the focused depressions is illustrated in fig 5 as a scatter plot and their differences are summarized in table 2 inira based wses for each shallow depression were calculated as the mean value within the square window of 10 10 pixels overall there is a relatively good correlation between the pt and inira based wses with r2 of 0 97 and rmse of 29 cm larger negative biases were shown across the focused depressions which indicates that the inira generally underestimated the wse compared to in situ measurements the absolute difference between the inira and pt based wse ranges from 3 to 55 cm with a mab of 24 cm the analysis results indicate that although the tg 2 inira has relatively comparable performance it cannot guarantee incredibly high accuracy for wse estimations fig 6 represents the distribution of the absolute biases between the inira and pt based wse for all 8 shallow depressions as illustrated in fig 6 the magnitude of absolute biases ranges widely depending on the size of the depressions absolute bias is generally higher than that of depressions with larger mask areas and the bias broadly increases in a power law fashion as the size of the depression decreases specifically the absolute bias of wse increased from 3 3 cm for the largest mask area l06 21 11 km2 to 55 7 cm for depression l03 which had a much smaller area of 2 2 km2 two of the other six depressions smaller than 2 2 km2 also had higher absolute bias values of more than 17 cm one possible explanation for the correlation between the absolute bias and mask area is that the smaller depressions tend to be shallow and probably have more emergent vegetation or mudflats these geophysical factors may lead to greater inira elevation biases and make it more difficult to filter out contaminated areas in addition other factors such as differential incidence angles interferometric phase drift and the inira wse retrieval algorithm may also determine this situation which could explain the lack of a constant correlation between the absolute bias of wses and the size of the depressions 4 3 river gradient mapping and assessment knowledge of a river s gradient is fundamental for the hydrologic and geomorphologic characterization of river systems jung et al 2010 gonzález del tánago et al 2016 to further investigate tg 2 s performance in characterizing the water surface slope two channel longitudinal profiles of wse across the centerline of the ganjiang river and xiushui river were generated and converted into point features to illustrate clearly spatial patterns of surface water gradient the average centerline point heights were implemented with a 1 km gaussian filter and then plotted versus distance surface water gradients were also calculated by using a moving 1 km window with an advance step of 400 m 100 points fig 7 illustrates the longitudinal wse profiles and corresponding surface water gradients along the xiushui and ganjiang rivers there were dominant hydraulic wses from south to north along the longitudinal profiles with values ranging from 9 59 to 14 94 m for the ganjiang river and 8 61 16 97 m for the xiushui river which were consistent with the flow directions of these two rivers the inira derived wses also showed good performance when compared with in situ water level measurements we noted that inira data generally overestimated the wse measurements and the absolute differences were 0 43 m 0 12 m and 0 42 m at yongxiu station changyi station and wucheng station respectively a possible explanation is that wse values along river longitudinal profiles are more sensitive to contamination by riverbanks which could be incorrectly interpreted as surface water signals additionally a map of the 1 km reach length mean gradient of the ganjiang and xiushui river is shown in fig 8 it reveals heterogeneous water surface gradients along the longitudinal profiles with a wide range from 0 to 110 cm km the phenomenon is likely affected by the meander morphology and differentiated topography along the rivers in general river gradients tended to be higher near upstream and lower gradients downstream fig 8 specifically the surface water gradient of the xiushui river was relatively steeper with an average slope of 21 57 cm km for the 1 km reach length and an overall slope of 18 72 cm km for the full longitudinal profile fig 7b in contrast the ganjiang river which isrelativelystraight and has many branches with a higher connectivity degree to other rivers or lakes had an average slope of 20 67 cm km for the 1 km reach length and an overall slope of 12 43 cm km for the full longitudinal profile fig 7c although the in situ measurements were not sufficient to validate these fine resolution river gradient estimates the gradient distribution results are in accordance with the practical situation shankman et al 2006 5 conclusions and discussion in conclusion characterizing fluctuations in water surface elevation over many of the world s small water bodies through current in situ or altimeters also remains challenging ku band wide swath inira onboard chinese tg 2 space laboratory is a new generation of radar altimeter which similar to the design concept of nasa s swot and makes it possible to improve this situation but no public work has been reported on its performance to our knowledge this is the first study to investigate the capability of the tg 2 inira observations for water level monitoring over spatially complex river lake wetland regions the poyang lake wetland which is the china s largest freshwater lake and presents complex river lake floodplain system was selected as the study area to evaluate its capabilities we compared tg 2 inira wses and gradients to in situ measurements within the selected shallow depressions and rivers it has been demonstrated that the tg 2 inira with a broad range of coverage can effectively capture the wses for most surface water bodies for focused eight shallow depressions strong correlations were observed between the wses derived from inira and in situ measurements with coefficient of determination r2 and rmse of 0 97 and 0 29 m we found that the absolute bias ranged from 3 to 55 cm with a mab of 24 cm the precision of inira derived wses decreased generally as the size of the depression increased which follows a power law fashion furthermore we calculated the wses and gradients along two selected rivers ganjiang and xiushui over our study area with smoothed water surface heights and a moving 1 km window the results suggested that the tg 2 inira also has the capacity of characterizing spatial patterns of river wses with overestimated differences of 0 43 m 0 12 m and 0 42 m when compared with the measurements from three hydrologic stations although larger biases between were revealed the results of the river gradient mapping were consistent with the practical situation and showed an overall slopes of 18 72 cm km and 12 43 cm km for the xiushui and ganjiang rivers respectively according to the influencing factors topographic and environmental conditions appear to be dominant controls on the variability of wses and hydraulic gradients within the study area due to the high complexity of the poyang lake wetland the wse values in smaller depressions and along the river s longitudinal profile are more sensitive to contamination by emergent vegetation riverbanks or mudflats and thus could be incorrectly interpreted as surface water signals and lead to larger wse biases furthermore other potential factors may also affect the precision of inira derived wse values for example wind and turbulence fluctuations over water bodies could influence radar backscattering with changed surface roughness thereby affecting the derived wse measurements in addition a correction is not available for the calculation results of wses due to insufficient ground based gps surveys while additional calibrations are essential to improving the accuracy of the derived wses we recommend that future swot research should devote more attention to calibrate wses with the integration of simultaneous ground measurements despite these issues as essential hydrological parameters wses and river gradients were successfully observed by space borne inira in this study first this study may bridge the gap between in situ measurements and the products of forthcoming swot mission which is helpful for improving instrument design data processing and algorithm effectiveness in future studies in addition it will undoubtedly contribute to a range of scientific applications concerning dynamic hydrologic connectivity channel discharge and sedimentation as well as surface water storage over gauge sparse regions of river lake systems such as in poyang lake declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was jointly supported by the national key research and development program of china 2018yfe0105900 the national natural science foundation of china 41871354 and the state key program of national natural science foundation of china 51839011 we would like to thank china manned space engineering for providing space science and application data products of tg 2 inira 
3082,as a significant hydrological parameter the water level of complex and shallow river lake wetland has important implications for a number of hydrological biological physical and chemical processes current in situ and satellite based methods both have disadvantages in capturing water levels over smaller surface water bodies with spatial heterogeneity or complexity interferometric imaging radar altimeter inira is a latest generation of ku band radar altimeter onboard tiangong 2 tg 2 the chinese space laboratory launched in 2016 similar to the design idea of nasa s swot it is expected to improve this situation through wide swath altimetry however the capacity of the tg 2 inira to measure water levels within small surface water bodies remains unexamined to explore these performances particularly their potential impact on hydrology research an assessment and comparison between in situ and tg 2 derived measurements was performed over the largest river lake wetland in china the results showed that the tg 2 inira with a broad range of coverage was able to effectively capture most of the surface water at its elevation for the focused shallow depressions a relatively good correlation was observed between the in situ and inira derived water surface elevations wses with an r2 of 0 97 rmse of 0 29 m and absolute difference ranging from 3 to 55 cm the accuracy of inira derived wses was highly controlled by the depression sizes and the absolute biases broadly increased in a power law fashion as the size of the depression decreased in addition surface water profiles along the ganjiang and xiushui river channels indicated that the tg 2 inira could also measure river wses with absolute differences below 0 43 m although differences were observed the results of the river gradient distribution were consistent with the practical situation and showed overall slopes of 18 72 cm km and 12 43 cm km for the xiushui and ganjiang rivers respectively such results demonstrated that tg 2 inira measurements can satisfactorily capture spatial patterns of the surface water gradient which is useful for accurately estimating river discharge this study will not only bridge the gap between in situ measurements and forthcoming swot products but also contribute to water resource management and hydrologic service assessments over data sparse regions of river lake systems within the study area keywords tangong 2 interferometric imaging radar altimeter small water bodies water level river gradient performance 1 introduction surface water refers to water retained in rivers streams lakes reservoirs and wetlands and accounts for less than 1 of the total amount of water on earth shiklomanov 1993 lehner and doll 2004 however it has a large impact on the hydrological and biogeochemical cycles strayer and dudgeon 2010 becker et al 2018 melo and getirana 2019 as a significant hydrological parameter the water level influences the transfer of water and energy between atmosphere and the land water surface vassiljev et al 1995 vincent 2009 quantifying the water level and its dynamics is highly relevant to a number of hydrological biological physical chemical processes and some industrial applications becker et al 2018 panagopoulos 2020 panagopoulos 2022 that are crucial for water resource management and other ecosystem services associated with surface water frappart et al 2012 wu and liu 2015 messager et al 2016 cael et al 2017 traditionally our knowledge of water level and its dynamics relies on in situ based measurements berg 1992 calmant et al 2008 but acquiring water level data across remote areas is particularly expensive and labor intensive smith 1997 alsdorf and lettenmaier 2003 over the past three decades the advent of earth observation technology has offered more opportunities to monitor water levels at global and regional scales several accepted satellite based methods have been developed to facilitate this purpose smith 1997 frappart et al 2006 schumann et al 2008 pan and nichols 2013 gao 2015 recently radar altimetry has emerged as a new and promising technique for directly measuring water levels from space bates et al 2013 tourian et al 2016 pham et al 2018 several altimeter satellites have also been launched cretaux et al 2015 birkett et al 2022 and widely used to derive water levels in large lakes and reservoirs troitskaya et al 2012 tong et al 2016 xu et al 2020a chen et al 2022 however traditional altimetry satellites only provide point based measurements with footprints from 3 to 5 km wide and spatial gaps greater than 100 km gao 2015 which are not ideal for capturing water levels over smaller water bodies smaller than 1 km2 or surface water with high spatial heterogeneity or complexity alsdorf et al 2007 calmant et al 2008 jiang et al 2020 thus quantifying water levels over these water bodies remains challenging papa and frappart 2021 cooley et al 2021 next generation three dimensionalimaging altimeter is expected to improve this situation through wide swath insar interferometric synthetic aperture radar prigent et al 2016 biancamaria et al 2016 currently to address this issue a proposed mission named swot surface water ocean topography based on the concept of wide swath imaging altimetry has been jointly developed by the united states and france which is expected to be launched in 2022 pavelsky et al 2014 biancamaria et al 2016 grippa et al 2019 similar to the design idea of swot through equipping a couple of near nadir interferometric imaging radar altimeters iniras china s tiangong 2 tg 2 space laboratory have acquired three dimensional imaging radar altimetry measurements globally from 2016 kong et al 2017 li et al 2018 zhang et al 2021 the tg 2 inira is the first spaceborne wide swath radar altimeter mainly aimed at generating spatially distributed measurements of water surface elevation which makes it possible to accurately measure much smaller water bodies than previously given this great potential it is imperative to determine the ability of the tg 2 inira to measure water levels in small water bodies which is currently unknown therefore the main objectives of this paper are to 1 quantify the water level from the tg 2 inira within small water bodies 2 assess the accuracy and influencing factors of the tg 2 inira derived water levels and 3 explore whether the accuracy of tg 2 observations is sufficient for capturing river gradients which are critical for characterizing the spatial hydraulics of regional river systems this paper is structured as follows section 2 introduces the study area and data used with an overview of the data collection and processing included in section 2 2 section 3 describes the methodology for estimating water levels from noisy tg 2 data and the validated analytical methods for quantifying water level section 4 presents the results section 5 discusses the conclusions 2 study area and data 2 1 study area the study area was chosen in the poyang lake wetland of china 28 22 29 45 n 115 47 116 45 e an internationally protected wetland that provides habitat for migratory birds and it is located in the middle reaches of the yangtze river the area receives seasonal water flows from five major tributary rivers namely xiushui ganjiang fuhe xinjiang and raohe and exchanges water with the yangtze river via the hukou fig 1 a which greatly contributes to the considerable variability of inundation extent and water level throughout a year wu and liu 2015 in the wet season april september the water levels reach a peak along with the inundation area exceeding 3000 km2 shankman et al 2006 in the dry season october march however the inundation area shrinks to less than 1000 km2 feng et al 2012 during this period the area is characterized by a wide floodplain hundreds of independent saucer shaped shallow depressions as small as 1 km2 and distributary channels with varying water levels and hydrologic connectivity from south to north wu and liu 2017 fig 1b which largely determine the habitat conditions for thousands of migratory birds barzen et al 2009 li et al 2019 given the complexity of the lake floodplain system and the importance of wetland conservation hydrological characteristics of the poyang lake have attracted wide attention from many scholars in recent years chen et al 2018 wang et al 2019 xu et al 2020b liu et al 2020 most previous studies primarily focused on water bodies of entire lake and none have comprehensively addressed the water level changes in small shallow depressions due to the limited measurements to assess the performance of the tg 2 wide swath imaging altimeter observations such a highly dynamic water regime and numerous independent smaller depressions make the poyang lake wetland an ideal test area in this study we chose to examine only the southwestern region of the poyang wetland that presents most saucer shaped shallow depressions within the study area fig 1b specifically eight typical depressions were selected as study sites from south to north these sites were chosen due to their varied size ranging from 2 to 80 km2 distinct morphometric characteristics and availability of in situ data for validation a list of the selected depressions and their geographic features are shown in table 1 fig 1d also displays a map that is magnified on the focused depressions in this subfigure some of the in situ sites used for validation are annotated with their ids additionally considering the accessibility of the data ganjiang river and xiushui river were also focused on for the purpose of assessing tg 2 s performance in characterizing the water surface slope fig 1d 2 2 data collection and preprocessing this study uses satellite imagery collected by tg 2 and sentinel 2 satellite platform in situ field measurements and other ancillary data 2 2 1 tg 2 inira data inira is a latest generation of ku band radar altimeter onboard tg 2 which launched on september 15 2016 and runs in a non sun synchronous orbit at an altitude of approximately 400 km it is dedicated to enabling three dimensional imaging measurement of the surface water height by integrating interferometry synthetic aperture and traditional altimeter height tracking techniques and it has a wide swath of 40 km li et al 2018 kang et al 2019 unlike traditional radar altimeters tg 2 inira adopts a short baseline 2 3 m and a near nadir incidence of 2 5 7 5 to acquire a high coherence interferometric phase from surface water which makes it possible to measure water surface height with wide swath and inland water bodies lakes rivers wetlands etc thus yielding a spatial resolution of 40 m kong et al 2017 zhang et al 2018 tg 2 inira data were produced by the technology and engineering center for space utilization chinese academy of sciences each inira dataset consists of four standard products in the form of an image radiometric correction product as a complex valued image level 1 a geometric correction product as a backscatter image level 2 an orthophoto correction product level 3 and a digital elevation model product with a universal transverse mercator utm projection level 4 kang et al 2019 in this study four level 4 products of the tg 2 inira as georeferenced raster images of elevation in meters above the world geodetic system 1984 wgs 84 ellipsoid were collected on 16 september 2017 over the focused shallow depressions the covered area was 30 km east to west and 110 km north to south as presented in fig 1b these data were acquired from the space application data promoting service platform for china manned space engineering https www msadc cn 2 2 2 in situ data for this study we evaluated the tg 2 inira s performance mainly through in situ measurements of elevation and position first eight sets of solinst 3001 self logging pressure transducers pts solinst ontario canada were fixed in the selected shallow depressions to record accurate measurements of surface water elevation fig 1c the reported accuracy for the transducers at a maximum depth of 10 m is 1 cm wu and liu 2017 water level measurements were continuously logged every 30 min from february 8 2015 and later aggregated to daily means additionally daily average water level records based on local wusong height datum from three gauging stations i e yongxiu wucheng and changyi stations were also collected for assessing the tg 2 inira retrieved river gradients fig 1d these records were acquired from the hydrological bureau of jiangxi province during the study period in fig 1d the location of each transducer is shown as a green dot while the location of the in situ gauging station is shown as a black circle with a cross inside in order to effectively evaluate the performance of the tg 2 inira measurements different types of water level measurements were transformed to the same vertical reference of the wusong height datum for this purpose several ground checkpoints along the focused depressions and water bodies of the main lake were surveyed using a huace a220gr real time kinematic rtk gps receiver huace ltd shanghai china on november 5 2015 fig 1c we used rtk gps surveys to convert the in situ measurements to absolute water levels by measuring the height difference between the water surface and the gps surveyed benchmarks the rtk system provides precise ground position and elevation with an accuracy as high as 1 cm bisnath et al 2004 tang et al 2010 and it can potentially be used to calibrate in situ measurements of water level to the same referenced datum of gauging stations 2 2 3 ancillary data two ancillary datasets namely sentinel 2 imagery and digitized topographic maps were also used in this study the selected sentinel 2 images bands 2 3 4 and 8 with 10 m spatial resolution were acquired on 18 september 2017 from the european space agency esa sentinel science hub https scihub copernicus eu these images are closest to the tg 2 inira observations with relatively less cloud coverage thus they were applied to delineate water masks the topographic digital map of the poyang lake wetland at a scale of 1 100000 hydrological bureau of jiangxi province china 2011 was also used in combination with sentinel 2 images to identify the depressions and streams all of the acquired imagery were projected onto the universal transverse mercator utm with the wgs 84 zone 50 n coordinate system 3 methodology 3 1 water mask derivation in this study we focus on the performance of the tg 2 inira in estimating the surface water level within complex river lake system to fulfill this goal we first generate a water mask from simultaneously acquired sentinel 2 images to isolate only open water pixels in the tg 2 inira data using a normalized difference water index ndwi which was calculated as follows mcfeeters 1996 1 ndwi d n g r e e n d n nir d n g r e e n d n nir where dngreen is the green band b3 and dnnir is the near infrared band b8 in the sentinel 2 data it has been proved to be one of the most effective index in detecting water surfaces jain et al 2005 liu and wu 2015 to enhance the detection accuracy a localized threshold segmentation algorithm was then used for each focused area otsu 1979 liu et al 2012 and the ndwi pixels were classified into water values greater than the threshold and non water values less than the threshold fig 2 3 2 tg 2 inira data filtering according to the derived water mask the tg 2 surface water elevation over the study sites can be extracted by multiplying the tg 2 inira images with the water mask to assess the ability of tg 2 to measure surface water elevations within complex and shallow wetlands several filtering and spatial averaging techniques are performed to refine the tg 2 inira images before comparing them with the in situ measurements first we employ a 2 km2 moving window to filter out extreme outliers 3 standard deviations and remove them from the mean tg 2 inira measurements which helps remove contaminated pixels of layover and improper height estimation from adjacent vegetation mudflat or misclassified water land pixels goldstein et al 1988 biancamaria et al 2016 second to quantify the water level differences between the tg 2 inira data and in situ measurements for each study site we also calculate the mean tg 2 elevation by spatially averaging within a square window of 10 10 pixels from the filtered tg 2 data which is commonly applied to reduce independent random errors rodriguez and martin 1992 altenau et al 2019 3 3 validation and uncertainty analysis once the averaged measurements were determined the performance of tg 2 derived wses was validated through comparing with simultaneously acquired in situ observations and the coefficient of determination r2 was then performed to assess the consistency between in situ measurements and tg 2 retrieved wses furthermore several statistical indicators including the root mean square error rmse absolute bias ab and mean absolute bias mab were calculated described as follows around each in situ site to quantify the performance of the tg 2 measurements 2 rmse i 1 n t i m i 2 n 3 ab t i m i 4 mab i 1 n t i m i n where t represents the tg 2 inira measurement m is the in situ measurement n is the in situ site number and i is the sequential order among these indicators the absolute bias was used to evaluate the magnitude of difference between inira derived wses and in situ measurements rmse describes the absolute accuracy of the inira derived water level accounting for all the effects of random and systematic errors 4 results 4 1 determination of tg 2 wses fig 3 shows the processing results of tg 2 wses over the study area as seen in fig 3c several contaminated pixels within shallow depressions are clearly shown which indicate improper estimations of wse likely resulting from a combination of wetland vegetation and more complex adjacent topography in fig 3d the areas affected by wse noise and outliers were successfully removed by standard deviation threshold filtering and spatially average processing the effect of the processing on the histogram of the data can be seen in fig 4 which illustrates the histogram distributions of inira wse pixels for the focused shallow depressions before blue and after orange filtering the histogram distributions of the original image present a relatively smooth variation of wse values after processing the histogram of the data showed a large improvement the image processing reallocates selected wse values to move them toward the center the frequency was also remarkably increased and a higher peak appeared in the processed histograms which indicates that the processing is effective to avoid large wse biases in the final results 4 2 tg 2 wse validation considering that the wgs 84 height datum inira derived measurements and the wusong height datum in situ measurements was about 0 4 m lower and 1 65 2 26 m higher than the china 1985 national height datum in the study area respectively li et al 2015 lin et al 2016 therefore the datum biases of all measurements were quantified and transformed into the wusong datum the validation of inira based wses with coincident pt measurements within the focused depressions is illustrated in fig 5 as a scatter plot and their differences are summarized in table 2 inira based wses for each shallow depression were calculated as the mean value within the square window of 10 10 pixels overall there is a relatively good correlation between the pt and inira based wses with r2 of 0 97 and rmse of 29 cm larger negative biases were shown across the focused depressions which indicates that the inira generally underestimated the wse compared to in situ measurements the absolute difference between the inira and pt based wse ranges from 3 to 55 cm with a mab of 24 cm the analysis results indicate that although the tg 2 inira has relatively comparable performance it cannot guarantee incredibly high accuracy for wse estimations fig 6 represents the distribution of the absolute biases between the inira and pt based wse for all 8 shallow depressions as illustrated in fig 6 the magnitude of absolute biases ranges widely depending on the size of the depressions absolute bias is generally higher than that of depressions with larger mask areas and the bias broadly increases in a power law fashion as the size of the depression decreases specifically the absolute bias of wse increased from 3 3 cm for the largest mask area l06 21 11 km2 to 55 7 cm for depression l03 which had a much smaller area of 2 2 km2 two of the other six depressions smaller than 2 2 km2 also had higher absolute bias values of more than 17 cm one possible explanation for the correlation between the absolute bias and mask area is that the smaller depressions tend to be shallow and probably have more emergent vegetation or mudflats these geophysical factors may lead to greater inira elevation biases and make it more difficult to filter out contaminated areas in addition other factors such as differential incidence angles interferometric phase drift and the inira wse retrieval algorithm may also determine this situation which could explain the lack of a constant correlation between the absolute bias of wses and the size of the depressions 4 3 river gradient mapping and assessment knowledge of a river s gradient is fundamental for the hydrologic and geomorphologic characterization of river systems jung et al 2010 gonzález del tánago et al 2016 to further investigate tg 2 s performance in characterizing the water surface slope two channel longitudinal profiles of wse across the centerline of the ganjiang river and xiushui river were generated and converted into point features to illustrate clearly spatial patterns of surface water gradient the average centerline point heights were implemented with a 1 km gaussian filter and then plotted versus distance surface water gradients were also calculated by using a moving 1 km window with an advance step of 400 m 100 points fig 7 illustrates the longitudinal wse profiles and corresponding surface water gradients along the xiushui and ganjiang rivers there were dominant hydraulic wses from south to north along the longitudinal profiles with values ranging from 9 59 to 14 94 m for the ganjiang river and 8 61 16 97 m for the xiushui river which were consistent with the flow directions of these two rivers the inira derived wses also showed good performance when compared with in situ water level measurements we noted that inira data generally overestimated the wse measurements and the absolute differences were 0 43 m 0 12 m and 0 42 m at yongxiu station changyi station and wucheng station respectively a possible explanation is that wse values along river longitudinal profiles are more sensitive to contamination by riverbanks which could be incorrectly interpreted as surface water signals additionally a map of the 1 km reach length mean gradient of the ganjiang and xiushui river is shown in fig 8 it reveals heterogeneous water surface gradients along the longitudinal profiles with a wide range from 0 to 110 cm km the phenomenon is likely affected by the meander morphology and differentiated topography along the rivers in general river gradients tended to be higher near upstream and lower gradients downstream fig 8 specifically the surface water gradient of the xiushui river was relatively steeper with an average slope of 21 57 cm km for the 1 km reach length and an overall slope of 18 72 cm km for the full longitudinal profile fig 7b in contrast the ganjiang river which isrelativelystraight and has many branches with a higher connectivity degree to other rivers or lakes had an average slope of 20 67 cm km for the 1 km reach length and an overall slope of 12 43 cm km for the full longitudinal profile fig 7c although the in situ measurements were not sufficient to validate these fine resolution river gradient estimates the gradient distribution results are in accordance with the practical situation shankman et al 2006 5 conclusions and discussion in conclusion characterizing fluctuations in water surface elevation over many of the world s small water bodies through current in situ or altimeters also remains challenging ku band wide swath inira onboard chinese tg 2 space laboratory is a new generation of radar altimeter which similar to the design concept of nasa s swot and makes it possible to improve this situation but no public work has been reported on its performance to our knowledge this is the first study to investigate the capability of the tg 2 inira observations for water level monitoring over spatially complex river lake wetland regions the poyang lake wetland which is the china s largest freshwater lake and presents complex river lake floodplain system was selected as the study area to evaluate its capabilities we compared tg 2 inira wses and gradients to in situ measurements within the selected shallow depressions and rivers it has been demonstrated that the tg 2 inira with a broad range of coverage can effectively capture the wses for most surface water bodies for focused eight shallow depressions strong correlations were observed between the wses derived from inira and in situ measurements with coefficient of determination r2 and rmse of 0 97 and 0 29 m we found that the absolute bias ranged from 3 to 55 cm with a mab of 24 cm the precision of inira derived wses decreased generally as the size of the depression increased which follows a power law fashion furthermore we calculated the wses and gradients along two selected rivers ganjiang and xiushui over our study area with smoothed water surface heights and a moving 1 km window the results suggested that the tg 2 inira also has the capacity of characterizing spatial patterns of river wses with overestimated differences of 0 43 m 0 12 m and 0 42 m when compared with the measurements from three hydrologic stations although larger biases between were revealed the results of the river gradient mapping were consistent with the practical situation and showed an overall slopes of 18 72 cm km and 12 43 cm km for the xiushui and ganjiang rivers respectively according to the influencing factors topographic and environmental conditions appear to be dominant controls on the variability of wses and hydraulic gradients within the study area due to the high complexity of the poyang lake wetland the wse values in smaller depressions and along the river s longitudinal profile are more sensitive to contamination by emergent vegetation riverbanks or mudflats and thus could be incorrectly interpreted as surface water signals and lead to larger wse biases furthermore other potential factors may also affect the precision of inira derived wse values for example wind and turbulence fluctuations over water bodies could influence radar backscattering with changed surface roughness thereby affecting the derived wse measurements in addition a correction is not available for the calculation results of wses due to insufficient ground based gps surveys while additional calibrations are essential to improving the accuracy of the derived wses we recommend that future swot research should devote more attention to calibrate wses with the integration of simultaneous ground measurements despite these issues as essential hydrological parameters wses and river gradients were successfully observed by space borne inira in this study first this study may bridge the gap between in situ measurements and the products of forthcoming swot mission which is helpful for improving instrument design data processing and algorithm effectiveness in future studies in addition it will undoubtedly contribute to a range of scientific applications concerning dynamic hydrologic connectivity channel discharge and sedimentation as well as surface water storage over gauge sparse regions of river lake systems such as in poyang lake declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was jointly supported by the national key research and development program of china 2018yfe0105900 the national natural science foundation of china 41871354 and the state key program of national natural science foundation of china 51839011 we would like to thank china manned space engineering for providing space science and application data products of tg 2 inira 
3083,the nonstationary flood frequency analysis ns ffa is conducted when the assumption of stationarity in hydrologic extremes is violated the commonly used approach is the performance based ns ffa which jointly determines the distribution and the nonstationary structure based upon the model performance however this approach is challenged from both theoretical and practical perspectives an alternative is the herein named decomposition based ns ffa which determines the two ns ffa model components separately and explicitly uses the available knowledge of the nonstationarity however it has been barely implemented in practice this paper proposed a novel decomposition procedure that strictly follows the theoretical decomposition of nonstationary stochastic processes to advance the decomposition based ns ffa the proposed decomposition procedure was compared with a previously reported method in both an analytical deduction and a simulation study the proposed decomposition based ns ffa was further compared to the performance based ns ffa using both synthetic and real datasets from north america which exhibit different patterns of nonstationarity the particle filter was adopted for uncertainty quantification and parameter estimation the results revealed that the proposed decomposition based approach was advantageous in preserving the moments of underlying stochastic component particularly the higher order moment i e skewness in addition the comparison of the two ns ffa approaches demonstrated the superiority of the proposed decomposition based approach in capturing the underlying known nonstationary stochastic process and being competitive with the performance based approach from the performance perspective in real applications the results from the simulation study and the real application also revealed several caveats of the performance based approach including the potential overfitting and equifinality problems as well as the selection of distinct models when adopting different performance metrics in addition differing from the performance based approach the decomposition based ns ffa avoided alleviated the ergodicity violation all these results demonstrated the advancements of the proposed decomposition based ns ffa and advocated its application in the ns ffa keywords nonstationarity nonstationary structure distribution extreme events uncertainty fitting efficiency 1 introduction assessing the recurrence of floods which is a key task in hydrology and water resources management is commonly conducted by flood frequency analysis ffa the conventional ffa predicts the flood quantiles corresponding to specific return periods and vice versa based on the core assumption that the underlying process is stationary and thus it is often referred to as the stationary ffa s ffa nonetheless the reliability of the s ffa is compromised when confronted with nonstationary processes as a result of climate change and changes in land use cover and or water management aghakouchak et al 2020 blöschl et al 2019 milly et al 2008 milly et al 2015 under this circumstance the nonstationary ffa ns ffa in which the probability distribution of the floods varies as a function of a selected covariate s has emerged as the theoretical solution to deal with nonstationary scenarios françois et al 2019 lópez and francés 2013 salas et al 2018 slater et al 2021 in this context the determination of a ns ffa model which is formed by coupling the probability distribution of floods and the nonstationary structure that governs its evolution over time is a key to implementing the ns ffa the conventional approach for determining a suitable ns ffa model is through selection by ranking a set of candidate models according to the pre selected performance metric e g information theoretic measures such as the akaike and bayesian information criteria aic and bic e g kim et al 2017 li et al 2019 ragno et al 2019 this approach is referred to as the performance based approach in this paper since the model is selected by jointly assessing the nonstationary structure and the distribution in this approach the determination of one model component is conditioned on its performance in conjunction with the other e g agilan and umamahesh 2017 mondal and mujumdar 2015 strupczewski et al 2001 besides this approach does not explicitly use the available knowledge of the nonstationarity beyond the time series such knowledge can be detailed physical derived from the causative process es behind the nonstationarity e g through physically based climate and hydrological models or statistical deducted knowledge derived from underlying datasets and deductive reasoning in addition the implementation of the performance based ns ffa has been challenged from the perspectives of both theoretical fundament and practical application firstly under nonstationarity a time series is not a set of random independent realizations of the same distribution and consequently differing from a stationary time series it does not visit most of its domain when the sample size increases von storch and zwiers 2002 as a result the ergodicity does not hold under nonstationarity and in turn the statistical properties of a nonstationary process cannot be reliably inferred from a single time series only koutsoyiannis and montanari 2015 serinaldi et al 2018 therefore the model determination is questionable in this approach secondly analogously to the black box modeling the ns ffa model is selected for achieving the best performance while bypassing the available knowledge of the system as a result the optimal model may yield a spuriously high performance due to increased fitting flexibility irrespective of its correspondence to the actual nonstationary stochastic process luke et al 2017 serinaldi and kilsby 2015 thirdly the use of different performance metrics such as for evaluating the uncertainty e g the average width aw or coverage width index cwi which has been commonly relegated to a secondary aspect and the fitting efficiency e g the aic or bic often leads to selecting different optimal models ouarda et al 2019 serinaldi and kilsby 2015 vidrio sahagún et al 2021 lastly in this ns ffa approach there might be two or more candidate models with almost identical performance evaluated according to a pre selected evaluation metric consequently there would be uncertainty in the model selection this issue is similar to the equifinality problem in the hydrological modeling beven 2006 beven and freer 2001 khatami et al 2019 namely different model structures and or parameter sets may be equally capable of reproducing the observations according to a or a few performance metric s however such equifinal models may not necessarily yield the same results in view of the above inferring the ns ffa model based on a performance metric is not necessarily reliable serinaldi et al 2018 on the other hand nonstationary stochastic processes are commonly considered as the superposition of a stationary stochastic component and a deterministic time dependent component koutsoyiannis and montanari 2015 milly et al 2015 serinaldi et al 2018 this notion suggests the potential of selecting the two components of the ns ffa model separately namely the nonstationary structure can be determined from the available knowledge on the deterministic time dependent component while the distribution can be inferred using the stationary component in which the ergodicity holds in this way the ergodicity violation for statistical inference faced in the performance based approach can be avoided when physical or statistical deducted knowledge complements the information from the time series despite the suggestion of incorporating physical knowledge into the ns ffa gilroy and mccuen 2012 its use is still constrained by its limitation to capture the nonstationarity of hydrological extremes giuntoli et al 2021 kundzewicz et al 2017 maraun et al 2010 maraun et al 2017 whereas the use of knowledge derived through deductive reasoning is considered feasible in the current practice and thus benefits the ns ffa model determination serinaldi et al 2018 it is worth highlighting that the identification of the deterministic component behind the observed changes is indispensable to justify the ns ffa irrespective of the approach adopted koutsoyiannis and montanari 2015 montanari and koutsoyiannis 2014 serinaldi et al 2018 serinaldi and kilsby 2015 changes produced by purely stochastic drivers would result in doubly stochastic rather than nonstationary processes which are preferably modeled with compound or mixed distributions serinaldi et al 2018 serinaldi and kilsby 2015 the lack of appropriate identification of the deterministic component can lead to unrealistic estimates and subsequently jeopardizes the reliable implementation of ns ffa models serinaldi and kilsby 2015 although the ns ffa alternative approach based on the notion of the nonstationary process decomposition called the decomposition based approach throughout this paper is not new it has not been well explored and developed yet to present only a few studies have applied this notion in the ffa for instance cunderlik and burn 2003 selected the distribution from detrended at site series assuming second order nonstationarity in the regional index flood ns ffa they employed the absolute deviations from the mean of the transformed series detrended in the mean to remove the trend in the second statistical moment zhang et al 2019 identified the overall best fit distribution across canada following the same detrending procedure as cunderlik and burn 2003 when nonstationarity presents sung et al 2018 also applied this procedure to select the distribution although considering nonstationarity in the mean only however such a detrending procedure does not guarantee the preservation of higher order moments e g skewness of the stochastic component and consequently might result in poor estimates e g kalai et al 2020 on the other hand just a few studies have specified the nonstationary structure according to the exploratory explanatory data analysis rather than according to the performance of the models namely the identified temporally variant statistical moments of the underlying dataset have been used to determine the nonstationary structure e g cheng et al 2014 cunderlik and burn 2003 ragno et al 2019 despite these attempts to determine the distribution and nonstationary structure according to the trend free datasets and the nonstationarity knowledge the procedure to remove the deterministic component while the higher order moments are preserved is still desired this improvement would advance the decomposition based approach and thus promote its practical application moreover the decomposition based approach has not been assessed validated or compared with the performance based approach in the literature so far in view of the above this paper proposes a novel general procedure that strictly follows the theoretical decomposition of nonstationary stochastic processes for advancing the model determination in the decomposition based ns ffa the proposed procedure has the capacity to retain the statistical characteristics of the underlying stationary stochastic component after removing the deterministic component this paper demonstrates such an advancement over the existing procedure of cunderlik and burn 2003 by both theoretical deduction and a simulation study in terms of preserving the statistical moments of the a priori known stationary stochastic component in addition the performance namely the fitting efficiency and uncertainty of the proposed decomposition based approach is further assessed and compared with the performance based approach which has been often adopted in the literature using both synthetic and real datasets these demonstrate both the advantages of the proposed decomposition based ns ffa and the caveats of the performance based approach in real applications thirteen annual maximum series ams of flow from canada and the usa which are perceived to have different patterns of nonstationarity are used herein as study cases 2 materials and methods apart from the proposed decomposition based approach this section also introduces the performance based approach the distributions considered the method used for parameter estimation and uncertainty quantification the generation of the synthetic datasets for the simulation study and the real datasets for the practical applications note that throughout this section the random variables and their realizations are denoted by uppercase and lowercase letters respectively while the vectors are denoted by boldface letters in practice the temporal covariate has been commonly used in the ns ffa as a surrogate for any time dependent physical driver s especially for fitting purposes e g obeysekera and salas 2016 sun et al 2018 and or bypassing the exploration of the physical process es both of which are case sensitive this is particularly convenient for methodology oriented rather than case specific research objectives e g prosdocimi and kjeldsen 2021 serago and vogel 2018 moreover the often low to moderate dependency of the nonstationarity on the physical driver s archfield et al 2016 burn and whitfield 2017 ray and goel 2019 has hindered the use of the physical covariate s thus the temporal covariate i e the time t was adopted to illustrate the application of both the performance based and the decomposition based approaches in this paper 2 1 candidate distributions in this paper three probability distributions were considered in both the performance based and the decomposition based approaches the candidate distributions are the generalized extreme value gev the pearson type iii pe3 and the log pearson type iii lpe3 distributions these three parameter distributions have been commonly adopted in the ffa in canada and the usa england et al 2019 gado and van nguyen 2016b stedinger et al 1993 zhang et al 2020 the cumulative gev function of the random variable y f y and the quantile y p e corresponding to an exceedance probability p e p e 1 f y y θ are given by 1 f y y θ exp 1 κ y ξ α 1 κ κ 0 exp exp y ξ α κ 0 2 y p e p e θ f y 1 p e θ ξ α κ 1 ln 1 p e k κ 0 ξ α ln ln 1 p e κ 0 where the parameter vector θ is composed of the shape scale and location parameters denoted by κ α and ξ respectively the cumulative pe3 distribution function is given by hosking and wallis 1997 3 f y y θ g α y ξ β γ α γ 0 φ y μ σ γ 0 1 g α ξ y β γ α γ 0 where the shape scale and location distribution parameters are α 4 γ 2 β 1 2 σ γ and ξ μ 2 σ γ respectively if γ 0 μ σ and γ are the mean standard deviation and skewness of y respectively γ is the gamma function g is the incomplete gamma function and φ is the normal cumulative distribution function there is no explicit analytical form of y p e for the pe3 distribution it has often been approximated using the frequency factor k p through the wilson hilferty transformation for γ 2 and 0 01 p 0 99 by stedinger et al 1993 4 y p e μ σ k p γ k p γ 2 γ 1 γ 6 2 γ z p 6 3 1 where k p γ is the pth quantile 1 p e of the pe3 distribution with zero mean unit standard deviation and skewness γ and z p is the pth quantile of the standard normal distribution the lpe3 distribution corresponds to the case in which the log transformed random variable y l o g 10 y follows the pe3 distribution thus eqs 3 and 4 of the pe3 distribution are applicable while μ σ and γ are the mean standard deviation and skewness of y respectively in the common practice of the ns ffa the nonstationarity has been depicted using time dependent distribution parameters thus in this paper a ns ffa model consists of an aforementioned distribution whose parameters are functions of the time 2 2 parameter estimation and uncertainty quantification the parameter estimation and uncertainty quantification were conducted employing the particle filter pf technique the pf is a method that integrates the recursive bayesian filters and the bootstrap resampling technique and is free from assumptions and reliable in uncertainty estimation doucet et al 2001 gordon et al 1993 särkkä 2013 the pf method has been previously adopted in both the s ffa and ns ffa sen et al 2020 vidrio sahagún et al 2021 in this paper the pf method was implemented following the methodology of vidrio sahagún et al 2021 in a nutshell the pf estimates the hidden state vector of a dynamic system w 0 i w 0 w 1 w i which is difficult if not impossible to be measured in the field from the related observations y 1 i y 1 y 2 y i here w r n w and y r n y denote the n w and n y dimensional hidden state and observation vectors respectively the general idea behind the pf as an inverse approach is to assess the error of the model predictions with respect to the observations and inversely map the model predictions to the parameter space in the context of the ffa the general structure of the pf is formulated by taking the set of model parameters θ and the annual maximum flows y as the hidden state variables and the associated measurements respectively as the parameter estimation is conducted for a given time period in the ns ffa the hidden states θ j θ 1 θ 2 θ n θ evolve in pseudo time replacing time with iterations in order to incorporate all available measurements unitized as a batch at each step j i e y j y t 1 y t 2 y t n j here n θ and n are the number of ns ffa model parameters and sample size of the ams respectively this can be expressed in the bayesian framework as the search for the joint posterior distribution of all the model parameters given the related observations the main steps of the pf include 1 the pf starts at j 1 with a set of m n n θ independent particles ψ j ψ j 1 ψ j 2 ψ j m in the parameter domain which is initially sampled from uninformative marginal prior distributions the initial priors were specified as uniform distributions with ranges large enough such that encompassing all realistic values and none of the posteriors were truncated each particle has an identical weight w ψ j i at j 1 2 then the w ψ j are updated over pseudo time steps i e from ψ j to ψ j 1 based on the particles likelihood and act as discrete posterior distributions the particles likelihoods are obtained from the error between the model outputs i e the estimated quantiles computed using the ψ j and the mapping function f here the quantile function and the observations i e the empirical quantiles derived using the measurements y j and their associated empirical exceedance probability 3 after each pseudo time step resampling is conducted and white noise perturbation is applied to ψ j 1 to avoid particle degeneracy and maintain adequate diversity respectively 4 steps 2 and 3 are repeated until the ψ j converges to a stable estimation with the progress of the pf over pseudo time steps the parameter estimation point estimates can then be carried out employing the stabilized ψ j the uncertainty metrics are calculated using several additional pseudo time steps until they achieve stability as well in the pf several hyperparameters need to be pre determined for instance the number of particles m is selected in a trade off between the precision of the estimates converge to the exact solution when m and the computational demand in this paper the m was set to 5 000 while the number of pseudo time steps to yield the point estimates was 1 000 and the number of additional pseudo time steps to stabilize the uncertainty approximations was 100 please refer to sen et al 2020 and vidrio sahagún et al 2021 for more details on the pf implementation and the setup of the hyperparameters in the s ffa and ns ffa 2 3 performance based ns ffa 2 3 1 candidate nonstationary structures following the common practice of the performance based approach several candidate nonstationary structures in which the distribution parameters except the shape parameter are either linear or nonlinear functions of the time were considered and are shown in table 1 the shape parameter was treated as constant as its estimation is difficult due to sample size limitations and particularly unrealistic when it is allowed to vary coles 2001 katz 2013 more complicated model structures can be taken into consideration but were not included as their parametrization could be highly uncertain serago and vogel 2018 serinaldi and kilsby 2015 the optimal ns ffa model was selected from the listed candidate models according to their performance 2 3 2 performance evaluation metrics the performance was assessed in terms of fitting efficiency and uncertainty the former has been commonly evaluated using the aic which deals with the trade off between the goodness of fit offered by a model and its complexity on the basis of information loss the latter was evaluated using the cwi kasiviswanathan et al 2019 the cwi simultaneously assesses the bandwidth and the observations coverage of the uncertainty band which conflict with each other the aic and cwi were calculated using the derived point estimates of the model parameters and the estimated quantiles by the full set of particles of the pf respectively by 5 aic n log r m s e 2 n θ 6 cwi a w exp 1 α cwi poc 100 2 where α cwi is the significance level 0 05 here and the root mean square error rmse the aw and percentage of coverage poc of the uncertainty bands are calculated by 7 rmse 1 n k 1 n m k o k 2 8 aw 1 n k 1 n m k u m k l 9 poc 1 n k 1 n c k w h e r e c k 1 k s t m k l o k m k u 0 e l s e w h e r e where o k are the empirical quantiles according to the plotting position formula p 1 n r 0 35 n where r is the rank of the k th observation m k are the corresponding modeled quantiles and m k u and m k l are the upper and lower uncertainty bounds of the estimates of the k th observation respectively the empirical plotting position formula was selected due to its better performance for estimating the empirical quantiles compared to other formulas for extreme value distributions hosking 1990 a model yielding a smaller aic is more efficient in fitting while a model with a lower cwi is less uncertain 2 4 decomposition based ns ffa 2 4 1 proposed decomposition procedure the decomposition based ns ffa relies on the widely adopted statistical representation of nonstationary processes y t in the hydrometeorological literature e g koutsoyiannis and montanari 2015 milly et al 2015 serinaldi et al 2018 namely 10 g y t g x t z t where x t and z t are the stationary stochastic and the time dependent deterministic components respectively and g is a generic operation such as e or var the stochastic processes y t and x t are the families of random variables that describe the evolution of the underlying time dependent and time independent physical processes over the period of interest i e for t t 1 t 2 t n respectively in this general decomposition z t governs the changes of g y t over time in the decomposition based approach x t is estimated by removing z t from y t a novel procedure that strictly follows the theoretical decomposition of nonstationary stochastic processes was proposed herein to remove z t in the first two statistical moments according to the definition of weak second order stationarity lindgren et al 2013 von storch and zwiers 2002 differing from the previous methodology of cunderlik and burn 2003 the proposed procedure mathematically guarantees the removal of the deterministic component in the first two moments without altering higher order moments and thus preserves the stationary stochastic component more details on the procedure of the benchmark method and its issues in preserving the stochastic component and its statistical moments according to the theoretical deduction are provided in appendix a in this paper μ x t σ x t and γ x t denote the unconditional stationary mean standard deviation and skewness of x t while μ y t σ y t and γ y t are the conditional nonstationary mean standard deviation and skewness of y t the conditional statistics refer to the time dependent statistical properties of the underlying nonstationary stochastic process based upon the second order stationarity three general classes of nonstationary scenarios were considered herein namely class 1 c1 presence of z t in the μ y t only class 2 c2 presence of z t in the σ y t only and class 3 c3 presence of z t in both the μ y t and σ y t for datasets in c1 i e whose process decomposition consists of e y t e x t z t the stationary dataset x t x t 1 x t 2 x t n was obtained following an additive scheme by simply subtracting z t in the form of f t from the ams y t y t 1 y t 2 y t n 11 x t y t f t when f t 0 t it is the special case of the absence of z t in the mean and thus y t is stationary equation 11 ensures that only the mean of the dataset is perturbed after z t is removed while both the conditional standard deviation and skewness are preserved 12 μ x t e x t e y t z t e y t z t μ y t f t 13 σ x t 2 e x t μ x t 2 e y t z t μ y t z t 2 e y t μ y t 2 σ y t 2 14 γ x t e x t μ x t 3 e x t μ x t 2 3 2 e y t z t μ y t z t 3 e y t z t μ y t z t 2 3 2 e y t μ y t 3 e y t μ y t 2 3 2 γ y t for datasets in c2 i e whose process decomposition consists of v a r y t 1 2 v a r x t 1 2 z t the x t was obtained following a multiplicative scheme by removing z t in the form of g t 15 x t μ y t y t μ y t h t w h e r e h t σ x t σ x t g t where μ y t is the estimated mean of y t which is time independent for datasets in c2 h t is a normalized multiplicative term depicting z t g t is the estimated z t in the σ y t and σ x t is the estimated standard deviation of x t using the beginning of the observation period as the reference when g t 0 t and thus h t 1 it is the special case of the absence of z t in the standard deviation and thus y t is stationary similar to illustrated in eqs 12 14 eq 15 also ensures that only the standard deviation is perturbed as σ x t 2 h t 2 σ y t 2 after removing z t from the dataset while both the conditional mean and skewness are preserved i e μ x t μ y t and γ x t γ y t refer to appendix b note that since z t is deterministic and or independent its covariances with x t and y t are zero and thus are not included in the formulation for datasets in c3 i e whose process decomposition consists of e y t e x t z 1 t v a r y t 1 2 v a r x t 1 2 z 2 t the x was obtained by combining eqs 11 and 15 16 x μ y t f t y t f t μ y t f t h t w h e r e h t σ x t σ x t g t where μ y t f t is the estimated mean of the partially decomposed dataset after z 1 t f t is removed from μ y t but z 2 t g t is still present in σ y t it can be also demonstrated that eq 16 ensures that only the mean and standard deviation are perturbed as μ x t μ y t f t and σ x t 2 h t 2 σ y t 2 after removing z t from both the mean and standard deviation while the conditional skewness is preserved i e γ x t γ y t refer to appendix b the above eqs 10 16 are applicable for any form of z t in the mean f t and or the standard deviation g t in the proposed decomposition procedure the deterministic component z t is a function of time that captures the available knowledge of the nonstationarity the z t is often expressed in a parsimonious mathematical form determined by exploratory statistical analysis hence in this paper the z t was determined as a linear function of time and parameterized using the non parametric sen s slope estimator for estimating the trend in the μ y t and the sen s slope estimator coupled with the moving window technique for estimating the trend in the σ y t the hyperparameters of the moving window method namely the window length and step were set to 10 and 5 years as they were found optimal in terms of bias among the ranges of window length from 10 to 50 years and window step from 1 year to the window length for the synthetic nonstationary gev and pe3 datasets appendix c note that in the proposed decomposition procedure the specification of z t is not limited to the methods mentioned above but can be further tailored whenever a more elaborate characterization of the nonstationarity e g using a nonlinear function is available 2 4 2 determination of the ns ffa model the distribution was selected based on the stationary component i e x t specifically the distribution was selected from the pool of candidate distributions using the l moment ratio diagram hosking 1990 which has been commonly used and acknowledged to be robust in the s ffa e g nguyen et al 2017 ouarda and charron 2019 papalexiou and koutsoyiannis 2013 the difference between the l kurtosis of the sample and the candidate theoretical distributions which is referred to as l kurtosis discrepancy was used to select the optimal distribution hosking and wallis 1997 the nonstationary structure of the ns ffa model was identified according to the distribution moment equations e g see stedinger et al 1993 or gado and van nguyen 2016a based on z t for instance a trend in the mean would be properly captured by a model with a time dependent location parameter while a trend in the standard deviation would be expressed by both the time dependent location and scale parameters for the three candidate distributions as linear trend s were considered in this paper the nonstationary structures considered are m 1 0 0 d or m 1 1 0 d where d is the distribution in table 1 2 5 simulation study a simulation study was conducted to demonstrate the advancements of the proposed decomposition method over the method employed by cunderlik and burn 2003 referred to as the benchmark method herein the comparison between these methods relied on the relative bias of the statistical moments of the derived stationary stochastic component with respect to the true statistical moments known a priori the synthetic nonstationary datasets amss have stationary stochastic components following the gev and pe3 distributions and are called the nonstationary gev and pe3 datasets respectively three classes of nonstationary datasets were generated namely c1 with μ y t μ x t a t where μ x t 100 and a 0 4 c2 with σ y t σ x t b t where σ x t 45 and b 0 2 and c3 with μ y t μ x t a t and σ y t σ x t b t with the same a and b as c1 and c2 respectively the skewness of all datasets γ x t is time independent and is equal to 3 all these selected values are within their commonly reported ranges in several studies agilan and umamahesh 2017 ouarda et al 2018 papalexiou and koutsoyiannis 2013 sun et al 2015 villarini and smith 2010 the nonstationary gev datasets for each class were generated by randomly sampling eq 2 with the time dependent distribution parameters derived from the moment equations stedinger et al 1993 stedinger 2017 the nonstationary pe3 datasets were generated employing eq 4 and the designated time dependent moment s for each class the sample size of all the synthetic datasets was fixed to 100 which is often considered long in practice and is similar to the sample size of the real datasets utilized in this paper for each distribution and class 5 000 datasets were generated and thus a total of 30 000 datasets were employed in this simulation study to further investigate the proposed decomposition based approach in the ns ffa its performance was assessed and compared with the performance based approach the fitting efficiency and uncertainty of the models determined by both approaches were examined in terms of aic and cwi respectively these metrics were estimated in two different ways namely with respect to the empirical and the true quantiles which are derived from the sample datasets and the a priori known true nonstationary distribution respectively these two assessment settings are named the theoretical and empirical assessments herein the use of different references to calculate the performance metrics in the two assessment settings led to the differences in the aic and cwi the aic was employed as the selection criterion in the performance based approach for the comparison as it has been commonly used in the literature the number of nonstationary datasets considered for the simulation study was determined such that the estimates for all the assessments were stabilized 2 6 application on real datasets flow amss of thirteen stations retrieved from the environment and climate change canada eccc and the united states geological survey usgs were employed the exploratory analyses were performed to detect the presence of temporal trends and change points in these datasets which are indicative of the presence of nonstationarity the non parametric mann kendall mk test was employed to detect monotonic trends in the mean the mk test was coupled with the moving window method with the window length of 10 and the window step of 5 to investigate the temporal trends in the standard deviation higher order statistical moments were not investigated due to their high sensitivity to extreme events limited sample sizes and consequently the difficulty in their estimation griffis and stedinger 2009 papalexiou and koutsoyiannis 2013 the non parametric mann kendall sneyers mks and mann whitney pettitt mwp tests were employed to detect the presence of change point s in the mean the detection of change points in the standard deviation was not conducted due to the limited sample sizes a significance level of 0 05 was used in all these tests the perceived nonstationarity in these datasets exhibits different patterns in the form of significant trends and or change points based upon the presence or absence of significant trends in the mean standard deviation and or change points the amss were grouped into the c1 c2 and c3 plus an additional class class 4 c4 more complex patterns containing a significant change point no change points were detected in the datasets of c1 c2 and c3 the classification of the datasets as well as their corresponding ids are presented in fig 1 among the thirteen datasets several of them including c1 1 c3 1 c4 1 c4 2 c4 3 and c4 4 have been also used in previous studies on the ns ffa e g ammar et al 2020 luke et al 2017 obeysekera and salas 2014 salas and obeysekera 2014 villarini et al 2009 the performance assessment of both approaches was also conducted in terms of aic and cwi in this case only the empirical assessment was undertaken because the true quantiles are unknown in real practice 3 results and discussion 3 1 simulation study 3 1 1 advancements of the proposed decomposition procedure fig 2 shows the relative bias of the proposed and benchmark methods for the synthetic datasets of c1 the two methods yielded the same estimates of μ x t σ x t and γ x t as they use the same procedure to remove the z t in μ y t in contrast the improvements of the proposed method over the benchmark method are apparent for the datasets of c2 and c3 the medians of the estimates of μ x t σ x t and γ x t of the proposed method are unbiased or close to unbiased close to zero in particular the proposed method considerably reduced the relative bias in the estimate of γ x t which describes the right tail of the distribution and consequently is particularly important in the distribution selection in addition the proposed method offered a narrower or approximately the same interquartile range of the estimates of μ x t σ x t and γ x t similarly the full ranges of the relative bias in the proposed method are narrower than or approximately equal to those in the benchmark method except for the estimates of σ x t for the nonstationary pe3 datasets these results demonstrate that the proposed method can not only reduce the biases in the estimates especially of higher order moments but also is more robust compared to the benchmark method consequently the proposed method would improve the distribution selection using the derived stationary stochastic component and in turn further advance the decomposition based ns ffa 3 1 2 performance of the proposed decomposition based ns ffa to further assess the proposed decomposition based approach its performance was compared with that of the performance based approach as shown in fig 3 the decomposition based approach yielded consistently lower medians and first and third quartiles of aics in all the nonstationarity classes and both parent distributions in the theoretical assessment i e with respect to the true quantiles in contrast the performance based approach produced lower aics medians first and third quartiles and minimum and maximum values throughout all the datasets in the empirical assessment i e with respect to the empirical quantiles in terms of the cwi the decomposition based approach outperformed the performance based approach in the theoretical assessment as it yielded lower medians and first and third quartiles whereas both approaches performed equivalently in the empirical assessment moreover the decomposition based approach was always superior to the performance based approach in capturing the true nonstationary structure and distribution as well as the true ns ffa model the combination of the two model components fig 4 in the simulation study the decomposition based approach always selected the true nonstationary structure as the a priori known temporal trends were directly employed to determine the nonstationary structure whereas the true distribution and thus the ns ffa models were selected on average in 56 7 of the datasets within the range of 45 6 to 66 7 the incorrect selections might be ascribed to the sample l ratios falling approximately in the middle of two theoretical distributions for several datasets whereas the performance based approach selected the true distribution and nonstationary structure on average in 41 4 within the range of 32 6 to 48 9 and 17 6 within the range of 5 7 to 26 4 of the datasets respectively resulting in the selection of true models on average in only 7 8 within the range of 3 1 to 10 5 of the datasets these results reveal that the two ns ffa approaches often yielded different models namely different distributions and or nonstationary structures and in turn different performance in the performance based ns ffa the ergodicity required for making the statistical inference is completely violated as both the distribution and the nonstationary structure are selected simultaneously by optimally fitting the nonstationary datasets thus the performance based approach may result in a ns ffa model that does not capture the underlying nonstationary stochastic process well despite it might fit the observations adequately according to the pre selected performance metric the misspecification of the distribution and or nonstationary structure can introduce errors in the quantile estimates in the fitting period and could be particularly critical when implementing the model for the out of sample predictions in contrast the decomposition based ns ffa avoids the ergodicity violation as the distribution is selected from the decomposed stationary stochastic component in which the ergodicity holds while the nonstationary structure is determined separately according to the available knowledge of the nonstationarity this fact makes the decomposition based approach especially advantageous over the performance based approach from the theoretical point of view in addition the simulation results demonstrate that the performance based approach outperforms the decomposition based approach in the empirical assessment although it is inferior in the theoretical assessment this argues that the performance based approach tends to select untrue models due to overfitting the samples irrespective of the actual nonstationary stochastic process in contrast the decomposition based approach is superior in capturing the underlying true nonstationary process as reflected by its higher percentage in selecting correct ns ffa models as well as its higher fitting efficiency and the same or less uncertainty than the performance based approach in the theoretical assessment therefore both the theory and the simulation results advocate that the decomposition based ns ffa is preferable in addition the comparison of these two approaches was also conducted when the bic instead of the aic was used as the selection metric in the performance based approach the results appendix d consistently supported the aforementioned findings 3 2 real applications 3 2 1 implementation of the proposed decomposition based approach fig 5 a c show one example of decomposed datasets of classes 1 2 and 3 respectively the proposed decomposition procedure effectively removed the significant monotonic trends in the mean and or the standard deviation no significant trends nor change points were detected in the decomposed datasets these are indicative that the decomposed datasets are stationary since the presence of change points was not considered in the ns ffa the perceived monotonic trends over the whole observation period were utilized for the decomposition of datasets in class 4 it is thus not surprising that the change point was still detected in two out of four decomposed datasets of this class i e c4 3 and c4 4 fig 5 d and e illustrate the decomposed datasets c4 2 without a change point and c4 3 still with a change point as examples respectively this issue is also present in the performance based approach in which no candidate models considering change points were adopted in this paper and most of the literature due to the highly uncertain model parameterization as a result the proposed decomposition procedure effectively removed the perceived z t in datasets in classes 1 2 and 3 and consequently the approaches commonly employed to select the distribution in the s ffa e g the l moment ratio diagram are applicable table 2 shows the l kurtosis discrepancies between each decomposed dataset and the candidate theoretical distributions and highlights the best fit distributions note that in some cases e g c1 1 and c1 2 the difference in the l kurtosis discrepancy between the best fit and the second best distributions is very minor 0 005 this situation is also common in the s ffa when the dataset s l moment ratio locates approximately in the middle of two candidate theoretical distributions in the l ratio diagram when more than one distribution fits acceptably well the data any of them could be a reasonable choice hosking and wallis 1997 although the distribution with slightly better fitting is usually selected e g cunderlik and burn 2003 ouarda et al 2019 further investigation using additional statistical tests could be conducted if desired however this would merely be an additional effort to privilege certain aspects of interest of the fitting as the true distribution is unknown rather than an issue of statistical inference table 2 also shows the selected nonstationary structures for each dataset according to the detected temporal trends in y t or its corresponding log transformation y t in several datasets i e c2 3 c3 1 c3 2 c3 3 and c4 2 the trends in the standard deviation of y t were no longer significant in y t the possible absence of a trend in the standard deviation due to the log transformation and consequently the use of a time invariant scale parameter in the lpe3 distribution have been discussed in luke et al 2017 and stedinger and griffis 2011 therefore when only a trend in the standard deviation is present the log transformation may lead to determining a stationary structure for the lpe3 distribution and consequently might be inadequate for conducting the ns ffa sometimes furthermore neglecting complex nonstationary patterns e g datasets in class 4 could yield misleading results in the analysis for instance a stationary model m s p e 3 was determined for c4 4 irrespective of the perceived nonstationarity in the dataset table 2 nevertheless the confident identification of complex nonstationary patterns is difficult due to the sample size of amss and the potential noise of natural climate variability occurring on timescales of up to 30 years françois et al 2019 this along with the high uncertainty in model parameterization constrains the use of complex nonstationary structures in the ns ffa in these applications the monotonic temporal trends were used as a proxy for z t despite that theoretically the violation of ergodicity is not completely avoided when using the perceived temporal trends only serinaldi et al 2018 this can be resolved by incorporating deductive reasoning and or the physical causative process es or driver s into the analysis whenever they can be properly identified case specific investigations in this regard are beyond the scope of this paper furthermore the determination of the nonstationary structure and the derivation of the stationary component in the proposed decomposition based approach depends on the methods used to identify z t although statistical tests such as the mk test can suggest the temporal dependence of the statistical moments and subsequently the corresponding distribution parameter s they do not discriminate if the dependence is linear or nonlinear this paper represented the nonstationarity using linear trends this is practically feasible and convenient considering the limitations to characterize z t in a higher degree of detail an appropriate and systematic delineation and attribution of z t is key to reliably implementing the proposed decomposition based ns ffa 3 2 2 comparison of the proposed decomposition based and performance based approaches figs 6 and 7 show the ns ffa models determined by the proposed decomposition based approach with the black box and selected from the candidate nonstationary models by the performance based approach with the numerical index 1 in terms of aic and cwi respectively for each dataset note that both the aic and cwi are normalized into the range of 0 1 for each dataset the top second and third ns ffa models selected by the performance based approach are also indicated with the indexes 2 and 3 respectively the corresponding stationary model counterparts m s gev m s p e 3 and m s l p e 3 are also included in these figures as the nonlinear nonstationary structures were considered in the performance based approach table 1 but not in the decomposition based approach the comparison of the selected models determined by the two approaches was conducted in a broad sense namely comparing if the models have the same distribution and the same time variant location parameter or time variant location and scale parameters in the nonstationary structure as shown in figs 6 and 7 these two approaches generally yielded different ns ffa models except in c1 1 and c1 2 in terms of aic and c3 2 and c3 3 in terms of cwi on one hand they selected different distributions in five c2 1 c2 2 c2 3 c3 1 and c3 3 and four c2 1 c2 2 c2 3 and c3 1 datasets in classes 1 3 based on the aic and cwi respectively and all datasets of class 4 except c4 4 based on the aic on the other hand they also determined different nonstationary structures in five c1 3 c2 1 c2 2 c2 3 and c3 2 and five c1 1 c1 2 c1 3 c2 1 and c2 2 datasets in classes 1 3 in terms of aic and cwi respectively and half of the datasets of class 4 c4 1 and c4 4 in terms of aic and c4 3 and c4 4 in terms of cwi recall that different nonstationary structures were identified for y t and y t in some cases due to the log transformation these results evidence that these two ns ffa approaches are fundamentally different in determining the model as they are formulated following distinct logics moreover fig 8 shows the relative discrepancies in aic and cwi of the model determined by the proposed decomposition based ns ffa with respect to that by the performance based ns ffa for all datasets as shown in fig 8 the relative discrepancy in aic ranges from zero to moderate 0 30 in six out of nine datasets of classes 1 3 whereas it is high 0 30 in the other three datasets i e c1 3 c2 3 and c3 1 among these three datasets the high discrepancy in c1 3 and c3 1 could be ascribed to the small aics close to zero of the models selected by the performance based ns ffa in the datasets of class 4 the discrepancy is high in two cases c4 1 and c4 2 the discrepancy in cwi is in the range from zero to moderate in the datasets of classes 1 3 overall except in c2 1 and c2 2 while it is high in two datasets of class 4 c4 1 and c4 4 this comparison does not intend to demonstrate which approach outperformed the other the two approaches often yield different ns ffa models and consequently their different performance is expected on one hand the comparison might imply the possible overfitting by the performance based approach as demonstrated in the simulation study on the other hand the zero to moderate discrepancies argue that the decomposition based approach is in general competitive to the performance based approach from the perspective of the performance therefore these results advocate the feasibility and applicability of the decomposition based approach in real practice if using more flexible e g nonlinear nonstationary structures in the decomposition based ns ffa its performance could be further enhanced in some cases for instance the discrepancy in aic can be reduced e g c2 2 c3 1 c3 2 and c3 3 or even eliminated e g c1 1 and c1 2 when a non linear nonstationary structure is adopted fig 6 however the use of a flexible and complicated nonstationary structure without justification could lead to the overfitting problem especially in data scarce locations using complicated nonstationary structures in the decomposition based ns ffa is mathematically feasible but requires understanding the physical mechanism s driver s behind the nonstationarity to avoid overfitting when they cannot be properly justified the use of a parsimonious nonstationary structure i e the linear nonstationary structure might be preferred 3 2 3 the challenges in implementing the performance based ns ffa as discussed previously in the simulation study the performance based ns ffa cannot avoid the violation of ergodicity and may lead to spuriously high performance in this approach there are two additional issues that deserve attention as shown in figs 6 and 7 when two different distributions are coupled with their corresponding most fitting efficient nonstationary structures which are often different they may yield approximately equal performance for instance for c1 1 the optimal model m 2 0 0 l p e 3 is marginally superior to m 1 1 0 p e 3 by 2 3 in aic while for c3 1 the optimal model m e 1 1 0 gev outperforms m 2 0 0 p e 3 by 5 4 fig 6 hence a different nonstationary structure may be selected when selecting a different distribution which in turn yields a completely distinct model in the performance based ns ffa this is analog to the equifinality in hydrological modeling namely different model structures may be equally capable of fitting the observations according to a certain or a few pre selected performance metric s these suggest that the selection of the nonstationary structure needs to be separated from the selection of the distribution in order to explicitly use the knowledge of the nonstationarity to guide the model determination as in the decomposition based ns ffa the other challenge lies in the selection of the evaluation criterion to determine the optimal model in the performance based approach when using different evaluation metrics to assess the models performance from a specific perspective such as the fitting efficiency e g using the aic and bic the results were consistent in general appendix d however the observed differences reveal that they might lead to determining different ns ffa models sometimes when assessing the models performance from different perspectives such as the fitting efficiency and uncertainty the results shown in figs 6 and 7 also reaffirm that different models may be selected by this approach when using different evaluation criteria the aic and cwi here when using the aic this approach always led to the selection of a nonstationary model however it is not uncommon that the stationary models yield the lowest uncertainty when a temporal trend is present in either mean or standard deviation e g c1 2 c1 3 and c2 2 this result supports the argument that the stationary model might outperform the nonstationary model from the uncertainty viewpoint even when the nonstationarity presents ouarda et al 2019 vidrio sahagún et al 2021 therefore the optimal model selected by the performance based approach is sensitive to the evaluation criterion used the model has been commonly selected according to the fitting efficiency criterion in the performance based ns ffa however the uncertainty is also a key when selecting the model in the ffa as it reflects the reliability of estimates and their applicability for design purposes serinaldi and kilsby 2015 hence the lack of agreement on which selection criterion should be employed challenges the implementation of the performance based approach as it would heavily depend on the performance criterion selected according to the preference of the modelers 3 3 future research recommendations in this paper the proposed decomposition method for the decomposition based ns ffa was theoretically justified and practically demonstrated through both the simulation study and practical applications using the datasets in c1 c2 and c3 yet more challenges arise when dealing with more complex nonstationary patterns in the ns ffa such as those in c4 containing change point s fig 1 and or non linear patterns mathematically both the performance based and decomposition based approaches are capable of handling any form of nonstationarity i e linear and non linear and or with change point s however the performance based method is challenged in practice when incorporating more complicated nonstationary structures as their use may be difficult to justify and lead to overfitting as demonstrated in the simulation study whereas the decomposition based approach is challenged to identify a more complex deterministic component behind the nonstationarity to decompose the underlying dataset to present it is difficult to reliably characterize complicated nonstationary patterns unless clear mechanisms behind the nonstationarity can be identified thus further research on nonstationarity characterization and attribution is required to ensure the successful practical implementation of the ns ffa when more complex nonstationary patterns present in both the decomposition based and performance based approaches the time was used as the covariate however it is worth mentioning that both approaches can employ the physical covariate s e g climatic indexes and land use cover parameters among others whenever they and or their deterministic law of time governing their evolution can be properly identified this would be achieved by the attribution of the nonstationarity to its physical driver s causative process es e g climate change and or changes in the watershed resulting from human intervention hence the proposed method as well as other ns ffa approaches e g the performance based approach would benefit from the improvement in the nonstationarity attribution which is beyond the scope of this paper in the decomposition based approach this would be particularly relevant to the identification of the physical driver s behind the nonstationarity and thus z t recall that the existence of a deterministic component z t behind the observed changes in the dataset is necessary to justify the implementation of the ns ffa in general this requirement does not imply the need for perfect knowledge of z t but can be validated by deductive reasoning serinaldi et al 2018 the nonstationary driver s can yield a z t enveloped by the variability due to chaotic nonlinear internal dynamics and unpredictable natural external forcings milly et al 2015 montanari and koutsoyiannis 2014 hence the proposed decomposition based approach is also applicable for physical drivers showing a deterministic signal of change accompanied by stochastic variability such as climate variables affected by climate change the opportunity to explicitly incorporate physical knowledge of the deterministic changes in the system into the model development and the subsequent inference is an attractive feature of the decomposition based approach it is noteworthy that uncertainty originates from distinct sources in different ns ffa approaches differing from the performance based approach the decomposition based method avoids the uncertainty in the model selection due to equifinal models according to the performance metric s whereas uncertainty arises from the specifications of the deterministic component and the distribution selection as in the s ffa in this approach thus improvements in these two aspects would further reduce the uncertainty in the model determination of the decomposition based approach lastly the decomposition based ns ffa might confront several practical challenges when making the out of sample predictions when implementing the performance based approach it can yield such predictions by a directly fitting projected floods b extrapolating the ns ffa models based on the temporal covariate and c extrapolating the models based on the physical covariate s and its projections these implementations would thus rely on the adequate projections of floods the validity of the assumption that the temporal trend detected in the historical period remains the same in the future and the reliable projections of the physical covariate s and the validity of its link function with the distribution parameters in the future respectively as similar strategies can be implemented when using the decomposition based ns ffa to make the out of sample predictions the reliability of this approach is also subject to similar practical challenges as the performance based approach 4 conclusion aiming to advance the ns ffa this paper proposed a novel decomposition procedure for determining the model in the decomposition based approach in which the distribution and the nonstationary structure are selected separately based upon the stationary stochastic component and the available knowledge of the nonstationarity the proposed method which strictly follows the theoretical decomposition of nonstationary stochastic processes was compared with a previous decomposition method in both an analytical deduction and a simulation study the analytical deduction demonstrated the theoretical advancement of the proposed method while the simulation results confirmed its effectiveness in reducing the bias in the moment estimates of the derived stationary stochastic component especially in the skewness that is key in the distribution selection these demonstrated the improvement in the decomposition based ns ffa moreover the proposed decomposition based approach was further compared with the performance based approach using both the synthetic and real flow amss exhibiting different patterns of nonstationarity on one hand the proposed decomposition based ns ffa was shown superior to the performance based approach as it captured the known underlying nonstationary process more efficiently and yielded a higher performance in the simulation on the other hand the performance based approach was found prone to overfitting the synthetic datasets while the zero to moderate discrepancies between the performance of the two approaches argued that the decomposition based approach is competitive in general from the performance perspective in real applications in addition the performance based approach was shown to be subject to the equifinality issue as well as the determination of distinct models when using different performance evaluation metrics furthermore differing from the performance based approach the decomposition based ns ffa avoided alleviated the ergodicity violation and was thus preferable from the theoretical viewpoint therefore this paper demonstrated the superiority and convenience of the proposed decomposition based ns ffa credit authorship contribution statement cuauhtémoc tonatiuh vidrio sahagún conceptualization methodology formal analysis investigation writing original draft visualization jianxun he supervision writing review editing resources funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the first author of this paper is funded by a doctoral scholarship from the national council for science and technology of mexico conacyt and the universidad de guadalajara this work is also partially funded by the discovery grant of natural sciences and engineering research council held by the second author appendix a benchmark detrending procedure cunderlik and burn 2003 the detrending methodology applied in cunderlik and burn 2003 was taken as the benchmark to assess the decomposition procedure proposed in this paper the benchmark method deals with the weak second order nonstationarity i e the temporal variability of the first two statistical moments of the underlying stochastic process while assuming the temporal invariance of the higher order moments e g the skewness lindgren et al 2013 von storch and zwiers 2002 in this procedure the y t is considered in the general form of a1 y t τ t ε t where τ t is the deterministic time dependent component in the mean and ε t is the residual time dependent stochastic component describing the irregular fluctuations around the τ t in practice the τ t is determined as a linear function of time i e τ t τ 0 τ 1 t where τ 0 and τ 1 are the regression coefficients using the non parametric sen s estimator hence the ε t which is the de trended series in the mean is given by a2 ε t y t τ t the ε t would contain the trend in the variability of y t the trend in the variability would then be approximated using the transformed series ε t which captures the change of ε t over time and is calculated by a3 ε t ε t ε where ε is the average of ε t and is time invariant the deterministic trend in the transformed series ε t which is denoted as φ t is then estimated similar to τ t the φ t is determined as a linear function of time i e φ t φ 0 φ 1 t where φ 0 and φ 1 are the regression coefficients using the sen s estimator the φ t is removed from ε t by a4 y t st ε t φ t ε t ε ε t φ t ε t ε f o r i n c r e a s i n g φ t ε t φ t ε t ε ε t φ t ε t ε f o r d e c r e a s i n g φ t where y t st y t 1 st y t 2 st y t n st is the resulting second order stationary series the y t st derived by the benchmark procedure is free from trends in the first two statistical moments and has been used to select and or parameterize the distribution e g cunderlik and burn 2003 gado and nguyen 2016 zhang et al 2020 for datasets in c1 presenting a temporal trend in the mean only the benchmark and the proposed methods coincide as the temporal invariant ε t reduces eq a 1 to an equivalent form of eq 10 in which g is the identity operator thus eqs 12 14 are also applicable for the benchmark method and thus the standard deviation and the skewness of the detrend time series are preserved in the benchmark method however the benchmark method does not preserve the stochastic component for datasets in c2 and c3 let λ t be the intermediate deterministic variable defined as a5 λ t φ t ε t ε φ t ε t ε f o r i n c r e a s i n g φ t φ t ε t ε φ t ε t ε f o r d e c r e a s i n g φ t this allows to express the derived stationary second order stationary series as a6 y t st y t τ t λ t for datasets in c2 presenting a temporal trend in the standard deviation only as τ t 0 t eq a 6 reduces to y t st y t λ t therefore by denoting the deterministic true yet unknown counterparts of τ t and λ t by their uppercase letters i e t t and λ t it can be shown that the benchmark method simply shifts the y t in its first moment μ y t without actually detrending its second moment σ y t 2 a7 μ y t st e y t st e y t λ t e y t e λ t μ y t λ t a8 σ y t st 2 e y t st e y t st 2 e y t λ t e y t λ t 2 e y t λ t μ y t λ t 2 e y t μ y t 2 σ y t 2 a9 γ y t st e y t st e y t st 3 e y t st e y t st 2 3 2 e y t λ t e y t λ t 3 e y t λ t e y t λ t 2 3 2 e y t λ t μ y t λ t 3 e y t λ t μ y t λ t 2 3 2 e y t e y t 3 e y t e y t 2 3 2 γ y t thus the effects of this procedure on the true nonstationary distribution can be thought of as irregular displacements of the marginal distributions of y t controlled by λ t without changing their width σ y t st 2 σ y t 2 and shape γ y t st γ y t as a result the removal of the trend in the variability of y t is ascribed to such displacements and thus this method fails in removing the actual deterministic component behind the nonstationarity and preserving the underlying stochastic component consequently this procedure would introduce errors in the estimates of the statistical moments of the stochastic component for instance the skewness would not be preserved unless the variability of y t is symmetrical this condition is intuitive as λ t φ t is derived from the ε t which takes the absolute deviations from the time invariant mean eq a 3 and thus ignores the signs of ε t ε reflecting the average variability only the even removal of the average deviations ignoring whether the series of ε t is skewed towards the high or low values would alter the proportionality of the variability ranges above and below the mean and in turn distort the asymmetricity properties of the underlying dataset for example if ε t is skewed towards the high values the computed λ t would remove the average deviations around the mean and over under detrend the variability for below above the mean respectively for datasets in c3 presenting temporal trends in both the mean and standard deviation the eq a 6 is directly used therefore it can be shown that the benchmark method also shifts the y t in its first moment without actually detrending its second moment and thus does not preserve the stochastic component either 10 μ y t st e y t st e y t t t λ t e y t e t t e λ t μ y t t t λ t 11 σ y t st 2 e y t st e y t st 2 e y t t t λ t e y t t t λ t 2 e y t t t λ t μ y t t t λ t 2 e y t μ y t 2 σ y t 2 12 γ y t st e y t st e y t st 3 e y t st e y t st 2 3 2 e y t t t λ t e y t t t λ t 3 e y t t t λ t e y t t t λ t 2 3 2 e y t t t λ t μ y t t t λ t 3 e y t t t λ t μ y t t t λ t 2 3 2 e y t e y t 3 e y t e y t 2 3 2 γ y t similar to c2 the skewness of datasets in c3 would only be preserved in the special case of symmetrical variability appendix b theoretical preservation of statistical moments of the stochastic component derived in the proposed decomposition procedure for datasets in c2 eq 15 ensures that only the standard deviation is perturbed after detrending the standard deviation while both the conditional mean and the skewness are preserved b1 μ x t e x t e μ y t y t μ y t h t e e y t y t e y t h t e y t h t e y t e y t e y t μ y t b2 σ x t 2 e x t μ x t 2 e μ y t y t μ y t h t μ y t 2 e y t μ y t h t 2 h t 2 e y t μ y t 2 h t 2 σ y t 2 b3 γ x t e x t μ x t 3 e x t μ x t 2 3 2 e μ y t y t μ y t h t μ y t 3 e μ y t y t μ y t h t μ y t 2 3 2 e y t μ y t h t 3 e y t μ y t h t 2 3 2 h t 3 e y t μ y t 3 h t 3 e y t μ y t 2 3 2 e y t μ y t 3 e y t μ y t 2 3 2 γ y t for datasets in c3 eq 16 ensures that only the mean and standard deviation are perturbed after detrending both the mean and standard deviation while the conditional skewness is preserved b4 μ x t e x t e μ y t f t y t f t μ y t f t h t e e y t f t y t f t e y t f t h t e y t f t h t e y t f t e y t f t e y t f t h t e y t e y t e y t f t μ y t f t b5 σ x t 2 e x t μ x t 2 e μ y t f t y t f t μ y t f t h t μ y t f t 2 e e y t f t y t f t e y t f t h t e y t f t 2 e e y t f t y t f t e y t f t h t e y t f t 2 e y t e y t h t 2 h t 2 e y t e y t 2 h t 2 σ y t 2 b6 γ x t e x t μ x t 3 e x t μ x t 2 3 2 e μ y t f t y t f t μ y t f t h t μ y t f t 3 e μ y t f t y t f t μ y t f t h t μ y t f t 2 3 2 e e y t f t y t f t e y t f t h t e y t f t 3 e e y t f t y t f t e y t f t h t e y t f t 2 3 2 e e y t f t y t f t e y t f t h t e y t f t 3 e e y t f t y t f t e y t f t h t e y t f t 2 3 2 e y t e y t h t 3 e y t e y t h t 2 3 2 h t 3 e y t μ y t 3 h t 2 e y t μ y t 2 3 2 e y t μ t 3 e y t μ y t 2 3 2 γ y t appendix c hyperparameter determination of the moving window method the influence of the window length and step in the moving window method which was coupled with the sen s slope estimator for estimating the trend in the σ y t was investigated since the window length and step only influence the estimation of the trend in the σ y t their selections affect the analysis of datasets of c2 and c3 as the synthetic nonstationary datasets of c2 present a trend in the σ y t only they were used for the investigation herein the assessment was conducted based on the mean absolute relative bias marb of the moment estimates the marb was estimated from the absolute bias values of each statistical moment of all the synthetic datasets for each combination of window length and step of a given parent distribution i e gev or pe3 fig c1 shows the marb of the moment estimates when using window lengths ranging from 10 to 50 years and window steps from 1 year to the window length for the nonstationary gev and pe3 datasets respectively as shown in this figure the marbs of μ x t σ x t and γ x t are in general higher when using the larger window steps for any window length it is also worth mentioning that the marbs slightly increase with the decrease of the window step when using small window steps especially for σ x t and γ x t these results are not surprising as for a given window length the large window step leads to the small sample size of the derived σ y t series and thus deteriorates the estimation of the trend whereas the small window step leads to the large window overlapping and thus the datapoints located in the center of the time series would be counted more times than those located at the bounds and have large influence on the estimates furthermore an assessment incorporating the biases of all the three estimated statistical moments μ x t σ x t and γ x t was conducted for each parent distribution to this end the marbs of μ x t σ x t and γ x t were averaged and then used to identify the optimal hyperparameters fig c2 presents the average of the marbs of μ x t σ x t and γ x t and highlights the minimum average of the marbs corresponding to the optimal window step for each window length these results indicate that the lowest average of the marbs is yielded when using the window length of 10 years and the window step of 5 years the selected hyperparameters are rational as they yield a long series of σ y t for estimating the trend and also avoid substantial bias of the datapoints located in the middle of the time series besides the window length would also skip potential decadal variability therefore these hyperparameters of the moving window method were adopted throughout this paper appendix d results of the performance based approach when using the bic as the selection criterion the potentially different results of the performance based approach due to using another evaluation metric of fitting efficiency were explored to this end the bic was employed in both the simulation study and real applications and was calculated by d1 bic n log r m s e n θ log n in the simulation study similar results were obtained when using the bic and aic as the selection metrics in the performance based approach as shown in figs d1 and 3 the use of bic or aic yielded practically equivalent results in both empirical and theoretical assessments when using the bic the performance based approach selected the true distribution and nonstationary structure on average in 41 6 within the range of 32 8 to 48 4 and 17 8 within the range of 10 0 to 23 8 of the datasets respectively resulting in the selection of true models on average in only 7 1 within the range of 3 7 to 9 0 of the datasets fig d2 compared to the results when using the aic as the selection metric fig 4 the changes in the results of the performance based approach were practically negligible thus the results from the performance based approach when using the bic as the selection metric also support that the performance based approach tended to select untrue models and overfit the samples compared to the decomposition based approach fig d3 shows the ns ffa models determined using the bic as the selection metric in the performance based approach in real applications as shown in this figure the results based on the bic were in general consistent with those based on the aic fig 6 comparing the results based on the aic and bic the use of different evaluation metrics led to selecting a different optimal model in only one dataset c3 1 while the differences in at least one of the top three models were observed from five out of thirteen datasets c1 3 c2 3 c3 1 c3 2 and c4 4 in addition the relative discrepancies in the bic of the model determined by the proposed decomposition based ns ffa with respect to that by the performance based ns ffa fig d4 were similar to previous results fig 8 thus the changes in the performance based approach were minor in general and the results based on the bic also supported the conclusions drawn based on the results obtained using the aic both the aic and bic are used to measure the fitting efficiency but employ a different weighting of the goodness of fit and the complexity of the model length of data despite the similar results in general the observed differences between the results based on the aic and bic demonstrated that the use of different selection metrics might lead to different results in the performance based approach these results further argue the issue on the selection of the evaluation criteria in the performance based approach 
3083,the nonstationary flood frequency analysis ns ffa is conducted when the assumption of stationarity in hydrologic extremes is violated the commonly used approach is the performance based ns ffa which jointly determines the distribution and the nonstationary structure based upon the model performance however this approach is challenged from both theoretical and practical perspectives an alternative is the herein named decomposition based ns ffa which determines the two ns ffa model components separately and explicitly uses the available knowledge of the nonstationarity however it has been barely implemented in practice this paper proposed a novel decomposition procedure that strictly follows the theoretical decomposition of nonstationary stochastic processes to advance the decomposition based ns ffa the proposed decomposition procedure was compared with a previously reported method in both an analytical deduction and a simulation study the proposed decomposition based ns ffa was further compared to the performance based ns ffa using both synthetic and real datasets from north america which exhibit different patterns of nonstationarity the particle filter was adopted for uncertainty quantification and parameter estimation the results revealed that the proposed decomposition based approach was advantageous in preserving the moments of underlying stochastic component particularly the higher order moment i e skewness in addition the comparison of the two ns ffa approaches demonstrated the superiority of the proposed decomposition based approach in capturing the underlying known nonstationary stochastic process and being competitive with the performance based approach from the performance perspective in real applications the results from the simulation study and the real application also revealed several caveats of the performance based approach including the potential overfitting and equifinality problems as well as the selection of distinct models when adopting different performance metrics in addition differing from the performance based approach the decomposition based ns ffa avoided alleviated the ergodicity violation all these results demonstrated the advancements of the proposed decomposition based ns ffa and advocated its application in the ns ffa keywords nonstationarity nonstationary structure distribution extreme events uncertainty fitting efficiency 1 introduction assessing the recurrence of floods which is a key task in hydrology and water resources management is commonly conducted by flood frequency analysis ffa the conventional ffa predicts the flood quantiles corresponding to specific return periods and vice versa based on the core assumption that the underlying process is stationary and thus it is often referred to as the stationary ffa s ffa nonetheless the reliability of the s ffa is compromised when confronted with nonstationary processes as a result of climate change and changes in land use cover and or water management aghakouchak et al 2020 blöschl et al 2019 milly et al 2008 milly et al 2015 under this circumstance the nonstationary ffa ns ffa in which the probability distribution of the floods varies as a function of a selected covariate s has emerged as the theoretical solution to deal with nonstationary scenarios françois et al 2019 lópez and francés 2013 salas et al 2018 slater et al 2021 in this context the determination of a ns ffa model which is formed by coupling the probability distribution of floods and the nonstationary structure that governs its evolution over time is a key to implementing the ns ffa the conventional approach for determining a suitable ns ffa model is through selection by ranking a set of candidate models according to the pre selected performance metric e g information theoretic measures such as the akaike and bayesian information criteria aic and bic e g kim et al 2017 li et al 2019 ragno et al 2019 this approach is referred to as the performance based approach in this paper since the model is selected by jointly assessing the nonstationary structure and the distribution in this approach the determination of one model component is conditioned on its performance in conjunction with the other e g agilan and umamahesh 2017 mondal and mujumdar 2015 strupczewski et al 2001 besides this approach does not explicitly use the available knowledge of the nonstationarity beyond the time series such knowledge can be detailed physical derived from the causative process es behind the nonstationarity e g through physically based climate and hydrological models or statistical deducted knowledge derived from underlying datasets and deductive reasoning in addition the implementation of the performance based ns ffa has been challenged from the perspectives of both theoretical fundament and practical application firstly under nonstationarity a time series is not a set of random independent realizations of the same distribution and consequently differing from a stationary time series it does not visit most of its domain when the sample size increases von storch and zwiers 2002 as a result the ergodicity does not hold under nonstationarity and in turn the statistical properties of a nonstationary process cannot be reliably inferred from a single time series only koutsoyiannis and montanari 2015 serinaldi et al 2018 therefore the model determination is questionable in this approach secondly analogously to the black box modeling the ns ffa model is selected for achieving the best performance while bypassing the available knowledge of the system as a result the optimal model may yield a spuriously high performance due to increased fitting flexibility irrespective of its correspondence to the actual nonstationary stochastic process luke et al 2017 serinaldi and kilsby 2015 thirdly the use of different performance metrics such as for evaluating the uncertainty e g the average width aw or coverage width index cwi which has been commonly relegated to a secondary aspect and the fitting efficiency e g the aic or bic often leads to selecting different optimal models ouarda et al 2019 serinaldi and kilsby 2015 vidrio sahagún et al 2021 lastly in this ns ffa approach there might be two or more candidate models with almost identical performance evaluated according to a pre selected evaluation metric consequently there would be uncertainty in the model selection this issue is similar to the equifinality problem in the hydrological modeling beven 2006 beven and freer 2001 khatami et al 2019 namely different model structures and or parameter sets may be equally capable of reproducing the observations according to a or a few performance metric s however such equifinal models may not necessarily yield the same results in view of the above inferring the ns ffa model based on a performance metric is not necessarily reliable serinaldi et al 2018 on the other hand nonstationary stochastic processes are commonly considered as the superposition of a stationary stochastic component and a deterministic time dependent component koutsoyiannis and montanari 2015 milly et al 2015 serinaldi et al 2018 this notion suggests the potential of selecting the two components of the ns ffa model separately namely the nonstationary structure can be determined from the available knowledge on the deterministic time dependent component while the distribution can be inferred using the stationary component in which the ergodicity holds in this way the ergodicity violation for statistical inference faced in the performance based approach can be avoided when physical or statistical deducted knowledge complements the information from the time series despite the suggestion of incorporating physical knowledge into the ns ffa gilroy and mccuen 2012 its use is still constrained by its limitation to capture the nonstationarity of hydrological extremes giuntoli et al 2021 kundzewicz et al 2017 maraun et al 2010 maraun et al 2017 whereas the use of knowledge derived through deductive reasoning is considered feasible in the current practice and thus benefits the ns ffa model determination serinaldi et al 2018 it is worth highlighting that the identification of the deterministic component behind the observed changes is indispensable to justify the ns ffa irrespective of the approach adopted koutsoyiannis and montanari 2015 montanari and koutsoyiannis 2014 serinaldi et al 2018 serinaldi and kilsby 2015 changes produced by purely stochastic drivers would result in doubly stochastic rather than nonstationary processes which are preferably modeled with compound or mixed distributions serinaldi et al 2018 serinaldi and kilsby 2015 the lack of appropriate identification of the deterministic component can lead to unrealistic estimates and subsequently jeopardizes the reliable implementation of ns ffa models serinaldi and kilsby 2015 although the ns ffa alternative approach based on the notion of the nonstationary process decomposition called the decomposition based approach throughout this paper is not new it has not been well explored and developed yet to present only a few studies have applied this notion in the ffa for instance cunderlik and burn 2003 selected the distribution from detrended at site series assuming second order nonstationarity in the regional index flood ns ffa they employed the absolute deviations from the mean of the transformed series detrended in the mean to remove the trend in the second statistical moment zhang et al 2019 identified the overall best fit distribution across canada following the same detrending procedure as cunderlik and burn 2003 when nonstationarity presents sung et al 2018 also applied this procedure to select the distribution although considering nonstationarity in the mean only however such a detrending procedure does not guarantee the preservation of higher order moments e g skewness of the stochastic component and consequently might result in poor estimates e g kalai et al 2020 on the other hand just a few studies have specified the nonstationary structure according to the exploratory explanatory data analysis rather than according to the performance of the models namely the identified temporally variant statistical moments of the underlying dataset have been used to determine the nonstationary structure e g cheng et al 2014 cunderlik and burn 2003 ragno et al 2019 despite these attempts to determine the distribution and nonstationary structure according to the trend free datasets and the nonstationarity knowledge the procedure to remove the deterministic component while the higher order moments are preserved is still desired this improvement would advance the decomposition based approach and thus promote its practical application moreover the decomposition based approach has not been assessed validated or compared with the performance based approach in the literature so far in view of the above this paper proposes a novel general procedure that strictly follows the theoretical decomposition of nonstationary stochastic processes for advancing the model determination in the decomposition based ns ffa the proposed procedure has the capacity to retain the statistical characteristics of the underlying stationary stochastic component after removing the deterministic component this paper demonstrates such an advancement over the existing procedure of cunderlik and burn 2003 by both theoretical deduction and a simulation study in terms of preserving the statistical moments of the a priori known stationary stochastic component in addition the performance namely the fitting efficiency and uncertainty of the proposed decomposition based approach is further assessed and compared with the performance based approach which has been often adopted in the literature using both synthetic and real datasets these demonstrate both the advantages of the proposed decomposition based ns ffa and the caveats of the performance based approach in real applications thirteen annual maximum series ams of flow from canada and the usa which are perceived to have different patterns of nonstationarity are used herein as study cases 2 materials and methods apart from the proposed decomposition based approach this section also introduces the performance based approach the distributions considered the method used for parameter estimation and uncertainty quantification the generation of the synthetic datasets for the simulation study and the real datasets for the practical applications note that throughout this section the random variables and their realizations are denoted by uppercase and lowercase letters respectively while the vectors are denoted by boldface letters in practice the temporal covariate has been commonly used in the ns ffa as a surrogate for any time dependent physical driver s especially for fitting purposes e g obeysekera and salas 2016 sun et al 2018 and or bypassing the exploration of the physical process es both of which are case sensitive this is particularly convenient for methodology oriented rather than case specific research objectives e g prosdocimi and kjeldsen 2021 serago and vogel 2018 moreover the often low to moderate dependency of the nonstationarity on the physical driver s archfield et al 2016 burn and whitfield 2017 ray and goel 2019 has hindered the use of the physical covariate s thus the temporal covariate i e the time t was adopted to illustrate the application of both the performance based and the decomposition based approaches in this paper 2 1 candidate distributions in this paper three probability distributions were considered in both the performance based and the decomposition based approaches the candidate distributions are the generalized extreme value gev the pearson type iii pe3 and the log pearson type iii lpe3 distributions these three parameter distributions have been commonly adopted in the ffa in canada and the usa england et al 2019 gado and van nguyen 2016b stedinger et al 1993 zhang et al 2020 the cumulative gev function of the random variable y f y and the quantile y p e corresponding to an exceedance probability p e p e 1 f y y θ are given by 1 f y y θ exp 1 κ y ξ α 1 κ κ 0 exp exp y ξ α κ 0 2 y p e p e θ f y 1 p e θ ξ α κ 1 ln 1 p e k κ 0 ξ α ln ln 1 p e κ 0 where the parameter vector θ is composed of the shape scale and location parameters denoted by κ α and ξ respectively the cumulative pe3 distribution function is given by hosking and wallis 1997 3 f y y θ g α y ξ β γ α γ 0 φ y μ σ γ 0 1 g α ξ y β γ α γ 0 where the shape scale and location distribution parameters are α 4 γ 2 β 1 2 σ γ and ξ μ 2 σ γ respectively if γ 0 μ σ and γ are the mean standard deviation and skewness of y respectively γ is the gamma function g is the incomplete gamma function and φ is the normal cumulative distribution function there is no explicit analytical form of y p e for the pe3 distribution it has often been approximated using the frequency factor k p through the wilson hilferty transformation for γ 2 and 0 01 p 0 99 by stedinger et al 1993 4 y p e μ σ k p γ k p γ 2 γ 1 γ 6 2 γ z p 6 3 1 where k p γ is the pth quantile 1 p e of the pe3 distribution with zero mean unit standard deviation and skewness γ and z p is the pth quantile of the standard normal distribution the lpe3 distribution corresponds to the case in which the log transformed random variable y l o g 10 y follows the pe3 distribution thus eqs 3 and 4 of the pe3 distribution are applicable while μ σ and γ are the mean standard deviation and skewness of y respectively in the common practice of the ns ffa the nonstationarity has been depicted using time dependent distribution parameters thus in this paper a ns ffa model consists of an aforementioned distribution whose parameters are functions of the time 2 2 parameter estimation and uncertainty quantification the parameter estimation and uncertainty quantification were conducted employing the particle filter pf technique the pf is a method that integrates the recursive bayesian filters and the bootstrap resampling technique and is free from assumptions and reliable in uncertainty estimation doucet et al 2001 gordon et al 1993 särkkä 2013 the pf method has been previously adopted in both the s ffa and ns ffa sen et al 2020 vidrio sahagún et al 2021 in this paper the pf method was implemented following the methodology of vidrio sahagún et al 2021 in a nutshell the pf estimates the hidden state vector of a dynamic system w 0 i w 0 w 1 w i which is difficult if not impossible to be measured in the field from the related observations y 1 i y 1 y 2 y i here w r n w and y r n y denote the n w and n y dimensional hidden state and observation vectors respectively the general idea behind the pf as an inverse approach is to assess the error of the model predictions with respect to the observations and inversely map the model predictions to the parameter space in the context of the ffa the general structure of the pf is formulated by taking the set of model parameters θ and the annual maximum flows y as the hidden state variables and the associated measurements respectively as the parameter estimation is conducted for a given time period in the ns ffa the hidden states θ j θ 1 θ 2 θ n θ evolve in pseudo time replacing time with iterations in order to incorporate all available measurements unitized as a batch at each step j i e y j y t 1 y t 2 y t n j here n θ and n are the number of ns ffa model parameters and sample size of the ams respectively this can be expressed in the bayesian framework as the search for the joint posterior distribution of all the model parameters given the related observations the main steps of the pf include 1 the pf starts at j 1 with a set of m n n θ independent particles ψ j ψ j 1 ψ j 2 ψ j m in the parameter domain which is initially sampled from uninformative marginal prior distributions the initial priors were specified as uniform distributions with ranges large enough such that encompassing all realistic values and none of the posteriors were truncated each particle has an identical weight w ψ j i at j 1 2 then the w ψ j are updated over pseudo time steps i e from ψ j to ψ j 1 based on the particles likelihood and act as discrete posterior distributions the particles likelihoods are obtained from the error between the model outputs i e the estimated quantiles computed using the ψ j and the mapping function f here the quantile function and the observations i e the empirical quantiles derived using the measurements y j and their associated empirical exceedance probability 3 after each pseudo time step resampling is conducted and white noise perturbation is applied to ψ j 1 to avoid particle degeneracy and maintain adequate diversity respectively 4 steps 2 and 3 are repeated until the ψ j converges to a stable estimation with the progress of the pf over pseudo time steps the parameter estimation point estimates can then be carried out employing the stabilized ψ j the uncertainty metrics are calculated using several additional pseudo time steps until they achieve stability as well in the pf several hyperparameters need to be pre determined for instance the number of particles m is selected in a trade off between the precision of the estimates converge to the exact solution when m and the computational demand in this paper the m was set to 5 000 while the number of pseudo time steps to yield the point estimates was 1 000 and the number of additional pseudo time steps to stabilize the uncertainty approximations was 100 please refer to sen et al 2020 and vidrio sahagún et al 2021 for more details on the pf implementation and the setup of the hyperparameters in the s ffa and ns ffa 2 3 performance based ns ffa 2 3 1 candidate nonstationary structures following the common practice of the performance based approach several candidate nonstationary structures in which the distribution parameters except the shape parameter are either linear or nonlinear functions of the time were considered and are shown in table 1 the shape parameter was treated as constant as its estimation is difficult due to sample size limitations and particularly unrealistic when it is allowed to vary coles 2001 katz 2013 more complicated model structures can be taken into consideration but were not included as their parametrization could be highly uncertain serago and vogel 2018 serinaldi and kilsby 2015 the optimal ns ffa model was selected from the listed candidate models according to their performance 2 3 2 performance evaluation metrics the performance was assessed in terms of fitting efficiency and uncertainty the former has been commonly evaluated using the aic which deals with the trade off between the goodness of fit offered by a model and its complexity on the basis of information loss the latter was evaluated using the cwi kasiviswanathan et al 2019 the cwi simultaneously assesses the bandwidth and the observations coverage of the uncertainty band which conflict with each other the aic and cwi were calculated using the derived point estimates of the model parameters and the estimated quantiles by the full set of particles of the pf respectively by 5 aic n log r m s e 2 n θ 6 cwi a w exp 1 α cwi poc 100 2 where α cwi is the significance level 0 05 here and the root mean square error rmse the aw and percentage of coverage poc of the uncertainty bands are calculated by 7 rmse 1 n k 1 n m k o k 2 8 aw 1 n k 1 n m k u m k l 9 poc 1 n k 1 n c k w h e r e c k 1 k s t m k l o k m k u 0 e l s e w h e r e where o k are the empirical quantiles according to the plotting position formula p 1 n r 0 35 n where r is the rank of the k th observation m k are the corresponding modeled quantiles and m k u and m k l are the upper and lower uncertainty bounds of the estimates of the k th observation respectively the empirical plotting position formula was selected due to its better performance for estimating the empirical quantiles compared to other formulas for extreme value distributions hosking 1990 a model yielding a smaller aic is more efficient in fitting while a model with a lower cwi is less uncertain 2 4 decomposition based ns ffa 2 4 1 proposed decomposition procedure the decomposition based ns ffa relies on the widely adopted statistical representation of nonstationary processes y t in the hydrometeorological literature e g koutsoyiannis and montanari 2015 milly et al 2015 serinaldi et al 2018 namely 10 g y t g x t z t where x t and z t are the stationary stochastic and the time dependent deterministic components respectively and g is a generic operation such as e or var the stochastic processes y t and x t are the families of random variables that describe the evolution of the underlying time dependent and time independent physical processes over the period of interest i e for t t 1 t 2 t n respectively in this general decomposition z t governs the changes of g y t over time in the decomposition based approach x t is estimated by removing z t from y t a novel procedure that strictly follows the theoretical decomposition of nonstationary stochastic processes was proposed herein to remove z t in the first two statistical moments according to the definition of weak second order stationarity lindgren et al 2013 von storch and zwiers 2002 differing from the previous methodology of cunderlik and burn 2003 the proposed procedure mathematically guarantees the removal of the deterministic component in the first two moments without altering higher order moments and thus preserves the stationary stochastic component more details on the procedure of the benchmark method and its issues in preserving the stochastic component and its statistical moments according to the theoretical deduction are provided in appendix a in this paper μ x t σ x t and γ x t denote the unconditional stationary mean standard deviation and skewness of x t while μ y t σ y t and γ y t are the conditional nonstationary mean standard deviation and skewness of y t the conditional statistics refer to the time dependent statistical properties of the underlying nonstationary stochastic process based upon the second order stationarity three general classes of nonstationary scenarios were considered herein namely class 1 c1 presence of z t in the μ y t only class 2 c2 presence of z t in the σ y t only and class 3 c3 presence of z t in both the μ y t and σ y t for datasets in c1 i e whose process decomposition consists of e y t e x t z t the stationary dataset x t x t 1 x t 2 x t n was obtained following an additive scheme by simply subtracting z t in the form of f t from the ams y t y t 1 y t 2 y t n 11 x t y t f t when f t 0 t it is the special case of the absence of z t in the mean and thus y t is stationary equation 11 ensures that only the mean of the dataset is perturbed after z t is removed while both the conditional standard deviation and skewness are preserved 12 μ x t e x t e y t z t e y t z t μ y t f t 13 σ x t 2 e x t μ x t 2 e y t z t μ y t z t 2 e y t μ y t 2 σ y t 2 14 γ x t e x t μ x t 3 e x t μ x t 2 3 2 e y t z t μ y t z t 3 e y t z t μ y t z t 2 3 2 e y t μ y t 3 e y t μ y t 2 3 2 γ y t for datasets in c2 i e whose process decomposition consists of v a r y t 1 2 v a r x t 1 2 z t the x t was obtained following a multiplicative scheme by removing z t in the form of g t 15 x t μ y t y t μ y t h t w h e r e h t σ x t σ x t g t where μ y t is the estimated mean of y t which is time independent for datasets in c2 h t is a normalized multiplicative term depicting z t g t is the estimated z t in the σ y t and σ x t is the estimated standard deviation of x t using the beginning of the observation period as the reference when g t 0 t and thus h t 1 it is the special case of the absence of z t in the standard deviation and thus y t is stationary similar to illustrated in eqs 12 14 eq 15 also ensures that only the standard deviation is perturbed as σ x t 2 h t 2 σ y t 2 after removing z t from the dataset while both the conditional mean and skewness are preserved i e μ x t μ y t and γ x t γ y t refer to appendix b note that since z t is deterministic and or independent its covariances with x t and y t are zero and thus are not included in the formulation for datasets in c3 i e whose process decomposition consists of e y t e x t z 1 t v a r y t 1 2 v a r x t 1 2 z 2 t the x was obtained by combining eqs 11 and 15 16 x μ y t f t y t f t μ y t f t h t w h e r e h t σ x t σ x t g t where μ y t f t is the estimated mean of the partially decomposed dataset after z 1 t f t is removed from μ y t but z 2 t g t is still present in σ y t it can be also demonstrated that eq 16 ensures that only the mean and standard deviation are perturbed as μ x t μ y t f t and σ x t 2 h t 2 σ y t 2 after removing z t from both the mean and standard deviation while the conditional skewness is preserved i e γ x t γ y t refer to appendix b the above eqs 10 16 are applicable for any form of z t in the mean f t and or the standard deviation g t in the proposed decomposition procedure the deterministic component z t is a function of time that captures the available knowledge of the nonstationarity the z t is often expressed in a parsimonious mathematical form determined by exploratory statistical analysis hence in this paper the z t was determined as a linear function of time and parameterized using the non parametric sen s slope estimator for estimating the trend in the μ y t and the sen s slope estimator coupled with the moving window technique for estimating the trend in the σ y t the hyperparameters of the moving window method namely the window length and step were set to 10 and 5 years as they were found optimal in terms of bias among the ranges of window length from 10 to 50 years and window step from 1 year to the window length for the synthetic nonstationary gev and pe3 datasets appendix c note that in the proposed decomposition procedure the specification of z t is not limited to the methods mentioned above but can be further tailored whenever a more elaborate characterization of the nonstationarity e g using a nonlinear function is available 2 4 2 determination of the ns ffa model the distribution was selected based on the stationary component i e x t specifically the distribution was selected from the pool of candidate distributions using the l moment ratio diagram hosking 1990 which has been commonly used and acknowledged to be robust in the s ffa e g nguyen et al 2017 ouarda and charron 2019 papalexiou and koutsoyiannis 2013 the difference between the l kurtosis of the sample and the candidate theoretical distributions which is referred to as l kurtosis discrepancy was used to select the optimal distribution hosking and wallis 1997 the nonstationary structure of the ns ffa model was identified according to the distribution moment equations e g see stedinger et al 1993 or gado and van nguyen 2016a based on z t for instance a trend in the mean would be properly captured by a model with a time dependent location parameter while a trend in the standard deviation would be expressed by both the time dependent location and scale parameters for the three candidate distributions as linear trend s were considered in this paper the nonstationary structures considered are m 1 0 0 d or m 1 1 0 d where d is the distribution in table 1 2 5 simulation study a simulation study was conducted to demonstrate the advancements of the proposed decomposition method over the method employed by cunderlik and burn 2003 referred to as the benchmark method herein the comparison between these methods relied on the relative bias of the statistical moments of the derived stationary stochastic component with respect to the true statistical moments known a priori the synthetic nonstationary datasets amss have stationary stochastic components following the gev and pe3 distributions and are called the nonstationary gev and pe3 datasets respectively three classes of nonstationary datasets were generated namely c1 with μ y t μ x t a t where μ x t 100 and a 0 4 c2 with σ y t σ x t b t where σ x t 45 and b 0 2 and c3 with μ y t μ x t a t and σ y t σ x t b t with the same a and b as c1 and c2 respectively the skewness of all datasets γ x t is time independent and is equal to 3 all these selected values are within their commonly reported ranges in several studies agilan and umamahesh 2017 ouarda et al 2018 papalexiou and koutsoyiannis 2013 sun et al 2015 villarini and smith 2010 the nonstationary gev datasets for each class were generated by randomly sampling eq 2 with the time dependent distribution parameters derived from the moment equations stedinger et al 1993 stedinger 2017 the nonstationary pe3 datasets were generated employing eq 4 and the designated time dependent moment s for each class the sample size of all the synthetic datasets was fixed to 100 which is often considered long in practice and is similar to the sample size of the real datasets utilized in this paper for each distribution and class 5 000 datasets were generated and thus a total of 30 000 datasets were employed in this simulation study to further investigate the proposed decomposition based approach in the ns ffa its performance was assessed and compared with the performance based approach the fitting efficiency and uncertainty of the models determined by both approaches were examined in terms of aic and cwi respectively these metrics were estimated in two different ways namely with respect to the empirical and the true quantiles which are derived from the sample datasets and the a priori known true nonstationary distribution respectively these two assessment settings are named the theoretical and empirical assessments herein the use of different references to calculate the performance metrics in the two assessment settings led to the differences in the aic and cwi the aic was employed as the selection criterion in the performance based approach for the comparison as it has been commonly used in the literature the number of nonstationary datasets considered for the simulation study was determined such that the estimates for all the assessments were stabilized 2 6 application on real datasets flow amss of thirteen stations retrieved from the environment and climate change canada eccc and the united states geological survey usgs were employed the exploratory analyses were performed to detect the presence of temporal trends and change points in these datasets which are indicative of the presence of nonstationarity the non parametric mann kendall mk test was employed to detect monotonic trends in the mean the mk test was coupled with the moving window method with the window length of 10 and the window step of 5 to investigate the temporal trends in the standard deviation higher order statistical moments were not investigated due to their high sensitivity to extreme events limited sample sizes and consequently the difficulty in their estimation griffis and stedinger 2009 papalexiou and koutsoyiannis 2013 the non parametric mann kendall sneyers mks and mann whitney pettitt mwp tests were employed to detect the presence of change point s in the mean the detection of change points in the standard deviation was not conducted due to the limited sample sizes a significance level of 0 05 was used in all these tests the perceived nonstationarity in these datasets exhibits different patterns in the form of significant trends and or change points based upon the presence or absence of significant trends in the mean standard deviation and or change points the amss were grouped into the c1 c2 and c3 plus an additional class class 4 c4 more complex patterns containing a significant change point no change points were detected in the datasets of c1 c2 and c3 the classification of the datasets as well as their corresponding ids are presented in fig 1 among the thirteen datasets several of them including c1 1 c3 1 c4 1 c4 2 c4 3 and c4 4 have been also used in previous studies on the ns ffa e g ammar et al 2020 luke et al 2017 obeysekera and salas 2014 salas and obeysekera 2014 villarini et al 2009 the performance assessment of both approaches was also conducted in terms of aic and cwi in this case only the empirical assessment was undertaken because the true quantiles are unknown in real practice 3 results and discussion 3 1 simulation study 3 1 1 advancements of the proposed decomposition procedure fig 2 shows the relative bias of the proposed and benchmark methods for the synthetic datasets of c1 the two methods yielded the same estimates of μ x t σ x t and γ x t as they use the same procedure to remove the z t in μ y t in contrast the improvements of the proposed method over the benchmark method are apparent for the datasets of c2 and c3 the medians of the estimates of μ x t σ x t and γ x t of the proposed method are unbiased or close to unbiased close to zero in particular the proposed method considerably reduced the relative bias in the estimate of γ x t which describes the right tail of the distribution and consequently is particularly important in the distribution selection in addition the proposed method offered a narrower or approximately the same interquartile range of the estimates of μ x t σ x t and γ x t similarly the full ranges of the relative bias in the proposed method are narrower than or approximately equal to those in the benchmark method except for the estimates of σ x t for the nonstationary pe3 datasets these results demonstrate that the proposed method can not only reduce the biases in the estimates especially of higher order moments but also is more robust compared to the benchmark method consequently the proposed method would improve the distribution selection using the derived stationary stochastic component and in turn further advance the decomposition based ns ffa 3 1 2 performance of the proposed decomposition based ns ffa to further assess the proposed decomposition based approach its performance was compared with that of the performance based approach as shown in fig 3 the decomposition based approach yielded consistently lower medians and first and third quartiles of aics in all the nonstationarity classes and both parent distributions in the theoretical assessment i e with respect to the true quantiles in contrast the performance based approach produced lower aics medians first and third quartiles and minimum and maximum values throughout all the datasets in the empirical assessment i e with respect to the empirical quantiles in terms of the cwi the decomposition based approach outperformed the performance based approach in the theoretical assessment as it yielded lower medians and first and third quartiles whereas both approaches performed equivalently in the empirical assessment moreover the decomposition based approach was always superior to the performance based approach in capturing the true nonstationary structure and distribution as well as the true ns ffa model the combination of the two model components fig 4 in the simulation study the decomposition based approach always selected the true nonstationary structure as the a priori known temporal trends were directly employed to determine the nonstationary structure whereas the true distribution and thus the ns ffa models were selected on average in 56 7 of the datasets within the range of 45 6 to 66 7 the incorrect selections might be ascribed to the sample l ratios falling approximately in the middle of two theoretical distributions for several datasets whereas the performance based approach selected the true distribution and nonstationary structure on average in 41 4 within the range of 32 6 to 48 9 and 17 6 within the range of 5 7 to 26 4 of the datasets respectively resulting in the selection of true models on average in only 7 8 within the range of 3 1 to 10 5 of the datasets these results reveal that the two ns ffa approaches often yielded different models namely different distributions and or nonstationary structures and in turn different performance in the performance based ns ffa the ergodicity required for making the statistical inference is completely violated as both the distribution and the nonstationary structure are selected simultaneously by optimally fitting the nonstationary datasets thus the performance based approach may result in a ns ffa model that does not capture the underlying nonstationary stochastic process well despite it might fit the observations adequately according to the pre selected performance metric the misspecification of the distribution and or nonstationary structure can introduce errors in the quantile estimates in the fitting period and could be particularly critical when implementing the model for the out of sample predictions in contrast the decomposition based ns ffa avoids the ergodicity violation as the distribution is selected from the decomposed stationary stochastic component in which the ergodicity holds while the nonstationary structure is determined separately according to the available knowledge of the nonstationarity this fact makes the decomposition based approach especially advantageous over the performance based approach from the theoretical point of view in addition the simulation results demonstrate that the performance based approach outperforms the decomposition based approach in the empirical assessment although it is inferior in the theoretical assessment this argues that the performance based approach tends to select untrue models due to overfitting the samples irrespective of the actual nonstationary stochastic process in contrast the decomposition based approach is superior in capturing the underlying true nonstationary process as reflected by its higher percentage in selecting correct ns ffa models as well as its higher fitting efficiency and the same or less uncertainty than the performance based approach in the theoretical assessment therefore both the theory and the simulation results advocate that the decomposition based ns ffa is preferable in addition the comparison of these two approaches was also conducted when the bic instead of the aic was used as the selection metric in the performance based approach the results appendix d consistently supported the aforementioned findings 3 2 real applications 3 2 1 implementation of the proposed decomposition based approach fig 5 a c show one example of decomposed datasets of classes 1 2 and 3 respectively the proposed decomposition procedure effectively removed the significant monotonic trends in the mean and or the standard deviation no significant trends nor change points were detected in the decomposed datasets these are indicative that the decomposed datasets are stationary since the presence of change points was not considered in the ns ffa the perceived monotonic trends over the whole observation period were utilized for the decomposition of datasets in class 4 it is thus not surprising that the change point was still detected in two out of four decomposed datasets of this class i e c4 3 and c4 4 fig 5 d and e illustrate the decomposed datasets c4 2 without a change point and c4 3 still with a change point as examples respectively this issue is also present in the performance based approach in which no candidate models considering change points were adopted in this paper and most of the literature due to the highly uncertain model parameterization as a result the proposed decomposition procedure effectively removed the perceived z t in datasets in classes 1 2 and 3 and consequently the approaches commonly employed to select the distribution in the s ffa e g the l moment ratio diagram are applicable table 2 shows the l kurtosis discrepancies between each decomposed dataset and the candidate theoretical distributions and highlights the best fit distributions note that in some cases e g c1 1 and c1 2 the difference in the l kurtosis discrepancy between the best fit and the second best distributions is very minor 0 005 this situation is also common in the s ffa when the dataset s l moment ratio locates approximately in the middle of two candidate theoretical distributions in the l ratio diagram when more than one distribution fits acceptably well the data any of them could be a reasonable choice hosking and wallis 1997 although the distribution with slightly better fitting is usually selected e g cunderlik and burn 2003 ouarda et al 2019 further investigation using additional statistical tests could be conducted if desired however this would merely be an additional effort to privilege certain aspects of interest of the fitting as the true distribution is unknown rather than an issue of statistical inference table 2 also shows the selected nonstationary structures for each dataset according to the detected temporal trends in y t or its corresponding log transformation y t in several datasets i e c2 3 c3 1 c3 2 c3 3 and c4 2 the trends in the standard deviation of y t were no longer significant in y t the possible absence of a trend in the standard deviation due to the log transformation and consequently the use of a time invariant scale parameter in the lpe3 distribution have been discussed in luke et al 2017 and stedinger and griffis 2011 therefore when only a trend in the standard deviation is present the log transformation may lead to determining a stationary structure for the lpe3 distribution and consequently might be inadequate for conducting the ns ffa sometimes furthermore neglecting complex nonstationary patterns e g datasets in class 4 could yield misleading results in the analysis for instance a stationary model m s p e 3 was determined for c4 4 irrespective of the perceived nonstationarity in the dataset table 2 nevertheless the confident identification of complex nonstationary patterns is difficult due to the sample size of amss and the potential noise of natural climate variability occurring on timescales of up to 30 years françois et al 2019 this along with the high uncertainty in model parameterization constrains the use of complex nonstationary structures in the ns ffa in these applications the monotonic temporal trends were used as a proxy for z t despite that theoretically the violation of ergodicity is not completely avoided when using the perceived temporal trends only serinaldi et al 2018 this can be resolved by incorporating deductive reasoning and or the physical causative process es or driver s into the analysis whenever they can be properly identified case specific investigations in this regard are beyond the scope of this paper furthermore the determination of the nonstationary structure and the derivation of the stationary component in the proposed decomposition based approach depends on the methods used to identify z t although statistical tests such as the mk test can suggest the temporal dependence of the statistical moments and subsequently the corresponding distribution parameter s they do not discriminate if the dependence is linear or nonlinear this paper represented the nonstationarity using linear trends this is practically feasible and convenient considering the limitations to characterize z t in a higher degree of detail an appropriate and systematic delineation and attribution of z t is key to reliably implementing the proposed decomposition based ns ffa 3 2 2 comparison of the proposed decomposition based and performance based approaches figs 6 and 7 show the ns ffa models determined by the proposed decomposition based approach with the black box and selected from the candidate nonstationary models by the performance based approach with the numerical index 1 in terms of aic and cwi respectively for each dataset note that both the aic and cwi are normalized into the range of 0 1 for each dataset the top second and third ns ffa models selected by the performance based approach are also indicated with the indexes 2 and 3 respectively the corresponding stationary model counterparts m s gev m s p e 3 and m s l p e 3 are also included in these figures as the nonlinear nonstationary structures were considered in the performance based approach table 1 but not in the decomposition based approach the comparison of the selected models determined by the two approaches was conducted in a broad sense namely comparing if the models have the same distribution and the same time variant location parameter or time variant location and scale parameters in the nonstationary structure as shown in figs 6 and 7 these two approaches generally yielded different ns ffa models except in c1 1 and c1 2 in terms of aic and c3 2 and c3 3 in terms of cwi on one hand they selected different distributions in five c2 1 c2 2 c2 3 c3 1 and c3 3 and four c2 1 c2 2 c2 3 and c3 1 datasets in classes 1 3 based on the aic and cwi respectively and all datasets of class 4 except c4 4 based on the aic on the other hand they also determined different nonstationary structures in five c1 3 c2 1 c2 2 c2 3 and c3 2 and five c1 1 c1 2 c1 3 c2 1 and c2 2 datasets in classes 1 3 in terms of aic and cwi respectively and half of the datasets of class 4 c4 1 and c4 4 in terms of aic and c4 3 and c4 4 in terms of cwi recall that different nonstationary structures were identified for y t and y t in some cases due to the log transformation these results evidence that these two ns ffa approaches are fundamentally different in determining the model as they are formulated following distinct logics moreover fig 8 shows the relative discrepancies in aic and cwi of the model determined by the proposed decomposition based ns ffa with respect to that by the performance based ns ffa for all datasets as shown in fig 8 the relative discrepancy in aic ranges from zero to moderate 0 30 in six out of nine datasets of classes 1 3 whereas it is high 0 30 in the other three datasets i e c1 3 c2 3 and c3 1 among these three datasets the high discrepancy in c1 3 and c3 1 could be ascribed to the small aics close to zero of the models selected by the performance based ns ffa in the datasets of class 4 the discrepancy is high in two cases c4 1 and c4 2 the discrepancy in cwi is in the range from zero to moderate in the datasets of classes 1 3 overall except in c2 1 and c2 2 while it is high in two datasets of class 4 c4 1 and c4 4 this comparison does not intend to demonstrate which approach outperformed the other the two approaches often yield different ns ffa models and consequently their different performance is expected on one hand the comparison might imply the possible overfitting by the performance based approach as demonstrated in the simulation study on the other hand the zero to moderate discrepancies argue that the decomposition based approach is in general competitive to the performance based approach from the perspective of the performance therefore these results advocate the feasibility and applicability of the decomposition based approach in real practice if using more flexible e g nonlinear nonstationary structures in the decomposition based ns ffa its performance could be further enhanced in some cases for instance the discrepancy in aic can be reduced e g c2 2 c3 1 c3 2 and c3 3 or even eliminated e g c1 1 and c1 2 when a non linear nonstationary structure is adopted fig 6 however the use of a flexible and complicated nonstationary structure without justification could lead to the overfitting problem especially in data scarce locations using complicated nonstationary structures in the decomposition based ns ffa is mathematically feasible but requires understanding the physical mechanism s driver s behind the nonstationarity to avoid overfitting when they cannot be properly justified the use of a parsimonious nonstationary structure i e the linear nonstationary structure might be preferred 3 2 3 the challenges in implementing the performance based ns ffa as discussed previously in the simulation study the performance based ns ffa cannot avoid the violation of ergodicity and may lead to spuriously high performance in this approach there are two additional issues that deserve attention as shown in figs 6 and 7 when two different distributions are coupled with their corresponding most fitting efficient nonstationary structures which are often different they may yield approximately equal performance for instance for c1 1 the optimal model m 2 0 0 l p e 3 is marginally superior to m 1 1 0 p e 3 by 2 3 in aic while for c3 1 the optimal model m e 1 1 0 gev outperforms m 2 0 0 p e 3 by 5 4 fig 6 hence a different nonstationary structure may be selected when selecting a different distribution which in turn yields a completely distinct model in the performance based ns ffa this is analog to the equifinality in hydrological modeling namely different model structures may be equally capable of fitting the observations according to a certain or a few pre selected performance metric s these suggest that the selection of the nonstationary structure needs to be separated from the selection of the distribution in order to explicitly use the knowledge of the nonstationarity to guide the model determination as in the decomposition based ns ffa the other challenge lies in the selection of the evaluation criterion to determine the optimal model in the performance based approach when using different evaluation metrics to assess the models performance from a specific perspective such as the fitting efficiency e g using the aic and bic the results were consistent in general appendix d however the observed differences reveal that they might lead to determining different ns ffa models sometimes when assessing the models performance from different perspectives such as the fitting efficiency and uncertainty the results shown in figs 6 and 7 also reaffirm that different models may be selected by this approach when using different evaluation criteria the aic and cwi here when using the aic this approach always led to the selection of a nonstationary model however it is not uncommon that the stationary models yield the lowest uncertainty when a temporal trend is present in either mean or standard deviation e g c1 2 c1 3 and c2 2 this result supports the argument that the stationary model might outperform the nonstationary model from the uncertainty viewpoint even when the nonstationarity presents ouarda et al 2019 vidrio sahagún et al 2021 therefore the optimal model selected by the performance based approach is sensitive to the evaluation criterion used the model has been commonly selected according to the fitting efficiency criterion in the performance based ns ffa however the uncertainty is also a key when selecting the model in the ffa as it reflects the reliability of estimates and their applicability for design purposes serinaldi and kilsby 2015 hence the lack of agreement on which selection criterion should be employed challenges the implementation of the performance based approach as it would heavily depend on the performance criterion selected according to the preference of the modelers 3 3 future research recommendations in this paper the proposed decomposition method for the decomposition based ns ffa was theoretically justified and practically demonstrated through both the simulation study and practical applications using the datasets in c1 c2 and c3 yet more challenges arise when dealing with more complex nonstationary patterns in the ns ffa such as those in c4 containing change point s fig 1 and or non linear patterns mathematically both the performance based and decomposition based approaches are capable of handling any form of nonstationarity i e linear and non linear and or with change point s however the performance based method is challenged in practice when incorporating more complicated nonstationary structures as their use may be difficult to justify and lead to overfitting as demonstrated in the simulation study whereas the decomposition based approach is challenged to identify a more complex deterministic component behind the nonstationarity to decompose the underlying dataset to present it is difficult to reliably characterize complicated nonstationary patterns unless clear mechanisms behind the nonstationarity can be identified thus further research on nonstationarity characterization and attribution is required to ensure the successful practical implementation of the ns ffa when more complex nonstationary patterns present in both the decomposition based and performance based approaches the time was used as the covariate however it is worth mentioning that both approaches can employ the physical covariate s e g climatic indexes and land use cover parameters among others whenever they and or their deterministic law of time governing their evolution can be properly identified this would be achieved by the attribution of the nonstationarity to its physical driver s causative process es e g climate change and or changes in the watershed resulting from human intervention hence the proposed method as well as other ns ffa approaches e g the performance based approach would benefit from the improvement in the nonstationarity attribution which is beyond the scope of this paper in the decomposition based approach this would be particularly relevant to the identification of the physical driver s behind the nonstationarity and thus z t recall that the existence of a deterministic component z t behind the observed changes in the dataset is necessary to justify the implementation of the ns ffa in general this requirement does not imply the need for perfect knowledge of z t but can be validated by deductive reasoning serinaldi et al 2018 the nonstationary driver s can yield a z t enveloped by the variability due to chaotic nonlinear internal dynamics and unpredictable natural external forcings milly et al 2015 montanari and koutsoyiannis 2014 hence the proposed decomposition based approach is also applicable for physical drivers showing a deterministic signal of change accompanied by stochastic variability such as climate variables affected by climate change the opportunity to explicitly incorporate physical knowledge of the deterministic changes in the system into the model development and the subsequent inference is an attractive feature of the decomposition based approach it is noteworthy that uncertainty originates from distinct sources in different ns ffa approaches differing from the performance based approach the decomposition based method avoids the uncertainty in the model selection due to equifinal models according to the performance metric s whereas uncertainty arises from the specifications of the deterministic component and the distribution selection as in the s ffa in this approach thus improvements in these two aspects would further reduce the uncertainty in the model determination of the decomposition based approach lastly the decomposition based ns ffa might confront several practical challenges when making the out of sample predictions when implementing the performance based approach it can yield such predictions by a directly fitting projected floods b extrapolating the ns ffa models based on the temporal covariate and c extrapolating the models based on the physical covariate s and its projections these implementations would thus rely on the adequate projections of floods the validity of the assumption that the temporal trend detected in the historical period remains the same in the future and the reliable projections of the physical covariate s and the validity of its link function with the distribution parameters in the future respectively as similar strategies can be implemented when using the decomposition based ns ffa to make the out of sample predictions the reliability of this approach is also subject to similar practical challenges as the performance based approach 4 conclusion aiming to advance the ns ffa this paper proposed a novel decomposition procedure for determining the model in the decomposition based approach in which the distribution and the nonstationary structure are selected separately based upon the stationary stochastic component and the available knowledge of the nonstationarity the proposed method which strictly follows the theoretical decomposition of nonstationary stochastic processes was compared with a previous decomposition method in both an analytical deduction and a simulation study the analytical deduction demonstrated the theoretical advancement of the proposed method while the simulation results confirmed its effectiveness in reducing the bias in the moment estimates of the derived stationary stochastic component especially in the skewness that is key in the distribution selection these demonstrated the improvement in the decomposition based ns ffa moreover the proposed decomposition based approach was further compared with the performance based approach using both the synthetic and real flow amss exhibiting different patterns of nonstationarity on one hand the proposed decomposition based ns ffa was shown superior to the performance based approach as it captured the known underlying nonstationary process more efficiently and yielded a higher performance in the simulation on the other hand the performance based approach was found prone to overfitting the synthetic datasets while the zero to moderate discrepancies between the performance of the two approaches argued that the decomposition based approach is competitive in general from the performance perspective in real applications in addition the performance based approach was shown to be subject to the equifinality issue as well as the determination of distinct models when using different performance evaluation metrics furthermore differing from the performance based approach the decomposition based ns ffa avoided alleviated the ergodicity violation and was thus preferable from the theoretical viewpoint therefore this paper demonstrated the superiority and convenience of the proposed decomposition based ns ffa credit authorship contribution statement cuauhtémoc tonatiuh vidrio sahagún conceptualization methodology formal analysis investigation writing original draft visualization jianxun he supervision writing review editing resources funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the first author of this paper is funded by a doctoral scholarship from the national council for science and technology of mexico conacyt and the universidad de guadalajara this work is also partially funded by the discovery grant of natural sciences and engineering research council held by the second author appendix a benchmark detrending procedure cunderlik and burn 2003 the detrending methodology applied in cunderlik and burn 2003 was taken as the benchmark to assess the decomposition procedure proposed in this paper the benchmark method deals with the weak second order nonstationarity i e the temporal variability of the first two statistical moments of the underlying stochastic process while assuming the temporal invariance of the higher order moments e g the skewness lindgren et al 2013 von storch and zwiers 2002 in this procedure the y t is considered in the general form of a1 y t τ t ε t where τ t is the deterministic time dependent component in the mean and ε t is the residual time dependent stochastic component describing the irregular fluctuations around the τ t in practice the τ t is determined as a linear function of time i e τ t τ 0 τ 1 t where τ 0 and τ 1 are the regression coefficients using the non parametric sen s estimator hence the ε t which is the de trended series in the mean is given by a2 ε t y t τ t the ε t would contain the trend in the variability of y t the trend in the variability would then be approximated using the transformed series ε t which captures the change of ε t over time and is calculated by a3 ε t ε t ε where ε is the average of ε t and is time invariant the deterministic trend in the transformed series ε t which is denoted as φ t is then estimated similar to τ t the φ t is determined as a linear function of time i e φ t φ 0 φ 1 t where φ 0 and φ 1 are the regression coefficients using the sen s estimator the φ t is removed from ε t by a4 y t st ε t φ t ε t ε ε t φ t ε t ε f o r i n c r e a s i n g φ t ε t φ t ε t ε ε t φ t ε t ε f o r d e c r e a s i n g φ t where y t st y t 1 st y t 2 st y t n st is the resulting second order stationary series the y t st derived by the benchmark procedure is free from trends in the first two statistical moments and has been used to select and or parameterize the distribution e g cunderlik and burn 2003 gado and nguyen 2016 zhang et al 2020 for datasets in c1 presenting a temporal trend in the mean only the benchmark and the proposed methods coincide as the temporal invariant ε t reduces eq a 1 to an equivalent form of eq 10 in which g is the identity operator thus eqs 12 14 are also applicable for the benchmark method and thus the standard deviation and the skewness of the detrend time series are preserved in the benchmark method however the benchmark method does not preserve the stochastic component for datasets in c2 and c3 let λ t be the intermediate deterministic variable defined as a5 λ t φ t ε t ε φ t ε t ε f o r i n c r e a s i n g φ t φ t ε t ε φ t ε t ε f o r d e c r e a s i n g φ t this allows to express the derived stationary second order stationary series as a6 y t st y t τ t λ t for datasets in c2 presenting a temporal trend in the standard deviation only as τ t 0 t eq a 6 reduces to y t st y t λ t therefore by denoting the deterministic true yet unknown counterparts of τ t and λ t by their uppercase letters i e t t and λ t it can be shown that the benchmark method simply shifts the y t in its first moment μ y t without actually detrending its second moment σ y t 2 a7 μ y t st e y t st e y t λ t e y t e λ t μ y t λ t a8 σ y t st 2 e y t st e y t st 2 e y t λ t e y t λ t 2 e y t λ t μ y t λ t 2 e y t μ y t 2 σ y t 2 a9 γ y t st e y t st e y t st 3 e y t st e y t st 2 3 2 e y t λ t e y t λ t 3 e y t λ t e y t λ t 2 3 2 e y t λ t μ y t λ t 3 e y t λ t μ y t λ t 2 3 2 e y t e y t 3 e y t e y t 2 3 2 γ y t thus the effects of this procedure on the true nonstationary distribution can be thought of as irregular displacements of the marginal distributions of y t controlled by λ t without changing their width σ y t st 2 σ y t 2 and shape γ y t st γ y t as a result the removal of the trend in the variability of y t is ascribed to such displacements and thus this method fails in removing the actual deterministic component behind the nonstationarity and preserving the underlying stochastic component consequently this procedure would introduce errors in the estimates of the statistical moments of the stochastic component for instance the skewness would not be preserved unless the variability of y t is symmetrical this condition is intuitive as λ t φ t is derived from the ε t which takes the absolute deviations from the time invariant mean eq a 3 and thus ignores the signs of ε t ε reflecting the average variability only the even removal of the average deviations ignoring whether the series of ε t is skewed towards the high or low values would alter the proportionality of the variability ranges above and below the mean and in turn distort the asymmetricity properties of the underlying dataset for example if ε t is skewed towards the high values the computed λ t would remove the average deviations around the mean and over under detrend the variability for below above the mean respectively for datasets in c3 presenting temporal trends in both the mean and standard deviation the eq a 6 is directly used therefore it can be shown that the benchmark method also shifts the y t in its first moment without actually detrending its second moment and thus does not preserve the stochastic component either 10 μ y t st e y t st e y t t t λ t e y t e t t e λ t μ y t t t λ t 11 σ y t st 2 e y t st e y t st 2 e y t t t λ t e y t t t λ t 2 e y t t t λ t μ y t t t λ t 2 e y t μ y t 2 σ y t 2 12 γ y t st e y t st e y t st 3 e y t st e y t st 2 3 2 e y t t t λ t e y t t t λ t 3 e y t t t λ t e y t t t λ t 2 3 2 e y t t t λ t μ y t t t λ t 3 e y t t t λ t μ y t t t λ t 2 3 2 e y t e y t 3 e y t e y t 2 3 2 γ y t similar to c2 the skewness of datasets in c3 would only be preserved in the special case of symmetrical variability appendix b theoretical preservation of statistical moments of the stochastic component derived in the proposed decomposition procedure for datasets in c2 eq 15 ensures that only the standard deviation is perturbed after detrending the standard deviation while both the conditional mean and the skewness are preserved b1 μ x t e x t e μ y t y t μ y t h t e e y t y t e y t h t e y t h t e y t e y t e y t μ y t b2 σ x t 2 e x t μ x t 2 e μ y t y t μ y t h t μ y t 2 e y t μ y t h t 2 h t 2 e y t μ y t 2 h t 2 σ y t 2 b3 γ x t e x t μ x t 3 e x t μ x t 2 3 2 e μ y t y t μ y t h t μ y t 3 e μ y t y t μ y t h t μ y t 2 3 2 e y t μ y t h t 3 e y t μ y t h t 2 3 2 h t 3 e y t μ y t 3 h t 3 e y t μ y t 2 3 2 e y t μ y t 3 e y t μ y t 2 3 2 γ y t for datasets in c3 eq 16 ensures that only the mean and standard deviation are perturbed after detrending both the mean and standard deviation while the conditional skewness is preserved b4 μ x t e x t e μ y t f t y t f t μ y t f t h t e e y t f t y t f t e y t f t h t e y t f t h t e y t f t e y t f t e y t f t h t e y t e y t e y t f t μ y t f t b5 σ x t 2 e x t μ x t 2 e μ y t f t y t f t μ y t f t h t μ y t f t 2 e e y t f t y t f t e y t f t h t e y t f t 2 e e y t f t y t f t e y t f t h t e y t f t 2 e y t e y t h t 2 h t 2 e y t e y t 2 h t 2 σ y t 2 b6 γ x t e x t μ x t 3 e x t μ x t 2 3 2 e μ y t f t y t f t μ y t f t h t μ y t f t 3 e μ y t f t y t f t μ y t f t h t μ y t f t 2 3 2 e e y t f t y t f t e y t f t h t e y t f t 3 e e y t f t y t f t e y t f t h t e y t f t 2 3 2 e e y t f t y t f t e y t f t h t e y t f t 3 e e y t f t y t f t e y t f t h t e y t f t 2 3 2 e y t e y t h t 3 e y t e y t h t 2 3 2 h t 3 e y t μ y t 3 h t 2 e y t μ y t 2 3 2 e y t μ t 3 e y t μ y t 2 3 2 γ y t appendix c hyperparameter determination of the moving window method the influence of the window length and step in the moving window method which was coupled with the sen s slope estimator for estimating the trend in the σ y t was investigated since the window length and step only influence the estimation of the trend in the σ y t their selections affect the analysis of datasets of c2 and c3 as the synthetic nonstationary datasets of c2 present a trend in the σ y t only they were used for the investigation herein the assessment was conducted based on the mean absolute relative bias marb of the moment estimates the marb was estimated from the absolute bias values of each statistical moment of all the synthetic datasets for each combination of window length and step of a given parent distribution i e gev or pe3 fig c1 shows the marb of the moment estimates when using window lengths ranging from 10 to 50 years and window steps from 1 year to the window length for the nonstationary gev and pe3 datasets respectively as shown in this figure the marbs of μ x t σ x t and γ x t are in general higher when using the larger window steps for any window length it is also worth mentioning that the marbs slightly increase with the decrease of the window step when using small window steps especially for σ x t and γ x t these results are not surprising as for a given window length the large window step leads to the small sample size of the derived σ y t series and thus deteriorates the estimation of the trend whereas the small window step leads to the large window overlapping and thus the datapoints located in the center of the time series would be counted more times than those located at the bounds and have large influence on the estimates furthermore an assessment incorporating the biases of all the three estimated statistical moments μ x t σ x t and γ x t was conducted for each parent distribution to this end the marbs of μ x t σ x t and γ x t were averaged and then used to identify the optimal hyperparameters fig c2 presents the average of the marbs of μ x t σ x t and γ x t and highlights the minimum average of the marbs corresponding to the optimal window step for each window length these results indicate that the lowest average of the marbs is yielded when using the window length of 10 years and the window step of 5 years the selected hyperparameters are rational as they yield a long series of σ y t for estimating the trend and also avoid substantial bias of the datapoints located in the middle of the time series besides the window length would also skip potential decadal variability therefore these hyperparameters of the moving window method were adopted throughout this paper appendix d results of the performance based approach when using the bic as the selection criterion the potentially different results of the performance based approach due to using another evaluation metric of fitting efficiency were explored to this end the bic was employed in both the simulation study and real applications and was calculated by d1 bic n log r m s e n θ log n in the simulation study similar results were obtained when using the bic and aic as the selection metrics in the performance based approach as shown in figs d1 and 3 the use of bic or aic yielded practically equivalent results in both empirical and theoretical assessments when using the bic the performance based approach selected the true distribution and nonstationary structure on average in 41 6 within the range of 32 8 to 48 4 and 17 8 within the range of 10 0 to 23 8 of the datasets respectively resulting in the selection of true models on average in only 7 1 within the range of 3 7 to 9 0 of the datasets fig d2 compared to the results when using the aic as the selection metric fig 4 the changes in the results of the performance based approach were practically negligible thus the results from the performance based approach when using the bic as the selection metric also support that the performance based approach tended to select untrue models and overfit the samples compared to the decomposition based approach fig d3 shows the ns ffa models determined using the bic as the selection metric in the performance based approach in real applications as shown in this figure the results based on the bic were in general consistent with those based on the aic fig 6 comparing the results based on the aic and bic the use of different evaluation metrics led to selecting a different optimal model in only one dataset c3 1 while the differences in at least one of the top three models were observed from five out of thirteen datasets c1 3 c2 3 c3 1 c3 2 and c4 4 in addition the relative discrepancies in the bic of the model determined by the proposed decomposition based ns ffa with respect to that by the performance based ns ffa fig d4 were similar to previous results fig 8 thus the changes in the performance based approach were minor in general and the results based on the bic also supported the conclusions drawn based on the results obtained using the aic both the aic and bic are used to measure the fitting efficiency but employ a different weighting of the goodness of fit and the complexity of the model length of data despite the similar results in general the observed differences between the results based on the aic and bic demonstrated that the use of different selection metrics might lead to different results in the performance based approach these results further argue the issue on the selection of the evaluation criteria in the performance based approach 
3084,large scale water infrastructure serves multiple purposes including provisioning of freshwater protection from floods navigation hydro electricity etc decision analysis for reservoir systems relies heavily on optimization techniques that identify optimal operational strategies using a dynamic systems model optimization requires the analyst to define performance indicators or objective functions that aggregate performance across multiple time periods in a planning horizon a question thus arises does the temporal resolution at which objective functions are aggregated have a substantial impact on resultant optimal operational strategy here we investigate this issue using three formulations of a multi objective optimization problem for a proposed inter basin water transfer in southern india the borg multi objective evolutionary algorithm moea is used to optimize five objectives the reliability resilience and vulnerability of demand satisfaction reliability of maintaining minimum environmental flows and reliability of preventing high flow exceedances downstream we alter the temporal resolution at which the reliability of demand satisfaction is calculated by aggregating water deficits at fortnightly seasonal and annual scale resulting in three formulations of the decision problem pareto approximate strategies resulting from the fortnightly seasonal and annual reliability formulations result in annual demand deficits ranging from 82 to 172 mm3 79 152 mm3 and 61 96 mm3 respectively this improvement in performance for annual reliability based strategies exploits greater flexibility within the systems model to store water temporarily in secondary storage structures like lakes ponds check dams in the command area serviced by the project we also find that the choice of temporal resolution of objective function has a substantial influence on the combination operators preferred during the search phase of the borg moea our results highlight the value of exploring objective functions at different temporal resolutions for reservoir optimization keywords reservoir management multi objective optimization reliability time scale objective function evolutionary algorithms borg moea inter basin water transfers data availability data will be made available on request 1 introduction surface water storage reservoirs are an important infrastructural intervention to guard against water scarcity and flood risks this is evident as more than 16 million reservoirs with surface area greater than 100 m2 exist across the globe lehner et al 2011 reservoirs also function as a hydro electricity source enable navigation may be used for fish culture etc while requiring to maintain minimum environmental flows thus multi reservoir systems often service various water sectors and sometimes multiple basins this poses a challenging decision context as the problems envisage several objective functions to represent different stakeholders matrosov et al 2015 do et al 2020 trindade et al 2017 herman et al 2014 zeff et al 2016 amaranto et al 2022 moallemi et al 2020 huskova et al 2016 multi objective optimization moo using evolutionary algorithms is often used to address such complex decision problems affecting multiple actors and sectors gao et al 2022 salazar et al 2017 gold et al 2019 salazar et al 2016 giuliani et al 2016 moo allows decision makers to explore the trade offs between potentially conflicting objectives and identify suitable compromises derepasko et al 2021a horne et al 2016 si et al 2019 gunantara 2018 zeff et al 2014 the application of moo requires specification of three main components of the decision problem i decision variables ii objective functions that quantify the performance requirements and iii a systems models that maps decision to consequences by simulating the objective function values from alternative decision variables lempert 2003 in addition quantification of relevant uncertainties in hydro climatic variables is also important for a realistic understanding of the impact of different decisions kasprzyk et al 2009 quinn et al 2017 quinn et al 2018 trindade et al 2017 gold et al 2019 bertoni et al 2019 herman et al 2014 cohen and herman 2021 giuliani et al 2014 watson and kasprzyk 2017 salazar et al 2017 optimized for hydropower revenue water supply reliability recreation and environmental flow requirement objectives for a multi purpose reservoir in the susquehanna river basin they show that moo can identify a range of possible compromise operational policies similarly quinn et al 2017 showed how stakeholder perception of trade offs can vary across different combinations of objective functions for a multi purpose reservoir system in red river basin in vietnam more recently veena et al 2021 explored multiple problem formulations of an inter basin water transfer considering a range of objectives across multiple water sectors and basins they showed that moo could identify compromise strategies that outperform previously proposed strategy by regional authorities by achieving higher demand satisfaction reliability with much lower volumes of annual transfers thus exploring multiple problem formulations of a decision problem have shown to yield decision relevant insights in several decision contexts quinn et al 2017 veena et al 2021 singh et al 2015 bertoni et al 2019 kasprzyk et al 2013 herman et al 2014 although specification of objective functions is known to play a central role in decision analysis for water management relatively few studies have explored the extent to which the temporal resolution of objective functions impact optimized decisions in reservoir operations objective functions aggregate output from a dynamic systems model that provides state information at a fine temporal resolution of hours to days for example the commonly used reliability objective translates a time series of success failure states to a single numeric value estimated as the fraction of time periods with success to total number of time periods in the planning horizon hashimoto et al 1982 zhang et al 2017 bayazit and ünal 1990 moy et al 1986 mondal and wasimi 2007 raje and mujumdar 2010 in a typical application the reliability is estimated at the temporal resolution of the system model although it can alternatively be estimated at any other temporal scale by estimating success failure states at that scale in their review of optimization tools for water resource management horne et al 2016 identify need to incorporate multiple temporal scales as a little unexplored research area similar findings are put forth by khan et al 2017 in their review of energy and water models for policy exploration there are some studies pointing to the value of exploring multiple temporal scales in objective function formulation gaudard et al 2018 brunner et al 2019 show that objectives calculated for longer time periods for drought management often need to be complemented with objectives for smaller time period like flood control to identify an adequate operational policy similarly georgakakos et al 2012 find it necessary to assess reservoir management policies explore objectives at different temporal resolutions for short term and long term water resources management they suggest that coarser resolution objectives are better to drive decisions related to drought management that consider water deficits across seasons or years similarly finer temporal resolution of objective functions are more relevant for evaluating performance goals related to flooding and hydropower that can be affected by sub daily variations in inflows in addition the interdisciplinary nature of water resource management problems also necessitates exploring multiple temporal scales of objective functions moss and newig 2010 moss and newig 2010 distinguish hydrological and political scales of management which are also subjected to choice of different spatial and temporal scales smith et al 2019 show that as cities in the united states grow water may be sourced from multiple geographic locations each with a different temporal periods of water rights as storage and streamflow diversion similarly institutional settings specify how much water farmers can use annually based on the water act of 1985 in spain guerrero baena et al 2019 on the other hand typical reservoir optimization setups evaluate demand satisfaction related objectives using model scale wang et al 2019 harou et al 2009 nourani et al 2020 hence legislative concerns may prioritize objectives at a temporal scale much different from model scale nearly all reservoir water balance studies optimize release decisions at a fine temporal resolution and use the demand at the current time step to decide on the releases required at that time step however stakeholder interactions have revealed that decision makers may be considering unmet demands at a coarse resolution to determine releases at the current time step for example during their stakeholder elicitation concerning reservoir operations of the cauvery river basin in southern india bhave et al 2018 found that reservoir operators considered annual demands to guide releases assuming that water users would resort to the use of secondary storage structures a large number of traditional secondary storage structures termed tanks are found throughout southern india and have been extensively used for irrigation and drinking water supply prior to modern water infrastructural interventions reddy et al 2018 van der zaag and gupta 2008 srivastava and chinnasamy 2021 similar considerations may be applicable to the nagarjuna sagar reservoir personal communications by first author this also makes intuitive sense as it allows water users also some flexibility in determining the crop sowing periods and make more optimal use of rainwater in a monsoon dominated variable rainfall regime recent developments in china also focus on conjunctively using secondary storage structures like lakes which are an important component of the water resources system wong et al 2017 wei et al 2020 fang et al 2019 currently reservoir simulation optimization models do not explicitly include these secondary storage structures within their operations hence a system scale model incorporating secondary storage within operational design of large reservoirs would be a more realistic representation for these regions concurrently exploring the consequences of formulations that determine release decisions based on cumulative seasonal or annual demands would also be of practical significance in these monsoon dominated agricultural regions these two facets are inherently connected as secondary storage structures are necessary for a systems model that allows conjunctive use of reservoir and secondary storage structures here we propose a such a modelling framework that focuses on managing large scale infrastructure utilizing secondary storage structures considering demands at varied time steps despite the likely benefit of exploring the impact of the temporal scale on optimized decisions there are only a few studies that focus on temporal scale and its implications for water resource management zhou et al 2017 hejazi and cai 2011 shiau and wu 2013 these studies defined multiple objectives at different temporal scales zhou et al 2017 used objective functions that span two temporal scales annual monthly and likely capture the multiple intended goals of the water transfers projects for the hanjiang river basin in china using a simulation optimization approach the goal was to identify the optimal water allocation among four water transfer projects for which water deficits transfers and hydropower generation are defined at an annual scale while water supply reliability resilience and vulnerability are defined at a monthly scale hejazi and cai 2011 show that objective functions estimated at coarse monthly temporal resolution would fail to mitigate flooding downstream and underestimate flood losses compared to those at fine weekly resolution studies show that improving the flexibility in optimization problem structure by including different temporal scales could help identify trade offs shiau and wu 2013 optimized water supply hydropower generation and environmental flow at five temporal scales for a multipurpose reservoir system in taiwan although these studies highlight the necessity of exploring multiple temporal scales of objective functions none so far have compared the optimized decisions obtained from different temporal resolutions of objective functions systematically and explored the associated dynamics of the multi objective evolutionary algorithms being employed recently derepasko et al 2021b reviewed the implications of different choices of temporal and spatial scale in optimization studies for water management they highlighted that the role of temporal resolution of objective functions on optimal solutions has not been specifically studied in the literature here we address this issue by using multiple formulations of an inter basin water transfer problem in southern india across the formulations the temporal scale of the reliability of demand satisfaction objective is varied with other objectives such as demand resilience demand vulnerability flood reliability and minimum environmental flow reliability at the same temporal scale we employ the borg moea to identify pareto approximate strategies that capture the trade offs between multiple objectives for each problem formulation the resultant water transfer strategies and resulting system performance are evaluated we further investigate the operator dynamics of the borg moea to assess whether there are differences among these formulations in terms of the search operators employed 2 the inchampalli nagarjuna sagar project the inchampalli nagarjuna sagar ins project proposes to transfer around 16 400 mm3 of water from the godavari river to the krishna river the two largest rivers of peninsular india fig 1 the project is motivated from the growing water stress on the recipient nagarjuna sagar reservoir due to upstream developments and rising water demands veena et al 2021 the reservoir services millions of farmers and the pharmaceutical hub of hyderabad on the other hand the proposed donor inchampalli reservoir in the godavari river basin will be drained by predominantly protected forest regions with relatively low water demands downstream the average annual inflows at the inchampalli site perur gauge station are more than the double that of the nagarjuna sagar against this backdrop the national water development agency nwda suggested monthly transfer volumes in a feasibility report for ins project using a water balance considering historical inflows and demands note also that both reservoirs are committed towards additional water transfers to other reservoirs the inchampalli reservoir is committed to transfer 4 370 mm3 annually to the pulichintala reservoir while the nagarjuna sagar is committed to transfer 14 200 mm3 annually to the somasila reservoir 2 1 data sources inflow data is obtained from central water commission for the donor reservoir for the time period 1967 2012 and irrigation and computer aided design department telangana for the recipient reservoir for the time period 1967 2016 details regarding the reservoir capacity and the maximum transfer capacity of the ins project are obtained from nwda 2021 demands in both the basins are primarily constituted by irrigation demands with smaller but substantial domestic and industrial water demands on the nagarjuna sagar veena et al 2021 we estimate irrigation demand from cropping patterns and command area which amounts to an annual demand of 7435 mm3 in the nagarjuna sagar that peaks in the months of september and october refer to fig 2 for details on command area irrigation demands in the command area of the inchampalli are much lower quantified as 603 mm3 using population estimates for major urban centres like city of hyderabad serviced the domestic demands for the nagarjuna sagar reservoir is estimated at 1000 mm3 3 methodology we identify water transfer strategies for the ins project using three problem formulations of a multi objective decision problem that differ in the temporal resolution of a water demand related objective fig 2 section 3 1 describes problem framing for the ins project detailing with the systems model uncertainty representation and decision variables section 3 2 details the objective functions used to evaluate alternative strategies along with details on the implementation of differing temporal resolutions that result in three problem formulations section 3 3 presents details on stochastic multi objective optimization to identify candidate operational strategies section 3 4 details the quantification of the inter relationships between decision variables using multi way correlation coefficient this information is used to understand their impact on search dynamics of the borg moea 3 1 problem framing for the ins project there are four main components to any decision problem decision variables performance measures that quantify the stakeholders objectives a model that maps decisions to objectives performance and exogenous uncertainties lempert 2003 the systems model is a dynamic water balance model that tracks the storage of donor and recipient reservoirs considering water transfers using eqs 1 and 2 veena et al 2021 1 st t d st t 1 d q t d e f t d o tr t d t r t d t d r t d 2 st t r st t 1 r q t r e f t r o t r t r t r t d t r r t r in eqs 2 and 3 st is reservoir storage q is the inflow to the reservoir ef is the release to satisfy minimum environmental flow requirements otr is the releases to other water transfers committed by either reservoirs tr is water transferred from the donor godavari to the recipient krishna d is water released for satisfying demands and r is the excess water released downstream subscript t refers to the time step and superscripts d and r represent donor and recipient reservoir fluxes respectively the model runs at 15 day time steps for a planning horizon of 15 years which represents a reasonable window where optimized decisions may be implemented before review and updating the storage values are in m3 while releases and transfers are in m3 15 days after accounting for all releases if the storage exceeds 95 of active storage capacity of the reservoir excess water is released downstream as r commitments related to water transfers are legislative requirements for satisfying their demands therefore they are prioritized over satisfying within basin demands veena et al 2021 we also prioritize environmental flows downstream of both reservoirs to consider a best case scenario w r t ecological requirements downstream it follows that water allocation priority order is minimum environmental flows other water transfers water transfer from donor to recipient reservoir and demand satisfaction for the ins project we specify decision variables as the volumes of water to be transferred from the donor godavari basin to recipient krishna basin every month we employ static open loop strategies that prescribe each decision in a time series as an independent decision and optimize them simultaneously thus the vector of decision variables θ is specified as 3 θ θ m m 1 2 12 0 θ 2800 the allocations of monthly transfers m is assumed to be fixed from year to year in a planning horizon the capacity of the proposed 299 km canal determines the upper limit of monthly transfers 3 1 1 quantification of inflow uncertainty the indian summer monsoon has a highly variable regime and can result in mean annual rainfall variations between 30 to 30 choudhury et al 2021 preethi et al 2019 consequently the inflow to reservoirs driven by the monsoonal cycle is also expected to vary annually considering this uncertainty during the optimization would be crucial to identify strategies that maintain acceptable performance across these stationary stochastic uncertainties to this end we generate multiple realizations of inflows to the donor and recipient site using a bootstrapping sampling technique developed by kirsch et al 2013 fig 2b each realization spans 15 years with a daily resolution these are then aggregated to 15 day resolution of the system model two sets containing 10 000 and 100 000 inflow realizations are used the former during the optimization process while the latter to re evaluate the performance of optimized strategy against uncertainty representation that is potentially wider than the training set the time series are generated using 45 years of available data at the inchampalli site and the nagarjuna sagar site respectively cholesky decomposition maintains autocorrelation and bootstrap resampling preserves multisite correlation herman et al 2016 kirsch et al 2013 for more details on the generator refer to herman et al 2016 and veena et al 2021 3 2 objective functions and problem formulations satisfaction of water demands is the main motivation behind the ins project and was considered as the only objective in the design phase by regional authorities nwda national water development agency 2021 apart from the ins project both reservoirs are involved in other water transfers the project is also likely to impact environmental flows downstream of the donor site in the godavari river basin due to the construction of the proposed inchampalli reservoir there are communities that derive their livelihood from the river itself in this region and will be negatively affected once minimum environmental requirements are not met sharma et al 2008 moreover the indian summer monsoon s internal variability implies that there is a possibility of years with exceptionally high rainfall in both basins in which case water levels downstream of either reservoir should remain within the historically observed limits the nagarjuna sagar reservoir has also demonstrated susceptibility to floods in historical records killada et al 2012 as we are dealing with a dynamic system with a strong seasonality performance metrics would need to aggregate time varying states of success failure in water resources literature reliability resilience and vulnerability are commonly adopted measures to aggregate the success failure states of a dynamic system hashimoto et al 1982 reliability is the probability of experiencing states where demands are satisfied resilience is the ability to recover from the demand deficits and vulnerability measures the magnitude of demand deficits all three metrics are often used due to the complementary relationship between them improving reliability often leads to deteriorating vulnerability bayazit and ünal 1990 hashimoto et al 1982 moy et al 1986 zhang et al 2017 in other words reservoir operators have to trade off between a large number of small failures or a few severe failures reliability of water supply is by far the most commonly used metric fowler et al 2003 raje and mujumdar 2010 mondal and wasimi 2007 however in the case of the ins project the regional planning for the water transfers was based on estimated demand deficits at annual scale veena et al 2021 nwda national water development agency 2021 we therefore choose to include all three metrics in our analysis we thus define five objective functions to evaluate the performance of water transfer strategies reliability vulnerability and resilience of demand satisfaction reliability of preventing high flow exceedances and reliability of maintaining minimum environmental flows table 1 these objectives were also employed by veena et al 2021 in their recent analysis of the ins project the flow thresholds to determine failure states for high flow exceedances are derived from historical data of downstream releases in the nagarjuna sagar reservoir and inflow data for the inchampalli site veena et al 2021 we set minimum environmental flows to be released downstream as 30 of the mean historical flow following recommendations by smakhtin 2006 although we do not explicitly optimize for satisfaction of water demands from other water transfer commitments of either reservoirs we include it as a constraint in the optimization problem similarly due to the importance of minimum environmental flows for the donor site we also require strategies to maintain a minimum performance for this objective by constraining its lower values to 80 the objective functions are calculated for the donor and recipient reservoir individually and are then aggregated using the mean value to estimate system level performance the averaging of objectives across the participating basins assumes a social planner perspective that aims to optimize for overall system level performance a similar approach was used in the design of proposed water transfers by the regional authorities nwda national water development agency 2021 this was also adopted by veena et al 2021 and retained here so as to enable a comparative baseline for performance measures veena et al 2021 provide a more detailed analysis of the trade offs between donor and recipient basin s objectives 3 2 1 problem formulations for differing temporal resolution of demand reliability the design and operation decisions of the ins project and other large scale water projects in india often focus on demand satisfaction at annual time scales nwda national water development agency 2021 for example interviews for the cauvery river basin by bhave et al 2018 revealed that regional authorities planned for water allocations considering annual demands they were likely to release more water at the beginning of the water year when a large deficit in annual demand exists towards the end of the water year much of the annual demands are likely to be already released and water may not be released for demand satisfaction any further there are two rationale for this type of operation first in a monsoon dominated regime much more water arrives at the beginning of the water year if not released for satisfying demands it will likely be released as excess releases for flood protection of the reservoir second released water in excess of demand at that time step requires that water users will use secondary storage structures this is reasonable given recent developments that consider lakes as an important component of the water resources system dai and labadie 2001 cai et al 2016 fang et al 2019 increasing capacity of artificial lakes to provide secondary storage flood protection and artificial recharge has been proposed by the regional authorities in china wong et al 2017 wei et al 2020 considering the prevalence of secondary storage structures in the recipient nagarjuna sagar s command area fig 1 we similarly assume that water user will avail their storage capacity to deal with release in excess of demand at that time step note that the demands as well as secondary storage structures at the proposed inchampalli site are negligible when compared with the recipient reservoir and therefore we apply these changes for operations of the recipient reservoir only we thus define the reliability of demand satisfaction and reservoir operations at seasonal and annual scales in addition to the 15 day fn model time scale the fn formulation considers demand at the current time step to inform the release decisions at that time step the annual seasonal formulation determines the release decisions for current time step considering cumulative unmet demands for year season which contains the current time step this allows the reservoir operators to release more water than is needed to satisfy the current demands which needs to be stored in secondary storage structures in all three formulations the reservoir is operated at fn resolution and only deficits used to determine demand releases are aggregated so at each time step water continues to be released for demand satisfaction if there is still an annual or seasonal deficit whether these alternatives will be advantageous or not depends on dynamics of water supply and demand in a hypothetical case where there are no variations in water supply and demand all three formulations would yield the same outcome however when inflows and or demands have a strong seasonality as in the case of the ins project the formulations are likely to yield different decisions and consequently performances as an example we have provided calculations for two hypothetical distributions one even and another strongly seasonal for inflows and demands in table s1 based on this we arrive at three problem formulations of the ins project as listed below i fortnightly fn formulation this is the baseline formulation that optimizes decisions θ considering the objectives and constraints in table 1 at 15 day resolution ii seasonal sn formulation this formulation estimates the reliability of demand satisfaction in the recipient reservoir at seasonal time scales eq 4 also the model is altered to make release decisions considering seasonal demand deficits seasonal demand values are updated at every 15 day time step based on the demand released in that the prior period three seasons are defined monsoon june september post monsoon october january and pre monsoon february may all other objectives and constraints are same as the fn formulation 4 j r l d s 1 nr j 1 nr 1 s 1 n s j d m s s j n s j dm s s j 1if d s s j ads s j 0 in eq 4 ds is the seasonal demand satisfied ads is the total seasonal demand subscript s refers to a season ns j is the total number seasons in the planning horizon iii annual an formulation this is same as the seasonal formulation except demand deficits are considered at annual time scales formulation eq 5 the year begins with a full deficit equal to total annual demand water continues to be released for demand satisfaction and annual deficits are updated every 15 days 5 j r l d a 1 nr j 1 nr 1 a 1 n a j d m a a j n a j dm a a j 1 if d a a j ada a j 0 in eq 5 da is the annual demand satisfied ada is the total annual demand na j is the total number of years in the planning horizon we re evaluate the optimized decisions from the sn and an formulations and estimate 15 day demand reliability in order to compare across formulations also as specified the release decisions for annual seasonal formulation at the current time step depend on the cumulative unmet demands for year season at that time step 3 3 generating alternatives using stochastic multi objective optimization the optimal transfer strategies for the fn formulation are identified by generating the non dominated set of decision variable vectors θ that minimize the vector of objectives j θ and the two constraints as detailed in table 1 6 θ argmi n θ j θ θ is a vector of 12 decision variables which are allocations of the monthly water transfers eq 3 for the sn and an formulations j r l d s and j r l d a from eq 4 and 5 replace j rld we use the borg moea to search the space of decision variables and identify pareto optimal strategies hadka and reed 2013 borg has now been widely applied to several water resource management problems and also emerged as a superior algorithm when compared to other moeas hadka et al 2012 reed et al 2013 salazar et al 2016 ward et al 2015 veena et al 2021 we apply a stochastic search procedure using a random sample of 100 out of 10 000 realizations of inflow time series in a single evaluation of the objective functions objective function values are evaluated for each realization and an aggregate value across the 100 realizations is returned to borg in each function call this reduces computational costs when compared to aggregating across all 10 000 realizations in each function call of the algorithm but essentially trains the algorithm over the entire set across multiple function evaluations singh et al 2015 veena et al 2021 ward et al 2015 zeff et al 2016 here we assume a risk neutral formulation and aggregate the objective functions by estimating their mean value a commonly used approach in water resources problems labadie 2004 quinn et al 2017 we re evaluate the attained solutions against a larger set of 100 000 realizations of inflow time series the details on epsilon values for each objective function and parameterization of the borg moea are provided in supplementary text s1 to ensure convergence of the algorithm we run our analysis across 15 random seeds and monitor the hypervolume metric the non dominated solutions across the 15 pareto approximate sets is then used to compose the reference set of solutions 3 4 the inter relationship between decision variables in water resource management problems decisions taken earlier may have an influence on the system states and consequently the optimized decisions at a later time step thierens et al 1998 conceptualize this as the temporal salience structure of the decision problem it likely also relates to the choice of operators employed during the moea process whether the perturbations to the decision variables at each model evaluation are random or systematic this degree of dependency of decision variables on each other has an important consequence for the type of search operators that are likely to do well during the optimization process or the type of moeas that are likely to perform well gupta et al 2020 hadka et al 2012 zheng et al 2017 hadka and reed 2013 we would like to ascertain whether the varying temporal resolution of demand reliability alters the salience structure of the optimized decision variables as the salience structure is a complex entity we use a quantifiable measure to capture its likely nature for our decision problem we quantify linear relationship between decision using a multi way correlation coefficient recently proposed by taylor 2020 eq 7 the multi way correlation coefficient builds on the concept of pearson correlation coefficient that quantifies the relationship between any two random variables it allows the quantification of the relationship between any number of random variables using eq 7 7 mc 1 n s d e i g e n v a l u e s c o r v 1 v 2 v n eq 7 estimates the multi way correlation coefficient mc for a combination of n random variables it does so by estimating the eigenvalues of the correlation matrix formed from the random variables the rth row and cth column of the correlation matrix is the correlation coefficient between the rth and cth variables in eq 7 vi is the column vector containing values of the ith random variable cor is the correlation matrix and sd is the standard deviation when n 2 the multiway correlation coefficient mc is similar to pearson s correlation coefficient a value of 0 indicates values are mutually independent while a value of 1 indicates linearly dependent variables an example calculation of mc for four variables with pre specified relationship is shown in supplementary material s3 estimating mc for 12 monthly water transfer decisions will shed light on their inter relationships a high mc will indicate a strong relationship between the values of the monthly transfers and vice versa mc can be estimated for any combination of 2 12 decision variables for example when estimating for n 2 mc values are the average across all possible combinations of two decision variables arising from the 12 monthly values in order to estimate mc a vector of values for each of the twelve decision variables is needed for each column in the r h s of eq 7 as the borg moea results in a pareto approximate set we use the value of decision variables across this set for mc calculation this calculation is repeated for 15 random seeds and the resulting variation of mc across random seeds discussed we also quantify mc for all subsets of 12 monthly transfer decisions in this way we can estimate the temporal relationships between 2 and 12 combinations of decision variables for example mc estimated for n 4 considers the relationship between all possible groups of four monthly water transfer decisions we surmise that this coefficient would capture an important aspect of the temporal salience structure of the decision variables that comprise the trade off surface to our knowledge this is the first application of multi way correlation coefficient in this context in water resource management problems the main steps followed to estimate mc are 1 for each problem formulation identify the pareto approximate set by stochastic multi objective optimization using the borg moea 2 estimate mc using eq 7 with vector vi set equal to the ith month s optimized transfer values 3 compute mc in step 2 for n 2 to 12 representing all possible combinations of 12 monthly transfer values 4 repeat steps 2 3 for each random seed of the borg moea to obtain the uncertainty bounds on estimated mc values 4 results 4 1 impact of temporal resolution of the reliability objective on strategy performance we find a substantial impact of the temporal resolution of demand reliability on the performance of the pareto approximate strategies identified from stochastic multi objective optimization fig 3 overall we obtained 295 alternative transfer strategies after optimization 99 167 29 of which were obtained by fn sn an formulations the range of objective function values across all strategies are 86 97 19 89 days 79 152 mm3 97 98 and 96 98 for reliability resilience vulnerability of demand satisfaction reliability of flood protection and reliability of maintaining minimum environmental flows respectively the vulnerability metric is multiplied by its demand value to represent average demand deficits in mm3 fig 3a visualizes three demand related objectives the reliability resilience and vulnerability represented by average deficits of demand satisfaction each objective is represented on an axis in the figure the black star marking the ideal solution which simultaneously optimizes all objectives due to conflicts between objectives the ideal solution is not achieved by any strategy strategies from the an formulation clearly outperform the sn and fn formulations as they are located much closer to the ideal point in fig 3a reliability of demand satisfaction attained by fn sn and an strategies range from 87 93 86 96 and 95 97 respectively the mean volumetric demand deficit for fn sn and an strategies is 131 mm3 110 mm3 and 73 mm3 respectively the mean values for resilience of demand satisfaction across all strategies for the fn sn and an formulation is 43 days 53 days and 27 days respectively it follows that there is a clear advantage of operating the water transfers considering annual deficits as long as secondary storage structures are available an strategies also indicate lower level tradeoffs when compared with fn strategies both at system level fig 3b and individual basin level fig 3c tradeoffs between objectives are indicated by lines representing different strategies crossing each other diagonally as they move from one objective axis to another we find that it is possible to maintain minimum environmental flows in both basins while achieving much higher benefits from demand satisfaction for an strategies when compared to fn strategies fn strategies show considerable trade offs between demand deficits and environmental flow maintenance both at system and basin scale see also supplementary fig s1 an and fn strategies attain a reliability of demand satisfaction value in the range 96 and 84 95 for the donor basin and 94 98 and 80 93 for the recipient basin respectively thus an strategies do not present a considerable tradeoff in demand satisfaction between the two participating basins of the ins project the only substantial tradeoff for the an strategies is between the resilience of demand satisfaction in donor and recipient basins an overall narrow range of the objective function value would indicate insignificant trade offs in this case the high flow exceedance objective does not vary much across strategies thus flood exceedances are not likely to pose a major concern for the ins project considering historical uncertainties 4 2 impact of the temporal resolution of the reliability objective on monthly water transfers a key advantage of the an strategies is that they attain high performance levels while maintaining relatively low volumes of water transferred fig 4 the mean value of annual transfer volumes across all strategies and time periods is 8023 mm3 and 8714 mm3 for the an and fn strategies respectively not only the mean values the upper quartile value is also lower for an strategies when compared to fn strategies the iqr difference between first and third quartile is higher for fn strategies compared to an strategies suggested a much greater variability among the annual water transfer volumes fn and an strategies differ not only in the mean annual volumes of water transferred but also in the monthly distribution of these transfers fig 5 fn strategies transfers a majority of their annual volumes injune july august whereas an transfers in july august september june being the beginning of monsoon an formulation focuses on satisfying the demands in both basins first when sufficient water is likely available in either reservoirs and additional water may also be released to secondary storages avoiding high water transfers in june also allows an strategies to guard the recipient reservoir against floods that may occur in july august high volumetric transfer is also suggested during the months of march to may for fn strategies which aim to satisfy demands as and when they arise an strategies however have focused on demand satisfaction using monsoon inflows during the earlier part of the year by exploiting secondary storages they therefore do not need to transfer water in the later periods this is quite advantageous as practically reservoir operators are reluctant to transfer water during the summer months 4 3 inter relationship between decision variables obtained from fn an and sn formulations we find a remarkable impact of the temporal resolution of the demand reliability on the mc values obtained by using 2 to 12 combinations decision variables fig 6 the values of mc are first estimated for each random seed and the boxplots showing variations across the random seeds are plotted for n 12 fn sn and an formulation attain a median mc value of 0 30 0 26 and 0 46 respectively across 15 random seeds the values are slightly less variable for n 2 which results in mc of 0 22 0 16 and 0 34 for fn sn and an formulations respectively recall that n 12 refers the case where mc is estimated for all the decision variables taken together thus quantifying the relationship between decisions across all months the mc value for the an case indicates that there is stronger influence of decisions upon each other in that formulation for lower values of n such as n 2 the mc values display greater variability and lower medians of 0 22 0 16 and 0 34 for fn sn and an formulations respectively recall that n 2 refers to all combinations of any two decision variables for example the relationship between monthly transfer volumes for january february or january april or november december etc overall 25 such combinations are defined the high variability in the mc values can be attributed to the fact that some combinations of monthly water transfers may not exhibit a strong relationship for example seasonal transfers in monsoon months may have a higher correlation with each other while transfers in monsoons may not have a similarly strong relationship with summer month transfers as we consider more and more decision variables together the mc values reduce in uncertainty while remaining high for an the fn and sn formulations lower mc values indicates that impact of decisions on each other is lower when compared to an this is also meaningful as the an formulation considers annual deficits and focuses on satisfying demands early on likely placing more importance to transfer decisions earlier in the water year and also creating a stronger relationship between sequential decisions hadka and reed 2013 developed the borg moea by including multiple combination operators this allowed it to perform across a range of problem structures the operators in borg moea are simulated binary crossover sbx differential evolution de parent centric crossover pcx simplex crossover spx uniform mutation um and unimodal normal distribution crossover undx the choice of operators depends upon the interaction between decision variables also termed epistasis pcx spx undx and de have high operator probability and perform better on epistatic interactive decision variables whereas sbx and um are designed for independent variables hadka et al 2012 elsayed et al 2013 hadka and reed 2015 we find that the main operators being utilized during the search phase of the borg moea varies based on the problem formulation fig 7 the an formulation preferably searches using all the six operators which indicates the complexity of the problem and importance borg s multi operator approach on the other hand the fn and sn formulations are dominated by pcx and sbx operators these differences in the dominant operators being employed by borg is also related to the multi way correlation between decision variables an with high correlation values does not prioritize any specific operator whereas sn and fn with lower correlation uses specific operators thus by changing the temporal resolution of the objective function has likely changed the temporal salience structure of the decision problem these insights also suggest the value of multi way correlation coefficient in understanding the structure of the decisions 5 discussion water resource systems models spanning multiple basins and reservoirs can become quite complex and optimization using these models constitute a suite of wicked decision problems reed and kasprzyk 2009 kasprzyk et al 2013 herman et al 2020 this suggests the need for advancing sensitivity analysis methods in water resource management problems that increase the understanding of the problem structure quinn et al 2019 mautner et al 2022 our findings related to the high multi way correlation coefficient have implications for understanding the underlying problem structures as well as ability of moeas in identifying optima for water resource management problems we show a substantial difference in mc values obtained across the three problem formulations which differ only in the estimation of the reliability objective function this indicates that presumably small changes to problem structure can have a profound impact on the response surface we also find differences in the dominant operators being employed by borg moea in response to the different formulations this suggests the need to advance such optimization algorithms that use the feedback from the optimization process to guide the choice of operators water resource management applications of moeas have shown that it may be impossible to gauge the nature of the response surface apriori and flexible algorithm design is better suited in such cases hadka and reed 2015 exploring the potential of metrics such as the multi way correlation coefficient that summarize high dimensional data should help guide this process veena et al 2021 analysed the ins project using the same objective functions but with a closed loop dynamic policy formulation that allowed releases to be conditioned on system states reservoir storage levels in this case in order to investigate whether our results hold for a closed loop formulation we re optimized the fn and an formulations using the dynamic policy search dps based adaptive rule developed by veena et al 2021 hereon termed the fn dps and an dps formulations we test the cooperative formulation by veena et al 2021 as it was found to outperform other formulations tested in that study in this formulation the volume of water to be transferred tr at each time step is estimated as a function of both donor and recipient reservoir storage volumes and the radial basis function rbfs are conditioned on the inflow and demands in both basins equation s1 we found that the closed loop formulations clearly outperform the open loop formulations supplementary text s2 however the overall multi objective gains as we move from open loop to close loop formulations are not very pronounced when considering the full range of objectives spanned by the four formulations and the decision relevance of the trade offs table s2 more importantly we find that our main result regarding the better performance of an formulation compared to fn formulation holds for the closed loop case as well an dps policies outperform fn dps policies albeit by a margin similar to the gains obtained as we transition from fn to an formulation on analyzing the mc values for transfer volumes obtained by re evaluating the an dps and fn dps solutions we find a relatively smaller impact of the temporal resolution on the mc values when compared to the open loop formulation supplementary fig s5 however the observation that an formulation result in higher mc values remain valid in both cases although differences are much smaller in our study the design of objective functions across differing temporal resolutions are not a mere scaling of the calculation time step rather a conceptual approach is employed where differing temporal resolutions indicate the scale at which release decisions are made thus the resultant difference in performance can be attributed both to the choice of calculation time step as well as the deficit calculations used to inform release decisions this conceptual framework is motivated from our interactions with the reservoir operators for the nagarjuna sagar and insights mentioned in bhave et al 2018 on operation of reservoirs in the cauvery basin india the better performance of the an strategies hinges on our assumption that the water released in excess of demand during the beginning of the water year will be stored in secondary storage structures by water users fig s4 increasing the size of the reservoirs may yield similar benefits however both approaches imply a significantly different governance and institutional context using secondary storages represents a decentralized management system while increasing the size of the already existing large ns reservoir puts emphasis on centralized management considering the improved value of decentralized systems and the generally large economic investments to increase reservoir sizes the former might be a better alternative van der zaag and gupta 2008 van den brandeler et al 2019 incidentally secondary storage structures called tanks already abound in southern india these are connected in different configurations and have long served the irrigation needs of these regions proving sustainable and cost effective but their usage declined with time perhaps due to the advent of large scale reservoirs reddy et al 2018 van der zaag and gupta 2008 srivastava and chinnasamy 2021 this study shows that consideration of decentralized systems i e utilization of secondary storage structures improves the performance of the system and also paves the way for an integrated analysis of centralized and decentralized systems our results therefore open a promising avenue for exploring integrated operations of large reservoirs and secondary storages in a river basin they highlight the value of exploring different temporal resolutions of objective functions in multi reservoir systems we found that the strong seasonality of our study area makes the an formulation use much larger secondary storage an average of 1741 mm3 per fn period than the sn formulation an average of 998 mm3 per fn period this indicates that as we reduce the aggregation time step used to cumulative demands the secondary storage used reduces fig 8 the fn case is the limiting case where secondary storage usage goes to zero in regions with weaker seasonality it is likely that the sn fn formulation perform equally well and the overall differences between the formulations are not that pronounced further research could focus on assessing the volume of the additional storage structures in the command area that may help in improving overall system performance fang et al 2019 li et al 2021 wei et al 2020 one can analyse the necessity of water transfers if secondary storages such as artificial lakes are constructed pérez uresti et al 2019 it would be also prudent to assess to what extent these secondary storage structures along with coarser resolution reliability metric can help alleviate water scarcity in a changing climate it is well known that reservoir operation strategies can be quite sensitive to the hydro climatic period on which they are optimized and therefore further testing of this approach under changing climate conditions is warranted herman et al 2015 quinn et al 2020 giuliani and castelletti 2016 kim et al 2021 borgomeo et al 2018 taner et al 2019 an important avenue for improving the systems models would be to understand how human water feedbacks evolve under extreme conditions such as droughts from a physical perspective water demands during droughts should increase due to reduced rainfall and soil moisture however previous studies have found that demands may in fact reduce during droughts due to adaptation of cropping patterns or reduction of crop area for irrigation by farmers in the command area of the nagarjuna sagar reservoir venot et al 2010 one possibility is to estimate irrigation water requirements using crop models xing et al 2020 wang et al 2016 kögler and söffker 2017 however detailed stakeholder elicitation needs to be carried out to guide dynamics of water demands during droughts 6 conclusions our analysis sheds light on two important aspect of reservoir operation first we find that decision makers need to consider multiple time scales while formulating the systems model and objective functions for the ins project the temporal resolution of the demand reliability has a considerable impact on performance of the water transfer strategies strategies that were based on annual scale reliability measures outperformed seasonal and fortnightly strategies considerably they were able to achieve this due to the assumption that water users will exploit the lakes in the region as secondary storage structures the value of lakes as secondary storage structures in particular and useful green infrastructure in general has been increasing wei et al 2020 we show that reservoir operation studies should explicitly consider them in system models the second important finding of our analysis is the value of multi way correlation coefficient in quantifying the temporal salience structure of decision variables we find that strategies based on annual resolution of demand reliability resulted in decisions that were much more correlated with each other when compared to strategies obtained by optimizing finer resolution demand reliability this also had direct implication for the dominating search operators employed by the borg moea we suggest that the multi way correlation coefficient may be used in future studies to better understand the dynamics of time varying water resource systems credit authorship contribution statement sunkara sai veena conceptualization methodology software validation formal analysis data curation writing original draft visualization riddhi singh conceptualization methodology validation resources writing review editing supervision funding acquisition project administration declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors would like to acknowledge the support of dst serb early career research award number ecr 2015 000355 the authors thank central water commission cwc and irrigation and cad department telangana for providing the data appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2022 128185 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
3084,large scale water infrastructure serves multiple purposes including provisioning of freshwater protection from floods navigation hydro electricity etc decision analysis for reservoir systems relies heavily on optimization techniques that identify optimal operational strategies using a dynamic systems model optimization requires the analyst to define performance indicators or objective functions that aggregate performance across multiple time periods in a planning horizon a question thus arises does the temporal resolution at which objective functions are aggregated have a substantial impact on resultant optimal operational strategy here we investigate this issue using three formulations of a multi objective optimization problem for a proposed inter basin water transfer in southern india the borg multi objective evolutionary algorithm moea is used to optimize five objectives the reliability resilience and vulnerability of demand satisfaction reliability of maintaining minimum environmental flows and reliability of preventing high flow exceedances downstream we alter the temporal resolution at which the reliability of demand satisfaction is calculated by aggregating water deficits at fortnightly seasonal and annual scale resulting in three formulations of the decision problem pareto approximate strategies resulting from the fortnightly seasonal and annual reliability formulations result in annual demand deficits ranging from 82 to 172 mm3 79 152 mm3 and 61 96 mm3 respectively this improvement in performance for annual reliability based strategies exploits greater flexibility within the systems model to store water temporarily in secondary storage structures like lakes ponds check dams in the command area serviced by the project we also find that the choice of temporal resolution of objective function has a substantial influence on the combination operators preferred during the search phase of the borg moea our results highlight the value of exploring objective functions at different temporal resolutions for reservoir optimization keywords reservoir management multi objective optimization reliability time scale objective function evolutionary algorithms borg moea inter basin water transfers data availability data will be made available on request 1 introduction surface water storage reservoirs are an important infrastructural intervention to guard against water scarcity and flood risks this is evident as more than 16 million reservoirs with surface area greater than 100 m2 exist across the globe lehner et al 2011 reservoirs also function as a hydro electricity source enable navigation may be used for fish culture etc while requiring to maintain minimum environmental flows thus multi reservoir systems often service various water sectors and sometimes multiple basins this poses a challenging decision context as the problems envisage several objective functions to represent different stakeholders matrosov et al 2015 do et al 2020 trindade et al 2017 herman et al 2014 zeff et al 2016 amaranto et al 2022 moallemi et al 2020 huskova et al 2016 multi objective optimization moo using evolutionary algorithms is often used to address such complex decision problems affecting multiple actors and sectors gao et al 2022 salazar et al 2017 gold et al 2019 salazar et al 2016 giuliani et al 2016 moo allows decision makers to explore the trade offs between potentially conflicting objectives and identify suitable compromises derepasko et al 2021a horne et al 2016 si et al 2019 gunantara 2018 zeff et al 2014 the application of moo requires specification of three main components of the decision problem i decision variables ii objective functions that quantify the performance requirements and iii a systems models that maps decision to consequences by simulating the objective function values from alternative decision variables lempert 2003 in addition quantification of relevant uncertainties in hydro climatic variables is also important for a realistic understanding of the impact of different decisions kasprzyk et al 2009 quinn et al 2017 quinn et al 2018 trindade et al 2017 gold et al 2019 bertoni et al 2019 herman et al 2014 cohen and herman 2021 giuliani et al 2014 watson and kasprzyk 2017 salazar et al 2017 optimized for hydropower revenue water supply reliability recreation and environmental flow requirement objectives for a multi purpose reservoir in the susquehanna river basin they show that moo can identify a range of possible compromise operational policies similarly quinn et al 2017 showed how stakeholder perception of trade offs can vary across different combinations of objective functions for a multi purpose reservoir system in red river basin in vietnam more recently veena et al 2021 explored multiple problem formulations of an inter basin water transfer considering a range of objectives across multiple water sectors and basins they showed that moo could identify compromise strategies that outperform previously proposed strategy by regional authorities by achieving higher demand satisfaction reliability with much lower volumes of annual transfers thus exploring multiple problem formulations of a decision problem have shown to yield decision relevant insights in several decision contexts quinn et al 2017 veena et al 2021 singh et al 2015 bertoni et al 2019 kasprzyk et al 2013 herman et al 2014 although specification of objective functions is known to play a central role in decision analysis for water management relatively few studies have explored the extent to which the temporal resolution of objective functions impact optimized decisions in reservoir operations objective functions aggregate output from a dynamic systems model that provides state information at a fine temporal resolution of hours to days for example the commonly used reliability objective translates a time series of success failure states to a single numeric value estimated as the fraction of time periods with success to total number of time periods in the planning horizon hashimoto et al 1982 zhang et al 2017 bayazit and ünal 1990 moy et al 1986 mondal and wasimi 2007 raje and mujumdar 2010 in a typical application the reliability is estimated at the temporal resolution of the system model although it can alternatively be estimated at any other temporal scale by estimating success failure states at that scale in their review of optimization tools for water resource management horne et al 2016 identify need to incorporate multiple temporal scales as a little unexplored research area similar findings are put forth by khan et al 2017 in their review of energy and water models for policy exploration there are some studies pointing to the value of exploring multiple temporal scales in objective function formulation gaudard et al 2018 brunner et al 2019 show that objectives calculated for longer time periods for drought management often need to be complemented with objectives for smaller time period like flood control to identify an adequate operational policy similarly georgakakos et al 2012 find it necessary to assess reservoir management policies explore objectives at different temporal resolutions for short term and long term water resources management they suggest that coarser resolution objectives are better to drive decisions related to drought management that consider water deficits across seasons or years similarly finer temporal resolution of objective functions are more relevant for evaluating performance goals related to flooding and hydropower that can be affected by sub daily variations in inflows in addition the interdisciplinary nature of water resource management problems also necessitates exploring multiple temporal scales of objective functions moss and newig 2010 moss and newig 2010 distinguish hydrological and political scales of management which are also subjected to choice of different spatial and temporal scales smith et al 2019 show that as cities in the united states grow water may be sourced from multiple geographic locations each with a different temporal periods of water rights as storage and streamflow diversion similarly institutional settings specify how much water farmers can use annually based on the water act of 1985 in spain guerrero baena et al 2019 on the other hand typical reservoir optimization setups evaluate demand satisfaction related objectives using model scale wang et al 2019 harou et al 2009 nourani et al 2020 hence legislative concerns may prioritize objectives at a temporal scale much different from model scale nearly all reservoir water balance studies optimize release decisions at a fine temporal resolution and use the demand at the current time step to decide on the releases required at that time step however stakeholder interactions have revealed that decision makers may be considering unmet demands at a coarse resolution to determine releases at the current time step for example during their stakeholder elicitation concerning reservoir operations of the cauvery river basin in southern india bhave et al 2018 found that reservoir operators considered annual demands to guide releases assuming that water users would resort to the use of secondary storage structures a large number of traditional secondary storage structures termed tanks are found throughout southern india and have been extensively used for irrigation and drinking water supply prior to modern water infrastructural interventions reddy et al 2018 van der zaag and gupta 2008 srivastava and chinnasamy 2021 similar considerations may be applicable to the nagarjuna sagar reservoir personal communications by first author this also makes intuitive sense as it allows water users also some flexibility in determining the crop sowing periods and make more optimal use of rainwater in a monsoon dominated variable rainfall regime recent developments in china also focus on conjunctively using secondary storage structures like lakes which are an important component of the water resources system wong et al 2017 wei et al 2020 fang et al 2019 currently reservoir simulation optimization models do not explicitly include these secondary storage structures within their operations hence a system scale model incorporating secondary storage within operational design of large reservoirs would be a more realistic representation for these regions concurrently exploring the consequences of formulations that determine release decisions based on cumulative seasonal or annual demands would also be of practical significance in these monsoon dominated agricultural regions these two facets are inherently connected as secondary storage structures are necessary for a systems model that allows conjunctive use of reservoir and secondary storage structures here we propose a such a modelling framework that focuses on managing large scale infrastructure utilizing secondary storage structures considering demands at varied time steps despite the likely benefit of exploring the impact of the temporal scale on optimized decisions there are only a few studies that focus on temporal scale and its implications for water resource management zhou et al 2017 hejazi and cai 2011 shiau and wu 2013 these studies defined multiple objectives at different temporal scales zhou et al 2017 used objective functions that span two temporal scales annual monthly and likely capture the multiple intended goals of the water transfers projects for the hanjiang river basin in china using a simulation optimization approach the goal was to identify the optimal water allocation among four water transfer projects for which water deficits transfers and hydropower generation are defined at an annual scale while water supply reliability resilience and vulnerability are defined at a monthly scale hejazi and cai 2011 show that objective functions estimated at coarse monthly temporal resolution would fail to mitigate flooding downstream and underestimate flood losses compared to those at fine weekly resolution studies show that improving the flexibility in optimization problem structure by including different temporal scales could help identify trade offs shiau and wu 2013 optimized water supply hydropower generation and environmental flow at five temporal scales for a multipurpose reservoir system in taiwan although these studies highlight the necessity of exploring multiple temporal scales of objective functions none so far have compared the optimized decisions obtained from different temporal resolutions of objective functions systematically and explored the associated dynamics of the multi objective evolutionary algorithms being employed recently derepasko et al 2021b reviewed the implications of different choices of temporal and spatial scale in optimization studies for water management they highlighted that the role of temporal resolution of objective functions on optimal solutions has not been specifically studied in the literature here we address this issue by using multiple formulations of an inter basin water transfer problem in southern india across the formulations the temporal scale of the reliability of demand satisfaction objective is varied with other objectives such as demand resilience demand vulnerability flood reliability and minimum environmental flow reliability at the same temporal scale we employ the borg moea to identify pareto approximate strategies that capture the trade offs between multiple objectives for each problem formulation the resultant water transfer strategies and resulting system performance are evaluated we further investigate the operator dynamics of the borg moea to assess whether there are differences among these formulations in terms of the search operators employed 2 the inchampalli nagarjuna sagar project the inchampalli nagarjuna sagar ins project proposes to transfer around 16 400 mm3 of water from the godavari river to the krishna river the two largest rivers of peninsular india fig 1 the project is motivated from the growing water stress on the recipient nagarjuna sagar reservoir due to upstream developments and rising water demands veena et al 2021 the reservoir services millions of farmers and the pharmaceutical hub of hyderabad on the other hand the proposed donor inchampalli reservoir in the godavari river basin will be drained by predominantly protected forest regions with relatively low water demands downstream the average annual inflows at the inchampalli site perur gauge station are more than the double that of the nagarjuna sagar against this backdrop the national water development agency nwda suggested monthly transfer volumes in a feasibility report for ins project using a water balance considering historical inflows and demands note also that both reservoirs are committed towards additional water transfers to other reservoirs the inchampalli reservoir is committed to transfer 4 370 mm3 annually to the pulichintala reservoir while the nagarjuna sagar is committed to transfer 14 200 mm3 annually to the somasila reservoir 2 1 data sources inflow data is obtained from central water commission for the donor reservoir for the time period 1967 2012 and irrigation and computer aided design department telangana for the recipient reservoir for the time period 1967 2016 details regarding the reservoir capacity and the maximum transfer capacity of the ins project are obtained from nwda 2021 demands in both the basins are primarily constituted by irrigation demands with smaller but substantial domestic and industrial water demands on the nagarjuna sagar veena et al 2021 we estimate irrigation demand from cropping patterns and command area which amounts to an annual demand of 7435 mm3 in the nagarjuna sagar that peaks in the months of september and october refer to fig 2 for details on command area irrigation demands in the command area of the inchampalli are much lower quantified as 603 mm3 using population estimates for major urban centres like city of hyderabad serviced the domestic demands for the nagarjuna sagar reservoir is estimated at 1000 mm3 3 methodology we identify water transfer strategies for the ins project using three problem formulations of a multi objective decision problem that differ in the temporal resolution of a water demand related objective fig 2 section 3 1 describes problem framing for the ins project detailing with the systems model uncertainty representation and decision variables section 3 2 details the objective functions used to evaluate alternative strategies along with details on the implementation of differing temporal resolutions that result in three problem formulations section 3 3 presents details on stochastic multi objective optimization to identify candidate operational strategies section 3 4 details the quantification of the inter relationships between decision variables using multi way correlation coefficient this information is used to understand their impact on search dynamics of the borg moea 3 1 problem framing for the ins project there are four main components to any decision problem decision variables performance measures that quantify the stakeholders objectives a model that maps decisions to objectives performance and exogenous uncertainties lempert 2003 the systems model is a dynamic water balance model that tracks the storage of donor and recipient reservoirs considering water transfers using eqs 1 and 2 veena et al 2021 1 st t d st t 1 d q t d e f t d o tr t d t r t d t d r t d 2 st t r st t 1 r q t r e f t r o t r t r t r t d t r r t r in eqs 2 and 3 st is reservoir storage q is the inflow to the reservoir ef is the release to satisfy minimum environmental flow requirements otr is the releases to other water transfers committed by either reservoirs tr is water transferred from the donor godavari to the recipient krishna d is water released for satisfying demands and r is the excess water released downstream subscript t refers to the time step and superscripts d and r represent donor and recipient reservoir fluxes respectively the model runs at 15 day time steps for a planning horizon of 15 years which represents a reasonable window where optimized decisions may be implemented before review and updating the storage values are in m3 while releases and transfers are in m3 15 days after accounting for all releases if the storage exceeds 95 of active storage capacity of the reservoir excess water is released downstream as r commitments related to water transfers are legislative requirements for satisfying their demands therefore they are prioritized over satisfying within basin demands veena et al 2021 we also prioritize environmental flows downstream of both reservoirs to consider a best case scenario w r t ecological requirements downstream it follows that water allocation priority order is minimum environmental flows other water transfers water transfer from donor to recipient reservoir and demand satisfaction for the ins project we specify decision variables as the volumes of water to be transferred from the donor godavari basin to recipient krishna basin every month we employ static open loop strategies that prescribe each decision in a time series as an independent decision and optimize them simultaneously thus the vector of decision variables θ is specified as 3 θ θ m m 1 2 12 0 θ 2800 the allocations of monthly transfers m is assumed to be fixed from year to year in a planning horizon the capacity of the proposed 299 km canal determines the upper limit of monthly transfers 3 1 1 quantification of inflow uncertainty the indian summer monsoon has a highly variable regime and can result in mean annual rainfall variations between 30 to 30 choudhury et al 2021 preethi et al 2019 consequently the inflow to reservoirs driven by the monsoonal cycle is also expected to vary annually considering this uncertainty during the optimization would be crucial to identify strategies that maintain acceptable performance across these stationary stochastic uncertainties to this end we generate multiple realizations of inflows to the donor and recipient site using a bootstrapping sampling technique developed by kirsch et al 2013 fig 2b each realization spans 15 years with a daily resolution these are then aggregated to 15 day resolution of the system model two sets containing 10 000 and 100 000 inflow realizations are used the former during the optimization process while the latter to re evaluate the performance of optimized strategy against uncertainty representation that is potentially wider than the training set the time series are generated using 45 years of available data at the inchampalli site and the nagarjuna sagar site respectively cholesky decomposition maintains autocorrelation and bootstrap resampling preserves multisite correlation herman et al 2016 kirsch et al 2013 for more details on the generator refer to herman et al 2016 and veena et al 2021 3 2 objective functions and problem formulations satisfaction of water demands is the main motivation behind the ins project and was considered as the only objective in the design phase by regional authorities nwda national water development agency 2021 apart from the ins project both reservoirs are involved in other water transfers the project is also likely to impact environmental flows downstream of the donor site in the godavari river basin due to the construction of the proposed inchampalli reservoir there are communities that derive their livelihood from the river itself in this region and will be negatively affected once minimum environmental requirements are not met sharma et al 2008 moreover the indian summer monsoon s internal variability implies that there is a possibility of years with exceptionally high rainfall in both basins in which case water levels downstream of either reservoir should remain within the historically observed limits the nagarjuna sagar reservoir has also demonstrated susceptibility to floods in historical records killada et al 2012 as we are dealing with a dynamic system with a strong seasonality performance metrics would need to aggregate time varying states of success failure in water resources literature reliability resilience and vulnerability are commonly adopted measures to aggregate the success failure states of a dynamic system hashimoto et al 1982 reliability is the probability of experiencing states where demands are satisfied resilience is the ability to recover from the demand deficits and vulnerability measures the magnitude of demand deficits all three metrics are often used due to the complementary relationship between them improving reliability often leads to deteriorating vulnerability bayazit and ünal 1990 hashimoto et al 1982 moy et al 1986 zhang et al 2017 in other words reservoir operators have to trade off between a large number of small failures or a few severe failures reliability of water supply is by far the most commonly used metric fowler et al 2003 raje and mujumdar 2010 mondal and wasimi 2007 however in the case of the ins project the regional planning for the water transfers was based on estimated demand deficits at annual scale veena et al 2021 nwda national water development agency 2021 we therefore choose to include all three metrics in our analysis we thus define five objective functions to evaluate the performance of water transfer strategies reliability vulnerability and resilience of demand satisfaction reliability of preventing high flow exceedances and reliability of maintaining minimum environmental flows table 1 these objectives were also employed by veena et al 2021 in their recent analysis of the ins project the flow thresholds to determine failure states for high flow exceedances are derived from historical data of downstream releases in the nagarjuna sagar reservoir and inflow data for the inchampalli site veena et al 2021 we set minimum environmental flows to be released downstream as 30 of the mean historical flow following recommendations by smakhtin 2006 although we do not explicitly optimize for satisfaction of water demands from other water transfer commitments of either reservoirs we include it as a constraint in the optimization problem similarly due to the importance of minimum environmental flows for the donor site we also require strategies to maintain a minimum performance for this objective by constraining its lower values to 80 the objective functions are calculated for the donor and recipient reservoir individually and are then aggregated using the mean value to estimate system level performance the averaging of objectives across the participating basins assumes a social planner perspective that aims to optimize for overall system level performance a similar approach was used in the design of proposed water transfers by the regional authorities nwda national water development agency 2021 this was also adopted by veena et al 2021 and retained here so as to enable a comparative baseline for performance measures veena et al 2021 provide a more detailed analysis of the trade offs between donor and recipient basin s objectives 3 2 1 problem formulations for differing temporal resolution of demand reliability the design and operation decisions of the ins project and other large scale water projects in india often focus on demand satisfaction at annual time scales nwda national water development agency 2021 for example interviews for the cauvery river basin by bhave et al 2018 revealed that regional authorities planned for water allocations considering annual demands they were likely to release more water at the beginning of the water year when a large deficit in annual demand exists towards the end of the water year much of the annual demands are likely to be already released and water may not be released for demand satisfaction any further there are two rationale for this type of operation first in a monsoon dominated regime much more water arrives at the beginning of the water year if not released for satisfying demands it will likely be released as excess releases for flood protection of the reservoir second released water in excess of demand at that time step requires that water users will use secondary storage structures this is reasonable given recent developments that consider lakes as an important component of the water resources system dai and labadie 2001 cai et al 2016 fang et al 2019 increasing capacity of artificial lakes to provide secondary storage flood protection and artificial recharge has been proposed by the regional authorities in china wong et al 2017 wei et al 2020 considering the prevalence of secondary storage structures in the recipient nagarjuna sagar s command area fig 1 we similarly assume that water user will avail their storage capacity to deal with release in excess of demand at that time step note that the demands as well as secondary storage structures at the proposed inchampalli site are negligible when compared with the recipient reservoir and therefore we apply these changes for operations of the recipient reservoir only we thus define the reliability of demand satisfaction and reservoir operations at seasonal and annual scales in addition to the 15 day fn model time scale the fn formulation considers demand at the current time step to inform the release decisions at that time step the annual seasonal formulation determines the release decisions for current time step considering cumulative unmet demands for year season which contains the current time step this allows the reservoir operators to release more water than is needed to satisfy the current demands which needs to be stored in secondary storage structures in all three formulations the reservoir is operated at fn resolution and only deficits used to determine demand releases are aggregated so at each time step water continues to be released for demand satisfaction if there is still an annual or seasonal deficit whether these alternatives will be advantageous or not depends on dynamics of water supply and demand in a hypothetical case where there are no variations in water supply and demand all three formulations would yield the same outcome however when inflows and or demands have a strong seasonality as in the case of the ins project the formulations are likely to yield different decisions and consequently performances as an example we have provided calculations for two hypothetical distributions one even and another strongly seasonal for inflows and demands in table s1 based on this we arrive at three problem formulations of the ins project as listed below i fortnightly fn formulation this is the baseline formulation that optimizes decisions θ considering the objectives and constraints in table 1 at 15 day resolution ii seasonal sn formulation this formulation estimates the reliability of demand satisfaction in the recipient reservoir at seasonal time scales eq 4 also the model is altered to make release decisions considering seasonal demand deficits seasonal demand values are updated at every 15 day time step based on the demand released in that the prior period three seasons are defined monsoon june september post monsoon october january and pre monsoon february may all other objectives and constraints are same as the fn formulation 4 j r l d s 1 nr j 1 nr 1 s 1 n s j d m s s j n s j dm s s j 1if d s s j ads s j 0 in eq 4 ds is the seasonal demand satisfied ads is the total seasonal demand subscript s refers to a season ns j is the total number seasons in the planning horizon iii annual an formulation this is same as the seasonal formulation except demand deficits are considered at annual time scales formulation eq 5 the year begins with a full deficit equal to total annual demand water continues to be released for demand satisfaction and annual deficits are updated every 15 days 5 j r l d a 1 nr j 1 nr 1 a 1 n a j d m a a j n a j dm a a j 1 if d a a j ada a j 0 in eq 5 da is the annual demand satisfied ada is the total annual demand na j is the total number of years in the planning horizon we re evaluate the optimized decisions from the sn and an formulations and estimate 15 day demand reliability in order to compare across formulations also as specified the release decisions for annual seasonal formulation at the current time step depend on the cumulative unmet demands for year season at that time step 3 3 generating alternatives using stochastic multi objective optimization the optimal transfer strategies for the fn formulation are identified by generating the non dominated set of decision variable vectors θ that minimize the vector of objectives j θ and the two constraints as detailed in table 1 6 θ argmi n θ j θ θ is a vector of 12 decision variables which are allocations of the monthly water transfers eq 3 for the sn and an formulations j r l d s and j r l d a from eq 4 and 5 replace j rld we use the borg moea to search the space of decision variables and identify pareto optimal strategies hadka and reed 2013 borg has now been widely applied to several water resource management problems and also emerged as a superior algorithm when compared to other moeas hadka et al 2012 reed et al 2013 salazar et al 2016 ward et al 2015 veena et al 2021 we apply a stochastic search procedure using a random sample of 100 out of 10 000 realizations of inflow time series in a single evaluation of the objective functions objective function values are evaluated for each realization and an aggregate value across the 100 realizations is returned to borg in each function call this reduces computational costs when compared to aggregating across all 10 000 realizations in each function call of the algorithm but essentially trains the algorithm over the entire set across multiple function evaluations singh et al 2015 veena et al 2021 ward et al 2015 zeff et al 2016 here we assume a risk neutral formulation and aggregate the objective functions by estimating their mean value a commonly used approach in water resources problems labadie 2004 quinn et al 2017 we re evaluate the attained solutions against a larger set of 100 000 realizations of inflow time series the details on epsilon values for each objective function and parameterization of the borg moea are provided in supplementary text s1 to ensure convergence of the algorithm we run our analysis across 15 random seeds and monitor the hypervolume metric the non dominated solutions across the 15 pareto approximate sets is then used to compose the reference set of solutions 3 4 the inter relationship between decision variables in water resource management problems decisions taken earlier may have an influence on the system states and consequently the optimized decisions at a later time step thierens et al 1998 conceptualize this as the temporal salience structure of the decision problem it likely also relates to the choice of operators employed during the moea process whether the perturbations to the decision variables at each model evaluation are random or systematic this degree of dependency of decision variables on each other has an important consequence for the type of search operators that are likely to do well during the optimization process or the type of moeas that are likely to perform well gupta et al 2020 hadka et al 2012 zheng et al 2017 hadka and reed 2013 we would like to ascertain whether the varying temporal resolution of demand reliability alters the salience structure of the optimized decision variables as the salience structure is a complex entity we use a quantifiable measure to capture its likely nature for our decision problem we quantify linear relationship between decision using a multi way correlation coefficient recently proposed by taylor 2020 eq 7 the multi way correlation coefficient builds on the concept of pearson correlation coefficient that quantifies the relationship between any two random variables it allows the quantification of the relationship between any number of random variables using eq 7 7 mc 1 n s d e i g e n v a l u e s c o r v 1 v 2 v n eq 7 estimates the multi way correlation coefficient mc for a combination of n random variables it does so by estimating the eigenvalues of the correlation matrix formed from the random variables the rth row and cth column of the correlation matrix is the correlation coefficient between the rth and cth variables in eq 7 vi is the column vector containing values of the ith random variable cor is the correlation matrix and sd is the standard deviation when n 2 the multiway correlation coefficient mc is similar to pearson s correlation coefficient a value of 0 indicates values are mutually independent while a value of 1 indicates linearly dependent variables an example calculation of mc for four variables with pre specified relationship is shown in supplementary material s3 estimating mc for 12 monthly water transfer decisions will shed light on their inter relationships a high mc will indicate a strong relationship between the values of the monthly transfers and vice versa mc can be estimated for any combination of 2 12 decision variables for example when estimating for n 2 mc values are the average across all possible combinations of two decision variables arising from the 12 monthly values in order to estimate mc a vector of values for each of the twelve decision variables is needed for each column in the r h s of eq 7 as the borg moea results in a pareto approximate set we use the value of decision variables across this set for mc calculation this calculation is repeated for 15 random seeds and the resulting variation of mc across random seeds discussed we also quantify mc for all subsets of 12 monthly transfer decisions in this way we can estimate the temporal relationships between 2 and 12 combinations of decision variables for example mc estimated for n 4 considers the relationship between all possible groups of four monthly water transfer decisions we surmise that this coefficient would capture an important aspect of the temporal salience structure of the decision variables that comprise the trade off surface to our knowledge this is the first application of multi way correlation coefficient in this context in water resource management problems the main steps followed to estimate mc are 1 for each problem formulation identify the pareto approximate set by stochastic multi objective optimization using the borg moea 2 estimate mc using eq 7 with vector vi set equal to the ith month s optimized transfer values 3 compute mc in step 2 for n 2 to 12 representing all possible combinations of 12 monthly transfer values 4 repeat steps 2 3 for each random seed of the borg moea to obtain the uncertainty bounds on estimated mc values 4 results 4 1 impact of temporal resolution of the reliability objective on strategy performance we find a substantial impact of the temporal resolution of demand reliability on the performance of the pareto approximate strategies identified from stochastic multi objective optimization fig 3 overall we obtained 295 alternative transfer strategies after optimization 99 167 29 of which were obtained by fn sn an formulations the range of objective function values across all strategies are 86 97 19 89 days 79 152 mm3 97 98 and 96 98 for reliability resilience vulnerability of demand satisfaction reliability of flood protection and reliability of maintaining minimum environmental flows respectively the vulnerability metric is multiplied by its demand value to represent average demand deficits in mm3 fig 3a visualizes three demand related objectives the reliability resilience and vulnerability represented by average deficits of demand satisfaction each objective is represented on an axis in the figure the black star marking the ideal solution which simultaneously optimizes all objectives due to conflicts between objectives the ideal solution is not achieved by any strategy strategies from the an formulation clearly outperform the sn and fn formulations as they are located much closer to the ideal point in fig 3a reliability of demand satisfaction attained by fn sn and an strategies range from 87 93 86 96 and 95 97 respectively the mean volumetric demand deficit for fn sn and an strategies is 131 mm3 110 mm3 and 73 mm3 respectively the mean values for resilience of demand satisfaction across all strategies for the fn sn and an formulation is 43 days 53 days and 27 days respectively it follows that there is a clear advantage of operating the water transfers considering annual deficits as long as secondary storage structures are available an strategies also indicate lower level tradeoffs when compared with fn strategies both at system level fig 3b and individual basin level fig 3c tradeoffs between objectives are indicated by lines representing different strategies crossing each other diagonally as they move from one objective axis to another we find that it is possible to maintain minimum environmental flows in both basins while achieving much higher benefits from demand satisfaction for an strategies when compared to fn strategies fn strategies show considerable trade offs between demand deficits and environmental flow maintenance both at system and basin scale see also supplementary fig s1 an and fn strategies attain a reliability of demand satisfaction value in the range 96 and 84 95 for the donor basin and 94 98 and 80 93 for the recipient basin respectively thus an strategies do not present a considerable tradeoff in demand satisfaction between the two participating basins of the ins project the only substantial tradeoff for the an strategies is between the resilience of demand satisfaction in donor and recipient basins an overall narrow range of the objective function value would indicate insignificant trade offs in this case the high flow exceedance objective does not vary much across strategies thus flood exceedances are not likely to pose a major concern for the ins project considering historical uncertainties 4 2 impact of the temporal resolution of the reliability objective on monthly water transfers a key advantage of the an strategies is that they attain high performance levels while maintaining relatively low volumes of water transferred fig 4 the mean value of annual transfer volumes across all strategies and time periods is 8023 mm3 and 8714 mm3 for the an and fn strategies respectively not only the mean values the upper quartile value is also lower for an strategies when compared to fn strategies the iqr difference between first and third quartile is higher for fn strategies compared to an strategies suggested a much greater variability among the annual water transfer volumes fn and an strategies differ not only in the mean annual volumes of water transferred but also in the monthly distribution of these transfers fig 5 fn strategies transfers a majority of their annual volumes injune july august whereas an transfers in july august september june being the beginning of monsoon an formulation focuses on satisfying the demands in both basins first when sufficient water is likely available in either reservoirs and additional water may also be released to secondary storages avoiding high water transfers in june also allows an strategies to guard the recipient reservoir against floods that may occur in july august high volumetric transfer is also suggested during the months of march to may for fn strategies which aim to satisfy demands as and when they arise an strategies however have focused on demand satisfaction using monsoon inflows during the earlier part of the year by exploiting secondary storages they therefore do not need to transfer water in the later periods this is quite advantageous as practically reservoir operators are reluctant to transfer water during the summer months 4 3 inter relationship between decision variables obtained from fn an and sn formulations we find a remarkable impact of the temporal resolution of the demand reliability on the mc values obtained by using 2 to 12 combinations decision variables fig 6 the values of mc are first estimated for each random seed and the boxplots showing variations across the random seeds are plotted for n 12 fn sn and an formulation attain a median mc value of 0 30 0 26 and 0 46 respectively across 15 random seeds the values are slightly less variable for n 2 which results in mc of 0 22 0 16 and 0 34 for fn sn and an formulations respectively recall that n 12 refers the case where mc is estimated for all the decision variables taken together thus quantifying the relationship between decisions across all months the mc value for the an case indicates that there is stronger influence of decisions upon each other in that formulation for lower values of n such as n 2 the mc values display greater variability and lower medians of 0 22 0 16 and 0 34 for fn sn and an formulations respectively recall that n 2 refers to all combinations of any two decision variables for example the relationship between monthly transfer volumes for january february or january april or november december etc overall 25 such combinations are defined the high variability in the mc values can be attributed to the fact that some combinations of monthly water transfers may not exhibit a strong relationship for example seasonal transfers in monsoon months may have a higher correlation with each other while transfers in monsoons may not have a similarly strong relationship with summer month transfers as we consider more and more decision variables together the mc values reduce in uncertainty while remaining high for an the fn and sn formulations lower mc values indicates that impact of decisions on each other is lower when compared to an this is also meaningful as the an formulation considers annual deficits and focuses on satisfying demands early on likely placing more importance to transfer decisions earlier in the water year and also creating a stronger relationship between sequential decisions hadka and reed 2013 developed the borg moea by including multiple combination operators this allowed it to perform across a range of problem structures the operators in borg moea are simulated binary crossover sbx differential evolution de parent centric crossover pcx simplex crossover spx uniform mutation um and unimodal normal distribution crossover undx the choice of operators depends upon the interaction between decision variables also termed epistasis pcx spx undx and de have high operator probability and perform better on epistatic interactive decision variables whereas sbx and um are designed for independent variables hadka et al 2012 elsayed et al 2013 hadka and reed 2015 we find that the main operators being utilized during the search phase of the borg moea varies based on the problem formulation fig 7 the an formulation preferably searches using all the six operators which indicates the complexity of the problem and importance borg s multi operator approach on the other hand the fn and sn formulations are dominated by pcx and sbx operators these differences in the dominant operators being employed by borg is also related to the multi way correlation between decision variables an with high correlation values does not prioritize any specific operator whereas sn and fn with lower correlation uses specific operators thus by changing the temporal resolution of the objective function has likely changed the temporal salience structure of the decision problem these insights also suggest the value of multi way correlation coefficient in understanding the structure of the decisions 5 discussion water resource systems models spanning multiple basins and reservoirs can become quite complex and optimization using these models constitute a suite of wicked decision problems reed and kasprzyk 2009 kasprzyk et al 2013 herman et al 2020 this suggests the need for advancing sensitivity analysis methods in water resource management problems that increase the understanding of the problem structure quinn et al 2019 mautner et al 2022 our findings related to the high multi way correlation coefficient have implications for understanding the underlying problem structures as well as ability of moeas in identifying optima for water resource management problems we show a substantial difference in mc values obtained across the three problem formulations which differ only in the estimation of the reliability objective function this indicates that presumably small changes to problem structure can have a profound impact on the response surface we also find differences in the dominant operators being employed by borg moea in response to the different formulations this suggests the need to advance such optimization algorithms that use the feedback from the optimization process to guide the choice of operators water resource management applications of moeas have shown that it may be impossible to gauge the nature of the response surface apriori and flexible algorithm design is better suited in such cases hadka and reed 2015 exploring the potential of metrics such as the multi way correlation coefficient that summarize high dimensional data should help guide this process veena et al 2021 analysed the ins project using the same objective functions but with a closed loop dynamic policy formulation that allowed releases to be conditioned on system states reservoir storage levels in this case in order to investigate whether our results hold for a closed loop formulation we re optimized the fn and an formulations using the dynamic policy search dps based adaptive rule developed by veena et al 2021 hereon termed the fn dps and an dps formulations we test the cooperative formulation by veena et al 2021 as it was found to outperform other formulations tested in that study in this formulation the volume of water to be transferred tr at each time step is estimated as a function of both donor and recipient reservoir storage volumes and the radial basis function rbfs are conditioned on the inflow and demands in both basins equation s1 we found that the closed loop formulations clearly outperform the open loop formulations supplementary text s2 however the overall multi objective gains as we move from open loop to close loop formulations are not very pronounced when considering the full range of objectives spanned by the four formulations and the decision relevance of the trade offs table s2 more importantly we find that our main result regarding the better performance of an formulation compared to fn formulation holds for the closed loop case as well an dps policies outperform fn dps policies albeit by a margin similar to the gains obtained as we transition from fn to an formulation on analyzing the mc values for transfer volumes obtained by re evaluating the an dps and fn dps solutions we find a relatively smaller impact of the temporal resolution on the mc values when compared to the open loop formulation supplementary fig s5 however the observation that an formulation result in higher mc values remain valid in both cases although differences are much smaller in our study the design of objective functions across differing temporal resolutions are not a mere scaling of the calculation time step rather a conceptual approach is employed where differing temporal resolutions indicate the scale at which release decisions are made thus the resultant difference in performance can be attributed both to the choice of calculation time step as well as the deficit calculations used to inform release decisions this conceptual framework is motivated from our interactions with the reservoir operators for the nagarjuna sagar and insights mentioned in bhave et al 2018 on operation of reservoirs in the cauvery basin india the better performance of the an strategies hinges on our assumption that the water released in excess of demand during the beginning of the water year will be stored in secondary storage structures by water users fig s4 increasing the size of the reservoirs may yield similar benefits however both approaches imply a significantly different governance and institutional context using secondary storages represents a decentralized management system while increasing the size of the already existing large ns reservoir puts emphasis on centralized management considering the improved value of decentralized systems and the generally large economic investments to increase reservoir sizes the former might be a better alternative van der zaag and gupta 2008 van den brandeler et al 2019 incidentally secondary storage structures called tanks already abound in southern india these are connected in different configurations and have long served the irrigation needs of these regions proving sustainable and cost effective but their usage declined with time perhaps due to the advent of large scale reservoirs reddy et al 2018 van der zaag and gupta 2008 srivastava and chinnasamy 2021 this study shows that consideration of decentralized systems i e utilization of secondary storage structures improves the performance of the system and also paves the way for an integrated analysis of centralized and decentralized systems our results therefore open a promising avenue for exploring integrated operations of large reservoirs and secondary storages in a river basin they highlight the value of exploring different temporal resolutions of objective functions in multi reservoir systems we found that the strong seasonality of our study area makes the an formulation use much larger secondary storage an average of 1741 mm3 per fn period than the sn formulation an average of 998 mm3 per fn period this indicates that as we reduce the aggregation time step used to cumulative demands the secondary storage used reduces fig 8 the fn case is the limiting case where secondary storage usage goes to zero in regions with weaker seasonality it is likely that the sn fn formulation perform equally well and the overall differences between the formulations are not that pronounced further research could focus on assessing the volume of the additional storage structures in the command area that may help in improving overall system performance fang et al 2019 li et al 2021 wei et al 2020 one can analyse the necessity of water transfers if secondary storages such as artificial lakes are constructed pérez uresti et al 2019 it would be also prudent to assess to what extent these secondary storage structures along with coarser resolution reliability metric can help alleviate water scarcity in a changing climate it is well known that reservoir operation strategies can be quite sensitive to the hydro climatic period on which they are optimized and therefore further testing of this approach under changing climate conditions is warranted herman et al 2015 quinn et al 2020 giuliani and castelletti 2016 kim et al 2021 borgomeo et al 2018 taner et al 2019 an important avenue for improving the systems models would be to understand how human water feedbacks evolve under extreme conditions such as droughts from a physical perspective water demands during droughts should increase due to reduced rainfall and soil moisture however previous studies have found that demands may in fact reduce during droughts due to adaptation of cropping patterns or reduction of crop area for irrigation by farmers in the command area of the nagarjuna sagar reservoir venot et al 2010 one possibility is to estimate irrigation water requirements using crop models xing et al 2020 wang et al 2016 kögler and söffker 2017 however detailed stakeholder elicitation needs to be carried out to guide dynamics of water demands during droughts 6 conclusions our analysis sheds light on two important aspect of reservoir operation first we find that decision makers need to consider multiple time scales while formulating the systems model and objective functions for the ins project the temporal resolution of the demand reliability has a considerable impact on performance of the water transfer strategies strategies that were based on annual scale reliability measures outperformed seasonal and fortnightly strategies considerably they were able to achieve this due to the assumption that water users will exploit the lakes in the region as secondary storage structures the value of lakes as secondary storage structures in particular and useful green infrastructure in general has been increasing wei et al 2020 we show that reservoir operation studies should explicitly consider them in system models the second important finding of our analysis is the value of multi way correlation coefficient in quantifying the temporal salience structure of decision variables we find that strategies based on annual resolution of demand reliability resulted in decisions that were much more correlated with each other when compared to strategies obtained by optimizing finer resolution demand reliability this also had direct implication for the dominating search operators employed by the borg moea we suggest that the multi way correlation coefficient may be used in future studies to better understand the dynamics of time varying water resource systems credit authorship contribution statement sunkara sai veena conceptualization methodology software validation formal analysis data curation writing original draft visualization riddhi singh conceptualization methodology validation resources writing review editing supervision funding acquisition project administration declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors would like to acknowledge the support of dst serb early career research award number ecr 2015 000355 the authors thank central water commission cwc and irrigation and cad department telangana for providing the data appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2022 128185 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
