index,text
25450,the software extrastar extremes abacus for statistical regionalization is here proposed it can represent a useful and quick tool for both regional and at site statistical analyses of annual maxima am time series the innovative aspects compared to other algorithms softwares are 1 an easy comparison of sample skewness of several observed series with pre determined monte carlo prediction intervals depending on sample size from hypothesized probability distributions for obtaining a first important indication about possible clusters and regional values for some parameters which can be afterward refined if deemed as necessary with specific and more complex algorithms softwares proposed in literature 2 for at site analyses the possibility to benefit from calibration of one function for some parameters of the others thus significantly reducing the computational costs extrastar was tested with the daily am series for the italian rain gauge network by implementing ev1 gev and tcev distributions keywords rainfall annual maxima extreme value distributions statistical regionalization data availability the used data can be freely downloaded at http 193 206 192 214 servertsdailyutm serietemporalidaily400 php in italian 1 introduction statistical analysis of rainfall and flood extremes and in particular the annual maxima am clearly plays a crucial role in hydrology for both research and technical purposes moccia et al 2021a panda et al 2022 and it can be diversified according to 1 the adopted probability function sect 1 1 2 the type of approach sect 1 2 stationary i e temporal invariance of model parameters or non stationary i e model parameters are time varying to take into account possible effects of climate changes 3 the method for parametric estimation at site or regional based on the sample size sect 1 3 4 the specific technique for parametric estimation sect 1 4 method of moments maximum likelihood least squares and so on therefore a technician has many combinations for statistical analysis of extreme values even in the context of possible climate change effects the following 4 subsections provide a brief literature review for all these four aspects while the goal of the proposed research is reported in sect 1 5 1 1 choice of the probabilistic model the widely used functions in literature are ev1 gumbel 1958 gev generalized extreme value jenkinson 1955 tcev two component extreme value rossi et al 1984 log pearson type iii bobee 1975 the 3 parameter lognormal johnson et al 1994 pp 208 238 the generalized pareto hosking and wallis 1987 johnson et al 1994 p 615 the generalized logistic balakrishnan and leung 1988 moreover other distributions were recently proposed among them we can mention i the burr xii type moccia et al 2021b ii probability functions which are based on non asymptotic approach marani and ignaccolo 2015 lombardo et al 2019 iii specifically for peak discharge time series derived distributions which take into account specific characteristics of rainfall input and geomorphic information of the investigated watershed iacobellis and fiorentino 2000 de michele and salvadori 2002 1 2 stationary and non stationary approaches for any adopted probability function the specific parameters can be considered as invariant i e a stationary approach is used or varying along the time salas and obeysekera 2014 with the goal of modelling effects induced by potential climate change scenarios in the former case the main source of uncertainty is constituted by extrapolation of the adopted probability distribution beyond the range of observed data in the latter case a further source of uncertainty is introduced that is related to the used mathematical expression for modelling the temporal variation of parameters together with its validity into the whole future horizon of interest moreover attention should be focused on ergodicity koutsoyiannis and montanari 2014 that allows for calibration of a stochastic process from a single realization i e the observed one if the process is non stationary then ergodicity cannot hold and then the temporal variation of parameters should be estimated by only using external data sets for example outputs from climatic models rootzén and katz 2013 at the end of sect 2 1 and more in detail in appendix 1 of this paper authors discussed about this topic and the relationships among the potential effects of climate change scenarios and a stationary non stationary modelling 1 3 at site and regional estimation of parameters in any case integration with other data sets can be necessary for both stationary and non stationary contexts if sample size n is not sufficient less than 10 20 data for obtaining robust at site estimates of parameters and of quantiles associated to larger return periods t then approaches of statistical regionalization can be carried out schaefer 1990 nguyen et al 2002 in this context the index flood method dalrymple 1960 regression analysis and geostatistical procedures represent the most adopted methodologies smith et al 2015 in particular the coupled use of the index flood method with l moments hosking and wallis 1993 has been widely used in many works abida and ellouze 2008 hussain and pasha 2009 noto and la loggia 2009 saf 2009 seckin et al 2011 laio et al 2011 biondi et al 2012 haddad and rahman 2012 aydogan et al 2016 regional frequency analysis can be usually carried out by identifying homogeneous regions hrs where some theoretical moments are assumed as constant while other can be expressed as functions of specific geomorphic covariates hrs are often identified by using geographical or administrative criteria which can be questionable in terms of hydrological homogeneity burn et al 1997 chebana and ouarda 2007 consequently cluster analysis alem et al 2019 lin and chen 2006 rao and srinivas 2008 yin et al 2016 or artificial intelligence techniques cassalho et al 2019 represent a valid alternative with this goal whatever technique is chosen for hrs identification critical aspects are i the assessment of the plausibility of the homogeneity hypothesis for the proposed regions viglione et al 2007 ii the estimates are not smooth both in geographic or in parameter space due to possible discontinuities consequently approaches that do not define fixed boundary regions stedinger and tasker 1985 griffis and stedinger 2007 borga et al 2005 libertino et al 2018 iliopoulou et al 2022 shehu et al 2022 are receiving an increasing attention chokmani and ouarda 2004 chebana and ouarda 2008 skoien et al 2006 1 4 choice of the specific technique for parameters estimation after establishing the use of an at site or a regional approach depending on the available sample size n the parametric estimation can be carried out by using singh 1998 i method of moments ii method of probability weighted moments pwms iii l moments iv maximum likelihood ml v least squares each mentioned technique is aimed to reproduce specific features of the observed sample singh 1998 kottegoda and rosso 2008 and then the choice is based on which aspects are of interest for a user moreover it is also possible to consider a context of equifinality beven and freer 2001 i e different sets of parameters values for a specific model can reproduce in an acceptable way the investigated time series 1 5 goal of the proposed work based on all the previously discussed aspects a user has many combinations for carrying out the statistical analysis of am time series also for taking into account possible effects from climate changes in this framework this paper describes a quick methodology implemented within the extrastar software extremes abacus for statistical regionalization a ms excel file with macros in vba visual basic for application the innovative aspects with respect to other algorithms softwares of extrastar consist in 1 an easy comparison of the sample size and skewness of several observed time series with pre determined monte carlo prediction intervals from hypothesized probability distributions in order to obtain a first important indication about possible clusters and regional values for some parameters which can be clearly refined afterward if it is deemed as necessary by using specific algorithms softwares proposed in literature regarding for example cluster analysis artificial intelligence techniques and suitable techniques of parametric estimation for hierarchical approaches see sect 1 3 starting from extrastar outcomes 2 for at site statistical analyses the possibility to do not carry out a specific calibration for each adopted probability distribution but to take advantage of obtained estimates of one function for some parameters of the other ones due to the choice of ms excel platform extrastar can be used by students technicians but obviously also by researchers the interface is user friendly but as also illustrated in the next pages it is clear that a background in statistical analysis of hydrologic variables is necessary in order to well understand the exstrastar outputs it is possible to download the software at the website https sites google com unical it extrastar home page last accessed on 12 12 2022 together with user s manual and video tutorials fig 1 extrastar was tested for the italian network of annual maxima am of daily rainfall time series related to the scia database of the italian institute for environmental protection and research ispra www scia isprambiente it last accessed on 12 12 2022 the obtained results have shown that extrastar can be considered as a useful tool for obtaining quick indications about possible clusters of different time series in statistically homogeneous areas as well as a reduction in the computational cost for at site parametric estimates the paper is organized with the following sections brief theoretical notes about the probability functions implemented in extrastar and the used data set sect 2 an overview of the software sect 3 discussion of the results sect 4 conclusions sect 5 2 methods and materials 2 1 theoretical background on implemented probability functions statistical analysis of hydrological extremes usually regards the time series of annual maxima am in this case the time series is composed by the maximum value observed in each year peaks over a threshold pot in which the time series comprises all the peak values that exceed a pre fixed threshold zhang et al 2021 by applying the theorem of total probability the well known relationship among am and pot series is obtained from which the asymptotic extreme value theory is derived todorovic 1970 de michele 2019 1 f x x n 0 p n n f x p o t x x 0 n where f x x is the cumulative distribution function cdf of am series and represents the probability that the random variable x assumes values no greater than x f x p o t x x 0 is the cdf of the pot series and x 0 indicates the threshold value above it a peak x belongs to the time series p n n represents the probability associated to n exeedances of the threshold in one year from eq 1 it is possible to obtain several expressions for f x x depending on the specific adopted mathematical formulas for p n n and f x p o t x x 0 in a stationary context if p n n is assumed as poisson distributed 2 p n n λ n e λ n in which λ represents the mean annual number of exeedances then eq 1 becomes 3 f x x e λ 1 f x p o t x x 0 if the exponential distribution is adopted for f x p o t x x 0 4 f x p o t x x 0 1 e x x 0 θ where θ is the mean annual intensity of the peaks then the well known ev1 probability distribution gumbel 1958 is obtained for eq 1 5a f x x e λ e x x 0 θ e e 1 θ x x 0 θ ln λ e e α x ε with α 1 θ and ε θ ln λ x 0 when we set x 0 0 todorovic and zelenhasic 1970 rossi et al 1984 eq 5a can be rewritten as 5b f x x e λ e x θ e e 1 θ x θ ln λ e e α x ε preserving the assumption of p n n as poisson distributed eq 1 it is possible to demonstrate that by using a generalized pareto wang 1991 for f x p o t x x 0 eq 1 becomes the gev generalized extreme value jenkinson 1955 6 f x x e 1 b x θ ln λ θ 1 b e 1 b α x ε 1 b in which b corresponds to the shape parameter for b 0 the gev distribution coincides with ev1 function ev2 and ev3 laws are obtained when b 0 and b 0 respectively it must be remarked that the k th moment of the gev distribution exists if b 1 k e g the mean exists if b 1 the variance if b 1 2 the skewness if b 1 3 gupta 2011 if a mixture of two exponential distributions is adopted for f x p o t x x 0 then eq 1 corresponds to tcev two component extreme value rossi et al 1984 probability function 7a f x x e λ 1 e x θ 1 λ 2 e x θ 2 in which λ 1 and λ 2 with λ 1 λ 2 are the mean annual number for ordinary and outlier events čampulová et al 2022 respectively while θ 1 and θ 2 with θ 1 θ 2 are the correspondent mean values for intensities a well known tcev formulation used in contexts of statistical regionalization is obtained by introducing two dimensionless parameters θ θ 2 θ 1 λ λ 2 λ 1 1 θ with θ 1 7b f x x e λ 1 e x θ 1 λ λ 1 1 θ e x θ θ 1 from eqs 5 7 it can be noted that application of asymptotic extreme value theory does not require the explicit definition of the threshold x 0 and of f x p o t x x 0 which can be a priori unknown in fact parameters of f x x can be directly estimated by data fitting of am time series however the asymptotic approach could be questionable from a physical point of view as n seems unrealistic for example the daily scale is characterized by n 365 de michele 2019 in this context as mentioned into the introduction non asymptotic approaches were proposed in literature marani and ignaccolo 2015 zorzetto et al 2016 lombardo et al 2019 which are based on the so called penultimate assumption gomes 1984 nevertheless these approaches are currently not so used because they request the definition of f x p o t x x 0 which is difficult to determine in practical cases de michele 2019 obviously an ad hoc procedure for statistical regionalization can be developed for any probability distribution ferrari and versace 1994 for the implemented models in extrastar the reduced ev1 variable rossi et al 1984 is considered 8 y x θ ln λ θ α x ε x θ 1 ln λ 1 θ 1 as y is a linear transformation of x these two random variables clearly present the same value for skewness then it is possible to rewrite eqs 5 7 as 9 e v 1 f y y e e y 10 g e v f y y e 1 b y 1 b 11 t c e v f y y e e y λ e y θ and to identify hrs where the theoretical skewness and therefore the parameters λ and θ for the tcev beran et al 1986 or the parameter b with b 0 in the case of ev1 for the gev can be assumed as constant gabriele and arnell 1991 or they follow laws based on geomorphological covariates altitude hydrographic basin area etc as an example fig 2 shows the comparison among the cdf expressed by eqs 9 11 on an ev1 probabilistic plot kottegoda and rosso 2008 by considering some values for b of gev and for λ and θ of tcev into a stationary context i e invariance in time for the parameters from fig 2 it is clear that for a fixed value of f y y i e a fixed value of ln ln f y y on the vertical axis the correspondent value y of the quantile increases from an ev1 distribution to a gev with b 0 or tcev with growing λ and θ thus obtaining distributions which are more skewed with respect to ev1 the developed methodology in extrastar useful for either at site or regional approaches is based on the following steps 1 sample sizes n between 20 and 200 were considered for each n and for each distribution 5000 series of the variable y were generated using the monte carlo methodology kottegoda and rosso 2008 in detail i for gev the following 6 values of parameter b were considered 0 ev1 0 05 0 1 0 15 0 2 0 25 ii 40 combinations of λ θ were used for tcev with 0 1 λ 0 5 step 0 1 and 1 5 θ 5 step 0 5 overall 46x5000 samples of the standardized variable y were generated for each value of n concerning gev the value 0 25 as the lower limit for b allows for the existence of mean variance and skewness as previously mentioned 2 for each set of 5000 series the two side 90 monte carlo prediction interval mcpi was evaluated for the sample skewness g and therefore 46 mcpis with respect to n can be represented within an abacus implemented within the extrastar software see sect 3 by inserting into the abacus figs 8 10 the information from observed am time series in terms of n and g it is possible to quickly assess which distributions and with reference values of the shape parameters are able to model the sample skewness and then the possible presence of outliers and therefore to cluster some many series in hrs it should be underlined that kurtosis also plays a significant role in these analyses hosking and wallis 1993 laio et al 2011 but authors did not consider this fourth order statistic because of its greater uncertainty with respect to skewness when short samples are analyzed furthermore this stationary methodology can be also useful in climate change contexts iliopoulou and koutsoyiannis 2020 mainly for specific time resolutions for which the hypothesis of stationarity could not be rejected in many cases see appendix 1 for further details obviously the generalization of eqs 5 11 to a non stationary modelling i e setting the parameters set φ λ t θ t or φ λ t θ t b t or φ λ 1 t θ 1 t λ 2 t θ 2 t or φ λ 1 t θ 1 t λ t θ t is straightforward however as thoroughly discussed in appendix 1 authors highlight that the possibility of using mainly at daily scale a stationary modelling in a changing climate does not exclude the adoption of a non stationary approach which should be necessarily calibrated with external information in order to do not violate the ergodicity koutsoyiannis and montanari 2014 in this context a very interesting web tool is proposed in simonovic et al 2016 in which a user can generate intensity duration frequency curves that account for future climate conditions forecasted by general circulation models gcms extrastar software currently allows for a quick stationary at site or regional statistical analysis only in terms of sample skewness but future developments will enable quick regional investigations for sample mean and standard deviation of x together with the implementation of specific modules for a non stationary modelling the choice of firstly using a stationary modelling is justified in appendix 1 moreover authors put in evidence that the quick regional evaluation for b or λ and θ from extrastar abacus must be considered as a first important indication about values that can be assumed clearly specific algorithms can be furtherly adopted by users for refining the estimates rossi et al 1984 gabriele and arnell 1991 and for the improvement of hrs identification by applying k means or other clustering models alem et al 2019 lin and chen 2006 rao and srinivas 2008 yin et al 2016 or else artificial intelligence techniques cassalho et al 2019 starting from extrastar outcomes 2 2 data set in this work authors considered the scia database of the italian institute for environmental protection and research ispra http www scia isprambiente it last accessed on 12 12 2022 and in particular the network of daily am rainfall series fig 3 specifically the whole national rain gauge network is reported in fig 3a but only a subset that affects about 80 of the whole territory is made available in scia by the several regional local agencies which manage the data overall it was possible to analyze 3351 samples with n 20 years in the time interval 1860 2020 fig 3b the main areas with no data in scia database or with n 20 years comprise many parts of piedmont and lombardia regions in northern italy and campania basilicata and apulia regions for the southern zone the mean value of n is approximately 50 years with a maximum of 146 years milan rain gauge the values of g vary from 0 71 to 5 10 with an average of 1 34 the scatterplots of sample skewness with sample sizes and elevation of rain gauges are illustrated in fig 4 in particular the elevation range 0 500 m a s l is characterized by a significant number of time series with sample skewness g 3 fig 4b from the numerical experiments shown in appendix 1 also based on the work of papalexiou and montanari 2019 for the daily scale it is clear that without any external information for example outputs from climatic models the values of sample size n allow for adoption of a stationary approach for the investigated time series 3 overview of the extrastar software when a user executes extrastar xlsm after having enabled the vba macros the home worksheet will appear as in fig 5 and it is possible to select the kind of analysis single time series i e an at site investigation or ensemble of time series i e a statistical regionalization by clicking on the command button start in addition extrastar contains the following worksheets input data n and g in which a user can upload the text file with the information about sample size n and sample skewness g of many rain gauges in order to carry out a statistical regionalization single time series that allows for uploading and analyzing the am data of a single rain gauge ev1 plot where the graphical results of statistical analysis for a single series are shown abacus in which a user can visualize the abacus aimed at the quick statistical regionalization abacus data that contains the values of the two side 90 mcpis in terms of sample skewness g for specific values of sample size n and for all the investigated options for ev1 gev and tcev distributions 3 1 ensemble of time series statistical regionalization the structure of input data n and g worksheet is represented in fig 6 the right part contains a very brief theoretical reminder about the reduced variable y the mathematical expressions of the implemented cdfs and the well known equivalence between the skewness coefficients of x and y variables the command button input file n g allows for uploading a two column text file without headers fig 7 in which the sample size n and the sample skewness g for all the investigated rain gauges are reported in the first and second column respectively these values are then pasted on the worksheet columns a and b afterward a user can visualize the information about n and g by clicking on the command button go to abacus the worksheet abacus will be shown fig 8 in which the scatterplot of the sample points n g is represented together with as default the two side 90 mcpi of ev1 distribution see sect 2 1 when a user clicks in the run gev and tcev options command button he can select with a dedicated form fig 9 the specific values of interest for the shape parameter b gev distribution and for the parameters λ θ tcev distribution for which the correspondent two side 90 mcpis will be visualized fig 10 the abacus is structured in such a way as to represent only one option at a time for both gev and tcev in order to facilitate the visualization of the results as a very quick guidance for users without a strong background in extreme value theory high values of sample skewness are better modelled with low b 0 for gev and with growing values mainly for θ when a tcev function is adopted beran et al 1986 3 2 single time series at site statistical analysis as previously mentioned extrastar also allows for a statistical analysis of a single time series the choice of this option fig 5 shows the worksheet named single time series and then a user can upload a 2 column txt file in which there is not any header the first column is related to the years and the second contains the rain data by clicking on the command button input data from txt file fig 11 after the data upload the running code calculates sample size mean standard deviation skewness and the statistic z m k value related to the mann kendall test for trend analysis see appendix 2 into the cells interval j1 j6 fig 12 and visualizes the correspondent chronological diagram the click on the command button am modelling shows a dedicated form fig 13 in which it possible 1 to select a stationary or a non stationary approach for the statistical analysis the code for the latter option is under construction 2 to carry out the parametric estimation of ev1 distribution by using the maximum likelihood ml method and to select specific options for gev and tcev fig 14 as previously remarked low values of b 0 for gev and very high values of θ for tcev beran et al 1986 should be preferred for samples that are strongly skewed i e g 3 on the contrary the options comprising 0 1 b 0 and θ 3 can be considered 3 to visualize the fitting of ev1 gev and tcev distributions on an ev1 plot fig 15 by clicking on the command button plot 4 to compare the two side 90 mcpis of each distribution with the sample data fig 16 by clicking on the command button monte carlo simulation see sect 2 1 in order to better evaluate the possibility of non rejection of each considered probability function if the sample falls inside the associated mcpi when many distributions cannot be rejected a user can choice the model with the small number of parameters into a context of parsimony or for a fixed type of distribution the parameter option with the lowest value of skewness see the previous step 2 this mcpi analysis represents a statistical test like chi square and kolmogorov smirnov tests kottegoda and rosso 2008 which can be clearly implemented in extrastar as future developments of the software 4 results and discussion from the statistical regionalization carried out by using scia database figs 6 10 the main results can be summarized as follows 1 almost 80 of the analyzed cases i e 2679 samples can be modelled with the ev1 model 2 concerning gev distribution the use of the shape parameter b 0 1 provides the best modeling among all the considered values for b 88 7 of cases equal to 2971 sample series moreover 2526 series from this subset i e the 75 4 of the whole data set can be also reproduced with ev1 3 the sample skewness of 3064 time series 91 4 of the whole data set can be reconstructed through the tcev function with λ 0 1 and θ 3 from this subset 2520 series 75 2 of the whole data set can be also modelled with ev1 and 2961 series 88 4 of the whole data set with gev distribution with b 0 1 the comparison between the possible uses for ev1 and tcev with λ 0 1 and θ 3 is represented in fig 17 in terms of geographic location and in fig 18 in terms of scatterplots between sample skewness g and rain gauge elevation tcev model clearly improves the modelling of time series with large values of sample skewness mainly located into the elevation range 0 100 m a s l but it should be rejected for some rain gauges with g 1 for which ev1 is more suitable however focusing on ev1 performances some global scale rainfall analyses e g papalexiou and koutsoyiannis 2013 serinaldi and kilsby 2014 koutsoyiannis 2004 pointed to the prevalence of ev2 i e gev with b 0 with respect to ev1 but this behaviour is often masked by the short record lengths available and may falsely appear as ev1 papalexiou and koutsoyiannis 2013 and serinaldi and kilsby 2014 also proposed methods for correcting the sample size bias for the shape parameter b overall the rain gauges for which tcev modelling is suitable with regional values λ 0 1 and θ 3 are uniformly distributed on the considered spatial domain green dots in fig 17 taking into account the areas with no available data in scia database sect 2 2 adoption of tcev with λ 0 1 and θ 3 is particularly suitable for sardinia and sicily islands for calabria region in southern italy and for the whole central and north eastern parts then it could be possible to assume a unique hr that comprises these investigated territories and to state that tcev with different sets for λ θ or ev1 could be considered for other isolated rain gauges red dots in fig 17 clearly further analyses concerning as example quality check of data could be necessary in order to explain justify sample skewness less than 0 for which ev1 distribution is not even appropriate fig 18a as well as integration with other databases of daily am series for completing the investigation for the whole italian territory it should be remarked that this carried out procedure is related to a first level of a hierarchical regional approach related to skewness but successive levels of analysis can be obviously implemented inside extrastar a second level of regionalization allows for identifying inside each hr homogeneous sub regions hsrs which are characterized by constant values or depending on geomorphic covariates with specific mathematical relationships for the coefficient of variation cov moreover a third level can be developed in which each hsr can be furtherly subdivided into homogeneous areas has where the mean value can be assumed as constant or as a function of some covariates for italian territory a hierarchical approach was developed in the last decades for flood and rainfall extremes see http www idrologia polito it gndci vapi htm in italian last accessed on 12 12 2022 each regional agency carried out this analysis for its own database a revision of the results is clearly necessary by using the updated databases and by extending the analysis at national scale as suggested in this work as an example of application for a single daily am time series see figs 11 16 authors focused on subiaco rain gauge close to rome characterized by n 103 g 1 53 and a mean value of 68 8 mm with a standard deviation of 21 1 mm as z m k 1 39 it is possible to do not reject the hypothesis of a stationary process with a significance level of 5 by inserting the information n 103 g 1 53 on the exstrastar abacus it emerges that subiaco am series can be easily modelled with ev1 and with gev b 0 1 and tcev λ 0 1 and θ 3 the maximum likelihood ml estimates for ev1 distribution provided θ 15 2 mm and λ 50 7 figs 14 and 15 the application of ml technique usually allows for an excellent fitting of the data with respect to ordinary values with the adoption of these estimates also for the corresponding parameters of gev and for the ordinary component of tcev the obtained fittings fig 15 would induce to consider ev1 and tcev as inadequate on the contrary if 5000 synthetic monte carlo generations each one with n 103 are carried out for each considered probabilistic functions and the corresponding two side 90 mcpis are estimated each distribution may not be rejected for the modelling of the investigated sample series fig 16 thus confirming what is hypothesized with the abacus analysis obviously in a context of parsimony a 2 parameter distribution should be preferred overall the proposed methodology could clearly be considered as a useful tool for very quick statistical analysis with an undoubted reduction of computational costs in fact for an at site analysis it is not necessary to carry out the specific calibration for each distribution but it can be only performed for ev1 specifically the obtained ml estimates for ev1 can be also used for gev and tcev for the correspondences see eqs 9 11 while the values of the remaining parameters b in the gev case and λ and θ in the tcev case can be set from analysis of extrastar abacus moreover the use of abacus for an ensemble analysis allows for easily obtaining indications about potential clusters of time series in homogeneous regions concerning the skewness as previously mentioned at the end of sect 2 1 these quick regional evaluations for b or λ and θ from extrastar abacus must be considered as a first important indication about their plausible values and then a refinement can be carried out by using suitable algorithms rossi et al 1984 gabriele and arnell 1991 which can be implemented in extrastar as future developments obviously hrs identification can be also improved if it is deemed as necessary as well by using for example clustering or artificial intelligence techniques alem et al 2019 cassalho et al 2019 lin and chen 2006 rao and srinivas 2008 yin et al 2016 starting from extrastar outcomes 5 conclusions the extrastar software can undoubtedly represent a user friendly tool for the statistical analysis regional or at site of annual maxima for an observed time series of interest it is possible to quickly test the modelling capability for ev1 and gev or tcev also with reference values for the shape parameters b for gev and λ and θ for tcev on the basis on the sample size and skewness furthermore by simultaneously analyzing the sample skewness of several time series a user can obtain a rapid indication about potential clusters in homogeneous regions another important aspect is the possibility of using ev1 maximum likelihood estimates also for gev and for the ordinary component of tcev which allows for a reduction of the computational costs with respect to carry out a specific calibration procedure for each probability function future developments will concern the implementation within extrastar of specific modules for i improving the basic statistical analysis e g routines for chi square and kolmogorov smirnov tests and using other probability functions ii modelling with non stationary approaches in order to take into account any effect due to climate changes iii second and third levels of statistical regionalization related to coefficient of variation and mean value respectively iv creating shapefiles suitable for a spatial representation into gis softwares v futherly reducing the estimation uncertainty of the shape parameter for example by using advanced methodologies that pool records of different stations see for example iliopoulou et al 2022 software availability name of software extrastar xlsm developers and contact information davide luciano de luca davide deluca unical it francesco napolitano francesco napolitano uniroma1 it year first available 2022 softwares required windows 8 or later versions as operating system os microsoft excel 2013 or later versions os settings dot as decimal separator is mandatory availability https sites google com unical it extrastar home page last accessed on 12 12 2022 cost free program language visual basic for application vba macros in ms excel program size 545 kb declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix 1 the goal of this appendix is to further focus the attention on the meaning of the terms stationary non stationary and change and their consequent use in modelling as regards the hydrological fields this debate in the scientific community is widely started see for examples koutsoyiannis and montanari 2014 lins and cohn 2011 montanari and koutsoyiannis 2014 vidrio sahagún et al 2021 in this context very important questions are is the term stationary a synonym of no change is the term non stationary a synonym of change first of all it should be remarked that the concept of change is not mutually exclusive with the term stationarity koutsoyiannis and montanari 2014 in fact according as examples with newton s laws 1 without an external force the position of a body in motion changes in time but the velocity is unchanged 2 a constant force implies a constant acceleration and a changing velocity consequently change is a concept related to the real world while stationarity and non stationarity only regard the adopted models to explain the observed data nevertheless some works in literature wrongly state that observed time series are stationary or non stationary and these conclusions are usually based on results from trend tests which are often applied to data sets with a short sample size serinaldi et al 2018 in detail trends in historical series of data are usually investigated with parametric and non parametric approaches among them the non parametric mann kendall mk statistical test mann 1945 kendall 1975 is frequently used to quantify the significance of trends see also von brömssen et al 2021 and it was widely recommended by the world meteorological organization wmo mitchell et al 1996 consequently another crucial question is is a result of mk test fully informative for establishing a stationary or non stationary modelling and or an unchanging or a changing climate scenario the correct reply has to consider the concept of ergodicity koutsoyiannis and montanari 2014 that implies the possibility to infer summary statistics from the only one available realization i e the observed series specifically if the process i e the used mathematical model to explain the data is considered as non stationary the ergodicity cannot hold see birkhoff 1931 mackey 1992 koutsoyiannis and montanari 2014 thus making inference from data theoretically impossible this implies that in a non stationary context the model structure i e the relationships among model parameters and predictable covariates cannot result from a direct estimation procedure from the data but need to be a priori defined and then tested based on external results information for example output from climatic models rootzén and katz 2013 therefore when the null hypothesis of no trend can be rejected and without exogenous information mk or other trend tests can only provide a generic indication if further investigation is required moreover a mere data fitting with arbitrary mathematical functions appears as very questionable because these data driven trends could clearly be modified when new observed data are available luke et al 2017 serinaldi et al 2018 then environmental changes can be also modelled with stationary approaches koutsoyiannis and montanari 2014 montanari and koutsoyiannis 2014 in some cases overall a good modelling strategy should firstly consider the simplest approach according to the principle of parsimony in fact using directly a more complex model instead of a simpler one could not be justified without a clear understanding of physical dynamics and or a marked evidence from observed data therefore the stationary framework should represent the default assumption lins and cohn 2011 iliopoulou and koutsoyiannis 2020 and even if there is evidence for non stationarity stationary models should be used as a benchmark for more complex approaches in order to evaluate the magnitude of the improvements although this important theoretical aspect about ergodicity literature contains works related to non stationary inference from data by using the ml method e g durocher et al 2019 lu et al 2019 vu and mishra 2019 chebana and ouarda 2021 xu et al 2021 and well known programming language and software such as extremes r package gilleland and katz 2016 gamlss r package stasinopoulos and rigby 2007 and neva software cheng and aghakouchak 2014 as a confirmation of what was previously stated about the possibility of using stationary approaches into a changing context results from simple numerical experiments are below illustrated specifically starting from an ev1 distribution in terms of y variable eq 9 5000 sets of 100 y data were generated with the monte carlo methodology from eq 9 it is possible to calculate the correspondent values for x variable as a1 1 x t θ t y t l n λ t θ 0 1 α t y t l n λ 0 1 β t i e by considering linear trends for θ and λ where θ 0 and λ 0 are initial values while α and β are the associated trend rates and 1 t n by setting specific values for θ 0 λ 0 α and β and varying the sample size n from 20 to 100 the mann kendall test was applied for each synthetic sample of x for which it is easy to demonstrate see appendix 2 that the statistic z mk does not depend on the initial value θ 0 assuming β 0 which implies the mean annual frequency as invariant in time the percentages of synthetic samples with z m k 1 96 i e the null hypothesis of no trend is rejected at 5 significance level are represented in fig a1 1 for 3 values of trend rate α 10 20 and 50 in 100 years and for different constant values for λ the obtained results highlight that there is a significant percentage of synthetic samples for which the assumed changes for θ does not imply z m k 1 96 and then a stationary modelling could be adopted for statistical analysis in these cases in detail i for a slight trend rate α fig a1 1a a sample size n 100 data could be not sufficient for having z m k 1 96 in any case of λ ii for α 20 in 100 years fig a1 1b the rejection of stationary hypothesis can be obtained with a probability greater than 50 for at least n 90 data and for λ greater than 100 iii for a marked value of the trend rate α fig a1 1c n 60 70 data and λ 20 are necessary for z m k 1 96 with a probability of at least 50 moreover papalexiou and montanari 2019 performed a worldwide analysis of 8730 daily precipitation records focusing on the 1964 2013 period they found that changes in magnitude are not so evident while changes in frequency of heavy events can be stated particularly in large parts of eurasia north australia and the midwestern united states focusing on these results another numerical analysis was carried out in this work by setting a not so high value for α i e 10 in 100 years while three trend rates were assumed for β 10 20 and 50 in 100 years respectively and several λ 0 values were tested also in this second analysis see fig a1 2 there are significant percentages greater than those in fig a1 1 of synthetic samples for which the assumed changes for λ does not imply z m k 1 96 and then a stationary modelling could be used as well two examples of synthetic time series generated from this second theoretical changing world with θ 0 20 mm λ 0 50 and β equal to 50 in 100 years are shown in fig a1 3 also in terms of 30 year moving average 30yma the former is characterized by z m k 1 96 z m k 5 35 and by an evident increase of 30yma while the latter presents z m k 1 96 z m k 0 38 and does not show significant variations for 30yma from the joint analysis with fig a1 2c the probability to observe a realization like that in fig a1 3b for which a stationary approach can be suitable is about 50 focusing on italian territory the investigated area in this work see sect 2 2 it must be highlighted that for a non negligible number of cases especially in the southern part de luca and galasso 2018 no significant trend emerged from the analysis of the observed rainfall am time series at daily and sub daily scales especially due to the occurrence of heavy events in the last century particularly for the daily scale the assumption of stationary am distributions for rainfall series may not be rejected in many cases de luca and galasso 2018 de luca et al 2020 a detailed state of the art on trend analysis for italian am rainfall series is reported in caporali et al 2021 fig a1 1 percentages of synthetic samples from a transient ev1 distribution with z m k 1 96 for β 0 and three values for α a 10 in 100 years b 20 in 100 years c 50 in 100 years fig a1 1 fig a1 2 percentages of synthetic samples from a transient ev1 distribution with z m k 1 96 for different values of β a 10 in 100 years b 20 in 100 years c 50 in 100 years authors fixed α equal to 10 in 100 years in all the three plots fig a1 2 fig a1 3 two examples of synthetic time series obtained from generation with eq a1 1 and θ 0 20 mm λ 0 50 and β equal to 50 in 100 years a a series with z m k 1 96 z m k 5 35 b a series with z m k 1 96 z m k 0 38 fig a1 3 appendix 2 the mann kendall mk test is a non parametric test for identifying trends in time series data it does not require any hypothesis about probability distribution of data and the following quantity s is firstly computed a2 1 s i 1 n 1 j i 1 n s i g n x t j x t i where n is the sample size x t j and x t i are two generic sequential data values at time t j and t i respectively and the function s i g n is defined as follows a2 2 s i g n x t j x t i 1 i f x t j x t i 0 o r e q u i v a l e n t l y x t j x t i 1 0 i f x t j x t i 0 o r e q u i v a l e n t l y x t j x t i 1 1 i f x t j x t i 0 o r e q u i v a l e n t l y x t j x t i 1 focusing on the possibility of calculating s i g n with x t j x t i from eq a1 1 this ratio becomes a2 3 x t j x t i θ 0 1 α t j y t j l n λ 0 1 β t j θ 0 1 α t i y t i l n λ 0 1 β t i 1 α t j y t j l n λ 0 1 β t j 1 α t i y t i l n λ 0 1 β t i and then it can be stated that s does not depend on θ 0 s is approximately normally distributed with the expected value e s 0 the variance v a r s is computed as a2 4 v a r s n n 1 2 n 5 k 1 m g k g k 1 2 g k 5 18 where n is the sample size m is the number of tied groups and g k is the number of ties of extent k a tied group is a set of sample data having the same values then for n 10 the standard normal test statistic z m k is evaluated as a2 5 z m k s 1 v a r s i f s 0 0 i f s 0 s 1 v a r s i f s 0 z m k 0 indicates increasing trends while z m k 0 is associated to decreasing trends for a specific significance level α the null hypothesis h 0 of no trend is rejected when z m k z m k 1 α 2 if α 0 05 h 0 is rejected when z m k 1 96 
25450,the software extrastar extremes abacus for statistical regionalization is here proposed it can represent a useful and quick tool for both regional and at site statistical analyses of annual maxima am time series the innovative aspects compared to other algorithms softwares are 1 an easy comparison of sample skewness of several observed series with pre determined monte carlo prediction intervals depending on sample size from hypothesized probability distributions for obtaining a first important indication about possible clusters and regional values for some parameters which can be afterward refined if deemed as necessary with specific and more complex algorithms softwares proposed in literature 2 for at site analyses the possibility to benefit from calibration of one function for some parameters of the others thus significantly reducing the computational costs extrastar was tested with the daily am series for the italian rain gauge network by implementing ev1 gev and tcev distributions keywords rainfall annual maxima extreme value distributions statistical regionalization data availability the used data can be freely downloaded at http 193 206 192 214 servertsdailyutm serietemporalidaily400 php in italian 1 introduction statistical analysis of rainfall and flood extremes and in particular the annual maxima am clearly plays a crucial role in hydrology for both research and technical purposes moccia et al 2021a panda et al 2022 and it can be diversified according to 1 the adopted probability function sect 1 1 2 the type of approach sect 1 2 stationary i e temporal invariance of model parameters or non stationary i e model parameters are time varying to take into account possible effects of climate changes 3 the method for parametric estimation at site or regional based on the sample size sect 1 3 4 the specific technique for parametric estimation sect 1 4 method of moments maximum likelihood least squares and so on therefore a technician has many combinations for statistical analysis of extreme values even in the context of possible climate change effects the following 4 subsections provide a brief literature review for all these four aspects while the goal of the proposed research is reported in sect 1 5 1 1 choice of the probabilistic model the widely used functions in literature are ev1 gumbel 1958 gev generalized extreme value jenkinson 1955 tcev two component extreme value rossi et al 1984 log pearson type iii bobee 1975 the 3 parameter lognormal johnson et al 1994 pp 208 238 the generalized pareto hosking and wallis 1987 johnson et al 1994 p 615 the generalized logistic balakrishnan and leung 1988 moreover other distributions were recently proposed among them we can mention i the burr xii type moccia et al 2021b ii probability functions which are based on non asymptotic approach marani and ignaccolo 2015 lombardo et al 2019 iii specifically for peak discharge time series derived distributions which take into account specific characteristics of rainfall input and geomorphic information of the investigated watershed iacobellis and fiorentino 2000 de michele and salvadori 2002 1 2 stationary and non stationary approaches for any adopted probability function the specific parameters can be considered as invariant i e a stationary approach is used or varying along the time salas and obeysekera 2014 with the goal of modelling effects induced by potential climate change scenarios in the former case the main source of uncertainty is constituted by extrapolation of the adopted probability distribution beyond the range of observed data in the latter case a further source of uncertainty is introduced that is related to the used mathematical expression for modelling the temporal variation of parameters together with its validity into the whole future horizon of interest moreover attention should be focused on ergodicity koutsoyiannis and montanari 2014 that allows for calibration of a stochastic process from a single realization i e the observed one if the process is non stationary then ergodicity cannot hold and then the temporal variation of parameters should be estimated by only using external data sets for example outputs from climatic models rootzén and katz 2013 at the end of sect 2 1 and more in detail in appendix 1 of this paper authors discussed about this topic and the relationships among the potential effects of climate change scenarios and a stationary non stationary modelling 1 3 at site and regional estimation of parameters in any case integration with other data sets can be necessary for both stationary and non stationary contexts if sample size n is not sufficient less than 10 20 data for obtaining robust at site estimates of parameters and of quantiles associated to larger return periods t then approaches of statistical regionalization can be carried out schaefer 1990 nguyen et al 2002 in this context the index flood method dalrymple 1960 regression analysis and geostatistical procedures represent the most adopted methodologies smith et al 2015 in particular the coupled use of the index flood method with l moments hosking and wallis 1993 has been widely used in many works abida and ellouze 2008 hussain and pasha 2009 noto and la loggia 2009 saf 2009 seckin et al 2011 laio et al 2011 biondi et al 2012 haddad and rahman 2012 aydogan et al 2016 regional frequency analysis can be usually carried out by identifying homogeneous regions hrs where some theoretical moments are assumed as constant while other can be expressed as functions of specific geomorphic covariates hrs are often identified by using geographical or administrative criteria which can be questionable in terms of hydrological homogeneity burn et al 1997 chebana and ouarda 2007 consequently cluster analysis alem et al 2019 lin and chen 2006 rao and srinivas 2008 yin et al 2016 or artificial intelligence techniques cassalho et al 2019 represent a valid alternative with this goal whatever technique is chosen for hrs identification critical aspects are i the assessment of the plausibility of the homogeneity hypothesis for the proposed regions viglione et al 2007 ii the estimates are not smooth both in geographic or in parameter space due to possible discontinuities consequently approaches that do not define fixed boundary regions stedinger and tasker 1985 griffis and stedinger 2007 borga et al 2005 libertino et al 2018 iliopoulou et al 2022 shehu et al 2022 are receiving an increasing attention chokmani and ouarda 2004 chebana and ouarda 2008 skoien et al 2006 1 4 choice of the specific technique for parameters estimation after establishing the use of an at site or a regional approach depending on the available sample size n the parametric estimation can be carried out by using singh 1998 i method of moments ii method of probability weighted moments pwms iii l moments iv maximum likelihood ml v least squares each mentioned technique is aimed to reproduce specific features of the observed sample singh 1998 kottegoda and rosso 2008 and then the choice is based on which aspects are of interest for a user moreover it is also possible to consider a context of equifinality beven and freer 2001 i e different sets of parameters values for a specific model can reproduce in an acceptable way the investigated time series 1 5 goal of the proposed work based on all the previously discussed aspects a user has many combinations for carrying out the statistical analysis of am time series also for taking into account possible effects from climate changes in this framework this paper describes a quick methodology implemented within the extrastar software extremes abacus for statistical regionalization a ms excel file with macros in vba visual basic for application the innovative aspects with respect to other algorithms softwares of extrastar consist in 1 an easy comparison of the sample size and skewness of several observed time series with pre determined monte carlo prediction intervals from hypothesized probability distributions in order to obtain a first important indication about possible clusters and regional values for some parameters which can be clearly refined afterward if it is deemed as necessary by using specific algorithms softwares proposed in literature regarding for example cluster analysis artificial intelligence techniques and suitable techniques of parametric estimation for hierarchical approaches see sect 1 3 starting from extrastar outcomes 2 for at site statistical analyses the possibility to do not carry out a specific calibration for each adopted probability distribution but to take advantage of obtained estimates of one function for some parameters of the other ones due to the choice of ms excel platform extrastar can be used by students technicians but obviously also by researchers the interface is user friendly but as also illustrated in the next pages it is clear that a background in statistical analysis of hydrologic variables is necessary in order to well understand the exstrastar outputs it is possible to download the software at the website https sites google com unical it extrastar home page last accessed on 12 12 2022 together with user s manual and video tutorials fig 1 extrastar was tested for the italian network of annual maxima am of daily rainfall time series related to the scia database of the italian institute for environmental protection and research ispra www scia isprambiente it last accessed on 12 12 2022 the obtained results have shown that extrastar can be considered as a useful tool for obtaining quick indications about possible clusters of different time series in statistically homogeneous areas as well as a reduction in the computational cost for at site parametric estimates the paper is organized with the following sections brief theoretical notes about the probability functions implemented in extrastar and the used data set sect 2 an overview of the software sect 3 discussion of the results sect 4 conclusions sect 5 2 methods and materials 2 1 theoretical background on implemented probability functions statistical analysis of hydrological extremes usually regards the time series of annual maxima am in this case the time series is composed by the maximum value observed in each year peaks over a threshold pot in which the time series comprises all the peak values that exceed a pre fixed threshold zhang et al 2021 by applying the theorem of total probability the well known relationship among am and pot series is obtained from which the asymptotic extreme value theory is derived todorovic 1970 de michele 2019 1 f x x n 0 p n n f x p o t x x 0 n where f x x is the cumulative distribution function cdf of am series and represents the probability that the random variable x assumes values no greater than x f x p o t x x 0 is the cdf of the pot series and x 0 indicates the threshold value above it a peak x belongs to the time series p n n represents the probability associated to n exeedances of the threshold in one year from eq 1 it is possible to obtain several expressions for f x x depending on the specific adopted mathematical formulas for p n n and f x p o t x x 0 in a stationary context if p n n is assumed as poisson distributed 2 p n n λ n e λ n in which λ represents the mean annual number of exeedances then eq 1 becomes 3 f x x e λ 1 f x p o t x x 0 if the exponential distribution is adopted for f x p o t x x 0 4 f x p o t x x 0 1 e x x 0 θ where θ is the mean annual intensity of the peaks then the well known ev1 probability distribution gumbel 1958 is obtained for eq 1 5a f x x e λ e x x 0 θ e e 1 θ x x 0 θ ln λ e e α x ε with α 1 θ and ε θ ln λ x 0 when we set x 0 0 todorovic and zelenhasic 1970 rossi et al 1984 eq 5a can be rewritten as 5b f x x e λ e x θ e e 1 θ x θ ln λ e e α x ε preserving the assumption of p n n as poisson distributed eq 1 it is possible to demonstrate that by using a generalized pareto wang 1991 for f x p o t x x 0 eq 1 becomes the gev generalized extreme value jenkinson 1955 6 f x x e 1 b x θ ln λ θ 1 b e 1 b α x ε 1 b in which b corresponds to the shape parameter for b 0 the gev distribution coincides with ev1 function ev2 and ev3 laws are obtained when b 0 and b 0 respectively it must be remarked that the k th moment of the gev distribution exists if b 1 k e g the mean exists if b 1 the variance if b 1 2 the skewness if b 1 3 gupta 2011 if a mixture of two exponential distributions is adopted for f x p o t x x 0 then eq 1 corresponds to tcev two component extreme value rossi et al 1984 probability function 7a f x x e λ 1 e x θ 1 λ 2 e x θ 2 in which λ 1 and λ 2 with λ 1 λ 2 are the mean annual number for ordinary and outlier events čampulová et al 2022 respectively while θ 1 and θ 2 with θ 1 θ 2 are the correspondent mean values for intensities a well known tcev formulation used in contexts of statistical regionalization is obtained by introducing two dimensionless parameters θ θ 2 θ 1 λ λ 2 λ 1 1 θ with θ 1 7b f x x e λ 1 e x θ 1 λ λ 1 1 θ e x θ θ 1 from eqs 5 7 it can be noted that application of asymptotic extreme value theory does not require the explicit definition of the threshold x 0 and of f x p o t x x 0 which can be a priori unknown in fact parameters of f x x can be directly estimated by data fitting of am time series however the asymptotic approach could be questionable from a physical point of view as n seems unrealistic for example the daily scale is characterized by n 365 de michele 2019 in this context as mentioned into the introduction non asymptotic approaches were proposed in literature marani and ignaccolo 2015 zorzetto et al 2016 lombardo et al 2019 which are based on the so called penultimate assumption gomes 1984 nevertheless these approaches are currently not so used because they request the definition of f x p o t x x 0 which is difficult to determine in practical cases de michele 2019 obviously an ad hoc procedure for statistical regionalization can be developed for any probability distribution ferrari and versace 1994 for the implemented models in extrastar the reduced ev1 variable rossi et al 1984 is considered 8 y x θ ln λ θ α x ε x θ 1 ln λ 1 θ 1 as y is a linear transformation of x these two random variables clearly present the same value for skewness then it is possible to rewrite eqs 5 7 as 9 e v 1 f y y e e y 10 g e v f y y e 1 b y 1 b 11 t c e v f y y e e y λ e y θ and to identify hrs where the theoretical skewness and therefore the parameters λ and θ for the tcev beran et al 1986 or the parameter b with b 0 in the case of ev1 for the gev can be assumed as constant gabriele and arnell 1991 or they follow laws based on geomorphological covariates altitude hydrographic basin area etc as an example fig 2 shows the comparison among the cdf expressed by eqs 9 11 on an ev1 probabilistic plot kottegoda and rosso 2008 by considering some values for b of gev and for λ and θ of tcev into a stationary context i e invariance in time for the parameters from fig 2 it is clear that for a fixed value of f y y i e a fixed value of ln ln f y y on the vertical axis the correspondent value y of the quantile increases from an ev1 distribution to a gev with b 0 or tcev with growing λ and θ thus obtaining distributions which are more skewed with respect to ev1 the developed methodology in extrastar useful for either at site or regional approaches is based on the following steps 1 sample sizes n between 20 and 200 were considered for each n and for each distribution 5000 series of the variable y were generated using the monte carlo methodology kottegoda and rosso 2008 in detail i for gev the following 6 values of parameter b were considered 0 ev1 0 05 0 1 0 15 0 2 0 25 ii 40 combinations of λ θ were used for tcev with 0 1 λ 0 5 step 0 1 and 1 5 θ 5 step 0 5 overall 46x5000 samples of the standardized variable y were generated for each value of n concerning gev the value 0 25 as the lower limit for b allows for the existence of mean variance and skewness as previously mentioned 2 for each set of 5000 series the two side 90 monte carlo prediction interval mcpi was evaluated for the sample skewness g and therefore 46 mcpis with respect to n can be represented within an abacus implemented within the extrastar software see sect 3 by inserting into the abacus figs 8 10 the information from observed am time series in terms of n and g it is possible to quickly assess which distributions and with reference values of the shape parameters are able to model the sample skewness and then the possible presence of outliers and therefore to cluster some many series in hrs it should be underlined that kurtosis also plays a significant role in these analyses hosking and wallis 1993 laio et al 2011 but authors did not consider this fourth order statistic because of its greater uncertainty with respect to skewness when short samples are analyzed furthermore this stationary methodology can be also useful in climate change contexts iliopoulou and koutsoyiannis 2020 mainly for specific time resolutions for which the hypothesis of stationarity could not be rejected in many cases see appendix 1 for further details obviously the generalization of eqs 5 11 to a non stationary modelling i e setting the parameters set φ λ t θ t or φ λ t θ t b t or φ λ 1 t θ 1 t λ 2 t θ 2 t or φ λ 1 t θ 1 t λ t θ t is straightforward however as thoroughly discussed in appendix 1 authors highlight that the possibility of using mainly at daily scale a stationary modelling in a changing climate does not exclude the adoption of a non stationary approach which should be necessarily calibrated with external information in order to do not violate the ergodicity koutsoyiannis and montanari 2014 in this context a very interesting web tool is proposed in simonovic et al 2016 in which a user can generate intensity duration frequency curves that account for future climate conditions forecasted by general circulation models gcms extrastar software currently allows for a quick stationary at site or regional statistical analysis only in terms of sample skewness but future developments will enable quick regional investigations for sample mean and standard deviation of x together with the implementation of specific modules for a non stationary modelling the choice of firstly using a stationary modelling is justified in appendix 1 moreover authors put in evidence that the quick regional evaluation for b or λ and θ from extrastar abacus must be considered as a first important indication about values that can be assumed clearly specific algorithms can be furtherly adopted by users for refining the estimates rossi et al 1984 gabriele and arnell 1991 and for the improvement of hrs identification by applying k means or other clustering models alem et al 2019 lin and chen 2006 rao and srinivas 2008 yin et al 2016 or else artificial intelligence techniques cassalho et al 2019 starting from extrastar outcomes 2 2 data set in this work authors considered the scia database of the italian institute for environmental protection and research ispra http www scia isprambiente it last accessed on 12 12 2022 and in particular the network of daily am rainfall series fig 3 specifically the whole national rain gauge network is reported in fig 3a but only a subset that affects about 80 of the whole territory is made available in scia by the several regional local agencies which manage the data overall it was possible to analyze 3351 samples with n 20 years in the time interval 1860 2020 fig 3b the main areas with no data in scia database or with n 20 years comprise many parts of piedmont and lombardia regions in northern italy and campania basilicata and apulia regions for the southern zone the mean value of n is approximately 50 years with a maximum of 146 years milan rain gauge the values of g vary from 0 71 to 5 10 with an average of 1 34 the scatterplots of sample skewness with sample sizes and elevation of rain gauges are illustrated in fig 4 in particular the elevation range 0 500 m a s l is characterized by a significant number of time series with sample skewness g 3 fig 4b from the numerical experiments shown in appendix 1 also based on the work of papalexiou and montanari 2019 for the daily scale it is clear that without any external information for example outputs from climatic models the values of sample size n allow for adoption of a stationary approach for the investigated time series 3 overview of the extrastar software when a user executes extrastar xlsm after having enabled the vba macros the home worksheet will appear as in fig 5 and it is possible to select the kind of analysis single time series i e an at site investigation or ensemble of time series i e a statistical regionalization by clicking on the command button start in addition extrastar contains the following worksheets input data n and g in which a user can upload the text file with the information about sample size n and sample skewness g of many rain gauges in order to carry out a statistical regionalization single time series that allows for uploading and analyzing the am data of a single rain gauge ev1 plot where the graphical results of statistical analysis for a single series are shown abacus in which a user can visualize the abacus aimed at the quick statistical regionalization abacus data that contains the values of the two side 90 mcpis in terms of sample skewness g for specific values of sample size n and for all the investigated options for ev1 gev and tcev distributions 3 1 ensemble of time series statistical regionalization the structure of input data n and g worksheet is represented in fig 6 the right part contains a very brief theoretical reminder about the reduced variable y the mathematical expressions of the implemented cdfs and the well known equivalence between the skewness coefficients of x and y variables the command button input file n g allows for uploading a two column text file without headers fig 7 in which the sample size n and the sample skewness g for all the investigated rain gauges are reported in the first and second column respectively these values are then pasted on the worksheet columns a and b afterward a user can visualize the information about n and g by clicking on the command button go to abacus the worksheet abacus will be shown fig 8 in which the scatterplot of the sample points n g is represented together with as default the two side 90 mcpi of ev1 distribution see sect 2 1 when a user clicks in the run gev and tcev options command button he can select with a dedicated form fig 9 the specific values of interest for the shape parameter b gev distribution and for the parameters λ θ tcev distribution for which the correspondent two side 90 mcpis will be visualized fig 10 the abacus is structured in such a way as to represent only one option at a time for both gev and tcev in order to facilitate the visualization of the results as a very quick guidance for users without a strong background in extreme value theory high values of sample skewness are better modelled with low b 0 for gev and with growing values mainly for θ when a tcev function is adopted beran et al 1986 3 2 single time series at site statistical analysis as previously mentioned extrastar also allows for a statistical analysis of a single time series the choice of this option fig 5 shows the worksheet named single time series and then a user can upload a 2 column txt file in which there is not any header the first column is related to the years and the second contains the rain data by clicking on the command button input data from txt file fig 11 after the data upload the running code calculates sample size mean standard deviation skewness and the statistic z m k value related to the mann kendall test for trend analysis see appendix 2 into the cells interval j1 j6 fig 12 and visualizes the correspondent chronological diagram the click on the command button am modelling shows a dedicated form fig 13 in which it possible 1 to select a stationary or a non stationary approach for the statistical analysis the code for the latter option is under construction 2 to carry out the parametric estimation of ev1 distribution by using the maximum likelihood ml method and to select specific options for gev and tcev fig 14 as previously remarked low values of b 0 for gev and very high values of θ for tcev beran et al 1986 should be preferred for samples that are strongly skewed i e g 3 on the contrary the options comprising 0 1 b 0 and θ 3 can be considered 3 to visualize the fitting of ev1 gev and tcev distributions on an ev1 plot fig 15 by clicking on the command button plot 4 to compare the two side 90 mcpis of each distribution with the sample data fig 16 by clicking on the command button monte carlo simulation see sect 2 1 in order to better evaluate the possibility of non rejection of each considered probability function if the sample falls inside the associated mcpi when many distributions cannot be rejected a user can choice the model with the small number of parameters into a context of parsimony or for a fixed type of distribution the parameter option with the lowest value of skewness see the previous step 2 this mcpi analysis represents a statistical test like chi square and kolmogorov smirnov tests kottegoda and rosso 2008 which can be clearly implemented in extrastar as future developments of the software 4 results and discussion from the statistical regionalization carried out by using scia database figs 6 10 the main results can be summarized as follows 1 almost 80 of the analyzed cases i e 2679 samples can be modelled with the ev1 model 2 concerning gev distribution the use of the shape parameter b 0 1 provides the best modeling among all the considered values for b 88 7 of cases equal to 2971 sample series moreover 2526 series from this subset i e the 75 4 of the whole data set can be also reproduced with ev1 3 the sample skewness of 3064 time series 91 4 of the whole data set can be reconstructed through the tcev function with λ 0 1 and θ 3 from this subset 2520 series 75 2 of the whole data set can be also modelled with ev1 and 2961 series 88 4 of the whole data set with gev distribution with b 0 1 the comparison between the possible uses for ev1 and tcev with λ 0 1 and θ 3 is represented in fig 17 in terms of geographic location and in fig 18 in terms of scatterplots between sample skewness g and rain gauge elevation tcev model clearly improves the modelling of time series with large values of sample skewness mainly located into the elevation range 0 100 m a s l but it should be rejected for some rain gauges with g 1 for which ev1 is more suitable however focusing on ev1 performances some global scale rainfall analyses e g papalexiou and koutsoyiannis 2013 serinaldi and kilsby 2014 koutsoyiannis 2004 pointed to the prevalence of ev2 i e gev with b 0 with respect to ev1 but this behaviour is often masked by the short record lengths available and may falsely appear as ev1 papalexiou and koutsoyiannis 2013 and serinaldi and kilsby 2014 also proposed methods for correcting the sample size bias for the shape parameter b overall the rain gauges for which tcev modelling is suitable with regional values λ 0 1 and θ 3 are uniformly distributed on the considered spatial domain green dots in fig 17 taking into account the areas with no available data in scia database sect 2 2 adoption of tcev with λ 0 1 and θ 3 is particularly suitable for sardinia and sicily islands for calabria region in southern italy and for the whole central and north eastern parts then it could be possible to assume a unique hr that comprises these investigated territories and to state that tcev with different sets for λ θ or ev1 could be considered for other isolated rain gauges red dots in fig 17 clearly further analyses concerning as example quality check of data could be necessary in order to explain justify sample skewness less than 0 for which ev1 distribution is not even appropriate fig 18a as well as integration with other databases of daily am series for completing the investigation for the whole italian territory it should be remarked that this carried out procedure is related to a first level of a hierarchical regional approach related to skewness but successive levels of analysis can be obviously implemented inside extrastar a second level of regionalization allows for identifying inside each hr homogeneous sub regions hsrs which are characterized by constant values or depending on geomorphic covariates with specific mathematical relationships for the coefficient of variation cov moreover a third level can be developed in which each hsr can be furtherly subdivided into homogeneous areas has where the mean value can be assumed as constant or as a function of some covariates for italian territory a hierarchical approach was developed in the last decades for flood and rainfall extremes see http www idrologia polito it gndci vapi htm in italian last accessed on 12 12 2022 each regional agency carried out this analysis for its own database a revision of the results is clearly necessary by using the updated databases and by extending the analysis at national scale as suggested in this work as an example of application for a single daily am time series see figs 11 16 authors focused on subiaco rain gauge close to rome characterized by n 103 g 1 53 and a mean value of 68 8 mm with a standard deviation of 21 1 mm as z m k 1 39 it is possible to do not reject the hypothesis of a stationary process with a significance level of 5 by inserting the information n 103 g 1 53 on the exstrastar abacus it emerges that subiaco am series can be easily modelled with ev1 and with gev b 0 1 and tcev λ 0 1 and θ 3 the maximum likelihood ml estimates for ev1 distribution provided θ 15 2 mm and λ 50 7 figs 14 and 15 the application of ml technique usually allows for an excellent fitting of the data with respect to ordinary values with the adoption of these estimates also for the corresponding parameters of gev and for the ordinary component of tcev the obtained fittings fig 15 would induce to consider ev1 and tcev as inadequate on the contrary if 5000 synthetic monte carlo generations each one with n 103 are carried out for each considered probabilistic functions and the corresponding two side 90 mcpis are estimated each distribution may not be rejected for the modelling of the investigated sample series fig 16 thus confirming what is hypothesized with the abacus analysis obviously in a context of parsimony a 2 parameter distribution should be preferred overall the proposed methodology could clearly be considered as a useful tool for very quick statistical analysis with an undoubted reduction of computational costs in fact for an at site analysis it is not necessary to carry out the specific calibration for each distribution but it can be only performed for ev1 specifically the obtained ml estimates for ev1 can be also used for gev and tcev for the correspondences see eqs 9 11 while the values of the remaining parameters b in the gev case and λ and θ in the tcev case can be set from analysis of extrastar abacus moreover the use of abacus for an ensemble analysis allows for easily obtaining indications about potential clusters of time series in homogeneous regions concerning the skewness as previously mentioned at the end of sect 2 1 these quick regional evaluations for b or λ and θ from extrastar abacus must be considered as a first important indication about their plausible values and then a refinement can be carried out by using suitable algorithms rossi et al 1984 gabriele and arnell 1991 which can be implemented in extrastar as future developments obviously hrs identification can be also improved if it is deemed as necessary as well by using for example clustering or artificial intelligence techniques alem et al 2019 cassalho et al 2019 lin and chen 2006 rao and srinivas 2008 yin et al 2016 starting from extrastar outcomes 5 conclusions the extrastar software can undoubtedly represent a user friendly tool for the statistical analysis regional or at site of annual maxima for an observed time series of interest it is possible to quickly test the modelling capability for ev1 and gev or tcev also with reference values for the shape parameters b for gev and λ and θ for tcev on the basis on the sample size and skewness furthermore by simultaneously analyzing the sample skewness of several time series a user can obtain a rapid indication about potential clusters in homogeneous regions another important aspect is the possibility of using ev1 maximum likelihood estimates also for gev and for the ordinary component of tcev which allows for a reduction of the computational costs with respect to carry out a specific calibration procedure for each probability function future developments will concern the implementation within extrastar of specific modules for i improving the basic statistical analysis e g routines for chi square and kolmogorov smirnov tests and using other probability functions ii modelling with non stationary approaches in order to take into account any effect due to climate changes iii second and third levels of statistical regionalization related to coefficient of variation and mean value respectively iv creating shapefiles suitable for a spatial representation into gis softwares v futherly reducing the estimation uncertainty of the shape parameter for example by using advanced methodologies that pool records of different stations see for example iliopoulou et al 2022 software availability name of software extrastar xlsm developers and contact information davide luciano de luca davide deluca unical it francesco napolitano francesco napolitano uniroma1 it year first available 2022 softwares required windows 8 or later versions as operating system os microsoft excel 2013 or later versions os settings dot as decimal separator is mandatory availability https sites google com unical it extrastar home page last accessed on 12 12 2022 cost free program language visual basic for application vba macros in ms excel program size 545 kb declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix 1 the goal of this appendix is to further focus the attention on the meaning of the terms stationary non stationary and change and their consequent use in modelling as regards the hydrological fields this debate in the scientific community is widely started see for examples koutsoyiannis and montanari 2014 lins and cohn 2011 montanari and koutsoyiannis 2014 vidrio sahagún et al 2021 in this context very important questions are is the term stationary a synonym of no change is the term non stationary a synonym of change first of all it should be remarked that the concept of change is not mutually exclusive with the term stationarity koutsoyiannis and montanari 2014 in fact according as examples with newton s laws 1 without an external force the position of a body in motion changes in time but the velocity is unchanged 2 a constant force implies a constant acceleration and a changing velocity consequently change is a concept related to the real world while stationarity and non stationarity only regard the adopted models to explain the observed data nevertheless some works in literature wrongly state that observed time series are stationary or non stationary and these conclusions are usually based on results from trend tests which are often applied to data sets with a short sample size serinaldi et al 2018 in detail trends in historical series of data are usually investigated with parametric and non parametric approaches among them the non parametric mann kendall mk statistical test mann 1945 kendall 1975 is frequently used to quantify the significance of trends see also von brömssen et al 2021 and it was widely recommended by the world meteorological organization wmo mitchell et al 1996 consequently another crucial question is is a result of mk test fully informative for establishing a stationary or non stationary modelling and or an unchanging or a changing climate scenario the correct reply has to consider the concept of ergodicity koutsoyiannis and montanari 2014 that implies the possibility to infer summary statistics from the only one available realization i e the observed series specifically if the process i e the used mathematical model to explain the data is considered as non stationary the ergodicity cannot hold see birkhoff 1931 mackey 1992 koutsoyiannis and montanari 2014 thus making inference from data theoretically impossible this implies that in a non stationary context the model structure i e the relationships among model parameters and predictable covariates cannot result from a direct estimation procedure from the data but need to be a priori defined and then tested based on external results information for example output from climatic models rootzén and katz 2013 therefore when the null hypothesis of no trend can be rejected and without exogenous information mk or other trend tests can only provide a generic indication if further investigation is required moreover a mere data fitting with arbitrary mathematical functions appears as very questionable because these data driven trends could clearly be modified when new observed data are available luke et al 2017 serinaldi et al 2018 then environmental changes can be also modelled with stationary approaches koutsoyiannis and montanari 2014 montanari and koutsoyiannis 2014 in some cases overall a good modelling strategy should firstly consider the simplest approach according to the principle of parsimony in fact using directly a more complex model instead of a simpler one could not be justified without a clear understanding of physical dynamics and or a marked evidence from observed data therefore the stationary framework should represent the default assumption lins and cohn 2011 iliopoulou and koutsoyiannis 2020 and even if there is evidence for non stationarity stationary models should be used as a benchmark for more complex approaches in order to evaluate the magnitude of the improvements although this important theoretical aspect about ergodicity literature contains works related to non stationary inference from data by using the ml method e g durocher et al 2019 lu et al 2019 vu and mishra 2019 chebana and ouarda 2021 xu et al 2021 and well known programming language and software such as extremes r package gilleland and katz 2016 gamlss r package stasinopoulos and rigby 2007 and neva software cheng and aghakouchak 2014 as a confirmation of what was previously stated about the possibility of using stationary approaches into a changing context results from simple numerical experiments are below illustrated specifically starting from an ev1 distribution in terms of y variable eq 9 5000 sets of 100 y data were generated with the monte carlo methodology from eq 9 it is possible to calculate the correspondent values for x variable as a1 1 x t θ t y t l n λ t θ 0 1 α t y t l n λ 0 1 β t i e by considering linear trends for θ and λ where θ 0 and λ 0 are initial values while α and β are the associated trend rates and 1 t n by setting specific values for θ 0 λ 0 α and β and varying the sample size n from 20 to 100 the mann kendall test was applied for each synthetic sample of x for which it is easy to demonstrate see appendix 2 that the statistic z mk does not depend on the initial value θ 0 assuming β 0 which implies the mean annual frequency as invariant in time the percentages of synthetic samples with z m k 1 96 i e the null hypothesis of no trend is rejected at 5 significance level are represented in fig a1 1 for 3 values of trend rate α 10 20 and 50 in 100 years and for different constant values for λ the obtained results highlight that there is a significant percentage of synthetic samples for which the assumed changes for θ does not imply z m k 1 96 and then a stationary modelling could be adopted for statistical analysis in these cases in detail i for a slight trend rate α fig a1 1a a sample size n 100 data could be not sufficient for having z m k 1 96 in any case of λ ii for α 20 in 100 years fig a1 1b the rejection of stationary hypothesis can be obtained with a probability greater than 50 for at least n 90 data and for λ greater than 100 iii for a marked value of the trend rate α fig a1 1c n 60 70 data and λ 20 are necessary for z m k 1 96 with a probability of at least 50 moreover papalexiou and montanari 2019 performed a worldwide analysis of 8730 daily precipitation records focusing on the 1964 2013 period they found that changes in magnitude are not so evident while changes in frequency of heavy events can be stated particularly in large parts of eurasia north australia and the midwestern united states focusing on these results another numerical analysis was carried out in this work by setting a not so high value for α i e 10 in 100 years while three trend rates were assumed for β 10 20 and 50 in 100 years respectively and several λ 0 values were tested also in this second analysis see fig a1 2 there are significant percentages greater than those in fig a1 1 of synthetic samples for which the assumed changes for λ does not imply z m k 1 96 and then a stationary modelling could be used as well two examples of synthetic time series generated from this second theoretical changing world with θ 0 20 mm λ 0 50 and β equal to 50 in 100 years are shown in fig a1 3 also in terms of 30 year moving average 30yma the former is characterized by z m k 1 96 z m k 5 35 and by an evident increase of 30yma while the latter presents z m k 1 96 z m k 0 38 and does not show significant variations for 30yma from the joint analysis with fig a1 2c the probability to observe a realization like that in fig a1 3b for which a stationary approach can be suitable is about 50 focusing on italian territory the investigated area in this work see sect 2 2 it must be highlighted that for a non negligible number of cases especially in the southern part de luca and galasso 2018 no significant trend emerged from the analysis of the observed rainfall am time series at daily and sub daily scales especially due to the occurrence of heavy events in the last century particularly for the daily scale the assumption of stationary am distributions for rainfall series may not be rejected in many cases de luca and galasso 2018 de luca et al 2020 a detailed state of the art on trend analysis for italian am rainfall series is reported in caporali et al 2021 fig a1 1 percentages of synthetic samples from a transient ev1 distribution with z m k 1 96 for β 0 and three values for α a 10 in 100 years b 20 in 100 years c 50 in 100 years fig a1 1 fig a1 2 percentages of synthetic samples from a transient ev1 distribution with z m k 1 96 for different values of β a 10 in 100 years b 20 in 100 years c 50 in 100 years authors fixed α equal to 10 in 100 years in all the three plots fig a1 2 fig a1 3 two examples of synthetic time series obtained from generation with eq a1 1 and θ 0 20 mm λ 0 50 and β equal to 50 in 100 years a a series with z m k 1 96 z m k 5 35 b a series with z m k 1 96 z m k 0 38 fig a1 3 appendix 2 the mann kendall mk test is a non parametric test for identifying trends in time series data it does not require any hypothesis about probability distribution of data and the following quantity s is firstly computed a2 1 s i 1 n 1 j i 1 n s i g n x t j x t i where n is the sample size x t j and x t i are two generic sequential data values at time t j and t i respectively and the function s i g n is defined as follows a2 2 s i g n x t j x t i 1 i f x t j x t i 0 o r e q u i v a l e n t l y x t j x t i 1 0 i f x t j x t i 0 o r e q u i v a l e n t l y x t j x t i 1 1 i f x t j x t i 0 o r e q u i v a l e n t l y x t j x t i 1 focusing on the possibility of calculating s i g n with x t j x t i from eq a1 1 this ratio becomes a2 3 x t j x t i θ 0 1 α t j y t j l n λ 0 1 β t j θ 0 1 α t i y t i l n λ 0 1 β t i 1 α t j y t j l n λ 0 1 β t j 1 α t i y t i l n λ 0 1 β t i and then it can be stated that s does not depend on θ 0 s is approximately normally distributed with the expected value e s 0 the variance v a r s is computed as a2 4 v a r s n n 1 2 n 5 k 1 m g k g k 1 2 g k 5 18 where n is the sample size m is the number of tied groups and g k is the number of ties of extent k a tied group is a set of sample data having the same values then for n 10 the standard normal test statistic z m k is evaluated as a2 5 z m k s 1 v a r s i f s 0 0 i f s 0 s 1 v a r s i f s 0 z m k 0 indicates increasing trends while z m k 0 is associated to decreasing trends for a specific significance level α the null hypothesis h 0 of no trend is rejected when z m k z m k 1 α 2 if α 0 05 h 0 is rejected when z m k 1 96 
25451,this study presents an interactive web based approach for modeling rainfall induced landslide susceptibility using free open source software foss the design is based on the r statistical framework and shiny package coupled with the shallow slope stability model shalstab from saga gis the easy to use real time application extends the potential of current modeling efforts to non expert r and gis users and can also be used in an educational context for classroom teaching activity and enabling research informed learning the parsimonious approach i e few parameter inputs is accomplished in two sequential steps including modeling and validation by the use of site specific datasets the approach was tested in a case study on the clearwater national forest and the results from the validation showed an overall accuracy of 0 894 kappa of 0 789 and auc from roc curve was 0 715 the modeled landslide potential may be used as a decision support tool for local planning keywords shalstab landslide susceptibility shiny slope stability web based modeling data availability the example data is included with the shiny app available at https geogis bgsu edu apps shalstabcv 1 introduction rainfall induced shallow landslides are the most common gravitational mass movements caused under conditions of transient infiltration into initially unsaturated soils baum et al 2010 lehmann et al 2013 the intense or prolonged rainfall is usually the main triggering mechanism that weakens the soil shear strength by increasing soil water content and pore water pressure the assessment of rainfall induced landslide susceptibility at local and regional scales which is necessary for emergency mitigation and development of effective management strategies is commonly modeled by empirical or statistical techniques which are based on spatial correlations of observed landslides rainfall intensity duration thresholds and environmental attributes atkinson and massari 2011 ayalew and yamagishi 2005 carrara et al 1995 dai and lee 2002 2003 gorsevski et al 2005 2006a 2006b 2010 kavzoglu et al 2015 reichenbach et al 2018 more recent efforts are focused on data mining techniques that are aimed at discovering patterns and relationships in large datasets these techniques produce high accuracies and can integrate a variety of models including statistical techniques fuzzy sets classification and regression trees artificial neural networks random forest models and support vector machines among others gorsevski 2021 gorsevski et al 2016 gorsevski and jankowski 2008 kavzoglu et al 2015 micheletti et al 2014 saro et al 2016 data driven approaches as well as traditional empirical approaches often rely on the assumption that knowledge derived from past failures can be used for predicting future failure conditions however scenarios that cause landslide failure are altered and changing due to climatic effects and need to be addressed by using physical laws hürlimann et al 2022 at present altered situations are addressed by widespread deployment of artificial intelligence techniques which accelerate discovery of physical laws and governing processes that influence instability factors despite all of that the existing physically based models are still an important asset for landslide modeling and understanding new conditions as a result of environmental changes physically based deterministic or probabilistic models which address spatio temporal distributed dynamics of slope stability are based on simple mechanical rules and describe the processes associated with landsliding bathurst et al 2010 gorsevski et al 2006b montgomery et al 1998 montgomery and dietrich 1994 pack et al 1998 as compared to spatially explicit data driven approaches which have strict requirements for satisfying certain data assumptions the physically based models enable parameterization for characterizing essential slope stability processes i e different rainfall or soil scenarios through simple calibration and validation however a major drawback of physically based models is the need for site specific input data that are often not available over broad geographical scales and the need for parameters which can contain various levels of uncertainties carrara et al 2008 medina et al 2021 within the physically based landslide models the shallow slope stability model shalstab is a well known approach that has been widely implemented for mapping patterns of potential shallow slope instability in various applications and research settings montgomery et al 1998 montgomery and dietrich 1994 shalstab represents a deterministic and distributed model that couples an infinite slope stability equation with a steady state hydrological model that is praised for its simplicity and straightforward implementation the implementation of shalstab takes advantage of digital elevation data model dem for robust computing and predicting instability associated with different landscape features that are subjected by different rainstorms and landslide producing landforms the model has been applied to prediction of susceptibility methods of different aspects such as predicting rainfall induced landslides kim et al 2014 comparing to other similar approaches such as the stability index mapping sinmap gorsevski et al 2006b michel et al 2014 pack et al 1998 tarboton 1997 zizioli et al 2013 mapping shallow susceptibility over large areas such as seamless shallow landslide susceptibility map for all of california bellugi 2011 and applying stochastic integration with generalized likelihood uncertainty estimation glue incorporated into shalstab for acquiring parameter uncertainties and confidence intervals beven and binley 1992 jr chuan huang et al 2006 moreover implementation of physically based models as tools for planning and management of slope stability also require significant knowledge and familiarity with the geospatial software and modeling approaches until now very few real time spatial assessment approaches have been developed where decision makers or different non expert planners can alter and input site specific datasets and visualize risk output maps under different environmental settings and projected scenarios remarkably web based applications have become popular method for providing needed assistance especially for non expert gis users since they integrate user friendly interactive environment for solving complex environmental planning problems gorsevski et al 2021 gorsevski and torregrosa 2016 liu et al 2016 mekonnen and gorsevski 2015 such ubiquitous web based environment could provide users with easy access to complex modeling schemes which link gis to spatially distributed physically based models indeed a number of web based approaches have been used for environmental modeling and planning in different areas such as hydrology and water resources borges de amorim et al 2020 erazo ramirez et al 2022 whateley et al 2015 groundwater management glass et al 2018 2022 agriculture redhead et al 2022 su and li 2020 vilas et al 2020 and decision support systems li 2020 liu et al 2016 mekonnen and gorsevski 2015 in recent years there is an increase in the number of web based spatial applications due to the availability of free and open source software for geoinformatics foss4g and the freedom to freely redistribute copies of the modeling tools to the end users within the foss4g framework the simplicity of new open source programming languages such as r and python offer complete environments for development of interactive geospatial web based applications jos é díaz et al 2021 li 2020 whateley et al 2015 in this research a graphical user interface gui for real time assessment of landslide susceptibility is presented the proposed web based framework integrates shiny r https www r project org chang et al 2020 ihaka and gentleman 1996 and shalstab model montgomery and dietrich 1994 which resides in the saga gis system for automated geoscientific analyses geographic information system conrad et al 2015 geoprocessing environment the integrated web based approach that was developed fully within r environment was used to demonstrate rainfall induced landslide susceptibility modeling in a case study of the clearwater national forest cnf in central idaho this paper uses a small subset from the cnf and builds on earlier work by gorsevski et al 2003 2010 the intention of this demonstration is also to highlight a new approach for web based modeling that is inclusive and expands its usability to a wider community i e non r non gis users for performing real time analysis the present paper is organized as follows in section 2 the methods discuss the system architecture study area and example dataset as well as the modeling theory section 3 discusses the implementation of the web application and results which are generated interactively in the cnf while sections 4 focuses on the discussion finally section 5 shows the conclusions and recommendations for future research 2 methods implementation of the web based shalstab user interface involves installation of ubuntu operating system engine x nginx which is the web server different processing software including saga gis r cran rstudio with a number of related packages shiny server for development and deployment of shiny applications and configuration of the client server environment that integrates the framework in the next section the conceptual framework and the system architecture are discussed for the proposed web based shalstab user interface 2 1 system architecture this project implements a client server architecture gorsevski et al 2021 gorsevski and torregrosa 2016 jayawardhana and gorsevski 2019 liu et al 2016 mekonnen and gorsevski 2015 based on open source software using ubuntu operating system https ubuntu com fig 1 the hypertext transfer protocol http communication is used to exchange data between client s and web server s through hypertext markup language html content of files images and query results the secure communication protocol that extends http and encrypts data for a secure transfer over the internet is hypertext transfer protocol secure https within the proposed architecture the reverse proxy is used to transform https into http requests as an additional security layer and for shielding the web server the reverse proxy is the intermediate connection point positioned behind a firewall that forwards requests from web or mobile browser s i e client s to web server s additional functions of the reverse proxy include increased performance through load balancing such as traffic management maintaining security and anonymity for protecting identities of backend web server s and implementing reliable smooth flow of network traffic between client s and server s the encryption protocol is achieved through transport layer security tls or secure sockets layer ssl which is a global standard security technology the secure internet communication is based on asymmetric public key infrastructure that uses private and public keys to encrypt communications the private key decrypts information encrypted by the public key and resides on the owner s web server controlled privately on the other hand the public key is available to everyone who interacts with the web server where the information encrypted by the public key can only be decrypted by the private key as such the security is maintained at all time because only the server with the relevant private key can decode the information in this work the nginx open source https www nginx com reverse proxy server was deployed on a single machine although the system allows for multiple machine deployment the non profit certificate authority ca called let s encrypt https letsencrypt org is a service that provided the free ssl tls certificates while the certbot open source client https certbot eff org was used to fetch the certificate shiny beeley 2016 chang et al 2020 is a relatively recent package by rstudio racine 2012 that provides a gui interface for r cran the comprehensive r archive network ihaka and gentleman 1996 rstudio is a free open source ide integrated development environment for r which is also an open source data analysis language http www r project org the applications produced by the shiny package are called shiny applications which are used for building interactive web applications with r hosting the shiny applications as a web service requires installation of shiny server which enables sharing the applications on the internet beeley 2016 brendel et al 2020 chang et al 2020 li 2020 su and li 2020 su et al 2019 vilas et al 2020 whateley et al 2015 the open source version of shiny server which was used here is characterized by a limited amount of features compared to the shiny server pro version that is not free within the shiny environment the simplest structure of the shiny application is represented by a directory that contains two r scripts including a user interface script ui r which controls the layout and appearance of the application and a server script server r that incorporates instructions for building the application for the user input processing data capabilities functions and outputs written in r language within this environment the extensibility by r packages with pre programmed routines and commands can be used for various scientific applications the current implementation presented in this work used the following r packages shiny shinythemes shinydashboard leaflet mapview raster rsaga maptools rgdal sp rcolorbrewer ggplot2 rmarkdown sf dplyr caret and rocr although the current implementation involves some of the key spatial packages in r i e raster sp and rgdal accessing additional tools from external geospatial software such as saga gis http www saga gis org extends the geoprocessing capabilities and puts forward a robust framework for geocomputation accessing saga s gis functionality from within r is based on executing saga modules via the command line interface using existing python application programming interfaces api so called saga python api that allows full access to all saga algorithms the rsaga brenning 2005 package provides direct access to saga gis functions and geoprocessing environment directly from r for example shalstab is a module within the slope stability library of the saga gis that can be executed directly from r the rsaga functions are executed in r without using saga gis in the background by wrapper functions that contain arguments and specified default values the rsaga env function form rsaga is used for setting the geoprocessing environment in an r script which can automatically detect the installation of saga gis by searching through computer directories and by adding the newest version to the path environment variable in this research rsaga was used to call saga gis through functions such as rsaga geoprocessor rsaga slope rsaga grid calculus and rsaga topdown processing the shalstab module was accessed by the rsaga geoprocessor which is the workhorse function for linking the command line from saga gis with passing arguments from an r script 2 2 study area and example datasets to illustrate the potential of the proposed web based framework a subset area was selected that encompasses papoose badger and squaw creek watersheds located northwest of lowell idaho in the lochsa basin of the cnf fig 2 gorsevski 2002 gorsevski et al 2003 2004 2005 2010 the cnf is situated west of the montana state border and is bounded on three sides by four other national forests the lolo national forest in montana the bitterroot national forest in montana and idaho the nez perce national forest in idaho and the panhandle national forests in idaho the area was impacted by major landslide events during the winter of 1995 96 following heavy rains snowmelt and high river flows landslide occurrence was widely distributed across the region and included disturbed i e clearcuts and roads and undisturbed forests the selected study area was used by gorsevski et al 2003 2004 as a training area in a fuzzy k means classification of continuous landforms in tandem with bayesian probabilistic modeling approach for mapping landslide susceptibility the area is 111 8 km2 with highly dissected mountainous topography with elevation that ranges between 966 m and 2154 m and slopes that vary between 0 and 45 precipitation averages about 1320 mm annually which changes significantly across the elevational gradient most of the annual average precipitation falls as snow during winter and spring while peak stream discharge occurs in late spring and early summer the soils are highly variable but typically well drained and primarily derived from parent materials such as granitics metamorphic rocks quartzites and basalts or surface colluvium the land cover is predominately forested with coniferous species and various other shrubs and grasses that have short growing seasons particularly at the higher elevations a single dem input is the only required datasets for running shalstab while other derivatives such as the slope in radians and a catchment area in m2 are directly computed from the dem the example dem used here are based on 7 5 min u s geological survey dems i e 30 m spatial resolution and correspond to the usgs 7 5 min topographic quadrangle map provided in the universal transverse mercator utm projection and wgs84 datum in addition the validation of the model requires a landslide coverage stored in a point data format i e x y coordinates and an attribute that denotes the binary response for presence or absence the initiation area of each landslide i e the area where the main scarp of the landslide occurred is interpreted as the point representing the presence of a landslide in the entire cnf a total of 865 landslides were assessed through aerial reconnaissance flights and field inventory in july 1996 the landslides interpreted from aerial photos were classified into road related rr which were human induced and non road related nrr landslides which were from natural causes i e 55 rr and 45 nrr in this demonstration the subset dataset contains a total of 96 shallow landslides regardless of previous classification and include both rr and nrr the non landslide areas were used to generate the absence of landslides dataset using random sampling approach 2 3 shalstab model shalstab is a deterministic model that calculates the critical shallow groundwater recharge conditions mm day that can destabilize a slope the model couples cohesionless infinite plane slope stability model and steady state shallow subsurface flow i e hydrological model beven and kirkby 1979 o loughlin 1986 the hydrological model calculates the spatial patterns of soil saturation i e wetness using upslope contributing areas soil transmissivity and local slope with a presumption that the flow infiltrates to a lower conductivity layer and follows topographically determined flow paths montgomery et al 1998 montgomery and dietrich 1988 1994 o loughlin 1986 the critical steady state rainfall q that is predicted to cause instability at each grid cell is solved by the following equation eq 1 q c t s i n θ a b ρ s ρ w 1 tan θ tan φ where topographic terms from the dem include drainage area that contributes subsurface flow a the outflow boundary length i e cell width b and the local slope of the ground surface or angle which is assumed to be parallel to the failure plan θ the rest of the parameters include the saturated bulk density of the soil ρ s the water bulk density is ρ w the angle of internal friction φ and the soil transmissivity t which is given by the product of the soil thickness and water level above the failure plane the proportion of the soil column that is saturated at instability is given by the ratio of the saturated depth h of the soil cover to the total soil depth z that is referred as h z for a given storm the spatial distribution of h z is determined by both hydrologic and topographic ratios the hydrologic ratio captures the magnitude of the precipitation event relative to transmissivity thus larger precipitation events relative to transmissivity most likely would generate larger ground saturation and subsequent instability the topographic ratio models the concentrating runoff and elevating pore pressures using a ratio of the contributing area i e drainage area to the corresponding cell width the topographic ratio governs the relative wetness defined by the hydrologic ratio where steeper slopes produce faster subsurface flow and lower saturation therefore lowland areas and valleys are associated with higher topographic ratio an important assumption that sets the limit on h z is that the failure plane and the shallow subsurface flow are parallel to hillslope such as assumption sets the range of the h z between 0 and 1 i e wetness exceeding 1 represents overland flow and any places with h z 1 are set to be unconditionally stable even under an extreme storm event and excess pore pressures is required for triggering slope instability on the other hand unstable slopes when h z 0 are set to be unconditionally unstable even under dry conditions of the site where soil accumulation is scarce using the range of the h z values that vary between 0 and 1 the critical rainfall values for each grid cell are calculated the slopes correspond to tan θ tan φ for h z 0 and tan θ tan φ 1 ρ s ρ w for h z 1 montgomery 2001 for example the shalstab module in saga gis generates continuous cr mm day raster with limits between unconditionally stable and unstable states which are set to nodata and zero respectively also the tool produces a classified cr grid that is represented by seven stability classes including stable unstable 0 50 mm day 50 100 mm day 100 200 mm day 200 400 mm day and 400 mm day 2 4 validation and assessment different measures of accuracy are used for binary classification which are derived from a confusion matrix also called a contingency table jayawardhana and gorsevski 2019 kuhn et al 2020 markoulidakis et al 2021 sing et al 2020 the square matrix consists of columns and rows that list the number of instances as absolute or relative actual class vs prediction class counts for example fig 3 illustrates that the prediction outcomes are represented by columns and actual or observed values by rows jayawardhana and gorsevski 2019 in the figure label p denotes a positive outcome i e landslide occurrence while n denotes a negative outcome i e non landslide occurrence the four outcomes in the confusion matrix report true positive tp and true negative tn conditions as well as false positive fp or type i error and false negative fn or type ii error conditions for example tp is the number of correctly predicted landslides while fp is the number of incorrectly predicted landslides on the other hand tn is the number of correctly predicted non landslides observations while fn is the number of incorrectly predicted non landslides the measurements from the landslide outcomes in each of the four cells can be used to produce indices that are referred to as sensitivity specificity precision accuracy and f1 score shown in equations 2 6 jayawardhana and gorsevski 2019 markoulidakis et al 2021 eq 2 s e n s i t i v i t y t p t p f n eq 3 s p e c i f i c i t y t n f p t n eq 4 p r e c i s i o n t p t p f p eq 5 a c c u r a c y t p t n t p t n f p f n eq 6 f 1 s c o r e 2 p r e c i s i o n r e c a l l p r e c i s i o n r e c a l l kappa eq 7 is similar to accuracy score but it takes into account the accuracy that would have happened i e expected probability through random predictions kuhn et al 2020 cohen s kappa is the most widely accepted measure of inter rater reliability when the outcome of interest is measured on a nominal categorical scale kappa values range between 0 and 1 but negative values occasionally can occur cohen s kappa of 1 indicates perfect agreement between the raters and 0 indicates that any agreement is due to chance in the example confusion matrix fig 3 the data on the main diagonal of the matrix tp and tn represent the count of agreements and off diagonal data fn and fp represent the count of disagreements eq 7 k a p p a o b s e r v e d a c c u r a c y e x p e c t e d a c c u r a c y 1 e x p e c t e d a c c u r a c y the receiver operating characteristic roc curves gorsevski 2002 2006 2013 are also useful tool in the assessment of the dichotomous performance outcome through graphical representation of different probability thresholds using the relationship between the tp correctly predicted landslides and the fp falsely predicted landslides each point on the roc curve can be tied to a specific decision criterion that represents the amount of risk associated with the accuracy of the prediction this point selection or the cut off point will be different among decision makers even when their roc curves are the same another property of the roc curve is the area under the curve auc that is used as a measure of overall model fit and comparison of different predictive outcomes an ideal model would have an area equal to 1 because then p true positive 1 and p false positive 0 regardless of the cutoff point gorsevski et al 2000 2006 3 results 3 1 web application the web based prototype for mapping landslide susceptibility provides a simple modeling interface for non experienced gis users the easy to use interface is shown in fig 4 which represents the main or the modeling page of the shalstab web based prototype the web based interface can be accessed by web browsers such as internet explorer chrome or mozilla firefox the main components that comprise the prototype are organized under two different themes including a modeling page and a validation page the themes reside in the header which is the horizontal navigation element on the top of the page where the user can switch between modeling and validation options the sidebar on the left hand side is used for navigation and contains the user input widgets where interactive web elements such as sliders buttons selection boxes and check boxes are placed the widgets enable dynamic html content that is generated on the server side and sent to the client for rendering this bidirectional communication between the client and the rserver is supported by reactive programming environment that automatically re executes modeling tasks when there is a change in the user input values i e increase or decrease in value of a parameter the required inputs for mapping landslide susceptibility are shown in the sidebar fig 4 an important simplification in the proposed prototype for enhancing friendliness and support for non experienced gis users is based on a requirement of a single grid input the required input is a dem in a geographic tagged image file format geotiff format provided with wgs84 utm zone projection while the maximum size of the file is limited to 10 mb regardless of spatial resolution such dem data can be downloaded directly in geotiff format from publicly available sources from the u s geological survey usgs the national map viewer tnm viewer https viewer nationalmap gov or the usgs earthexplorer ee https earthexplorer usgs gov spatial data web portals in the modeling page the required parameters initially are set to the default global values assigned by saga gis which include material density g cm3 1 6 hydraulic conductivity m hr 2 7 material friction angle degree 33 0 material thickness m 1 0 and bulk cohesion mpa 0 0 however hydrological and geotechnical parameters should be determined and used from field observations or laboratory tests for site specific applications the rainfall data is not required input by shalstab but observed rainfall from local rain gauge stations can be used to calculate the daily uniform recharge rates scanlon et al 2002 since shalstab is a deterministic model the input values of the parameters are assumed to be uniformly distributed the transparency slider is used for visualization purposes that controls the translucence of the modeled shalstab output and controls clarity of the corresponding basemap or reference map that is underneath the default value is set to 0 7 where a value of 0 0 is fully transparent map while a value of 1 0 represents an opaque map fig 5 illustrates the modeled output generated by uploading the dem from the cnf the classified output of critical shallow groundwater recharge values cr in mm day was generated by the default parameter values the histogram associated with the landslide susceptibility map shows the distribution of raster cell values across the seven classes as well as the no information i e nodata cells the labels of the slope stability classes are displayed in both the legend and the histogram for instance the histogram suggests that the stability class with critical shallow groundwater recharge values that ranges between 0 and 50 mm day is the most abundant followed by the unconditionally unstable areas from the histogram also the unconditionally stable area is smaller than the unconditionally unstable area which suggests high overall susceptibility the drop down choose a layer menu top right in fig 5 offers a selection of the intermediate layers derived from the dem which are required for running the shalstab model the dem derivatives include slope and catchement while additional outputs from the same menu include continuous critical recharge cr and classified critical recharge cr layers that represent the continuous and the discrete solutions of the modeled landslide susceptibility for example the continuous critical recharge cr is represented by a plot with a different continuous curve as well as a legend with customized modeled values and color scale fig 6 shows controls at the top left corner of the map such as the zoom in the zoom out and the interactive layer display that allows users to switch between different layers the types of layers include a selection of a basemap i e only one basemap can be visible at a time and overlays such as the model outputs for example in fig 6 the basemap is shown by a topographic map that contains shaded relief and contours while the overlay is represented by the slope layer that is draped on the top of the shaded relief basemap further enhancements for the visualization can be controlled by adjusting the transparency of the slope layer i e 0 5 or by the interactive layer display that allows to toggle the visibility of the slope layer i e showing hiding by switching to the slope layer the user is presented by the histogram that accompanies the slope layer and a new legend that is color coded to reflect the values in the map for instance the study area is associated with relatively steep slopes whereas the majority of the slopes range between 0 4 and 0 6 radians or 23 34 for better clarity histogram panel can be moved by clicking and dragging so obstructed areas can be exposed for exploration and visualization of slope patterns the validation page can be accessed after the dem grid file was uploaded in the modeling page fig 7 the validation page also requires a single input of landslide initiation areas i e location of the scarps in a comma separated value csv using coordinates that have matching datum and projection as the original dem the required columns and the order of fields in the cvs file include id x y and slides where the id is a sequential number for identifying the landslides the x and y are the easting and northing geographic coordinates and finally the slides are represented in a binary format where 1 is presence and 0 is absence the size of the file is limited to max size of 5 mb the results generated in the validation page are shown in fig 7 after the csv file was uploaded in the figure the continuous cr is overlaid on a topographic shaded relief basemap while the landslides are overlaid on the top of the cr map as points where red color represents absence and green color represents presence the interactive layer display can be used to show and hide the layers or the landslides observations that belong to the binary groups i e presence and absence the range of cr values is shown in the legend which is roughly between 0 and 550 mm day another useful feature is the interactive exploration of cell values when the user scrolls the mouse pointer over the cr layer the actual cr grid values are displayed in the top right hand corner i e layer cr 61 70 such exploration can be used to adjust the cr values threshold slider for representing different cut offs associated with the binary solution the default cr values threshold is set to 200 mm day the confusion matrix in the figure shows the results that are generated by the module the rows in the confusion matrix show the actual data used for the prediction while the columns show the predicted outcomes the roc curve in fig 7 represents a plot of false positive rate x axis and true positive rate y axis and shows the trade off between all possible threshold values for better clarity visualization and selection of potential threshold values the curve is colorized according to cr values with a legend shown at the right hand side of the plot i e the distribution of values matches the cr values thresholds range which is between 0 and 200 mm day the performance of the roc curve is assessed by the auc which represents the degree or measure of separability the auc under the black diagonal line in the figures represent an area of 0 5 which indicates a random classification model or no separability the web based gui provides a module for generating detailed dynamic reports from the shalstab analysis that can be converted to different output formats including pdf html and word files this is accomplished by r markdown and knitr http yihui name knitr that embeds r code within the document and executes the code to generate histograms plots and figures tables and other calculations derived from the analysis xie et al 2020 the reproducible analysis are automatically regenerated when parameters are modified interactively by the user which allows for spatial modeling and testing of multiple landslide susceptibility scenarios for example the reports have a total of three different parts including header top of the document a markdown section that describes the workflow which is written in markdown syntax and the code chunks written in r the header is written in yaml format and contains metadata about the report i e title author date and output for rendering an html or pdf file and different settings can be used for the representation of the outputs within the report such as different fonts or text styles for instance for keeping track of different simulations the report contains the values of the input parameters and basic information about the input raster used in the modeling however the report outputs are customizable and different layouts can be produced programmatically the last two items in the web based gui are options for downloading the output associated with the continuous cr model that can be used for visualization and additional analysis the options for saving the cr model includes a geotiff and a keyhole markup language kml formats both outputs save the projection information that is uploaded with the original dem and csv inputs for example the foss implementation is shown in fig 8 where visualization of the results are displayed in google earth https www google com earth some of the important visualization features in google earth include an interactive 3d environment of overlaid grid model with adjustable transparency zooming in and out and changing perspective i e rotating and tilting at different scales over exaggerated topographic relief the kml is exported with a matching color coded legend placed in the map window on the lower left side for deciphering the low and high susceptibility areas for instance the figure demonstrates that areas with gentle and moderate slopes such as channels ridges and flatter areas are with low susceptibility while mid to upper slope areas especially oriented in northwestern western and southwestern direction are with higher susceptibility however additional analysis and comparison could be generated by analyzing the geotiff output in a specialized geospatial foss packages such as saga or r 3 2 case study modeling and validation the presented shiny web based framework for assessing landslide susceptibility in idaho s cnf was tested using site specific knowledge of the modeling parameters in tandem with an interactive calibration gorsevski 2002 gorsevski et al 2000 2004 the following parameter values along with the dem and the validation dataset were used in the presented analysis material density g cm3 1 6 hydraulic conductivity m hr 1 05 material friction angle degree 37 0 material thickness m 1 15 and bulk cohesion mpa 0 002 the results from the confusion matrix showed that a total of 82 landslides and 87 non landslides were correctly classified as presence or absence from a total of 189 the performance of the predictive model generated the following matrices sensitivity 0 854 specificity 0 935 precision 0 932 recall 0 854 f1 score 0 891 accuracy 0 894 and kappa 0 789 fig 7 for example the specificity value of 0 935 implies that very few non landslides are misclassified or a high proportion of non landslides among all non landslides are classified correctly the sensitivity or recall value of 0 854 is lower which suggest that a larger number of the landslides were misclassified among all landslides the precision value of 0 932 implies that correctness is achieved in the prediction of the non landslides population since very few are misclassified as landslides however in case of higher recall and lower precision the correctness of the landslide population would be higher in this study the model has higher precision but lower recall with an f1 score of 0 891 the f1 score which represents the harmonic mean of precision and recall can be interpreted as a weighted average of the precision and recall similar to the arithmetic mean the f1 score is always between the precision and recall but the importance of precision over recall or vice versa can be adjusted towards both classes an adequate f1 score is associated with low fp falsely predicted landslides and low fn falsely predicted non landslides on the other hand when fp and fn counts are close the overall accuracy measure can be used for the performance the accuracy of 0 894 should be interpreted with caution since different class proportions are associated with the fp and fn counts in order to deal with unbalanced data the kappa statistics should be used since is more informative than overall accuracy performance the evaluation of the kappa statistic which is 0 789 should also be interpreted with caution and compared with the accompanied confusion matrix by considering the importance of landslides vs non landslides predictions also in case of imbalanced classes since kappa statistic compares an observed with an expected accuracy random chance an evaluation of expected accuracy which is directly related to the number of instances of each class is needed for example unlike the overall accuracy kappa statistics allows for models with skewed class distributions to be objectively compared by controlling the expected accuracy however in general a high score in any metric should be regarded with caution when interpreting the metrics and corroborated with the values from the confusion matrix the visualization of the performance measures for classification are available in the reports generated interactively by the module beside roc curve as presented in fig 7 the dynamic reports include additional graphical representation of a classifier s performances which contain precision recall curve averaging across multiple runs curve and peak of average accuracies across a range of cutoffs curve such performance measures are useful for identification and selection of different cutoff values from the roc curve that are used for comparison of various outcomes rather than a single value i e accuracy f1 score kappa in addition the auc i e area between the curve and the x axis is used to summarize the roc curve performance and can be useful for comparisons of different models produced by varying parameters from multiple runs in this particular case study the overall quantitative index of accuracy corresponds to 0 715 auc which suggest that there is a 71 5 chance that the model will be able to distinguish between landslides and non landslides 4 discussion the presented foss web based tool shows a promising approach for providing real time modeling capabilities to non expert gis users as implemented with the shiny framework the web based tool can offer number of advantages to different end users by means of compatibility across platforms i e windows linux or macos accessibility based on minimal software installation requirement which is a single web browser i e internet explorer firefox chrome safari flexibility to use site specific datasets across different regions a possibility for powerful modeling and geovisualization and feasibility for real time collaboration including interactive teaching in both face to face classroom and online settings which could expand diversity of demographic representation in science technology engineering and mathematics stem of women and traditionally underrepresented students access to affordable higher education or successful post graduate careers until now very few real time spatial assessment approaches have been developed where decision makers or different planners can alter and input site specific datasets and visualize risk output maps under different environmental settings and projected scenarios to make the forecasting of landslide susceptibility less dependent on the judgment of spatial modelers there is a potential for local or regional decision makers to generate and update susceptibility maps when necessary or needed the proposed framework provides user friendly web interface for modeling and validation using very few inputs and without the need of familiarity with geospatial software or programming experience in this study the web based approach demonstrated that the simplicity of shalstab could be extended as a rapid assessment tool in addition the tool can be used in response to proliferating landslide susceptibility caused mostly by human disturbances and the new realm of increased extreme weather events which are projected to become even more severe and frequent as a result of climate change effects gariano and guzzetti 2016 under such conditions decision makers are faced with difficult choices for focusing limited resources and prioritizing policy for tackling growing complexity of environmental problems that require quick response periods for instance susceptibility maps for forest planning involving the maintenance obliteration or development of new forest roads in steep mountainous terrain can be generated by site specific input parameters by local planners such assessments may help the planners to determine various criteria of system use considering all possible sets of conditions that could lead to landsliding for example if the local planner knows that recent wildfire endangered the landscape the acceptable susceptibility risk can be adjusted by selecting different threshold values from the roc curve based on knowledge of contextual information however one of the major shortcomings is that the free version of shiny server is single threaded which supports concurrent number of users but with significantly reduced speeds and longer processing times for example in a classroom settings with multiple students this can be disadvantage unless shiny app is distributed and deployed locally on individual student computers which could bypass the external server the size of the input data can also affect the responsiveness and performance associated with the modeling which often requires size limits to be set for the uploads in the case study the limits were set to 10 mb for the dem and 5 mb for the landslides in addition there are a number of limitations with the efficiencies of deterministic approaches as compared to other statistical methods a study from the northern apennines italy suggested that the predictive capability of shalstab was much lower 0 56 auc compared to weight of evidence method and fuzzy logic method which yielded 0 77 and 0 74 auc respectively cervi et al 2010 in this demonstration shalstab produced an accuracy of 0 715 auc which is comparable to previous work in the lochsa basin cnf which used logistic regression and produced accuracy of 0 716 auc gorsevski et al 2000 that said other efforts in the cnf which used classified dataset of non road related nrr and road related rr landslides exhibited higher index of accuracy which corresponded to 0 84 auc for the nrr and 0 80 auc for the rr landslides gorsevski et al 2006 in the same study area the comparison of shalstab to an integrated fuzzy k means bayes theorem and forest service fsmet approaches suggests that spatial predictions from shalstab exhibited lower performances gorsevski et al 2003 in summary the intention of this study was to show the potential of a shiny web based real time modeling and visualization approach for new technological ideas in environmental modeling the flexible methodology shows a potential blueprint for modification and integration with different modules where different strengths and capabilities can be utilized as such different processing chains could integrate multiple functions and algorithms to be joined in a complex processing sequences for example different packages from r can be used to access foss4g such as rsaga which establishes an interface between r and saga gis rqgis which establishes an interface between r and qgis https www qgis org or rgrass an interface between r and grass geographical information system https grass osgeo org 5 conclusions the presented web based framework extends the capabilities of existing spatial models for landslide susceptibility for non r and non gis users by lowering technical hurdles of this web based approach powerful functions are made accessible to a wider user community the importance of the paper sheds light on an interactive modeling and visualization that can facilitate decision making by local planners who are often required to alter the level of risk which depends on different circumstances the case study demonstration showed an implementation of interactive modeling through sequential set of steps that produced an overall accuracy of 0 894 kappa of 0 789 and 0 715 auc interestingly the approach does not offer an optimal solution or a threshold value but local expert knowledge and integration of other susceptibility models can be used in conjunction with the generated outputs by the framework in addition other practical aspects of this r shiny framework can be implemented through an educational context as a teaching tool for presenting advanced modeling concepts to students the attractive and easy to use r shiny gui interface potentially can also initiate curiosity by students and motivation for learning r scripting extend into other statistical methods use advanced analytics and promote collaborative discussions that can elevate the understanding of model implications recommendation for future research and improvements of this prototype include integration with other modules such as the stability index mapping sinmap which is a stochastic and distributed model i e requires the maximum and minimum input parameter values providing tools for interactive downloads and preprocessing of dem datasets adding flexibility to spatial services such as the u s landslide inventory and access to standardized landslide datasets and enabling real time group based spatial decision support systems sdss as a collaborative framework for landslide susceptibility software availability product title interactive web based modeling of shallow landslide susceptibility developer pece v gorsevski contact address school of earth environment society bowling green state university bowling green oh 43403 usa contact email peterg bgsu edu available since 2019 programming languages r saga gis html availability https geogis bgsu edu apps shalstabcv source code https github com pecevg shiny saga landslide cost free declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the author would like to thank the three anonymous reviewers who provided helpful suggestions for excellent additions to the manuscript 
25451,this study presents an interactive web based approach for modeling rainfall induced landslide susceptibility using free open source software foss the design is based on the r statistical framework and shiny package coupled with the shallow slope stability model shalstab from saga gis the easy to use real time application extends the potential of current modeling efforts to non expert r and gis users and can also be used in an educational context for classroom teaching activity and enabling research informed learning the parsimonious approach i e few parameter inputs is accomplished in two sequential steps including modeling and validation by the use of site specific datasets the approach was tested in a case study on the clearwater national forest and the results from the validation showed an overall accuracy of 0 894 kappa of 0 789 and auc from roc curve was 0 715 the modeled landslide potential may be used as a decision support tool for local planning keywords shalstab landslide susceptibility shiny slope stability web based modeling data availability the example data is included with the shiny app available at https geogis bgsu edu apps shalstabcv 1 introduction rainfall induced shallow landslides are the most common gravitational mass movements caused under conditions of transient infiltration into initially unsaturated soils baum et al 2010 lehmann et al 2013 the intense or prolonged rainfall is usually the main triggering mechanism that weakens the soil shear strength by increasing soil water content and pore water pressure the assessment of rainfall induced landslide susceptibility at local and regional scales which is necessary for emergency mitigation and development of effective management strategies is commonly modeled by empirical or statistical techniques which are based on spatial correlations of observed landslides rainfall intensity duration thresholds and environmental attributes atkinson and massari 2011 ayalew and yamagishi 2005 carrara et al 1995 dai and lee 2002 2003 gorsevski et al 2005 2006a 2006b 2010 kavzoglu et al 2015 reichenbach et al 2018 more recent efforts are focused on data mining techniques that are aimed at discovering patterns and relationships in large datasets these techniques produce high accuracies and can integrate a variety of models including statistical techniques fuzzy sets classification and regression trees artificial neural networks random forest models and support vector machines among others gorsevski 2021 gorsevski et al 2016 gorsevski and jankowski 2008 kavzoglu et al 2015 micheletti et al 2014 saro et al 2016 data driven approaches as well as traditional empirical approaches often rely on the assumption that knowledge derived from past failures can be used for predicting future failure conditions however scenarios that cause landslide failure are altered and changing due to climatic effects and need to be addressed by using physical laws hürlimann et al 2022 at present altered situations are addressed by widespread deployment of artificial intelligence techniques which accelerate discovery of physical laws and governing processes that influence instability factors despite all of that the existing physically based models are still an important asset for landslide modeling and understanding new conditions as a result of environmental changes physically based deterministic or probabilistic models which address spatio temporal distributed dynamics of slope stability are based on simple mechanical rules and describe the processes associated with landsliding bathurst et al 2010 gorsevski et al 2006b montgomery et al 1998 montgomery and dietrich 1994 pack et al 1998 as compared to spatially explicit data driven approaches which have strict requirements for satisfying certain data assumptions the physically based models enable parameterization for characterizing essential slope stability processes i e different rainfall or soil scenarios through simple calibration and validation however a major drawback of physically based models is the need for site specific input data that are often not available over broad geographical scales and the need for parameters which can contain various levels of uncertainties carrara et al 2008 medina et al 2021 within the physically based landslide models the shallow slope stability model shalstab is a well known approach that has been widely implemented for mapping patterns of potential shallow slope instability in various applications and research settings montgomery et al 1998 montgomery and dietrich 1994 shalstab represents a deterministic and distributed model that couples an infinite slope stability equation with a steady state hydrological model that is praised for its simplicity and straightforward implementation the implementation of shalstab takes advantage of digital elevation data model dem for robust computing and predicting instability associated with different landscape features that are subjected by different rainstorms and landslide producing landforms the model has been applied to prediction of susceptibility methods of different aspects such as predicting rainfall induced landslides kim et al 2014 comparing to other similar approaches such as the stability index mapping sinmap gorsevski et al 2006b michel et al 2014 pack et al 1998 tarboton 1997 zizioli et al 2013 mapping shallow susceptibility over large areas such as seamless shallow landslide susceptibility map for all of california bellugi 2011 and applying stochastic integration with generalized likelihood uncertainty estimation glue incorporated into shalstab for acquiring parameter uncertainties and confidence intervals beven and binley 1992 jr chuan huang et al 2006 moreover implementation of physically based models as tools for planning and management of slope stability also require significant knowledge and familiarity with the geospatial software and modeling approaches until now very few real time spatial assessment approaches have been developed where decision makers or different non expert planners can alter and input site specific datasets and visualize risk output maps under different environmental settings and projected scenarios remarkably web based applications have become popular method for providing needed assistance especially for non expert gis users since they integrate user friendly interactive environment for solving complex environmental planning problems gorsevski et al 2021 gorsevski and torregrosa 2016 liu et al 2016 mekonnen and gorsevski 2015 such ubiquitous web based environment could provide users with easy access to complex modeling schemes which link gis to spatially distributed physically based models indeed a number of web based approaches have been used for environmental modeling and planning in different areas such as hydrology and water resources borges de amorim et al 2020 erazo ramirez et al 2022 whateley et al 2015 groundwater management glass et al 2018 2022 agriculture redhead et al 2022 su and li 2020 vilas et al 2020 and decision support systems li 2020 liu et al 2016 mekonnen and gorsevski 2015 in recent years there is an increase in the number of web based spatial applications due to the availability of free and open source software for geoinformatics foss4g and the freedom to freely redistribute copies of the modeling tools to the end users within the foss4g framework the simplicity of new open source programming languages such as r and python offer complete environments for development of interactive geospatial web based applications jos é díaz et al 2021 li 2020 whateley et al 2015 in this research a graphical user interface gui for real time assessment of landslide susceptibility is presented the proposed web based framework integrates shiny r https www r project org chang et al 2020 ihaka and gentleman 1996 and shalstab model montgomery and dietrich 1994 which resides in the saga gis system for automated geoscientific analyses geographic information system conrad et al 2015 geoprocessing environment the integrated web based approach that was developed fully within r environment was used to demonstrate rainfall induced landslide susceptibility modeling in a case study of the clearwater national forest cnf in central idaho this paper uses a small subset from the cnf and builds on earlier work by gorsevski et al 2003 2010 the intention of this demonstration is also to highlight a new approach for web based modeling that is inclusive and expands its usability to a wider community i e non r non gis users for performing real time analysis the present paper is organized as follows in section 2 the methods discuss the system architecture study area and example dataset as well as the modeling theory section 3 discusses the implementation of the web application and results which are generated interactively in the cnf while sections 4 focuses on the discussion finally section 5 shows the conclusions and recommendations for future research 2 methods implementation of the web based shalstab user interface involves installation of ubuntu operating system engine x nginx which is the web server different processing software including saga gis r cran rstudio with a number of related packages shiny server for development and deployment of shiny applications and configuration of the client server environment that integrates the framework in the next section the conceptual framework and the system architecture are discussed for the proposed web based shalstab user interface 2 1 system architecture this project implements a client server architecture gorsevski et al 2021 gorsevski and torregrosa 2016 jayawardhana and gorsevski 2019 liu et al 2016 mekonnen and gorsevski 2015 based on open source software using ubuntu operating system https ubuntu com fig 1 the hypertext transfer protocol http communication is used to exchange data between client s and web server s through hypertext markup language html content of files images and query results the secure communication protocol that extends http and encrypts data for a secure transfer over the internet is hypertext transfer protocol secure https within the proposed architecture the reverse proxy is used to transform https into http requests as an additional security layer and for shielding the web server the reverse proxy is the intermediate connection point positioned behind a firewall that forwards requests from web or mobile browser s i e client s to web server s additional functions of the reverse proxy include increased performance through load balancing such as traffic management maintaining security and anonymity for protecting identities of backend web server s and implementing reliable smooth flow of network traffic between client s and server s the encryption protocol is achieved through transport layer security tls or secure sockets layer ssl which is a global standard security technology the secure internet communication is based on asymmetric public key infrastructure that uses private and public keys to encrypt communications the private key decrypts information encrypted by the public key and resides on the owner s web server controlled privately on the other hand the public key is available to everyone who interacts with the web server where the information encrypted by the public key can only be decrypted by the private key as such the security is maintained at all time because only the server with the relevant private key can decode the information in this work the nginx open source https www nginx com reverse proxy server was deployed on a single machine although the system allows for multiple machine deployment the non profit certificate authority ca called let s encrypt https letsencrypt org is a service that provided the free ssl tls certificates while the certbot open source client https certbot eff org was used to fetch the certificate shiny beeley 2016 chang et al 2020 is a relatively recent package by rstudio racine 2012 that provides a gui interface for r cran the comprehensive r archive network ihaka and gentleman 1996 rstudio is a free open source ide integrated development environment for r which is also an open source data analysis language http www r project org the applications produced by the shiny package are called shiny applications which are used for building interactive web applications with r hosting the shiny applications as a web service requires installation of shiny server which enables sharing the applications on the internet beeley 2016 brendel et al 2020 chang et al 2020 li 2020 su and li 2020 su et al 2019 vilas et al 2020 whateley et al 2015 the open source version of shiny server which was used here is characterized by a limited amount of features compared to the shiny server pro version that is not free within the shiny environment the simplest structure of the shiny application is represented by a directory that contains two r scripts including a user interface script ui r which controls the layout and appearance of the application and a server script server r that incorporates instructions for building the application for the user input processing data capabilities functions and outputs written in r language within this environment the extensibility by r packages with pre programmed routines and commands can be used for various scientific applications the current implementation presented in this work used the following r packages shiny shinythemes shinydashboard leaflet mapview raster rsaga maptools rgdal sp rcolorbrewer ggplot2 rmarkdown sf dplyr caret and rocr although the current implementation involves some of the key spatial packages in r i e raster sp and rgdal accessing additional tools from external geospatial software such as saga gis http www saga gis org extends the geoprocessing capabilities and puts forward a robust framework for geocomputation accessing saga s gis functionality from within r is based on executing saga modules via the command line interface using existing python application programming interfaces api so called saga python api that allows full access to all saga algorithms the rsaga brenning 2005 package provides direct access to saga gis functions and geoprocessing environment directly from r for example shalstab is a module within the slope stability library of the saga gis that can be executed directly from r the rsaga functions are executed in r without using saga gis in the background by wrapper functions that contain arguments and specified default values the rsaga env function form rsaga is used for setting the geoprocessing environment in an r script which can automatically detect the installation of saga gis by searching through computer directories and by adding the newest version to the path environment variable in this research rsaga was used to call saga gis through functions such as rsaga geoprocessor rsaga slope rsaga grid calculus and rsaga topdown processing the shalstab module was accessed by the rsaga geoprocessor which is the workhorse function for linking the command line from saga gis with passing arguments from an r script 2 2 study area and example datasets to illustrate the potential of the proposed web based framework a subset area was selected that encompasses papoose badger and squaw creek watersheds located northwest of lowell idaho in the lochsa basin of the cnf fig 2 gorsevski 2002 gorsevski et al 2003 2004 2005 2010 the cnf is situated west of the montana state border and is bounded on three sides by four other national forests the lolo national forest in montana the bitterroot national forest in montana and idaho the nez perce national forest in idaho and the panhandle national forests in idaho the area was impacted by major landslide events during the winter of 1995 96 following heavy rains snowmelt and high river flows landslide occurrence was widely distributed across the region and included disturbed i e clearcuts and roads and undisturbed forests the selected study area was used by gorsevski et al 2003 2004 as a training area in a fuzzy k means classification of continuous landforms in tandem with bayesian probabilistic modeling approach for mapping landslide susceptibility the area is 111 8 km2 with highly dissected mountainous topography with elevation that ranges between 966 m and 2154 m and slopes that vary between 0 and 45 precipitation averages about 1320 mm annually which changes significantly across the elevational gradient most of the annual average precipitation falls as snow during winter and spring while peak stream discharge occurs in late spring and early summer the soils are highly variable but typically well drained and primarily derived from parent materials such as granitics metamorphic rocks quartzites and basalts or surface colluvium the land cover is predominately forested with coniferous species and various other shrubs and grasses that have short growing seasons particularly at the higher elevations a single dem input is the only required datasets for running shalstab while other derivatives such as the slope in radians and a catchment area in m2 are directly computed from the dem the example dem used here are based on 7 5 min u s geological survey dems i e 30 m spatial resolution and correspond to the usgs 7 5 min topographic quadrangle map provided in the universal transverse mercator utm projection and wgs84 datum in addition the validation of the model requires a landslide coverage stored in a point data format i e x y coordinates and an attribute that denotes the binary response for presence or absence the initiation area of each landslide i e the area where the main scarp of the landslide occurred is interpreted as the point representing the presence of a landslide in the entire cnf a total of 865 landslides were assessed through aerial reconnaissance flights and field inventory in july 1996 the landslides interpreted from aerial photos were classified into road related rr which were human induced and non road related nrr landslides which were from natural causes i e 55 rr and 45 nrr in this demonstration the subset dataset contains a total of 96 shallow landslides regardless of previous classification and include both rr and nrr the non landslide areas were used to generate the absence of landslides dataset using random sampling approach 2 3 shalstab model shalstab is a deterministic model that calculates the critical shallow groundwater recharge conditions mm day that can destabilize a slope the model couples cohesionless infinite plane slope stability model and steady state shallow subsurface flow i e hydrological model beven and kirkby 1979 o loughlin 1986 the hydrological model calculates the spatial patterns of soil saturation i e wetness using upslope contributing areas soil transmissivity and local slope with a presumption that the flow infiltrates to a lower conductivity layer and follows topographically determined flow paths montgomery et al 1998 montgomery and dietrich 1988 1994 o loughlin 1986 the critical steady state rainfall q that is predicted to cause instability at each grid cell is solved by the following equation eq 1 q c t s i n θ a b ρ s ρ w 1 tan θ tan φ where topographic terms from the dem include drainage area that contributes subsurface flow a the outflow boundary length i e cell width b and the local slope of the ground surface or angle which is assumed to be parallel to the failure plan θ the rest of the parameters include the saturated bulk density of the soil ρ s the water bulk density is ρ w the angle of internal friction φ and the soil transmissivity t which is given by the product of the soil thickness and water level above the failure plane the proportion of the soil column that is saturated at instability is given by the ratio of the saturated depth h of the soil cover to the total soil depth z that is referred as h z for a given storm the spatial distribution of h z is determined by both hydrologic and topographic ratios the hydrologic ratio captures the magnitude of the precipitation event relative to transmissivity thus larger precipitation events relative to transmissivity most likely would generate larger ground saturation and subsequent instability the topographic ratio models the concentrating runoff and elevating pore pressures using a ratio of the contributing area i e drainage area to the corresponding cell width the topographic ratio governs the relative wetness defined by the hydrologic ratio where steeper slopes produce faster subsurface flow and lower saturation therefore lowland areas and valleys are associated with higher topographic ratio an important assumption that sets the limit on h z is that the failure plane and the shallow subsurface flow are parallel to hillslope such as assumption sets the range of the h z between 0 and 1 i e wetness exceeding 1 represents overland flow and any places with h z 1 are set to be unconditionally stable even under an extreme storm event and excess pore pressures is required for triggering slope instability on the other hand unstable slopes when h z 0 are set to be unconditionally unstable even under dry conditions of the site where soil accumulation is scarce using the range of the h z values that vary between 0 and 1 the critical rainfall values for each grid cell are calculated the slopes correspond to tan θ tan φ for h z 0 and tan θ tan φ 1 ρ s ρ w for h z 1 montgomery 2001 for example the shalstab module in saga gis generates continuous cr mm day raster with limits between unconditionally stable and unstable states which are set to nodata and zero respectively also the tool produces a classified cr grid that is represented by seven stability classes including stable unstable 0 50 mm day 50 100 mm day 100 200 mm day 200 400 mm day and 400 mm day 2 4 validation and assessment different measures of accuracy are used for binary classification which are derived from a confusion matrix also called a contingency table jayawardhana and gorsevski 2019 kuhn et al 2020 markoulidakis et al 2021 sing et al 2020 the square matrix consists of columns and rows that list the number of instances as absolute or relative actual class vs prediction class counts for example fig 3 illustrates that the prediction outcomes are represented by columns and actual or observed values by rows jayawardhana and gorsevski 2019 in the figure label p denotes a positive outcome i e landslide occurrence while n denotes a negative outcome i e non landslide occurrence the four outcomes in the confusion matrix report true positive tp and true negative tn conditions as well as false positive fp or type i error and false negative fn or type ii error conditions for example tp is the number of correctly predicted landslides while fp is the number of incorrectly predicted landslides on the other hand tn is the number of correctly predicted non landslides observations while fn is the number of incorrectly predicted non landslides the measurements from the landslide outcomes in each of the four cells can be used to produce indices that are referred to as sensitivity specificity precision accuracy and f1 score shown in equations 2 6 jayawardhana and gorsevski 2019 markoulidakis et al 2021 eq 2 s e n s i t i v i t y t p t p f n eq 3 s p e c i f i c i t y t n f p t n eq 4 p r e c i s i o n t p t p f p eq 5 a c c u r a c y t p t n t p t n f p f n eq 6 f 1 s c o r e 2 p r e c i s i o n r e c a l l p r e c i s i o n r e c a l l kappa eq 7 is similar to accuracy score but it takes into account the accuracy that would have happened i e expected probability through random predictions kuhn et al 2020 cohen s kappa is the most widely accepted measure of inter rater reliability when the outcome of interest is measured on a nominal categorical scale kappa values range between 0 and 1 but negative values occasionally can occur cohen s kappa of 1 indicates perfect agreement between the raters and 0 indicates that any agreement is due to chance in the example confusion matrix fig 3 the data on the main diagonal of the matrix tp and tn represent the count of agreements and off diagonal data fn and fp represent the count of disagreements eq 7 k a p p a o b s e r v e d a c c u r a c y e x p e c t e d a c c u r a c y 1 e x p e c t e d a c c u r a c y the receiver operating characteristic roc curves gorsevski 2002 2006 2013 are also useful tool in the assessment of the dichotomous performance outcome through graphical representation of different probability thresholds using the relationship between the tp correctly predicted landslides and the fp falsely predicted landslides each point on the roc curve can be tied to a specific decision criterion that represents the amount of risk associated with the accuracy of the prediction this point selection or the cut off point will be different among decision makers even when their roc curves are the same another property of the roc curve is the area under the curve auc that is used as a measure of overall model fit and comparison of different predictive outcomes an ideal model would have an area equal to 1 because then p true positive 1 and p false positive 0 regardless of the cutoff point gorsevski et al 2000 2006 3 results 3 1 web application the web based prototype for mapping landslide susceptibility provides a simple modeling interface for non experienced gis users the easy to use interface is shown in fig 4 which represents the main or the modeling page of the shalstab web based prototype the web based interface can be accessed by web browsers such as internet explorer chrome or mozilla firefox the main components that comprise the prototype are organized under two different themes including a modeling page and a validation page the themes reside in the header which is the horizontal navigation element on the top of the page where the user can switch between modeling and validation options the sidebar on the left hand side is used for navigation and contains the user input widgets where interactive web elements such as sliders buttons selection boxes and check boxes are placed the widgets enable dynamic html content that is generated on the server side and sent to the client for rendering this bidirectional communication between the client and the rserver is supported by reactive programming environment that automatically re executes modeling tasks when there is a change in the user input values i e increase or decrease in value of a parameter the required inputs for mapping landslide susceptibility are shown in the sidebar fig 4 an important simplification in the proposed prototype for enhancing friendliness and support for non experienced gis users is based on a requirement of a single grid input the required input is a dem in a geographic tagged image file format geotiff format provided with wgs84 utm zone projection while the maximum size of the file is limited to 10 mb regardless of spatial resolution such dem data can be downloaded directly in geotiff format from publicly available sources from the u s geological survey usgs the national map viewer tnm viewer https viewer nationalmap gov or the usgs earthexplorer ee https earthexplorer usgs gov spatial data web portals in the modeling page the required parameters initially are set to the default global values assigned by saga gis which include material density g cm3 1 6 hydraulic conductivity m hr 2 7 material friction angle degree 33 0 material thickness m 1 0 and bulk cohesion mpa 0 0 however hydrological and geotechnical parameters should be determined and used from field observations or laboratory tests for site specific applications the rainfall data is not required input by shalstab but observed rainfall from local rain gauge stations can be used to calculate the daily uniform recharge rates scanlon et al 2002 since shalstab is a deterministic model the input values of the parameters are assumed to be uniformly distributed the transparency slider is used for visualization purposes that controls the translucence of the modeled shalstab output and controls clarity of the corresponding basemap or reference map that is underneath the default value is set to 0 7 where a value of 0 0 is fully transparent map while a value of 1 0 represents an opaque map fig 5 illustrates the modeled output generated by uploading the dem from the cnf the classified output of critical shallow groundwater recharge values cr in mm day was generated by the default parameter values the histogram associated with the landslide susceptibility map shows the distribution of raster cell values across the seven classes as well as the no information i e nodata cells the labels of the slope stability classes are displayed in both the legend and the histogram for instance the histogram suggests that the stability class with critical shallow groundwater recharge values that ranges between 0 and 50 mm day is the most abundant followed by the unconditionally unstable areas from the histogram also the unconditionally stable area is smaller than the unconditionally unstable area which suggests high overall susceptibility the drop down choose a layer menu top right in fig 5 offers a selection of the intermediate layers derived from the dem which are required for running the shalstab model the dem derivatives include slope and catchement while additional outputs from the same menu include continuous critical recharge cr and classified critical recharge cr layers that represent the continuous and the discrete solutions of the modeled landslide susceptibility for example the continuous critical recharge cr is represented by a plot with a different continuous curve as well as a legend with customized modeled values and color scale fig 6 shows controls at the top left corner of the map such as the zoom in the zoom out and the interactive layer display that allows users to switch between different layers the types of layers include a selection of a basemap i e only one basemap can be visible at a time and overlays such as the model outputs for example in fig 6 the basemap is shown by a topographic map that contains shaded relief and contours while the overlay is represented by the slope layer that is draped on the top of the shaded relief basemap further enhancements for the visualization can be controlled by adjusting the transparency of the slope layer i e 0 5 or by the interactive layer display that allows to toggle the visibility of the slope layer i e showing hiding by switching to the slope layer the user is presented by the histogram that accompanies the slope layer and a new legend that is color coded to reflect the values in the map for instance the study area is associated with relatively steep slopes whereas the majority of the slopes range between 0 4 and 0 6 radians or 23 34 for better clarity histogram panel can be moved by clicking and dragging so obstructed areas can be exposed for exploration and visualization of slope patterns the validation page can be accessed after the dem grid file was uploaded in the modeling page fig 7 the validation page also requires a single input of landslide initiation areas i e location of the scarps in a comma separated value csv using coordinates that have matching datum and projection as the original dem the required columns and the order of fields in the cvs file include id x y and slides where the id is a sequential number for identifying the landslides the x and y are the easting and northing geographic coordinates and finally the slides are represented in a binary format where 1 is presence and 0 is absence the size of the file is limited to max size of 5 mb the results generated in the validation page are shown in fig 7 after the csv file was uploaded in the figure the continuous cr is overlaid on a topographic shaded relief basemap while the landslides are overlaid on the top of the cr map as points where red color represents absence and green color represents presence the interactive layer display can be used to show and hide the layers or the landslides observations that belong to the binary groups i e presence and absence the range of cr values is shown in the legend which is roughly between 0 and 550 mm day another useful feature is the interactive exploration of cell values when the user scrolls the mouse pointer over the cr layer the actual cr grid values are displayed in the top right hand corner i e layer cr 61 70 such exploration can be used to adjust the cr values threshold slider for representing different cut offs associated with the binary solution the default cr values threshold is set to 200 mm day the confusion matrix in the figure shows the results that are generated by the module the rows in the confusion matrix show the actual data used for the prediction while the columns show the predicted outcomes the roc curve in fig 7 represents a plot of false positive rate x axis and true positive rate y axis and shows the trade off between all possible threshold values for better clarity visualization and selection of potential threshold values the curve is colorized according to cr values with a legend shown at the right hand side of the plot i e the distribution of values matches the cr values thresholds range which is between 0 and 200 mm day the performance of the roc curve is assessed by the auc which represents the degree or measure of separability the auc under the black diagonal line in the figures represent an area of 0 5 which indicates a random classification model or no separability the web based gui provides a module for generating detailed dynamic reports from the shalstab analysis that can be converted to different output formats including pdf html and word files this is accomplished by r markdown and knitr http yihui name knitr that embeds r code within the document and executes the code to generate histograms plots and figures tables and other calculations derived from the analysis xie et al 2020 the reproducible analysis are automatically regenerated when parameters are modified interactively by the user which allows for spatial modeling and testing of multiple landslide susceptibility scenarios for example the reports have a total of three different parts including header top of the document a markdown section that describes the workflow which is written in markdown syntax and the code chunks written in r the header is written in yaml format and contains metadata about the report i e title author date and output for rendering an html or pdf file and different settings can be used for the representation of the outputs within the report such as different fonts or text styles for instance for keeping track of different simulations the report contains the values of the input parameters and basic information about the input raster used in the modeling however the report outputs are customizable and different layouts can be produced programmatically the last two items in the web based gui are options for downloading the output associated with the continuous cr model that can be used for visualization and additional analysis the options for saving the cr model includes a geotiff and a keyhole markup language kml formats both outputs save the projection information that is uploaded with the original dem and csv inputs for example the foss implementation is shown in fig 8 where visualization of the results are displayed in google earth https www google com earth some of the important visualization features in google earth include an interactive 3d environment of overlaid grid model with adjustable transparency zooming in and out and changing perspective i e rotating and tilting at different scales over exaggerated topographic relief the kml is exported with a matching color coded legend placed in the map window on the lower left side for deciphering the low and high susceptibility areas for instance the figure demonstrates that areas with gentle and moderate slopes such as channels ridges and flatter areas are with low susceptibility while mid to upper slope areas especially oriented in northwestern western and southwestern direction are with higher susceptibility however additional analysis and comparison could be generated by analyzing the geotiff output in a specialized geospatial foss packages such as saga or r 3 2 case study modeling and validation the presented shiny web based framework for assessing landslide susceptibility in idaho s cnf was tested using site specific knowledge of the modeling parameters in tandem with an interactive calibration gorsevski 2002 gorsevski et al 2000 2004 the following parameter values along with the dem and the validation dataset were used in the presented analysis material density g cm3 1 6 hydraulic conductivity m hr 1 05 material friction angle degree 37 0 material thickness m 1 15 and bulk cohesion mpa 0 002 the results from the confusion matrix showed that a total of 82 landslides and 87 non landslides were correctly classified as presence or absence from a total of 189 the performance of the predictive model generated the following matrices sensitivity 0 854 specificity 0 935 precision 0 932 recall 0 854 f1 score 0 891 accuracy 0 894 and kappa 0 789 fig 7 for example the specificity value of 0 935 implies that very few non landslides are misclassified or a high proportion of non landslides among all non landslides are classified correctly the sensitivity or recall value of 0 854 is lower which suggest that a larger number of the landslides were misclassified among all landslides the precision value of 0 932 implies that correctness is achieved in the prediction of the non landslides population since very few are misclassified as landslides however in case of higher recall and lower precision the correctness of the landslide population would be higher in this study the model has higher precision but lower recall with an f1 score of 0 891 the f1 score which represents the harmonic mean of precision and recall can be interpreted as a weighted average of the precision and recall similar to the arithmetic mean the f1 score is always between the precision and recall but the importance of precision over recall or vice versa can be adjusted towards both classes an adequate f1 score is associated with low fp falsely predicted landslides and low fn falsely predicted non landslides on the other hand when fp and fn counts are close the overall accuracy measure can be used for the performance the accuracy of 0 894 should be interpreted with caution since different class proportions are associated with the fp and fn counts in order to deal with unbalanced data the kappa statistics should be used since is more informative than overall accuracy performance the evaluation of the kappa statistic which is 0 789 should also be interpreted with caution and compared with the accompanied confusion matrix by considering the importance of landslides vs non landslides predictions also in case of imbalanced classes since kappa statistic compares an observed with an expected accuracy random chance an evaluation of expected accuracy which is directly related to the number of instances of each class is needed for example unlike the overall accuracy kappa statistics allows for models with skewed class distributions to be objectively compared by controlling the expected accuracy however in general a high score in any metric should be regarded with caution when interpreting the metrics and corroborated with the values from the confusion matrix the visualization of the performance measures for classification are available in the reports generated interactively by the module beside roc curve as presented in fig 7 the dynamic reports include additional graphical representation of a classifier s performances which contain precision recall curve averaging across multiple runs curve and peak of average accuracies across a range of cutoffs curve such performance measures are useful for identification and selection of different cutoff values from the roc curve that are used for comparison of various outcomes rather than a single value i e accuracy f1 score kappa in addition the auc i e area between the curve and the x axis is used to summarize the roc curve performance and can be useful for comparisons of different models produced by varying parameters from multiple runs in this particular case study the overall quantitative index of accuracy corresponds to 0 715 auc which suggest that there is a 71 5 chance that the model will be able to distinguish between landslides and non landslides 4 discussion the presented foss web based tool shows a promising approach for providing real time modeling capabilities to non expert gis users as implemented with the shiny framework the web based tool can offer number of advantages to different end users by means of compatibility across platforms i e windows linux or macos accessibility based on minimal software installation requirement which is a single web browser i e internet explorer firefox chrome safari flexibility to use site specific datasets across different regions a possibility for powerful modeling and geovisualization and feasibility for real time collaboration including interactive teaching in both face to face classroom and online settings which could expand diversity of demographic representation in science technology engineering and mathematics stem of women and traditionally underrepresented students access to affordable higher education or successful post graduate careers until now very few real time spatial assessment approaches have been developed where decision makers or different planners can alter and input site specific datasets and visualize risk output maps under different environmental settings and projected scenarios to make the forecasting of landslide susceptibility less dependent on the judgment of spatial modelers there is a potential for local or regional decision makers to generate and update susceptibility maps when necessary or needed the proposed framework provides user friendly web interface for modeling and validation using very few inputs and without the need of familiarity with geospatial software or programming experience in this study the web based approach demonstrated that the simplicity of shalstab could be extended as a rapid assessment tool in addition the tool can be used in response to proliferating landslide susceptibility caused mostly by human disturbances and the new realm of increased extreme weather events which are projected to become even more severe and frequent as a result of climate change effects gariano and guzzetti 2016 under such conditions decision makers are faced with difficult choices for focusing limited resources and prioritizing policy for tackling growing complexity of environmental problems that require quick response periods for instance susceptibility maps for forest planning involving the maintenance obliteration or development of new forest roads in steep mountainous terrain can be generated by site specific input parameters by local planners such assessments may help the planners to determine various criteria of system use considering all possible sets of conditions that could lead to landsliding for example if the local planner knows that recent wildfire endangered the landscape the acceptable susceptibility risk can be adjusted by selecting different threshold values from the roc curve based on knowledge of contextual information however one of the major shortcomings is that the free version of shiny server is single threaded which supports concurrent number of users but with significantly reduced speeds and longer processing times for example in a classroom settings with multiple students this can be disadvantage unless shiny app is distributed and deployed locally on individual student computers which could bypass the external server the size of the input data can also affect the responsiveness and performance associated with the modeling which often requires size limits to be set for the uploads in the case study the limits were set to 10 mb for the dem and 5 mb for the landslides in addition there are a number of limitations with the efficiencies of deterministic approaches as compared to other statistical methods a study from the northern apennines italy suggested that the predictive capability of shalstab was much lower 0 56 auc compared to weight of evidence method and fuzzy logic method which yielded 0 77 and 0 74 auc respectively cervi et al 2010 in this demonstration shalstab produced an accuracy of 0 715 auc which is comparable to previous work in the lochsa basin cnf which used logistic regression and produced accuracy of 0 716 auc gorsevski et al 2000 that said other efforts in the cnf which used classified dataset of non road related nrr and road related rr landslides exhibited higher index of accuracy which corresponded to 0 84 auc for the nrr and 0 80 auc for the rr landslides gorsevski et al 2006 in the same study area the comparison of shalstab to an integrated fuzzy k means bayes theorem and forest service fsmet approaches suggests that spatial predictions from shalstab exhibited lower performances gorsevski et al 2003 in summary the intention of this study was to show the potential of a shiny web based real time modeling and visualization approach for new technological ideas in environmental modeling the flexible methodology shows a potential blueprint for modification and integration with different modules where different strengths and capabilities can be utilized as such different processing chains could integrate multiple functions and algorithms to be joined in a complex processing sequences for example different packages from r can be used to access foss4g such as rsaga which establishes an interface between r and saga gis rqgis which establishes an interface between r and qgis https www qgis org or rgrass an interface between r and grass geographical information system https grass osgeo org 5 conclusions the presented web based framework extends the capabilities of existing spatial models for landslide susceptibility for non r and non gis users by lowering technical hurdles of this web based approach powerful functions are made accessible to a wider user community the importance of the paper sheds light on an interactive modeling and visualization that can facilitate decision making by local planners who are often required to alter the level of risk which depends on different circumstances the case study demonstration showed an implementation of interactive modeling through sequential set of steps that produced an overall accuracy of 0 894 kappa of 0 789 and 0 715 auc interestingly the approach does not offer an optimal solution or a threshold value but local expert knowledge and integration of other susceptibility models can be used in conjunction with the generated outputs by the framework in addition other practical aspects of this r shiny framework can be implemented through an educational context as a teaching tool for presenting advanced modeling concepts to students the attractive and easy to use r shiny gui interface potentially can also initiate curiosity by students and motivation for learning r scripting extend into other statistical methods use advanced analytics and promote collaborative discussions that can elevate the understanding of model implications recommendation for future research and improvements of this prototype include integration with other modules such as the stability index mapping sinmap which is a stochastic and distributed model i e requires the maximum and minimum input parameter values providing tools for interactive downloads and preprocessing of dem datasets adding flexibility to spatial services such as the u s landslide inventory and access to standardized landslide datasets and enabling real time group based spatial decision support systems sdss as a collaborative framework for landslide susceptibility software availability product title interactive web based modeling of shallow landslide susceptibility developer pece v gorsevski contact address school of earth environment society bowling green state university bowling green oh 43403 usa contact email peterg bgsu edu available since 2019 programming languages r saga gis html availability https geogis bgsu edu apps shalstabcv source code https github com pecevg shiny saga landslide cost free declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the author would like to thank the three anonymous reviewers who provided helpful suggestions for excellent additions to the manuscript 
25452,dams and reservoirs predictably alter the discharge of a river often confounding large scale flow simulations the routing application for parallel computation of discharge rapid model has been widely used operationally until recently rapid could only represent reservoir outflow by directly specifying discharges from dams herein we develop a non data driven reservoir routing approach within rapid results are comprehensively evaluated across the entire mississippi river basin mrb the reservoir routing resulted in considerable improvement in the lower mississippi and arkansas white red regions further the accuracy of estimated reservoir releases also benefits when the inflow estimation improves simulation of the reservoir release with observed reservoir inflow resulted in highest kling gupta efficiency kge among all experiments inclusion of reservoir routing in rapid will enhance the utility of this model in basins where dams exist and more importantly improves streamflow predictions by providing better hydrological information for decision makers keywords reservoir routing continental scale hydrological modeling rapid model rivers streams mississippi river basin data availability we provided links to the source code and datasets in the software and data availability section that we used in this study 1 introduction dams and their corresponding reservoirs segment river systems all around the world dynesius and nilsson 1994 postel et al 1996 currently dams immensely distort the natural streamflow for 77 of rivers while roughly half of the river networks are regulated worldwide boulange et al 2021 nilsson et al 2005 with more than 45 000 large dams around the world these structures moderate flood risk by attenuating river flow fluctuations gao et al 2012 they also support socio economic development by generating hydropower and providing water supply and recreational opportunities getirana et al 2020 passaia et al 2020 vanderkelen et al 2022 dams and their impoundments also impact regional climate change by affecting the local hydrological cycle zhao et al 2016 dams also exert influence on the environmental and ecological aspects of riverine systems such as sediment transport gupta et al 2012 renwick et al 2005 fish migration pathways stone 2016 and water quality indicators eiriksdottir et al 2017 kunz et al 2011 tavakoly et al 2016 winton et al 2019 notably the construction of large dams is escalating in the developing world in basins such as the amazon nile and tigris euphrates basins due to the growing demand for water and energy timpe and kaplan 2017 winemiller et al 2016 thus better understanding the impact of reservoir systems on hydrology and the environment is of paramount importance given the important role of dams and their associated reservoirs the integration of dam operations into continental scale hydrological modeling is imperative to support sustainable water resources management moreover inclusion of reservoir routing is critical for hydrological flood prediction salas et al 2017 tavakoly et al 2021 zajac et al 2017 owing to the lack of storage dependent release rule curves and local discretion in reservoir operations non data driven reservoir routing methods have been widely used in large scale hydrological models burek et al 2013 pokhrel et al 2016 yassin et al 2019 zajac et al 2017 in non data driven reservoir methods the operation of a reservoir is conceptualized without explicitly including operational rule curves or actual reservoir operations determining the optimal non data driven method involves tradeoffs between the complexity of the method and the data available de vos 2015 gutenson et al 2020 previous studies in continental scale modeling represented the impact of reservoir regulations by focusing mainly on mass balance and seasonal to sub seasonal variation of downstream river flow regimes e g chawanda et al 2020 hanasaki et al 2006 masaki et al 2017 pokhrel et al 2018 shin et al 2019 2020 despite great strides in simulating reservoir impacts on streamflow the spatial effect of a system of dams and reservoirs on diurnal and sub diurnal streamflow simulation is rarely emphasized in the existing literatures accordingly quantifying the effects from numerous upstream regulations on downstream flow conditions and on simulating reservoir release flows across continental river basins have received less attention simulation of river and reservoir systems at the sub diurnal scale is crucial before during and after flood events since floods typically happen over a relatively short time period and flood forecasting systems tend to have a relatively short lead time 0 10 days as a pioneering river network routing model the routing application for parallel computation of discharge rapid david et al 2011b modernized continental scale river flow modeling by using vector based river networks instead of traditional raster cell based datasets this feature plays a pivotal role in flood prediction and identifying potential flooded areas follum et al 2017 tavakoly et al 2021 the rapid model is an open source numerical model that is computationally efficient so it can provide timely streamflow estimates over a river network with hundreds of thousands of river reaches rapid has more than a decade of development evaluation validation and application over regional to global scales david et al 2011a 2013 2016 lin et al 2018 2019 sikder et al 2019 tavakoly et al 2017 2021 yang et al 2021 rapid has been applied to address diverse objectives of water resources and management including nutrient transport tavakoly et al 2016 2019 climate change forbes et al 2019 lewis et al 2022 meselhe et al 2021 and river aquifer interaction saleh et al 2011 flipo et al 2012 rapid is the main river routing component of the autorapid modeling system which simulates high resolution flood inundation maps over continental scale regions follum et al 2017 2020 within the hydrologic community rapid has had a significant impact including the following 1 the first published global application to produce an a priori dataset for nasa s surface water ocean topography swot mission 2 the first river routing component of noaa s national water model maidment 2017 salas et al 2017 3 the key modeling component in quasi operational hydrologic modeling frameworks supporting the u s military army air force and marine corps snow et al 2016 wahl et al 2016 4 operational implementation at the european centre for medium range weather forecasting https geoglows ecmwf int with global high resolution vector based river system qiao et al 2019 souffront et al 2019 and 5 lastly rapid is integrated in the nasa land information system lis kumar et al 2006 as part of lis hydro project and will be one of the main river routing models in the global hydro intelligence ghi operating plan jerry wegiel et al 2020 the main operational purposes in items 2 3 4 and 5 are to provide timely streamflow prediction as a hindcast nowcast and forecast regardless of all these applications until recently gutenson et al 2020 tavakoly et al 2021 rapid has lacked a reservoir routing scheme that can be used operationally from the comprehensive analysis of reservoir routings in our recent work gutenson et al 2020 we inferred that a reservoir routing method by döll et al 2003 hereafter referred to as d03 consistently improved diurnal streamflow simulation over naturalized flow conditions d03 requires minimum reservoir information is computationally inexpensive and can be applied by knowing an estimated inflow and the minimum and maximum storage volumes the d03 method can be globally applied since d03 requires such minimal reservoir information in this paper we offer a direct succession to our previous efforts gutenson et al 2020 evaluated the reservoir routing model d03 but did not implement the reservoir routing model within rapid david et al 2015 applied similar geospatial datasets and runoff products within rapid but did not include a reservoir routing component tavakoly et al 2021 explored reservoir outflow simulation by directly inserting observed reservoir releases into rapid rather than performing the reservoir routing building upon the previous research this paper fills the knowledge gap by developing and implementing a d03 inspired reservoir routing module within the rapid river network model we implemented the d03 model and evaluated the streamflow simulation results at the continental scale of the mississippi river basin mrb we identify the impact of reservoirs in different regions of the mrb and compare the model results with observations at different stream gage locations we also compare reservoir release flow with different reservoir implementations in the rapid model lastly we study the impact of a system of dams on the downstream flow dynamics 2 methods 2 1 streamflow river routing rapid david et al 2011b solves the matrix version of muskingum equation mccarthy 1938 in a river network comprised of hundreds of thousands river segments the equations and discretization of the governing equations were described in previous studies e g david et al 2011b emery et al 2020 tavakoly et al 2017 the model requires a pair of muskingum parameters k and x the k coefficient represents the propagation time of the flood wave down the river channels and x relates to diffusion of the flood wave cunge 1969 the rapid model uses a topological grid based a traditionally gridded geospatial setup in many river routing models or a vector based river network i e the blue lines on the maps this latter feature was first introduced in rapid and contributed to its adoption in the aforementioned forecasting systems using vector river networks also supports better topological inclusion of water bodies such as lakes and reservoirs interacting with the river network rapid requires runoff commonly simulated by various land surface models lsms as the dynamic input to compute streamflow the rapid model is agnostic to the source of runoff and topological river network hence it can be implemented over a variety of river networks with forcing from various lsms 2 2 development of reservoir routing in rapid the d03 reservoir routing conceptually attenuates flow variations according to changes in reservoir storage döll et al 2003 as follows 1 q t k d 03 δ t s t s min s t s min s max s min 1 5 where qt is the simulated reservoir outflow m3 s st is the reservoir storage m3 and smin and smax are the minimum and maximum reservoir storages m3 respectively δ t is the döll simulation time step in seconds and k d03 is the reservoir release coefficient typically ranging from 0 01 to 0 90 gutenson et al 2020 we consider the reservoirs dead pool storage to be smin δ t is equivalent to the runoff input time step represented by zs taur in the rapid code hence river routing and reservoir outflow are synchronized note that the actual river routing equations are usually solved at a finer time step david et al 2011b in this study rapid with reservoir routing runs at a 3 hourly runoff time step and a 30 min routing time step the k d03 coefficient is also an input parameter this allows the user to change the k d03 value to calibrate the model for a given study domain in addition to the rapid input files such as the runoff dataset river connectivity and muskingum parameters the location of the dam and two reservoir minimum and maximum storages values are required fig 1 to run rapid with d03 reservoir routing we created two additional input files and adapted the rapid source code for their use appendix a includes the instructions for preparing the dam input files along with sample reservoir data for running the reservoir routing module in rapid furthermore the location of reservoirs must be associated with the river network one advantage of using a vector river network is that the location of reservoirs can be easily assigned to the river segment in the rapid model with continental scale models running on a coarse grid some reservoirs and smaller tributaries might not be adequately represented this can result in grid based models wrongfully assigning a dam location and subsequently miscalculating the reservoir release flow shin et al 2019 reservoir locations are indicated with corresponding river ids in a separate rapid input file in appendix a this file is identified as a second reservoir input file 2 3 study domain and data processing we selected the mississippi river basin mrb with an area of 3 179 875 9 km2 to test and evaluate the rapid reservoir routing module the mrb has been extensively studied in continental hydrological contexts e g bain et al 2022 david et al 2015 tavakoly et al 2017 2021 we obtained the rapid input data excluding reservoir information from david et al 2015 the vector river network and contributing catchment area were based upon the shuttle elevation derivatives at multiple scales hydrosheds dataset lehner et al 2008 the study domain includes 102 229 river segments with an average length of 3 3 km and contributing catchment area of 31 11 km2 fig 2 the runoff input file was computed using the variable infiltration capacity land surface model vic lsm liang et al 1994 available from the nasa north land data assimilation system 2 nldas 2 mitchell et al 2004 xia et al 2012 similar lsm runoff was used by david et al 2015 and tavakoly et al 2017 and 2021 to drive rapid in the mrb we selected 62 dams and reservoirs to include into the rapid model for reservoir routing maximum storage varied in size from 39 540 to 26 000 016 50 acre feet 48 771 799 to 32 070 500 352 m3 the smallest and largest reservoirs are big hill lake in kansas and lake sakakawea created by garrison dam in north dakota respectively the selected dams cover major mrb regions differing in both topography and climate furthermore we were able to identify usgs gages with full record of observed discharge within the study time period downstream of these dams we collected reservoir information for 60 dams from nine united states army corps of engineers usace districts kansas city little rock nashville omaha pittsburgh rock island st paul tulsa and vicksburg districts we obtained additional reservoir information for pickwick and kentucky dams from the tennessee valley authority tva the locations of our selected reservoirs are geographically diverse including upstream regions of the ohio river the upper mississippi river the arkansas river the red river and the missouri river we compared model results with daily streamflow data from the u s geological survey usgs national water information system nwis for the simulation time period 01 01 2000 12 31 2009 we identified 43 gages that do not have missing records during the simulation period and are located downstream of at least one dam in the domain fig 2 therefore comparison of rapid streamflow with observation shows the impact of reservoir inclusion on the modeling results the drainage area of gages ranges from 572 0 km2 at the french creek near union city station in pennsylvania to 2 964 241 4 km2 at the vicksburg station in mississippi of the 43 usgs gages we selected seven gages to present hydrograph comparisons with and without rapid reservoir routing in major basins of the mrb shown with the red colored dots in fig 2 these seven gages cover spatially diverse topographic areas drainage area hydrologically different regimes and flow magnitudes table 1 lists the station name usgs id location longitude and latitude and contributing drainage areas of each usgs gage the mississippi river at vicksburg station was selected to represent nearly all major tributaries of the mrb 3 results 3 1 model parameter sensitivity analysis before comparing the streamflow simulation with and without reservoir routing we conduct a sensitivity analysis for the reservoir release coefficient k d03 used in the d03 method the k d03 parameter is defined as an arbitrary parameter in the rapid input file döll et al 2003 originally held the k d03 parameter constant at 0 01 there has been no relationship confirmed between the k d03 parameter and measurable characteristics of a reservoir or its geographic setting thus for practical purposes the k d03 parameter can serve as a calibration term for rapid gutenson et al 2020 we describe one method below for setting default values of the k d03 parameter for global applications in this instance the rapid model with reservoir routing was run with multiple k d03 coefficient values 0 01 0 02 0 04 0 06 0 08 0 10 0 30 0 50 0 70 and 0 90 we ran rapid for the study domain using each of these as the k d03 coefficient value based on the selected values of k d03 as the only parameter for calibration rapid was run ten different times the kling gupta efficiency kge gupta et al 2012 was calculated for each rapid run and the k d03 coefficient value resulting in the highest kge at each of 43 gage locations was then identified in a practical calibration for global applications reservoirs with no stream gage observations downstream of their location can be set to the most common k d03 coefficient value defined in this process either regionally or globally the result of this sensitivity test differed from the previous study by gutenson et al 2020 where 0 9 tends to be the best performing k d03 value our sensitivity analysis yielded 0 01 as the predominant k d03 value resulting in the highest kge value i e the maximum value of kge among ten rapid runs with each of the k d03 values at gage locations which matches the original formulation of d03 that utilizes a k d03 0 01 döll et al 2003 fig 3 shows that k d03 0 01 derived the highest kge in 70 of gages there are three differences that may lead to the different k d03 coefficients found by gutenson et al 2020 and this study the differences relate to the time step of the simulation the locations targeted for optimization and the type of inflow values utilized we utilized a delta t value of 3 h 10 800 s in this study while gutenson et al 2020 utilized a daily time step δt 86 400 s this suggests that k d03 is sensitive to the time step additionally this study computes kge at stream gages downstream of the reservoir while gutenson et al 2020 computed kge at the reservoir this study used a simulated inflow whereas gutenson et al 2020 used an observed inflow thus we speculate that some combination of the simulation time step location where kge values are computed and type of inflow values utilized influence the best performing k d03 parameter at the dam another heuristic approach when dealing with hundreds of dams could involve the impoundment ratio i e difference between maximum and minim storage volume annual flow volume as a non data driven means for parameterizing the model the döll method uses a release coefficient to relate discharge to reservoir inflow dam discharge is heavily correlated with inflow when the impoundment ratio is low in other words a run of the river dam has very little impoundment storage and so discharge closely tracks inflow gutenson et al 2020 found that this relationship breaks down for dams with high impoundment ratios ir 3 thus there is an inverse relationship between the release coefficient and the impoundment ratio as opposed to applying a uniform release coefficient at each reservoir the release coefficients can be assigned as the inverse of the impoundment ratio 1 ir perhaps bounded between 0 01 and 0 9 to remain within the range considered by gutenson et al 2020 this appears to provide a reasonable estimate of the release coefficient although more robust parameter optimization could be beneficial when feasible based on fig 3b the optimal k d03 has an inverse relationship with the difference between maximum and minimum storage volumes of dams in ohio and tennessee regions regions 5 and 6 as well as missouri region region 10 for the relative small changes in storage volume k d03 0 90 was obtained as a best coefficient value in the regions 5 and 6 in contrast in the region 10 where there are the largest differences between storage changes k d03 0 01 resulted in the highest kge at the gage locations the results are not inclusive for other major regions of the mrb particularly region 10 we suggest further studies on relationship between k d03 and reservoir storage characters application of search algorithms such as practical particle swarm optimization pso and genetic algorithm ga can also be considered to calibrate the k d03 coefficient these search algorithms can facilitate the calibration performance hosseiny 2022 implemented pso and ga search algorithms to calibrate roughness coefficient in the international river interface cooperative iric model two dimensional hydraulic model a similar approach can be developed to calibrate the döll release coefficient 3 2 spatial kge distribution we compared rapid streamflow simulations with and without the d03 reservoir routing model against streamflow observations the d03 method with rapid simulation used the calibrated k d03 values from the previous section we calculated kge for all 44 daily usgs gages downstream of dams within the major regions of the mrb major regions of the mrb include 5 6 7 8 10 and 11 fig 4 a table 2 also lists the name of each major region in the mrb the kge values represent an overall model performance since it considers correlation variability and bias fig 4a shows the spatial distribution of kge values for rapid streamflow without reservoir routing fig 4b illustrates kge values for rapid streamflow with the optimized d03 release coefficient we also calculated the skill score of kge in fig 4c to identify normalized effect of reservoirs on the streamflow following the skill score equation introduced by zajac et al 2017 2 s k g e k g e w i t h r e s e r v o i r k g e w i t h o u t r e s e r v o i r 1 k g e w i t h o u t r e s e r v o i r where skge is the skill score of kge k g e w i t h r e s e r v o i r k g e w i t h o u t r e s e r v o i r are daily kge of rapid streamflow with and without reservoir consideration respectively the improvement of simulations at the gages differs from minor to major the ohio and tennessee regions regions 5 and 6 showed the smallest change in the flow simulations when reservoir routing was implemented compared to other major mrb regions table 2 as presented in fig 4a the streamflow simulation without reservoirs performed well with an average kge 0 46 the hydrological modeling of the eastern mrb outperformed the western regions mainly due to the lower hydrologic complexity of frozen ground and snow modeling methods within lsms mcguire 2014 tavakoly et al 2017 2021 xia et al 2012 reservoirs in the ohio and tennessee regions are primarily used for hydropower generation recreation and water supply so the behavior of reservoirs overall may have less impact on the discharge and flood mitigation since the d03 method calculates outflow based on the changes in storage it confirms the impact of reservoirs on outflow is not as high in this part of the mrb despite only minor improvement of kge score in the ohio and tennessee regions the d03 implementation captured the peak flows better overall we compared the hydrograph at the cumberland river at woodland station in section 3 3 we observed a considerable increase of s kge in the missouri river basin region 10 which includes four of the largest dams and reservoirs of the united states the major improvement shows that although this basin is heavily regulated it is possible to improve results using a non data driven approach to reservoir routing the highest maximum skge values were also derived in hydrologic unit code huc regions 8 11 7 and 10 in the descending order considerable improvement of results showed that reservoir routing is more impactful in the western regions of the mrb regions 10 and 11 that suggests reservoir regulation is a major contributor in the relatively poor performance of streamflow simulations in the western part of the mrb apparent in fig 4a and in identified in previous studies david et al 2015 tavakoly et al 2017 2021 the rapid model with reservoir routing demonstrated substantial kge increases at gages in the lower mississippi region two of the gages in this region are located downstream of four usace reservoirs that comprise the yazoo basin headwaters project the tallahatchie river at money gage usgs id 07281600 is downstream of arkabutla sardis and enid dams the second gage in this section of the mrb is the yalobusha river at grenada station usgs id 07285500 located downstream of grenada dam we found a skge 0 5 for both stations with even more improvement at the grenada station the kge increased from 0 65 to 0 21 at this gage location we further analyze the impact of parallel operation of these four dams in section 3 5 3 3 daily hydrograph comparison for selected gages this section evaluates hydrographs at seven gage locations fig 5 presents the hydrograph comparison for mississippi river at grand rapids cumberland river at woodland station and yalobusha river at grenada the kge values increased from negative to positive values at the grand rapids from 0 23 to 0 1 and grenada from 0 65 to 0 21 stations respectively table 3 the s kge value is 0 27 at the grand rapids station and 0 52 at the grenada station a moderate kge improvement occurred when rapid considers reservoir routing at the cumberland river at woodland gage the kge values are 0 56 and 0 63 in the rapid model without and with the reservoir routing module for this station although the kge is reasonable even without reservoir routing rapid better captured peak flows when reservoir routing was activated in the rapid fig 5b located in the upper mississippi region the mississippi river at grand rapids station is immediately downstream of pokegama lake dam and consequently in a heavily regulated zone the observed discharge at this station closely follows the pokegama dam release flow tavakoly et al 2021 and unregulated streamflow simulations showed major differences compared to observations the impact of inflow and reservoir outflow are further discussed in the next section for the pokegama dam visual evaluation of hydrographs for the yalobusha river at grenada gage demonstrated that the reservoir routing option improved the simulation considerably so the model better matched observations fig 5c the reservoir routing option lowered peak flows at the woodland station which resulted in better hydrograph simulation compared to the without reservoir simulation scenario fig 5b we compared daily hydrographs for missouri river at the garrison dam neosho river at burlington mountain fork near eagletown and mississippi river at vicksburg in fig 6 the kge values for the rapid streamflow simulation with and without reservoir routing for these gages are listed in table 3 the missouri at garrison dam station is in the missouri river basin downstream of garrison and fort peck dams the model results showed a 50 kge increase and kge skill of 0 11 at this location although the model results showed improvement with reservoir routing results do not compare well with observations the kge value for rapid with reservoir routing at this location is a meager 2 50 as expected non data driven reservoir routing methods like d03 method tend to oversimplify heavily regulated dam operations in the next section we set up full experiments to show the performance of model with respect to the different inflow scenarios for the garrison dam the kge results showed major improvement at the neosho river at burlington 50 and mountain fork near eagletown 230 stations when reservoir routing was considered in the model these two gages are located in the western part of the mississippi river basin previous studies identified these stations as locations where streamflow simulations without reservoirs were inadequate mainly due to the existence of reservoirs tavakoly et al 2017 2021 tavakoly et al 2021 showed that incorporation of the reservoir release flow in the streamflow simulation significantly improved results at the neosho river at burlington the result of this study indicates considerable improvement in the streamflow simulation with non data driven reservoir routing we further analyzed the upstream reservoir john redmond lake inflow and outflow simulations for the neosho river at burlington in the following sections routing 62 dams in this study also improved the streamflow simulation at the mouth of the mrb represented by the mississippi river at vicksburg gage fig 6d despite the fact that there is no immediate reservoir on the main mississippi river upstream of this gage and the magnitude of the river discharge yet the rapid streamflow results with d03 routing improved kge at this gage table 3 3 4 comparison of reservoir outflow simulations we conducted three experiments to compare reservoir outflow simulations with a rapid simulation scenario without any dam regulations in the first experiment we simulated reservoir outflow with rapid and the d03 module we applied the d03 method to simulate reservoir outflow using observed reservoir inflows in the second experiment this experiment is similar to previous work by gutenson et al 2020 in which we used observed inflow to the reservoir to evaluate the performance of reservoir routing methods hydrological models and lsms have inherent biases to better understand the effect of biases associated with lsm runoff products on reservoir outflow simulations we designed the third experiment for this purpose we induced the observed release flow from the upstream reservoir to the modeling system and then evaluated improvement of reservoir outflow simulation with rapid and the d03 module for the selected dams we compared outflow simulation results for pokegama lake dam john redmond lake and garrison dam three upstream dams including winnibigoshish dam council grove lake and fort peck dam are shown in fig 7 as an example for the third experiment the observed release flow from fort peck dam was introduced to the rapid model combined with additional downstream flow estimated by rapid from the lsm runoff which turns into an inflow to the garrison dam we described how to input reservoir release flow into rapid in our previous study tavakoly et al 2021 based on the first experiment inclusion of reservoir routing in the rapid model improved the reservoir outflow simulations compared to the rapid simulation without reservoir routing table 4 the highest improvement of modeling results was obtained for the pokegama lake dam s kge 0 42 the maximum storage of pokegama lake dam is 148 million cubic meters which is small when compared to john redmond lake 772 167 114 m3 and garrison dam 32 070 500 352 m3 moreover pokegama dam is emplaced on tributaries while john redmond lake and garrison dam are situated on larger rivers the kge and correlation coefficients substantially improved when rapid was run with the reservoir routing module for the pokegama lake dam the rmse values also decreased when rapid ran with reservoir routing the rmse improved 45 43 and 30 at the pokegama lake dam john redmond lake and garrison dam respectively results of the first experiment indicate that the döll routing scheme performs better for smaller reservoirs located on tributaries and lower order rivers based on the second experiment the outflow simulation results also showed that the best estimation of reservoir release flow was obtained with observed inflow for all three dams fig 8 highest skge and correlation as well as lowest rmse were gained in the second experiment for all three dams table 4 the results from this experiment are consistent with findings by turner et al 2020 the highest reduction of rmse for the observed inflow scenario was obtained for the garrison dam the fifth largest earthen dam in the world this dam is in series along with several other large dams on missouri river this cascading reservoir system modulates inflow and outflow well hence the streamflow simulation using observed inflow outperformed the other scenarios as expected fig 8c results from the third experiment indicated that reservoir outflow estimation using rapid and d03 improved when observed reservoir outflow from upstream reservoirs are introduced to the modeling system compared to the first experiment reservoir outflow simulation with rapid and the d03 module s kge values increased in the third experiment in all three reservoirs for the pokegama lake dam s kge increased substantially when observed outflow from the upstream dam winnibigoshish dam mn was considered in the simulation table 4 similarly s kge improved for the john redmond lake and the garrison dam in the third experiment in comparison with running rapid along with the reservoir routing scenario the main contribution to the reservoir routing improvement in the third experiment is due to the improvement of reservoir inflow estimation by inclusion of observed reservoir outflow to the system the accuracy of inflow estimation in the downstream reservoir increased and subsequently the reservoir outflow estimation showed higher accuracy compared to the first experiment analysis of inflow estimation is shown in table 4 and fig 9 inflow estimation improved when the upstream reservoir outflow was considered in the modeling system fig 9 the kge values of inflow simulated using upstream outflow showed 200 improvement compared to the simulated inflow by rapid scenario for the pokegama lake dam table 4 the estimated inflow to this dam is less flashy by inclusion of observed reservoir outflow from winnibigoshish dam therefore the significant improvement of inflow estimation resulted in better outflow simulation for the pokegama lake dam the short distance between winnibigoshish dam and pokegama lake dam approximately 40 km makes the inflow to the pokegama lake dam smooth with low differences between minimum and maximum flow fig 9a as a result the estimation of outflow using observed inflow for the pokegama lake dam obtained the highest correlation and kge among three experiments table 4 inflow estimation also improved 40 at the john redmond lake by including reservoir outflow from the upstream dam council grover lake the outflow estimation also showed 34 increase in the kge value for this dam inclusion of observed release flow from fort peck dam also improved inflow estimation of the garrison dam hydrograph comparison of inflow for the garrison dam showed lower peaks for the simulated inflow with upstream dam outflow scenario fig 9c the adjustment of inflow rendered better results with the outflow estimation of garrison dam as well 3 5 a system of dams and reservoir routing as part of the yazoo basin headwaters project usace vicksburg district owns and operates four dams to mitigate flooding in mississippi s delta region arkabutla lake history u s army corps of engineers 1987 although these four dams are not directly connected they operate in parallel to minimize the effect of flooding these four dams are located on tributaries of the lower mississippi river coldwater river arkabutla dam little tallahatchie river sardis dam yocona river enid dam and yalobusha river grenada dam from these four dams arkabutla sardis and enid dams are located upstream of tallahatchie river at money station fig 10 we ran rapid with the reservoir module for three scenarios and compared results with observations and without inclusion of dams first we only included sardis dam second we considered both sardis and enid dams third we included all three dams in the rapid model the improvement in the streamflow simulation proportionally increased when more dams were included at the tallahatchie river at money station the highest kge skill score occurred when we consider all three dams while the lowest s kge occurred for the rapid with only enid dam table 5 running rapid with only sardis dam improves the streamflow simulation in the downstream gage more than twice that of only enid dam in the rapid run a similar trend occurred for the correlation at the tallahatchie river at money station from these results we can imply that sardis dam has more impact on the downstream gage compared with enid dam the reservoir storage at sardis reservoir is twofold larger than the enid reservoir storage this suggests that the impact of reservoirs in the parallel system may be related to the size of reservoir storage bearing in mind the d03 reservoir routing is based on the changes in reservoir storage 4 conclusions in this study we introduce a new feature into the rapid model to consider the impact of reservoirs we implement the döll et al 2003 method a non data driven reservoir routing model within the rapid model the source code for the latest version of rapid with reservoir routing is available at the rapid github repository https github com c h david rapid we evaluate the reservoir routing at the continental scale of the mississippi river basin using 62 usace and tva reservoirs the mrb encompasses different geospatial regions such as ohio river basin where lsms have demonstrated satisfactory results even without reservoir routing as well as the heavily regulated missouri river basin with some of the largest dams in the united states where lsm performance has been relatively poor we ran the rapid model with reservoir routing at a 3 h time step and demonstrated the influence of reservoirs on the streamflow simulations we also compare the reservoir outflow simulation for different scenarios a comprehensive analysis of results yields the following conclusions the sensitivity analysis in this study shows that k d03 0 01 provides the best kge in approximately 70 of gages used in this study simulation time step the locations of optimization and the use of observed reservoir inflow may have an impact on determining an appropriate k d03 value furthermore we identified the release coefficient k d03 as an input parameter in the rapid model rather than a hard coded constant value this flexibility will allow a user to calibrate this parameter for a given study we suggested a few framework for future studies such as consideration of the impoundment ratio and search algorithms the streamflow results show that development of reservoir routing in the rapid model consistently improves model results improvements vary in different regions of the mrb with higher improvement in western regions lower mississippi and arkansas white red regions have the first and second highest skge values among mrb major regions although average skge was relatively low in the eastern regions of mrb ohio and tennessee regions the model simulations with d03 method better capture peak flows simulation of streamflow with reservoir routing also determines that dams in the ohio and tennessee regions have less impact on the downstream flow regime and flood mitigation skge results also indicate that non data driven reservoir routing like the döll method still improve model results in heavily regulation basin such as missouri river basin analysis of reservoir release simulations reveals that the rapid model with d03 reservoir routing is more impactful for dams with smaller reservoir storage simulation of reservoir outflow at the pokegama lake dam illustrates the highest skge among all the reservoirs furthermore our most skillful simulations of outflow occur when observed reservoir inflow was used to estimate release flow our experiments also demonstrate that in simulation cases where the observed reservoir release is available for at least one dam the streamflow prediction for non data driven reservoir routing downstream improves analysis also indicates that with better runoff estimation and subsequently streamflow simulation non data driven reservoir routing skill should improve the reservoir routing results at the downstream gage of a parallel system of dams shows the impact of upstream reservoirs may be proportionate to the reservoir storage however this conclusion is based on the d03 method reservoir routing specifically looking at the yazoo basin headwaters project in this paper we recommend further study using different reservoir routings on additional dam systems as a topic of future work the inclusion of d03 in rapid constitutes the first development of a non data driven method for reservoir routing in the rapid model to approximate reservoir discharge in data sparse areas the new reservoir routing feature in the rapid model is an important step forward in improving streamflow simulations and flood forecasting especially in an operational environment where release information may not be known as for future work we recommend including reservoir routing in the operational systems that use rapid such as the geoglows ecmwf system and streamflow prediction tool the new version of rapid could also help to better understand environmental impacts of dams on riverine systems such as water quality and sediment budgeting in addition the impact of climate change on the river flow can be better represented with reservoir routing enabled in rapid declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this project was supported by the mississippi river geomorphology and potamology mrg p program and the u s army corps of engineers engineer research and development center coastal inlets research program via congressionally directed r d with the national oceanic and atmospheric administration s national water center the mrg p program is part of the mississippi river and tributaries project and is managed by the u s army corps of engineers usace mississippi valley division mvd cédric h david is supported by the jet propulsion laboratory california institute of technology under a contract with the national aeronautics and space administration we would like to thank mr sean chi for his editorial assistance we gratefully acknowledge two anonymous reviewers for their highly constructive and valuable feedback which helped improve the impact of this article appendix a rapid reservoir module this document outlines requirements to run rapid with the reservoir routing option setting up the rapid model to run the reservoir routing includes two steps a specify the reservoir routing option in the namelist file and b prepare the input files that rapid requires to run the reservoir simulation a specify the reservoir routing option in the rapid namelist file the following line needs to be modified to run reservoir routing option image 1 number of dams in the study domain and paths to the reservoir information also need to be defined in the namelist corresponding variables to the reservoir option in the namelist with example paths are shown below in this example the total number of dams that were used in the study is eight river ids where the dams are located are listed in the dam tot id file and reservoir properties for these eight dams are listed in the dam file the format of dam tot id file and dam file are described in the next section image 2 rapid has an option to use a subset of total dams in a study domain this option can be determined by specifying number of dams in the is dam use variable the dam use id file variable includes the list of river ids corresponds to is dam use image 3 b input files to run the reservoir routing option in the rapid model two files are required to run reservoir routing with rapid in the first file dam file we list reservoir storage information along with the routing parameters for each dam figure a 1a in the second file dam tot id file we list river id corresponds to each reservoir figure a 1b columns c and d in fig 1a refer to the parameters in the döll method döll et al 2003 gutenson et al 2020 in the döll method the power coefficient was defined as 1 5 however this parameter is defined as an input parameter in the rapid so the user has flexibility to try different values and compare the results the release coefficient is also defined as an input parameter so the user can change the value in the calibration process note for the rapid simulation with reservoir routing the suggestion is to use constant value of 1 5 as a power coefficient the release coefficient can be varied in the calibration process using values of 0 01 0 02 0 04 0 06 0 08 0 10 0 20 0 40 0 50 0 60 0 70 0 80 and 0 90 fig ure a 1 example rapid input files to run the reservoir routing a the reservoir information file and b the river ids corresponding to dam locations in the study domain fig ure a 1 
25452,dams and reservoirs predictably alter the discharge of a river often confounding large scale flow simulations the routing application for parallel computation of discharge rapid model has been widely used operationally until recently rapid could only represent reservoir outflow by directly specifying discharges from dams herein we develop a non data driven reservoir routing approach within rapid results are comprehensively evaluated across the entire mississippi river basin mrb the reservoir routing resulted in considerable improvement in the lower mississippi and arkansas white red regions further the accuracy of estimated reservoir releases also benefits when the inflow estimation improves simulation of the reservoir release with observed reservoir inflow resulted in highest kling gupta efficiency kge among all experiments inclusion of reservoir routing in rapid will enhance the utility of this model in basins where dams exist and more importantly improves streamflow predictions by providing better hydrological information for decision makers keywords reservoir routing continental scale hydrological modeling rapid model rivers streams mississippi river basin data availability we provided links to the source code and datasets in the software and data availability section that we used in this study 1 introduction dams and their corresponding reservoirs segment river systems all around the world dynesius and nilsson 1994 postel et al 1996 currently dams immensely distort the natural streamflow for 77 of rivers while roughly half of the river networks are regulated worldwide boulange et al 2021 nilsson et al 2005 with more than 45 000 large dams around the world these structures moderate flood risk by attenuating river flow fluctuations gao et al 2012 they also support socio economic development by generating hydropower and providing water supply and recreational opportunities getirana et al 2020 passaia et al 2020 vanderkelen et al 2022 dams and their impoundments also impact regional climate change by affecting the local hydrological cycle zhao et al 2016 dams also exert influence on the environmental and ecological aspects of riverine systems such as sediment transport gupta et al 2012 renwick et al 2005 fish migration pathways stone 2016 and water quality indicators eiriksdottir et al 2017 kunz et al 2011 tavakoly et al 2016 winton et al 2019 notably the construction of large dams is escalating in the developing world in basins such as the amazon nile and tigris euphrates basins due to the growing demand for water and energy timpe and kaplan 2017 winemiller et al 2016 thus better understanding the impact of reservoir systems on hydrology and the environment is of paramount importance given the important role of dams and their associated reservoirs the integration of dam operations into continental scale hydrological modeling is imperative to support sustainable water resources management moreover inclusion of reservoir routing is critical for hydrological flood prediction salas et al 2017 tavakoly et al 2021 zajac et al 2017 owing to the lack of storage dependent release rule curves and local discretion in reservoir operations non data driven reservoir routing methods have been widely used in large scale hydrological models burek et al 2013 pokhrel et al 2016 yassin et al 2019 zajac et al 2017 in non data driven reservoir methods the operation of a reservoir is conceptualized without explicitly including operational rule curves or actual reservoir operations determining the optimal non data driven method involves tradeoffs between the complexity of the method and the data available de vos 2015 gutenson et al 2020 previous studies in continental scale modeling represented the impact of reservoir regulations by focusing mainly on mass balance and seasonal to sub seasonal variation of downstream river flow regimes e g chawanda et al 2020 hanasaki et al 2006 masaki et al 2017 pokhrel et al 2018 shin et al 2019 2020 despite great strides in simulating reservoir impacts on streamflow the spatial effect of a system of dams and reservoirs on diurnal and sub diurnal streamflow simulation is rarely emphasized in the existing literatures accordingly quantifying the effects from numerous upstream regulations on downstream flow conditions and on simulating reservoir release flows across continental river basins have received less attention simulation of river and reservoir systems at the sub diurnal scale is crucial before during and after flood events since floods typically happen over a relatively short time period and flood forecasting systems tend to have a relatively short lead time 0 10 days as a pioneering river network routing model the routing application for parallel computation of discharge rapid david et al 2011b modernized continental scale river flow modeling by using vector based river networks instead of traditional raster cell based datasets this feature plays a pivotal role in flood prediction and identifying potential flooded areas follum et al 2017 tavakoly et al 2021 the rapid model is an open source numerical model that is computationally efficient so it can provide timely streamflow estimates over a river network with hundreds of thousands of river reaches rapid has more than a decade of development evaluation validation and application over regional to global scales david et al 2011a 2013 2016 lin et al 2018 2019 sikder et al 2019 tavakoly et al 2017 2021 yang et al 2021 rapid has been applied to address diverse objectives of water resources and management including nutrient transport tavakoly et al 2016 2019 climate change forbes et al 2019 lewis et al 2022 meselhe et al 2021 and river aquifer interaction saleh et al 2011 flipo et al 2012 rapid is the main river routing component of the autorapid modeling system which simulates high resolution flood inundation maps over continental scale regions follum et al 2017 2020 within the hydrologic community rapid has had a significant impact including the following 1 the first published global application to produce an a priori dataset for nasa s surface water ocean topography swot mission 2 the first river routing component of noaa s national water model maidment 2017 salas et al 2017 3 the key modeling component in quasi operational hydrologic modeling frameworks supporting the u s military army air force and marine corps snow et al 2016 wahl et al 2016 4 operational implementation at the european centre for medium range weather forecasting https geoglows ecmwf int with global high resolution vector based river system qiao et al 2019 souffront et al 2019 and 5 lastly rapid is integrated in the nasa land information system lis kumar et al 2006 as part of lis hydro project and will be one of the main river routing models in the global hydro intelligence ghi operating plan jerry wegiel et al 2020 the main operational purposes in items 2 3 4 and 5 are to provide timely streamflow prediction as a hindcast nowcast and forecast regardless of all these applications until recently gutenson et al 2020 tavakoly et al 2021 rapid has lacked a reservoir routing scheme that can be used operationally from the comprehensive analysis of reservoir routings in our recent work gutenson et al 2020 we inferred that a reservoir routing method by döll et al 2003 hereafter referred to as d03 consistently improved diurnal streamflow simulation over naturalized flow conditions d03 requires minimum reservoir information is computationally inexpensive and can be applied by knowing an estimated inflow and the minimum and maximum storage volumes the d03 method can be globally applied since d03 requires such minimal reservoir information in this paper we offer a direct succession to our previous efforts gutenson et al 2020 evaluated the reservoir routing model d03 but did not implement the reservoir routing model within rapid david et al 2015 applied similar geospatial datasets and runoff products within rapid but did not include a reservoir routing component tavakoly et al 2021 explored reservoir outflow simulation by directly inserting observed reservoir releases into rapid rather than performing the reservoir routing building upon the previous research this paper fills the knowledge gap by developing and implementing a d03 inspired reservoir routing module within the rapid river network model we implemented the d03 model and evaluated the streamflow simulation results at the continental scale of the mississippi river basin mrb we identify the impact of reservoirs in different regions of the mrb and compare the model results with observations at different stream gage locations we also compare reservoir release flow with different reservoir implementations in the rapid model lastly we study the impact of a system of dams on the downstream flow dynamics 2 methods 2 1 streamflow river routing rapid david et al 2011b solves the matrix version of muskingum equation mccarthy 1938 in a river network comprised of hundreds of thousands river segments the equations and discretization of the governing equations were described in previous studies e g david et al 2011b emery et al 2020 tavakoly et al 2017 the model requires a pair of muskingum parameters k and x the k coefficient represents the propagation time of the flood wave down the river channels and x relates to diffusion of the flood wave cunge 1969 the rapid model uses a topological grid based a traditionally gridded geospatial setup in many river routing models or a vector based river network i e the blue lines on the maps this latter feature was first introduced in rapid and contributed to its adoption in the aforementioned forecasting systems using vector river networks also supports better topological inclusion of water bodies such as lakes and reservoirs interacting with the river network rapid requires runoff commonly simulated by various land surface models lsms as the dynamic input to compute streamflow the rapid model is agnostic to the source of runoff and topological river network hence it can be implemented over a variety of river networks with forcing from various lsms 2 2 development of reservoir routing in rapid the d03 reservoir routing conceptually attenuates flow variations according to changes in reservoir storage döll et al 2003 as follows 1 q t k d 03 δ t s t s min s t s min s max s min 1 5 where qt is the simulated reservoir outflow m3 s st is the reservoir storage m3 and smin and smax are the minimum and maximum reservoir storages m3 respectively δ t is the döll simulation time step in seconds and k d03 is the reservoir release coefficient typically ranging from 0 01 to 0 90 gutenson et al 2020 we consider the reservoirs dead pool storage to be smin δ t is equivalent to the runoff input time step represented by zs taur in the rapid code hence river routing and reservoir outflow are synchronized note that the actual river routing equations are usually solved at a finer time step david et al 2011b in this study rapid with reservoir routing runs at a 3 hourly runoff time step and a 30 min routing time step the k d03 coefficient is also an input parameter this allows the user to change the k d03 value to calibrate the model for a given study domain in addition to the rapid input files such as the runoff dataset river connectivity and muskingum parameters the location of the dam and two reservoir minimum and maximum storages values are required fig 1 to run rapid with d03 reservoir routing we created two additional input files and adapted the rapid source code for their use appendix a includes the instructions for preparing the dam input files along with sample reservoir data for running the reservoir routing module in rapid furthermore the location of reservoirs must be associated with the river network one advantage of using a vector river network is that the location of reservoirs can be easily assigned to the river segment in the rapid model with continental scale models running on a coarse grid some reservoirs and smaller tributaries might not be adequately represented this can result in grid based models wrongfully assigning a dam location and subsequently miscalculating the reservoir release flow shin et al 2019 reservoir locations are indicated with corresponding river ids in a separate rapid input file in appendix a this file is identified as a second reservoir input file 2 3 study domain and data processing we selected the mississippi river basin mrb with an area of 3 179 875 9 km2 to test and evaluate the rapid reservoir routing module the mrb has been extensively studied in continental hydrological contexts e g bain et al 2022 david et al 2015 tavakoly et al 2017 2021 we obtained the rapid input data excluding reservoir information from david et al 2015 the vector river network and contributing catchment area were based upon the shuttle elevation derivatives at multiple scales hydrosheds dataset lehner et al 2008 the study domain includes 102 229 river segments with an average length of 3 3 km and contributing catchment area of 31 11 km2 fig 2 the runoff input file was computed using the variable infiltration capacity land surface model vic lsm liang et al 1994 available from the nasa north land data assimilation system 2 nldas 2 mitchell et al 2004 xia et al 2012 similar lsm runoff was used by david et al 2015 and tavakoly et al 2017 and 2021 to drive rapid in the mrb we selected 62 dams and reservoirs to include into the rapid model for reservoir routing maximum storage varied in size from 39 540 to 26 000 016 50 acre feet 48 771 799 to 32 070 500 352 m3 the smallest and largest reservoirs are big hill lake in kansas and lake sakakawea created by garrison dam in north dakota respectively the selected dams cover major mrb regions differing in both topography and climate furthermore we were able to identify usgs gages with full record of observed discharge within the study time period downstream of these dams we collected reservoir information for 60 dams from nine united states army corps of engineers usace districts kansas city little rock nashville omaha pittsburgh rock island st paul tulsa and vicksburg districts we obtained additional reservoir information for pickwick and kentucky dams from the tennessee valley authority tva the locations of our selected reservoirs are geographically diverse including upstream regions of the ohio river the upper mississippi river the arkansas river the red river and the missouri river we compared model results with daily streamflow data from the u s geological survey usgs national water information system nwis for the simulation time period 01 01 2000 12 31 2009 we identified 43 gages that do not have missing records during the simulation period and are located downstream of at least one dam in the domain fig 2 therefore comparison of rapid streamflow with observation shows the impact of reservoir inclusion on the modeling results the drainage area of gages ranges from 572 0 km2 at the french creek near union city station in pennsylvania to 2 964 241 4 km2 at the vicksburg station in mississippi of the 43 usgs gages we selected seven gages to present hydrograph comparisons with and without rapid reservoir routing in major basins of the mrb shown with the red colored dots in fig 2 these seven gages cover spatially diverse topographic areas drainage area hydrologically different regimes and flow magnitudes table 1 lists the station name usgs id location longitude and latitude and contributing drainage areas of each usgs gage the mississippi river at vicksburg station was selected to represent nearly all major tributaries of the mrb 3 results 3 1 model parameter sensitivity analysis before comparing the streamflow simulation with and without reservoir routing we conduct a sensitivity analysis for the reservoir release coefficient k d03 used in the d03 method the k d03 parameter is defined as an arbitrary parameter in the rapid input file döll et al 2003 originally held the k d03 parameter constant at 0 01 there has been no relationship confirmed between the k d03 parameter and measurable characteristics of a reservoir or its geographic setting thus for practical purposes the k d03 parameter can serve as a calibration term for rapid gutenson et al 2020 we describe one method below for setting default values of the k d03 parameter for global applications in this instance the rapid model with reservoir routing was run with multiple k d03 coefficient values 0 01 0 02 0 04 0 06 0 08 0 10 0 30 0 50 0 70 and 0 90 we ran rapid for the study domain using each of these as the k d03 coefficient value based on the selected values of k d03 as the only parameter for calibration rapid was run ten different times the kling gupta efficiency kge gupta et al 2012 was calculated for each rapid run and the k d03 coefficient value resulting in the highest kge at each of 43 gage locations was then identified in a practical calibration for global applications reservoirs with no stream gage observations downstream of their location can be set to the most common k d03 coefficient value defined in this process either regionally or globally the result of this sensitivity test differed from the previous study by gutenson et al 2020 where 0 9 tends to be the best performing k d03 value our sensitivity analysis yielded 0 01 as the predominant k d03 value resulting in the highest kge value i e the maximum value of kge among ten rapid runs with each of the k d03 values at gage locations which matches the original formulation of d03 that utilizes a k d03 0 01 döll et al 2003 fig 3 shows that k d03 0 01 derived the highest kge in 70 of gages there are three differences that may lead to the different k d03 coefficients found by gutenson et al 2020 and this study the differences relate to the time step of the simulation the locations targeted for optimization and the type of inflow values utilized we utilized a delta t value of 3 h 10 800 s in this study while gutenson et al 2020 utilized a daily time step δt 86 400 s this suggests that k d03 is sensitive to the time step additionally this study computes kge at stream gages downstream of the reservoir while gutenson et al 2020 computed kge at the reservoir this study used a simulated inflow whereas gutenson et al 2020 used an observed inflow thus we speculate that some combination of the simulation time step location where kge values are computed and type of inflow values utilized influence the best performing k d03 parameter at the dam another heuristic approach when dealing with hundreds of dams could involve the impoundment ratio i e difference between maximum and minim storage volume annual flow volume as a non data driven means for parameterizing the model the döll method uses a release coefficient to relate discharge to reservoir inflow dam discharge is heavily correlated with inflow when the impoundment ratio is low in other words a run of the river dam has very little impoundment storage and so discharge closely tracks inflow gutenson et al 2020 found that this relationship breaks down for dams with high impoundment ratios ir 3 thus there is an inverse relationship between the release coefficient and the impoundment ratio as opposed to applying a uniform release coefficient at each reservoir the release coefficients can be assigned as the inverse of the impoundment ratio 1 ir perhaps bounded between 0 01 and 0 9 to remain within the range considered by gutenson et al 2020 this appears to provide a reasonable estimate of the release coefficient although more robust parameter optimization could be beneficial when feasible based on fig 3b the optimal k d03 has an inverse relationship with the difference between maximum and minimum storage volumes of dams in ohio and tennessee regions regions 5 and 6 as well as missouri region region 10 for the relative small changes in storage volume k d03 0 90 was obtained as a best coefficient value in the regions 5 and 6 in contrast in the region 10 where there are the largest differences between storage changes k d03 0 01 resulted in the highest kge at the gage locations the results are not inclusive for other major regions of the mrb particularly region 10 we suggest further studies on relationship between k d03 and reservoir storage characters application of search algorithms such as practical particle swarm optimization pso and genetic algorithm ga can also be considered to calibrate the k d03 coefficient these search algorithms can facilitate the calibration performance hosseiny 2022 implemented pso and ga search algorithms to calibrate roughness coefficient in the international river interface cooperative iric model two dimensional hydraulic model a similar approach can be developed to calibrate the döll release coefficient 3 2 spatial kge distribution we compared rapid streamflow simulations with and without the d03 reservoir routing model against streamflow observations the d03 method with rapid simulation used the calibrated k d03 values from the previous section we calculated kge for all 44 daily usgs gages downstream of dams within the major regions of the mrb major regions of the mrb include 5 6 7 8 10 and 11 fig 4 a table 2 also lists the name of each major region in the mrb the kge values represent an overall model performance since it considers correlation variability and bias fig 4a shows the spatial distribution of kge values for rapid streamflow without reservoir routing fig 4b illustrates kge values for rapid streamflow with the optimized d03 release coefficient we also calculated the skill score of kge in fig 4c to identify normalized effect of reservoirs on the streamflow following the skill score equation introduced by zajac et al 2017 2 s k g e k g e w i t h r e s e r v o i r k g e w i t h o u t r e s e r v o i r 1 k g e w i t h o u t r e s e r v o i r where skge is the skill score of kge k g e w i t h r e s e r v o i r k g e w i t h o u t r e s e r v o i r are daily kge of rapid streamflow with and without reservoir consideration respectively the improvement of simulations at the gages differs from minor to major the ohio and tennessee regions regions 5 and 6 showed the smallest change in the flow simulations when reservoir routing was implemented compared to other major mrb regions table 2 as presented in fig 4a the streamflow simulation without reservoirs performed well with an average kge 0 46 the hydrological modeling of the eastern mrb outperformed the western regions mainly due to the lower hydrologic complexity of frozen ground and snow modeling methods within lsms mcguire 2014 tavakoly et al 2017 2021 xia et al 2012 reservoirs in the ohio and tennessee regions are primarily used for hydropower generation recreation and water supply so the behavior of reservoirs overall may have less impact on the discharge and flood mitigation since the d03 method calculates outflow based on the changes in storage it confirms the impact of reservoirs on outflow is not as high in this part of the mrb despite only minor improvement of kge score in the ohio and tennessee regions the d03 implementation captured the peak flows better overall we compared the hydrograph at the cumberland river at woodland station in section 3 3 we observed a considerable increase of s kge in the missouri river basin region 10 which includes four of the largest dams and reservoirs of the united states the major improvement shows that although this basin is heavily regulated it is possible to improve results using a non data driven approach to reservoir routing the highest maximum skge values were also derived in hydrologic unit code huc regions 8 11 7 and 10 in the descending order considerable improvement of results showed that reservoir routing is more impactful in the western regions of the mrb regions 10 and 11 that suggests reservoir regulation is a major contributor in the relatively poor performance of streamflow simulations in the western part of the mrb apparent in fig 4a and in identified in previous studies david et al 2015 tavakoly et al 2017 2021 the rapid model with reservoir routing demonstrated substantial kge increases at gages in the lower mississippi region two of the gages in this region are located downstream of four usace reservoirs that comprise the yazoo basin headwaters project the tallahatchie river at money gage usgs id 07281600 is downstream of arkabutla sardis and enid dams the second gage in this section of the mrb is the yalobusha river at grenada station usgs id 07285500 located downstream of grenada dam we found a skge 0 5 for both stations with even more improvement at the grenada station the kge increased from 0 65 to 0 21 at this gage location we further analyze the impact of parallel operation of these four dams in section 3 5 3 3 daily hydrograph comparison for selected gages this section evaluates hydrographs at seven gage locations fig 5 presents the hydrograph comparison for mississippi river at grand rapids cumberland river at woodland station and yalobusha river at grenada the kge values increased from negative to positive values at the grand rapids from 0 23 to 0 1 and grenada from 0 65 to 0 21 stations respectively table 3 the s kge value is 0 27 at the grand rapids station and 0 52 at the grenada station a moderate kge improvement occurred when rapid considers reservoir routing at the cumberland river at woodland gage the kge values are 0 56 and 0 63 in the rapid model without and with the reservoir routing module for this station although the kge is reasonable even without reservoir routing rapid better captured peak flows when reservoir routing was activated in the rapid fig 5b located in the upper mississippi region the mississippi river at grand rapids station is immediately downstream of pokegama lake dam and consequently in a heavily regulated zone the observed discharge at this station closely follows the pokegama dam release flow tavakoly et al 2021 and unregulated streamflow simulations showed major differences compared to observations the impact of inflow and reservoir outflow are further discussed in the next section for the pokegama dam visual evaluation of hydrographs for the yalobusha river at grenada gage demonstrated that the reservoir routing option improved the simulation considerably so the model better matched observations fig 5c the reservoir routing option lowered peak flows at the woodland station which resulted in better hydrograph simulation compared to the without reservoir simulation scenario fig 5b we compared daily hydrographs for missouri river at the garrison dam neosho river at burlington mountain fork near eagletown and mississippi river at vicksburg in fig 6 the kge values for the rapid streamflow simulation with and without reservoir routing for these gages are listed in table 3 the missouri at garrison dam station is in the missouri river basin downstream of garrison and fort peck dams the model results showed a 50 kge increase and kge skill of 0 11 at this location although the model results showed improvement with reservoir routing results do not compare well with observations the kge value for rapid with reservoir routing at this location is a meager 2 50 as expected non data driven reservoir routing methods like d03 method tend to oversimplify heavily regulated dam operations in the next section we set up full experiments to show the performance of model with respect to the different inflow scenarios for the garrison dam the kge results showed major improvement at the neosho river at burlington 50 and mountain fork near eagletown 230 stations when reservoir routing was considered in the model these two gages are located in the western part of the mississippi river basin previous studies identified these stations as locations where streamflow simulations without reservoirs were inadequate mainly due to the existence of reservoirs tavakoly et al 2017 2021 tavakoly et al 2021 showed that incorporation of the reservoir release flow in the streamflow simulation significantly improved results at the neosho river at burlington the result of this study indicates considerable improvement in the streamflow simulation with non data driven reservoir routing we further analyzed the upstream reservoir john redmond lake inflow and outflow simulations for the neosho river at burlington in the following sections routing 62 dams in this study also improved the streamflow simulation at the mouth of the mrb represented by the mississippi river at vicksburg gage fig 6d despite the fact that there is no immediate reservoir on the main mississippi river upstream of this gage and the magnitude of the river discharge yet the rapid streamflow results with d03 routing improved kge at this gage table 3 3 4 comparison of reservoir outflow simulations we conducted three experiments to compare reservoir outflow simulations with a rapid simulation scenario without any dam regulations in the first experiment we simulated reservoir outflow with rapid and the d03 module we applied the d03 method to simulate reservoir outflow using observed reservoir inflows in the second experiment this experiment is similar to previous work by gutenson et al 2020 in which we used observed inflow to the reservoir to evaluate the performance of reservoir routing methods hydrological models and lsms have inherent biases to better understand the effect of biases associated with lsm runoff products on reservoir outflow simulations we designed the third experiment for this purpose we induced the observed release flow from the upstream reservoir to the modeling system and then evaluated improvement of reservoir outflow simulation with rapid and the d03 module for the selected dams we compared outflow simulation results for pokegama lake dam john redmond lake and garrison dam three upstream dams including winnibigoshish dam council grove lake and fort peck dam are shown in fig 7 as an example for the third experiment the observed release flow from fort peck dam was introduced to the rapid model combined with additional downstream flow estimated by rapid from the lsm runoff which turns into an inflow to the garrison dam we described how to input reservoir release flow into rapid in our previous study tavakoly et al 2021 based on the first experiment inclusion of reservoir routing in the rapid model improved the reservoir outflow simulations compared to the rapid simulation without reservoir routing table 4 the highest improvement of modeling results was obtained for the pokegama lake dam s kge 0 42 the maximum storage of pokegama lake dam is 148 million cubic meters which is small when compared to john redmond lake 772 167 114 m3 and garrison dam 32 070 500 352 m3 moreover pokegama dam is emplaced on tributaries while john redmond lake and garrison dam are situated on larger rivers the kge and correlation coefficients substantially improved when rapid was run with the reservoir routing module for the pokegama lake dam the rmse values also decreased when rapid ran with reservoir routing the rmse improved 45 43 and 30 at the pokegama lake dam john redmond lake and garrison dam respectively results of the first experiment indicate that the döll routing scheme performs better for smaller reservoirs located on tributaries and lower order rivers based on the second experiment the outflow simulation results also showed that the best estimation of reservoir release flow was obtained with observed inflow for all three dams fig 8 highest skge and correlation as well as lowest rmse were gained in the second experiment for all three dams table 4 the results from this experiment are consistent with findings by turner et al 2020 the highest reduction of rmse for the observed inflow scenario was obtained for the garrison dam the fifth largest earthen dam in the world this dam is in series along with several other large dams on missouri river this cascading reservoir system modulates inflow and outflow well hence the streamflow simulation using observed inflow outperformed the other scenarios as expected fig 8c results from the third experiment indicated that reservoir outflow estimation using rapid and d03 improved when observed reservoir outflow from upstream reservoirs are introduced to the modeling system compared to the first experiment reservoir outflow simulation with rapid and the d03 module s kge values increased in the third experiment in all three reservoirs for the pokegama lake dam s kge increased substantially when observed outflow from the upstream dam winnibigoshish dam mn was considered in the simulation table 4 similarly s kge improved for the john redmond lake and the garrison dam in the third experiment in comparison with running rapid along with the reservoir routing scenario the main contribution to the reservoir routing improvement in the third experiment is due to the improvement of reservoir inflow estimation by inclusion of observed reservoir outflow to the system the accuracy of inflow estimation in the downstream reservoir increased and subsequently the reservoir outflow estimation showed higher accuracy compared to the first experiment analysis of inflow estimation is shown in table 4 and fig 9 inflow estimation improved when the upstream reservoir outflow was considered in the modeling system fig 9 the kge values of inflow simulated using upstream outflow showed 200 improvement compared to the simulated inflow by rapid scenario for the pokegama lake dam table 4 the estimated inflow to this dam is less flashy by inclusion of observed reservoir outflow from winnibigoshish dam therefore the significant improvement of inflow estimation resulted in better outflow simulation for the pokegama lake dam the short distance between winnibigoshish dam and pokegama lake dam approximately 40 km makes the inflow to the pokegama lake dam smooth with low differences between minimum and maximum flow fig 9a as a result the estimation of outflow using observed inflow for the pokegama lake dam obtained the highest correlation and kge among three experiments table 4 inflow estimation also improved 40 at the john redmond lake by including reservoir outflow from the upstream dam council grover lake the outflow estimation also showed 34 increase in the kge value for this dam inclusion of observed release flow from fort peck dam also improved inflow estimation of the garrison dam hydrograph comparison of inflow for the garrison dam showed lower peaks for the simulated inflow with upstream dam outflow scenario fig 9c the adjustment of inflow rendered better results with the outflow estimation of garrison dam as well 3 5 a system of dams and reservoir routing as part of the yazoo basin headwaters project usace vicksburg district owns and operates four dams to mitigate flooding in mississippi s delta region arkabutla lake history u s army corps of engineers 1987 although these four dams are not directly connected they operate in parallel to minimize the effect of flooding these four dams are located on tributaries of the lower mississippi river coldwater river arkabutla dam little tallahatchie river sardis dam yocona river enid dam and yalobusha river grenada dam from these four dams arkabutla sardis and enid dams are located upstream of tallahatchie river at money station fig 10 we ran rapid with the reservoir module for three scenarios and compared results with observations and without inclusion of dams first we only included sardis dam second we considered both sardis and enid dams third we included all three dams in the rapid model the improvement in the streamflow simulation proportionally increased when more dams were included at the tallahatchie river at money station the highest kge skill score occurred when we consider all three dams while the lowest s kge occurred for the rapid with only enid dam table 5 running rapid with only sardis dam improves the streamflow simulation in the downstream gage more than twice that of only enid dam in the rapid run a similar trend occurred for the correlation at the tallahatchie river at money station from these results we can imply that sardis dam has more impact on the downstream gage compared with enid dam the reservoir storage at sardis reservoir is twofold larger than the enid reservoir storage this suggests that the impact of reservoirs in the parallel system may be related to the size of reservoir storage bearing in mind the d03 reservoir routing is based on the changes in reservoir storage 4 conclusions in this study we introduce a new feature into the rapid model to consider the impact of reservoirs we implement the döll et al 2003 method a non data driven reservoir routing model within the rapid model the source code for the latest version of rapid with reservoir routing is available at the rapid github repository https github com c h david rapid we evaluate the reservoir routing at the continental scale of the mississippi river basin using 62 usace and tva reservoirs the mrb encompasses different geospatial regions such as ohio river basin where lsms have demonstrated satisfactory results even without reservoir routing as well as the heavily regulated missouri river basin with some of the largest dams in the united states where lsm performance has been relatively poor we ran the rapid model with reservoir routing at a 3 h time step and demonstrated the influence of reservoirs on the streamflow simulations we also compare the reservoir outflow simulation for different scenarios a comprehensive analysis of results yields the following conclusions the sensitivity analysis in this study shows that k d03 0 01 provides the best kge in approximately 70 of gages used in this study simulation time step the locations of optimization and the use of observed reservoir inflow may have an impact on determining an appropriate k d03 value furthermore we identified the release coefficient k d03 as an input parameter in the rapid model rather than a hard coded constant value this flexibility will allow a user to calibrate this parameter for a given study we suggested a few framework for future studies such as consideration of the impoundment ratio and search algorithms the streamflow results show that development of reservoir routing in the rapid model consistently improves model results improvements vary in different regions of the mrb with higher improvement in western regions lower mississippi and arkansas white red regions have the first and second highest skge values among mrb major regions although average skge was relatively low in the eastern regions of mrb ohio and tennessee regions the model simulations with d03 method better capture peak flows simulation of streamflow with reservoir routing also determines that dams in the ohio and tennessee regions have less impact on the downstream flow regime and flood mitigation skge results also indicate that non data driven reservoir routing like the döll method still improve model results in heavily regulation basin such as missouri river basin analysis of reservoir release simulations reveals that the rapid model with d03 reservoir routing is more impactful for dams with smaller reservoir storage simulation of reservoir outflow at the pokegama lake dam illustrates the highest skge among all the reservoirs furthermore our most skillful simulations of outflow occur when observed reservoir inflow was used to estimate release flow our experiments also demonstrate that in simulation cases where the observed reservoir release is available for at least one dam the streamflow prediction for non data driven reservoir routing downstream improves analysis also indicates that with better runoff estimation and subsequently streamflow simulation non data driven reservoir routing skill should improve the reservoir routing results at the downstream gage of a parallel system of dams shows the impact of upstream reservoirs may be proportionate to the reservoir storage however this conclusion is based on the d03 method reservoir routing specifically looking at the yazoo basin headwaters project in this paper we recommend further study using different reservoir routings on additional dam systems as a topic of future work the inclusion of d03 in rapid constitutes the first development of a non data driven method for reservoir routing in the rapid model to approximate reservoir discharge in data sparse areas the new reservoir routing feature in the rapid model is an important step forward in improving streamflow simulations and flood forecasting especially in an operational environment where release information may not be known as for future work we recommend including reservoir routing in the operational systems that use rapid such as the geoglows ecmwf system and streamflow prediction tool the new version of rapid could also help to better understand environmental impacts of dams on riverine systems such as water quality and sediment budgeting in addition the impact of climate change on the river flow can be better represented with reservoir routing enabled in rapid declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this project was supported by the mississippi river geomorphology and potamology mrg p program and the u s army corps of engineers engineer research and development center coastal inlets research program via congressionally directed r d with the national oceanic and atmospheric administration s national water center the mrg p program is part of the mississippi river and tributaries project and is managed by the u s army corps of engineers usace mississippi valley division mvd cédric h david is supported by the jet propulsion laboratory california institute of technology under a contract with the national aeronautics and space administration we would like to thank mr sean chi for his editorial assistance we gratefully acknowledge two anonymous reviewers for their highly constructive and valuable feedback which helped improve the impact of this article appendix a rapid reservoir module this document outlines requirements to run rapid with the reservoir routing option setting up the rapid model to run the reservoir routing includes two steps a specify the reservoir routing option in the namelist file and b prepare the input files that rapid requires to run the reservoir simulation a specify the reservoir routing option in the rapid namelist file the following line needs to be modified to run reservoir routing option image 1 number of dams in the study domain and paths to the reservoir information also need to be defined in the namelist corresponding variables to the reservoir option in the namelist with example paths are shown below in this example the total number of dams that were used in the study is eight river ids where the dams are located are listed in the dam tot id file and reservoir properties for these eight dams are listed in the dam file the format of dam tot id file and dam file are described in the next section image 2 rapid has an option to use a subset of total dams in a study domain this option can be determined by specifying number of dams in the is dam use variable the dam use id file variable includes the list of river ids corresponds to is dam use image 3 b input files to run the reservoir routing option in the rapid model two files are required to run reservoir routing with rapid in the first file dam file we list reservoir storage information along with the routing parameters for each dam figure a 1a in the second file dam tot id file we list river id corresponds to each reservoir figure a 1b columns c and d in fig 1a refer to the parameters in the döll method döll et al 2003 gutenson et al 2020 in the döll method the power coefficient was defined as 1 5 however this parameter is defined as an input parameter in the rapid so the user has flexibility to try different values and compare the results the release coefficient is also defined as an input parameter so the user can change the value in the calibration process note for the rapid simulation with reservoir routing the suggestion is to use constant value of 1 5 as a power coefficient the release coefficient can be varied in the calibration process using values of 0 01 0 02 0 04 0 06 0 08 0 10 0 20 0 40 0 50 0 60 0 70 0 80 and 0 90 fig ure a 1 example rapid input files to run the reservoir routing a the reservoir information file and b the river ids corresponding to dam locations in the study domain fig ure a 1 
25453,mid winter breakups mwbs are an increasingly common event on canadian rivers resulting in the early breakup of river ice cover with complex and numerous drivers this study focuses on the development of an mwb ontology which allows the key data events and relationships in an ice season to be defined and analyzed the mwb ontology is applied to a national case study of 54 rivers in canada through assessment of the mwb ontology with network analysis techniques a hybrid modelling framework coupling the mwb ontology with machine learning is developed the hybrid models produce greatly reduced errors in their forecasts of the timing breakup events with the best performance being a mean absolute error of 12 47 days for mwbs and 10 68 days for spring breakup the results demonstrate the utility of the mwb ontology as a tool for collating data and a means for analysis and forecasting of these events keywords mid winter breakups semantic modelling river ice machine learning ontology based analytics data availability data is available via a public repository linked in the manuscript 1 introduction the breakup of developed ice covers is an important event in the hydrology and morphology of many northern rivers often governing the peak flows of the season beltaos and prowse 2009 these events occur after a combination of increased temperatures liquid precipitation and increased snowmelt runoff raise water levels and increase the rate of thermal decay of the ice cover leading to open water flows being restored the ice cover can melt in place in the case of a thermal breakup or be lifted out of place and carried with the flow of the river in the case of a dynamic breakup which often leads to the highest flows of the year and the potential for ice jam flooding beltaos 2003 while these events most often occur in the spring and result in the restoration of open water flow the occurrence of mid winter breakups mwbs is becoming increasingly common in canada newton et al 2017 these events consist of the early breakup of a river ice cover either thermally or dynamically as a result of unseasonal high temperatures and thaws and can trigger early high flows outside of the typical breakup season these will be followed by a second freeze up of ice on the river restoring ice affected flows and heavily altering the flow regime of the river for the remainder of the season while also resulting in a less developed ice cover being present for breakup in the spring de coste et al 2022a the result of this is an increased potential for mid winter ice jams and an increased likelihood of thermal breakup in the spring because these events have a significant impact on the latter half of the ice season it is of great interest to be able to accurately predict both their occurrence and the subsequent occurrence of the spring breakup the prediction of these events however is challenging due to the complexity and availability of data involved in their prediction the drivers and processes of either breakup event type are well researched with the main factors governing breakup occurrence being linked to air temperature flow rates within the river and the occurrence of liquid precipitation beltaos et al 2006 though these factors play a key role in both the timing and type of breakup the breakup mechanism is also governed by the morphology of the river as well as the hydrological factors of the surrounding basin de rham et al 2008 these factors are also affected by the presence of regulation which has a significant impact on the timing and mechanism of breakup as well and with the presence of these structures also generally reducing the magnitude of flows and potential flooding downstream turcotte and morse 2013 spring breakups in general are trending towards an earlier occurrence with shorter ice seasons and longer breakup periods throughout the country yang et al 2020 while mwbs are becoming increasingly common on a national scale and often causing some of the most significant flows for a given river de coste et al 2022a mwbs have been typically studied on a per river basis extending up to regional scales with hydroclimatic thresholds developed to predict their occurrence prowse et al 2002 carr and vuyovich 2014 though these thresholds were successful for the locations they were developed for subsequent studies have demonstrated their limitations and have produced new defined thresholds for mwb predictions exceeding their accuracy newton et al 2017 applications of machine learning based on the suggestions of these studies have been able to successfully predict the severity of mwbs and the timing though novel applications of multi level models were required for the latter case de coste et al 2022a 2022b machine learning has also been extended to the prediction of spring breakup using techniques such as artificial neural networks ann zhao et al 2012 guo et al 2018 support vector machines svm wang et al 2010 barzegar et al 2019 k nearest neighbors knn sun et al 2020 adaptive neuro fuzzy inference systems anfis sun and trevor 2015 2018a and ensemble techniques such as stacking ensembles sun 2018 similar studies have applied these techniques in the prediction of breakup ice jams using methods such as anns massie et al 2002 anfis mahabir et al 2006 and stacking ensembles de coste et al 2021 the challenges faced by these studies often revolve around data availability with studies focussing on single rivers having smaller amounts of data to train models with and studies focussing on larger regions encountering issues related to the management of complex data this also presents challenges in the delivery of user friendly decision making tools to affected locations as the selection of data and presentation of final models can be unintuitive ontology based semantic modelling presents an opportunity to address the knowledge and data management issues inherent to the complexity of the prediction of these types of ice related hydrologic events these models represent a domain in terms of concepts and relationships the relationships inherent between the data contained in these systems can be formalized into computational ontologies with a conceptual structure allowing further analysis of the data and relationships within guarino et al 2009 an ontology can be defined to describe a specific concept or process with multiple ontological structures being possible to describe the same domain brank et al 2005 more comprehensive ontological structures can be developed through combining individual ontologies or iterating upon existing structures allowing a versatility not possible in other model types while also opening the way for application of network analysis techniques noy and musen 2004 the structure of ontologies enables the smooth sharing of data between systems within the same domain while associating the data with the domain knowledge providing a direct link between disparate data points that would not otherwise be present while also allowing model functionality when data is missing munir and anjum 2018 many successful applications of ontologies to the conceptualization and description of hydrology and hydrology related concepts exist including flood management mughal et al 2021 agresta et al 2002 roller et al 2015 yi and sun 2013 flood risk assessment scheur et al 2013 flood forecasting agresta et al 2014 management of hydrological metadata essawy et al 2017 and remote sensing potnis et al 2018 applications of ontologies in river ice related fields however has not yet been completed and remains a field open to novel application of these techniques this paper details the development of an ontology describing the domain of an ice season with an mwb providing a means for further development of a hybrid modelling system to forecast the timing of mwbs and the subsequent spring breakups the developed mwb ontology allows a novel means of organizing the data and analyzing the connections between the relevant hydrological variables linked to both events while the machine learning models developed based on the conclusions of the ontology allow for accurate predictions of both breakups with long lead times this framework provides a novel means of predicting the timing of these rare events while also producing an easy to use tool to sort and analyse relevant data for end users in affected areas this is the first attempt at applying these techniques in the prediction of mwbs in canada a national case study of rivers in canada is used to demonstrate the effectiveness of the developed models showing its applicability throughout the country 2 methodology 2 1 ontology based semantic model the primary goal of the ontology based semantic modelling in this study was the development of an mwb ontology the domain of this ontology is an ice season including an mwb with relationships and entities sourced from the relevant crid data clearly defined the scope of the ontology needs to be clear before construction can begin thus the key concepts relevant to this ontology need to be well understood mughal et al 2021 the domain is an mwb ice season with the events and data being the entities timing and datatypes defining relationships between the different events in the season subdivision of aspects for each event of the ontology is important due to the complexity and variability of the data measured for each event within the mwb ontology individuals are the values in a node such as flows and water levels relationships are the links from one node to another such as mwm date of and axioms are the definitions of the relationship type between nodes guarino et al 2009 datatypes are clearly defined for each of the nodes containing direct data float integer date etc the construction of the mwb ontology was based around the methontology framework lopez et al 1999 with 4 key stages in the process 1 specification at this stage the developer defines the scope and usage purpose of the ontology competency questions used for assessing the effectiveness of the ontology in achieving the scope and usage are also formulated at this stage 2 conceptualization the general structure and key entities are identified and defined in this stage properties relationships and axioms are formulated connecting the relevant nodes together in order to satisfy the scope and usage parameters defined in the specification stage 3 formalization and implementation the structure developed in the conceptualization stage is constructed in an ontology web language owl 4 evaluation the constructed ontology is tested against the previously specified competency questions to assess it s capability at fulfilling the scope and usage purposes if the model is found to be unsatisfactory in answering the competency questions the process can iterate and refinements can be made to the scope and the competency questions to ensure a better model if answers to the competency questions are considered satisfactory then the model construction is considered complete 2 2 machine learning models and development 2 2 1 regression algorithms algorithms utilised in this study have demonstrated success in hybridization applications with ontology in previous studies de coste et al 2022c and are detailed below multiple linear regression mlr mlr models develop a single equation describing the relationship between a single or series of variables and a target predictand uyanik and guler 2013 the equation is fit using least squares by minimising the mean squared error mse applications of mlr equations have been successful in the prediction of ice affected and open water river flows chokmani et al 2007 stampoulis et al 2020 yang et al 2021 seidou and ouarda 2007 regression trees rt these models use a series of decision nodes based on values of specific variables used in the regression lewis 2000 each node passes the observation to another dependent on variable values until a final leaf node is reached producing the final model prediction the structure of the tree including the number of nodes splits and decisions is specified through the user typically based on the analysis of prediction errors rt models have been successfully applied to the prediction of breakup timing and flooding sun 2018 janizadeh et al 2021 k nearest neighbors knn knn models estimate the value of an observation based on the observation variables similarities to its nearest neighbors the amount of neighbors considered for each observation k is specified by the user song et al 2017 the value for the observation is assigned using a distance function with compares the values to the selected amount of neighbors knn has been used for regression based modelling of river ice breakup timing and open water flow predictions sun et al 2020 poul et al 2019 random forest rf these models are considered an ensemble algorithm in which a forest of rt models are constructed with each trained on using a bootstrap sample taken from the full dataset segal 2003 as each of the bootstrap samples is unique each of the constructed trees are likewise unique resulting in more versatile predictions obtained through combining the results of the separate models liaw and wiener 2002 the final prediction of the model is obtained using equation 4 rf models have been successfully applied to the prediction of mwb severity de coste et al 2022a extreme gradient boosting xgboost xgboost is an ensemble algorithm focussed around the iterative construction of rt models with each built sequentially in response to the rate of accuracy of the previous refinements are made to each subsequent tree with the use of second order gradients to minimize the overall error of the model as the loss function chen and guestrin 2016 xgboost models have been implemented in the prediction of river ice phenomena graf et al 2022 2 2 2 model construction and assessment each model required the application of five fold cross validation to select optimal hyperparameters for the given data this process subdivides the data into 5 equal portions and trains the model on 4 with the remaining portion being used to test the accuracy the process is repeated with each of the 5 portions used for testing allowing a comprehensive accuracy of the given model hyperparameter configuration to be calculated this process was repeated for each configuration of hyperparameters using an exhaustive grid search the most successful hyperparameter configuration is then selected based on the best performance refaeilzadeh et al 2009 performance for the regression based analysis in this study was assessed using root mean square error rmse the square root of mean square error mse and mean absolute error mae nakagawa and schielzeth 2013 the smaller the values for each of the considered error metrics the better the fit of the model 2 3 hybrid modelling framework network analysis techniques are applicable to the structure of an ontology these techniques allow the most critical nodes within the network or ontology to be identified the three selected network analysis metrics for this study are centrality measures which can be utilised to identify the key nodes within an ontology de coste et al 2022c betweenness centrality describes the number of shortest paths through a node brandes 2001 this indicates the level of connectivity to other nodes within the network closeness centrality measures the closeness of a given node to all other nodes in the network okamoto et al 2008 this indicates the overall level of connectivity to all nodes in the network degree centrality describes the number of links to a node against the total network linkage zhang and luo 2017 this indicates the centrality of the variable against the total network the values of each centrality measure vary between 0 and 1 with higher values representing a more central node by calculating these variables for each of the individuals defined in the mwb ontology an assessment of the centrality of each data point contained in the crid can be made through this the most central variables within the network can be identified for the development of machine learning models predicting mwb and spring breakup timing as the goal of this study was the prediction of both mwb and spring breakup timing two machine learning algorithms were required with separate modelling goals fig 1 describes the process 2 4 model implementation the mwb ontology was constructed in protégé version 5 5 0 musen 2015 machine learning analysis was performed using python version 3 7 using the packages numpy oliphant 2006 seaborn waskom 2020 pandas mckinney and others 2010 scikit learn pedregosa et al 2011 geopandas jordahl 2014 xgboost chen and guestrin 2016 scipy virtanen et al 2020 and ddot yu et al 2019 3 case study 3 1 problem statement though all ice seasons contain spring breakups and many studies have developed models to predict these breakups sun and trevor 2018a 2018b sun 2018 modelling in this study focussed solely on ice seasons with mwbs to ensure the best modelling performance seasons without mwbs undergo a significantly different progression of ice development resulting in different progressions of final spring breakup occurrence of mwbs results in water levels and flows that greatly exceed those of seasons without during the period of the season that typically has the lowest flows an example of this is provided in fig 2 showing the flow profiles from november 25th to march 25th for the wsc gauge nashwaak river at durham for the years 2005 2015 the large spikes in flow for the seasons with mwbs are indicative of the large amounts of runoff and liquid precipitation entering the channels during these events demonstrating the hydrological differences between these two season types these differences also extend to the progression of ice cover growth as after the ice cover reforms a new set of freeze up and winter low events will occur as a result seasons with mwbs have an additional 6 ice events described in section 3 2 in comparison to seasons without requiring a different ontological structure these differences also result in a different set of relevant predictive variables being available for the prediction of spring breakup thus requiring differing machine learning structures to be developed in comparison to those used for the prediction of solely spring breakup de coste et al 2022c thus this study focuses the ontology and machine learning development solely on seasons with mwb occurrence with the possibility of future development including non mwb seasons within these seasons seasons with mwb occurrence predictions of both the timing of the mwb and the subsequent timing of spring breakup can be performed using the same ontology and data 3 2 data the canadian river ice database crid is the primary data source in this study de rham et al 2020 the crid contains data on 196 rivers across the country including 54 that have experienced mwbs historically the gauges the data are sourced from include 10 of the 11 climate regions within the country thus including a variety of hydroclimatic conditions affecting the development of ice covers on these rivers of the 196 rivers included 150 are natural channels and 46 have some form of regulation such as dams or reservoirs affecting flows these gauges are mapped in fig 3 with gauges that have experienced mwbs highlighted in red the locations of these gauges are clustered around alberta and the british columbia interior and southern ontario and quebec and are the focus of analysis in this study in total 15 distinct ice season events were extracted when available from each of the gauges per season though only 10 were of relevance to this study occurring prior to spring breakup the method of extraction based on analysis of stage and discharge hydrographs was a technique originally developed by beltaos 1990 the first event monitored is the initiation of freeze up beginning with the first detection of ice backwater effects on flow first b date which does not indicate the full development of an ice cover hf is used to denote the full development of ice cover extracted based on a frictional resistance induced spike in water levels that the presence of a fully developed ice cover triggers following this 30 days of water level are also included if available from the gauge the next events of the season are the first low water level hlw1 and first low flow hlq1 both extracted from monitoring of the respective time series for each variable though these events typically occur close together as the presence of fully developed ice cover alter the stage discharge relationship of a river it is possible for these events to not coincide and occur on different days united states geological survey 1977 the next event and key to this study is the initialisation mwbs where a decrease in frictional resistance triggers a spike in the hydrograph indicating the fully developed ice cover no longer exists hmwb the subsequent maximum water level hmwm during the mwb event is also included through analysis of the stage time series the subsequent secondary freeze hf2 hf2max and secondary low flow and low water level hlq2 hlw2 are extracted through the same means as the initial events of the ice season the final events of the season starting with the initiation of spring breakup hb and peak breakup level hm are extracted through the same means as the mwb with the final event last b date being the final day that ice backwater effects were measured and indicating fully open water flow has resumed for many of these events the date water level and flow were included if available some seasons also include a measurement of ice thickness but the dates that these are measured are not consistent due to the availability of site access varying with many seasons not including this measurement at all the full set of ice season events are summarized in table 1 below including counts of each relevant event and abbreviations used in the mwb ontology before implementation into either of the models in the hybrid framework the data of the crid required cleaning and rearrangement to better suit the modelling goals of both models observations in the dataset containing missing values were removed replacement of these values through interpolation was not possible due to the variability of the values from season to season and the lengths of time between the observations dastorani et al 2009 flows and water levels were normalized through division of the means due to the spatial variance in measurements taken from the gauges with many having significantly differing water levels and flows dery et al 2009 dates contained in the crid for each of the relevant events were converted to the number of days since a reference date the beginning of the canadian water year on october 1st to better accommodate the machine learning algorithms boyd 1979 4 results and analysis 4 1 mwb ontology the mwb ontology that was the focus of this study was constructed according to the methontology framework outlined in section 3 1 with the 4 key stages of the process detailed below 1 specification the primary domain defined for the mwb ontology was an ice season with an mwb this structure would include all ice season events from the crid outlined in section 2 4 competency questions were defined to test the functionality of the developed ontology 1 what are the dates of initiation of all mwbs 2 which gauges measured mwb flows and what were their values 3 what years were mwb max water levels measured and what were their values 4 what is the timeline of mwb and spring breakup occurrence for each ice season these questions comprehensively test the ability of the ontology to draw up the relevant data with appropriate relationships between separate events as well as testing the ability to call up all considered datatypes within the domain 2 conceptualization at this stage the structure of the mwb ontology was organized each variable that would be included would be considered as a child of both event type freeze up first winter low flow etc and datatype flow water level etc additional key characteristic of the ice season would also be included such as the location information the year of occurrence and the ice measurements if any were taken 3 formalization and implementation this was conducted in protégé to develop the owl of the mwb ontology crid data was imported for each of the ice seasons with an mwb ice seasons without mwb occurrences were excluded due to the differing ice progression thus requiring differing machine learning targets however this data could be easily included in the existing mwb ontology due to the benefits of its structure 4 evaluation at this stage the ability of the mwb ontology to satisfactorily answer the competency questions posed above were tested with satisfactory results obtained which are discussed in depth below the final mwb ontology is shown in fig 4 additional figures of the ontology showing focussed views of the structure filtered by event type or datatype are provided in figures a1 a6 in appendix a figure a1 presents freeze up events figure a2 mid winter events including mwbs and figure a3 the late winter and breakup events figure a4 provides details focussed on dates of events figure a5 flows of events and figure a6 the water levels of event summary stats of the developed ontology describing its complexity and structure with all crid data imported are given in table 2 samples of the responses obtained to the 4 competency questions obtained from the sparql software are shown in figure a7 the significance is summarized below for each question 1 produced a text list of each ice season with mwbs and the corresponding date this demonstrates the ability of the ontology to produce the relevant mwb data that is the focus of this study a sample is shown in figure a7a 2 produced a text list of ice season and gauge of locations with flows measured as well as the relevant flows this shows the ontologies versatility in combining separate axioms of the ontology with differing datatypes a sample is shown in figure a7b 3 produces a text list of season year and water level of the relevant event this further demonstrates the ontologies ability to combine calls for different datatypes covering the remainder of the relevant variable datatypes that will be used in further modeling a sample is shown in figure a7c 4 produced list of ice season and dates of mwb mwb max mwb freeze and spring breakup events this shows the ability to call and combine separate events simultaneously while demonstrating the versatility for more complex calls a sample is shown in figure a7d 4 2 modelling framework results five models were constructed to predict both the timing of the mwbs and the subsequent timing of the spring breakups for each season using the five algorithms discussed above mlr knn dt rf and xgboost the models were trained and tested on an 80 20 split of the data and 5 fold cross validation was used for the selection of model hyperparameters 9 variables in total were identified for mwb forecasting sourced from events preceding their occurrence in the ice season while 27 variables were identified for spring breakup forecasting the mwb ontology was used to calculate the 3 centrality values for each of the key ontology components included in these calculations were both the individual variables and the parent categories such as events and datatypes the obtained values are shown in table 3 based on the obtained values the events first low flow first low level mwb freeze mwb freeze max second low flow and second low level were noted to have lower betweenness centralities than the initial freezeup or breakup the date and flow parent variables were in general also found to have a lesser degree centrality than level while the dates of the above listed events were found to have both lower degree and closeness centralities therefore combinations of the date and flow of the above listed events were removed from the modelling pools of the two models the reduced variable selection with 4 variables removed from the mwb model resulting in a total of 5 and 12 removed from the spring breakup model resulting in a total of 15 are listed in tables 4 and 5 the refined selection was used to train the 5 machine learning algorithms for both modelling targets the resulting accuracies of the hybridized models for both targets are shown in tables 6 and 7 in both model cases the hybrid rf model was ultimately the most accurate of the developed algorithms fig 5 below graphs the accuracies of the hybrid model predictions for the testing set for mwb prediction and spring breakup predictions the hybrid rf model predicts best on shorter mwb timings with accuracies beginning to generally decrease with longer lead times while having a much better fit on predictions of spring breakup this pattern of accuracy extends to each of the other 4 hybrid algorithms considered 4 3 discussion to demonstrate the effectiveness of the hybridization additional models of each algorithm were trained and tested for both prediction targets using the full input set without the application of the hybrid technique these machine learning models were developed using the same techniques for selection of model hyperparameters and model training but with no input for variable selection from the centrality measures from the ontology when compared to the non hybrid models significant improvements in modelling error were observed for both forecasting targets after hybridization especially for spring breakup a maximum reduction of 8 32 days of mae for spring breakup and 2 90 days of mae for mwb prediction were obtained for the rf models the most accurate hybrid rf model for each target was mapped on a national scale using the full dataset at each of the considered gauges shown below in figs 6 and 7 for the majority of the gauges in both prediction scenarios the errors were low with the exceptions of the gauges northwest miramichi river at trout brook natashquan river downstream from lake alieste and wapiti river near grande prairie for the mwbs which had maes above 20 days and similkameen river near hedley and saint paul river at chanion creek for the spring breakups with maes above 12 5 days the common feature of these rivers in the data is that thought they have many ice seasons in their record they have few complete records of mwbs without any missing data between 1 and 3 complete ice seasons for all of the listed gauges thus having little data to be trained on for the machine learning models resulting in high errors this was not the case for many of the more accurately predicted gauges the results of the spring breakup prediction were can also be contrasted against those obtained from a previous study focussing on the prediction of spring breakup in seasons without mwb occurrence de coste et al 2022c in this study the most accurate model an rf predicted spring breakup with an mae of 10 85 days the mae obtained in this study for the most accurate model 10 68 days is comparable albeit a marginal improvement differences in the considered variables and number of observations used in model development are a key factor in the differences in the accuracies with the prediction of spring breakups following an mwb occurrence having a much smaller data pool available this is offset by the available variables utilised particularly the mwb and second winter low related events having a much closer proximity to the spring breakup thus allowing for slightly increased accuracy with the trade off of reduced lead times for these predictions though these models target the same goal the timing of spring breakup they apply to very different ice season types and thus function independently future work for these models will include the testing of other regression algorithms and further refinements of the variable selection currently the hybrid model relies solely on analysis of the ontology as the basis of variable selection for the purpose of illustrating the applications of the technique however coupling the method with other variable selection methods such as input omission forwards backwards or stepwise lasso regression and importance analysis the hybrid methodology may also benefit from further refinement of the selected centrality measures though each of the selected measures provided some input on the ultimate variable selection betweenness centrality contributed less information a more comprehensive investigation of other potential centrality or network analysis metrics may yield improved results in hybridization refinement and improvement of the mwb ontology will also be necessary incorporating new observations of the same type of data included in the crid while also expanding the scope of the ontology the current ontology exists only as a resource driven domain based on the data within the crid so the addition of other existing ontologies could greatly expand the utility of the model though some climatic data is implied within the data such as the speed of warming corresponding with the timing of breakup additional data observations would be highly valuable potential new inclusions are climatic data measured at or near the locations of each considered gauge as well as geomorphological factors of the included rivers if available this will expand the usability of the ontology while also providing new pathways for network analysis and new potential candidate variables for forecasting via data driven modelling full integration of the machine learning models and the ontology will also be necessary providing a single convenient to use model that is fully featured for end users and decision making support a final consideration would be the integration of a classification component of the ice season predicting whether an mwb is likely to occur such as the techniques used in de coste et al 2022b this would produce a fully functional system predicting if and when mwbs would occur based on early data 5 conclusions this study focussed on the development of an mwb ontology to fully describe the domain and associated concepts of an ice season where an mwb occurs this mwb ontology includes all of the relevant ice season events and concepts that are currently commonly monitored by river gauges and is configured such that seasons with or without mwbs can be included with only seasons experiencing mwbs considered in modelling in this study due to their unique characteristics the mwb ontology was applied to a national case study of mwbs in canada with data from the crid successfully imported and used to demonstrate the effectiveness of the developed ontology structure the utility of the mwb ontology was further demonstrated through the application of a hybrid modelling scheme with machine learning producing models capable of forecasting the timing of mwbs and subsequent spring breakups based on variables suggested by analysis of the mwb ontology structure the success of this hybridization was demonstrated through comparison to non hybrid models with lower errors obtained for the majority of the models and a high level of success at the majority of the gauges throughout the country that were used in modelling this case study demonstrates both the novelty and utility of ontologies in applications to ice related hydroclimatic events on rivers as well as the effectiveness of the hybrid modelling scheme whose utility is not limited to solely mwb events but can easily be expanded to encompass other flooding or ice condition forecasting systems and river related domains within the realm of hydrology as well as more general applications within engineering as a whole declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was supported by natural sciences and engineering research council of canada nserc data used is available from environment and climate change canada https open canada ca data en dataset c5b58ccd 0011 4a80 8f24 034c86cbc14d appendix a supplementary ontology figures structure and competency questions fig a1 detail of freeze up events in mwb ontology fig a1 fig a2 detail of mid winter events in mwb ontology fig a2 fig a3 detail of spring breakup events and additional ice season characteristics in mwb ontology fig a3 fig a4 detail of date variables in mwb ontology fig a4 fig a5 detail of flow variables in mwb ontology fig a5 fig a6 detail of water level variables in mwb ontology fig a6 fig a7 sample response to a competency question 1 b competency question 2 c competency question 3 and d competency question 4 fig a7 
25453,mid winter breakups mwbs are an increasingly common event on canadian rivers resulting in the early breakup of river ice cover with complex and numerous drivers this study focuses on the development of an mwb ontology which allows the key data events and relationships in an ice season to be defined and analyzed the mwb ontology is applied to a national case study of 54 rivers in canada through assessment of the mwb ontology with network analysis techniques a hybrid modelling framework coupling the mwb ontology with machine learning is developed the hybrid models produce greatly reduced errors in their forecasts of the timing breakup events with the best performance being a mean absolute error of 12 47 days for mwbs and 10 68 days for spring breakup the results demonstrate the utility of the mwb ontology as a tool for collating data and a means for analysis and forecasting of these events keywords mid winter breakups semantic modelling river ice machine learning ontology based analytics data availability data is available via a public repository linked in the manuscript 1 introduction the breakup of developed ice covers is an important event in the hydrology and morphology of many northern rivers often governing the peak flows of the season beltaos and prowse 2009 these events occur after a combination of increased temperatures liquid precipitation and increased snowmelt runoff raise water levels and increase the rate of thermal decay of the ice cover leading to open water flows being restored the ice cover can melt in place in the case of a thermal breakup or be lifted out of place and carried with the flow of the river in the case of a dynamic breakup which often leads to the highest flows of the year and the potential for ice jam flooding beltaos 2003 while these events most often occur in the spring and result in the restoration of open water flow the occurrence of mid winter breakups mwbs is becoming increasingly common in canada newton et al 2017 these events consist of the early breakup of a river ice cover either thermally or dynamically as a result of unseasonal high temperatures and thaws and can trigger early high flows outside of the typical breakup season these will be followed by a second freeze up of ice on the river restoring ice affected flows and heavily altering the flow regime of the river for the remainder of the season while also resulting in a less developed ice cover being present for breakup in the spring de coste et al 2022a the result of this is an increased potential for mid winter ice jams and an increased likelihood of thermal breakup in the spring because these events have a significant impact on the latter half of the ice season it is of great interest to be able to accurately predict both their occurrence and the subsequent occurrence of the spring breakup the prediction of these events however is challenging due to the complexity and availability of data involved in their prediction the drivers and processes of either breakup event type are well researched with the main factors governing breakup occurrence being linked to air temperature flow rates within the river and the occurrence of liquid precipitation beltaos et al 2006 though these factors play a key role in both the timing and type of breakup the breakup mechanism is also governed by the morphology of the river as well as the hydrological factors of the surrounding basin de rham et al 2008 these factors are also affected by the presence of regulation which has a significant impact on the timing and mechanism of breakup as well and with the presence of these structures also generally reducing the magnitude of flows and potential flooding downstream turcotte and morse 2013 spring breakups in general are trending towards an earlier occurrence with shorter ice seasons and longer breakup periods throughout the country yang et al 2020 while mwbs are becoming increasingly common on a national scale and often causing some of the most significant flows for a given river de coste et al 2022a mwbs have been typically studied on a per river basis extending up to regional scales with hydroclimatic thresholds developed to predict their occurrence prowse et al 2002 carr and vuyovich 2014 though these thresholds were successful for the locations they were developed for subsequent studies have demonstrated their limitations and have produced new defined thresholds for mwb predictions exceeding their accuracy newton et al 2017 applications of machine learning based on the suggestions of these studies have been able to successfully predict the severity of mwbs and the timing though novel applications of multi level models were required for the latter case de coste et al 2022a 2022b machine learning has also been extended to the prediction of spring breakup using techniques such as artificial neural networks ann zhao et al 2012 guo et al 2018 support vector machines svm wang et al 2010 barzegar et al 2019 k nearest neighbors knn sun et al 2020 adaptive neuro fuzzy inference systems anfis sun and trevor 2015 2018a and ensemble techniques such as stacking ensembles sun 2018 similar studies have applied these techniques in the prediction of breakup ice jams using methods such as anns massie et al 2002 anfis mahabir et al 2006 and stacking ensembles de coste et al 2021 the challenges faced by these studies often revolve around data availability with studies focussing on single rivers having smaller amounts of data to train models with and studies focussing on larger regions encountering issues related to the management of complex data this also presents challenges in the delivery of user friendly decision making tools to affected locations as the selection of data and presentation of final models can be unintuitive ontology based semantic modelling presents an opportunity to address the knowledge and data management issues inherent to the complexity of the prediction of these types of ice related hydrologic events these models represent a domain in terms of concepts and relationships the relationships inherent between the data contained in these systems can be formalized into computational ontologies with a conceptual structure allowing further analysis of the data and relationships within guarino et al 2009 an ontology can be defined to describe a specific concept or process with multiple ontological structures being possible to describe the same domain brank et al 2005 more comprehensive ontological structures can be developed through combining individual ontologies or iterating upon existing structures allowing a versatility not possible in other model types while also opening the way for application of network analysis techniques noy and musen 2004 the structure of ontologies enables the smooth sharing of data between systems within the same domain while associating the data with the domain knowledge providing a direct link between disparate data points that would not otherwise be present while also allowing model functionality when data is missing munir and anjum 2018 many successful applications of ontologies to the conceptualization and description of hydrology and hydrology related concepts exist including flood management mughal et al 2021 agresta et al 2002 roller et al 2015 yi and sun 2013 flood risk assessment scheur et al 2013 flood forecasting agresta et al 2014 management of hydrological metadata essawy et al 2017 and remote sensing potnis et al 2018 applications of ontologies in river ice related fields however has not yet been completed and remains a field open to novel application of these techniques this paper details the development of an ontology describing the domain of an ice season with an mwb providing a means for further development of a hybrid modelling system to forecast the timing of mwbs and the subsequent spring breakups the developed mwb ontology allows a novel means of organizing the data and analyzing the connections between the relevant hydrological variables linked to both events while the machine learning models developed based on the conclusions of the ontology allow for accurate predictions of both breakups with long lead times this framework provides a novel means of predicting the timing of these rare events while also producing an easy to use tool to sort and analyse relevant data for end users in affected areas this is the first attempt at applying these techniques in the prediction of mwbs in canada a national case study of rivers in canada is used to demonstrate the effectiveness of the developed models showing its applicability throughout the country 2 methodology 2 1 ontology based semantic model the primary goal of the ontology based semantic modelling in this study was the development of an mwb ontology the domain of this ontology is an ice season including an mwb with relationships and entities sourced from the relevant crid data clearly defined the scope of the ontology needs to be clear before construction can begin thus the key concepts relevant to this ontology need to be well understood mughal et al 2021 the domain is an mwb ice season with the events and data being the entities timing and datatypes defining relationships between the different events in the season subdivision of aspects for each event of the ontology is important due to the complexity and variability of the data measured for each event within the mwb ontology individuals are the values in a node such as flows and water levels relationships are the links from one node to another such as mwm date of and axioms are the definitions of the relationship type between nodes guarino et al 2009 datatypes are clearly defined for each of the nodes containing direct data float integer date etc the construction of the mwb ontology was based around the methontology framework lopez et al 1999 with 4 key stages in the process 1 specification at this stage the developer defines the scope and usage purpose of the ontology competency questions used for assessing the effectiveness of the ontology in achieving the scope and usage are also formulated at this stage 2 conceptualization the general structure and key entities are identified and defined in this stage properties relationships and axioms are formulated connecting the relevant nodes together in order to satisfy the scope and usage parameters defined in the specification stage 3 formalization and implementation the structure developed in the conceptualization stage is constructed in an ontology web language owl 4 evaluation the constructed ontology is tested against the previously specified competency questions to assess it s capability at fulfilling the scope and usage purposes if the model is found to be unsatisfactory in answering the competency questions the process can iterate and refinements can be made to the scope and the competency questions to ensure a better model if answers to the competency questions are considered satisfactory then the model construction is considered complete 2 2 machine learning models and development 2 2 1 regression algorithms algorithms utilised in this study have demonstrated success in hybridization applications with ontology in previous studies de coste et al 2022c and are detailed below multiple linear regression mlr mlr models develop a single equation describing the relationship between a single or series of variables and a target predictand uyanik and guler 2013 the equation is fit using least squares by minimising the mean squared error mse applications of mlr equations have been successful in the prediction of ice affected and open water river flows chokmani et al 2007 stampoulis et al 2020 yang et al 2021 seidou and ouarda 2007 regression trees rt these models use a series of decision nodes based on values of specific variables used in the regression lewis 2000 each node passes the observation to another dependent on variable values until a final leaf node is reached producing the final model prediction the structure of the tree including the number of nodes splits and decisions is specified through the user typically based on the analysis of prediction errors rt models have been successfully applied to the prediction of breakup timing and flooding sun 2018 janizadeh et al 2021 k nearest neighbors knn knn models estimate the value of an observation based on the observation variables similarities to its nearest neighbors the amount of neighbors considered for each observation k is specified by the user song et al 2017 the value for the observation is assigned using a distance function with compares the values to the selected amount of neighbors knn has been used for regression based modelling of river ice breakup timing and open water flow predictions sun et al 2020 poul et al 2019 random forest rf these models are considered an ensemble algorithm in which a forest of rt models are constructed with each trained on using a bootstrap sample taken from the full dataset segal 2003 as each of the bootstrap samples is unique each of the constructed trees are likewise unique resulting in more versatile predictions obtained through combining the results of the separate models liaw and wiener 2002 the final prediction of the model is obtained using equation 4 rf models have been successfully applied to the prediction of mwb severity de coste et al 2022a extreme gradient boosting xgboost xgboost is an ensemble algorithm focussed around the iterative construction of rt models with each built sequentially in response to the rate of accuracy of the previous refinements are made to each subsequent tree with the use of second order gradients to minimize the overall error of the model as the loss function chen and guestrin 2016 xgboost models have been implemented in the prediction of river ice phenomena graf et al 2022 2 2 2 model construction and assessment each model required the application of five fold cross validation to select optimal hyperparameters for the given data this process subdivides the data into 5 equal portions and trains the model on 4 with the remaining portion being used to test the accuracy the process is repeated with each of the 5 portions used for testing allowing a comprehensive accuracy of the given model hyperparameter configuration to be calculated this process was repeated for each configuration of hyperparameters using an exhaustive grid search the most successful hyperparameter configuration is then selected based on the best performance refaeilzadeh et al 2009 performance for the regression based analysis in this study was assessed using root mean square error rmse the square root of mean square error mse and mean absolute error mae nakagawa and schielzeth 2013 the smaller the values for each of the considered error metrics the better the fit of the model 2 3 hybrid modelling framework network analysis techniques are applicable to the structure of an ontology these techniques allow the most critical nodes within the network or ontology to be identified the three selected network analysis metrics for this study are centrality measures which can be utilised to identify the key nodes within an ontology de coste et al 2022c betweenness centrality describes the number of shortest paths through a node brandes 2001 this indicates the level of connectivity to other nodes within the network closeness centrality measures the closeness of a given node to all other nodes in the network okamoto et al 2008 this indicates the overall level of connectivity to all nodes in the network degree centrality describes the number of links to a node against the total network linkage zhang and luo 2017 this indicates the centrality of the variable against the total network the values of each centrality measure vary between 0 and 1 with higher values representing a more central node by calculating these variables for each of the individuals defined in the mwb ontology an assessment of the centrality of each data point contained in the crid can be made through this the most central variables within the network can be identified for the development of machine learning models predicting mwb and spring breakup timing as the goal of this study was the prediction of both mwb and spring breakup timing two machine learning algorithms were required with separate modelling goals fig 1 describes the process 2 4 model implementation the mwb ontology was constructed in protégé version 5 5 0 musen 2015 machine learning analysis was performed using python version 3 7 using the packages numpy oliphant 2006 seaborn waskom 2020 pandas mckinney and others 2010 scikit learn pedregosa et al 2011 geopandas jordahl 2014 xgboost chen and guestrin 2016 scipy virtanen et al 2020 and ddot yu et al 2019 3 case study 3 1 problem statement though all ice seasons contain spring breakups and many studies have developed models to predict these breakups sun and trevor 2018a 2018b sun 2018 modelling in this study focussed solely on ice seasons with mwbs to ensure the best modelling performance seasons without mwbs undergo a significantly different progression of ice development resulting in different progressions of final spring breakup occurrence of mwbs results in water levels and flows that greatly exceed those of seasons without during the period of the season that typically has the lowest flows an example of this is provided in fig 2 showing the flow profiles from november 25th to march 25th for the wsc gauge nashwaak river at durham for the years 2005 2015 the large spikes in flow for the seasons with mwbs are indicative of the large amounts of runoff and liquid precipitation entering the channels during these events demonstrating the hydrological differences between these two season types these differences also extend to the progression of ice cover growth as after the ice cover reforms a new set of freeze up and winter low events will occur as a result seasons with mwbs have an additional 6 ice events described in section 3 2 in comparison to seasons without requiring a different ontological structure these differences also result in a different set of relevant predictive variables being available for the prediction of spring breakup thus requiring differing machine learning structures to be developed in comparison to those used for the prediction of solely spring breakup de coste et al 2022c thus this study focuses the ontology and machine learning development solely on seasons with mwb occurrence with the possibility of future development including non mwb seasons within these seasons seasons with mwb occurrence predictions of both the timing of the mwb and the subsequent timing of spring breakup can be performed using the same ontology and data 3 2 data the canadian river ice database crid is the primary data source in this study de rham et al 2020 the crid contains data on 196 rivers across the country including 54 that have experienced mwbs historically the gauges the data are sourced from include 10 of the 11 climate regions within the country thus including a variety of hydroclimatic conditions affecting the development of ice covers on these rivers of the 196 rivers included 150 are natural channels and 46 have some form of regulation such as dams or reservoirs affecting flows these gauges are mapped in fig 3 with gauges that have experienced mwbs highlighted in red the locations of these gauges are clustered around alberta and the british columbia interior and southern ontario and quebec and are the focus of analysis in this study in total 15 distinct ice season events were extracted when available from each of the gauges per season though only 10 were of relevance to this study occurring prior to spring breakup the method of extraction based on analysis of stage and discharge hydrographs was a technique originally developed by beltaos 1990 the first event monitored is the initiation of freeze up beginning with the first detection of ice backwater effects on flow first b date which does not indicate the full development of an ice cover hf is used to denote the full development of ice cover extracted based on a frictional resistance induced spike in water levels that the presence of a fully developed ice cover triggers following this 30 days of water level are also included if available from the gauge the next events of the season are the first low water level hlw1 and first low flow hlq1 both extracted from monitoring of the respective time series for each variable though these events typically occur close together as the presence of fully developed ice cover alter the stage discharge relationship of a river it is possible for these events to not coincide and occur on different days united states geological survey 1977 the next event and key to this study is the initialisation mwbs where a decrease in frictional resistance triggers a spike in the hydrograph indicating the fully developed ice cover no longer exists hmwb the subsequent maximum water level hmwm during the mwb event is also included through analysis of the stage time series the subsequent secondary freeze hf2 hf2max and secondary low flow and low water level hlq2 hlw2 are extracted through the same means as the initial events of the ice season the final events of the season starting with the initiation of spring breakup hb and peak breakup level hm are extracted through the same means as the mwb with the final event last b date being the final day that ice backwater effects were measured and indicating fully open water flow has resumed for many of these events the date water level and flow were included if available some seasons also include a measurement of ice thickness but the dates that these are measured are not consistent due to the availability of site access varying with many seasons not including this measurement at all the full set of ice season events are summarized in table 1 below including counts of each relevant event and abbreviations used in the mwb ontology before implementation into either of the models in the hybrid framework the data of the crid required cleaning and rearrangement to better suit the modelling goals of both models observations in the dataset containing missing values were removed replacement of these values through interpolation was not possible due to the variability of the values from season to season and the lengths of time between the observations dastorani et al 2009 flows and water levels were normalized through division of the means due to the spatial variance in measurements taken from the gauges with many having significantly differing water levels and flows dery et al 2009 dates contained in the crid for each of the relevant events were converted to the number of days since a reference date the beginning of the canadian water year on october 1st to better accommodate the machine learning algorithms boyd 1979 4 results and analysis 4 1 mwb ontology the mwb ontology that was the focus of this study was constructed according to the methontology framework outlined in section 3 1 with the 4 key stages of the process detailed below 1 specification the primary domain defined for the mwb ontology was an ice season with an mwb this structure would include all ice season events from the crid outlined in section 2 4 competency questions were defined to test the functionality of the developed ontology 1 what are the dates of initiation of all mwbs 2 which gauges measured mwb flows and what were their values 3 what years were mwb max water levels measured and what were their values 4 what is the timeline of mwb and spring breakup occurrence for each ice season these questions comprehensively test the ability of the ontology to draw up the relevant data with appropriate relationships between separate events as well as testing the ability to call up all considered datatypes within the domain 2 conceptualization at this stage the structure of the mwb ontology was organized each variable that would be included would be considered as a child of both event type freeze up first winter low flow etc and datatype flow water level etc additional key characteristic of the ice season would also be included such as the location information the year of occurrence and the ice measurements if any were taken 3 formalization and implementation this was conducted in protégé to develop the owl of the mwb ontology crid data was imported for each of the ice seasons with an mwb ice seasons without mwb occurrences were excluded due to the differing ice progression thus requiring differing machine learning targets however this data could be easily included in the existing mwb ontology due to the benefits of its structure 4 evaluation at this stage the ability of the mwb ontology to satisfactorily answer the competency questions posed above were tested with satisfactory results obtained which are discussed in depth below the final mwb ontology is shown in fig 4 additional figures of the ontology showing focussed views of the structure filtered by event type or datatype are provided in figures a1 a6 in appendix a figure a1 presents freeze up events figure a2 mid winter events including mwbs and figure a3 the late winter and breakup events figure a4 provides details focussed on dates of events figure a5 flows of events and figure a6 the water levels of event summary stats of the developed ontology describing its complexity and structure with all crid data imported are given in table 2 samples of the responses obtained to the 4 competency questions obtained from the sparql software are shown in figure a7 the significance is summarized below for each question 1 produced a text list of each ice season with mwbs and the corresponding date this demonstrates the ability of the ontology to produce the relevant mwb data that is the focus of this study a sample is shown in figure a7a 2 produced a text list of ice season and gauge of locations with flows measured as well as the relevant flows this shows the ontologies versatility in combining separate axioms of the ontology with differing datatypes a sample is shown in figure a7b 3 produces a text list of season year and water level of the relevant event this further demonstrates the ontologies ability to combine calls for different datatypes covering the remainder of the relevant variable datatypes that will be used in further modeling a sample is shown in figure a7c 4 produced list of ice season and dates of mwb mwb max mwb freeze and spring breakup events this shows the ability to call and combine separate events simultaneously while demonstrating the versatility for more complex calls a sample is shown in figure a7d 4 2 modelling framework results five models were constructed to predict both the timing of the mwbs and the subsequent timing of the spring breakups for each season using the five algorithms discussed above mlr knn dt rf and xgboost the models were trained and tested on an 80 20 split of the data and 5 fold cross validation was used for the selection of model hyperparameters 9 variables in total were identified for mwb forecasting sourced from events preceding their occurrence in the ice season while 27 variables were identified for spring breakup forecasting the mwb ontology was used to calculate the 3 centrality values for each of the key ontology components included in these calculations were both the individual variables and the parent categories such as events and datatypes the obtained values are shown in table 3 based on the obtained values the events first low flow first low level mwb freeze mwb freeze max second low flow and second low level were noted to have lower betweenness centralities than the initial freezeup or breakup the date and flow parent variables were in general also found to have a lesser degree centrality than level while the dates of the above listed events were found to have both lower degree and closeness centralities therefore combinations of the date and flow of the above listed events were removed from the modelling pools of the two models the reduced variable selection with 4 variables removed from the mwb model resulting in a total of 5 and 12 removed from the spring breakup model resulting in a total of 15 are listed in tables 4 and 5 the refined selection was used to train the 5 machine learning algorithms for both modelling targets the resulting accuracies of the hybridized models for both targets are shown in tables 6 and 7 in both model cases the hybrid rf model was ultimately the most accurate of the developed algorithms fig 5 below graphs the accuracies of the hybrid model predictions for the testing set for mwb prediction and spring breakup predictions the hybrid rf model predicts best on shorter mwb timings with accuracies beginning to generally decrease with longer lead times while having a much better fit on predictions of spring breakup this pattern of accuracy extends to each of the other 4 hybrid algorithms considered 4 3 discussion to demonstrate the effectiveness of the hybridization additional models of each algorithm were trained and tested for both prediction targets using the full input set without the application of the hybrid technique these machine learning models were developed using the same techniques for selection of model hyperparameters and model training but with no input for variable selection from the centrality measures from the ontology when compared to the non hybrid models significant improvements in modelling error were observed for both forecasting targets after hybridization especially for spring breakup a maximum reduction of 8 32 days of mae for spring breakup and 2 90 days of mae for mwb prediction were obtained for the rf models the most accurate hybrid rf model for each target was mapped on a national scale using the full dataset at each of the considered gauges shown below in figs 6 and 7 for the majority of the gauges in both prediction scenarios the errors were low with the exceptions of the gauges northwest miramichi river at trout brook natashquan river downstream from lake alieste and wapiti river near grande prairie for the mwbs which had maes above 20 days and similkameen river near hedley and saint paul river at chanion creek for the spring breakups with maes above 12 5 days the common feature of these rivers in the data is that thought they have many ice seasons in their record they have few complete records of mwbs without any missing data between 1 and 3 complete ice seasons for all of the listed gauges thus having little data to be trained on for the machine learning models resulting in high errors this was not the case for many of the more accurately predicted gauges the results of the spring breakup prediction were can also be contrasted against those obtained from a previous study focussing on the prediction of spring breakup in seasons without mwb occurrence de coste et al 2022c in this study the most accurate model an rf predicted spring breakup with an mae of 10 85 days the mae obtained in this study for the most accurate model 10 68 days is comparable albeit a marginal improvement differences in the considered variables and number of observations used in model development are a key factor in the differences in the accuracies with the prediction of spring breakups following an mwb occurrence having a much smaller data pool available this is offset by the available variables utilised particularly the mwb and second winter low related events having a much closer proximity to the spring breakup thus allowing for slightly increased accuracy with the trade off of reduced lead times for these predictions though these models target the same goal the timing of spring breakup they apply to very different ice season types and thus function independently future work for these models will include the testing of other regression algorithms and further refinements of the variable selection currently the hybrid model relies solely on analysis of the ontology as the basis of variable selection for the purpose of illustrating the applications of the technique however coupling the method with other variable selection methods such as input omission forwards backwards or stepwise lasso regression and importance analysis the hybrid methodology may also benefit from further refinement of the selected centrality measures though each of the selected measures provided some input on the ultimate variable selection betweenness centrality contributed less information a more comprehensive investigation of other potential centrality or network analysis metrics may yield improved results in hybridization refinement and improvement of the mwb ontology will also be necessary incorporating new observations of the same type of data included in the crid while also expanding the scope of the ontology the current ontology exists only as a resource driven domain based on the data within the crid so the addition of other existing ontologies could greatly expand the utility of the model though some climatic data is implied within the data such as the speed of warming corresponding with the timing of breakup additional data observations would be highly valuable potential new inclusions are climatic data measured at or near the locations of each considered gauge as well as geomorphological factors of the included rivers if available this will expand the usability of the ontology while also providing new pathways for network analysis and new potential candidate variables for forecasting via data driven modelling full integration of the machine learning models and the ontology will also be necessary providing a single convenient to use model that is fully featured for end users and decision making support a final consideration would be the integration of a classification component of the ice season predicting whether an mwb is likely to occur such as the techniques used in de coste et al 2022b this would produce a fully functional system predicting if and when mwbs would occur based on early data 5 conclusions this study focussed on the development of an mwb ontology to fully describe the domain and associated concepts of an ice season where an mwb occurs this mwb ontology includes all of the relevant ice season events and concepts that are currently commonly monitored by river gauges and is configured such that seasons with or without mwbs can be included with only seasons experiencing mwbs considered in modelling in this study due to their unique characteristics the mwb ontology was applied to a national case study of mwbs in canada with data from the crid successfully imported and used to demonstrate the effectiveness of the developed ontology structure the utility of the mwb ontology was further demonstrated through the application of a hybrid modelling scheme with machine learning producing models capable of forecasting the timing of mwbs and subsequent spring breakups based on variables suggested by analysis of the mwb ontology structure the success of this hybridization was demonstrated through comparison to non hybrid models with lower errors obtained for the majority of the models and a high level of success at the majority of the gauges throughout the country that were used in modelling this case study demonstrates both the novelty and utility of ontologies in applications to ice related hydroclimatic events on rivers as well as the effectiveness of the hybrid modelling scheme whose utility is not limited to solely mwb events but can easily be expanded to encompass other flooding or ice condition forecasting systems and river related domains within the realm of hydrology as well as more general applications within engineering as a whole declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was supported by natural sciences and engineering research council of canada nserc data used is available from environment and climate change canada https open canada ca data en dataset c5b58ccd 0011 4a80 8f24 034c86cbc14d appendix a supplementary ontology figures structure and competency questions fig a1 detail of freeze up events in mwb ontology fig a1 fig a2 detail of mid winter events in mwb ontology fig a2 fig a3 detail of spring breakup events and additional ice season characteristics in mwb ontology fig a3 fig a4 detail of date variables in mwb ontology fig a4 fig a5 detail of flow variables in mwb ontology fig a5 fig a6 detail of water level variables in mwb ontology fig a6 fig a7 sample response to a competency question 1 b competency question 2 c competency question 3 and d competency question 4 fig a7 
25454,the use of computational fluid dynamics cfd for environmental studies is continuously growing in this context simulations are usually carried out solving reynolds averaged navier stokes rans equations that require an appropriate implementation of the atmospheric boundary layer abl this paper proposes a framework based on the shear stress transport sst k ω turbulence model which is highly recommended for microclimate analysis since it is known to well reproduce homogeneous abl flows in the context of rans simulations besides a new blending approach is developed to account for the presence of obstacles in the domain the model is implemented in the open source openfoam code and validated against well known benchmark cases spanning different configurations i e an empty fetch a single building and an array of buildings the performance of the proposed model is very satisfactory for instance the factor of 2 validation metric is fac2 0 8 for both velocity and turbulent kinetic energy in nearly all cases keywords cfd atmospheric boundary layer rans improved sst k ωmodel data availability data will be made available on request 1 introduction one of the most important and concerning issues for society is represented by exposure to air pollution indeed it is reported as one of the first causes of death and health issues by several international and national agencies european environmental agency 2022 environmental protection agency 2022 who 2016 in this framework environmental modelling is of primary importance to provide useful data for decision making both to aid the emanation of new policies on air quality control and for the development of improved technologies to reduce air pollution in the common industrial practice air quality studies are usually carried out by employing large scale models bezyk et al 2021 isakov et al 2017 shekarrizfard et al 2017 which can predict pollutants dispersion in large areas where environmental impact assessment should be performed however these models can have accuracy issues when dealing with the assessment of dispersion in the local scale of urban districts as a matter of fact they are characterized by a global statistical treatment of the turbulence without the implementation of transport equations for the turbulent variables for this reason computational fluid dynamics cfd models have gained a lot of attention to perform environmental studies indeed these models solve the 3 dimensional fluid dynamics equations together with transport equations and an accurate treatment of turbulence these features give the possibility to investigate the dispersion phenomena in every kind of domain also in very complex layouts like urban canopies moreover cfd has been demonstrated to be a reliable tool to perform pollutants dispersion studies du et al 2021 lateb et al 2016 but also for investigating other topics with a high level of importance for society like the analysis of pedestrian comfort du et al 2018 blocken et al 2016 2012 or also the mitigation of the effects of hazardous materials releases shen et al 2020 in the environmental context cfd simulations are generally carried out using the reynolds averaging approach thus performing rans simulations lauriks et al 2021 bellegoni et al 2021 chen and mak 2021 defraeye and carmeliet 2010 however the application of cfd simulations to the atmospheric environment requires some modifications of the equations to accurately take into account the characteristics of the atmospheric boundary layer abl which is the zone of the atmosphere influenced by human activities in particular the wall treatment has to be modified with respect to the one implemented in cfd software packages to meet the rough conditions of the terrain and source terms are needed to prove the consistency between the turbulence closure equations and the abl wind profiles parente et al 2011a from the point of view of the turbulence models several works have been devoted to build an improved abl description inside the framework of the standard k ɛ model the different approaches proposed throughout the years started from the well known wind profiles for velocity turbulent kinetic energy and turbulent dissipation rate indicated by richards and hoxey 1993 pontiggia et al 2009 built the atmospheric stability sub model assm using the mentioned profiles and employing a first order matching between these profiles and the wall functions in the standard k ɛ model to solve problems at the wall reported in blocken et al 2007 afterwards yang et al 2009 derived a new formulation for the turbulent kinetic energy profile more consistent with experimental observations since it considered the decreasing behaviour with height of this variable finally the most general procedure was suggested by parente et al 2011a with the comprehensive k ɛ approach where the expression of c μ derived in gorlé et al 2009 was used to obtain a more general turbulent kinetic energy profile a new treatment at the wall was also proposed going over the first order matching and considering also the roughness length within the wall functions the works described so far were intended to develop models able to ensure the homogeneity of wind profiles in cfd simulations of open field configurations however when dealing with the presence of obstacles further modifications are needed to consider the deviation from the parallel flow of the abl field that are consequent for this reason parente et al 2011a developed a blending approach in which the turbulence closure model was changed inside the so called building influence area bia the bia was identified as the region where the velocity deviates from the condition of parallel flow in the homogeneous abl further improvements of this technique were implemented by longo et al 2017 and subsequently longo et al 2020 who considered a hybrid way to detect the bia including also deviations of the turbulent kinetic energy and dissipation rate testing different models inside the bia from these previous works it has been shown that using the standard k ɛ model a non linear eddy viscosity nlev model is the most reliable to predict the flow inside the region influenced by the presence of obstacles hence a lot of research has been conducted so far to build an improved cfd model to accurately reproduce abl characteristics employing the standard k ɛ turbulence model in particular the aforementioned comprehensive approach together with the blending approach in the case of the presence of obstacles has reached a high level of accuracy for this kind of analysis however several academic and institutional guidelines for the use of cfd in environmental modelling or pedestrian wind comfort studies state that the standard k ɛ model is not suitable in this context city of london 2019 tominaga et al 2008 as a matter of fact this model is not sufficiently accurate in the simulation of boundary layers due to the over prediction of the turbulent kinetic energy near the walls for this reason the approach described above cannot be used for the assessment of wind comfort or air quality in studies that should follow the above mentioned guidelines among the turbulence closure models that are suggested by these guidelines the shear stress transport sst k ω is considered the most reliable since it has been built with the aim of overcoming the limitations of both the standard k ɛ and wilcox s k ω models some works have been performed in recent years employing this turbulence model in the simulation of abl flows also proposing some improvements an and fung 2018 yu and the 2016 however these studies do not account for the presence of obstacles in the computational domain and this creates a knowledge gap in this aspect therefore in this paper we intend to build an improved turbulence modelling framework for the analysis of the abl both in open configurations and in the presence of obstacles employing the sst k ω turbulence closure model more specifically previous knowledge in the framework of the standard k ɛ model is adapted to the sst k ω closure equations and a new blending strategy is proposed to account for the presence of obstacles in the flow the methodology is implemented in the open source openfoam cfd software and validated against well known experimental cases these were devised to span different flow configurations an open field setup to validate the accuracy in reproducing and sustaining abl profiles in the cfd domain while a single building case and an idealized urban canopy case were used to validate the blending approach in case obstacles are present 2 theory 2 1 standard k ɛ model for homogeneous abl flows the simulation of abl flows with cfd models is generally carried out applying the reynolds averaging to the navier stokes equations as a result reynolds stresses should be closed i e expressed as a function of the unknowns which are the reynolds averaged variables here the k ɛ two equations closure model can be applied the implementation of the general model of a homogeneous abl is derived starting from the equations that describe a 2 dimensional neutral abl with the hypothesis of zero vertical velocity constant pressure in vertical and streamwise directions and constant shear stress 1 μ t u z τ w ρ u 2 2 z μ t σ k k z g k ρ ɛ 0 3 z μ t σ ɛ ɛ z c ɛ 1 g k ɛ k c ɛ 2 ρ ɛ 2 k 0 where g k is the production of turbulent kinetic energy and μ t the turbulent viscosity which are respectively expressed as 4 g k μ t u z 2 μ t ρ c μ k 2 ɛ while σ k σ ɛ c ɛ 1 and c ɛ 2 are empirical constants for which the values in jones and launder 1972 are used moreover fully developed inlet profiles are usually imposed as boundary conditions in terms of velocity and turbulence characteristics such those defined by richards and hoxey 1993 computed to have an equilibrium boundary layer 5 u u κ l n z z 0 z 0 6 k u 2 c μ 7 ɛ u 3 κ z z 0 where u is the friction velocity z 0 the surface roughness and κ the von karman constant i e κ 0 42 since a constant turbulent kinetic energy profile with respect to the vertical direction is not consistent with experimental observations parente et al 2011a derived a new expression for this profile as a solution of the turbulent kinetic energy transport equation 8 k z c 1 l n z z 0 c 2 eq 8 was obtained enforcing the equilibrium assumption between turbulence production and dissipation which is known to stand in a homogeneous abl flow and the height dependent formulation of c μ derived by gorlé et al 2009 with the same assumption 9 c μ u 4 k z 2 moreover a source term is needed for the turbulent dissipation rate equation with the aim of keeping a constant value for σ ɛ while making the above mentioned wind profiles analytical solutions of the transport equation this source was calculated by both pontiggia et al 2009 and parente et al 2011a and it is 10 s ɛ ρ u 4 z z 0 2 c ɛ 2 c ɛ 1 c μ κ 2 1 σ ɛ regarding the wall functions parente et al 2011b proposed to impose the conditions by richards and hoxey 1993 in terms of velocity turbulent kinetic energy and dissipation rate at the wall to accurately take into account rough walls and ensure the homogeneity of wind profiles throughout the computational domain in particular this is implemented as follows 11 u w u κ l n z p z 0 z 0 12 ɛ w c μ 0 75 k 1 5 κ z p z 0 13 g k τ w 2 ρ κ c μ 0 25 k 0 5 z p z 0 where z p represents the height of the wall adjacent cell centroid and g k is computed at this position the model developed by parente et al 2011a was called comprehensive approach by the authors since it contains all the aspects of the cfd simulation of homogeneous abl flows thus the inlet conditions the modified wall functions and the source term for the turbulent dissipation rate equation therefore it will be referred to as comprehensive approach also in the present work 2 2 sst k ω model for homogeneous abl flows in this work we extended the previously explained approach to the sst k ω model menter 1994 more specifically the same methodology as in the comprehensive approach by parente et al 2011a was employed to compute the expressions of the boundary conditions in terms of wind velocity turbulent kinetic energy and turbulent specific dissipation rate mathematically the sst k ω turbulence model represents a blending approach between the standard k ɛ and the wilcox s k ω models the equations for the turbulent kinetic energy and the turbulent specific dissipation rate with the above mentioned hypothesis in a neutral abl are the following 14 z μ t σ k k z g k ρ β k ω 0 15 z μ t σ ω ω z ρ γ ν t g k ρ β ω 2 2 1 f 1 ρ σ ω 2 1 ω k z ω z 0 the last term of eq 15 is an additional term used for the blending in particular when the blending function f 1 is equal to zero the turbulence equations are the same of the standard k ɛ model while for f 1 1 the wilcox s k ω model is applied the f 1 function has been defined in the original paper menter 1994 via a hyperbolic tangent expression depending on the distance from a wall in this manner wilcox s k ω model is employed near the walls and the standard k ɛ model far from them besides the turbulent viscosity has been defined as 16 μ t a 1 ρ k m a x a 1 ω f 2 s where a 1 is a constant of the model s represents the strain rate and f 2 is another function used to limit the eddy viscosity in particular when f 2 s is low the expression for the turbulent viscosity of wilcox s k ω model i e μ t ρ k ω is used while for a large f 2 s this term limits the value of the turbulent viscosity also the function f 2 is defined with a hyperbolic tangent so that the limiter is applied near the walls and not far from them applying the usual assumption of equilibrium between turbulence production and dissipation in a homogeneous abl flow mathematically ρ k ω u z 2 ρ β k ω resulted in 17 β u z 2 1 ω 2 ρ 2 u 4 μ t 2 1 ω 2 u 4 k z 2 from which we can state that β is equivalent to c μ in the standard k ɛ model combining eqs 14 and 17 we obtained 18 k β k z u z c o n s t that led to the same expression of the turbulent kinetic energy inlet profile in the comprehensive k ɛ approach eq 8 at this point using eq 15 and the wind speed profile eq 5 we computed the ω profile as 19 ω z u β 1 κ z z 0 as in the case of the standard k ɛ model a source term was calculated to make eq 19 analytical solution of eq 15 in particular considering the definition of the eddy viscosity like μ t ρ k ω and substituting the expression 19 in 15 we obtained 20 s ω z ρ u 2 z z 0 2 β β α β β κ 2 1 σ 2 1 f 1 σ ω 2 c 1 β u 2 the last hypothesis for the definition of μ t was possible because both velocity and turbulence profiles were sustained along the computational domain by means of the wall functions in this manner it was unnecessary to consider the viscosity limiter in the calculation of the source term since it was applied only near walls the wall functions were applied in the same way of the comprehensive approach by parente et al 2011b as explained in section 2 1 the comprehensive approach for the sst k ω model was validated performing cfd simulations of a test case which consists of an empty domain thus reproducing a homogeneous abl flow see section 3 1 2 3 model modification in presence of obstacles it is known that the consistency achieved with the comprehensive approach does not stand anymore when an obstacle is present in the domain in particular the expressions for c μ and the source term of the turbulent dissipation equation are not valid in the region influenced by the obstacle as a matter of fact their formulations were derived employing the equilibrium and horizontal homogeneity assumptions which is not applicable anymore in this zone to solve this problem parente et al 2011a longo et al 2017 and then longo et al 2020 proposed the blending with another turbulence closure model in the so called building influence area bia which is detected through the computation of the deviation from the undisturbed abl flow in particular parente et al 2011a implemented a pure blending considering only the deviation in terms of velocity longo et al 2017 considered a hybrid approach implying also the departure on the turbulent kinetic energy while longo et al 2020 finally improved the bia detection adding the deviation on the dissipation rate the transition from the undisturbed to the disturbed flow region can be taken into account by using either a polynomial or sinusoidal function longo et al 2017 21 ϕ δ α ϕ wake 1 δ α ϕ abl ϕ wake 1 δ α ϕ abl ϕ wake 22 ϕ ϕ wake ϕ abl ϕ wake 1 0 5 1 s i n δ α δ π m a x δ 0 5 0 5 where ϕ represents the parameter to be modified when moving from the abl region to the wake of the building thus referring to the source term for the turbulent dissipation equation or the constant c μ δ is the local deviation of velocity turbulent kinetic energy and specific dissipation rate with the imposed profiles at the inlet and it is expressed as in eq 23 while δ is a parameter in the range π 2 π 2 23 δ m a x δ u δ k δ ɛ w h e r e δ u m i n a u u u abl u abl 1 δ k m i n a k k k abl k abl 1 δ ɛ m i n a ɛ ɛ ɛ abl ɛ abl 1 in particular employing eqs 22 and 23 the value of a generic variable ϕ goes from the value in the homogeneous abl to that in the wake following a sinusoidal function depending on the value of δ specifically the extreme values are reached for δ 0 ϕ ϕ a b l and for δ 1 ϕ ϕ w a k e the distribution of the values taken by δ are reported in fig 1 for the case of a single ground mounted obstacle in the previous equations a u a k a ɛ represent attenuation parameters used to avoid an over extent of the bia in particular the employed values are a u 1 a k 0 1 and a ɛ 0 1 which were calibrated by longo et al 2020 using different wind tunnel and real scale test cases at this point a non linear eddy viscosity nlev model was proposed for the solution in the bia region this consists in extending the boussinesq s hypothesis to higher order terms and practically to use a different formulation for c μ for which the expression by ehrhard and moussiopoulos 2000 eq 24 was demonstrated to be the most accurate longo et al 2017 24 c μ m i n 0 15 1 0 9 s 1 4 0 4 ω 1 4 3 5 where s and ω are the strain rate and the vorticity invariants respectively more specifically this procedure allows to predict the flow conditions in both regions i e in the undisturbed abl flow where the equilibrium stands and in the bia region where another turbulence model is employed to take into account the deviation from an equilibrium boundary layer in the sst k ω framework a new blending approach was implemented here from a mathematical point of view it consisted in applying the sinusoidal blending function in eq 22 to the f 1 function of the sst k ω model as follows 25 b f 1 f 1 1 0 5 1 s i n δ α where b f 1 indicates the blended f 1 function from a physical point of view this means that the standard k ɛ model was employed inside the bia but keeping the viscosity limitation near the walls since the function f 2 was not blended while the sst k ω model was used in the undisturbed flow region as a matter of fact the sinusoidal blending function is zero in the bia thus giving b f 1 0 which means to blend to standard k ɛ equations on the contrary in the undisturbed flow region the sinusoidal function goes to 1 and so the usual equations of the sst k ω model are applied this novel blending technique was compared here with the above mentioned blending to an nlev model in the bia using the same expression in eq 24 for the variable β that is equivalent to c μ as stated above from the point of view of the bia detection the hybrid approach by longo et al 2020 was considered where the deviation in terms of specific dissipation rate i e ω was used instead of that on the dissipation rate i e ɛ this blending approaches applied to the comprehensive sst k ω model in case of the presence of obstacles were tested comparing simulation results to experimental data coming from two well known wind tunnel test cases as described in sections 3 2 and 3 3 3 validation test cases this section presents the test cases used for the validation of the comprehensive sst k ω model more specifically an empty fetch configuration was employed to validate the consistency of the wind profiles while a single building case and one consisting of an array of buildings were used to validate the new blending approach the simulations were performed with the openfoam v 7 code and employing the simplefoam solver which is steady state and incompressible the discretization schemes were set as gauss linear for gradients and laplacian and bounded gauss upwind for the divergence of the variables while the simple algorithm was used for the pressure velocity coupling 3 1 wind tunnel scale empty fetch the empty fetch test case consisted of a neutral atmospheric boundary layer reproduced during cedval experiments leitl and schatzmann 2010 for which measurements of the velocity components and turbulence intensity were available in particular a 2 dimensional wind tunnel which was 4 m long and 1 m high was considered the same computational mesh and boundary conditions as in parente et al 2011a were used in particular the surface roughness was z 0 0 00075 m the friction velocity u 0 374 m s and the fitting constants of the k profiles were c 1 0 04 and c 2 0 52 the grid was composed of 28 400 elements i e of 400 uniform cells in the longitudinal direction and 71 cells in the vertical direction which were stretched to have the wall adjacent cell centroid at a height of 0 0025 m the wind inlet was implemented as a velocity inlet and shear was specified at the top boundary as τ ρ u 2 the outlet was a pressure outlet and rough wall conditions were applied on the terrain boundary 3 2 single building for the validation on a single building test case the cedval a1 1 case leitl and schatzmann 2010 was chosen this experimental test was performed by researchers of the university of hamburg and it is one of the most common choices for validation studies of abl models indeed the test was considered by parente et al 2011a and here the same numerical set and computational mesh of that work were used the test case consisted of a building with a height of h 0 125 m length l 0 1 m and width w 0 15 m immersed in a flow of a wind tunnel with a height of 1 m and a width of 1 5 m the atmospheric boundary layer was the same of the empty fetch case and measurements were collected in a total number of 603 data points distributed in the domain from a location put 0 2 m upstream the building to one located 0 4 m downstream for a total of 44 locations fig 2 reports the dimensions of the computational domain together with the area in which measurements were collected indicated in blue with the values of minimum and maximum x y and z coordinates the origin was set at the ground in the centre of the building fig 2 also indicates the locations of the measurement lines chosen for the validation of cfd results the first line was located in front of the building two lines were on the roof and the last two were in correspondence of the wake the height and width of the domain were the same of the wind tunnel size the distance of the inlet from the building was the one where abl profiles were measured during experiments while the outlet was put 3 m downstream of the building since the case setup presents a symmetry in the longitudinal direction simulations were carried out employing a computational domain with half of the width of the wind tunnel the grid was composed of 2 4 millions hexahedral elements with a ground adjacent cell of 0 00075 m a grid sensitivity analysis was performed in the original paper generating two other grids with 1 74 and 1 26 millions cells the grid convergence index gci was calculated using a safety factor of f s 1 25 as suggested by roache 1998 values of 5 and 3 were obtained for the finest grid for the variables of interest i e velocity and turbulent kinetic energy respectively the same fully developed conditions of the empty fetch case were used for the implementation of the homogeneous abl while smooth conditions are considered for the walls of the building thus calculating the wall functions at the wall adjacent cell centroid without the translation of the z coordinate by z 0 as done in rough walls 3 3 array of buildings the validation of the blending approaches was performed also for an array of buildings the cedval b1 1 experimental case of the university of hamburg leitl and schatzmann 2010 this setup can be considered as an idealized series of urban street canyons and it was built by creating sets of buildings with the same dimensions of the a1 1 case and with a space of 0 1 m between each other in both x and y directions the buildings were immersed in the same atmospheric boundary layer of the empty fetch case this test was already employed by longo et al 2017 and the same computational domain and mesh were used here fig 3 represents the computational domain dimensions with the origin at the ground and in the centre of the reference building in red measurements were available for locations starting from the reference building roof to the right edge of the building downstream of it for a total of 532 data points and 81 locations included in the blue range of fig 3 three of these locations were considered for the validation and comparison of the models and their positioning is indicated in fig 3 in particular the first line was located inside of a canyon while the other two lines were on top of the building downstream of the reference one the domain dimensions were also in this case the same of the wind tunnel size in terms of height and width the inlet was again put at the distance where abl profiles were measured 1 85 m upstream the reference building while the outlet was put at 4 45 m from the reference building which is 4 m downstream the end of the array the mesh was composed of 3 5 millions hexahedral cells and a grid independence study was carried out in the original paper creating a coarser mesh with 2 3 millions elements and employing a safety factor of f s 3 roache 1998 gci indexes were calculated resulting in a value of 2 for the finest grid for both velocity and turbulent kinetic energy also here the same fully developed conditions were implemented to characterize the homogeneous abl 4 results 4 1 wind tunnel scale empty fetch the purpose of this validation is to demonstrate that the profiles for wind velocity turbulent kinetic energy and specific dissipation rate imposed at the inlet when using the comprehensive sst k ω model are sustained throughout an empty domain towards the outlet for the sake of comparison fig 4 reports the results obtained employing the standard k ɛ model without any modification to ensure homogeneity thus without the wall treatment and the source term described above and the comprehensive approach developed by parente et al 2011a on the other hand fig 5 reports the comparison between profiles at inlet and outlet for the improved sst k ω model proposed in the present work it clearly appears from fig 4 that the standard k ɛ model without the modifications of the comprehensive approach is not capable of sustaining wind profiles throughout the domain as already demonstrated by parente et al 2011b this is particularly evident for the turbulent kinetic energy profiles which are really different between the inlet and the outlet sections indeed values at the outlet are almost half of those at the inlet in the region near the wall other variables are better maintained along the domain a maximum difference of 0 3 m s can be observed for velocity and around 1 m 2 s 3 for the turbulent dissipation rate however the most accurate results are reached employing the comprehensive approach for which profiles are almost super imposed this performance is valid also for the sst k ω model in fig 5 which again provides a perfect sustainability of the wind profiles therefore only models including the implementation of the comprehensive approach were considered for the next test cases 4 2 single building cedval a1 1 in this case the ability of the new blending approach of detecting one obstacle is validated against experimental observations and verified comparing its results with the existing approach to blend to an nlev model inside the bia figs 6 and 7 show wind profiles in terms of non dimensional mean velocity and turbulent kinetic energy in the five locations where measurements were taken as described in section 3 2 the comprehensive k ɛ model with the blending to the nlev model in the bia proposed in longo et al 2020 is presented together with the sst k ω results where the two types of blending i e nlev and the blending of f 1 function are tested for the sake of comparison also results obtained using the standard k ɛ model without any blending but with the modification of the comprehensive approach by parente et al 2011a are reported from the point of view of velocity all the models provide similar results however it is worth noting that sst k ω model performs better especially near the terrain meaning that the k ω equations are more effective in this zone as a matter of fact considering the locations x 0 072 m and x 0 105 m the standard k ɛ model with the blending to the nlev one provides values of u u r e f 0 17 and u u r e f 0 13 versus experimental values of 0 28 and 0 21 respectively this corresponds to a relative percentage error of about 38 in both cases the error reduces when considering the sst k ω predictions to 28 and 3 respectively moreover it can be noted that using the two different blending approaches within the sst k ω model does not have a strong impact since the obtained profiles differ by less than 5 almost everywhere the main differences between cfd results with all the implemented models and experiments can be observed in the far wake see fig 6 e where an under prediction is present in the vertical range z 0 0 15 m in particular the maximum discrepancy is notable near the ground where the first experimental observation is u u r e f 0 11 while cfd results stay near the value u u r e f 0 10 with all the turbulence models the error in this location is then quite high 180 but this deviation progressively decreases with height and then after z 0 15 m all models provide a good agreement with experiments if we look at the turbulent kinetic energy profiles we can notice quite a large discrepancy of all the models with respect to experiments in the location x 0 m see fig 7 c which is exactly in the middle of the building in particular the peak observed in the experiments k u r e f 2 0 12 is not present in the cfd simulations where the maximum is reached employing the comprehensive k ɛ model without blending k u r e f 2 0 06 all the other cfd models provide even smaller values but this discrepancy could also be imputed to the low resolution of experimental measurements in this zone an under prediction can be then noticed in the first region of the wake see fig 7 d with a peak on the experimental values of k u r e f 2 0 04 and values of cfd results around k u r e f 2 0 018 thus with an error of about 55 overall the other locations are in good agreement between all the models and experiments apart from the location x 0 072 m where it can be noticed that the comprehensive standard k ɛ model without blending is completely wrong in predicting the turbulent kinetic energy profile indeed the prediction with this model provides a peak values which is 8 times higher than experimental ones where a peak is not actually present this last behaviour can be due to the fact that the standard k ɛ model without the modifications to detect the obstacle fails in predicting the characteristics of the impingement zone at the upwind face of the building however it is worth to note that this model performs better than the others in the prediction of the turbulent kinetic energy at the location x 0 m indeed it provides the highest values near the roof where the experimental data are characterized by a peak generally the sst k ω model with both the blending approaches tends to predict smaller values with respect to the k ɛ model with nlev blending but no great differences can be underlined finally although it provides good results at some locations such as x 0 105 m for velocity and x 0 m for turbulent kinetic energy the comprehensive standard k ɛ model without a blending strategy can be considered the less reliable one therefore this reinforces the necessity of providing blending approaches when dealing with obstacles other qualitative details can be analysed by observing the contour plots of the axial velocity and turbulent kinetic energy reported in figs 8 and 9 from the point of view of velocity we can notice that the sst k ω model provides a good prediction of the recirculation zones both in the front and in the wake of the building if compared to the k ɛ model with nlev blending which has been validated in longo et al 2017 as a matter of fact no discrepancies are visible with both the blending approaches tested here inside the sst k ω framework a discrepancy can be discerned in the turbulent kinetic energy distributions where a peak of almost 2 times can be observed for this variable when applying the sst k ω model with blending of f 1 function with respect to the comprehensive standard k ɛ model with nlev blending this can be imputed to the fact that the k ɛ model is known to overpredict the turbulent kinetic energy at impingement zones due to the linearity of the eddy viscosity assumption made in boussinesq hypothesis wright and easom 2003 as a matter of fact the field obtained with the standard k ɛ model presents the highest peak of this variable in the corner of the building in the upstream face a quantitative validation is performed by computing metrics which have been proposed and used by several authors franke et al 2007 for the non dimensional streamwise velocity component and turbulent kinetic energy in particular the factor of two fac2 and the modified normalized mean bias mnmb metrics are considered here their expressions are as follows 26 f a c 2 1 n i 1 n α i α i 1 i f 0 5 p i e i 2 0 o t h e r w i s e 27 m n m b 2 n i 1 n p i e i p i e i where e represents values measured during experiments while p the values predicted by cfd simulations and n is the total number of measurement points the metrics are computed for all the available data points thus they are representative of the general performance of each model conversely to the results reported in figs 6 and 7 where the local performance has been evaluated in this case metrics are calculated for the vertical plane that passes in the middle of the building where 603 data points are available the first metric f a c 2 is a generic performance evaluation of a model that indicates the fraction of points in which the predicted values are within a factor of two of experimental measurements the normalized mean bias m n m b represents the general trend of a model to overestimate or underestimate the values of a variable of interest more specifically negative values of this metric indicate underprediction while positive values represent overprediction the values of the metrics for all the tested models are reported in tables 1 and 2 it can be noticed that all models present similar performances especially in predicting the turbulent kinetic energy field from what concerns the fac2 metric from the point of view of the trend for the other metric the comprehensive standard k ɛ model is clearly the less reliable as expected while the sst k ω model behaves slightly better than the k ɛ with nlev blending using both the blending approaches and for both variables besides an interesting fact is that while the blending of f 1 function provides better results with respect to the blending to the nlev model when dealing with the non dimensional velocity component an opposite behaviour is shown for turbulent kinetic energy where the value of mnmb indicates a higher overprediction for this model this last fact reinforces what has been underlined in the previous comparison of contour plots 4 3 array of buildings cedval b1 1 the validation of the blending approaches with the case of an array of buildings is presented here a comparison between the three types of abl models and experiments is again shown in terms of mean non dimensional velocity and turbulent kinetic energy in the three locations described in section 3 3 and it is reported in figs 10 and 11 a small over prediction can be noticed in the velocity profiles for all the models but here the difference between k ɛ and sst k ω models is more recognizable with the latter being more reliable in the prediction especially in the last two locations indeed at x 0 145 m and x 0 190 m the maximum discrepancy is for the standard k ɛ model with nlev blending which provides values of u u r e f 0 63 and u u r e f 0 61 respectively against the experimental measurements 0 5 and 0 45 thus with an error of 21 and 27 these values employing sst k ω model reduce to 0 58 and 0 56 with errors of 8 in both the locations however it is worth noting that the minimum error in these locations i e 6 is reached by employing the comprehensive standard k ɛ model without blending from the point of view of the turbulent kinetic energy the models seem to behave in very similar ways and a general good agreement with experimental observation can be noticed again discrepancies are present at the locations on the roof of the building near the wall with a peak of experimental measurements which is not predicted by the cfd results in particular the maximum deviation is at x 0 145 m where predicted values of the non dimensional turbulent kinetic energy at the wall are all around 0 01 while the measurement closest to the wall is 0 03 thus performing with an error of about 60 contour plots of axial velocity and turbulent kinetic energy are reported also in this case in figs 12 and 13 it can be noticed that the velocity distribution is well predicted by all the models also for an array of buildings with recirculation zones almost equal in the three cases also in this case small differences are present in the turbulent kinetic energy distribution in the front part of the built region in particular at the edge of the first building models with nlev blending approach figs 13 b and 13 c seem to give higher values of this variable with respect to the blending of f 1 function in the order of 29 again it clearly appears that the employment of the boussinesq hypothesis creates a high overprediction at the edge of the first building indeed the comprehensive standard k ɛ model provides unreliable predictions for the turbulent kinetic energy field which is very far from the results of all the other models with a high overprediction of about 81 on the upstream face of the first building of the array however the predictions of this model inside the array of buildings are very similar to those of the other models a quantitative comparison between the different models is performed taking advantage of the same validation metrics used for the single building case in section 4 2 also in this case metrics are calculated for all the available data points in the vertical plane in the middle of the domain thus considering a number of 532 points values are reported in tables 3 and 4 from the point of view of velocity an improvement in the prediction can be observed with the use of the sst k ω model with both the blending approaches in particular the metrics values show that fac2 remains at the same level of that using the k ɛ model with nlev blending but with a lower underprediction the quantitative comparison of the non dimensional turbulent kinetic energy furtherly demonstrates the unreliability of the comprehensive standard k ɛ without any blending approach model while proving again the improvement of the use of sst k ω model this is true for the general performance of the models even though the standard k ɛ provides good results in some locations as described above in any case higher values of fac2 are reached with the sst k ω model coupled with the blending of f 1 function which also provides the lowest underprediction as indicated by the value of mnmb however it is worth noting that a different behaviour is noticeable for the sst k ω model with the nlev blending for which the fac2 is the lowest and also the level of underprediction is higher than the other improved models 5 conclusions the present study presents a framework for the rans simulation of abl flows using the sst k ω turbulence model which is particularly indicated as the most suited model for environmental modelling in several guidelines in particular a comprehensive approach following that by parente et al 2011a for the standard k ɛ model is extended to this model to accurately reproduce the homogeneous abl flow moreover a novel blending approach is developed to modify the turbulence model for taking into account the presence of obstacles in the computational domain and thus extend the applicability of the model to complex domains and layouts more specifically the f 1 function is multiplied by a blending function in a way that the sst k ω model is employed in the undisturbed flow while k ɛ model is used inside the building influence area bia but keeping the viscosity limitation near walls preliminary simulations are carried out for an empty computational domain to verify the consistency of the model in sustaining wind profiles in an undisturbed abl flow subsequently the blending approach is validated and compared to the existing procedure of blending to a non linear eddy viscosity nlev model with two test cases consisting of a single building and an array of buildings results show perfectly matched profiles between the inlet and outlet sections for the empty domain proving the reliability of the comprehensive approach implementation moreover obstacles are well detected in the other test cases where the modification of the model inside the bia gives a good agreement with experiments the performance of the comprehensive sst k ω model is generally more accurate than the comprehensive k ɛ model in regions near walls indeed errors in the prediction of the velocity field are lowered from 38 to 28 and 3 in two locations of the single building case when employing sst k ω model with respect to the standard k ɛ model with nlev blending this comparison in the case of buildings array shows a reduction of the error from 21 and 27 in two locations at the roof of a building to 8 besides the new blending approach performs slightly better than the blending to an nlev model proposed in longo et al 2020 providing higher values of the fac2 metric in almost all cases this together with the more straightforward implementation of this methodology makes this approach advisable for abl simulations dealing with complex layouts declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors from buildwind sprl gratefully acknowledge innoviris brussels public organisation for research and innovation for financial support under grant number 2021 rdir 17b the authors from université libre de bruxelles gratefully acknowledge innoviris brussels public organisation for research and innovation for financial support under grant number 2021 rdir 17a 
25454,the use of computational fluid dynamics cfd for environmental studies is continuously growing in this context simulations are usually carried out solving reynolds averaged navier stokes rans equations that require an appropriate implementation of the atmospheric boundary layer abl this paper proposes a framework based on the shear stress transport sst k ω turbulence model which is highly recommended for microclimate analysis since it is known to well reproduce homogeneous abl flows in the context of rans simulations besides a new blending approach is developed to account for the presence of obstacles in the domain the model is implemented in the open source openfoam code and validated against well known benchmark cases spanning different configurations i e an empty fetch a single building and an array of buildings the performance of the proposed model is very satisfactory for instance the factor of 2 validation metric is fac2 0 8 for both velocity and turbulent kinetic energy in nearly all cases keywords cfd atmospheric boundary layer rans improved sst k ωmodel data availability data will be made available on request 1 introduction one of the most important and concerning issues for society is represented by exposure to air pollution indeed it is reported as one of the first causes of death and health issues by several international and national agencies european environmental agency 2022 environmental protection agency 2022 who 2016 in this framework environmental modelling is of primary importance to provide useful data for decision making both to aid the emanation of new policies on air quality control and for the development of improved technologies to reduce air pollution in the common industrial practice air quality studies are usually carried out by employing large scale models bezyk et al 2021 isakov et al 2017 shekarrizfard et al 2017 which can predict pollutants dispersion in large areas where environmental impact assessment should be performed however these models can have accuracy issues when dealing with the assessment of dispersion in the local scale of urban districts as a matter of fact they are characterized by a global statistical treatment of the turbulence without the implementation of transport equations for the turbulent variables for this reason computational fluid dynamics cfd models have gained a lot of attention to perform environmental studies indeed these models solve the 3 dimensional fluid dynamics equations together with transport equations and an accurate treatment of turbulence these features give the possibility to investigate the dispersion phenomena in every kind of domain also in very complex layouts like urban canopies moreover cfd has been demonstrated to be a reliable tool to perform pollutants dispersion studies du et al 2021 lateb et al 2016 but also for investigating other topics with a high level of importance for society like the analysis of pedestrian comfort du et al 2018 blocken et al 2016 2012 or also the mitigation of the effects of hazardous materials releases shen et al 2020 in the environmental context cfd simulations are generally carried out using the reynolds averaging approach thus performing rans simulations lauriks et al 2021 bellegoni et al 2021 chen and mak 2021 defraeye and carmeliet 2010 however the application of cfd simulations to the atmospheric environment requires some modifications of the equations to accurately take into account the characteristics of the atmospheric boundary layer abl which is the zone of the atmosphere influenced by human activities in particular the wall treatment has to be modified with respect to the one implemented in cfd software packages to meet the rough conditions of the terrain and source terms are needed to prove the consistency between the turbulence closure equations and the abl wind profiles parente et al 2011a from the point of view of the turbulence models several works have been devoted to build an improved abl description inside the framework of the standard k ɛ model the different approaches proposed throughout the years started from the well known wind profiles for velocity turbulent kinetic energy and turbulent dissipation rate indicated by richards and hoxey 1993 pontiggia et al 2009 built the atmospheric stability sub model assm using the mentioned profiles and employing a first order matching between these profiles and the wall functions in the standard k ɛ model to solve problems at the wall reported in blocken et al 2007 afterwards yang et al 2009 derived a new formulation for the turbulent kinetic energy profile more consistent with experimental observations since it considered the decreasing behaviour with height of this variable finally the most general procedure was suggested by parente et al 2011a with the comprehensive k ɛ approach where the expression of c μ derived in gorlé et al 2009 was used to obtain a more general turbulent kinetic energy profile a new treatment at the wall was also proposed going over the first order matching and considering also the roughness length within the wall functions the works described so far were intended to develop models able to ensure the homogeneity of wind profiles in cfd simulations of open field configurations however when dealing with the presence of obstacles further modifications are needed to consider the deviation from the parallel flow of the abl field that are consequent for this reason parente et al 2011a developed a blending approach in which the turbulence closure model was changed inside the so called building influence area bia the bia was identified as the region where the velocity deviates from the condition of parallel flow in the homogeneous abl further improvements of this technique were implemented by longo et al 2017 and subsequently longo et al 2020 who considered a hybrid way to detect the bia including also deviations of the turbulent kinetic energy and dissipation rate testing different models inside the bia from these previous works it has been shown that using the standard k ɛ model a non linear eddy viscosity nlev model is the most reliable to predict the flow inside the region influenced by the presence of obstacles hence a lot of research has been conducted so far to build an improved cfd model to accurately reproduce abl characteristics employing the standard k ɛ turbulence model in particular the aforementioned comprehensive approach together with the blending approach in the case of the presence of obstacles has reached a high level of accuracy for this kind of analysis however several academic and institutional guidelines for the use of cfd in environmental modelling or pedestrian wind comfort studies state that the standard k ɛ model is not suitable in this context city of london 2019 tominaga et al 2008 as a matter of fact this model is not sufficiently accurate in the simulation of boundary layers due to the over prediction of the turbulent kinetic energy near the walls for this reason the approach described above cannot be used for the assessment of wind comfort or air quality in studies that should follow the above mentioned guidelines among the turbulence closure models that are suggested by these guidelines the shear stress transport sst k ω is considered the most reliable since it has been built with the aim of overcoming the limitations of both the standard k ɛ and wilcox s k ω models some works have been performed in recent years employing this turbulence model in the simulation of abl flows also proposing some improvements an and fung 2018 yu and the 2016 however these studies do not account for the presence of obstacles in the computational domain and this creates a knowledge gap in this aspect therefore in this paper we intend to build an improved turbulence modelling framework for the analysis of the abl both in open configurations and in the presence of obstacles employing the sst k ω turbulence closure model more specifically previous knowledge in the framework of the standard k ɛ model is adapted to the sst k ω closure equations and a new blending strategy is proposed to account for the presence of obstacles in the flow the methodology is implemented in the open source openfoam cfd software and validated against well known experimental cases these were devised to span different flow configurations an open field setup to validate the accuracy in reproducing and sustaining abl profiles in the cfd domain while a single building case and an idealized urban canopy case were used to validate the blending approach in case obstacles are present 2 theory 2 1 standard k ɛ model for homogeneous abl flows the simulation of abl flows with cfd models is generally carried out applying the reynolds averaging to the navier stokes equations as a result reynolds stresses should be closed i e expressed as a function of the unknowns which are the reynolds averaged variables here the k ɛ two equations closure model can be applied the implementation of the general model of a homogeneous abl is derived starting from the equations that describe a 2 dimensional neutral abl with the hypothesis of zero vertical velocity constant pressure in vertical and streamwise directions and constant shear stress 1 μ t u z τ w ρ u 2 2 z μ t σ k k z g k ρ ɛ 0 3 z μ t σ ɛ ɛ z c ɛ 1 g k ɛ k c ɛ 2 ρ ɛ 2 k 0 where g k is the production of turbulent kinetic energy and μ t the turbulent viscosity which are respectively expressed as 4 g k μ t u z 2 μ t ρ c μ k 2 ɛ while σ k σ ɛ c ɛ 1 and c ɛ 2 are empirical constants for which the values in jones and launder 1972 are used moreover fully developed inlet profiles are usually imposed as boundary conditions in terms of velocity and turbulence characteristics such those defined by richards and hoxey 1993 computed to have an equilibrium boundary layer 5 u u κ l n z z 0 z 0 6 k u 2 c μ 7 ɛ u 3 κ z z 0 where u is the friction velocity z 0 the surface roughness and κ the von karman constant i e κ 0 42 since a constant turbulent kinetic energy profile with respect to the vertical direction is not consistent with experimental observations parente et al 2011a derived a new expression for this profile as a solution of the turbulent kinetic energy transport equation 8 k z c 1 l n z z 0 c 2 eq 8 was obtained enforcing the equilibrium assumption between turbulence production and dissipation which is known to stand in a homogeneous abl flow and the height dependent formulation of c μ derived by gorlé et al 2009 with the same assumption 9 c μ u 4 k z 2 moreover a source term is needed for the turbulent dissipation rate equation with the aim of keeping a constant value for σ ɛ while making the above mentioned wind profiles analytical solutions of the transport equation this source was calculated by both pontiggia et al 2009 and parente et al 2011a and it is 10 s ɛ ρ u 4 z z 0 2 c ɛ 2 c ɛ 1 c μ κ 2 1 σ ɛ regarding the wall functions parente et al 2011b proposed to impose the conditions by richards and hoxey 1993 in terms of velocity turbulent kinetic energy and dissipation rate at the wall to accurately take into account rough walls and ensure the homogeneity of wind profiles throughout the computational domain in particular this is implemented as follows 11 u w u κ l n z p z 0 z 0 12 ɛ w c μ 0 75 k 1 5 κ z p z 0 13 g k τ w 2 ρ κ c μ 0 25 k 0 5 z p z 0 where z p represents the height of the wall adjacent cell centroid and g k is computed at this position the model developed by parente et al 2011a was called comprehensive approach by the authors since it contains all the aspects of the cfd simulation of homogeneous abl flows thus the inlet conditions the modified wall functions and the source term for the turbulent dissipation rate equation therefore it will be referred to as comprehensive approach also in the present work 2 2 sst k ω model for homogeneous abl flows in this work we extended the previously explained approach to the sst k ω model menter 1994 more specifically the same methodology as in the comprehensive approach by parente et al 2011a was employed to compute the expressions of the boundary conditions in terms of wind velocity turbulent kinetic energy and turbulent specific dissipation rate mathematically the sst k ω turbulence model represents a blending approach between the standard k ɛ and the wilcox s k ω models the equations for the turbulent kinetic energy and the turbulent specific dissipation rate with the above mentioned hypothesis in a neutral abl are the following 14 z μ t σ k k z g k ρ β k ω 0 15 z μ t σ ω ω z ρ γ ν t g k ρ β ω 2 2 1 f 1 ρ σ ω 2 1 ω k z ω z 0 the last term of eq 15 is an additional term used for the blending in particular when the blending function f 1 is equal to zero the turbulence equations are the same of the standard k ɛ model while for f 1 1 the wilcox s k ω model is applied the f 1 function has been defined in the original paper menter 1994 via a hyperbolic tangent expression depending on the distance from a wall in this manner wilcox s k ω model is employed near the walls and the standard k ɛ model far from them besides the turbulent viscosity has been defined as 16 μ t a 1 ρ k m a x a 1 ω f 2 s where a 1 is a constant of the model s represents the strain rate and f 2 is another function used to limit the eddy viscosity in particular when f 2 s is low the expression for the turbulent viscosity of wilcox s k ω model i e μ t ρ k ω is used while for a large f 2 s this term limits the value of the turbulent viscosity also the function f 2 is defined with a hyperbolic tangent so that the limiter is applied near the walls and not far from them applying the usual assumption of equilibrium between turbulence production and dissipation in a homogeneous abl flow mathematically ρ k ω u z 2 ρ β k ω resulted in 17 β u z 2 1 ω 2 ρ 2 u 4 μ t 2 1 ω 2 u 4 k z 2 from which we can state that β is equivalent to c μ in the standard k ɛ model combining eqs 14 and 17 we obtained 18 k β k z u z c o n s t that led to the same expression of the turbulent kinetic energy inlet profile in the comprehensive k ɛ approach eq 8 at this point using eq 15 and the wind speed profile eq 5 we computed the ω profile as 19 ω z u β 1 κ z z 0 as in the case of the standard k ɛ model a source term was calculated to make eq 19 analytical solution of eq 15 in particular considering the definition of the eddy viscosity like μ t ρ k ω and substituting the expression 19 in 15 we obtained 20 s ω z ρ u 2 z z 0 2 β β α β β κ 2 1 σ 2 1 f 1 σ ω 2 c 1 β u 2 the last hypothesis for the definition of μ t was possible because both velocity and turbulence profiles were sustained along the computational domain by means of the wall functions in this manner it was unnecessary to consider the viscosity limiter in the calculation of the source term since it was applied only near walls the wall functions were applied in the same way of the comprehensive approach by parente et al 2011b as explained in section 2 1 the comprehensive approach for the sst k ω model was validated performing cfd simulations of a test case which consists of an empty domain thus reproducing a homogeneous abl flow see section 3 1 2 3 model modification in presence of obstacles it is known that the consistency achieved with the comprehensive approach does not stand anymore when an obstacle is present in the domain in particular the expressions for c μ and the source term of the turbulent dissipation equation are not valid in the region influenced by the obstacle as a matter of fact their formulations were derived employing the equilibrium and horizontal homogeneity assumptions which is not applicable anymore in this zone to solve this problem parente et al 2011a longo et al 2017 and then longo et al 2020 proposed the blending with another turbulence closure model in the so called building influence area bia which is detected through the computation of the deviation from the undisturbed abl flow in particular parente et al 2011a implemented a pure blending considering only the deviation in terms of velocity longo et al 2017 considered a hybrid approach implying also the departure on the turbulent kinetic energy while longo et al 2020 finally improved the bia detection adding the deviation on the dissipation rate the transition from the undisturbed to the disturbed flow region can be taken into account by using either a polynomial or sinusoidal function longo et al 2017 21 ϕ δ α ϕ wake 1 δ α ϕ abl ϕ wake 1 δ α ϕ abl ϕ wake 22 ϕ ϕ wake ϕ abl ϕ wake 1 0 5 1 s i n δ α δ π m a x δ 0 5 0 5 where ϕ represents the parameter to be modified when moving from the abl region to the wake of the building thus referring to the source term for the turbulent dissipation equation or the constant c μ δ is the local deviation of velocity turbulent kinetic energy and specific dissipation rate with the imposed profiles at the inlet and it is expressed as in eq 23 while δ is a parameter in the range π 2 π 2 23 δ m a x δ u δ k δ ɛ w h e r e δ u m i n a u u u abl u abl 1 δ k m i n a k k k abl k abl 1 δ ɛ m i n a ɛ ɛ ɛ abl ɛ abl 1 in particular employing eqs 22 and 23 the value of a generic variable ϕ goes from the value in the homogeneous abl to that in the wake following a sinusoidal function depending on the value of δ specifically the extreme values are reached for δ 0 ϕ ϕ a b l and for δ 1 ϕ ϕ w a k e the distribution of the values taken by δ are reported in fig 1 for the case of a single ground mounted obstacle in the previous equations a u a k a ɛ represent attenuation parameters used to avoid an over extent of the bia in particular the employed values are a u 1 a k 0 1 and a ɛ 0 1 which were calibrated by longo et al 2020 using different wind tunnel and real scale test cases at this point a non linear eddy viscosity nlev model was proposed for the solution in the bia region this consists in extending the boussinesq s hypothesis to higher order terms and practically to use a different formulation for c μ for which the expression by ehrhard and moussiopoulos 2000 eq 24 was demonstrated to be the most accurate longo et al 2017 24 c μ m i n 0 15 1 0 9 s 1 4 0 4 ω 1 4 3 5 where s and ω are the strain rate and the vorticity invariants respectively more specifically this procedure allows to predict the flow conditions in both regions i e in the undisturbed abl flow where the equilibrium stands and in the bia region where another turbulence model is employed to take into account the deviation from an equilibrium boundary layer in the sst k ω framework a new blending approach was implemented here from a mathematical point of view it consisted in applying the sinusoidal blending function in eq 22 to the f 1 function of the sst k ω model as follows 25 b f 1 f 1 1 0 5 1 s i n δ α where b f 1 indicates the blended f 1 function from a physical point of view this means that the standard k ɛ model was employed inside the bia but keeping the viscosity limitation near the walls since the function f 2 was not blended while the sst k ω model was used in the undisturbed flow region as a matter of fact the sinusoidal blending function is zero in the bia thus giving b f 1 0 which means to blend to standard k ɛ equations on the contrary in the undisturbed flow region the sinusoidal function goes to 1 and so the usual equations of the sst k ω model are applied this novel blending technique was compared here with the above mentioned blending to an nlev model in the bia using the same expression in eq 24 for the variable β that is equivalent to c μ as stated above from the point of view of the bia detection the hybrid approach by longo et al 2020 was considered where the deviation in terms of specific dissipation rate i e ω was used instead of that on the dissipation rate i e ɛ this blending approaches applied to the comprehensive sst k ω model in case of the presence of obstacles were tested comparing simulation results to experimental data coming from two well known wind tunnel test cases as described in sections 3 2 and 3 3 3 validation test cases this section presents the test cases used for the validation of the comprehensive sst k ω model more specifically an empty fetch configuration was employed to validate the consistency of the wind profiles while a single building case and one consisting of an array of buildings were used to validate the new blending approach the simulations were performed with the openfoam v 7 code and employing the simplefoam solver which is steady state and incompressible the discretization schemes were set as gauss linear for gradients and laplacian and bounded gauss upwind for the divergence of the variables while the simple algorithm was used for the pressure velocity coupling 3 1 wind tunnel scale empty fetch the empty fetch test case consisted of a neutral atmospheric boundary layer reproduced during cedval experiments leitl and schatzmann 2010 for which measurements of the velocity components and turbulence intensity were available in particular a 2 dimensional wind tunnel which was 4 m long and 1 m high was considered the same computational mesh and boundary conditions as in parente et al 2011a were used in particular the surface roughness was z 0 0 00075 m the friction velocity u 0 374 m s and the fitting constants of the k profiles were c 1 0 04 and c 2 0 52 the grid was composed of 28 400 elements i e of 400 uniform cells in the longitudinal direction and 71 cells in the vertical direction which were stretched to have the wall adjacent cell centroid at a height of 0 0025 m the wind inlet was implemented as a velocity inlet and shear was specified at the top boundary as τ ρ u 2 the outlet was a pressure outlet and rough wall conditions were applied on the terrain boundary 3 2 single building for the validation on a single building test case the cedval a1 1 case leitl and schatzmann 2010 was chosen this experimental test was performed by researchers of the university of hamburg and it is one of the most common choices for validation studies of abl models indeed the test was considered by parente et al 2011a and here the same numerical set and computational mesh of that work were used the test case consisted of a building with a height of h 0 125 m length l 0 1 m and width w 0 15 m immersed in a flow of a wind tunnel with a height of 1 m and a width of 1 5 m the atmospheric boundary layer was the same of the empty fetch case and measurements were collected in a total number of 603 data points distributed in the domain from a location put 0 2 m upstream the building to one located 0 4 m downstream for a total of 44 locations fig 2 reports the dimensions of the computational domain together with the area in which measurements were collected indicated in blue with the values of minimum and maximum x y and z coordinates the origin was set at the ground in the centre of the building fig 2 also indicates the locations of the measurement lines chosen for the validation of cfd results the first line was located in front of the building two lines were on the roof and the last two were in correspondence of the wake the height and width of the domain were the same of the wind tunnel size the distance of the inlet from the building was the one where abl profiles were measured during experiments while the outlet was put 3 m downstream of the building since the case setup presents a symmetry in the longitudinal direction simulations were carried out employing a computational domain with half of the width of the wind tunnel the grid was composed of 2 4 millions hexahedral elements with a ground adjacent cell of 0 00075 m a grid sensitivity analysis was performed in the original paper generating two other grids with 1 74 and 1 26 millions cells the grid convergence index gci was calculated using a safety factor of f s 1 25 as suggested by roache 1998 values of 5 and 3 were obtained for the finest grid for the variables of interest i e velocity and turbulent kinetic energy respectively the same fully developed conditions of the empty fetch case were used for the implementation of the homogeneous abl while smooth conditions are considered for the walls of the building thus calculating the wall functions at the wall adjacent cell centroid without the translation of the z coordinate by z 0 as done in rough walls 3 3 array of buildings the validation of the blending approaches was performed also for an array of buildings the cedval b1 1 experimental case of the university of hamburg leitl and schatzmann 2010 this setup can be considered as an idealized series of urban street canyons and it was built by creating sets of buildings with the same dimensions of the a1 1 case and with a space of 0 1 m between each other in both x and y directions the buildings were immersed in the same atmospheric boundary layer of the empty fetch case this test was already employed by longo et al 2017 and the same computational domain and mesh were used here fig 3 represents the computational domain dimensions with the origin at the ground and in the centre of the reference building in red measurements were available for locations starting from the reference building roof to the right edge of the building downstream of it for a total of 532 data points and 81 locations included in the blue range of fig 3 three of these locations were considered for the validation and comparison of the models and their positioning is indicated in fig 3 in particular the first line was located inside of a canyon while the other two lines were on top of the building downstream of the reference one the domain dimensions were also in this case the same of the wind tunnel size in terms of height and width the inlet was again put at the distance where abl profiles were measured 1 85 m upstream the reference building while the outlet was put at 4 45 m from the reference building which is 4 m downstream the end of the array the mesh was composed of 3 5 millions hexahedral cells and a grid independence study was carried out in the original paper creating a coarser mesh with 2 3 millions elements and employing a safety factor of f s 3 roache 1998 gci indexes were calculated resulting in a value of 2 for the finest grid for both velocity and turbulent kinetic energy also here the same fully developed conditions were implemented to characterize the homogeneous abl 4 results 4 1 wind tunnel scale empty fetch the purpose of this validation is to demonstrate that the profiles for wind velocity turbulent kinetic energy and specific dissipation rate imposed at the inlet when using the comprehensive sst k ω model are sustained throughout an empty domain towards the outlet for the sake of comparison fig 4 reports the results obtained employing the standard k ɛ model without any modification to ensure homogeneity thus without the wall treatment and the source term described above and the comprehensive approach developed by parente et al 2011a on the other hand fig 5 reports the comparison between profiles at inlet and outlet for the improved sst k ω model proposed in the present work it clearly appears from fig 4 that the standard k ɛ model without the modifications of the comprehensive approach is not capable of sustaining wind profiles throughout the domain as already demonstrated by parente et al 2011b this is particularly evident for the turbulent kinetic energy profiles which are really different between the inlet and the outlet sections indeed values at the outlet are almost half of those at the inlet in the region near the wall other variables are better maintained along the domain a maximum difference of 0 3 m s can be observed for velocity and around 1 m 2 s 3 for the turbulent dissipation rate however the most accurate results are reached employing the comprehensive approach for which profiles are almost super imposed this performance is valid also for the sst k ω model in fig 5 which again provides a perfect sustainability of the wind profiles therefore only models including the implementation of the comprehensive approach were considered for the next test cases 4 2 single building cedval a1 1 in this case the ability of the new blending approach of detecting one obstacle is validated against experimental observations and verified comparing its results with the existing approach to blend to an nlev model inside the bia figs 6 and 7 show wind profiles in terms of non dimensional mean velocity and turbulent kinetic energy in the five locations where measurements were taken as described in section 3 2 the comprehensive k ɛ model with the blending to the nlev model in the bia proposed in longo et al 2020 is presented together with the sst k ω results where the two types of blending i e nlev and the blending of f 1 function are tested for the sake of comparison also results obtained using the standard k ɛ model without any blending but with the modification of the comprehensive approach by parente et al 2011a are reported from the point of view of velocity all the models provide similar results however it is worth noting that sst k ω model performs better especially near the terrain meaning that the k ω equations are more effective in this zone as a matter of fact considering the locations x 0 072 m and x 0 105 m the standard k ɛ model with the blending to the nlev one provides values of u u r e f 0 17 and u u r e f 0 13 versus experimental values of 0 28 and 0 21 respectively this corresponds to a relative percentage error of about 38 in both cases the error reduces when considering the sst k ω predictions to 28 and 3 respectively moreover it can be noted that using the two different blending approaches within the sst k ω model does not have a strong impact since the obtained profiles differ by less than 5 almost everywhere the main differences between cfd results with all the implemented models and experiments can be observed in the far wake see fig 6 e where an under prediction is present in the vertical range z 0 0 15 m in particular the maximum discrepancy is notable near the ground where the first experimental observation is u u r e f 0 11 while cfd results stay near the value u u r e f 0 10 with all the turbulence models the error in this location is then quite high 180 but this deviation progressively decreases with height and then after z 0 15 m all models provide a good agreement with experiments if we look at the turbulent kinetic energy profiles we can notice quite a large discrepancy of all the models with respect to experiments in the location x 0 m see fig 7 c which is exactly in the middle of the building in particular the peak observed in the experiments k u r e f 2 0 12 is not present in the cfd simulations where the maximum is reached employing the comprehensive k ɛ model without blending k u r e f 2 0 06 all the other cfd models provide even smaller values but this discrepancy could also be imputed to the low resolution of experimental measurements in this zone an under prediction can be then noticed in the first region of the wake see fig 7 d with a peak on the experimental values of k u r e f 2 0 04 and values of cfd results around k u r e f 2 0 018 thus with an error of about 55 overall the other locations are in good agreement between all the models and experiments apart from the location x 0 072 m where it can be noticed that the comprehensive standard k ɛ model without blending is completely wrong in predicting the turbulent kinetic energy profile indeed the prediction with this model provides a peak values which is 8 times higher than experimental ones where a peak is not actually present this last behaviour can be due to the fact that the standard k ɛ model without the modifications to detect the obstacle fails in predicting the characteristics of the impingement zone at the upwind face of the building however it is worth to note that this model performs better than the others in the prediction of the turbulent kinetic energy at the location x 0 m indeed it provides the highest values near the roof where the experimental data are characterized by a peak generally the sst k ω model with both the blending approaches tends to predict smaller values with respect to the k ɛ model with nlev blending but no great differences can be underlined finally although it provides good results at some locations such as x 0 105 m for velocity and x 0 m for turbulent kinetic energy the comprehensive standard k ɛ model without a blending strategy can be considered the less reliable one therefore this reinforces the necessity of providing blending approaches when dealing with obstacles other qualitative details can be analysed by observing the contour plots of the axial velocity and turbulent kinetic energy reported in figs 8 and 9 from the point of view of velocity we can notice that the sst k ω model provides a good prediction of the recirculation zones both in the front and in the wake of the building if compared to the k ɛ model with nlev blending which has been validated in longo et al 2017 as a matter of fact no discrepancies are visible with both the blending approaches tested here inside the sst k ω framework a discrepancy can be discerned in the turbulent kinetic energy distributions where a peak of almost 2 times can be observed for this variable when applying the sst k ω model with blending of f 1 function with respect to the comprehensive standard k ɛ model with nlev blending this can be imputed to the fact that the k ɛ model is known to overpredict the turbulent kinetic energy at impingement zones due to the linearity of the eddy viscosity assumption made in boussinesq hypothesis wright and easom 2003 as a matter of fact the field obtained with the standard k ɛ model presents the highest peak of this variable in the corner of the building in the upstream face a quantitative validation is performed by computing metrics which have been proposed and used by several authors franke et al 2007 for the non dimensional streamwise velocity component and turbulent kinetic energy in particular the factor of two fac2 and the modified normalized mean bias mnmb metrics are considered here their expressions are as follows 26 f a c 2 1 n i 1 n α i α i 1 i f 0 5 p i e i 2 0 o t h e r w i s e 27 m n m b 2 n i 1 n p i e i p i e i where e represents values measured during experiments while p the values predicted by cfd simulations and n is the total number of measurement points the metrics are computed for all the available data points thus they are representative of the general performance of each model conversely to the results reported in figs 6 and 7 where the local performance has been evaluated in this case metrics are calculated for the vertical plane that passes in the middle of the building where 603 data points are available the first metric f a c 2 is a generic performance evaluation of a model that indicates the fraction of points in which the predicted values are within a factor of two of experimental measurements the normalized mean bias m n m b represents the general trend of a model to overestimate or underestimate the values of a variable of interest more specifically negative values of this metric indicate underprediction while positive values represent overprediction the values of the metrics for all the tested models are reported in tables 1 and 2 it can be noticed that all models present similar performances especially in predicting the turbulent kinetic energy field from what concerns the fac2 metric from the point of view of the trend for the other metric the comprehensive standard k ɛ model is clearly the less reliable as expected while the sst k ω model behaves slightly better than the k ɛ with nlev blending using both the blending approaches and for both variables besides an interesting fact is that while the blending of f 1 function provides better results with respect to the blending to the nlev model when dealing with the non dimensional velocity component an opposite behaviour is shown for turbulent kinetic energy where the value of mnmb indicates a higher overprediction for this model this last fact reinforces what has been underlined in the previous comparison of contour plots 4 3 array of buildings cedval b1 1 the validation of the blending approaches with the case of an array of buildings is presented here a comparison between the three types of abl models and experiments is again shown in terms of mean non dimensional velocity and turbulent kinetic energy in the three locations described in section 3 3 and it is reported in figs 10 and 11 a small over prediction can be noticed in the velocity profiles for all the models but here the difference between k ɛ and sst k ω models is more recognizable with the latter being more reliable in the prediction especially in the last two locations indeed at x 0 145 m and x 0 190 m the maximum discrepancy is for the standard k ɛ model with nlev blending which provides values of u u r e f 0 63 and u u r e f 0 61 respectively against the experimental measurements 0 5 and 0 45 thus with an error of 21 and 27 these values employing sst k ω model reduce to 0 58 and 0 56 with errors of 8 in both the locations however it is worth noting that the minimum error in these locations i e 6 is reached by employing the comprehensive standard k ɛ model without blending from the point of view of the turbulent kinetic energy the models seem to behave in very similar ways and a general good agreement with experimental observation can be noticed again discrepancies are present at the locations on the roof of the building near the wall with a peak of experimental measurements which is not predicted by the cfd results in particular the maximum deviation is at x 0 145 m where predicted values of the non dimensional turbulent kinetic energy at the wall are all around 0 01 while the measurement closest to the wall is 0 03 thus performing with an error of about 60 contour plots of axial velocity and turbulent kinetic energy are reported also in this case in figs 12 and 13 it can be noticed that the velocity distribution is well predicted by all the models also for an array of buildings with recirculation zones almost equal in the three cases also in this case small differences are present in the turbulent kinetic energy distribution in the front part of the built region in particular at the edge of the first building models with nlev blending approach figs 13 b and 13 c seem to give higher values of this variable with respect to the blending of f 1 function in the order of 29 again it clearly appears that the employment of the boussinesq hypothesis creates a high overprediction at the edge of the first building indeed the comprehensive standard k ɛ model provides unreliable predictions for the turbulent kinetic energy field which is very far from the results of all the other models with a high overprediction of about 81 on the upstream face of the first building of the array however the predictions of this model inside the array of buildings are very similar to those of the other models a quantitative comparison between the different models is performed taking advantage of the same validation metrics used for the single building case in section 4 2 also in this case metrics are calculated for all the available data points in the vertical plane in the middle of the domain thus considering a number of 532 points values are reported in tables 3 and 4 from the point of view of velocity an improvement in the prediction can be observed with the use of the sst k ω model with both the blending approaches in particular the metrics values show that fac2 remains at the same level of that using the k ɛ model with nlev blending but with a lower underprediction the quantitative comparison of the non dimensional turbulent kinetic energy furtherly demonstrates the unreliability of the comprehensive standard k ɛ without any blending approach model while proving again the improvement of the use of sst k ω model this is true for the general performance of the models even though the standard k ɛ provides good results in some locations as described above in any case higher values of fac2 are reached with the sst k ω model coupled with the blending of f 1 function which also provides the lowest underprediction as indicated by the value of mnmb however it is worth noting that a different behaviour is noticeable for the sst k ω model with the nlev blending for which the fac2 is the lowest and also the level of underprediction is higher than the other improved models 5 conclusions the present study presents a framework for the rans simulation of abl flows using the sst k ω turbulence model which is particularly indicated as the most suited model for environmental modelling in several guidelines in particular a comprehensive approach following that by parente et al 2011a for the standard k ɛ model is extended to this model to accurately reproduce the homogeneous abl flow moreover a novel blending approach is developed to modify the turbulence model for taking into account the presence of obstacles in the computational domain and thus extend the applicability of the model to complex domains and layouts more specifically the f 1 function is multiplied by a blending function in a way that the sst k ω model is employed in the undisturbed flow while k ɛ model is used inside the building influence area bia but keeping the viscosity limitation near walls preliminary simulations are carried out for an empty computational domain to verify the consistency of the model in sustaining wind profiles in an undisturbed abl flow subsequently the blending approach is validated and compared to the existing procedure of blending to a non linear eddy viscosity nlev model with two test cases consisting of a single building and an array of buildings results show perfectly matched profiles between the inlet and outlet sections for the empty domain proving the reliability of the comprehensive approach implementation moreover obstacles are well detected in the other test cases where the modification of the model inside the bia gives a good agreement with experiments the performance of the comprehensive sst k ω model is generally more accurate than the comprehensive k ɛ model in regions near walls indeed errors in the prediction of the velocity field are lowered from 38 to 28 and 3 in two locations of the single building case when employing sst k ω model with respect to the standard k ɛ model with nlev blending this comparison in the case of buildings array shows a reduction of the error from 21 and 27 in two locations at the roof of a building to 8 besides the new blending approach performs slightly better than the blending to an nlev model proposed in longo et al 2020 providing higher values of the fac2 metric in almost all cases this together with the more straightforward implementation of this methodology makes this approach advisable for abl simulations dealing with complex layouts declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors from buildwind sprl gratefully acknowledge innoviris brussels public organisation for research and innovation for financial support under grant number 2021 rdir 17b the authors from université libre de bruxelles gratefully acknowledge innoviris brussels public organisation for research and innovation for financial support under grant number 2021 rdir 17a 
