index,text
25985,the characteristics of a landscape and drainage size defines the abiotic character of the stream we advance on an existing clustering method by making it faster more stable capable of processing large areas and adding several new user defined options the software clusters interconfluence stream reaches into larger homogenous stream segments using a series of rules about network adjacency upstream catchment area and similarity of landscape characteristics landscape variables and clustering parameters are discussed to provide guidance for helping users achieve their clustering objectives we provide an example of clustering stream reaches in ontario canada that was used to develop a provincial aquatic classification keywords clustering ecology stream network landscape character upstream catchment computer program 1 introduction the physical character of a landscape defines the abiotic character of the stream it drains or as hynes 1975 aptly states the valley rules the stream in turn a stream s abiotic character is the template that determines the composition of ecological communities southwood 1977 poff 1997 this connection between streams and their landscapes allows us to make inferences about lotic communities using broad scale digital landscape data e g geology land cover this means that costly field data on every stream reach is not required for inventory the growth of computing power and remote sensing technology has enabled network analysis to rapidly change thinking in stream ecology and opened new avenues of discovery jones and schmidt 2017a however there are challenges to overcome before meaningful inferences can be made based on the habitat templates because they are not discretely compartmentalized in space and time cullum et al 2016 instead streams flow through a complex collection of overlapping gradients of environmental conditions ecological classification attempts to impose boundaries at meaningful thresholds along these gradients to create ecologically homogenous habitat units that are useful for management omernik 1987 melles et al 2014 the fundamental premise is that ecological variability within the classified units is less than the variability between units and that they support similar biota which respond similarly to stressors the resulting classification reduces complexity allowing resource managers to make informed decisions not possible in the past a major challenge is determining boundaries that discretize stream networks into homogenous parts previous efforts have used expert opinion to define stream units which is fine if the landscape is well known baker 2006 seelbach et al 2006 but as noted by brenden et al 2008a individual aquatic ecologists will likely interpret the same maps differently which may result in different river valley segment boundaries for a river network earlier works used broad zones to define longitudinal characteristics illies and botosaneanu 1963 and combinations of watersheds and terrestrial land classifications omernik and bailey 1997 defining stream units also requires a hierarchical approach to account for the different spatial scales at which the physical processes operate frissell et al 1986 poole 2002 a bottom up classification approach can be used where smaller spatial units are aggregated into increasingly larger units forming the different levels of the hierarchy melles et al 2014 a suitable fundamental unit i e the lowest level of the hierarchy in river networks is the interconfluence reach referred to as a reach from herein reaches have a clear spatial definition and there are existing methods to delineate them using geographic information systems gis and digital elevation models dem fig 1 the reaches and their catchments i e areas of land contributing water to them can be attributed with a variety of landscape data thought to influence ecohydrological processes this reach based landscape data is simply an inventory of fundamental spatial units a process is needed that will bring these reach units together to form larger ecologically meaningful units using a set of objective and repeatable aggregation rules these larger units are called stream segments the segments represent the next hierarchical level of the stream classification and they are thought to be relatively homogenous units for abiotic and biotic characteristics and persistent over space and time seelbach et al 1997 seelbach et al 2006 the conceptual foundation for the reach to segment aggregation rules has been laid by existing theoretical knowledge pertaining to how lotic systems change from headwaters to river mouth starting with the geomorphological concepts of horton 1945 and strahler 1957 to the ecological thinking introduced with the river continuum concept rcc vannote et al 1980 followed by the serial discontinuity concept sdc ward and stanford 1983 and more recently the network composition hypothesis nch jones and schmidt 2017a we present a software tool called reach affinity tool raft the tool is capable of objectively applying a series of upstream catchment area i e size and similarity rules based on the concepts outlined by the nch to classify streams across large geographic extents 1 000 000 km2 at high spatial resolution 1 100 000 scale or 30 m cell size using readily available spatial digital data e g dems geology land cover the software has already been applied during the development of the aquatic ecosystem classification aec for the province of ontario canada jones and schmidt 2017b although we are presenting raft from an ecological perspective it is not limited to ecological applications other potential uses for raft include clustering reaches into segments of homogenous valley morphology e g incision depth flood plain width potential for bank erosion and sediment transport e g stream power bed material and sinuosity and nutrient loading potential e g landcover geology and soils to name a few examples the spatial structure and attribute content of raft input data is entirely in the hands of the user for example if the user would like to explicitly include dams and barriers in their results then they must prepare their input stream network data to include break points at these features to create unique reaches prior to running raft in the example we provide for ontario anthropogenic factors were intentionally not included because 1 we were interested in assessing fundamental natural drivers e g geological setting of stream type and 2 including anthropogenic factors e g land use would require that the classification be continuously updated which defeats its purpose anthropogenic influences are addressed by a separate stream stress analysis we conducted which can be overlaid on the stream classification unpublished data 2 program design and operation raft is a spatially aware network based software tool that clusters stream reaches into stream segments at the next hierarchical level the reaches that come together to form the segments should be of similar size and landscape character the degree of similarity can be adjusted by the user via a variety of settings that can be manipulated using a graphical user interface gui raft is based on the valley affinity search technique vast which was developed by researchers at the university of michigan seelbach et al 2006 brenden et al 2008a with their permission we used their ms excel visual basic for applications vba source code as a foundation our goal was to adapt and enhance the algorithms contained in the vast code for classifying ontario s rivers our software allows for the inclusion of lake influences within the network it enhances the stream reach size and similarity aggregation rules while greatly increasing processing stability and speed needed for use across large geographic areas 2 1 input data requirements input data for raft is composed of tabular data describing reaches within a river network where the characteristics of each reach line is described as a record i e row in a table the lines of a network are called edges and the junction points between edges are called nodes the edges of a network have an inherent direction describing the direction of movement along them e g water flowing downstream this directionality is crucial when working with stream networks two key attributes of the reach attribute table i e fields or columns are called the from node and to node identifiers we used arc hydro maidment 2002 to generate the stream network arc hydro is spatial database structure that includes a set of network building and manipulation tools for use within arcgis esri 2012 arc hydro provides the benefit of generating a variety of spatial layers that can be used to collect and summarize information about the landscape draining to each reach fig 1 also see brenden et al 2006 understanding streams in landscapes featuring many lakes requires the lakes to be taken into consideration when developing descriptive stream network algorithms brenden et al 2006 jones 2010 raft includes the option to use reach data that includes waterbody inlet outlet nodes in addition to the required basic confluence nodes required by arc hydro fig 2 other means can be used to generate the required data table given that it includes the mandatory fields of table 1 and the table is formatted as a comma separated text file it is recommended that the choice of landscape variables and clustering thresholds is hypothesis driven ecologically meaningful and match the scale of the process for example the amount of groundwater contributions to a stream i e base flow plays a large part in determining summer water temperatures which in turn determines the type of fish species that can tolerate those temperatures the hydrologic process scale of this variable is the geology driven base flow contributions of the entire upstream catchment area not just the reach contributing area see fig 1 a second recommendation is keeping the list of variables as parsimonious as possible because as more variables are included in the similarity comparison it becomes less likely to find meaningful differences amongst reaches houle et al 2010 melles et al 2014 provide a comprehensive summary of existing aquatic classifications and the landscape variables they have used some specific examples include catchment surficial geology catchment slope catchment landcover reach july mean temperature valley width and channel sinuosity snelder and biggs 2007 brenden et al 2008b not all variables included in the input table need to be used for the reach similarity comparisons setting a variable s weight to zero will cause raft to ignore it during the comparison but it still produces segment summary statistics for those variables while a value of 999 will cause raft to ignore the variable entirely 2 2 raft clustering algorithm the probability of an ecological change downstream from a tributary is higher when the two merging streams are of similar size conversely small tributaries flowing into to larger main stems have a lower probability of causing ecological changes jones and schmidt 2017a the size relationship between the main stem and the tributary can be described by the confluence symmetry ratio csr which is calculated by dividing the drainage area of the main stem by the drainage area of the tributary benda et al 2004 in addition to size the probability of change downstream from the confluence increases as the landscape character of the two drainages becomes more dissimilar this means that even a small tributary with a low csr can influence downstream characteristics on the larger main stem if it is draining a very different type of landscape for example a relatively small warm and turbid tributary flowing into a large cold clear trout stream may change the main stem more than a similarly sized tributary that is also cold and clear these relationships set up simple expectations about the relative importance of confluences in stream networks and provide a physical basis for the likelihood of change downstream of tributaries and network heterogeneity of habitats jones and schmidt 2017a raft groups reaches into segments based on upstream catchment area and similarity in landscape characteristics using two stages of processing during both stages only adjacent reaches are compared i e those connected directly downstream the first stage divides the entire network into intermediate spatial units called neighbourhoods based on upstream catchment area uca rules at confluence network nodes the second stage joins reaches into segments within the neighbourhoods using landscape similarity comparisons neighbourhoods represent a classification by size while segments refine the size classification by considering landscape similarity 2 3 grouping reaches into neighbourhoods abrupt changes in the volume of flow temperature and sediment along the length of a stream are identified using the csr a csr equal to one indicates that both reaches at the confluence have the same uca as tributary size decreases relative to the main stem csr approaches zero virtual reaches within lakes i e virtual network connectors within waterbody polygons are automatically grouped into lake neighbourhoods which subsequently become lake segments with one segment per lake three rules are applied using the csr values as illustrated in fig 3 the csr rules are not applied at the confluences of 1st strahler order headwater streams which are automatically assigned to the same neighbourhood to prevent unnecessary complexity in headwater areas at the size comparison stage thus allowing the subsequent landscape similarity comparison to be the determining factor during clustering 1 stream reaches between the lower and upper csr e g 0 25 0 5 are allowed to join together into a neighbourhood because they have similar sizes and thus potentially similar ecological characteristics fig 3i 2 if the csr is below a lower threshold e g 0 25 a tributary is considered too small to significantly affect the main channel fig 3ii in this scenario the main channel neighbourhood remains uninterrupted while the tributary becomes part of another neighbourhood the rationale is that a small tributary should not become part of the main stem neighbourhood because they likely have different channel morphology e g riparian shading bankfull width 3 conversely when the csr at a confluence exceeds the upper threshold e g 0 5 a new neighbourhood is initiated beginning with the reach directly downstream of the confluence fig 3iii the reasoning is that the combined volume of water in the downstream channel increases enough to change channel morphology e g channel width shading temperature riparian influence subsequently a fourth rule is applied that prevents large main stem river neighbourhoods from becoming too large the mainstems of rivers of higher strahler order have large drainage areas and few tributaries are large enough relative to these mainstem to split the neighbourhood based on the upper csr threshold fig 3iii however many small tributaries may join the mainstem causing its flow volume to gradually increase without causing any abrupt changes in stream character the larger the mainstem catchment grows the less likely it is that a tributary will be large enough to cause a split and as a result large portions of the river are likely to be grouped into a single neighbourhood fig 4 a this is problematic because the stream reaches at the upstream end of such a neighbourhood might have a bankfull width of 25 m i e uca 1000 km2 whereas at the reaches at the downstream end might be 50 m wide i e uca 2000 km2 and therefore the reaches of this neighbourhood should not be considered ecologically homogeneous consequently neighbourhoods with such an unacceptably wide range of reach ucas need to be divided raft determines the best location to make the split within each of these neighbourhoods by calculating a ratio of a neighbourhood s minimum and maximum ucas called the neighbourhood upstream catchment area ratio nucar the nucar is a relative measure of the difference in uca within a neighbourhood within each neighbourhood raft identifies the reaches whose uca fall between the lower and upper threshold of the user defined nucar window e g 1 5 to 1 75 the window allows raft some flexibility to find the largest tributary for making the split this prevents neighbourhoods from being divided at small insignificant tributary confluences that can push the nucar above the upper threshold fig 4 b and 4c the process repeats until all neighbourhoods have a nucar below the upper threshold value stream width increases with uca at a decreasing rate caissie 2006 raft can account for this variable rate of width increase by allowing the user to apply an exclusion cut off value below which the nucar process is not applied e g uca 50 km2 for example the nucar for a mid order neighbourhood e g 350 km2 150 km2 2 33 is the same as the nucar for a low order neighbourhood 3 5 km2 1 5 km2 2 33 ecologically speaking the difference in width may not be different enough in the low order neighbourhood e g riparian vegetation shading unaffected to warrant a split hence a user may want to use the nucar exclusion rule an optional fifth rule can be applied during the neighbourhood creation process it makes use of the zone of influence zoi of lakes below their outlets lakes and rivers are intimately connected in an alternating series of lentic and lotic reaches in the canadian shield geologic region which is approximately 4 6 million km2 or nearly half the land area of canada and other similar geologic regions around the world e g scandinavia russia rapid and predictable changes in the ecological characteristics of streams occur at the interface with lakes jones 2010 in many cases water leaving the lake will be warmer and carry large amounts of seston e g dissolved organic carbon phytoplankton zooplankton flows will be more constant i e less flashy the stream s water at the lake outlet might be clearer as particulate matter precipitates out in the lake the combination of changes in the abiotic and biotic conditions fundamentally changes the ecology of the outlet stream however changes are context dependent for example large rivers with catchment areas above 1000 km2 maybe too large to be influenced by lakes to make use of this optional rule the user must estimate a zoi below every lake outlet based on some attenuation function where each reach in the network is assigned a lake influence li value ranging from zero to one where one is the maximum influence a lake a can exert usually at the lake outlet this maximum value attenuates to zero some distance downstream from the outlet indicating that a reach is not influenced by an upstream lake it is the responsibility of the user to assign li values to the reaches prior to running raft if they wish to include it during clustering 2 4 grouping reaches into segments within the neighbourhoods raft s second stage groups reaches into segments within neighbourhoods based on their landscape similarities e g geology land cover channel slope segments cannot expand outside their neighbourhood boundary but a neighbourhood can contain multiple segments this process begins by sorting all reaches within a neighbourhood in descending shreve order shreve 1966 which means that processing will proceed from downstream to the upstream raft starts a new segment using the first reach in the sorted list the attributes of this first reach are compared with the attributes of its adjacent upstream reach along the main channel using a euclidean distance ed measurement romesburg 1989 when the euclidean distance is below a user defined affinity threshold at the adjacent reach is considered to be similar and part of the same segment the algorithm continues the comparison process along the mainstem while smaller adjacent tributary reaches in the neighbourhood along the way are set aside for the moment as the segment grows each new candidate reach is compared to each one of the reaches already part of the segment once the ed of the most upstream reach of the mainstem is evaluated the algorithm then moves on to the tributaries previously set aside again working from largest to smallest shreve order the ed to at comparison can use one of several different methods 1 the complete linkage clink method which uses the largest ed between the candidate reach and the reaches of the segment 2 the single linkage slink method which uses the smallest ed between the candidate reach and the reaches of the segment and 3 the unweighted pair group method with arithmetic mean upgma method which uses the average of the eds of the segment to the candidate reach details about differences amongst these methods can be found in romesburg 1989 the ed calculations use equal attribute weighting by default but the user can specify custom weights to emphasize importance of one or more variables before any segment is considered finished it is treated to a within segment double check where the affinity of each reach to every other reach in the segment is checked once more this ensures that all reaches in the segment remain under the at because reaches added earlier in the process might exceed the at when compared to reaches added later due to the processing order if a reach is found with an affinity at or above the at then it is assigned its own segment and the remainder of the reaches are returned for re clustering if no such reaches are found then the segment is finished and a new segment is started for processing the algorithm runs until all reaches in a neighbourhood have been assigned to a finished segment the last stage of the algorithm is one more double check amongst segments it may be the case where a reach has a higher affinity to an adjacent segment than to the segment to which it is currently assigned due to processing order an affinity check is performed at the interface of segments but not across neighbourhood boundaries reaches with higher affinity i e lower ed to an adjacent segment are reassigned to that segment this process is iterative until no reaches can be reassigned indicating that optimal segmentation stability has been achieved 2 5 graphical user interface the user interacts with raft using its graphical user interface gui the gui allows the user to load their input data set all the necessary parameters required for raft and to specify output name and location when raft is running the gui also displays performance metrics and general progress status fig 5 2 6 user defined threshold choices the raft algorithm requires several user defined threshold values to produce optimal results the threshold values will influence the size and structure of the resulting segments there are no right or wrong choices but the chosen values should take the user s tolerance for reach heterogeneity within segments into consideration higher similarity requirements will produce segments that are smaller i e composed of fewer reaches and more homogenous in character at the cost of increased segment count and classification complexity considering the rapid speed at which raft operates we encourage users to experiment with different threshold values to find the set that optimally suits their requirements the csr thresholds are used to determine the size and structure of network neighbourhoods the user must provide a lower and upper value or use default values the lower threshold must be less than the upper threshold and both must be between zero and one decreasing the lower threshold value will result in more small tributaries being allowed to join a neighbourhood increasing the upper threshold will result in increasingly larger tributaries to join the neighbourhood the larger the difference between the two csr thresholds the larger neighbourhoods that are created i e more reaches the li threshold works in tandem with the csr thresholds its value must be a between zero and one li has a strong effect on the way neighbourhoods are created because reaches that are considered to be influenced by lakes cannot be joined to reaches that are free of lake influence the li threshold value allows the user to specify a cut off below which the neighbourhood creation part of the raft algorithm ignores non zero li values this gives the user some flexibility to exclude inconsequentially small li values this is especially useful when using li attenuating functions that produce asymptotic results e g natural decay raft ignores all input li values during the neighbourhood creation process when the threshold is set to 1 the nucar window is defined by a pair of threshold values that work together in tandem they refine the size and structure of network neighbourhoods after the csr thresholds have been applied the upper nucar dictates the maximum reach uca difference allowed within a neighbourhood increasing the upper nucar threshold will allow larger neighbourhoods to form at the cost of increased inter neighbourhood reach uca dissimilarity it is recommended to keep the nucar window width below 0 5 to allow for flexibility while making results more interpretable the nucar application threshold allows the user to prevent the nucar rules from being applied to reaches below the thresholds uca value setting this value to zero will result in the nucar rules to be applied to all reaches in the processing extent while a value equal to the maximum uca of the processing extent will prevent the nucar rules from being applied entirely the choice of at affects the size of the segments raft creates within neighbourhoods lower at values create smaller more homogenous segments i e fewer reaches and higher within segment similar while higher values create larger more heterogeneous segments i e more reaches and lower within segment similarity the theoretical range of ed values is a function of the range of the landscape characteristics that are used to calculate them for example three landscape variables that range from zero to one using equal weights that sum to one i e 1 3 should result in a set of ed values also between zero and one meaning that the at must also fall inside this range when the ed is zero the landscape characteristics of the reaches are identical while ed of one indicates perfectly opposite reaches in practice these extremes are never reached thus we advise users to become acquainted with distribution of actual ed value within the entire processing extent prior to using raft this distribution can be calculated by finding the ed of every reach to all other reaches if the maximum ed is 0 6 then it would not be wise to choose an at of 0 5 because the threshold would rarely be exceeded during the reach comparisons producing large segments with unacceptable amounts of landscape character heterogeneity conversely choosing an at that is too low e g 0 1 would result in highly homogenous segments however such segments would be so small that they would not reduce complexity much beyond the input reaches both scenarios defeat the purpose of clustering it is recommended that several potential at values be tested to allow evaluation of the different results against the hypothesized expectations field measurements and the objectives of the clustering exercise 2 7 raft tabular output raft outputs an ms excel workbook containing several worksheets the segment reaches worksheet contains information for the individual reaches including the segmentid to which they have been assigned and the variable values used to calculate ed during clustering the segment summaries sheet contains a variety of summary statistics of each segment s composite reach input variables the summaries include the variables used for clustering as well as the inert variables i e those assigned a weight of zero the neighbourhood sheet contains the neighbourhood identifier associated with each reach capturing the results of size rule clustering alone the distances sheet provides the absolute i e global ed of the reach to all other reaches of the segment and relative i e local within segment distance values the absolute values are the raw ed similarity measures of each reach compared to all the other reaches inside the segment using the chosen comparison method e g upgma the relative local values are a range scaled measure of each reach s similarity within its segment a relative local distance of zero indicates that the reach has the smallest within segment ed while a value of one indicates the reach with largest within segment ed the settings sheet contains relevant metadata for each analysis run 3 a practical demonstration of raft the aquatic ecosystem classification aec for ontario canada serves as a real world application of raft ability to cluster reaches into ecologically homogenous segments jones and schmidt 2017b raft was used to process all the streams and rivers of ontario totalling 710 000 reaches across 1 000 000 km2 area of land resulting in 335 000 segments and 180 000 virtual lake segments the processing was achieved by dividing the provinces into 47 smaller work units containing up to 100 000 reaches each raft runs very efficiently with the largest work units finishing in a matter of minutes on workstation class computer in 2018 we do not see any reason why larger areas with more reaches 100 000 could not be processed during a single raft run the saugeen river watershed serves as an example at a suitable scale for illustrating the reach to segment clustering process that was applied across the whole province the saugeen river flows into lake huron at southampton ontario outlet at 44 30 3 n 81 22 26 w and drains an area of 3983 km2 it has a diverse landscape of post glacial geologic deposits e g gravel sand clay with underlying mesozoic bedrock e g shale limestone and a temperate climate the spatial data was generated using arc hydro tools with the ontario integrated hydrology 30 m dem omnrf 2016 and the ontario hydro network omnrf 2015 the saugeen river watershed contains 2957 reaches fig 6 a raft clusters these reaches into 2245 segments fig 6b using the thresholds and landscape variables found in table 2 we used three landscape variables for segment clustering the first two are surrogates based on uca geologic character they are baseflow index bfi which is a measure of the proportion of groundwater contribution potential and low flow turbidity potential the third variable is reach channel slope which is a surrogate for flow velocity the input landscape variables used for the ed comparison were all range scaled to between 0 and 1 legendre and legendre 2012 and given equal weights these settings correspond to the values used to produce the first version of the aec for ontario jones and schmidt 2017b the higher order main stem segments are large i e composed of many reaches due to the homogenizing landscape averaging effect described by the network composition hypothesis nch jones and schmidt 2017a conversely the segments in the headwaters are generally smaller because of the increased potential for landscape heterogeneity amongst their component reaches i e higher dissimilarity for the lake influence li all reaches in the processing extent were sorted in ascending shreve order to ensure that processing occurs from the outermost branches of the network to the pour point the initial li at the beginning of a lake outlet reach was set to one the terminal li of the reach was calculated by multiplying the reach length by an attenuation rate which is the rate of change of li per kilometre of reach length e g lake influence rate of change liroc 1 66 ucareach 0 6 then subtracting the product of the multiplication from the initial li the initial li at a confluence is the combination of the terminal li of the confluent tributaries using an uca weighted average the final li value assigned to a reach is the average of the initial and terminal li if the li attenuates to zero along the reach the proportion of reach length at zero is used to weight the averaging of the final li value first shreve order streams without upstream headwater lakes are simply assigned a final li of zero the process is repeated until all reaches in the processing extent have been assigned an li value the segments can be classified to form larger units and thus form another hierarchical classification level for the aec the segments where grouped into classes based on their averaged reach attributes this classification was achieved aspatially i e no flow adjacency required by binning the segments based on their groundwater water inflow potential i e base flow turbidity potential and dem derived channel slopes the binning process produced 16 stream classes a drastic reduction in complexity fig 6b vs fig 7 4 conclusions modern stressors like water taking pollution land use changes flow alterations climate change and over exploitation are threatening the ecological health of river systems around the globe resource managers tasked with mitigating or preventing further decline of degraded rivers or preserving pristine systems need new tools to help them gain a new perspective on their rivers at reach to regional scales raft and resultant classifications allow these resource managers to rapidly gain such a perspective by reducing the inherent multi scale stream network complexity using an objective stream reach aggregation method the resulting segments are thought to represent the smallest useful units for resource management seelbach et al 1997 segments can be aggregated into additional spatial hierarchies e g classes zones to further reduce complexity jones and schmidt 2017b we recommended that users clearly define their management objectives or hypotheses because it will improve their choices of landscape variables the process scales at which they operate and clustering thresholds raft can be applied over large geographic extents using widely available global scale gis landscape data e g dem geology land cover making it suitable for national applications even in regions of the world that are deficient in locally collected gis or field data declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we would like to thank brendan quigley at the ontario provincial geomatics service centre for assisting with the initial migration of the michigan valley affinity search technique vast code from vba to matlab we also appreciate the contributions of paul seelbach lizhu wang kevin wehrly james mckenna and catherine riseng at the university of michigan in ann arbor for sharing their experience with clustering the stream segments of michigan this research was funded by the government of ontario 
25985,the characteristics of a landscape and drainage size defines the abiotic character of the stream we advance on an existing clustering method by making it faster more stable capable of processing large areas and adding several new user defined options the software clusters interconfluence stream reaches into larger homogenous stream segments using a series of rules about network adjacency upstream catchment area and similarity of landscape characteristics landscape variables and clustering parameters are discussed to provide guidance for helping users achieve their clustering objectives we provide an example of clustering stream reaches in ontario canada that was used to develop a provincial aquatic classification keywords clustering ecology stream network landscape character upstream catchment computer program 1 introduction the physical character of a landscape defines the abiotic character of the stream it drains or as hynes 1975 aptly states the valley rules the stream in turn a stream s abiotic character is the template that determines the composition of ecological communities southwood 1977 poff 1997 this connection between streams and their landscapes allows us to make inferences about lotic communities using broad scale digital landscape data e g geology land cover this means that costly field data on every stream reach is not required for inventory the growth of computing power and remote sensing technology has enabled network analysis to rapidly change thinking in stream ecology and opened new avenues of discovery jones and schmidt 2017a however there are challenges to overcome before meaningful inferences can be made based on the habitat templates because they are not discretely compartmentalized in space and time cullum et al 2016 instead streams flow through a complex collection of overlapping gradients of environmental conditions ecological classification attempts to impose boundaries at meaningful thresholds along these gradients to create ecologically homogenous habitat units that are useful for management omernik 1987 melles et al 2014 the fundamental premise is that ecological variability within the classified units is less than the variability between units and that they support similar biota which respond similarly to stressors the resulting classification reduces complexity allowing resource managers to make informed decisions not possible in the past a major challenge is determining boundaries that discretize stream networks into homogenous parts previous efforts have used expert opinion to define stream units which is fine if the landscape is well known baker 2006 seelbach et al 2006 but as noted by brenden et al 2008a individual aquatic ecologists will likely interpret the same maps differently which may result in different river valley segment boundaries for a river network earlier works used broad zones to define longitudinal characteristics illies and botosaneanu 1963 and combinations of watersheds and terrestrial land classifications omernik and bailey 1997 defining stream units also requires a hierarchical approach to account for the different spatial scales at which the physical processes operate frissell et al 1986 poole 2002 a bottom up classification approach can be used where smaller spatial units are aggregated into increasingly larger units forming the different levels of the hierarchy melles et al 2014 a suitable fundamental unit i e the lowest level of the hierarchy in river networks is the interconfluence reach referred to as a reach from herein reaches have a clear spatial definition and there are existing methods to delineate them using geographic information systems gis and digital elevation models dem fig 1 the reaches and their catchments i e areas of land contributing water to them can be attributed with a variety of landscape data thought to influence ecohydrological processes this reach based landscape data is simply an inventory of fundamental spatial units a process is needed that will bring these reach units together to form larger ecologically meaningful units using a set of objective and repeatable aggregation rules these larger units are called stream segments the segments represent the next hierarchical level of the stream classification and they are thought to be relatively homogenous units for abiotic and biotic characteristics and persistent over space and time seelbach et al 1997 seelbach et al 2006 the conceptual foundation for the reach to segment aggregation rules has been laid by existing theoretical knowledge pertaining to how lotic systems change from headwaters to river mouth starting with the geomorphological concepts of horton 1945 and strahler 1957 to the ecological thinking introduced with the river continuum concept rcc vannote et al 1980 followed by the serial discontinuity concept sdc ward and stanford 1983 and more recently the network composition hypothesis nch jones and schmidt 2017a we present a software tool called reach affinity tool raft the tool is capable of objectively applying a series of upstream catchment area i e size and similarity rules based on the concepts outlined by the nch to classify streams across large geographic extents 1 000 000 km2 at high spatial resolution 1 100 000 scale or 30 m cell size using readily available spatial digital data e g dems geology land cover the software has already been applied during the development of the aquatic ecosystem classification aec for the province of ontario canada jones and schmidt 2017b although we are presenting raft from an ecological perspective it is not limited to ecological applications other potential uses for raft include clustering reaches into segments of homogenous valley morphology e g incision depth flood plain width potential for bank erosion and sediment transport e g stream power bed material and sinuosity and nutrient loading potential e g landcover geology and soils to name a few examples the spatial structure and attribute content of raft input data is entirely in the hands of the user for example if the user would like to explicitly include dams and barriers in their results then they must prepare their input stream network data to include break points at these features to create unique reaches prior to running raft in the example we provide for ontario anthropogenic factors were intentionally not included because 1 we were interested in assessing fundamental natural drivers e g geological setting of stream type and 2 including anthropogenic factors e g land use would require that the classification be continuously updated which defeats its purpose anthropogenic influences are addressed by a separate stream stress analysis we conducted which can be overlaid on the stream classification unpublished data 2 program design and operation raft is a spatially aware network based software tool that clusters stream reaches into stream segments at the next hierarchical level the reaches that come together to form the segments should be of similar size and landscape character the degree of similarity can be adjusted by the user via a variety of settings that can be manipulated using a graphical user interface gui raft is based on the valley affinity search technique vast which was developed by researchers at the university of michigan seelbach et al 2006 brenden et al 2008a with their permission we used their ms excel visual basic for applications vba source code as a foundation our goal was to adapt and enhance the algorithms contained in the vast code for classifying ontario s rivers our software allows for the inclusion of lake influences within the network it enhances the stream reach size and similarity aggregation rules while greatly increasing processing stability and speed needed for use across large geographic areas 2 1 input data requirements input data for raft is composed of tabular data describing reaches within a river network where the characteristics of each reach line is described as a record i e row in a table the lines of a network are called edges and the junction points between edges are called nodes the edges of a network have an inherent direction describing the direction of movement along them e g water flowing downstream this directionality is crucial when working with stream networks two key attributes of the reach attribute table i e fields or columns are called the from node and to node identifiers we used arc hydro maidment 2002 to generate the stream network arc hydro is spatial database structure that includes a set of network building and manipulation tools for use within arcgis esri 2012 arc hydro provides the benefit of generating a variety of spatial layers that can be used to collect and summarize information about the landscape draining to each reach fig 1 also see brenden et al 2006 understanding streams in landscapes featuring many lakes requires the lakes to be taken into consideration when developing descriptive stream network algorithms brenden et al 2006 jones 2010 raft includes the option to use reach data that includes waterbody inlet outlet nodes in addition to the required basic confluence nodes required by arc hydro fig 2 other means can be used to generate the required data table given that it includes the mandatory fields of table 1 and the table is formatted as a comma separated text file it is recommended that the choice of landscape variables and clustering thresholds is hypothesis driven ecologically meaningful and match the scale of the process for example the amount of groundwater contributions to a stream i e base flow plays a large part in determining summer water temperatures which in turn determines the type of fish species that can tolerate those temperatures the hydrologic process scale of this variable is the geology driven base flow contributions of the entire upstream catchment area not just the reach contributing area see fig 1 a second recommendation is keeping the list of variables as parsimonious as possible because as more variables are included in the similarity comparison it becomes less likely to find meaningful differences amongst reaches houle et al 2010 melles et al 2014 provide a comprehensive summary of existing aquatic classifications and the landscape variables they have used some specific examples include catchment surficial geology catchment slope catchment landcover reach july mean temperature valley width and channel sinuosity snelder and biggs 2007 brenden et al 2008b not all variables included in the input table need to be used for the reach similarity comparisons setting a variable s weight to zero will cause raft to ignore it during the comparison but it still produces segment summary statistics for those variables while a value of 999 will cause raft to ignore the variable entirely 2 2 raft clustering algorithm the probability of an ecological change downstream from a tributary is higher when the two merging streams are of similar size conversely small tributaries flowing into to larger main stems have a lower probability of causing ecological changes jones and schmidt 2017a the size relationship between the main stem and the tributary can be described by the confluence symmetry ratio csr which is calculated by dividing the drainage area of the main stem by the drainage area of the tributary benda et al 2004 in addition to size the probability of change downstream from the confluence increases as the landscape character of the two drainages becomes more dissimilar this means that even a small tributary with a low csr can influence downstream characteristics on the larger main stem if it is draining a very different type of landscape for example a relatively small warm and turbid tributary flowing into a large cold clear trout stream may change the main stem more than a similarly sized tributary that is also cold and clear these relationships set up simple expectations about the relative importance of confluences in stream networks and provide a physical basis for the likelihood of change downstream of tributaries and network heterogeneity of habitats jones and schmidt 2017a raft groups reaches into segments based on upstream catchment area and similarity in landscape characteristics using two stages of processing during both stages only adjacent reaches are compared i e those connected directly downstream the first stage divides the entire network into intermediate spatial units called neighbourhoods based on upstream catchment area uca rules at confluence network nodes the second stage joins reaches into segments within the neighbourhoods using landscape similarity comparisons neighbourhoods represent a classification by size while segments refine the size classification by considering landscape similarity 2 3 grouping reaches into neighbourhoods abrupt changes in the volume of flow temperature and sediment along the length of a stream are identified using the csr a csr equal to one indicates that both reaches at the confluence have the same uca as tributary size decreases relative to the main stem csr approaches zero virtual reaches within lakes i e virtual network connectors within waterbody polygons are automatically grouped into lake neighbourhoods which subsequently become lake segments with one segment per lake three rules are applied using the csr values as illustrated in fig 3 the csr rules are not applied at the confluences of 1st strahler order headwater streams which are automatically assigned to the same neighbourhood to prevent unnecessary complexity in headwater areas at the size comparison stage thus allowing the subsequent landscape similarity comparison to be the determining factor during clustering 1 stream reaches between the lower and upper csr e g 0 25 0 5 are allowed to join together into a neighbourhood because they have similar sizes and thus potentially similar ecological characteristics fig 3i 2 if the csr is below a lower threshold e g 0 25 a tributary is considered too small to significantly affect the main channel fig 3ii in this scenario the main channel neighbourhood remains uninterrupted while the tributary becomes part of another neighbourhood the rationale is that a small tributary should not become part of the main stem neighbourhood because they likely have different channel morphology e g riparian shading bankfull width 3 conversely when the csr at a confluence exceeds the upper threshold e g 0 5 a new neighbourhood is initiated beginning with the reach directly downstream of the confluence fig 3iii the reasoning is that the combined volume of water in the downstream channel increases enough to change channel morphology e g channel width shading temperature riparian influence subsequently a fourth rule is applied that prevents large main stem river neighbourhoods from becoming too large the mainstems of rivers of higher strahler order have large drainage areas and few tributaries are large enough relative to these mainstem to split the neighbourhood based on the upper csr threshold fig 3iii however many small tributaries may join the mainstem causing its flow volume to gradually increase without causing any abrupt changes in stream character the larger the mainstem catchment grows the less likely it is that a tributary will be large enough to cause a split and as a result large portions of the river are likely to be grouped into a single neighbourhood fig 4 a this is problematic because the stream reaches at the upstream end of such a neighbourhood might have a bankfull width of 25 m i e uca 1000 km2 whereas at the reaches at the downstream end might be 50 m wide i e uca 2000 km2 and therefore the reaches of this neighbourhood should not be considered ecologically homogeneous consequently neighbourhoods with such an unacceptably wide range of reach ucas need to be divided raft determines the best location to make the split within each of these neighbourhoods by calculating a ratio of a neighbourhood s minimum and maximum ucas called the neighbourhood upstream catchment area ratio nucar the nucar is a relative measure of the difference in uca within a neighbourhood within each neighbourhood raft identifies the reaches whose uca fall between the lower and upper threshold of the user defined nucar window e g 1 5 to 1 75 the window allows raft some flexibility to find the largest tributary for making the split this prevents neighbourhoods from being divided at small insignificant tributary confluences that can push the nucar above the upper threshold fig 4 b and 4c the process repeats until all neighbourhoods have a nucar below the upper threshold value stream width increases with uca at a decreasing rate caissie 2006 raft can account for this variable rate of width increase by allowing the user to apply an exclusion cut off value below which the nucar process is not applied e g uca 50 km2 for example the nucar for a mid order neighbourhood e g 350 km2 150 km2 2 33 is the same as the nucar for a low order neighbourhood 3 5 km2 1 5 km2 2 33 ecologically speaking the difference in width may not be different enough in the low order neighbourhood e g riparian vegetation shading unaffected to warrant a split hence a user may want to use the nucar exclusion rule an optional fifth rule can be applied during the neighbourhood creation process it makes use of the zone of influence zoi of lakes below their outlets lakes and rivers are intimately connected in an alternating series of lentic and lotic reaches in the canadian shield geologic region which is approximately 4 6 million km2 or nearly half the land area of canada and other similar geologic regions around the world e g scandinavia russia rapid and predictable changes in the ecological characteristics of streams occur at the interface with lakes jones 2010 in many cases water leaving the lake will be warmer and carry large amounts of seston e g dissolved organic carbon phytoplankton zooplankton flows will be more constant i e less flashy the stream s water at the lake outlet might be clearer as particulate matter precipitates out in the lake the combination of changes in the abiotic and biotic conditions fundamentally changes the ecology of the outlet stream however changes are context dependent for example large rivers with catchment areas above 1000 km2 maybe too large to be influenced by lakes to make use of this optional rule the user must estimate a zoi below every lake outlet based on some attenuation function where each reach in the network is assigned a lake influence li value ranging from zero to one where one is the maximum influence a lake a can exert usually at the lake outlet this maximum value attenuates to zero some distance downstream from the outlet indicating that a reach is not influenced by an upstream lake it is the responsibility of the user to assign li values to the reaches prior to running raft if they wish to include it during clustering 2 4 grouping reaches into segments within the neighbourhoods raft s second stage groups reaches into segments within neighbourhoods based on their landscape similarities e g geology land cover channel slope segments cannot expand outside their neighbourhood boundary but a neighbourhood can contain multiple segments this process begins by sorting all reaches within a neighbourhood in descending shreve order shreve 1966 which means that processing will proceed from downstream to the upstream raft starts a new segment using the first reach in the sorted list the attributes of this first reach are compared with the attributes of its adjacent upstream reach along the main channel using a euclidean distance ed measurement romesburg 1989 when the euclidean distance is below a user defined affinity threshold at the adjacent reach is considered to be similar and part of the same segment the algorithm continues the comparison process along the mainstem while smaller adjacent tributary reaches in the neighbourhood along the way are set aside for the moment as the segment grows each new candidate reach is compared to each one of the reaches already part of the segment once the ed of the most upstream reach of the mainstem is evaluated the algorithm then moves on to the tributaries previously set aside again working from largest to smallest shreve order the ed to at comparison can use one of several different methods 1 the complete linkage clink method which uses the largest ed between the candidate reach and the reaches of the segment 2 the single linkage slink method which uses the smallest ed between the candidate reach and the reaches of the segment and 3 the unweighted pair group method with arithmetic mean upgma method which uses the average of the eds of the segment to the candidate reach details about differences amongst these methods can be found in romesburg 1989 the ed calculations use equal attribute weighting by default but the user can specify custom weights to emphasize importance of one or more variables before any segment is considered finished it is treated to a within segment double check where the affinity of each reach to every other reach in the segment is checked once more this ensures that all reaches in the segment remain under the at because reaches added earlier in the process might exceed the at when compared to reaches added later due to the processing order if a reach is found with an affinity at or above the at then it is assigned its own segment and the remainder of the reaches are returned for re clustering if no such reaches are found then the segment is finished and a new segment is started for processing the algorithm runs until all reaches in a neighbourhood have been assigned to a finished segment the last stage of the algorithm is one more double check amongst segments it may be the case where a reach has a higher affinity to an adjacent segment than to the segment to which it is currently assigned due to processing order an affinity check is performed at the interface of segments but not across neighbourhood boundaries reaches with higher affinity i e lower ed to an adjacent segment are reassigned to that segment this process is iterative until no reaches can be reassigned indicating that optimal segmentation stability has been achieved 2 5 graphical user interface the user interacts with raft using its graphical user interface gui the gui allows the user to load their input data set all the necessary parameters required for raft and to specify output name and location when raft is running the gui also displays performance metrics and general progress status fig 5 2 6 user defined threshold choices the raft algorithm requires several user defined threshold values to produce optimal results the threshold values will influence the size and structure of the resulting segments there are no right or wrong choices but the chosen values should take the user s tolerance for reach heterogeneity within segments into consideration higher similarity requirements will produce segments that are smaller i e composed of fewer reaches and more homogenous in character at the cost of increased segment count and classification complexity considering the rapid speed at which raft operates we encourage users to experiment with different threshold values to find the set that optimally suits their requirements the csr thresholds are used to determine the size and structure of network neighbourhoods the user must provide a lower and upper value or use default values the lower threshold must be less than the upper threshold and both must be between zero and one decreasing the lower threshold value will result in more small tributaries being allowed to join a neighbourhood increasing the upper threshold will result in increasingly larger tributaries to join the neighbourhood the larger the difference between the two csr thresholds the larger neighbourhoods that are created i e more reaches the li threshold works in tandem with the csr thresholds its value must be a between zero and one li has a strong effect on the way neighbourhoods are created because reaches that are considered to be influenced by lakes cannot be joined to reaches that are free of lake influence the li threshold value allows the user to specify a cut off below which the neighbourhood creation part of the raft algorithm ignores non zero li values this gives the user some flexibility to exclude inconsequentially small li values this is especially useful when using li attenuating functions that produce asymptotic results e g natural decay raft ignores all input li values during the neighbourhood creation process when the threshold is set to 1 the nucar window is defined by a pair of threshold values that work together in tandem they refine the size and structure of network neighbourhoods after the csr thresholds have been applied the upper nucar dictates the maximum reach uca difference allowed within a neighbourhood increasing the upper nucar threshold will allow larger neighbourhoods to form at the cost of increased inter neighbourhood reach uca dissimilarity it is recommended to keep the nucar window width below 0 5 to allow for flexibility while making results more interpretable the nucar application threshold allows the user to prevent the nucar rules from being applied to reaches below the thresholds uca value setting this value to zero will result in the nucar rules to be applied to all reaches in the processing extent while a value equal to the maximum uca of the processing extent will prevent the nucar rules from being applied entirely the choice of at affects the size of the segments raft creates within neighbourhoods lower at values create smaller more homogenous segments i e fewer reaches and higher within segment similar while higher values create larger more heterogeneous segments i e more reaches and lower within segment similarity the theoretical range of ed values is a function of the range of the landscape characteristics that are used to calculate them for example three landscape variables that range from zero to one using equal weights that sum to one i e 1 3 should result in a set of ed values also between zero and one meaning that the at must also fall inside this range when the ed is zero the landscape characteristics of the reaches are identical while ed of one indicates perfectly opposite reaches in practice these extremes are never reached thus we advise users to become acquainted with distribution of actual ed value within the entire processing extent prior to using raft this distribution can be calculated by finding the ed of every reach to all other reaches if the maximum ed is 0 6 then it would not be wise to choose an at of 0 5 because the threshold would rarely be exceeded during the reach comparisons producing large segments with unacceptable amounts of landscape character heterogeneity conversely choosing an at that is too low e g 0 1 would result in highly homogenous segments however such segments would be so small that they would not reduce complexity much beyond the input reaches both scenarios defeat the purpose of clustering it is recommended that several potential at values be tested to allow evaluation of the different results against the hypothesized expectations field measurements and the objectives of the clustering exercise 2 7 raft tabular output raft outputs an ms excel workbook containing several worksheets the segment reaches worksheet contains information for the individual reaches including the segmentid to which they have been assigned and the variable values used to calculate ed during clustering the segment summaries sheet contains a variety of summary statistics of each segment s composite reach input variables the summaries include the variables used for clustering as well as the inert variables i e those assigned a weight of zero the neighbourhood sheet contains the neighbourhood identifier associated with each reach capturing the results of size rule clustering alone the distances sheet provides the absolute i e global ed of the reach to all other reaches of the segment and relative i e local within segment distance values the absolute values are the raw ed similarity measures of each reach compared to all the other reaches inside the segment using the chosen comparison method e g upgma the relative local values are a range scaled measure of each reach s similarity within its segment a relative local distance of zero indicates that the reach has the smallest within segment ed while a value of one indicates the reach with largest within segment ed the settings sheet contains relevant metadata for each analysis run 3 a practical demonstration of raft the aquatic ecosystem classification aec for ontario canada serves as a real world application of raft ability to cluster reaches into ecologically homogenous segments jones and schmidt 2017b raft was used to process all the streams and rivers of ontario totalling 710 000 reaches across 1 000 000 km2 area of land resulting in 335 000 segments and 180 000 virtual lake segments the processing was achieved by dividing the provinces into 47 smaller work units containing up to 100 000 reaches each raft runs very efficiently with the largest work units finishing in a matter of minutes on workstation class computer in 2018 we do not see any reason why larger areas with more reaches 100 000 could not be processed during a single raft run the saugeen river watershed serves as an example at a suitable scale for illustrating the reach to segment clustering process that was applied across the whole province the saugeen river flows into lake huron at southampton ontario outlet at 44 30 3 n 81 22 26 w and drains an area of 3983 km2 it has a diverse landscape of post glacial geologic deposits e g gravel sand clay with underlying mesozoic bedrock e g shale limestone and a temperate climate the spatial data was generated using arc hydro tools with the ontario integrated hydrology 30 m dem omnrf 2016 and the ontario hydro network omnrf 2015 the saugeen river watershed contains 2957 reaches fig 6 a raft clusters these reaches into 2245 segments fig 6b using the thresholds and landscape variables found in table 2 we used three landscape variables for segment clustering the first two are surrogates based on uca geologic character they are baseflow index bfi which is a measure of the proportion of groundwater contribution potential and low flow turbidity potential the third variable is reach channel slope which is a surrogate for flow velocity the input landscape variables used for the ed comparison were all range scaled to between 0 and 1 legendre and legendre 2012 and given equal weights these settings correspond to the values used to produce the first version of the aec for ontario jones and schmidt 2017b the higher order main stem segments are large i e composed of many reaches due to the homogenizing landscape averaging effect described by the network composition hypothesis nch jones and schmidt 2017a conversely the segments in the headwaters are generally smaller because of the increased potential for landscape heterogeneity amongst their component reaches i e higher dissimilarity for the lake influence li all reaches in the processing extent were sorted in ascending shreve order to ensure that processing occurs from the outermost branches of the network to the pour point the initial li at the beginning of a lake outlet reach was set to one the terminal li of the reach was calculated by multiplying the reach length by an attenuation rate which is the rate of change of li per kilometre of reach length e g lake influence rate of change liroc 1 66 ucareach 0 6 then subtracting the product of the multiplication from the initial li the initial li at a confluence is the combination of the terminal li of the confluent tributaries using an uca weighted average the final li value assigned to a reach is the average of the initial and terminal li if the li attenuates to zero along the reach the proportion of reach length at zero is used to weight the averaging of the final li value first shreve order streams without upstream headwater lakes are simply assigned a final li of zero the process is repeated until all reaches in the processing extent have been assigned an li value the segments can be classified to form larger units and thus form another hierarchical classification level for the aec the segments where grouped into classes based on their averaged reach attributes this classification was achieved aspatially i e no flow adjacency required by binning the segments based on their groundwater water inflow potential i e base flow turbidity potential and dem derived channel slopes the binning process produced 16 stream classes a drastic reduction in complexity fig 6b vs fig 7 4 conclusions modern stressors like water taking pollution land use changes flow alterations climate change and over exploitation are threatening the ecological health of river systems around the globe resource managers tasked with mitigating or preventing further decline of degraded rivers or preserving pristine systems need new tools to help them gain a new perspective on their rivers at reach to regional scales raft and resultant classifications allow these resource managers to rapidly gain such a perspective by reducing the inherent multi scale stream network complexity using an objective stream reach aggregation method the resulting segments are thought to represent the smallest useful units for resource management seelbach et al 1997 segments can be aggregated into additional spatial hierarchies e g classes zones to further reduce complexity jones and schmidt 2017b we recommended that users clearly define their management objectives or hypotheses because it will improve their choices of landscape variables the process scales at which they operate and clustering thresholds raft can be applied over large geographic extents using widely available global scale gis landscape data e g dem geology land cover making it suitable for national applications even in regions of the world that are deficient in locally collected gis or field data declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we would like to thank brendan quigley at the ontario provincial geomatics service centre for assisting with the initial migration of the michigan valley affinity search technique vast code from vba to matlab we also appreciate the contributions of paul seelbach lizhu wang kevin wehrly james mckenna and catherine riseng at the university of michigan in ann arbor for sharing their experience with clustering the stream segments of michigan this research was funded by the government of ontario 
25986,this study describes an automated system that generates a statewide real time quantitative precipitation estimation qpe product for flood forecasting in iowa the qpe system comprises real time data acquisition processing and product visualization subsystems combined with information retrieved from numerical weather prediction the system processes data from multiple radars using various algorithms accounting for precipitation microphysics and radar remote sensing uncertainties the system generates a composite rainfall map covering the entire state of iowa at a resolution of 0 5 km updated every 5 min with the help of the system s flexible modular configuration we have recently added a new polarimetric algorithm based on specific attenuation independent evaluations based on comparisons with rain gauge data and hydrologic model prediction of streamflow demonstrate that the new implementation significantly improves the rainfall estimation accuracy the new qpe product shows performance comparable to the multi radar multi sensor product that contains a rain gauge correction keywords qpe radar rainfall flood forecasting specific attenuation 1 introduction using data from the u s weather surveillance radar 1988 doppler wsr 88d network the iowa flood center ifc has provided a statewide real time rainfall product since the ifc s establishment in 2009 this product generation was motivated by the need for real time flood prediction in iowa which has repeatedly experienced devastating floods at various scales in recent decades e g smith et al 2013 vennapusa and white 2015 seo et al 2018 our goal to forecast everywhere and cover all iowa communities e g regardless of catchment scale has led to the use of a distributed hydrologic model which requires spatially variable rainfall inputs krajewski et al 2017 the ifc quantitative precipitation estimation qpe framework was initially built on the real time hydro nexrad application krajewski et al 2011a 2013 kruger et al 2011 seo et al 2011 most scientific algorithms in our qpe system have evolved according to the wsr 88d s hardware and polarimetric upgrades e g istok et al 2009 the ifc qpe system acquires real time data from seven wsr 88ds karx in la crosse wisconsin kdmx in des moines iowa kdvn in davenport iowa keax in kansas city missouri kfsd in sioux falls south dakota kmpx in minneapolis minnesota and koax in omaha nebraska as shown in fig 1 the system then generates a composite rain rate map covering the entire domain fig 1 with temporal and spatial resolutions of 5 min and 0 5 km respectively using a variety of processing algorithms as documented in seo et al 2011 2015 and seo and krajewski 2015 as of early 2019 we had added a state of the art polarimetric algorithm known as specific attenuation e g ryzhkov et al 2014 wang et al 2019 to our qpe procedures to fully benefit from the wsr 88d s dual polarization dp capability this new algorithm required several new elements for example one that retrieves temperature soundings from the numerical weather prediction nwp model analyses to identify the melting layer ml location before this new implementation the use of dp in the ifc system was limited to basic data quality control e g removal of non meteorological returns and the system s main estimator was a single polarization based algorithm using a reflectivity rain rate z r relation the new implementation using the specific attenuation method promises to be the most significant milestone in our system s 10 year history as the method has demonstrated meaningful improvements in qpe accuracy seo et al 2020b therefore we take this opportunity to document the architecture and capabilities of our fully automated qpe system including algorithm updates and new developments as well as the way it complements the outdated descriptions presented in seo et al 2011 to validate the attainable improvement in qpe and subsequent hydrologic prediction we generated the statewide qpe products using the latest and prior algorithms for a three year period 2016 2018 and evaluate the performance of each one using rain and stream gauge observations we also compared the performance of our qpe products with that of u s national qpe products e g zhang et al 2016 cunha et al 2013 that have been widely used for meteorological and hydrological applications in section 2 we describe the architecture of our qpe system by specifying the three main subsystems associated with real time data acquisition nwp analysis and radar data processing and final product visualization section 3 provides the algorithm details of module elements in the nwp individual radar and composite data processing section 4 evaluates the qpe products generated by our algorithms using rain gauge data and hydrologic simulations in section 5 we summarize the algorithm features and main findings from the product evaluation finally we discuss the improvements gained by implementing the new polarimetric algorithm and potential future developments 2 system architecture a real time qpe system requires several modular elements ranging from data acquisition to final product generation these modules include real time radar and nwp data retrieval algorithms for data quality control volume scan data processing and precipitation estimation and digital product and statewide map generation our qpe system comprises three main subsystems 1 the local data manager ldm e g fulker et al 1997 system for real time nwp analysis and radar data acquisition 2 the data processing system that contains a variety of scientific algorithms associated with precipitation estimation as illustrated in fig 2 and 3 the product visualization system for weather monitoring 2 1 local data manager the ifc ldm system acquires real time streaming level ii radar volume data e g crum et al 1993 kelleher et al 2007 and nwp model analyses using internet data distribution idd technology yoksas et al 2006 level ii data containing six radar observables reflectivity radial velocity spectrum width differential reflectivity copular correlation coefficient and differential phase are split into multiple files usually more than 100 for a single volume scan and then distributed for faster data transfer once our ldm has confirmed receipt of all the radar volume file pieces labelled with sequential integers it combines them and performs a basic quality check e g counting the number of elevation angles for file completeness more detailed descriptions of the level ii data reception quality check data packing and format conversion are documented in krajewski et al 2013 we also obtain analysis results of temperature and geopotential height at various pressure levels e g between 100 and 1000 mb and at ground level e g 2 m above from the rapid refresh rap and high resolution rapid refresh hrrr models e g weygandt et al 2009 benjamin et al 2016 rap and hrrr are continental scale real time convection allowing models updated hourly with 13 and 3 km resolution horizontal grids respectively the hrrr model is fully dependent on its parent models the radar assimilating rap and the radar enhanced rapid update cycle ruc the atmospheric variables we retrieve from the two nwp models are not model forecasts but rather the model analysis used for model initializations we use the retrieved nwp variables to build the information required for precipitation classification e g rain snow and ml identification which can significantly affect the accuracy of radar derived qpe since the latency of this nwp analysis through ldm can be up to 2 h based on our several years of operational experience we assume that the retrieved nwp variables e g temperature and geopotential height do not change substantially over this time 2 2 data processing this subsystem includes processing algorithm modules for radar data quality control precipitation estimation and correction map coordinate transformation and precipitation product generation as illustrated in fig 2 the system manager i e python scripts sequentially executes the algorithm modules and organizes the input and output data of each sequential procedure we use the nasa radar software library an object oriented library written in c to decode radar level ii data our algorithm modules are also written in c for compatibility and efficient data processing the key feature of the data processing system is its modular algorithm structure which makes it easier to upgrade replace and append algorithm elements the modular structure also provides users with flexible options to bypass e g correction algorithms and select e g rain rate estimators specific algorithms or their elements depending on the user s interests and purpose this flexibility allowed us to append a new polarimetric algorithm seo et al 2020b with some additional procedures e g ml layer identification we evaluate our rainfall estimates generated using the new and former qpe algorithms in section 4 once radar volume data pass the file completeness check in the ldm system the data processing system takes over and implements several algorithm procedures to generate qpe products the first step performed in individual radar data processing is data quality control qc to eliminate non meteorological radar returns e g anomalous propagation and ground clutter in this step we also filter out noise presented in the radar observables e g differential phase the qc and other individual radar data processing modules function based on spherical coordinates e g 0 5 by 250 m and are applied to all elevation angles after the qc the system builds temperature soundings within each individual radar domain using the retrieved nwp information and identifies the ml altitude using three dimensional 3d spherical coordinates the system then classifies and assigns precipitation types e g convective and stratiform on two dimensional 2d spherical grids using an approach documented in seo et al 2020a the rain rate estimation module then uses the ml and classification information to apply a suitable estimator to each grid and generates 2d rain rate maps for all elevation angles the hybrid scan module e g fulton et al 1998 seo et al 2011 combines these multiple elevation maps using a non parametric kernel function and feeds the final 2d individual product into the composite map processing step this composite step requires temporal and spatial synchronization since all individual radar maps have different temporal depending on the radar s volume coverage patterns or vcp and spatial coverage after synchronization the system generates a composite rain rate map every 5 min and corrects temporal sampling e g radar scanning errors using the two latest composite maps seo and krajewski 2015 while the uncorrected instantaneous map is used for map visualization only the corrected one is fed into a distributed hydrologic model the hillslope link model hlm e g krajewski et al 2017 quintero et al 2020a for real time streamflow forecasting the details of these algorithm modules are provided in section 3 2 3 product visualization as illustrated in fig 3 we visualize instantaneous rain rate and cumulative rain maps for the entire state of iowa through the iowa flood information system ifis http ifis iowafloodcenter org ifis main the map interface supports visual selection and flexible navigation for the domain of radars or watersheds of interest using the google maps api e g demir and krajewski 2013 this subsystem delivers a new rain rate map to ifis and offers an animation of these maps for the last 6 h this rain map contains a rain snow classification particularly useful in the winter season this classification uses environmental factors e g atmospheric layer thickness and surface temperature retrieved from the nwp analysis and the spatial coverage of snow is overlaid with the rain map as shown in fig 3 a as illustrated in fig 3 c the system also accumulates the rain maps over time and visualizes daily and cumulative rain maps for up to the last 14 days based to the following steps 1 today s cumulative rain map is generated by accumulating the temporal sampling error corrected rain map once the map is available e g every 5 min and is reset at midnight 2 before resetting today s rain map at midnight it is saved as a daily rain map and 3 the cumulative rain maps are generated by adding today s map to the aggregation of daily maps over a specified period e g up to 14 days for faster computation and visualization of cumulative rain maps the system maintains multi day aggregation files by adding up daily rain maps and updating them at midnight 3 algorithms in this section we provide details on scientific algorithms implemented in the data processing subsystem described in section 2 the processing algorithms are categorized into three groups 1 nwp analysis processing 2 individual radar data processing and 3 composite data processing because the system structure and algorithm elements have continuously evolved over time since their initial deployment we also discuss the changes and advancements of the algorithm components that aim to better represent atmospheric phenomena associated with precipitation microphysics 3 1 nwp processing 3 1 1 ml identification the ml is one of the most significant uncertainty factors in radar qpe because most precipitation estimators e g marshall and palmer 1948 fulton et al 1998 are based on liquid rain drop size distributions dsds as such estimation accuracy for solid and mixed precipitation within and above the ml tends to be relatively lower than that for liquid precipitation below the ml to identify and correct radar observations affected by the ml many approaches have examined the vertical structure of reflectivity e g fabry and zawadzki 1995 or enhanced dp capabilities e g baldini and gorgucci 2006 giangrande et al 2008 the ml identification module is a new procedure appended to our qpe system in accordance with the recent implementation of a polarimetric qpe algorithm known as specific attenuation seo et al 2020b which is valid only for liquid precipitation below the ml upon the receipt of hourly rap model analysis in ldm the module retrieves temperature soundings on the rap grids within our entire domain the module refers to a lookup table that assigns a horizontal rap grid to a corresponding radar grid and builds temperature profiles on the spherical coordinates within a radar domain the module then identifies the ml altitude and flags radar grids below the ml by checking if the temperature at the top of a radar beam at a given location is greater than 5 c this procedure is applied to all elevation angle data and was tested in seo et al 2020a 3 1 2 rain snow classification although many winter precipitation classification algorithms have used radar data e g park et al 2009 thompson et al 2014 we apply a rather simple scheme that does not depend on radar geometry occasionally the class designation obtained from radar only algorithms does not agree with the one observed on the ground this is because of the likely phase changes of precipitation along a falling path between a radar beam elevation and the ground in our system we use a classification approach based on the critical thickness e g keeter and cline 1991 heppner 1992 and surface temperature together as retrieved from hrrr model analysis with a 3 km horizontal grid the critical thickness is defined as the difference of geopotential height between 1000 and 850 mb fig 4 shows the relation between the two variables e g thickness and surface temperature and precipitation classes observed during the winter seasons in 2012 and 2013 we obtained the observed classes in fig 4 from automated surface observing system asos rain gauges see fig 1 which are equipped with two sensors to detect precipitation classes e g ramsay 1997 based on the preliminary analysis presented in fig 4 we selected two thresholds i e 3 c for surface temperature and 1310 m for the thickness and applies them to delineate regions with high likelihood of snow these regions are then overlaid with the instantaneous rain rate map as shown in fig 3 a we note that this snow information is used for qualitative purposes only e g map visualization for weather monitoring and is not transferred to quantitative i e hydrologic applications because fig 4 demonstrates some overlaps e g possible errors among rain freezing rain and snow in the regions near the two threshold values in section 5 we discuss this weakness and the probable improvement that could be achieved with a new approach 3 2 individual radar data processing 3 2 1 data quality control qc the qc modules encompass elimination of non meteorological radar returns and noise filtering of radar observations while it is comparatively easier to manage anomalous propagation ground clutter and biological returns using their polarimetric features e g ryzhkov and zrnic 1998 rico ramirez and cluckie 2008 the effects from wind farms wf have become a growing challenge e g vogt et al 2007 to eliminate the effects from wf and other non meteorological targets we previously had used the multiple thresholds of copular correlation coefficient  hv conditioned on the reflectivity strength seo et al 2015 however we found that an aggressive filter e g  hv 0 98 proposed to handle wf echoes occasionally removes some rain echoes as such the current qc module bypasses the aggressive filter and applies a new method instead which uses vertically integrated liquid water content vil to identify wf echoes seo et al 2020a the basis of this new idea was the fact that estimated vils for wf echoes tend to be much lower e g a weaker vertical extension than those for actual convective cells despite their strong reflectivity contamination unprocessed differential phase  dp data require unfolding and smoothing steps e g wang and chandrasekar 2009 because the data are rather noisy and are folded to a limited range e g 0 360 for wsr 88d the noise filtering module incorporates phase unfolding and smoothing routines used in the wsr 88d common operations and development environment code public package https www weather gov code88d the smoothing routines for  dp processing consist of averaging and median filters the averaging filter is also applied to horizontal reflectivity z differential reflectivity z dr and  hv along a radial direction the module also derives specific differential phase k dp from processed  dp in accordance with the code procedure 3 2 2 precipitation type and hydrometeor classification this module splits precipitation into convective and stratiform rain by estimating vil to define convective cores and gradually expanding the convective area from the cores to the adjacent grids based on a region growing method e g adams and bischof 1994 the vil estimation using 3d reflectivity and the ml information mitigates the uncertainty associated with the effects of the bright band and radar beam geometry near the radar site seo et al 2020a the convective stratiform classification was originally designed to apply separate rain rate estimators to different rain types as well as to correct the error arising from the non uniform vertical profile of reflectivity vpr for the stratiform regions e g bellon et al 2005 krajewski et al 2011b the correction module using a real time vpr is currently inactive because the default rain rate estimator has been switched from the reflectivity based method to the specific attenuation method the real time vpr correction approach is documented in seo et al 2011 we describe both estimators in the next subsection the module also performs hydrometeor classification for solid and mixed precipitation e g ice and dry wet snow this classification uses the polarimetric signatures of z z dr and  hv described in straka et al 2000 and ryzhkov et al 2005 combined with ml information hail is simply identified when reflectivity is greater than 53 dbz 3 2 3 rain rate estimation the module provides multiple rain rate r estimators including the new development of specific attenuation a 1 r z 0 017 z 0 714 2 r z 0 017 z 0 714 f o r c o n v e c t i v e r a i n 0 036 z 0 625 f o r s t r a t i f o r m r a i n 3 r z z d r 0 0067 z 0 927 z d r 3 43 4 r k d p 40 5 k d p 0 85 5 r a 4120 a 1 03 where the unit scale of z and z dr is decibels db in eqs 1 3 although the default rain rate estimator for our qpe system is currently set to specific attenuation using eq 5 such a selective option enables us to investigate the event dependent performance of the various estimators e g thurai et al 2017 while all the estimators presented in eqs 1 5 are valid only for pure rain below the ml the module selectively applies one of the estimators in eq 6 to solid and mixed precipitation within and above the ml e g istok et al 2009 6 wet snow 0 6 r z dry snow within the ml r z dry snow above the top of the ml 2 8 r z ice crystals 2 8 r z hail r k d p 29 0 k d p 0 77 in the case where the nwp analysis for the ml identification is missing we simply apply eq 1 to the entire radar domain we note that estimation of specific attenuation a is quite complicated e g ryzhkov et al 2014 wang et al 2019 and detailed procedures implemented in our qpe algorithm are provided in seo et al 2020b 3 2 4 hybrid scan after generating rain rate maps at all elevation angles e g 3d the hybrid scan module constructs a 2d rain map by vertically combining the 3d rain maps using a linear weighting scheme a non parametric kernel function e g log normal assigns weights to the elevation data at a given azimuth and range location depending on the vertical distance between the center of the data grid and a predefined constant altitude e g 1 5 km the total of weights assigned along a vertical column must be unity through the normalization of assigned weights and the angle data closer to the constant altitude are given greater weight this smoothing technique tends to mitigate the discontinuity often observed between the transition areas of elevation angles e g seo et al 2011 the hybrid scan approach tends to suppress significant ground clutter caused by the side lobe effect which occasionally appears in the lowest elevation angle data e g fulton et al 1998 3 2 5 temporal synchronization temporal synchronization of individual rain maps is vital for generating a composite rain product because the observation times and intervals among the radars shown in fig 1 differ depending on their local weather conditions to synchronize all irregular data map timestamps we define a 5 min nominal timestamp and generate new rain maps at those timestamps using the latest two consecutive rain maps see fig 2 our previous synchronization scheme interpolated the two consecutive maps by assigning weights determined by the time separation between the observed maps and the nominal time for which a new map is generated e g langston et al 2007 however we found two weaknesses in this scheme 1 the interpolation of the same rain system e g a squall line observed at different times yielded duplicate or spatially expanded systems even if the system moved from one place to another and 2 the timestamp of the interpolated map was always earlier than that of the latest map observed implying that the latency of a composite map was sometimes up to 15 min because the observed map is labelled by the moment that a radar starts scanning it already has a delay of 4 10 min to resolve these two issues with the interpolation scheme the current module executes simple tracking of the storm velocity vector using a cross correlation method e g fabry et al 1994 seo and krajewski 2015 this tracking requires grid transformation from radar spherical coordinates to geographic ones this geographic coordinate system with a resolution of quarter decimal minutes equivalent to approximately 0 5 km also provides a common reference grid for the spatial mosaic of individual rain maps to accelerate the tracking computation the module generates rain maps that are four times coarser and captures the main movement e g single vector returns to the original grid and finds the exact vector within an averaging range between the original and coarser scales the module then projects the estimated vector to the next two timestamps and produces new rain maps see fig 2 this extrapolation assumes that the estimated vector is valid within a projection time domain e g up to 10 min 3 3 composite data processing 3 3 1 spatial mosaic once the temporally synchronized rain maps are ready the qpe system manager takes over and feeds them into the spatial mosaic process every 5 min the mosaic module reads the geographic reference points e g upper left corner from the map header of all individual rain map files and places the maps onto the common grid for overlapping areas covered by multiple radars rainfall values are combined based on a linear weighting scheme using the distance and altitude between the sampling observation location and each involved radar site 7 w i exp r i 2 r 2 exp h i 2 h 2 where w i denotes an assigned weight for the i th radar and r and h are the distance and altitude both are in km between the radar sampling location and the radar sites r and h represent scale parameters and a greater smaller value for these parameters leads to a gradual rapid change of the weight values we set 100 and 2 km as default values for the distance and altitude scale parameters respectively the use of 100 km for the distance parameter smooths the sharp boundaries that occasionally appear in overlapping zones because of relative calibration biases e g seo et al 2013 keem et al 2019 between individual radars the calculated weights for a grid location in the overlapping zone are also normalized to sum up to 1 3 3 2 advection correction it has been recognized that rainfall accumulations using rain rate maps produced from radar s sporadic sampling often result in inaccurate representation e g discontinuous patterns of actual rain fields e g huebner et al 1986 liu and krajewski 1996 this temporal sampling error is often significant particularly as the storm velocity and the map spatial resolution become faster and higher respectively to mitigate this error in the composite rain map we apply an advection approach using the same vector tracking method as used in the time synchronization module the advection correction module linearly interpolates static storm locations detected at two consecutive maps and corrects for the missed rainfall accumulations the module takes two instantaneous composite rain maps with a 5 min interval calculates and projects a velocity vector and generates new intermediate rain maps with a 1 min interval within the 5 min domain seo and krajewski 2015 the module splits the entire spatial domain shown in fig 1 into six tiles for faster computation and fulfills the aforementioned advection procedure an average of the six composite maps e g two originals and four new ones within the 5 min domain is then delivered to the cumulative process for product visualization and to the hydrologic modeling component for streamflow prediction 4 product evaluation we assess the qpe products generated via the combination of algorithms described in section 3 for the three year period of 2016 2018 in two ways 1 quantitative comparison with ground reference i e rain gauge observations and 2 evaluation of hydrologic simulations driven by the qpe products using streamflow observations we generated the products using a qpe reproduction system seo et al 2019 that can readily retrieve the radar level ii data from amazon s cloud archive e g ansari et al 2017 with some substantial modifications to append newly developed processing modules we acquired rain gauge data from the national weather service nws cooperative observer program coop mosbacher et al 1989 network and streamflow data from the u s geological survey usgs the domain illustrated in fig 1 shows about 220 rain gauge and 140 streamflow stations the purpose of this evaluation is to demonstrate improvements achieved by our latest algorithms after the recent implementation of the specific attenuation method we also compare the performance of our qpe products with that of the national qpe products in the united states multi radar multi sensor mrms zhang et al 2016 and nws level iii digital precipitation rate dpr we note that the evaluation analyses are limited to the months of april through october because the quantitative estimation of winter precipitation both radar and ground based is still challenging 4 1 rain gauge evaluation in this section we use the daily coop data to evaluate four qpe products mrms nwsdp ifc r z and ifcdp r a dp denotes polarimetric estimates our ldm has been receiving a set of mrms qpe products e g radar only gauge corrected and gauge only in real time we use the hourly gauge corrected one in this evaluation the other three products are radar only estimates concerning nwsdp we obtained the dpr product for seven wsr 88d radars within the domain shown in fig 1 and applied our time synchronization and spatial mosaic procedures to generate a composite product as we did not apply our qc and correction modules in nwsdp processing the primary features of the nws polarimetric algorithm should be preserved in nwsdp ifc r z provides reflectivity based estimates generated by our former algorithm combination which does not contain algorithm modules such as noise filtering and ml identification as such we estimated ifc r z using only eq 1 for the entire radar domain the estimation procedure of ifcdp r a is primarily based on the specific attenuation method seo et al 2020b with the combination of the latest algorithm elements as presented in section 3 fig 5 illustrates the product maps of rain totals accumulated over the month of september 2016 in fig 5 we discover that ifc r z shows an underestimation tendency while the spatial rainfall structures of the other three products look quite similar it seems that ifcdp r a effectively manages the partial beam blockage effects which appear clearly in the southeast direction from the kfsd radar in mrms nwsdp and ifc r z this is because the specific attenuation method is known to be less sensitive to radar miscalibration partial beam blockage and the variability of dsds e g ryzhkov et al 2014 wang et al 2019 we observed many isolated spikes in nwsdp which correspond to wf in iowa and minnesota for exact wf locations see seo et al 2020a these spikes indicate that the nws algorithm has a qc issue and several spikes near the border between iowa and minnesota are also detected in mrms in figs 6 and 7 we present the quantitative evaluation results for the four products using the independent coop gauge data at daily and yearly april to october scales with three statistical metrics multiplicative bias b defined as a radar gauge ratio r g pearson correlation coefficient r and mean absolute error mae the term independent indicates that the data from the hydrometeorological automated data system e g kim et al 2009 included in the mrms gauge correction procedure were not involved in this evaluation as shown in fig 6 the dots representing mrms nwsdp and ifcdp r a tend to align along the one to one line with somewhat different degrees of scatter ifc r z shows the largest scatter as well as inconsistent bias changes from year to year ifcdp r a a radar only product shows performance e g accuracy quite comparable to mrms which contains a bias correction based on rain gauge observations nwsdp also provides good estimates except for an underestimation observed in 2018 and somewhat larger dispersion when compared to mrms and ifcdp r a the daily evaluation results represented as 2d histograms are provided in fig 7 the observed degrees of overall bias and dispersion for each qpe product shown in fig 6 are consistent with those displayed in fig 7 fig 7 also reveals that the four products are characterized by different uncertainty features conditioned on rainfall magnitude 4 2 hydrologic evaluation the hillslope link model hlm is a fully distributed hydrologic model that currently provides real time streamflow predictions for the entire iowa domain krajewski et al 2017 quintero et al 2020a b the hlm is based on landscape decomposition into hillslopes and channel links which enables the physical representation of runoff generation and water transport the hlm is calibration free its parameters are determined a priori therefore the model does not favor any specific forcing inputs further details regarding the hlm equations configuration and numerical solver are documented in earlier studies e g small et al 2013 quintero et al 2016 krajewski et al 2017 quintero et al 2020a we ran hlm using the hourly mrms ifc r z and ifcdp r a products as model forcing inputs and generated streamflow simulations at a 15 min time interval we excluded nwsdp from the model forcing stream because of its erroneous features e g strong spikes and unknown radial patterns shown in fig 5 the hlm simulations driven by different forcing products for each year started with the same initial condition e g model state on april 1 captured from the mrms hlm run for an antecedent period we did not calibrate model parameters because model calibration may obscure forcing dependent differences in streamflow generation for a hydrologic evaluation of the different qpe forcing products we use the performance metrics of kling gupta efficiency kge gupta et al 2009 pearson correlation coefficient r mae and root mean square error rmse kge describes the predictive power of hydrologic models and addresses deficiencies in nash sutcliffe efficiency nse nash and sutcliffe 1970 fig 8 shows a direct comparison of the effects on performance represented by kge derived from different forcing products in fig 8 hydrologic simulation results driven by ifcdp r a are compared with those driven by ifc r z and mrms at about 140 usgs stations in iowa for the years 2016 and 2017 ifcdp r a outperforms ifc r z and the performance of ifcdp r a looks comparable with that of the reference qpe product mrms similar to fig 6 although mrms seems slightly better than ifcdp r a for 2018 kge differences between the two products are likely not significant at a kge range of about 0 3 0 7 e g a cluster of dots slightly above the one to one line at the kge range to further examine the performance of mrms and ifcdp r a we compare the results from both forcing products regarding catchment scale e g upstream basin areas of the usgs stations in fig 9 on average the performance based on all four metrics tends to be better for both forcing products as the catchment scale becomes larger performance varies from year to year and no qpe product mrms or ifcdp r a is consistently better 5 summary and discussion in this paper we document the architecture algorithm configuration and estimation performance of our real time qpe system used primarily for flood prediction in iowa the ifc system is the only academia based large scale statewide real time radar rainfall monitoring system in the united states the system retrieves data from operational radars and nwp models and uses a variety of scientific algorithms to account for precipitation microphysics and uncertainties in radar remote sensing e g villarini and krajewski 2010 nwp analysis e g temperature and geopotential height acquired through ldm facilitates the ml identification and rain snow classification real time streaming data transferred from individual radars undergo several sequential processing procedures before they are combined into a composite product that covers the entire iowa domain fig 1 this sequential processing includes the radar data qc removal of non meteorological echoes and noise filtering precipitation type and hydrometeor classification rain rate estimation hybrid scan construction and time synchronization of individual rain maps the composite processing modules then spatially combine the synchronized rain maps and correct for the error arising from the radar s periodic temporal sampling finally the composite instantaneous and accumulated rain maps are visualized through ifis e g krajewski et al 2017 a flood information portal open to the general public these statewide rain maps are updated every 5 min implying that all individual and composite data processing procedures are done within a 5 min window the key feature of our qpe system is its flexible system structure e g modular configuration which allows us to improve its utility and performance by upgrading and appending algorithm elements one notable example is the recent implementation of new polarimetric algorithms while maintaining the main system structure and existing algorithm modules the new polarimetric algorithms include the precipitation estimators presented in eqs 3 5 and several subject elements such as noise filtering and ml identification procedures the specific attenuation method is a default estimator in our current operation and has improved the estimation accuracy significantly compared to the former one based on a conventional z r relation seo et al 2020b we have demonstrated that our composite specific attenuation estimates are more reliable and robust with regard to the variability of dsds and partial beam blockage and their performance e g accuracy is comparable with mrms which relies on a gauge correction fig 6 we have also verified that qpe improvements achieved through this new implementation lead to better simulation of streamflow fig 8 the next module element we plan to update or replace is the rain snow classification because the current threshold scheme has revealed many cases of misclassification see e g fig 4 data driven approaches e g machine learning will likely be useful to inspect the multi dimensional features of many more variables e g thickness temperature web bulb temperature relative humidity and radar observations relevant to winter precipitation processes bringing radar observations into this classification framework will also enable a rapid map update the rain snow boundary in the current one stays static for an hour based on the nwp s update cycle while we have demonstrated our algorithm s application to a limited spatial domain i e iowa the framework described in this paper is transferrable to other geographic areas and expandable to a larger e g regional scale the new estimator e g specific attenuation in our qpe system should be more robust through this transition because its estimates are less sensitive to the variability of dsds e g ryzhkov et al 2014 which could vary considerably in different geographic regions the required resources for the domain change include radar information e g id geographic location and spatial coverage and lookup tables for grid matching between the individual radar and nwp domains our experience with the successful application of prior developments e g krajewski et al 2013 to different regions of the united states e g lin et al 2010 yang et al 2014 confirms the system s flexibility with regard to domain change and expansion we are planning to expand the system to a much larger domain including neighboring states e g kansas missouri and nebraska for the purpose of flood mitigation in the mississippi and the missouri river basins we hope to report the results and discuss the challenging aspects of the domain expansion soon declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the iowa flood center at the university of iowa supported this work the evaluation of various qpe products was partially supported by the noaa hydrometeorology testbed hmt program within noaa oar office of weather and air quality under grant na17oar4590131 thanks to dr felipe quintero at the iowa flood center for providing the hlm simulation results driven by the different qpe products for the hydrologic evaluations in this project appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104791 
25986,this study describes an automated system that generates a statewide real time quantitative precipitation estimation qpe product for flood forecasting in iowa the qpe system comprises real time data acquisition processing and product visualization subsystems combined with information retrieved from numerical weather prediction the system processes data from multiple radars using various algorithms accounting for precipitation microphysics and radar remote sensing uncertainties the system generates a composite rainfall map covering the entire state of iowa at a resolution of 0 5 km updated every 5 min with the help of the system s flexible modular configuration we have recently added a new polarimetric algorithm based on specific attenuation independent evaluations based on comparisons with rain gauge data and hydrologic model prediction of streamflow demonstrate that the new implementation significantly improves the rainfall estimation accuracy the new qpe product shows performance comparable to the multi radar multi sensor product that contains a rain gauge correction keywords qpe radar rainfall flood forecasting specific attenuation 1 introduction using data from the u s weather surveillance radar 1988 doppler wsr 88d network the iowa flood center ifc has provided a statewide real time rainfall product since the ifc s establishment in 2009 this product generation was motivated by the need for real time flood prediction in iowa which has repeatedly experienced devastating floods at various scales in recent decades e g smith et al 2013 vennapusa and white 2015 seo et al 2018 our goal to forecast everywhere and cover all iowa communities e g regardless of catchment scale has led to the use of a distributed hydrologic model which requires spatially variable rainfall inputs krajewski et al 2017 the ifc quantitative precipitation estimation qpe framework was initially built on the real time hydro nexrad application krajewski et al 2011a 2013 kruger et al 2011 seo et al 2011 most scientific algorithms in our qpe system have evolved according to the wsr 88d s hardware and polarimetric upgrades e g istok et al 2009 the ifc qpe system acquires real time data from seven wsr 88ds karx in la crosse wisconsin kdmx in des moines iowa kdvn in davenport iowa keax in kansas city missouri kfsd in sioux falls south dakota kmpx in minneapolis minnesota and koax in omaha nebraska as shown in fig 1 the system then generates a composite rain rate map covering the entire domain fig 1 with temporal and spatial resolutions of 5 min and 0 5 km respectively using a variety of processing algorithms as documented in seo et al 2011 2015 and seo and krajewski 2015 as of early 2019 we had added a state of the art polarimetric algorithm known as specific attenuation e g ryzhkov et al 2014 wang et al 2019 to our qpe procedures to fully benefit from the wsr 88d s dual polarization dp capability this new algorithm required several new elements for example one that retrieves temperature soundings from the numerical weather prediction nwp model analyses to identify the melting layer ml location before this new implementation the use of dp in the ifc system was limited to basic data quality control e g removal of non meteorological returns and the system s main estimator was a single polarization based algorithm using a reflectivity rain rate z r relation the new implementation using the specific attenuation method promises to be the most significant milestone in our system s 10 year history as the method has demonstrated meaningful improvements in qpe accuracy seo et al 2020b therefore we take this opportunity to document the architecture and capabilities of our fully automated qpe system including algorithm updates and new developments as well as the way it complements the outdated descriptions presented in seo et al 2011 to validate the attainable improvement in qpe and subsequent hydrologic prediction we generated the statewide qpe products using the latest and prior algorithms for a three year period 2016 2018 and evaluate the performance of each one using rain and stream gauge observations we also compared the performance of our qpe products with that of u s national qpe products e g zhang et al 2016 cunha et al 2013 that have been widely used for meteorological and hydrological applications in section 2 we describe the architecture of our qpe system by specifying the three main subsystems associated with real time data acquisition nwp analysis and radar data processing and final product visualization section 3 provides the algorithm details of module elements in the nwp individual radar and composite data processing section 4 evaluates the qpe products generated by our algorithms using rain gauge data and hydrologic simulations in section 5 we summarize the algorithm features and main findings from the product evaluation finally we discuss the improvements gained by implementing the new polarimetric algorithm and potential future developments 2 system architecture a real time qpe system requires several modular elements ranging from data acquisition to final product generation these modules include real time radar and nwp data retrieval algorithms for data quality control volume scan data processing and precipitation estimation and digital product and statewide map generation our qpe system comprises three main subsystems 1 the local data manager ldm e g fulker et al 1997 system for real time nwp analysis and radar data acquisition 2 the data processing system that contains a variety of scientific algorithms associated with precipitation estimation as illustrated in fig 2 and 3 the product visualization system for weather monitoring 2 1 local data manager the ifc ldm system acquires real time streaming level ii radar volume data e g crum et al 1993 kelleher et al 2007 and nwp model analyses using internet data distribution idd technology yoksas et al 2006 level ii data containing six radar observables reflectivity radial velocity spectrum width differential reflectivity copular correlation coefficient and differential phase are split into multiple files usually more than 100 for a single volume scan and then distributed for faster data transfer once our ldm has confirmed receipt of all the radar volume file pieces labelled with sequential integers it combines them and performs a basic quality check e g counting the number of elevation angles for file completeness more detailed descriptions of the level ii data reception quality check data packing and format conversion are documented in krajewski et al 2013 we also obtain analysis results of temperature and geopotential height at various pressure levels e g between 100 and 1000 mb and at ground level e g 2 m above from the rapid refresh rap and high resolution rapid refresh hrrr models e g weygandt et al 2009 benjamin et al 2016 rap and hrrr are continental scale real time convection allowing models updated hourly with 13 and 3 km resolution horizontal grids respectively the hrrr model is fully dependent on its parent models the radar assimilating rap and the radar enhanced rapid update cycle ruc the atmospheric variables we retrieve from the two nwp models are not model forecasts but rather the model analysis used for model initializations we use the retrieved nwp variables to build the information required for precipitation classification e g rain snow and ml identification which can significantly affect the accuracy of radar derived qpe since the latency of this nwp analysis through ldm can be up to 2 h based on our several years of operational experience we assume that the retrieved nwp variables e g temperature and geopotential height do not change substantially over this time 2 2 data processing this subsystem includes processing algorithm modules for radar data quality control precipitation estimation and correction map coordinate transformation and precipitation product generation as illustrated in fig 2 the system manager i e python scripts sequentially executes the algorithm modules and organizes the input and output data of each sequential procedure we use the nasa radar software library an object oriented library written in c to decode radar level ii data our algorithm modules are also written in c for compatibility and efficient data processing the key feature of the data processing system is its modular algorithm structure which makes it easier to upgrade replace and append algorithm elements the modular structure also provides users with flexible options to bypass e g correction algorithms and select e g rain rate estimators specific algorithms or their elements depending on the user s interests and purpose this flexibility allowed us to append a new polarimetric algorithm seo et al 2020b with some additional procedures e g ml layer identification we evaluate our rainfall estimates generated using the new and former qpe algorithms in section 4 once radar volume data pass the file completeness check in the ldm system the data processing system takes over and implements several algorithm procedures to generate qpe products the first step performed in individual radar data processing is data quality control qc to eliminate non meteorological radar returns e g anomalous propagation and ground clutter in this step we also filter out noise presented in the radar observables e g differential phase the qc and other individual radar data processing modules function based on spherical coordinates e g 0 5 by 250 m and are applied to all elevation angles after the qc the system builds temperature soundings within each individual radar domain using the retrieved nwp information and identifies the ml altitude using three dimensional 3d spherical coordinates the system then classifies and assigns precipitation types e g convective and stratiform on two dimensional 2d spherical grids using an approach documented in seo et al 2020a the rain rate estimation module then uses the ml and classification information to apply a suitable estimator to each grid and generates 2d rain rate maps for all elevation angles the hybrid scan module e g fulton et al 1998 seo et al 2011 combines these multiple elevation maps using a non parametric kernel function and feeds the final 2d individual product into the composite map processing step this composite step requires temporal and spatial synchronization since all individual radar maps have different temporal depending on the radar s volume coverage patterns or vcp and spatial coverage after synchronization the system generates a composite rain rate map every 5 min and corrects temporal sampling e g radar scanning errors using the two latest composite maps seo and krajewski 2015 while the uncorrected instantaneous map is used for map visualization only the corrected one is fed into a distributed hydrologic model the hillslope link model hlm e g krajewski et al 2017 quintero et al 2020a for real time streamflow forecasting the details of these algorithm modules are provided in section 3 2 3 product visualization as illustrated in fig 3 we visualize instantaneous rain rate and cumulative rain maps for the entire state of iowa through the iowa flood information system ifis http ifis iowafloodcenter org ifis main the map interface supports visual selection and flexible navigation for the domain of radars or watersheds of interest using the google maps api e g demir and krajewski 2013 this subsystem delivers a new rain rate map to ifis and offers an animation of these maps for the last 6 h this rain map contains a rain snow classification particularly useful in the winter season this classification uses environmental factors e g atmospheric layer thickness and surface temperature retrieved from the nwp analysis and the spatial coverage of snow is overlaid with the rain map as shown in fig 3 a as illustrated in fig 3 c the system also accumulates the rain maps over time and visualizes daily and cumulative rain maps for up to the last 14 days based to the following steps 1 today s cumulative rain map is generated by accumulating the temporal sampling error corrected rain map once the map is available e g every 5 min and is reset at midnight 2 before resetting today s rain map at midnight it is saved as a daily rain map and 3 the cumulative rain maps are generated by adding today s map to the aggregation of daily maps over a specified period e g up to 14 days for faster computation and visualization of cumulative rain maps the system maintains multi day aggregation files by adding up daily rain maps and updating them at midnight 3 algorithms in this section we provide details on scientific algorithms implemented in the data processing subsystem described in section 2 the processing algorithms are categorized into three groups 1 nwp analysis processing 2 individual radar data processing and 3 composite data processing because the system structure and algorithm elements have continuously evolved over time since their initial deployment we also discuss the changes and advancements of the algorithm components that aim to better represent atmospheric phenomena associated with precipitation microphysics 3 1 nwp processing 3 1 1 ml identification the ml is one of the most significant uncertainty factors in radar qpe because most precipitation estimators e g marshall and palmer 1948 fulton et al 1998 are based on liquid rain drop size distributions dsds as such estimation accuracy for solid and mixed precipitation within and above the ml tends to be relatively lower than that for liquid precipitation below the ml to identify and correct radar observations affected by the ml many approaches have examined the vertical structure of reflectivity e g fabry and zawadzki 1995 or enhanced dp capabilities e g baldini and gorgucci 2006 giangrande et al 2008 the ml identification module is a new procedure appended to our qpe system in accordance with the recent implementation of a polarimetric qpe algorithm known as specific attenuation seo et al 2020b which is valid only for liquid precipitation below the ml upon the receipt of hourly rap model analysis in ldm the module retrieves temperature soundings on the rap grids within our entire domain the module refers to a lookup table that assigns a horizontal rap grid to a corresponding radar grid and builds temperature profiles on the spherical coordinates within a radar domain the module then identifies the ml altitude and flags radar grids below the ml by checking if the temperature at the top of a radar beam at a given location is greater than 5 c this procedure is applied to all elevation angle data and was tested in seo et al 2020a 3 1 2 rain snow classification although many winter precipitation classification algorithms have used radar data e g park et al 2009 thompson et al 2014 we apply a rather simple scheme that does not depend on radar geometry occasionally the class designation obtained from radar only algorithms does not agree with the one observed on the ground this is because of the likely phase changes of precipitation along a falling path between a radar beam elevation and the ground in our system we use a classification approach based on the critical thickness e g keeter and cline 1991 heppner 1992 and surface temperature together as retrieved from hrrr model analysis with a 3 km horizontal grid the critical thickness is defined as the difference of geopotential height between 1000 and 850 mb fig 4 shows the relation between the two variables e g thickness and surface temperature and precipitation classes observed during the winter seasons in 2012 and 2013 we obtained the observed classes in fig 4 from automated surface observing system asos rain gauges see fig 1 which are equipped with two sensors to detect precipitation classes e g ramsay 1997 based on the preliminary analysis presented in fig 4 we selected two thresholds i e 3 c for surface temperature and 1310 m for the thickness and applies them to delineate regions with high likelihood of snow these regions are then overlaid with the instantaneous rain rate map as shown in fig 3 a we note that this snow information is used for qualitative purposes only e g map visualization for weather monitoring and is not transferred to quantitative i e hydrologic applications because fig 4 demonstrates some overlaps e g possible errors among rain freezing rain and snow in the regions near the two threshold values in section 5 we discuss this weakness and the probable improvement that could be achieved with a new approach 3 2 individual radar data processing 3 2 1 data quality control qc the qc modules encompass elimination of non meteorological radar returns and noise filtering of radar observations while it is comparatively easier to manage anomalous propagation ground clutter and biological returns using their polarimetric features e g ryzhkov and zrnic 1998 rico ramirez and cluckie 2008 the effects from wind farms wf have become a growing challenge e g vogt et al 2007 to eliminate the effects from wf and other non meteorological targets we previously had used the multiple thresholds of copular correlation coefficient  hv conditioned on the reflectivity strength seo et al 2015 however we found that an aggressive filter e g  hv 0 98 proposed to handle wf echoes occasionally removes some rain echoes as such the current qc module bypasses the aggressive filter and applies a new method instead which uses vertically integrated liquid water content vil to identify wf echoes seo et al 2020a the basis of this new idea was the fact that estimated vils for wf echoes tend to be much lower e g a weaker vertical extension than those for actual convective cells despite their strong reflectivity contamination unprocessed differential phase  dp data require unfolding and smoothing steps e g wang and chandrasekar 2009 because the data are rather noisy and are folded to a limited range e g 0 360 for wsr 88d the noise filtering module incorporates phase unfolding and smoothing routines used in the wsr 88d common operations and development environment code public package https www weather gov code88d the smoothing routines for  dp processing consist of averaging and median filters the averaging filter is also applied to horizontal reflectivity z differential reflectivity z dr and  hv along a radial direction the module also derives specific differential phase k dp from processed  dp in accordance with the code procedure 3 2 2 precipitation type and hydrometeor classification this module splits precipitation into convective and stratiform rain by estimating vil to define convective cores and gradually expanding the convective area from the cores to the adjacent grids based on a region growing method e g adams and bischof 1994 the vil estimation using 3d reflectivity and the ml information mitigates the uncertainty associated with the effects of the bright band and radar beam geometry near the radar site seo et al 2020a the convective stratiform classification was originally designed to apply separate rain rate estimators to different rain types as well as to correct the error arising from the non uniform vertical profile of reflectivity vpr for the stratiform regions e g bellon et al 2005 krajewski et al 2011b the correction module using a real time vpr is currently inactive because the default rain rate estimator has been switched from the reflectivity based method to the specific attenuation method the real time vpr correction approach is documented in seo et al 2011 we describe both estimators in the next subsection the module also performs hydrometeor classification for solid and mixed precipitation e g ice and dry wet snow this classification uses the polarimetric signatures of z z dr and  hv described in straka et al 2000 and ryzhkov et al 2005 combined with ml information hail is simply identified when reflectivity is greater than 53 dbz 3 2 3 rain rate estimation the module provides multiple rain rate r estimators including the new development of specific attenuation a 1 r z 0 017 z 0 714 2 r z 0 017 z 0 714 f o r c o n v e c t i v e r a i n 0 036 z 0 625 f o r s t r a t i f o r m r a i n 3 r z z d r 0 0067 z 0 927 z d r 3 43 4 r k d p 40 5 k d p 0 85 5 r a 4120 a 1 03 where the unit scale of z and z dr is decibels db in eqs 1 3 although the default rain rate estimator for our qpe system is currently set to specific attenuation using eq 5 such a selective option enables us to investigate the event dependent performance of the various estimators e g thurai et al 2017 while all the estimators presented in eqs 1 5 are valid only for pure rain below the ml the module selectively applies one of the estimators in eq 6 to solid and mixed precipitation within and above the ml e g istok et al 2009 6 wet snow 0 6 r z dry snow within the ml r z dry snow above the top of the ml 2 8 r z ice crystals 2 8 r z hail r k d p 29 0 k d p 0 77 in the case where the nwp analysis for the ml identification is missing we simply apply eq 1 to the entire radar domain we note that estimation of specific attenuation a is quite complicated e g ryzhkov et al 2014 wang et al 2019 and detailed procedures implemented in our qpe algorithm are provided in seo et al 2020b 3 2 4 hybrid scan after generating rain rate maps at all elevation angles e g 3d the hybrid scan module constructs a 2d rain map by vertically combining the 3d rain maps using a linear weighting scheme a non parametric kernel function e g log normal assigns weights to the elevation data at a given azimuth and range location depending on the vertical distance between the center of the data grid and a predefined constant altitude e g 1 5 km the total of weights assigned along a vertical column must be unity through the normalization of assigned weights and the angle data closer to the constant altitude are given greater weight this smoothing technique tends to mitigate the discontinuity often observed between the transition areas of elevation angles e g seo et al 2011 the hybrid scan approach tends to suppress significant ground clutter caused by the side lobe effect which occasionally appears in the lowest elevation angle data e g fulton et al 1998 3 2 5 temporal synchronization temporal synchronization of individual rain maps is vital for generating a composite rain product because the observation times and intervals among the radars shown in fig 1 differ depending on their local weather conditions to synchronize all irregular data map timestamps we define a 5 min nominal timestamp and generate new rain maps at those timestamps using the latest two consecutive rain maps see fig 2 our previous synchronization scheme interpolated the two consecutive maps by assigning weights determined by the time separation between the observed maps and the nominal time for which a new map is generated e g langston et al 2007 however we found two weaknesses in this scheme 1 the interpolation of the same rain system e g a squall line observed at different times yielded duplicate or spatially expanded systems even if the system moved from one place to another and 2 the timestamp of the interpolated map was always earlier than that of the latest map observed implying that the latency of a composite map was sometimes up to 15 min because the observed map is labelled by the moment that a radar starts scanning it already has a delay of 4 10 min to resolve these two issues with the interpolation scheme the current module executes simple tracking of the storm velocity vector using a cross correlation method e g fabry et al 1994 seo and krajewski 2015 this tracking requires grid transformation from radar spherical coordinates to geographic ones this geographic coordinate system with a resolution of quarter decimal minutes equivalent to approximately 0 5 km also provides a common reference grid for the spatial mosaic of individual rain maps to accelerate the tracking computation the module generates rain maps that are four times coarser and captures the main movement e g single vector returns to the original grid and finds the exact vector within an averaging range between the original and coarser scales the module then projects the estimated vector to the next two timestamps and produces new rain maps see fig 2 this extrapolation assumes that the estimated vector is valid within a projection time domain e g up to 10 min 3 3 composite data processing 3 3 1 spatial mosaic once the temporally synchronized rain maps are ready the qpe system manager takes over and feeds them into the spatial mosaic process every 5 min the mosaic module reads the geographic reference points e g upper left corner from the map header of all individual rain map files and places the maps onto the common grid for overlapping areas covered by multiple radars rainfall values are combined based on a linear weighting scheme using the distance and altitude between the sampling observation location and each involved radar site 7 w i exp r i 2 r 2 exp h i 2 h 2 where w i denotes an assigned weight for the i th radar and r and h are the distance and altitude both are in km between the radar sampling location and the radar sites r and h represent scale parameters and a greater smaller value for these parameters leads to a gradual rapid change of the weight values we set 100 and 2 km as default values for the distance and altitude scale parameters respectively the use of 100 km for the distance parameter smooths the sharp boundaries that occasionally appear in overlapping zones because of relative calibration biases e g seo et al 2013 keem et al 2019 between individual radars the calculated weights for a grid location in the overlapping zone are also normalized to sum up to 1 3 3 2 advection correction it has been recognized that rainfall accumulations using rain rate maps produced from radar s sporadic sampling often result in inaccurate representation e g discontinuous patterns of actual rain fields e g huebner et al 1986 liu and krajewski 1996 this temporal sampling error is often significant particularly as the storm velocity and the map spatial resolution become faster and higher respectively to mitigate this error in the composite rain map we apply an advection approach using the same vector tracking method as used in the time synchronization module the advection correction module linearly interpolates static storm locations detected at two consecutive maps and corrects for the missed rainfall accumulations the module takes two instantaneous composite rain maps with a 5 min interval calculates and projects a velocity vector and generates new intermediate rain maps with a 1 min interval within the 5 min domain seo and krajewski 2015 the module splits the entire spatial domain shown in fig 1 into six tiles for faster computation and fulfills the aforementioned advection procedure an average of the six composite maps e g two originals and four new ones within the 5 min domain is then delivered to the cumulative process for product visualization and to the hydrologic modeling component for streamflow prediction 4 product evaluation we assess the qpe products generated via the combination of algorithms described in section 3 for the three year period of 2016 2018 in two ways 1 quantitative comparison with ground reference i e rain gauge observations and 2 evaluation of hydrologic simulations driven by the qpe products using streamflow observations we generated the products using a qpe reproduction system seo et al 2019 that can readily retrieve the radar level ii data from amazon s cloud archive e g ansari et al 2017 with some substantial modifications to append newly developed processing modules we acquired rain gauge data from the national weather service nws cooperative observer program coop mosbacher et al 1989 network and streamflow data from the u s geological survey usgs the domain illustrated in fig 1 shows about 220 rain gauge and 140 streamflow stations the purpose of this evaluation is to demonstrate improvements achieved by our latest algorithms after the recent implementation of the specific attenuation method we also compare the performance of our qpe products with that of the national qpe products in the united states multi radar multi sensor mrms zhang et al 2016 and nws level iii digital precipitation rate dpr we note that the evaluation analyses are limited to the months of april through october because the quantitative estimation of winter precipitation both radar and ground based is still challenging 4 1 rain gauge evaluation in this section we use the daily coop data to evaluate four qpe products mrms nwsdp ifc r z and ifcdp r a dp denotes polarimetric estimates our ldm has been receiving a set of mrms qpe products e g radar only gauge corrected and gauge only in real time we use the hourly gauge corrected one in this evaluation the other three products are radar only estimates concerning nwsdp we obtained the dpr product for seven wsr 88d radars within the domain shown in fig 1 and applied our time synchronization and spatial mosaic procedures to generate a composite product as we did not apply our qc and correction modules in nwsdp processing the primary features of the nws polarimetric algorithm should be preserved in nwsdp ifc r z provides reflectivity based estimates generated by our former algorithm combination which does not contain algorithm modules such as noise filtering and ml identification as such we estimated ifc r z using only eq 1 for the entire radar domain the estimation procedure of ifcdp r a is primarily based on the specific attenuation method seo et al 2020b with the combination of the latest algorithm elements as presented in section 3 fig 5 illustrates the product maps of rain totals accumulated over the month of september 2016 in fig 5 we discover that ifc r z shows an underestimation tendency while the spatial rainfall structures of the other three products look quite similar it seems that ifcdp r a effectively manages the partial beam blockage effects which appear clearly in the southeast direction from the kfsd radar in mrms nwsdp and ifc r z this is because the specific attenuation method is known to be less sensitive to radar miscalibration partial beam blockage and the variability of dsds e g ryzhkov et al 2014 wang et al 2019 we observed many isolated spikes in nwsdp which correspond to wf in iowa and minnesota for exact wf locations see seo et al 2020a these spikes indicate that the nws algorithm has a qc issue and several spikes near the border between iowa and minnesota are also detected in mrms in figs 6 and 7 we present the quantitative evaluation results for the four products using the independent coop gauge data at daily and yearly april to october scales with three statistical metrics multiplicative bias b defined as a radar gauge ratio r g pearson correlation coefficient r and mean absolute error mae the term independent indicates that the data from the hydrometeorological automated data system e g kim et al 2009 included in the mrms gauge correction procedure were not involved in this evaluation as shown in fig 6 the dots representing mrms nwsdp and ifcdp r a tend to align along the one to one line with somewhat different degrees of scatter ifc r z shows the largest scatter as well as inconsistent bias changes from year to year ifcdp r a a radar only product shows performance e g accuracy quite comparable to mrms which contains a bias correction based on rain gauge observations nwsdp also provides good estimates except for an underestimation observed in 2018 and somewhat larger dispersion when compared to mrms and ifcdp r a the daily evaluation results represented as 2d histograms are provided in fig 7 the observed degrees of overall bias and dispersion for each qpe product shown in fig 6 are consistent with those displayed in fig 7 fig 7 also reveals that the four products are characterized by different uncertainty features conditioned on rainfall magnitude 4 2 hydrologic evaluation the hillslope link model hlm is a fully distributed hydrologic model that currently provides real time streamflow predictions for the entire iowa domain krajewski et al 2017 quintero et al 2020a b the hlm is based on landscape decomposition into hillslopes and channel links which enables the physical representation of runoff generation and water transport the hlm is calibration free its parameters are determined a priori therefore the model does not favor any specific forcing inputs further details regarding the hlm equations configuration and numerical solver are documented in earlier studies e g small et al 2013 quintero et al 2016 krajewski et al 2017 quintero et al 2020a we ran hlm using the hourly mrms ifc r z and ifcdp r a products as model forcing inputs and generated streamflow simulations at a 15 min time interval we excluded nwsdp from the model forcing stream because of its erroneous features e g strong spikes and unknown radial patterns shown in fig 5 the hlm simulations driven by different forcing products for each year started with the same initial condition e g model state on april 1 captured from the mrms hlm run for an antecedent period we did not calibrate model parameters because model calibration may obscure forcing dependent differences in streamflow generation for a hydrologic evaluation of the different qpe forcing products we use the performance metrics of kling gupta efficiency kge gupta et al 2009 pearson correlation coefficient r mae and root mean square error rmse kge describes the predictive power of hydrologic models and addresses deficiencies in nash sutcliffe efficiency nse nash and sutcliffe 1970 fig 8 shows a direct comparison of the effects on performance represented by kge derived from different forcing products in fig 8 hydrologic simulation results driven by ifcdp r a are compared with those driven by ifc r z and mrms at about 140 usgs stations in iowa for the years 2016 and 2017 ifcdp r a outperforms ifc r z and the performance of ifcdp r a looks comparable with that of the reference qpe product mrms similar to fig 6 although mrms seems slightly better than ifcdp r a for 2018 kge differences between the two products are likely not significant at a kge range of about 0 3 0 7 e g a cluster of dots slightly above the one to one line at the kge range to further examine the performance of mrms and ifcdp r a we compare the results from both forcing products regarding catchment scale e g upstream basin areas of the usgs stations in fig 9 on average the performance based on all four metrics tends to be better for both forcing products as the catchment scale becomes larger performance varies from year to year and no qpe product mrms or ifcdp r a is consistently better 5 summary and discussion in this paper we document the architecture algorithm configuration and estimation performance of our real time qpe system used primarily for flood prediction in iowa the ifc system is the only academia based large scale statewide real time radar rainfall monitoring system in the united states the system retrieves data from operational radars and nwp models and uses a variety of scientific algorithms to account for precipitation microphysics and uncertainties in radar remote sensing e g villarini and krajewski 2010 nwp analysis e g temperature and geopotential height acquired through ldm facilitates the ml identification and rain snow classification real time streaming data transferred from individual radars undergo several sequential processing procedures before they are combined into a composite product that covers the entire iowa domain fig 1 this sequential processing includes the radar data qc removal of non meteorological echoes and noise filtering precipitation type and hydrometeor classification rain rate estimation hybrid scan construction and time synchronization of individual rain maps the composite processing modules then spatially combine the synchronized rain maps and correct for the error arising from the radar s periodic temporal sampling finally the composite instantaneous and accumulated rain maps are visualized through ifis e g krajewski et al 2017 a flood information portal open to the general public these statewide rain maps are updated every 5 min implying that all individual and composite data processing procedures are done within a 5 min window the key feature of our qpe system is its flexible system structure e g modular configuration which allows us to improve its utility and performance by upgrading and appending algorithm elements one notable example is the recent implementation of new polarimetric algorithms while maintaining the main system structure and existing algorithm modules the new polarimetric algorithms include the precipitation estimators presented in eqs 3 5 and several subject elements such as noise filtering and ml identification procedures the specific attenuation method is a default estimator in our current operation and has improved the estimation accuracy significantly compared to the former one based on a conventional z r relation seo et al 2020b we have demonstrated that our composite specific attenuation estimates are more reliable and robust with regard to the variability of dsds and partial beam blockage and their performance e g accuracy is comparable with mrms which relies on a gauge correction fig 6 we have also verified that qpe improvements achieved through this new implementation lead to better simulation of streamflow fig 8 the next module element we plan to update or replace is the rain snow classification because the current threshold scheme has revealed many cases of misclassification see e g fig 4 data driven approaches e g machine learning will likely be useful to inspect the multi dimensional features of many more variables e g thickness temperature web bulb temperature relative humidity and radar observations relevant to winter precipitation processes bringing radar observations into this classification framework will also enable a rapid map update the rain snow boundary in the current one stays static for an hour based on the nwp s update cycle while we have demonstrated our algorithm s application to a limited spatial domain i e iowa the framework described in this paper is transferrable to other geographic areas and expandable to a larger e g regional scale the new estimator e g specific attenuation in our qpe system should be more robust through this transition because its estimates are less sensitive to the variability of dsds e g ryzhkov et al 2014 which could vary considerably in different geographic regions the required resources for the domain change include radar information e g id geographic location and spatial coverage and lookup tables for grid matching between the individual radar and nwp domains our experience with the successful application of prior developments e g krajewski et al 2013 to different regions of the united states e g lin et al 2010 yang et al 2014 confirms the system s flexibility with regard to domain change and expansion we are planning to expand the system to a much larger domain including neighboring states e g kansas missouri and nebraska for the purpose of flood mitigation in the mississippi and the missouri river basins we hope to report the results and discuss the challenging aspects of the domain expansion soon declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the iowa flood center at the university of iowa supported this work the evaluation of various qpe products was partially supported by the noaa hydrometeorology testbed hmt program within noaa oar office of weather and air quality under grant na17oar4590131 thanks to dr felipe quintero at the iowa flood center for providing the hlm simulation results driven by the different qpe products for the hydrologic evaluations in this project appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104791 
25987,with expanding human uses at sea the objective of maritime spatial planning msp to promote sustainable coexistence between marine uses becomes an increasingly challenging task in order to assess coexistence options both use use interactions and use environment interactions are important to explore tools for doing cumulative impact assessments cia on the environment provide a means for spatially exploring environmental impacts finding inspiration in such ecosystem based spatial use environment approaches while drawing on pairwise marine use compatibility knowledge from existing literature a spatial approach to model potential synergies and conflicts between marine uses through an expert based scoring system is presented and implemented in seanergy an arcmap based opensource toolbox a test based on baltic sea gis data demonstrates how seanergy supplements cia analyses with knowledge about potential use use synergies potential use use conflicts and their spatial extents useful for optimising the use of marine space in msp without putting too much cumulative pressure on the environment graphical abstract image 1 keywords coexistence use use interaction seanergy maritime spatial planning msp cumulative impact assessments cia spatial decision support tools dsts 1 introduction human activities at sea are continually expanding their use of marine space and marine resources a trend implemented in the european politic agenda with the blue growth strategy ec 2012 relatively new uses such as renewable energy aquaculture and growing cruise tourism compete with more traditional marine uses such as military fishing and shipping kannen 2014 due to the growing marine human activity level the pressures on marine ecosystems are increasing and the competition for marine space is growing coccoli et al 2018 in europe the increasing intensity of marine uses has resulted in the eu maritime spatial planning directive being implemented in 2014 ec 2014 putting marine spatial planning msp on the european agenda as a cross sectoral approach to iteratively and more systematically plan the use of oceans with increased pressures on oceans co location can be considered a necessary focus and the msp directive directly states as an important objective to promote sustainable coexistence between marine uses ec 2014 co locating compatible marine uses into coexistence in close spatial temporal proximity implies a willingness to increase synergies between marine uses when possible while separating more conflicting marine uses bonnevie et al 2019 at the one hand co location options are affected by use environment interactions marine uses might put too high pressures on the environment especially if one or more activities are concentrated and intensified in time and place through co location hansen 2019 applying an ecosystem based approach to msp and reaching a good environmental status of the world s oceans are stated as important goals in the marine strategy framework directive an important environmental pillar of european msp ec 2008 on the other hand the need for coexistence to optimise marine space underlines the importance of understanding use use interactions with the increased use of marine space marine uses increasingly interact with each other through their increasingly close spatial temporal proximity such use use interactions can constitute spatial temporal technical environmental and or user based links bonnevie et al 2019 they can be mostly negative conflicts or mostly positive synergies for example artificial reefs from wind farms can benefit aquaculture in multi use constellations that optimise the use of marine space but other marine uses might to a higher degree conflict with each other stuiver et al 2016 an antagonistic relation where one or more marine uses benefit on behalf of other marine uses is not a synergic relation but a conflicting one e g when sand extraction ships keep fishing boats away from mutually attractive seamounts klinger et al 2018 understanding the dynamics between human activities in close spatial temporal proximity can increase knowledge about synergies to be strengthened as well as knowledge about marine use conflicts to be avoided minimised in msp rempis et al 2018 to assist msp processes and decision making spatial decision support tools dsts can provide marine planners with spatial geographic data analyses map comparisons and spatial statistics pnarba et al 2017 many existing dsts e g symphony hammar et al 2020 mytilus hansen 2019 tools4msp menegon et al 2018a and ecoimpactmapper stock 2016 support a focus on use environment interactions through implementing the well known approach of cumulative impact assessments cia assessing cumulative impacts from marine uses on the environment the cia approach consists of three key elements originally defined by halpern et al 2008 1 spatial raster datasets of environmental pressures and or human uses 2 spatial raster datasets representing ecosystems or ecosystem environmental components and 3 relative sensitivity scores deduced from experts that describe how much each pressure impacts each ecosystem component the scenario based logic behind cia follows the dapsi w r m framework drivers activities pressures state changes impacts on welfare responses as measures elliot et al 2017 the cia main result is a cumulative impact map that both shows where the marine environment is more impacted by human uses and where it is less impacted by human uses often calculated based on an additive linear model hansen 2019 stock 2016 halpern et al 2008 with a cia map it is possible to only enable multi use and marine uses in close spatial temporal proximity where the cumulative impacts on the environment are deemed by planners and politicians to be not too high however not so much attention has been given by existing dsts to use use interactions bonnevie et al 2019 some tools do consider use use interactions but to a very limited degree for example the adriplan maritime use conflict muc tool counts spatial temporal overlaps of marine uses menegon et al 2018b however it does not consider whether and to which extent marine uses experience conflicts and or synergies but assume all overlaps to be conflicts bonnevie et al 2019 other examples are marxan with zones watts et al 2009 and a game theory approach by kyriazi et al 2017 that both include options to enable multi use zones as part of their focus on space allocation but they do not enable their users to explore multi use knowledge instead the users must be aware of relevant multi use constellations before using the tools bonnevie et al 2019 despite the lack of focus on use use interactions among existing dsts the concept of multi use has seen an increasing attention within academic msp research in the last decades przedrzymirska et al 2018 while terrestrial planning likewise has seen interests in space optimising concepts the concept of multi use has been particularly strong at sea where systematic planning is newer and where the depths of oceans and the quick movements of water provide two extra dimensions to consider than on land where planning is mostly surface based schupp et al 2019 multi use can be defined as the joint use of resources in close geographic proximity schupp et al 2019 various international projects within the last decades have put multi use on the marine spatial planning agenda the eu muses project can be acknowledged for its systematic review of the contribution to multi use knowledge from many previous international projects such as maribe mermaid orecca etc schultz zehden et al 2017 furthermore different msp literature has gathered knowledge on spatial compatibility between marine uses e g ehler and douvere 2009 and kannen 2014 spatial compatibility is the degree to which marine uses can spatially temporally overlap without causing negative impacts on each other kannen 2014 simple spatial compatibility knowledge about spatial temporal extents of marine uses enables more weak multi use constellations where marine uses only share links by being located on top of each other at different places in the water column or by occupying the same space at different times schupp et al 2019 bonnevie et al 2019 at the same time as multi use and spatial compatibility knowledge has seen an increasing focus conflicts between marine uses have still been investigated to facilitate understandings of which marine uses that need to be spatially temporally separated kyriazi et al 2017 the increasing amount of existing use use interaction knowledge can advantageously be utilised in msp by developing a new spatial dst that supplements the existing cia approach with a focus on use use interactions and thereby closes the gap of existing dsts and enables a holistic approach to co location this is the ambition behind seanergy a new approach and spatial decision support tool to map potential conflicts and potential synergies between marine uses to explore use use interactions the aim of this article is to present the methodology and novelty of seanergy with the new dst called seanergy used in combination with the already existing cia approach it becomes possible to spatially co locate marine uses with use use synergies and separate marine uses with too many use use and use environment conflicts first the methodology behind seanergy will be introduced it covers a short description of the toolbox its tools and their mathematical foundation and a stepwise guide to using seanergy including a reflection on expected tool users furthermore it presents relevant conflict synergy knowledge and gis data for the baltic sea area to which the seanergy approach has been applied second findings from the baltic sea illustrate a proof of concept of how seanergy supplements cia results to enable co location studies third strengths and limitations behind seanergy will be discussed before a conclusion is reached 2 materials and methods with seanergy we introduce a new spatial decision support tool dst that takes a holistic pairwise approach to potential conflicts and synergies between marine uses by updating the spatial compatibility details from existing pairwise marine use tables with multi use knowledge and spatial temporal attributes of marine uses as well as providing spatial maps to explore existing conflict synergy knowledge seanergy is introducing a spatial approach to explore potential use use interactions 2 1 seanergy implementation seanergy has been developed as an arcmap toolbox the arcmap toolbox design provides an already existing user interface that is intuitive for gis users and its built in options to implement explanatory and user responsive metadata to guide the users through the tool steps have been an advantage during the development of seanergy for easily presenting user options for each seanergy tool the toolbox consists of six tools in total two score tools two count tools a non spatial lookup tool as well as a monte carlo uncertainty tool the python scripts behind all seanergy tools are opensource free to download on github together with a read me guide that includes metadata 2 1 1 seanergy score tools the first of two score tools calculate score map is both inspired by and supplements the cia approach by halpern et al 2008 ecoimpactmapper stock 2016 and mytilus hansen 2019 instead of evaluating impacts from pressures on ecosystem components calculate score map evaluates the cumulative positive and negative impacts from spatially located marine uses on each other through using expert derived and pairwise use use conflict synergy scores the mathematical formula behind the calculate score map is the following f1 s x y u 1 1 n 1 u 2 u 1 1 n u u 1 x y u u 2 x y s u 1 u 2 the three capital letters s u u 1 and u u 2 in f1 represent raster map datasets the rasters u u 1 and u u 2 contain values describing the spatial presence of different unique marine uses u 1 and u 2 defined on the same scale for example a binary scale or a continuous scale from 0 no presence to 1 full presence for each raster cell indexed by x and y the total conflict synergy score s x y is the sum of all pairwise marine use score combinations the two sum signs range over all the unordered unique and unidentical use use combinations each pairwise score combination is the multiplication of the raster cell of the first use u u 1 x y and the raster cell of the second use u u 2 x y and the conflict synergy score s u 1 u 2 belonging to that pairwise combination of uses the result is a total conflict synergy score map for spatially located marine uses in a status quo or imagined planned scenario to allow some user flexibility the tool implementation of calculate score map enables the user to move away from the default setting of exploring all marine use combinations to focus on one specific marine use and its conflict synergy scores with all other marine uses this optional tool setting corresponds to fixing u u 1 x y to a specific marine use while still iterating over all other marine uses being u u 2 x y in the f1 formula the second score tool find synergy potential scores for a new marine use has a similar but different focus instead of focusing on potential synergy conflicts between already spatially located marine uses it focuses on one specific not yet spatially located new marine use u a and its potential synergy areas with already located marine uses u 1 which we iterate over by assuming that the new marine use u a in theory could be located in all locations where existing marine uses take place in practice its location will be limited by technical and operational requirements its total synergy score potential s  a x y where no conflicts are allowed to exist can be expressed with the following formula f2 s  a x y u 1 1 u 1 u a n u u 1 x y s p o s u 1 u a i f u 1 1 u 1 u a n u u 1 x y s n e g u 1 u a 0 1 i f u 1 1 u 1 u a n u u 1 x y s n e g u 1 u a 0 in formula f2 the conflict synergy score matrix s u 1 u 2 from f1 has been changed to s u 1 u a and divided into two different matrices one with only positive scores s pos u 1 u a and another with only negative scores s neg u 1 u a where the lack of respectively positive and negative scores for some marine use pairs will be represented by zeros this enables an if else condition where the synergy potential is only calculated for raster cells that contain no negative pairwise potential scores for the new marine use for raster cells that contain at least one negative pairwise potential score their synergy potential is per definition set to be 1 to represent areas unfit for locating the marine use in focus both score tools can be used ex ante in the sense that they can estimate potential synergies and potential conflicts of spatial scenarios before they take place however calculate score map implementing f1 is to a higher degree an ex post tool in that it shows all potential conflicts and potential synergies of a given scenario not focusing on changes from a status quo whereas find synergy potential scores for a new marine use implementing f2 is created only with an ex ante purpose since it focuses on showing locations where a new specific marine use would lead to synergies 2 1 2 seanergy count tools the two count tools calculate count map and find synergy potential counts for a new marine use implement specific instances of respectively formula f1 and f2 instead of enabling a user flexible span of conflict synergy score inputs these two tools only allow binary scores of 1 and 0 to count synergies and conflicts when certain conditions are met a score of 1 when the condition is met a score of 0 when the condition is not met counting instead of summing synergies and or conflicts allows the user to explore density based hotspots of synergies and conflicts without weighting the size of conflicts and synergies in the tool find synergy potential counts for a new marine use the condition is simple that all positive scores are defined to be 1 to count the synergy potential for a new marine use in focus the tool calculate count map on the other hand includes a long list of conditions that the tool user can choose between such as e g the basic choice to count all pairwise use combinations covering both conflicts and synergies to only count conflicts or to only count synergies a full list of conditions that calculate count map enables the tool user to choose between is available in appendix 1 together with mathematical formulas for converting f1 and f2 into count approaches 2 1 3 seanergy look up tool the tool conflict synergy matrix lookup is the only non spatial tool it provides a print statement of the conflict synergy matrix input for a user requested specific pairwise marine use combination enabling the user to explore the matrix inputs for specific pairwise marine use combinations 2 1 4 seanergy tool to test model robustness in order to increase model confidence it can be useful to supplement model results with an uncertainty analysis an uncertainty analysis explores how uncertainties of different inputs to a model lead to overall uncertainty in the results of the model lilburne and tarantola 2009 monte carlo simulations is a common technique in uncertainty and sensitivity analyses to iteratively explore how the outcomes of a model respond to smaller random changes in the model inputs for different runs of the model lilburne and tarantola 2009 applied to seanergy a monte carlo simulation can advantageously supplement maps that show model outputs of potential conflicts and synergies based on formula f1 with maps that imply the robustness of the synergy conflict patterns to uncertainty derived variations in the inputs often when expert based knowledge is used in models it can cause uncertainties related to confidence challenges knowledge gaps and expert disagreements menegon et al 2018a three uncertainty tests surrounding all pairwise conflict synergy score inputs for the tool calculate score maps have been developed in seanergy and implemented in a tool called monte carlo score map iteration all three tests evaluate the sensitivity of the total conflict synergy score map pattern to input variations sensitivity is here defined to be when the majority positive neutral negative sign of a monte carlo total score result raster cell after all monte carlo runs with test specific randomised input changes does not match the positive neutral negative sign of the corresponding total score raster cell without score input changes all three tests are based on the presumption that the pairwise inputted conflict synergy scores are on a scale from 3 to 3 with steps of 1 if the scoring scale is changed the tests should be updated correspondingly for further details on how to do this see the read me file in the tool package the first uncertainty test allows the input scores to change with a maximum of 1 or 1 to mirror the uncertainty case that experts could be in doubt about which synergy conflict category that a pairwise combination belongs to the second uncertainty test allows the relative importance between conflict synergy scores to change by letting all scores that are not scale start end scores of 3 3 to be a random ratio of the neighbouring score category the third uncertainty test is the combination of the first and second uncertainty tests 2 1 5 a stepwise guide to seanergy fig 1 places all six tools in a stepwise process of how to use seanergy and links the model steps to expected tool users for further introduction to the specific technical components of seanergy see appendix 2 as fig 1 highlights the pre processing step should be performed outside seanergy it consists of converting spatial marine use datasets into marine use raster datasets with similar cell size and extent and with raster values of 1 for fully present use and 0 for fully missing use the processing step consists of two parts a producing pairwise comparison rasters of marine uses through pairwise multiplication of all the unique marine uses and b using the pairwise comparison rasters to either produce pairwise score rasters that weights synergies and conflicts or to produce pairwise condition based binary score rasters that count synergies and conflicts when they satisfy a certain condition to increase transparency of the score knowledge applied in the processing step the non spatial tool conflict synergy matrix lookup can be used to browse and print the score inputs for specific marine use pairs the map exploration step visualises a final map consisting of the total sum of all the final pairwise rasters from the processing step the final map is either a total score raster or a total count raster and can be supplemented with optional statistical tables and extra tool output maps depending on flexible user choices regarding the five spatial seanergy tools the pre processing and processing steps of seanergy are rather technical for which reason they can be expected to be carried out by gis scientists working in collaboration with msp researchers msp practitioners this group of users can produce maps for the map result phase the map exploration phase is less technical for which reason stakeholders with an msp practitioner as facilitator could have the leading role in the map discussions and map explorations taking place in this last phase stakeholders could be local planners representatives from different maritime sectors and or citizens the feedback from these map result discussion sessions could cause the need for more processing rounds maybe even pre processing rounds if new data is introduced supporting the iterative trends of msp processes 2 2 data for the baltic sea in order to demonstrate how the seanergy tool package supports co location analyses and supplements cumulative impact assessments cia on the environment seanergy has been applied to analyse potential synergies and conflicts for the baltic sea area based on marine use data mostly from the helcom map and data service portal the baltic sea has been chosen as a first case study area for seanergy due to a high degree of available conflict synergy information for this region the baltic sea region covers eight northern european countries denmark sweden finland estonia latvia lithuania poland germany and one country not being an eu member russia a visual overview of the baltic sea with its bathymetry profile and surrounding countries is presented in fig 2 if the seanergy user wishes to use seanergy for another sea basin or marine area the read me file in the seanergy tool package describes how to replace the existing data and conflict synergy knowledge 2 2 1 gathering pairwise marine use synergy conflict inputs synergy draws on existing baltic sea conflict synergy knowledge it includes pairwise spatial compatibility matrices developed by the international baltic sea marine research projects baltic scope veidemane et al 2017 plan bothnia backer et al 2013 and partiseapate project 2014 it includes the number of conflicts and synergies mentioned by stakeholders between different marine use pairs in the partiseapate project ruskule et al 2014 and it includes expert knowledge about spatial and temporal attributes for marine uses in the baltic sea from the eu financed coexist project coexist eu project 2013 furthermore seanergy includes knowledge from the eu muses project about which marine use combinations experience potentials for multi use in the european oceans in particular in the baltic sea schultz zehden et al 2017 all this knowledge much of it already table based has been synthesised into a marine use pairwise matrix available in an excel file in the seanergy github package the synthesised matrix goes further than its individual literature inputs since it categorises all the pairwise knowledge into synergy conflict categories the synergy conflict categories are presented in fig 3 with a specific marine use pair example for each category the synergy conflict categories have been ranked by the authors of this article based on how conflicting and synergic each category is with a preliminary score from 3 mostly conflicting to 3 mostly synergic the score for each category is based on a primitive ranking of the categories by order and is thus not an actual expert based ranking of each pairwise combination the order of the marine uses in the pair is irrelevant to reflect the point that an antagonistic relation between two uses is still a conflict since one use then benefits on behalf of another use a neutral score of zero does not exist since use combinations with no other link than a close spatial proximity share the advantage of an optimised use of space through a positive location link bonnevie et al 2019 despite the choice of excluding zero as a score some of the pairwise combination matrix inputs are currently empty due to no available input related knowledge to close some of the knowledge gaps compatibility knowledge from outside of the baltic sea area has been entered into the synthesised matrix as well even though full weight has been given to baltic sea knowledge if available other seanergy compatibility knowledge sources are the following information from the swedish agency for marine and water management swam the swedish agency for marine and water management 2015 information from the aquaspace project in germany gimpel et al 2018 theoretical general information from kannen 2014 and unesco ehler and douvere 2009 and information from israel technion 2015 to standardise the marine use definitions that vary between the matrix input studies it has been chosen to use the general human use categories in table 2b of annex iii that was implemented into the msfd directive through an amendment to the original 2008 directive ec 2017 furthermore this article perceives human uses and activities as synonymous concepts similarly to menegon et al 2018a since this study focuses on the area relevant for marine spatial plans only marine human use categories have been considered excluding land based uses furthermore all the matrix input studies behind fig 3 include nature environmental protection in their marine use category examples for which reason such nature environmental protection categories have been added to the marine use list as supplements to the msfd marine use categories 2 2 2 marine use gis data marine use data primarily from the helcom map and data service portal 2 2 link to helcom map and data service portal http maps helcom fi website mapservice has been converted into raster data with the pre processing tool a simple binary approach has been used for the raster conversion and binary thresholds have been used for the very space consuming marine uses e g commercial fishing a detailed table of all raw data metadata and thresholds is provided in the read me file in the seanergy tool package as a template for the raster conversion a baltic sea raster has been used the baltic sea raster has been developed by using the raster template over the baltic sea region from the mytilus tool hansen 2019 and merging it with the ocean coverage of the helcom cia raster map 3 3 link to the helcom cia map metadata http metadata helcom fi geonetwork srv eng catalog search metadata 9477be37 94a9 4201 824a f079bc27d097 the resulting raster template is available in the seanergy tool package together with the pre processed baltic sea marine use raster data the raster template consists of raster cells with 1 km2 resolution making up an ocean surface of 436 375 km2 2 2 3 cumulative impact assessment gis data to visualise how seanergy enable co location analyses by supplementing cia analyses cia data processed by helcom and downloaded from the helcom map and data service portal is included in the result section 2 3 baltic sea results a proof of concept an overall seanergy analysis for existing baltic sea marine use combinations is presented followed by a seanergy analysis of synergy potentials for a new not yet located marine use 2 4 analysing co location options for status quo or for a scenario map a in fig 4 shows the spatial distribution of total potential use use conflict synergy scores for existing marine uses in the baltic sea created with seanergy the total positive score classes and the total negative score classes each apply a quantile classification method all raster cells that have returned as sensitive from 100 iterations of any of the three monte carlo tests have been coloured black in the figure the sensitive raster cells are particularly concentrated around the danish island bornholm and the finnish archipelago in these two areas the total conflict synergy scores are very sensitive to small changes in synergy conflict input category scores as well as to changes in the ranking scale for which reason the positive neutral negative trend of total conflict synergy scores cannot be trusted in these areas to compare the total potential use use conflict synergy distribution with the baltic sea cia distribution in order to assess potentials for successful co existence map b in fig 4 shows helcom s cia result classified after a quantile classification method a visual comparison between map a and map b gives the impression that much of the dark red area in map b with very high cumulative environment impacts overlap dark green areas with high potential total synergy scores in map a this trend is confirmed in graph a in fig 5 which shows how each cia class overlaps with each synergy conflict score class the highest total conflict synergy score class containing the positive total scores 4 58 75 has the highest overlap with the highest cia class containing index values 21 67 78 71 which illustrates a trend that high potential total synergies often correlate with high cumulative impacts on the environment however a certain amount of overlaps with total synergy areas do exist for the lowest cia class containing index values 2 16 4 86 since it has a noticeable amount of the total positive score classes 2 75 3 75 and 1 75 within such areas with both high synergy potentials and very low cumulative impacts on the environment marine uses might coexist peacefully without negatively impacting the environment too much map d and map e in fig 4 together visually illustrate where areas do exist that contain green coloured relatively good results for both seanergy total scores and helcom s cia map at once map d shows only robust non sensitive raster cells with a positive potential total conflict synergy score indicating areas with the highest potentials for total synergies map e shows helcom s cia result for the same raster cells as in map d whereof a certain percentage are also green in map e green areas in both these two maps illustrate good candidate areas for co existence between current marine uses such green areas typically do not contain too many marine use combinations which is shown in map f in fig 4 map f shows the count of unique marine use overlaps for the same raster cells as in map d and e map c in fig 4 shows total pairwise overlap counts for the whole baltic sea area compared with map a it gives the visual impression that sensitive raster cells seem to contain a medium high or high number of unique pairwise raster cell overlaps a pattern confirmed in the circle diagram in fig 5 which shows that all sensitive raster cells contain at least 3 pairwise overlaps compared with map b in fig 4 map c gives the visual impression that raster cells with a high number of unique pairwise overlaps often correlate with high cumulative impacts on the environment a pattern confirmed with graph b in fig 5 increasing the number of pairwise marine use overlaps thus in this analysis tend to increase the score sensitivity and the degree of the cumulative impacts on the environment 2 5 assessing use use synergy potentials for a new marine use that has no specific location map a in fig 6 shows areas with green colour where new diving activities might experience synergies with existing marine uses without experiencing any conflicts areas with any conflicts are shown with a red colour the darker the green colour the higher the potential total synergy score map b in fig 6 shows the number of individual marine uses that diving would entail synergies for diving has been deemed a good example to focus on here since it is the marine use that has the highest sum of total matrix input synergy scores with other marine uses in this baltic sea analysis graph c in fig 6 lists marine uses that experience synergies with diving according to the seanergy knowledge inputs furthermore it shows the total potential score summed over all raster cells and the raster cell distribution for each marine use the graph points out that artificial reefs areas for fish regeneration is the marine use which has the highest input synergy score with diving however commercial fishing has the highest total synergy score summed across all raster cells despite its much lower pairwise synergy score with diving since commercial fishing covers a very large area a graph such as this one that highlights potential use use synergies from locating new marine uses can be produced with seanergy statistics outputs 3 discussion as any spatial tool seanergy has strengths limitations and further development potentials 3 1 overall reflections 3 1 1 strengths as has been demonstrated seanergy advantageously enables marine planners and stakeholders to spatially explore potential synergies and conflicts between marine uses the results exemplify how to find synergy areas and conflict areas for existing marine uses in status quo but the same approach could be used to spatially point out synergy conflict patterns for marine uses in various marine spatial planning scenarios furthermore seanergy enables options to explore synergy potentials for new not yet located marine uses seanergy even provides a novel way to spatially assess co location options if it is used in combination with cia results combining seanergy with cia results can help ensure that marine uses are located to increase synergies and to minimise conflicts between marine uses while not having too high total impacts on the environment that would destroy ecosystem services or decrease their quality substantially thereby an optimal use of space can be achieved ideally benefitting all marine uses and ideally enabling more space for environmental protection 3 1 2 limitations and further development potentials seanergy currently does not include the cia approach in its tool package since the preliminary focus has been on developing a use use interaction approach whereas a use environment approach already is implemented in other spatial tools by combining the methodology behind seanergy more directly with cia tools e g mytilus hansen 2019 in the same software package in the future it could enable users to more easily combine the seanergy approach with the cia approach it is furthermore important to be aware of the limitations to the seanergy methodology some msp conflicts and synergies might be about planning processes and political institutional and individual interests moodie et al 2019 seanergy takes a general spatial temporal approach to conflicts and synergies leaving out non spatial planning derived and interest derived conflicts and synergies such non spatial conflicts and synergies are still highly of interest to work with in transboundary msp moodie et al 2019 but other methods than the gis based approach of seanergy are more suited to explore conflicts and synergies that do not have a spatial temporal proximity origin voinov et al 2018 3 2 reflections on the conflict synergy matrix knowledge 3 2 1 strengths the conflict synergy matrix implemented in seanergy for the baltic sea provides a systematic general overview over how conflicting and synergic different pairs of marine uses are perceived to be in the baltic sea area it is a novel attempt to expand the compatibility approach of the pairwise tables developed within many marine projects with conflict synergy knowledge an approach that could advantageously be carried out for other marine areas 3 2 2 limitations and further development potentials currently knowledge gaps exist for some marine use pairs in the conflict synergy matrix applied in seanergy for example the synthesised baltic sea knowledge unfortunately does not enable a specification as to whether the presented diving synergies apply to recreational or scientific professional diving however there are some indications that recreational diving has strong potentials in the baltic sea area the eu muses project has highlighted the multi use category of underwater cultural heritage and tourism and environmental protection which includes cultural heritage diving to be the multi use category with the highest potential in the baltic sea przedrzymirska et al 2018 the brackish water of the baltic sea has enabled many shipwrecks to continue being in a good condition for which reason many underwater parks that include diving activities already exist in many baltic sea countries schultz zehden et al 2017 the matrix could advantageously be updated with more details on conflicts and synergies to reduce pairwise knowledge gaps if sector experts and or sector representatives assembled across sectors to a list different conflicts and synergies as well as b rank the relative importance of them the descriptions and scores in the conflict synergy matrix could be further updated only synergies and conflicts with a clear spatial temporal proximity origin should be included in the process of listing conflicts and synergies to be inputted in seanergy more systematic marine use interaction information is needed about horizontal vertical temporal compatible and non compatible overlaps vertical neighbours temporal neighbours decreases and increases in space availability for existing and planned marine uses natural biofiltration zones around mussels and or seaweed aquaculture artificial reef zones contamination types and their link to other uses visibility links noise links infrastructure and or tool overlaps existing and planned security distance zones and the spatial temporal location of uses which share the same types of users bonnevie et al 2019 when getting sector experts to rank conflicts and synergies between marine uses across sectors it might be a good idea to introduce a no go status for some marine conflicts and synergies that are so severe they should never take place for example the aquaspace tool which has been developed to find and compare aquaculture candidate sites defines its most severe conflict category to be when aquaculture is not only conflicting with other marine uses but cannot be physically located near them in space gimpel et al 2018 implementing such a no go category would overrule any synergies that might otherwise be found to exist besides ranking severity of conflicts and synergies it is also important to gather information about the likelihood of such synergies and conflicts to happen rempis et al 2018 however the complexity of models increases with an increasing number of model parameters douglas smith et al 2020 for which reason it might be best to model synergy conflict likelihood as an independent model the likelihood is also linked to the temporal scale for when synergies and conflicts can be expected to be carried out in time marine uses might move deeper out at sea enabling new synergies e g when wind farms become floating stefanakou et al 2019 if the temporal uncertainty behind synergies and conflicts are not communicated it can lead to demotivation among sectoral representatives in planning situations in the baltic scope projects some planners were demotivated by the perceived tendency for some cross sectoral synergies such as combining wind farms with certain types of fishing to be purely theoretical and not real practical options moodie et al 2019 thus to avoid miscommunication it could be an advantage to extend the conflict synergy knowledge in the matrix with knowledge about the likelihood for conflicts and synergies to take place 3 3 reflections on the needed analysis detail level 3 3 1 strengths like cia tools seanergy provides options to gain an overview over spatially interesting patterns in larger areas at once while the detail level of the conflicts and synergies that seanergy describes is rather limited it is the whole purpose of seanergy to stay at an overall level to get an overview of which spatial areas and which marine use combinations need further more detailed exploration the msp directive emphasises the importance of cross country cooperation that ensures coherent planning for whole sea basins at once ec 2014 a requirement that seanergy helps facilitate 3 3 2 limitations and further development potentials the large variation in marine use categories between existing pairwise compatibility approaches indicates that varying opinions exist regarding what level of detail is needed for categorising different marine uses in seanergy it has been deemed appropriate to use general overall marine use definitions inspired by the msfd directive annex iii table 2b categories and natural environmental protection area categories regardless of whether a general or more specific marine use definition is used one needs to think about what aspects of the marine use should be covered by data for example having spatial data about fishing catches and or temporal knowledge about the presence of fishing boats is better than just knowing the spatial routes of the fishing boats since the former knowledge enables planners to prioritise where the most important fishing areas are as always the resolution and level of detail of spatial tools depend on the quality and availability of data moodie et al 2019 pnarba et al 2017 the resolution of some marine use data from helcom could be much better particularly for the mobile marine uses e g shipping commercial fishing benthic trawling etc in future versions of seanergy it might be better to provide a continuous classification of marine use presence between 0 and 100 such as the one used for marine pressure data in mytilus hansen 2019 for areas where limited spatial data exist public participatory gis methods ppgis can help gather a large amount of information with limited resources voinov et al 2018 ppgis might be needed in areas where no spatial data over marine uses exist to gather information on conflicts and synergies in these areas as a methodological supplement or maybe even replacement of seanergy in such data empty areas allowing the spatial synergy conflict approach to move from potential conflicts synergies to more actual perceived conflicts synergies to be included in msp it is an essential requirement for marine uses that they are spatially captured on maps since msp is an area based approach for example ppgis processes can help to point out and prioritise culturally and recreational significant areas giving space to marine uses not often included in msp gee et al 2017 in this study purely terrestrial uses have been excluded as was also the case in the research by halpern et al 2008 that originally defined the cia approach through a global study covering all oceans of the earth some uses are both marine and terrestrial such as harbours that are already included in the seanergy baltic sea analysis harbours are key access points in combining the marine sphere with the terrestrial one however it might be an advantage in the future to also include land based coastal marine uses to improve the land sea interactions rempis et al 2018 for example recreational coastal activities westerberg et al 2013 might experience conflict links and or synergy links with other human uses in the ocean 3 4 reflections on the technical implementation 3 4 1 strengths the adriplan muc tool has similarly to seanergy applied a spatial approach to conflict analysis menegon et al 2018b however the approach presented in seanergy goes further than the adriplan muc tool by including synergies and working directly with conflicts instead of using spatial temporal extents and overlaps as proxy for conflict degrees seanergy provides a descriptive read me file as well as metadata directly implemented in the tools themselves to iteratively guide the tool users furthermore seanergy offers the users much flexibility by applying five different synergy conflict tools with different conflict synergy options and with a combination of mandatory and optional settings seanergy also provides an uncertainty analysis douglas smith et al 2020 are urging uncertainty and sensitivity tools to demand a high quality for user accessibility user ease and user guides and to avoid too high a complexity that scares away potential tool users the uncertainty analysis implemented in seanergy is rather simple and specific in scope and thus attempts to follow the advice by douglas smith et al 2020 it aims at easily guiding the user through the uncertainty model steps requested by douglas smith et al 2020 of sampling the parameter space run the model in this case a monte carlo model and analyse results in this case creating maps showing overall score sensitivity while seanergy requires an arcmap license for the interface and tools to be used all python scripts behind the tools are opensource and freely available on github 3 4 2 limitations and further development potentials currently seanergy only considers use use links within the same raster cells a limitation shared with the adriplan muc tool menegon et al 2018b a technical improvement to seanergy could be to include synergy conflict neighbourhood links across raster cells if trawling takes place very close to a natura 2000 area it might for example still cause negative impacts to the nature protected area even though the two marine uses do not overlap in the same raster cell allowing conflicts and synergies to decrease with distance models could be an improvement to seanergy in future development for example the cia methodology by tools4msp include a spatial distance model in their cumulative impacts model menegon et al 2018a furthermore an actual sensitivity analysis has not been implemented in seanergy at the current state a sensitivity analysis apportions the overall uncertainty of a model to different model inputs douglas smith et al 2020 lilburne and tarantola 2009 a sensitivity analysis could be implemented at a later development state of seanergy lastly improving the calculation time of seanergy could be an advantage to enable seanergy to be used directly as part of in situ planning situations with stakeholders currently seanergy is better to use before stakeholder meetings to have the maps ready for co location discussions already when the meetings begin since the running time is app 10 min for all marine use combinations or even an hour for the uncertainty analysis if only one marine use is in focus however the running time is currently only a few minutes for which reason seanergy could currently be used in situ in meetings that focus on one sector and its relation to other sectors one way to improve the calculation time could be to implement the python code in a new opensource fast tool package which would be independent from an arcmap license and which could from its start be combined with a cia approach for example the cia tool mytilus requires no commercial software licenses to run and offers fast calculations due to it being developed in delphi 10 1 a faster programming environment than python hansen 2019 4 conclusion a need exists for spatial decision support tools to model co location options between marine uses based not only on environmental capacity but also on ambitions to increase synergies and decrease conflicts between marine uses in close spatial temporal proximity managing use use interactions by increasing use use synergies and decreasing use use conflicts can secure an optimised use of marine space potentially enabling more area for environmental protection in increasingly crowded seas by synthesising available compatibility conflict multi use and synergy knowledge into a pairwise matrix and providing the matrix with a spatial dimension this research has introduced a novel approach implemented in the arcmap based toolbox seanergy to assist planners in managing marine use use interactions seanergy consists of six tools two score tools and two count tools produce potential synergy conflict maps based on raster based marine use data as well as a pairwise conflict synergy knowledge matrix they provide a high degree of user flexibility in that they can both focus on one specific marine use smaller subsets of marine use combinations or they can explore spatial patterns for all marine use combinations additionally an uncertainty analysis is provided which tests the sensitivity of positive and negative spatial patterns in the total synergy conflict score maps regarding specific modelled input uncertainty in the expert based score inputs lastly a non spatial tool to browse the matrix content is provided to support a transparent overview of the inputted conflict synergy knowledge the high degree of user flexibility of seanergy tools supports an iterative maritime spatial planning msp approach as demonstrated with a test for the baltic sea seanergy enables planners and other msp stakeholders to consider a holistic cross sectoral spatial approach to the planning of different marine uses such an approach can advantageously support the transboundary scope provided in ongoing cross country projects in the baltic sea in future developments of seanergy the toolbox can advantageously be updated with e g more thorough matrix knowledge as well as introducing a synergy conflict distance model to allow synergy conflict calculations across raster cells however seanergy as has been demonstrated already provides an important supplement to cumulative impact assessments cia on the environment marine uses both contribute to and depend on marine ecosystem services by combining the two methods of seanergy and cia tools co location options can more fully be explored by marine spatial planners software availability name of software seanergy developer ida maria bonnevie e mail idarei plan aau dk year first available 2020 program specifications an arcmap based toolbox called seanergy with tools developed in python for spatially locating potential synergies and conflicts between marine uses based on a pairwise comparison matrix metadata is located in the tool package software and license dependencies the full program depends on a windows 10 system with esri arcmap installed including the arcmap extension spatial analyst and including the python modules arcpy os pandas numpy collections time datetime the program has been developed for the versions arcmap 10 7 and python 2 7 the script tool interface depends on an esri arcmap license including the extension spatial analyst however the python code is open source and the methodology can thus freely be implemented into own programs with credits to the author of the approach program size 4 07 mb available here https github com idambonnevie seanergy git with a readme file a pre processing tool is available here https github com idambonnevie seanergy preprocessing git declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper declarations of interest none acknowledgements this research is carried out within the project baltic sea maritime spatial planning for sustainable ecosystem services bonus basmati which has received funding from bonus art 185 call number call 2015 77 funded jointly by the eu innovation fund denmark swedish research council formas academy of finland latvian ministry of education and science and forschungszentrum julich gmbh germany appendix 1 converting seanergy score formulas into seanergy count formulas if the score s u 1 u 2 in formula f1 is defined to be a binary case of either 1 or 0 1 if a condition con is met 0 if the condition is not met the formula can be used to count the number of synergies and conflicts instead of weighting the conflict synergy degrees formula a1 f1 shows this specific instance of f1 which is implemented in the tool calculate score map a1 f1 s con x y u 1 1 n 1 u 2 u 1 1 n u u 1 x y u u 2 x y s con 1 u 1 u 2 the binary score of either 1 or 0 based on whether the condition con is met is represented as s con 1 u 1 u 2 instances of conditions con implemented in the tool calculate score map for the user to choose between are a all pairwise use combinations b pairwise use combinations that have a positive or negative conflict synergy score c pairwise use combinations belonging to a specific synergy conflict category d all pairwise use combinations or only the combinations with a conflict synergy score that include a specific use e pairwise use combinations that are mobile f pairwise use combinations with an overlap in spatial vertical location g pairwise use combinations with different spatial vertical locations h pairwise use combinations that could be part of a multi use constellation according to the muses project just as a1 f1 is a specific instance of f1 a specific instance a1 f2 of f2 can be deduced in a1 f2 the scores in s p o s u 1 u a in f2 are binarily defined to always be 1 when they are positive and 0 when they are not positive to count the number of synergies instead of weighting their synergy size a1 f2 s  pos 1 a x y u 1 1 u 1 u a n u u 1 x y s p o s 1 u 1 u a i f u 1 1 u 1 u a n u u 1 x y s n e g u 1 u a 0 1 i f u 1 1 u 1 u a n u u 1 x y s n e g u 1 u a 0 a1 f2 is implemented in the tool find synergy potential counts for a new marine use appendix 2 technical components of seanergy besides the toolbox itself and its six python scripts a script exists for each tool besides four function scripts the seanergy tool package exists of a copyright statement a read me file a python module folder containing python modules that might need manual instalment in arcmap for the toolbox to work a data folder with pre processed raster data for different marine uses an extra input folder with a marine area raster and optional symbology files to symbolise seanergy outputs and three excel sheets one with the pairwise matrix content and scores one with spatial temporal attributes for each marine use and one with marine use categories linked to raster names a visual overview over the technical components are presented in figure a2 1 running time of tools typically spans from 20 s to 14 min depending on the parameter settings except the uncertainty tool which takes approximately 1 h to run for 100 iterations fig a2 1 technical components within the seanergy tool package fig a2 1 to illustrate an example of a seanergy tool user interface and the high degree of flexibility in the tool setting choices figure a2 2 presents the user interface of the tool calculate score map mandatory inputs required for this tool to run include the folder with pre processed marine use data the excel file with marine use categories linked to the marine use raster names the pairwise conflict synergy score matrix whether all inputs scores will be used in the calculation default or only positive or negative scores will be used a name for the total score raster output and the ocean raster to define the analysis area optional extra inputs consist of a symbology file to style the total score raster output a symbology file to style the polygons with scores for specific marine use pairs that as a default setting are outputted as extra information below the mandatory total score raster output 1 3 excel tables outputting statistical information about the score distribution in the matrix or map and options to limit the number of requested polygons with scores for specific marine use pairs to only include that number of the pairs selecting only the pairs with respectively the highest and the lowest scores across the map this number of requested polygons could also be zero if the user does not want to explore individual marine use pairs as a last option it is possible to calculate the total score maps only for one specific inputted marine use e g shipping as in figure a2 2 to see where that marine use potentially experiences conflicts and synergies with other marine uses a full list of user interfaces is available in the read me file in the tool package and all tools are supported with built in metadata during use information about the separate pre processing toolbox can be found in the read me file in the pre processing tool package available on github fig a2 2 user interface of the tool calculate score map fig a2 2 
25987,with expanding human uses at sea the objective of maritime spatial planning msp to promote sustainable coexistence between marine uses becomes an increasingly challenging task in order to assess coexistence options both use use interactions and use environment interactions are important to explore tools for doing cumulative impact assessments cia on the environment provide a means for spatially exploring environmental impacts finding inspiration in such ecosystem based spatial use environment approaches while drawing on pairwise marine use compatibility knowledge from existing literature a spatial approach to model potential synergies and conflicts between marine uses through an expert based scoring system is presented and implemented in seanergy an arcmap based opensource toolbox a test based on baltic sea gis data demonstrates how seanergy supplements cia analyses with knowledge about potential use use synergies potential use use conflicts and their spatial extents useful for optimising the use of marine space in msp without putting too much cumulative pressure on the environment graphical abstract image 1 keywords coexistence use use interaction seanergy maritime spatial planning msp cumulative impact assessments cia spatial decision support tools dsts 1 introduction human activities at sea are continually expanding their use of marine space and marine resources a trend implemented in the european politic agenda with the blue growth strategy ec 2012 relatively new uses such as renewable energy aquaculture and growing cruise tourism compete with more traditional marine uses such as military fishing and shipping kannen 2014 due to the growing marine human activity level the pressures on marine ecosystems are increasing and the competition for marine space is growing coccoli et al 2018 in europe the increasing intensity of marine uses has resulted in the eu maritime spatial planning directive being implemented in 2014 ec 2014 putting marine spatial planning msp on the european agenda as a cross sectoral approach to iteratively and more systematically plan the use of oceans with increased pressures on oceans co location can be considered a necessary focus and the msp directive directly states as an important objective to promote sustainable coexistence between marine uses ec 2014 co locating compatible marine uses into coexistence in close spatial temporal proximity implies a willingness to increase synergies between marine uses when possible while separating more conflicting marine uses bonnevie et al 2019 at the one hand co location options are affected by use environment interactions marine uses might put too high pressures on the environment especially if one or more activities are concentrated and intensified in time and place through co location hansen 2019 applying an ecosystem based approach to msp and reaching a good environmental status of the world s oceans are stated as important goals in the marine strategy framework directive an important environmental pillar of european msp ec 2008 on the other hand the need for coexistence to optimise marine space underlines the importance of understanding use use interactions with the increased use of marine space marine uses increasingly interact with each other through their increasingly close spatial temporal proximity such use use interactions can constitute spatial temporal technical environmental and or user based links bonnevie et al 2019 they can be mostly negative conflicts or mostly positive synergies for example artificial reefs from wind farms can benefit aquaculture in multi use constellations that optimise the use of marine space but other marine uses might to a higher degree conflict with each other stuiver et al 2016 an antagonistic relation where one or more marine uses benefit on behalf of other marine uses is not a synergic relation but a conflicting one e g when sand extraction ships keep fishing boats away from mutually attractive seamounts klinger et al 2018 understanding the dynamics between human activities in close spatial temporal proximity can increase knowledge about synergies to be strengthened as well as knowledge about marine use conflicts to be avoided minimised in msp rempis et al 2018 to assist msp processes and decision making spatial decision support tools dsts can provide marine planners with spatial geographic data analyses map comparisons and spatial statistics pnarba et al 2017 many existing dsts e g symphony hammar et al 2020 mytilus hansen 2019 tools4msp menegon et al 2018a and ecoimpactmapper stock 2016 support a focus on use environment interactions through implementing the well known approach of cumulative impact assessments cia assessing cumulative impacts from marine uses on the environment the cia approach consists of three key elements originally defined by halpern et al 2008 1 spatial raster datasets of environmental pressures and or human uses 2 spatial raster datasets representing ecosystems or ecosystem environmental components and 3 relative sensitivity scores deduced from experts that describe how much each pressure impacts each ecosystem component the scenario based logic behind cia follows the dapsi w r m framework drivers activities pressures state changes impacts on welfare responses as measures elliot et al 2017 the cia main result is a cumulative impact map that both shows where the marine environment is more impacted by human uses and where it is less impacted by human uses often calculated based on an additive linear model hansen 2019 stock 2016 halpern et al 2008 with a cia map it is possible to only enable multi use and marine uses in close spatial temporal proximity where the cumulative impacts on the environment are deemed by planners and politicians to be not too high however not so much attention has been given by existing dsts to use use interactions bonnevie et al 2019 some tools do consider use use interactions but to a very limited degree for example the adriplan maritime use conflict muc tool counts spatial temporal overlaps of marine uses menegon et al 2018b however it does not consider whether and to which extent marine uses experience conflicts and or synergies but assume all overlaps to be conflicts bonnevie et al 2019 other examples are marxan with zones watts et al 2009 and a game theory approach by kyriazi et al 2017 that both include options to enable multi use zones as part of their focus on space allocation but they do not enable their users to explore multi use knowledge instead the users must be aware of relevant multi use constellations before using the tools bonnevie et al 2019 despite the lack of focus on use use interactions among existing dsts the concept of multi use has seen an increasing attention within academic msp research in the last decades przedrzymirska et al 2018 while terrestrial planning likewise has seen interests in space optimising concepts the concept of multi use has been particularly strong at sea where systematic planning is newer and where the depths of oceans and the quick movements of water provide two extra dimensions to consider than on land where planning is mostly surface based schupp et al 2019 multi use can be defined as the joint use of resources in close geographic proximity schupp et al 2019 various international projects within the last decades have put multi use on the marine spatial planning agenda the eu muses project can be acknowledged for its systematic review of the contribution to multi use knowledge from many previous international projects such as maribe mermaid orecca etc schultz zehden et al 2017 furthermore different msp literature has gathered knowledge on spatial compatibility between marine uses e g ehler and douvere 2009 and kannen 2014 spatial compatibility is the degree to which marine uses can spatially temporally overlap without causing negative impacts on each other kannen 2014 simple spatial compatibility knowledge about spatial temporal extents of marine uses enables more weak multi use constellations where marine uses only share links by being located on top of each other at different places in the water column or by occupying the same space at different times schupp et al 2019 bonnevie et al 2019 at the same time as multi use and spatial compatibility knowledge has seen an increasing focus conflicts between marine uses have still been investigated to facilitate understandings of which marine uses that need to be spatially temporally separated kyriazi et al 2017 the increasing amount of existing use use interaction knowledge can advantageously be utilised in msp by developing a new spatial dst that supplements the existing cia approach with a focus on use use interactions and thereby closes the gap of existing dsts and enables a holistic approach to co location this is the ambition behind seanergy a new approach and spatial decision support tool to map potential conflicts and potential synergies between marine uses to explore use use interactions the aim of this article is to present the methodology and novelty of seanergy with the new dst called seanergy used in combination with the already existing cia approach it becomes possible to spatially co locate marine uses with use use synergies and separate marine uses with too many use use and use environment conflicts first the methodology behind seanergy will be introduced it covers a short description of the toolbox its tools and their mathematical foundation and a stepwise guide to using seanergy including a reflection on expected tool users furthermore it presents relevant conflict synergy knowledge and gis data for the baltic sea area to which the seanergy approach has been applied second findings from the baltic sea illustrate a proof of concept of how seanergy supplements cia results to enable co location studies third strengths and limitations behind seanergy will be discussed before a conclusion is reached 2 materials and methods with seanergy we introduce a new spatial decision support tool dst that takes a holistic pairwise approach to potential conflicts and synergies between marine uses by updating the spatial compatibility details from existing pairwise marine use tables with multi use knowledge and spatial temporal attributes of marine uses as well as providing spatial maps to explore existing conflict synergy knowledge seanergy is introducing a spatial approach to explore potential use use interactions 2 1 seanergy implementation seanergy has been developed as an arcmap toolbox the arcmap toolbox design provides an already existing user interface that is intuitive for gis users and its built in options to implement explanatory and user responsive metadata to guide the users through the tool steps have been an advantage during the development of seanergy for easily presenting user options for each seanergy tool the toolbox consists of six tools in total two score tools two count tools a non spatial lookup tool as well as a monte carlo uncertainty tool the python scripts behind all seanergy tools are opensource free to download on github together with a read me guide that includes metadata 2 1 1 seanergy score tools the first of two score tools calculate score map is both inspired by and supplements the cia approach by halpern et al 2008 ecoimpactmapper stock 2016 and mytilus hansen 2019 instead of evaluating impacts from pressures on ecosystem components calculate score map evaluates the cumulative positive and negative impacts from spatially located marine uses on each other through using expert derived and pairwise use use conflict synergy scores the mathematical formula behind the calculate score map is the following f1 s x y u 1 1 n 1 u 2 u 1 1 n u u 1 x y u u 2 x y s u 1 u 2 the three capital letters s u u 1 and u u 2 in f1 represent raster map datasets the rasters u u 1 and u u 2 contain values describing the spatial presence of different unique marine uses u 1 and u 2 defined on the same scale for example a binary scale or a continuous scale from 0 no presence to 1 full presence for each raster cell indexed by x and y the total conflict synergy score s x y is the sum of all pairwise marine use score combinations the two sum signs range over all the unordered unique and unidentical use use combinations each pairwise score combination is the multiplication of the raster cell of the first use u u 1 x y and the raster cell of the second use u u 2 x y and the conflict synergy score s u 1 u 2 belonging to that pairwise combination of uses the result is a total conflict synergy score map for spatially located marine uses in a status quo or imagined planned scenario to allow some user flexibility the tool implementation of calculate score map enables the user to move away from the default setting of exploring all marine use combinations to focus on one specific marine use and its conflict synergy scores with all other marine uses this optional tool setting corresponds to fixing u u 1 x y to a specific marine use while still iterating over all other marine uses being u u 2 x y in the f1 formula the second score tool find synergy potential scores for a new marine use has a similar but different focus instead of focusing on potential synergy conflicts between already spatially located marine uses it focuses on one specific not yet spatially located new marine use u a and its potential synergy areas with already located marine uses u 1 which we iterate over by assuming that the new marine use u a in theory could be located in all locations where existing marine uses take place in practice its location will be limited by technical and operational requirements its total synergy score potential s  a x y where no conflicts are allowed to exist can be expressed with the following formula f2 s  a x y u 1 1 u 1 u a n u u 1 x y s p o s u 1 u a i f u 1 1 u 1 u a n u u 1 x y s n e g u 1 u a 0 1 i f u 1 1 u 1 u a n u u 1 x y s n e g u 1 u a 0 in formula f2 the conflict synergy score matrix s u 1 u 2 from f1 has been changed to s u 1 u a and divided into two different matrices one with only positive scores s pos u 1 u a and another with only negative scores s neg u 1 u a where the lack of respectively positive and negative scores for some marine use pairs will be represented by zeros this enables an if else condition where the synergy potential is only calculated for raster cells that contain no negative pairwise potential scores for the new marine use for raster cells that contain at least one negative pairwise potential score their synergy potential is per definition set to be 1 to represent areas unfit for locating the marine use in focus both score tools can be used ex ante in the sense that they can estimate potential synergies and potential conflicts of spatial scenarios before they take place however calculate score map implementing f1 is to a higher degree an ex post tool in that it shows all potential conflicts and potential synergies of a given scenario not focusing on changes from a status quo whereas find synergy potential scores for a new marine use implementing f2 is created only with an ex ante purpose since it focuses on showing locations where a new specific marine use would lead to synergies 2 1 2 seanergy count tools the two count tools calculate count map and find synergy potential counts for a new marine use implement specific instances of respectively formula f1 and f2 instead of enabling a user flexible span of conflict synergy score inputs these two tools only allow binary scores of 1 and 0 to count synergies and conflicts when certain conditions are met a score of 1 when the condition is met a score of 0 when the condition is not met counting instead of summing synergies and or conflicts allows the user to explore density based hotspots of synergies and conflicts without weighting the size of conflicts and synergies in the tool find synergy potential counts for a new marine use the condition is simple that all positive scores are defined to be 1 to count the synergy potential for a new marine use in focus the tool calculate count map on the other hand includes a long list of conditions that the tool user can choose between such as e g the basic choice to count all pairwise use combinations covering both conflicts and synergies to only count conflicts or to only count synergies a full list of conditions that calculate count map enables the tool user to choose between is available in appendix 1 together with mathematical formulas for converting f1 and f2 into count approaches 2 1 3 seanergy look up tool the tool conflict synergy matrix lookup is the only non spatial tool it provides a print statement of the conflict synergy matrix input for a user requested specific pairwise marine use combination enabling the user to explore the matrix inputs for specific pairwise marine use combinations 2 1 4 seanergy tool to test model robustness in order to increase model confidence it can be useful to supplement model results with an uncertainty analysis an uncertainty analysis explores how uncertainties of different inputs to a model lead to overall uncertainty in the results of the model lilburne and tarantola 2009 monte carlo simulations is a common technique in uncertainty and sensitivity analyses to iteratively explore how the outcomes of a model respond to smaller random changes in the model inputs for different runs of the model lilburne and tarantola 2009 applied to seanergy a monte carlo simulation can advantageously supplement maps that show model outputs of potential conflicts and synergies based on formula f1 with maps that imply the robustness of the synergy conflict patterns to uncertainty derived variations in the inputs often when expert based knowledge is used in models it can cause uncertainties related to confidence challenges knowledge gaps and expert disagreements menegon et al 2018a three uncertainty tests surrounding all pairwise conflict synergy score inputs for the tool calculate score maps have been developed in seanergy and implemented in a tool called monte carlo score map iteration all three tests evaluate the sensitivity of the total conflict synergy score map pattern to input variations sensitivity is here defined to be when the majority positive neutral negative sign of a monte carlo total score result raster cell after all monte carlo runs with test specific randomised input changes does not match the positive neutral negative sign of the corresponding total score raster cell without score input changes all three tests are based on the presumption that the pairwise inputted conflict synergy scores are on a scale from 3 to 3 with steps of 1 if the scoring scale is changed the tests should be updated correspondingly for further details on how to do this see the read me file in the tool package the first uncertainty test allows the input scores to change with a maximum of 1 or 1 to mirror the uncertainty case that experts could be in doubt about which synergy conflict category that a pairwise combination belongs to the second uncertainty test allows the relative importance between conflict synergy scores to change by letting all scores that are not scale start end scores of 3 3 to be a random ratio of the neighbouring score category the third uncertainty test is the combination of the first and second uncertainty tests 2 1 5 a stepwise guide to seanergy fig 1 places all six tools in a stepwise process of how to use seanergy and links the model steps to expected tool users for further introduction to the specific technical components of seanergy see appendix 2 as fig 1 highlights the pre processing step should be performed outside seanergy it consists of converting spatial marine use datasets into marine use raster datasets with similar cell size and extent and with raster values of 1 for fully present use and 0 for fully missing use the processing step consists of two parts a producing pairwise comparison rasters of marine uses through pairwise multiplication of all the unique marine uses and b using the pairwise comparison rasters to either produce pairwise score rasters that weights synergies and conflicts or to produce pairwise condition based binary score rasters that count synergies and conflicts when they satisfy a certain condition to increase transparency of the score knowledge applied in the processing step the non spatial tool conflict synergy matrix lookup can be used to browse and print the score inputs for specific marine use pairs the map exploration step visualises a final map consisting of the total sum of all the final pairwise rasters from the processing step the final map is either a total score raster or a total count raster and can be supplemented with optional statistical tables and extra tool output maps depending on flexible user choices regarding the five spatial seanergy tools the pre processing and processing steps of seanergy are rather technical for which reason they can be expected to be carried out by gis scientists working in collaboration with msp researchers msp practitioners this group of users can produce maps for the map result phase the map exploration phase is less technical for which reason stakeholders with an msp practitioner as facilitator could have the leading role in the map discussions and map explorations taking place in this last phase stakeholders could be local planners representatives from different maritime sectors and or citizens the feedback from these map result discussion sessions could cause the need for more processing rounds maybe even pre processing rounds if new data is introduced supporting the iterative trends of msp processes 2 2 data for the baltic sea in order to demonstrate how the seanergy tool package supports co location analyses and supplements cumulative impact assessments cia on the environment seanergy has been applied to analyse potential synergies and conflicts for the baltic sea area based on marine use data mostly from the helcom map and data service portal the baltic sea has been chosen as a first case study area for seanergy due to a high degree of available conflict synergy information for this region the baltic sea region covers eight northern european countries denmark sweden finland estonia latvia lithuania poland germany and one country not being an eu member russia a visual overview of the baltic sea with its bathymetry profile and surrounding countries is presented in fig 2 if the seanergy user wishes to use seanergy for another sea basin or marine area the read me file in the seanergy tool package describes how to replace the existing data and conflict synergy knowledge 2 2 1 gathering pairwise marine use synergy conflict inputs synergy draws on existing baltic sea conflict synergy knowledge it includes pairwise spatial compatibility matrices developed by the international baltic sea marine research projects baltic scope veidemane et al 2017 plan bothnia backer et al 2013 and partiseapate project 2014 it includes the number of conflicts and synergies mentioned by stakeholders between different marine use pairs in the partiseapate project ruskule et al 2014 and it includes expert knowledge about spatial and temporal attributes for marine uses in the baltic sea from the eu financed coexist project coexist eu project 2013 furthermore seanergy includes knowledge from the eu muses project about which marine use combinations experience potentials for multi use in the european oceans in particular in the baltic sea schultz zehden et al 2017 all this knowledge much of it already table based has been synthesised into a marine use pairwise matrix available in an excel file in the seanergy github package the synthesised matrix goes further than its individual literature inputs since it categorises all the pairwise knowledge into synergy conflict categories the synergy conflict categories are presented in fig 3 with a specific marine use pair example for each category the synergy conflict categories have been ranked by the authors of this article based on how conflicting and synergic each category is with a preliminary score from 3 mostly conflicting to 3 mostly synergic the score for each category is based on a primitive ranking of the categories by order and is thus not an actual expert based ranking of each pairwise combination the order of the marine uses in the pair is irrelevant to reflect the point that an antagonistic relation between two uses is still a conflict since one use then benefits on behalf of another use a neutral score of zero does not exist since use combinations with no other link than a close spatial proximity share the advantage of an optimised use of space through a positive location link bonnevie et al 2019 despite the choice of excluding zero as a score some of the pairwise combination matrix inputs are currently empty due to no available input related knowledge to close some of the knowledge gaps compatibility knowledge from outside of the baltic sea area has been entered into the synthesised matrix as well even though full weight has been given to baltic sea knowledge if available other seanergy compatibility knowledge sources are the following information from the swedish agency for marine and water management swam the swedish agency for marine and water management 2015 information from the aquaspace project in germany gimpel et al 2018 theoretical general information from kannen 2014 and unesco ehler and douvere 2009 and information from israel technion 2015 to standardise the marine use definitions that vary between the matrix input studies it has been chosen to use the general human use categories in table 2b of annex iii that was implemented into the msfd directive through an amendment to the original 2008 directive ec 2017 furthermore this article perceives human uses and activities as synonymous concepts similarly to menegon et al 2018a since this study focuses on the area relevant for marine spatial plans only marine human use categories have been considered excluding land based uses furthermore all the matrix input studies behind fig 3 include nature environmental protection in their marine use category examples for which reason such nature environmental protection categories have been added to the marine use list as supplements to the msfd marine use categories 2 2 2 marine use gis data marine use data primarily from the helcom map and data service portal 2 2 link to helcom map and data service portal http maps helcom fi website mapservice has been converted into raster data with the pre processing tool a simple binary approach has been used for the raster conversion and binary thresholds have been used for the very space consuming marine uses e g commercial fishing a detailed table of all raw data metadata and thresholds is provided in the read me file in the seanergy tool package as a template for the raster conversion a baltic sea raster has been used the baltic sea raster has been developed by using the raster template over the baltic sea region from the mytilus tool hansen 2019 and merging it with the ocean coverage of the helcom cia raster map 3 3 link to the helcom cia map metadata http metadata helcom fi geonetwork srv eng catalog search metadata 9477be37 94a9 4201 824a f079bc27d097 the resulting raster template is available in the seanergy tool package together with the pre processed baltic sea marine use raster data the raster template consists of raster cells with 1 km2 resolution making up an ocean surface of 436 375 km2 2 2 3 cumulative impact assessment gis data to visualise how seanergy enable co location analyses by supplementing cia analyses cia data processed by helcom and downloaded from the helcom map and data service portal is included in the result section 2 3 baltic sea results a proof of concept an overall seanergy analysis for existing baltic sea marine use combinations is presented followed by a seanergy analysis of synergy potentials for a new not yet located marine use 2 4 analysing co location options for status quo or for a scenario map a in fig 4 shows the spatial distribution of total potential use use conflict synergy scores for existing marine uses in the baltic sea created with seanergy the total positive score classes and the total negative score classes each apply a quantile classification method all raster cells that have returned as sensitive from 100 iterations of any of the three monte carlo tests have been coloured black in the figure the sensitive raster cells are particularly concentrated around the danish island bornholm and the finnish archipelago in these two areas the total conflict synergy scores are very sensitive to small changes in synergy conflict input category scores as well as to changes in the ranking scale for which reason the positive neutral negative trend of total conflict synergy scores cannot be trusted in these areas to compare the total potential use use conflict synergy distribution with the baltic sea cia distribution in order to assess potentials for successful co existence map b in fig 4 shows helcom s cia result classified after a quantile classification method a visual comparison between map a and map b gives the impression that much of the dark red area in map b with very high cumulative environment impacts overlap dark green areas with high potential total synergy scores in map a this trend is confirmed in graph a in fig 5 which shows how each cia class overlaps with each synergy conflict score class the highest total conflict synergy score class containing the positive total scores 4 58 75 has the highest overlap with the highest cia class containing index values 21 67 78 71 which illustrates a trend that high potential total synergies often correlate with high cumulative impacts on the environment however a certain amount of overlaps with total synergy areas do exist for the lowest cia class containing index values 2 16 4 86 since it has a noticeable amount of the total positive score classes 2 75 3 75 and 1 75 within such areas with both high synergy potentials and very low cumulative impacts on the environment marine uses might coexist peacefully without negatively impacting the environment too much map d and map e in fig 4 together visually illustrate where areas do exist that contain green coloured relatively good results for both seanergy total scores and helcom s cia map at once map d shows only robust non sensitive raster cells with a positive potential total conflict synergy score indicating areas with the highest potentials for total synergies map e shows helcom s cia result for the same raster cells as in map d whereof a certain percentage are also green in map e green areas in both these two maps illustrate good candidate areas for co existence between current marine uses such green areas typically do not contain too many marine use combinations which is shown in map f in fig 4 map f shows the count of unique marine use overlaps for the same raster cells as in map d and e map c in fig 4 shows total pairwise overlap counts for the whole baltic sea area compared with map a it gives the visual impression that sensitive raster cells seem to contain a medium high or high number of unique pairwise raster cell overlaps a pattern confirmed in the circle diagram in fig 5 which shows that all sensitive raster cells contain at least 3 pairwise overlaps compared with map b in fig 4 map c gives the visual impression that raster cells with a high number of unique pairwise overlaps often correlate with high cumulative impacts on the environment a pattern confirmed with graph b in fig 5 increasing the number of pairwise marine use overlaps thus in this analysis tend to increase the score sensitivity and the degree of the cumulative impacts on the environment 2 5 assessing use use synergy potentials for a new marine use that has no specific location map a in fig 6 shows areas with green colour where new diving activities might experience synergies with existing marine uses without experiencing any conflicts areas with any conflicts are shown with a red colour the darker the green colour the higher the potential total synergy score map b in fig 6 shows the number of individual marine uses that diving would entail synergies for diving has been deemed a good example to focus on here since it is the marine use that has the highest sum of total matrix input synergy scores with other marine uses in this baltic sea analysis graph c in fig 6 lists marine uses that experience synergies with diving according to the seanergy knowledge inputs furthermore it shows the total potential score summed over all raster cells and the raster cell distribution for each marine use the graph points out that artificial reefs areas for fish regeneration is the marine use which has the highest input synergy score with diving however commercial fishing has the highest total synergy score summed across all raster cells despite its much lower pairwise synergy score with diving since commercial fishing covers a very large area a graph such as this one that highlights potential use use synergies from locating new marine uses can be produced with seanergy statistics outputs 3 discussion as any spatial tool seanergy has strengths limitations and further development potentials 3 1 overall reflections 3 1 1 strengths as has been demonstrated seanergy advantageously enables marine planners and stakeholders to spatially explore potential synergies and conflicts between marine uses the results exemplify how to find synergy areas and conflict areas for existing marine uses in status quo but the same approach could be used to spatially point out synergy conflict patterns for marine uses in various marine spatial planning scenarios furthermore seanergy enables options to explore synergy potentials for new not yet located marine uses seanergy even provides a novel way to spatially assess co location options if it is used in combination with cia results combining seanergy with cia results can help ensure that marine uses are located to increase synergies and to minimise conflicts between marine uses while not having too high total impacts on the environment that would destroy ecosystem services or decrease their quality substantially thereby an optimal use of space can be achieved ideally benefitting all marine uses and ideally enabling more space for environmental protection 3 1 2 limitations and further development potentials seanergy currently does not include the cia approach in its tool package since the preliminary focus has been on developing a use use interaction approach whereas a use environment approach already is implemented in other spatial tools by combining the methodology behind seanergy more directly with cia tools e g mytilus hansen 2019 in the same software package in the future it could enable users to more easily combine the seanergy approach with the cia approach it is furthermore important to be aware of the limitations to the seanergy methodology some msp conflicts and synergies might be about planning processes and political institutional and individual interests moodie et al 2019 seanergy takes a general spatial temporal approach to conflicts and synergies leaving out non spatial planning derived and interest derived conflicts and synergies such non spatial conflicts and synergies are still highly of interest to work with in transboundary msp moodie et al 2019 but other methods than the gis based approach of seanergy are more suited to explore conflicts and synergies that do not have a spatial temporal proximity origin voinov et al 2018 3 2 reflections on the conflict synergy matrix knowledge 3 2 1 strengths the conflict synergy matrix implemented in seanergy for the baltic sea provides a systematic general overview over how conflicting and synergic different pairs of marine uses are perceived to be in the baltic sea area it is a novel attempt to expand the compatibility approach of the pairwise tables developed within many marine projects with conflict synergy knowledge an approach that could advantageously be carried out for other marine areas 3 2 2 limitations and further development potentials currently knowledge gaps exist for some marine use pairs in the conflict synergy matrix applied in seanergy for example the synthesised baltic sea knowledge unfortunately does not enable a specification as to whether the presented diving synergies apply to recreational or scientific professional diving however there are some indications that recreational diving has strong potentials in the baltic sea area the eu muses project has highlighted the multi use category of underwater cultural heritage and tourism and environmental protection which includes cultural heritage diving to be the multi use category with the highest potential in the baltic sea przedrzymirska et al 2018 the brackish water of the baltic sea has enabled many shipwrecks to continue being in a good condition for which reason many underwater parks that include diving activities already exist in many baltic sea countries schultz zehden et al 2017 the matrix could advantageously be updated with more details on conflicts and synergies to reduce pairwise knowledge gaps if sector experts and or sector representatives assembled across sectors to a list different conflicts and synergies as well as b rank the relative importance of them the descriptions and scores in the conflict synergy matrix could be further updated only synergies and conflicts with a clear spatial temporal proximity origin should be included in the process of listing conflicts and synergies to be inputted in seanergy more systematic marine use interaction information is needed about horizontal vertical temporal compatible and non compatible overlaps vertical neighbours temporal neighbours decreases and increases in space availability for existing and planned marine uses natural biofiltration zones around mussels and or seaweed aquaculture artificial reef zones contamination types and their link to other uses visibility links noise links infrastructure and or tool overlaps existing and planned security distance zones and the spatial temporal location of uses which share the same types of users bonnevie et al 2019 when getting sector experts to rank conflicts and synergies between marine uses across sectors it might be a good idea to introduce a no go status for some marine conflicts and synergies that are so severe they should never take place for example the aquaspace tool which has been developed to find and compare aquaculture candidate sites defines its most severe conflict category to be when aquaculture is not only conflicting with other marine uses but cannot be physically located near them in space gimpel et al 2018 implementing such a no go category would overrule any synergies that might otherwise be found to exist besides ranking severity of conflicts and synergies it is also important to gather information about the likelihood of such synergies and conflicts to happen rempis et al 2018 however the complexity of models increases with an increasing number of model parameters douglas smith et al 2020 for which reason it might be best to model synergy conflict likelihood as an independent model the likelihood is also linked to the temporal scale for when synergies and conflicts can be expected to be carried out in time marine uses might move deeper out at sea enabling new synergies e g when wind farms become floating stefanakou et al 2019 if the temporal uncertainty behind synergies and conflicts are not communicated it can lead to demotivation among sectoral representatives in planning situations in the baltic scope projects some planners were demotivated by the perceived tendency for some cross sectoral synergies such as combining wind farms with certain types of fishing to be purely theoretical and not real practical options moodie et al 2019 thus to avoid miscommunication it could be an advantage to extend the conflict synergy knowledge in the matrix with knowledge about the likelihood for conflicts and synergies to take place 3 3 reflections on the needed analysis detail level 3 3 1 strengths like cia tools seanergy provides options to gain an overview over spatially interesting patterns in larger areas at once while the detail level of the conflicts and synergies that seanergy describes is rather limited it is the whole purpose of seanergy to stay at an overall level to get an overview of which spatial areas and which marine use combinations need further more detailed exploration the msp directive emphasises the importance of cross country cooperation that ensures coherent planning for whole sea basins at once ec 2014 a requirement that seanergy helps facilitate 3 3 2 limitations and further development potentials the large variation in marine use categories between existing pairwise compatibility approaches indicates that varying opinions exist regarding what level of detail is needed for categorising different marine uses in seanergy it has been deemed appropriate to use general overall marine use definitions inspired by the msfd directive annex iii table 2b categories and natural environmental protection area categories regardless of whether a general or more specific marine use definition is used one needs to think about what aspects of the marine use should be covered by data for example having spatial data about fishing catches and or temporal knowledge about the presence of fishing boats is better than just knowing the spatial routes of the fishing boats since the former knowledge enables planners to prioritise where the most important fishing areas are as always the resolution and level of detail of spatial tools depend on the quality and availability of data moodie et al 2019 pnarba et al 2017 the resolution of some marine use data from helcom could be much better particularly for the mobile marine uses e g shipping commercial fishing benthic trawling etc in future versions of seanergy it might be better to provide a continuous classification of marine use presence between 0 and 100 such as the one used for marine pressure data in mytilus hansen 2019 for areas where limited spatial data exist public participatory gis methods ppgis can help gather a large amount of information with limited resources voinov et al 2018 ppgis might be needed in areas where no spatial data over marine uses exist to gather information on conflicts and synergies in these areas as a methodological supplement or maybe even replacement of seanergy in such data empty areas allowing the spatial synergy conflict approach to move from potential conflicts synergies to more actual perceived conflicts synergies to be included in msp it is an essential requirement for marine uses that they are spatially captured on maps since msp is an area based approach for example ppgis processes can help to point out and prioritise culturally and recreational significant areas giving space to marine uses not often included in msp gee et al 2017 in this study purely terrestrial uses have been excluded as was also the case in the research by halpern et al 2008 that originally defined the cia approach through a global study covering all oceans of the earth some uses are both marine and terrestrial such as harbours that are already included in the seanergy baltic sea analysis harbours are key access points in combining the marine sphere with the terrestrial one however it might be an advantage in the future to also include land based coastal marine uses to improve the land sea interactions rempis et al 2018 for example recreational coastal activities westerberg et al 2013 might experience conflict links and or synergy links with other human uses in the ocean 3 4 reflections on the technical implementation 3 4 1 strengths the adriplan muc tool has similarly to seanergy applied a spatial approach to conflict analysis menegon et al 2018b however the approach presented in seanergy goes further than the adriplan muc tool by including synergies and working directly with conflicts instead of using spatial temporal extents and overlaps as proxy for conflict degrees seanergy provides a descriptive read me file as well as metadata directly implemented in the tools themselves to iteratively guide the tool users furthermore seanergy offers the users much flexibility by applying five different synergy conflict tools with different conflict synergy options and with a combination of mandatory and optional settings seanergy also provides an uncertainty analysis douglas smith et al 2020 are urging uncertainty and sensitivity tools to demand a high quality for user accessibility user ease and user guides and to avoid too high a complexity that scares away potential tool users the uncertainty analysis implemented in seanergy is rather simple and specific in scope and thus attempts to follow the advice by douglas smith et al 2020 it aims at easily guiding the user through the uncertainty model steps requested by douglas smith et al 2020 of sampling the parameter space run the model in this case a monte carlo model and analyse results in this case creating maps showing overall score sensitivity while seanergy requires an arcmap license for the interface and tools to be used all python scripts behind the tools are opensource and freely available on github 3 4 2 limitations and further development potentials currently seanergy only considers use use links within the same raster cells a limitation shared with the adriplan muc tool menegon et al 2018b a technical improvement to seanergy could be to include synergy conflict neighbourhood links across raster cells if trawling takes place very close to a natura 2000 area it might for example still cause negative impacts to the nature protected area even though the two marine uses do not overlap in the same raster cell allowing conflicts and synergies to decrease with distance models could be an improvement to seanergy in future development for example the cia methodology by tools4msp include a spatial distance model in their cumulative impacts model menegon et al 2018a furthermore an actual sensitivity analysis has not been implemented in seanergy at the current state a sensitivity analysis apportions the overall uncertainty of a model to different model inputs douglas smith et al 2020 lilburne and tarantola 2009 a sensitivity analysis could be implemented at a later development state of seanergy lastly improving the calculation time of seanergy could be an advantage to enable seanergy to be used directly as part of in situ planning situations with stakeholders currently seanergy is better to use before stakeholder meetings to have the maps ready for co location discussions already when the meetings begin since the running time is app 10 min for all marine use combinations or even an hour for the uncertainty analysis if only one marine use is in focus however the running time is currently only a few minutes for which reason seanergy could currently be used in situ in meetings that focus on one sector and its relation to other sectors one way to improve the calculation time could be to implement the python code in a new opensource fast tool package which would be independent from an arcmap license and which could from its start be combined with a cia approach for example the cia tool mytilus requires no commercial software licenses to run and offers fast calculations due to it being developed in delphi 10 1 a faster programming environment than python hansen 2019 4 conclusion a need exists for spatial decision support tools to model co location options between marine uses based not only on environmental capacity but also on ambitions to increase synergies and decrease conflicts between marine uses in close spatial temporal proximity managing use use interactions by increasing use use synergies and decreasing use use conflicts can secure an optimised use of marine space potentially enabling more area for environmental protection in increasingly crowded seas by synthesising available compatibility conflict multi use and synergy knowledge into a pairwise matrix and providing the matrix with a spatial dimension this research has introduced a novel approach implemented in the arcmap based toolbox seanergy to assist planners in managing marine use use interactions seanergy consists of six tools two score tools and two count tools produce potential synergy conflict maps based on raster based marine use data as well as a pairwise conflict synergy knowledge matrix they provide a high degree of user flexibility in that they can both focus on one specific marine use smaller subsets of marine use combinations or they can explore spatial patterns for all marine use combinations additionally an uncertainty analysis is provided which tests the sensitivity of positive and negative spatial patterns in the total synergy conflict score maps regarding specific modelled input uncertainty in the expert based score inputs lastly a non spatial tool to browse the matrix content is provided to support a transparent overview of the inputted conflict synergy knowledge the high degree of user flexibility of seanergy tools supports an iterative maritime spatial planning msp approach as demonstrated with a test for the baltic sea seanergy enables planners and other msp stakeholders to consider a holistic cross sectoral spatial approach to the planning of different marine uses such an approach can advantageously support the transboundary scope provided in ongoing cross country projects in the baltic sea in future developments of seanergy the toolbox can advantageously be updated with e g more thorough matrix knowledge as well as introducing a synergy conflict distance model to allow synergy conflict calculations across raster cells however seanergy as has been demonstrated already provides an important supplement to cumulative impact assessments cia on the environment marine uses both contribute to and depend on marine ecosystem services by combining the two methods of seanergy and cia tools co location options can more fully be explored by marine spatial planners software availability name of software seanergy developer ida maria bonnevie e mail idarei plan aau dk year first available 2020 program specifications an arcmap based toolbox called seanergy with tools developed in python for spatially locating potential synergies and conflicts between marine uses based on a pairwise comparison matrix metadata is located in the tool package software and license dependencies the full program depends on a windows 10 system with esri arcmap installed including the arcmap extension spatial analyst and including the python modules arcpy os pandas numpy collections time datetime the program has been developed for the versions arcmap 10 7 and python 2 7 the script tool interface depends on an esri arcmap license including the extension spatial analyst however the python code is open source and the methodology can thus freely be implemented into own programs with credits to the author of the approach program size 4 07 mb available here https github com idambonnevie seanergy git with a readme file a pre processing tool is available here https github com idambonnevie seanergy preprocessing git declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper declarations of interest none acknowledgements this research is carried out within the project baltic sea maritime spatial planning for sustainable ecosystem services bonus basmati which has received funding from bonus art 185 call number call 2015 77 funded jointly by the eu innovation fund denmark swedish research council formas academy of finland latvian ministry of education and science and forschungszentrum julich gmbh germany appendix 1 converting seanergy score formulas into seanergy count formulas if the score s u 1 u 2 in formula f1 is defined to be a binary case of either 1 or 0 1 if a condition con is met 0 if the condition is not met the formula can be used to count the number of synergies and conflicts instead of weighting the conflict synergy degrees formula a1 f1 shows this specific instance of f1 which is implemented in the tool calculate score map a1 f1 s con x y u 1 1 n 1 u 2 u 1 1 n u u 1 x y u u 2 x y s con 1 u 1 u 2 the binary score of either 1 or 0 based on whether the condition con is met is represented as s con 1 u 1 u 2 instances of conditions con implemented in the tool calculate score map for the user to choose between are a all pairwise use combinations b pairwise use combinations that have a positive or negative conflict synergy score c pairwise use combinations belonging to a specific synergy conflict category d all pairwise use combinations or only the combinations with a conflict synergy score that include a specific use e pairwise use combinations that are mobile f pairwise use combinations with an overlap in spatial vertical location g pairwise use combinations with different spatial vertical locations h pairwise use combinations that could be part of a multi use constellation according to the muses project just as a1 f1 is a specific instance of f1 a specific instance a1 f2 of f2 can be deduced in a1 f2 the scores in s p o s u 1 u a in f2 are binarily defined to always be 1 when they are positive and 0 when they are not positive to count the number of synergies instead of weighting their synergy size a1 f2 s  pos 1 a x y u 1 1 u 1 u a n u u 1 x y s p o s 1 u 1 u a i f u 1 1 u 1 u a n u u 1 x y s n e g u 1 u a 0 1 i f u 1 1 u 1 u a n u u 1 x y s n e g u 1 u a 0 a1 f2 is implemented in the tool find synergy potential counts for a new marine use appendix 2 technical components of seanergy besides the toolbox itself and its six python scripts a script exists for each tool besides four function scripts the seanergy tool package exists of a copyright statement a read me file a python module folder containing python modules that might need manual instalment in arcmap for the toolbox to work a data folder with pre processed raster data for different marine uses an extra input folder with a marine area raster and optional symbology files to symbolise seanergy outputs and three excel sheets one with the pairwise matrix content and scores one with spatial temporal attributes for each marine use and one with marine use categories linked to raster names a visual overview over the technical components are presented in figure a2 1 running time of tools typically spans from 20 s to 14 min depending on the parameter settings except the uncertainty tool which takes approximately 1 h to run for 100 iterations fig a2 1 technical components within the seanergy tool package fig a2 1 to illustrate an example of a seanergy tool user interface and the high degree of flexibility in the tool setting choices figure a2 2 presents the user interface of the tool calculate score map mandatory inputs required for this tool to run include the folder with pre processed marine use data the excel file with marine use categories linked to the marine use raster names the pairwise conflict synergy score matrix whether all inputs scores will be used in the calculation default or only positive or negative scores will be used a name for the total score raster output and the ocean raster to define the analysis area optional extra inputs consist of a symbology file to style the total score raster output a symbology file to style the polygons with scores for specific marine use pairs that as a default setting are outputted as extra information below the mandatory total score raster output 1 3 excel tables outputting statistical information about the score distribution in the matrix or map and options to limit the number of requested polygons with scores for specific marine use pairs to only include that number of the pairs selecting only the pairs with respectively the highest and the lowest scores across the map this number of requested polygons could also be zero if the user does not want to explore individual marine use pairs as a last option it is possible to calculate the total score maps only for one specific inputted marine use e g shipping as in figure a2 2 to see where that marine use potentially experiences conflicts and synergies with other marine uses a full list of user interfaces is available in the read me file in the tool package and all tools are supported with built in metadata during use information about the separate pre processing toolbox can be found in the read me file in the pre processing tool package available on github fig a2 2 user interface of the tool calculate score map fig a2 2 
25988,headwater streams account for more than half of the streams in the united states by length the substantial occurrence and susceptibility to change of headwater streams makes regular updating of related maps vital to the accuracy of associated analysis and display here we present work testing new methods of completely automated remote headwater stream identification using metrics derived from channel digital elevation model dem cross sections a jump in standard deviation of curvature sk is found to correlate with the presence of stream heads field and remotely validated stream and channel initiation points from 4 diverse study areas in north carolina as well as a simulated surface are used to test the sk findings the sk value within individual catchments equal to 0 5 tukey s upper inner fence is found to be a reliable threshold for identifying the upslope extent of channels in varied landscapes keywords lidar dem stream head headwater change point cross sections disclaimer any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government 1 introduction headwater streams make up over 50 of streams by length in the conterminous united states fig 1 and inaccuracies in their representation can be a significant cause of error in stream statistics and riparian zone distribution nadeau and rains 2007 the small streams have a large impact on waterways and ecosystems given their proportion of total stream length in a catchment and resultant sediment and nutrient contributions as well as species habitat and hyporheic and riparian zones naiman et al 1993 understanding the impact of the contributions made by small streams is necessary to accurate hydrologic modeling for sediment transport estimation flood control and other hydrology management tasks the curation of accurate headwater maps is challenging given the extensive distribution small scale and dynamic nature of the channels such water courses are regularly horizontally and vertically altered in single extreme weather events small shifts in climate conditions can have large effects in the length of low order streams with shifting extent due to change in groundwater levels and precipitation wohl 2018 small streams and drainage patterns are also often altered by anthropogenic disturbance such as agriculture forestry urban development and water usage change the factors listed above make regular update of headwater maps necessary in this work we present a strategy for automated pruning of flow accumulation lines and extraction of stream heads using open source tools including python and qgis qgis development team 2019 to support the regular update and validation of the national hydrography dataset nhd automation is needed for the regular validation and update of the nhd the nhd is a digital database of surface water features in the united states the original nhd was compiled from digital cartographic data created from topographic maps moore and dewald 2016 hydrographic data compiled from topographic maps such as nhd have varying degrees of accuracy and may not be suitable for some hydrologic regulatory and engineering uses due to inconsistent drainage density and missing headwater content colson et al 2008 fritz et al 2013 stanislawski et al 2015 updating of these features is ongoing and involves extraction of drainage networks from digital elevation models dem with pixel resolution between 1 and 10 m lidar light detection and ranging point cloud data are being collected at quality level 2 or better for the united states excluding alaska and these data provide for 1 m digital elevation model dem generation along with reflective properties and multi return structure from the point cloud the point cloud data provides insight into fine scale surface features that can be useful for surface water mapping shavers and stanislawski 2018 an important component of headwater mapping is determining the channel or stream head the stream head is defined by some as the initiation or origin of perennial stream flow and channel head as the initiation of surface flow concentration and sediment transport jaeger et al 2007 wohl 2018 the upstream initiation of flow or transition from non confined flow to well defined flow path can also be referred to as the stream head hancock and evans 2006 others describe the stream head as the concentration of active flow at any given time dietrich and dunne 1993 still others use the term flowhead for the upstream extent of active flow whiting and godsey 2016 whereas channel head is generally accepted as the upstream extent of detectable regular concentrated flow i e ephemeral stream head here channel head defined as the initiation of detectable surface flow and sediment transport will be used unless specifying intermittent perennial of ephemeral stream heads a great deal of research has been devoted to the development of methods for surface water mapping and the derivation extraction of stream lines and drainage channels from remotely sensed data with equally copious degrees of success tribe 1991 heine et al 2004 james et al 2007 james and hunt 2010 fritz et al 2013 jensen et al 2018 hamada et al 2016 julian et al 2012 pirotti and tarolli 2010 clubb et al 2014 hooshyar et al 2016 hfle et al 2009 shavers and stanislawski 2018 yet little work exists addressing strategies for automated channel head derivation clubb et al 2014 offer a comprehensive review of related literature and test several methods with the aim of automated headwater modeling using elevation data yet all of the methods tested require multiple user designated parameters the results of the study show that the pelletier method pelletier 2013 performed among the best of the methods tested and proved the least sensitive to changing parameters clubb et al 2014 hooshyar et al 2016 present a strategy for automated channel modeling using contour curvature the method determines a curvature k threshold for skeleton delineation by taking the maximum number of convergent positive k clusters with increasing k value following network skeleton generation channel cross sections for each tributary are analyzed for significant clustering to determine the transition from non channel to channel and location of channel head the contour k method is reported to perform comparably to the pelletier method yet the authors note that the contour k method was developed for landscapes with minimal human impact and well defined distinctions between unchannelized valleys hooshyar et al 2016 the work presented here builds upon earlier work by testing a process for automated identification of headwater stream channels and their channel head locations using analysis of cross sections cs james et al 2007 hooshyar et al 2016 to prune over extracted flow accumulation networks sangireddy et al 2016 the channel cross section analysis csa methods described here are fully automated methods for stream channel validation and channel head detection using high resolution elevation data the analysis generates channel cs statistics from unfiltered and filtered dems and the values are compared to field validated stream head and remotely derived channel head data from four topographically diverse study areas it is found that a jump in cs standard deviation sk value correlates with the presence of channel and stream heads with channel heads aligning slightly better the field validated intermittent and perennial stream heads used in this work are stream head estimates gathered from single observations and provided by the north carolina department of environment and natural resource channel heads are identified manually through analysis of elevation and image data the csa is also tested on a simulated stream channel to illustrate the response of the elevation metrics to channel development variations of surface curvature such as plan and profile curvature are used in other stream and channel extraction strategies julian et al 2012 jensen et al 2018 passalacqua et al 2010 tarolli and fontana 2009 test the use of multiples ranging from 1 to 3 times standard deviation of landform curvature as a curvature threshold for stream head identification shavers and stanislawski 2018 first introduced the standard deviation of curvature along cs as a gauge of channel development for stream mapping this work tests the sk metric on multiple landscapes with varying elevation relief and forest cover methods are analyzed for automating the analysis signal processing strategies for change point cp detection along individual modeled channels are tested with mixed results while filtering channel skeletons using a sk threshold is more computationally efficient and returns good results the sk value equal to 0 5 times tukey s upper inner fence tukey 1977 for individual catchments is shown to be useful as a threshold for pruning flow accumulation models and cp detection methods appear useful for stream head detection tukey s upper inner fence is an outlier detection method detailed by brant 1990 the workflow described here is implemented with open source tools and python programming language and uses lidar derived elevation data and road shapefiles from the u s geological survey usgs national transportation dataset as the inputs csa is rapid processing technique compared to alternative approaches that may involve processing entire dem datasets elevation and derivative raster maps require large storage capacity when working at a regional scale the focus on channel features in large part avoids the noise introduced by other land surface features thus the strategies tested here have the potential for completely automated and computational efficient channel network modeling in diverse environments for comparison the four study areas are also analyzed using the pelletier method pelletier 2013 and the results reported 2 methods the strategy for headwater stream identification tested here applies the csa the processes are optimized using the rowan county dataset the rowan site is located in the north carolina piedmont region and has a dense network of field validated stream head locations the topography of the piedmont represents a literal and figurative middle ground between the lower relief coastal plain to the east and higher relief appalachian region to the west the parameters that are determined using the rowan site include best fit elevation metric cs size and spacing and sk values for stream fitting see section 2 2 the workflow is then tested on the remaining three study area s in order to identify trends and the potential for unsupervised automation 2 1 study areas and data the study areas are four watersheds spread across north carolina fig 2 the rowan area has an elevation range between 195 and 255 m with an area of 6 3 square kilometers the area is located in the piedmont region of central north carolina with primarily forest landcover with some agricultural and residential areas the gaston area is also in the piedmont and has an elevation range between 185 and 250 m with an area of 2 8 square kilometers the land cover is primarily forest with some residential areas the asheville study area has an elevation range between 630 and 1170 m with an area of 20 7 square kilometers the asheville watershed is located in the mountainous appalachian region of western north carolina and has primarily forest landcover the sandy creek watershed has an elevation range between 2 and 62 m and an area of 7 5 square kilometers the area is located in the coastal plain region of eastern north carolina with agriculture wetland and forest landcovers the field validated stream head validation data consists of field collected intermittent and perennial flow initiation points determined by the presence of benthic species and geomorphology the field data were supplied by the north carolina department of environment and natural resources ncdwq 2005 the sandy creek points were collected in 2010 gaston in 2014 rowan between 2013 and 14 and asheville between 2010 and 2017 with the majority between 2010 and 2011 the time difference between lidar collection and field work must be considered as a potential source of error a 5 m buffer around stream head locations is used to address potential discrepancies in gps location apparent channel head validation data are manually identified channels with fv points are analyzed using elevation and image data to identify signs of concentrated flow and channel formation the inputs for the csa are national transportation dataset road and rail shapefiles ntd 2019 and dems derived from lidar data made publicly available by the usgs 3d elevation program 3dep the lidar is quality level 2 ql2 heidemann 2018 having 0 71 m maximum nominal pulse spacing or better and collected in leaf off conditions ql 1 geiger mode lidar stoker et al 2016 with a maximum of 0 35 m maximum nominal pulse spacing is used here with the exception of the sandy creek area where ql2 is used ashville gaston and rowan lidar collections took place between 2016 and 2017 and the sandy creek collection occurred in 2014 2 2 cross section generation the first step in the csa is generation of the dems from lidar point cloud data the dem s are generated using inverse distance weighting from the lidar classified ground points all of the dems have a minimum non vegetated vertical accuracy nva of 19 6 cm cm at the 95 percent confidence level 10 cm rmsez heidemann 2018 the next step is the generation of elevation derived lines using the open source geonet tool sangireddy et al 2016 the lines are extracted with a standard flow accumulation threshold of 1000 cells or 1000 m squared given the 1 m cells this value is used as an approximate minimum value for regions in the conterminous united states the lines are over extracted with a goal of intersecting all stream heads the resulting networks fall within 30 m of all but one of the stream heads with the exception found in the ashville area channel cross sections cs are placed along and perpendicular to flow accumulation lines with a spacing of 3 m and a width of 40 m i e one cs every 3 m and each extending out 20 m on either side of the flow accumulation lines the cs spacing is used to gain the greatest detail possible while minimizing noise and redundancy given the 1 m resolution and the size of channel features 3 m cs spacing is found here to be optimal data resolution and feature scale also inform cs length or width across the channel being measured stream channel and bank shape may inform the results of this work but other riparian corridor morphologies may also be indicative of stream presence and contribute to the results work by the united states forest service merritt et al 2017 suggests a minimum cs width of 6 m for analysis of headwater streams studies of riparian zone vegetation variation patterns clinton et al 2010 suggest the riparian corridor can have a width of up to 40 m a cs wider than 40 m could be used yet the density of the channel features and the geomorphology observed here make the 40 m width optimal i e describing the channel slopes with minimal overlap the cs line feature elevation values are extracted from the 1 m lidar generated dem with 80 values evenly spaced along the cs the transportation layer is a set of road features from the usgs national map vector database which is used here to generate a 20 m wide buffer around roads an intersect process removes individual cs that intersect the road buffer this is done because roads often shaped like inverted streams in profile can create false positives for the csa method there is a chance that channel features will not be identified by the flow accumulation process optimization of the flow modelling and road masking will be addressed further in following sections several cs metrics were tested for correlation with the presence of stream and channel heads including area under the curve area divided by height average slope maximum slope average curvature standard deviation of curvature sk and curvature coefficient of variation the listed metrics were tested on unfiltered dems and dems smoothed using a nonlinear diffusion equation catt et al 1992 the area under the curve or integral of a cs is the sum of the elevation sample points minus the minimum elevation sampled for a given cs slope classification is a common measurement in geomorphic studies and is simply the difference in elevation over one given that the sampling is evenly spaced at 1 m intervals through visual analysis the change in value of the sk statistic was found to best correlate with the presence of channel and stream heads and is therefore employed in the automated analysis testing there are a number of strategies for calculating curvature goldman 2005 the method applied here for calculating the curvature k along a cs is the derivative of the tangent equation 1 kreyszig 1991 here the tangent line is fit to three consecutive sample segments along the cs k can be described as the inverse of the radius of a circle fit to a curve or radius of curvature where the smaller the radius becomes the larger the curvature value values of k typically range from 1 to 1 and are close to zero positive k values correspond with convex curves while negative values correspond with concave or downward curving surfaces or lines negative k is often used in channel delineation jensen et al 2018 julian et al 2012 as these values often decrease with channel formation and incision negative curvature value extremes can also be useful as indicators of bank formation and increasing longitudinal slope from the cs curvature values standard deviation is calculated for each individual cs table 1 1 k d 2 y d x 2 1 d y d x 2 3 2 2 3 stream simulation in order to assess the response of sk to the idealized development of a stream channel from a flat surface to a widening channel a first order stream channel is modeled using equation 2 the gaussian kernel applied above 0 6 times the total channel depth 0 6 z is a cumulative sum that decreases in depth with increasing sigma value  x is a given horizontal distance from the line origin  or channel head here the origin is 0 the formula applied below 0 6 z generates a channel profile with increasing depth and width the modifier value c allows the increase in simulated channel width with a flattened channel floor and is defined as 0 6 z 4 0 6 z  the resulting channel derived here with a total depth of 6 3 approximates cross sections of a channel that incises with increasing distance downstream and increased flow at a depth of 6 the incision slows and the channel widens with increasing bank erosion fig 3 the simulation values are unitless and are only used for plotting 2 f  0 0 1 e x  2 2 c 2  0 6 z 0 6 z e x 2 2  2 2   2  0 6 z c 0 6 z 4 0 6 z  2 4 change point detection cp detection is a strategy typically applied to identify anomalies in time series or signal data while signal processing is typically discussed in relation to disciplines such as electrical or medical research geomorphic features often present patterns that resemble time series signals and have been studied using signal processing methods by others wang 1998 van dijk et al 2008 a valley profile series moving down slope can be thought of progressing linearly with increasing width and or depth a deviation in this signal from the trend would cause a shift in the response such shifts could correlate with a knickpoint depositional features or other channel formation features resulting from concentrated flow here we process evenly spaced cs statistical data as signals to attempt to automate identification of shifts in channel structure in response to the presence of water flow the signal response that appears to coincide with stream heads in the model development stage is a jump in sk mean and variance magnitude and variance shift in signal data are common phenomena and a good deal of work has been devoted to detecting these shifts truong et al 2019 tsay 1988 two methods are tested here for use in identifying significant shifts in mean or variance that occur near stream heads the cp analysis begins on channels at the perimeter of catchments by extracting single channels beginning at the point in a channel where the flow accumulation reaches 450 000 cells following the channel upstream the 450 k initiation value was determined by comparing the flow accumulation of all field validated stream heads of the 200 stream heads only 3 had flow accumulation values greater than 450 k and the median is 50 k when confluences are encountered the path with the highest flow accumulation value was followed for initial analysis the automated determination of stream head location from cs statistics is tested using two methods adapted from time series cp detection equations the cumulative sum of squares css method uses a normalized cumulative sum of squares dk function equation 3 4 plotted against distance down channel to identify peaks or troughs that correlate with proportionally large changes in sk value inclan and tiao 1994 here dk sqrt t 2 d k c is the css of a series of values a k is a given value k 1 t and t is the total number of values the asymptotic response of dk allows the maximum absolute value of dk to be tested for significance with boundaries associated with specific significance levels inclan and tiao 1994 used simulated series of varied length to develop significance bounds 3 d k c k c t k t 4 c k t 1 k a t 2 the second method is the pruned exact linear time pelt method killick et al 2012 the pelt method is computationally efficient using a method that aims to minimize a linear cost function for segmenting the data by statistical properties here the linear cost function is a gaussian kernel or radial basis function the penalty function found here to be optimal is the log of the length of the segment where killick et al 2012 suggest twice the negative log the method is implemented using the ruptures open source cp detection program ens paris saclay 2017 the analysis returns a list of integer cps ranging from 0 to 6 the csa uses these values to determine weights of the level change the variance and mean of each data segment separated by the cps is normalized by calculating the difference divided by the normal of the vector of two segments about each cp these values are multiplied to give a score or weight for each detected cp the cp algorithms must determine the location of cps and whether the found cp is significant suggesting that the cp represents a stream head the performance of the models for cp significance is assessed using decision matrices and the geometric mean g mean section 3 2 kubat et al 1998 the g mean is calculated as g mean g square root of acc acc where acc is the accuracy on positive variables also known as recall channels with stream heads present and acc is accuracy on negative variables channels without a stream head higher g mean values indicate higher accuracy 2 5 cs filter a cs filter test was developed to filter the flow accumulation skeleton and determine the location of stream and channel heads using the rowan area to develop the model the sk value of 0 5 times tukey s upper fence equation 5 tukey 1977 was selected as the cs threshold cs that overlap adjacent channels were first excluded from the dataset feature segments with all cs falling within 20 m of another channel are also excluded finally features with 10 or more cs having sk values above the threshold are identified as the extent of the channel network the extent determined all features up channel from there are eliminated while all features down channel are retained to preserve the connected network fig 5 the upstream extent is pruned to the threshold cs 5 s k t h r e s h o l d 0 5 interquartile range 1 5 m e d i a n s k 2 6 pelletier method the pelletier method pelletier 2013 uses a tangential curvature raster threshold for channel extraction there are three main steps in the pelletier method the first step requires filtering of the dem using an optimal wiener filter the second step is generation of the curvature raster and channel skeleton with pixels having positive values identified as part of the channel network the final step is application of a k threshold to reflect appropriate channel density pelletier 2013 suggests that a threshold value of 0 1 is broadly applicable yet others suggest the value should vary with landscape condition tarolli and fontana 2009 in addition to the k threshold the pelletier method also requires a skeleton minimum drainage area though this value has less bearing on the final results clubb et al 2014 the pelletier method is implemented using the open source lsdtopotools lsdtt channel extraction program clubb et al 2017 the default minimum drainage area of 400 m2 is used the default k threshold used by lsdtt is 0 01 here k values of 0 1 0 05 and 0 01 are tested for optimal channel network extraction 3 results and discussion 3 1 stream channel simulation the sk statistic is generated for the resulting simulated channel css the location of the peak in the scatter plot of sk vs distance downstream here cs count fig 3 corresponds with the most extreme downcutting before the channel widens the trend indicates that the highest sk values resulting from channel formation will be found among narrow channels yet the sk values are a function of dem and sampling resolution a dem cell size or elevation sample spacing that is wider than the channel will not resolve narrow deep channels well the simulated channel sk values demonstrate the distribution of values given different bank structure and channel development as can be seen in the plot of the simulated channel fig 3 valley formation and bank shoulder structures lead to higher sk values the rise in sk value is expected given that as the channel deepens and angles or changes in slope increase the sk increases fig 3 shows that a maximum sk value is likely to occur in headwater channels where surfaces allow for down cutting exceptions to the even distribution seen in fig 3b are likely to be the result of anomalies in the data and therefore beneficial for source data and results validation outlier values may also be the result of anthropogenic surface features dams flood walls and other infrastructure may present surface bends and resulting radius of curvature values smaller than is found in the natural channels the exponential curve in the scatter plot formed by the incising channel peaks where the base begins to widen the values begin to lower after the peak is reached yet not at the rate that they increased building up to the peak the more gradual drop off and elevated floor reached in the vicinity of 140 on the x axis is due to lower radius of curvature values maintained by valley corners and bank ridges clearly the max sk value would be an important input for modeling and extracting stream lines from elevation in a natural environment the sk peak value is elusive for several reasons the small radius of curvature and elevated sk will often be found at the minimum of a v shaped formation often forming at the base of small or narrow channels not all channel systems will feature the narrow valley required and the dem resolution may obscure detection the common occurrence of anthropogenic features can also obscure a natural sk plateau 3 2 change point detection as noted by higher sk values for unfiltered dems than filtered dems table 1 the unfiltered dem returned noisier sk values than the filtered dem as is expected also expected is the preserved increasing sk value with increasing bank formation as can be seen in the simulated channel fig 3b the filtered dem proved to eliminate some common false positives due to built environment surface structure and makes interpretation of results clearer tables 2 and 3 show whether the cp detection methods determined significant cps are present in channels with the fv stream heads if the model finds a significant cp and there is a fv stream head in the channel it is scored as tp if there is no stream head it is scored false positive given that there are no fv ephemeral stream heads in the dataset this is a test of the presence of intermittent or perennial stream heads on a channel the distance along the channel between fv stream heads and cps identified by the css and pelt models is displayed in fig 4 as can be seen in tables 2 and 3 recall scores vary widely this is a common occurrence especially with data that has unbalanced true and false samples kubat et al 1998 therefore the g mean score is employed it considers true and false samples and weights them higher when there are fewer the cp detection models did not perform well overall when considering whether there is a stream head present with the exception of the rowan area having g 0 725 with the css model the true negative and acc scores for the asheville area are promising the determination of cp significance is important for thinning flow accumulation networks and challenging given the common noise found in the sk signals especially in mountainous areas the accuracy of the detected stream heads 56 and 50 of the stream heads for the pelt and css models respectively can be seen in fig 4 with roughly 50 of detected stream heads falling within 50 m of the fv stream heads the negative values in fig 4 represent instances where the fv stream heads are up channel from the modeled points indicating an under extracted network positive values are instances where the modeled location is up channel from the fv point the occurrence of a positive skew is expected given that the channel heads or ephemeral stream heads are underrepresented in the data yet often develop well defined geomorphic characteristics yet the opposite appears to be the case judging by fig 4 the negative skew is caused for the most part by sk jumps occurring as a channel nears a confluence and encounters a developed flood plain or wider and deeper channel the large jumps are located at the end of channels leading to modeled cps or stream heads far down channel from fv stream heads and resulting in negative values one possible strategy to remedy inaccurate cp location is an adaptation of the model to account for multiple cps which also introduces increased complexity 3 3 cs filter results the cs filter results are presented relative to the fv stream head data and apparent channel head locations apparent channel heads are identified on channels with fv points using manual analysis of elevation and image data there is a strong trend of over extracting lines relative to fv points the over extraction relative to fv points is expected given that the fv points are represented as intermittent and perennial stream heads the results are more accurate as well as trending towards under extraction relative to apparent channel initiation the result histograms spread fig 6 is distributed wide due to outliers in over and under extraction the lines generated by the csa are best aligned with the fv and apparent channel heads in the two piedmont locations rowan and gaston fig 6 one channel in the rowan area has a distance of nearly 300 m between the fv and apparent stream head the dem shows a broad channel 20 m wide in places a culvert under a road and a well defined channel head upstream from the fv stream head the results for the ashville study area show many places where the channel network is under extracted though often by a matter of meters fig 7 a the under extraction is generally the result of channelization and flow concentration occurring high in the catchment incorrect flow accumulation is responsible for under extraction in places a dense network of roads exists in the study area that are not included in the road feature layer used here it appears that less than 10 of roads visible in the elevation data are present in the national map roads layer roads generate several false positive channels where the flow accumulation lines incorrectly build the skeleton yet breaching roads in a dem at channel intersections can be less of a problem in higher relief areas the gaston results also indicate under extracted channel networks several small lakes are not handled consistently as a result of the channel structure up stream of the lakes fig 7b yet the lakes are correctly traversed with a flow line dissecting them in some cases roads also lead to inaccuracies in the gaston study area the inaccuracies are generally the result of flow accumulation deviations that contribute to unidentified channels the sandy area results fig 8 show several well defined channels that are not identified by the fv stream heads i e ephemeral channels there are extensive drainage or irrigation canals surrounding agricultural areas the canals are largely unidentified by the flow accumulation skeleton given the minimum contribution area where canals are extracted by the flow accumulation model they are clearly identified by the csa method wetlands also complicate the results the wetlands are not documented in the nhd here and identification is challenging yet where open water is visible relative results appear correct a disturbed or undulating surface in wetland environments resulting from inconsistent lidar return points reflecting from the saturated and or flooded surface can be seen and correlates with elevated sk values though not as a response to bank structure lakes here are not generally defined by the extracted channels given the 40 m cs width and 3 m spacing used here there is a potential to not identify stream heads on channels running parallel and within 20 m of each other this did not occur in this study but could be mitigated by variation in cs width with stream order or width 3 4 pelletier method the pelletier method pm for estimating channels heads performs comparably to the csa method as can be seen in fig 6 and table 4 fig 6 indicates that a majority of validated channel heads are less than about 100 m from modeled channel heads using either the csa or pm method likewise the average distance between modeled and validated channel heads is less than 100 m for the all study areas using either the csa or pm method table 4 except for the sandy creek watershed where results are relatively unreliable the average distance between modeled and manually interpreted channel heads is larger than 100 m for the sandy creek watershed using both csa and pm methods and average distances for sandy creek are highly variable with standard deviations between 180 and 254 m whereas standard deviations of mean distances range between 63 and 168 m for the other three watersheds sandy creek is the only study area in the deposition dominated coastal plains with lower relief and more agriculturally controlled drainage the other three study areas have higher topographic relief and predominantly erosional conditions consequently the sandy creek drainage model is more prone to error and variability than drainage models for the other watersheds having more deeply eroded channels and rugged terrain that is not as susceptible to man made drainage controls the channel networks derived from the pm method using the k curvature thresholds between 0 1 0 05 and 0 01 represent a broad range in extent from a sparse central channel with few branches to a dense web including nearly every valley and cleft the k threshold that was found to perform best for the rowan area is k 0 05 setting k to 0 1 reduces the network by more than half and a value of 0 01 similarly more than doubles the network length the rowan pm results k 0 05 generally extend further up valleys than the csa results yet the networks are very similar the k value found to perform best for the other areas is k 0 1 the asheville csa results are slightly more dense than the pm results and like rowan are similar in density the gaston area pm network is a good deal more sparse than the csa results in 6 of the 10 valleys with fv stream heads the extracted channel heads are more than 50 m down valley the sandy area pm results align better with the fv and apparent initiation points than the csa results as can be seen in table 3 and 4 the sandy area has the least dense channel network as well as irrigation canals and wetlands that create surface curvature anomalies which likely explain the csa over extraction 4 summary the csa strategy estimates stream channel network extent using cross section standard deviation of curvature values sk and tukey s upper inner fence the initial drainage network is extracted with a flow accumulation model set to over extract drainage channels from high resolution dem data and the csa method prunes the network back to identified channel heads this automated network and channel head extraction method is shown to perform well when compared to fv stream heads and channel heads manually identified from remote sensing data the results are similar in accuracy to the pelletier method which requires user defined thresholds it is noted that both the csa and pelletier methods appear more reliable in terrains with higher topographic relief and erosional fluvial conditions than in flatter coastal or flood plain environments with depositional fluvial conditions in general the methods rely on natural erosional and geomorphic channel development conditions and as more engineered structures alter the natural drainage conditions greater variation in results are expected and more complex techniques must be employed the sk value appropriate for discerning likely stream channels will be controlled by geomorphology dem resolution and desired stream permanence detection the simulated channel sk values show that high curvature values will often be associated with narrow channels the sk response to narrow channels is beneficial for constraining network extent but channel shape and sk values will be terrain specific using the 0 5 times tukey s upper inner fence threshold for detecting outliers has the benefit of raising the detection threshold in areas with dense channels and vice versa while not being affected greatly by the distribution of stream and non stream sk values the sk threshold used in the csa requires a minimum catchment channel density that has not yet been defined it is likely that such low channel density would only be encountered in extreme environments and that it could be overcome by increasing catchment size identification of bank edges in cross sections is being explored and has the potential to improve stream identification and offer more detail on channel classification and change detection geomorphic floodplain boundaries could be used to constrain cross section width work on the automated identification and breach of roads in flow models is needed channel confluences remain a challenge and a source of false positives especially along larger flood plains these segments will likely require unique treatment in future tool development future work needed includes testing of alternate curvature formulas testing dynamic cross section spacing and improvement of the change point detection strategies the css model shows promise and the inclusion of a strategy for multiple change point detection and optimized significance levels are needed declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we are grateful to the editor and reviewers of this journal we are also grateful to andrew kiley for comments that improved this paper this work was supported by the u s geological survey mendenhall research fellowship program 
25988,headwater streams account for more than half of the streams in the united states by length the substantial occurrence and susceptibility to change of headwater streams makes regular updating of related maps vital to the accuracy of associated analysis and display here we present work testing new methods of completely automated remote headwater stream identification using metrics derived from channel digital elevation model dem cross sections a jump in standard deviation of curvature sk is found to correlate with the presence of stream heads field and remotely validated stream and channel initiation points from 4 diverse study areas in north carolina as well as a simulated surface are used to test the sk findings the sk value within individual catchments equal to 0 5 tukey s upper inner fence is found to be a reliable threshold for identifying the upslope extent of channels in varied landscapes keywords lidar dem stream head headwater change point cross sections disclaimer any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government 1 introduction headwater streams make up over 50 of streams by length in the conterminous united states fig 1 and inaccuracies in their representation can be a significant cause of error in stream statistics and riparian zone distribution nadeau and rains 2007 the small streams have a large impact on waterways and ecosystems given their proportion of total stream length in a catchment and resultant sediment and nutrient contributions as well as species habitat and hyporheic and riparian zones naiman et al 1993 understanding the impact of the contributions made by small streams is necessary to accurate hydrologic modeling for sediment transport estimation flood control and other hydrology management tasks the curation of accurate headwater maps is challenging given the extensive distribution small scale and dynamic nature of the channels such water courses are regularly horizontally and vertically altered in single extreme weather events small shifts in climate conditions can have large effects in the length of low order streams with shifting extent due to change in groundwater levels and precipitation wohl 2018 small streams and drainage patterns are also often altered by anthropogenic disturbance such as agriculture forestry urban development and water usage change the factors listed above make regular update of headwater maps necessary in this work we present a strategy for automated pruning of flow accumulation lines and extraction of stream heads using open source tools including python and qgis qgis development team 2019 to support the regular update and validation of the national hydrography dataset nhd automation is needed for the regular validation and update of the nhd the nhd is a digital database of surface water features in the united states the original nhd was compiled from digital cartographic data created from topographic maps moore and dewald 2016 hydrographic data compiled from topographic maps such as nhd have varying degrees of accuracy and may not be suitable for some hydrologic regulatory and engineering uses due to inconsistent drainage density and missing headwater content colson et al 2008 fritz et al 2013 stanislawski et al 2015 updating of these features is ongoing and involves extraction of drainage networks from digital elevation models dem with pixel resolution between 1 and 10 m lidar light detection and ranging point cloud data are being collected at quality level 2 or better for the united states excluding alaska and these data provide for 1 m digital elevation model dem generation along with reflective properties and multi return structure from the point cloud the point cloud data provides insight into fine scale surface features that can be useful for surface water mapping shavers and stanislawski 2018 an important component of headwater mapping is determining the channel or stream head the stream head is defined by some as the initiation or origin of perennial stream flow and channel head as the initiation of surface flow concentration and sediment transport jaeger et al 2007 wohl 2018 the upstream initiation of flow or transition from non confined flow to well defined flow path can also be referred to as the stream head hancock and evans 2006 others describe the stream head as the concentration of active flow at any given time dietrich and dunne 1993 still others use the term flowhead for the upstream extent of active flow whiting and godsey 2016 whereas channel head is generally accepted as the upstream extent of detectable regular concentrated flow i e ephemeral stream head here channel head defined as the initiation of detectable surface flow and sediment transport will be used unless specifying intermittent perennial of ephemeral stream heads a great deal of research has been devoted to the development of methods for surface water mapping and the derivation extraction of stream lines and drainage channels from remotely sensed data with equally copious degrees of success tribe 1991 heine et al 2004 james et al 2007 james and hunt 2010 fritz et al 2013 jensen et al 2018 hamada et al 2016 julian et al 2012 pirotti and tarolli 2010 clubb et al 2014 hooshyar et al 2016 hfle et al 2009 shavers and stanislawski 2018 yet little work exists addressing strategies for automated channel head derivation clubb et al 2014 offer a comprehensive review of related literature and test several methods with the aim of automated headwater modeling using elevation data yet all of the methods tested require multiple user designated parameters the results of the study show that the pelletier method pelletier 2013 performed among the best of the methods tested and proved the least sensitive to changing parameters clubb et al 2014 hooshyar et al 2016 present a strategy for automated channel modeling using contour curvature the method determines a curvature k threshold for skeleton delineation by taking the maximum number of convergent positive k clusters with increasing k value following network skeleton generation channel cross sections for each tributary are analyzed for significant clustering to determine the transition from non channel to channel and location of channel head the contour k method is reported to perform comparably to the pelletier method yet the authors note that the contour k method was developed for landscapes with minimal human impact and well defined distinctions between unchannelized valleys hooshyar et al 2016 the work presented here builds upon earlier work by testing a process for automated identification of headwater stream channels and their channel head locations using analysis of cross sections cs james et al 2007 hooshyar et al 2016 to prune over extracted flow accumulation networks sangireddy et al 2016 the channel cross section analysis csa methods described here are fully automated methods for stream channel validation and channel head detection using high resolution elevation data the analysis generates channel cs statistics from unfiltered and filtered dems and the values are compared to field validated stream head and remotely derived channel head data from four topographically diverse study areas it is found that a jump in cs standard deviation sk value correlates with the presence of channel and stream heads with channel heads aligning slightly better the field validated intermittent and perennial stream heads used in this work are stream head estimates gathered from single observations and provided by the north carolina department of environment and natural resource channel heads are identified manually through analysis of elevation and image data the csa is also tested on a simulated stream channel to illustrate the response of the elevation metrics to channel development variations of surface curvature such as plan and profile curvature are used in other stream and channel extraction strategies julian et al 2012 jensen et al 2018 passalacqua et al 2010 tarolli and fontana 2009 test the use of multiples ranging from 1 to 3 times standard deviation of landform curvature as a curvature threshold for stream head identification shavers and stanislawski 2018 first introduced the standard deviation of curvature along cs as a gauge of channel development for stream mapping this work tests the sk metric on multiple landscapes with varying elevation relief and forest cover methods are analyzed for automating the analysis signal processing strategies for change point cp detection along individual modeled channels are tested with mixed results while filtering channel skeletons using a sk threshold is more computationally efficient and returns good results the sk value equal to 0 5 times tukey s upper inner fence tukey 1977 for individual catchments is shown to be useful as a threshold for pruning flow accumulation models and cp detection methods appear useful for stream head detection tukey s upper inner fence is an outlier detection method detailed by brant 1990 the workflow described here is implemented with open source tools and python programming language and uses lidar derived elevation data and road shapefiles from the u s geological survey usgs national transportation dataset as the inputs csa is rapid processing technique compared to alternative approaches that may involve processing entire dem datasets elevation and derivative raster maps require large storage capacity when working at a regional scale the focus on channel features in large part avoids the noise introduced by other land surface features thus the strategies tested here have the potential for completely automated and computational efficient channel network modeling in diverse environments for comparison the four study areas are also analyzed using the pelletier method pelletier 2013 and the results reported 2 methods the strategy for headwater stream identification tested here applies the csa the processes are optimized using the rowan county dataset the rowan site is located in the north carolina piedmont region and has a dense network of field validated stream head locations the topography of the piedmont represents a literal and figurative middle ground between the lower relief coastal plain to the east and higher relief appalachian region to the west the parameters that are determined using the rowan site include best fit elevation metric cs size and spacing and sk values for stream fitting see section 2 2 the workflow is then tested on the remaining three study area s in order to identify trends and the potential for unsupervised automation 2 1 study areas and data the study areas are four watersheds spread across north carolina fig 2 the rowan area has an elevation range between 195 and 255 m with an area of 6 3 square kilometers the area is located in the piedmont region of central north carolina with primarily forest landcover with some agricultural and residential areas the gaston area is also in the piedmont and has an elevation range between 185 and 250 m with an area of 2 8 square kilometers the land cover is primarily forest with some residential areas the asheville study area has an elevation range between 630 and 1170 m with an area of 20 7 square kilometers the asheville watershed is located in the mountainous appalachian region of western north carolina and has primarily forest landcover the sandy creek watershed has an elevation range between 2 and 62 m and an area of 7 5 square kilometers the area is located in the coastal plain region of eastern north carolina with agriculture wetland and forest landcovers the field validated stream head validation data consists of field collected intermittent and perennial flow initiation points determined by the presence of benthic species and geomorphology the field data were supplied by the north carolina department of environment and natural resources ncdwq 2005 the sandy creek points were collected in 2010 gaston in 2014 rowan between 2013 and 14 and asheville between 2010 and 2017 with the majority between 2010 and 2011 the time difference between lidar collection and field work must be considered as a potential source of error a 5 m buffer around stream head locations is used to address potential discrepancies in gps location apparent channel head validation data are manually identified channels with fv points are analyzed using elevation and image data to identify signs of concentrated flow and channel formation the inputs for the csa are national transportation dataset road and rail shapefiles ntd 2019 and dems derived from lidar data made publicly available by the usgs 3d elevation program 3dep the lidar is quality level 2 ql2 heidemann 2018 having 0 71 m maximum nominal pulse spacing or better and collected in leaf off conditions ql 1 geiger mode lidar stoker et al 2016 with a maximum of 0 35 m maximum nominal pulse spacing is used here with the exception of the sandy creek area where ql2 is used ashville gaston and rowan lidar collections took place between 2016 and 2017 and the sandy creek collection occurred in 2014 2 2 cross section generation the first step in the csa is generation of the dems from lidar point cloud data the dem s are generated using inverse distance weighting from the lidar classified ground points all of the dems have a minimum non vegetated vertical accuracy nva of 19 6 cm cm at the 95 percent confidence level 10 cm rmsez heidemann 2018 the next step is the generation of elevation derived lines using the open source geonet tool sangireddy et al 2016 the lines are extracted with a standard flow accumulation threshold of 1000 cells or 1000 m squared given the 1 m cells this value is used as an approximate minimum value for regions in the conterminous united states the lines are over extracted with a goal of intersecting all stream heads the resulting networks fall within 30 m of all but one of the stream heads with the exception found in the ashville area channel cross sections cs are placed along and perpendicular to flow accumulation lines with a spacing of 3 m and a width of 40 m i e one cs every 3 m and each extending out 20 m on either side of the flow accumulation lines the cs spacing is used to gain the greatest detail possible while minimizing noise and redundancy given the 1 m resolution and the size of channel features 3 m cs spacing is found here to be optimal data resolution and feature scale also inform cs length or width across the channel being measured stream channel and bank shape may inform the results of this work but other riparian corridor morphologies may also be indicative of stream presence and contribute to the results work by the united states forest service merritt et al 2017 suggests a minimum cs width of 6 m for analysis of headwater streams studies of riparian zone vegetation variation patterns clinton et al 2010 suggest the riparian corridor can have a width of up to 40 m a cs wider than 40 m could be used yet the density of the channel features and the geomorphology observed here make the 40 m width optimal i e describing the channel slopes with minimal overlap the cs line feature elevation values are extracted from the 1 m lidar generated dem with 80 values evenly spaced along the cs the transportation layer is a set of road features from the usgs national map vector database which is used here to generate a 20 m wide buffer around roads an intersect process removes individual cs that intersect the road buffer this is done because roads often shaped like inverted streams in profile can create false positives for the csa method there is a chance that channel features will not be identified by the flow accumulation process optimization of the flow modelling and road masking will be addressed further in following sections several cs metrics were tested for correlation with the presence of stream and channel heads including area under the curve area divided by height average slope maximum slope average curvature standard deviation of curvature sk and curvature coefficient of variation the listed metrics were tested on unfiltered dems and dems smoothed using a nonlinear diffusion equation catt et al 1992 the area under the curve or integral of a cs is the sum of the elevation sample points minus the minimum elevation sampled for a given cs slope classification is a common measurement in geomorphic studies and is simply the difference in elevation over one given that the sampling is evenly spaced at 1 m intervals through visual analysis the change in value of the sk statistic was found to best correlate with the presence of channel and stream heads and is therefore employed in the automated analysis testing there are a number of strategies for calculating curvature goldman 2005 the method applied here for calculating the curvature k along a cs is the derivative of the tangent equation 1 kreyszig 1991 here the tangent line is fit to three consecutive sample segments along the cs k can be described as the inverse of the radius of a circle fit to a curve or radius of curvature where the smaller the radius becomes the larger the curvature value values of k typically range from 1 to 1 and are close to zero positive k values correspond with convex curves while negative values correspond with concave or downward curving surfaces or lines negative k is often used in channel delineation jensen et al 2018 julian et al 2012 as these values often decrease with channel formation and incision negative curvature value extremes can also be useful as indicators of bank formation and increasing longitudinal slope from the cs curvature values standard deviation is calculated for each individual cs table 1 1 k d 2 y d x 2 1 d y d x 2 3 2 2 3 stream simulation in order to assess the response of sk to the idealized development of a stream channel from a flat surface to a widening channel a first order stream channel is modeled using equation 2 the gaussian kernel applied above 0 6 times the total channel depth 0 6 z is a cumulative sum that decreases in depth with increasing sigma value  x is a given horizontal distance from the line origin  or channel head here the origin is 0 the formula applied below 0 6 z generates a channel profile with increasing depth and width the modifier value c allows the increase in simulated channel width with a flattened channel floor and is defined as 0 6 z 4 0 6 z  the resulting channel derived here with a total depth of 6 3 approximates cross sections of a channel that incises with increasing distance downstream and increased flow at a depth of 6 the incision slows and the channel widens with increasing bank erosion fig 3 the simulation values are unitless and are only used for plotting 2 f  0 0 1 e x  2 2 c 2  0 6 z 0 6 z e x 2 2  2 2   2  0 6 z c 0 6 z 4 0 6 z  2 4 change point detection cp detection is a strategy typically applied to identify anomalies in time series or signal data while signal processing is typically discussed in relation to disciplines such as electrical or medical research geomorphic features often present patterns that resemble time series signals and have been studied using signal processing methods by others wang 1998 van dijk et al 2008 a valley profile series moving down slope can be thought of progressing linearly with increasing width and or depth a deviation in this signal from the trend would cause a shift in the response such shifts could correlate with a knickpoint depositional features or other channel formation features resulting from concentrated flow here we process evenly spaced cs statistical data as signals to attempt to automate identification of shifts in channel structure in response to the presence of water flow the signal response that appears to coincide with stream heads in the model development stage is a jump in sk mean and variance magnitude and variance shift in signal data are common phenomena and a good deal of work has been devoted to detecting these shifts truong et al 2019 tsay 1988 two methods are tested here for use in identifying significant shifts in mean or variance that occur near stream heads the cp analysis begins on channels at the perimeter of catchments by extracting single channels beginning at the point in a channel where the flow accumulation reaches 450 000 cells following the channel upstream the 450 k initiation value was determined by comparing the flow accumulation of all field validated stream heads of the 200 stream heads only 3 had flow accumulation values greater than 450 k and the median is 50 k when confluences are encountered the path with the highest flow accumulation value was followed for initial analysis the automated determination of stream head location from cs statistics is tested using two methods adapted from time series cp detection equations the cumulative sum of squares css method uses a normalized cumulative sum of squares dk function equation 3 4 plotted against distance down channel to identify peaks or troughs that correlate with proportionally large changes in sk value inclan and tiao 1994 here dk sqrt t 2 d k c is the css of a series of values a k is a given value k 1 t and t is the total number of values the asymptotic response of dk allows the maximum absolute value of dk to be tested for significance with boundaries associated with specific significance levels inclan and tiao 1994 used simulated series of varied length to develop significance bounds 3 d k c k c t k t 4 c k t 1 k a t 2 the second method is the pruned exact linear time pelt method killick et al 2012 the pelt method is computationally efficient using a method that aims to minimize a linear cost function for segmenting the data by statistical properties here the linear cost function is a gaussian kernel or radial basis function the penalty function found here to be optimal is the log of the length of the segment where killick et al 2012 suggest twice the negative log the method is implemented using the ruptures open source cp detection program ens paris saclay 2017 the analysis returns a list of integer cps ranging from 0 to 6 the csa uses these values to determine weights of the level change the variance and mean of each data segment separated by the cps is normalized by calculating the difference divided by the normal of the vector of two segments about each cp these values are multiplied to give a score or weight for each detected cp the cp algorithms must determine the location of cps and whether the found cp is significant suggesting that the cp represents a stream head the performance of the models for cp significance is assessed using decision matrices and the geometric mean g mean section 3 2 kubat et al 1998 the g mean is calculated as g mean g square root of acc acc where acc is the accuracy on positive variables also known as recall channels with stream heads present and acc is accuracy on negative variables channels without a stream head higher g mean values indicate higher accuracy 2 5 cs filter a cs filter test was developed to filter the flow accumulation skeleton and determine the location of stream and channel heads using the rowan area to develop the model the sk value of 0 5 times tukey s upper fence equation 5 tukey 1977 was selected as the cs threshold cs that overlap adjacent channels were first excluded from the dataset feature segments with all cs falling within 20 m of another channel are also excluded finally features with 10 or more cs having sk values above the threshold are identified as the extent of the channel network the extent determined all features up channel from there are eliminated while all features down channel are retained to preserve the connected network fig 5 the upstream extent is pruned to the threshold cs 5 s k t h r e s h o l d 0 5 interquartile range 1 5 m e d i a n s k 2 6 pelletier method the pelletier method pelletier 2013 uses a tangential curvature raster threshold for channel extraction there are three main steps in the pelletier method the first step requires filtering of the dem using an optimal wiener filter the second step is generation of the curvature raster and channel skeleton with pixels having positive values identified as part of the channel network the final step is application of a k threshold to reflect appropriate channel density pelletier 2013 suggests that a threshold value of 0 1 is broadly applicable yet others suggest the value should vary with landscape condition tarolli and fontana 2009 in addition to the k threshold the pelletier method also requires a skeleton minimum drainage area though this value has less bearing on the final results clubb et al 2014 the pelletier method is implemented using the open source lsdtopotools lsdtt channel extraction program clubb et al 2017 the default minimum drainage area of 400 m2 is used the default k threshold used by lsdtt is 0 01 here k values of 0 1 0 05 and 0 01 are tested for optimal channel network extraction 3 results and discussion 3 1 stream channel simulation the sk statistic is generated for the resulting simulated channel css the location of the peak in the scatter plot of sk vs distance downstream here cs count fig 3 corresponds with the most extreme downcutting before the channel widens the trend indicates that the highest sk values resulting from channel formation will be found among narrow channels yet the sk values are a function of dem and sampling resolution a dem cell size or elevation sample spacing that is wider than the channel will not resolve narrow deep channels well the simulated channel sk values demonstrate the distribution of values given different bank structure and channel development as can be seen in the plot of the simulated channel fig 3 valley formation and bank shoulder structures lead to higher sk values the rise in sk value is expected given that as the channel deepens and angles or changes in slope increase the sk increases fig 3 shows that a maximum sk value is likely to occur in headwater channels where surfaces allow for down cutting exceptions to the even distribution seen in fig 3b are likely to be the result of anomalies in the data and therefore beneficial for source data and results validation outlier values may also be the result of anthropogenic surface features dams flood walls and other infrastructure may present surface bends and resulting radius of curvature values smaller than is found in the natural channels the exponential curve in the scatter plot formed by the incising channel peaks where the base begins to widen the values begin to lower after the peak is reached yet not at the rate that they increased building up to the peak the more gradual drop off and elevated floor reached in the vicinity of 140 on the x axis is due to lower radius of curvature values maintained by valley corners and bank ridges clearly the max sk value would be an important input for modeling and extracting stream lines from elevation in a natural environment the sk peak value is elusive for several reasons the small radius of curvature and elevated sk will often be found at the minimum of a v shaped formation often forming at the base of small or narrow channels not all channel systems will feature the narrow valley required and the dem resolution may obscure detection the common occurrence of anthropogenic features can also obscure a natural sk plateau 3 2 change point detection as noted by higher sk values for unfiltered dems than filtered dems table 1 the unfiltered dem returned noisier sk values than the filtered dem as is expected also expected is the preserved increasing sk value with increasing bank formation as can be seen in the simulated channel fig 3b the filtered dem proved to eliminate some common false positives due to built environment surface structure and makes interpretation of results clearer tables 2 and 3 show whether the cp detection methods determined significant cps are present in channels with the fv stream heads if the model finds a significant cp and there is a fv stream head in the channel it is scored as tp if there is no stream head it is scored false positive given that there are no fv ephemeral stream heads in the dataset this is a test of the presence of intermittent or perennial stream heads on a channel the distance along the channel between fv stream heads and cps identified by the css and pelt models is displayed in fig 4 as can be seen in tables 2 and 3 recall scores vary widely this is a common occurrence especially with data that has unbalanced true and false samples kubat et al 1998 therefore the g mean score is employed it considers true and false samples and weights them higher when there are fewer the cp detection models did not perform well overall when considering whether there is a stream head present with the exception of the rowan area having g 0 725 with the css model the true negative and acc scores for the asheville area are promising the determination of cp significance is important for thinning flow accumulation networks and challenging given the common noise found in the sk signals especially in mountainous areas the accuracy of the detected stream heads 56 and 50 of the stream heads for the pelt and css models respectively can be seen in fig 4 with roughly 50 of detected stream heads falling within 50 m of the fv stream heads the negative values in fig 4 represent instances where the fv stream heads are up channel from the modeled points indicating an under extracted network positive values are instances where the modeled location is up channel from the fv point the occurrence of a positive skew is expected given that the channel heads or ephemeral stream heads are underrepresented in the data yet often develop well defined geomorphic characteristics yet the opposite appears to be the case judging by fig 4 the negative skew is caused for the most part by sk jumps occurring as a channel nears a confluence and encounters a developed flood plain or wider and deeper channel the large jumps are located at the end of channels leading to modeled cps or stream heads far down channel from fv stream heads and resulting in negative values one possible strategy to remedy inaccurate cp location is an adaptation of the model to account for multiple cps which also introduces increased complexity 3 3 cs filter results the cs filter results are presented relative to the fv stream head data and apparent channel head locations apparent channel heads are identified on channels with fv points using manual analysis of elevation and image data there is a strong trend of over extracting lines relative to fv points the over extraction relative to fv points is expected given that the fv points are represented as intermittent and perennial stream heads the results are more accurate as well as trending towards under extraction relative to apparent channel initiation the result histograms spread fig 6 is distributed wide due to outliers in over and under extraction the lines generated by the csa are best aligned with the fv and apparent channel heads in the two piedmont locations rowan and gaston fig 6 one channel in the rowan area has a distance of nearly 300 m between the fv and apparent stream head the dem shows a broad channel 20 m wide in places a culvert under a road and a well defined channel head upstream from the fv stream head the results for the ashville study area show many places where the channel network is under extracted though often by a matter of meters fig 7 a the under extraction is generally the result of channelization and flow concentration occurring high in the catchment incorrect flow accumulation is responsible for under extraction in places a dense network of roads exists in the study area that are not included in the road feature layer used here it appears that less than 10 of roads visible in the elevation data are present in the national map roads layer roads generate several false positive channels where the flow accumulation lines incorrectly build the skeleton yet breaching roads in a dem at channel intersections can be less of a problem in higher relief areas the gaston results also indicate under extracted channel networks several small lakes are not handled consistently as a result of the channel structure up stream of the lakes fig 7b yet the lakes are correctly traversed with a flow line dissecting them in some cases roads also lead to inaccuracies in the gaston study area the inaccuracies are generally the result of flow accumulation deviations that contribute to unidentified channels the sandy area results fig 8 show several well defined channels that are not identified by the fv stream heads i e ephemeral channels there are extensive drainage or irrigation canals surrounding agricultural areas the canals are largely unidentified by the flow accumulation skeleton given the minimum contribution area where canals are extracted by the flow accumulation model they are clearly identified by the csa method wetlands also complicate the results the wetlands are not documented in the nhd here and identification is challenging yet where open water is visible relative results appear correct a disturbed or undulating surface in wetland environments resulting from inconsistent lidar return points reflecting from the saturated and or flooded surface can be seen and correlates with elevated sk values though not as a response to bank structure lakes here are not generally defined by the extracted channels given the 40 m cs width and 3 m spacing used here there is a potential to not identify stream heads on channels running parallel and within 20 m of each other this did not occur in this study but could be mitigated by variation in cs width with stream order or width 3 4 pelletier method the pelletier method pm for estimating channels heads performs comparably to the csa method as can be seen in fig 6 and table 4 fig 6 indicates that a majority of validated channel heads are less than about 100 m from modeled channel heads using either the csa or pm method likewise the average distance between modeled and validated channel heads is less than 100 m for the all study areas using either the csa or pm method table 4 except for the sandy creek watershed where results are relatively unreliable the average distance between modeled and manually interpreted channel heads is larger than 100 m for the sandy creek watershed using both csa and pm methods and average distances for sandy creek are highly variable with standard deviations between 180 and 254 m whereas standard deviations of mean distances range between 63 and 168 m for the other three watersheds sandy creek is the only study area in the deposition dominated coastal plains with lower relief and more agriculturally controlled drainage the other three study areas have higher topographic relief and predominantly erosional conditions consequently the sandy creek drainage model is more prone to error and variability than drainage models for the other watersheds having more deeply eroded channels and rugged terrain that is not as susceptible to man made drainage controls the channel networks derived from the pm method using the k curvature thresholds between 0 1 0 05 and 0 01 represent a broad range in extent from a sparse central channel with few branches to a dense web including nearly every valley and cleft the k threshold that was found to perform best for the rowan area is k 0 05 setting k to 0 1 reduces the network by more than half and a value of 0 01 similarly more than doubles the network length the rowan pm results k 0 05 generally extend further up valleys than the csa results yet the networks are very similar the k value found to perform best for the other areas is k 0 1 the asheville csa results are slightly more dense than the pm results and like rowan are similar in density the gaston area pm network is a good deal more sparse than the csa results in 6 of the 10 valleys with fv stream heads the extracted channel heads are more than 50 m down valley the sandy area pm results align better with the fv and apparent initiation points than the csa results as can be seen in table 3 and 4 the sandy area has the least dense channel network as well as irrigation canals and wetlands that create surface curvature anomalies which likely explain the csa over extraction 4 summary the csa strategy estimates stream channel network extent using cross section standard deviation of curvature values sk and tukey s upper inner fence the initial drainage network is extracted with a flow accumulation model set to over extract drainage channels from high resolution dem data and the csa method prunes the network back to identified channel heads this automated network and channel head extraction method is shown to perform well when compared to fv stream heads and channel heads manually identified from remote sensing data the results are similar in accuracy to the pelletier method which requires user defined thresholds it is noted that both the csa and pelletier methods appear more reliable in terrains with higher topographic relief and erosional fluvial conditions than in flatter coastal or flood plain environments with depositional fluvial conditions in general the methods rely on natural erosional and geomorphic channel development conditions and as more engineered structures alter the natural drainage conditions greater variation in results are expected and more complex techniques must be employed the sk value appropriate for discerning likely stream channels will be controlled by geomorphology dem resolution and desired stream permanence detection the simulated channel sk values show that high curvature values will often be associated with narrow channels the sk response to narrow channels is beneficial for constraining network extent but channel shape and sk values will be terrain specific using the 0 5 times tukey s upper inner fence threshold for detecting outliers has the benefit of raising the detection threshold in areas with dense channels and vice versa while not being affected greatly by the distribution of stream and non stream sk values the sk threshold used in the csa requires a minimum catchment channel density that has not yet been defined it is likely that such low channel density would only be encountered in extreme environments and that it could be overcome by increasing catchment size identification of bank edges in cross sections is being explored and has the potential to improve stream identification and offer more detail on channel classification and change detection geomorphic floodplain boundaries could be used to constrain cross section width work on the automated identification and breach of roads in flow models is needed channel confluences remain a challenge and a source of false positives especially along larger flood plains these segments will likely require unique treatment in future tool development future work needed includes testing of alternate curvature formulas testing dynamic cross section spacing and improvement of the change point detection strategies the css model shows promise and the inclusion of a strategy for multiple change point detection and optimized significance levels are needed declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we are grateful to the editor and reviewers of this journal we are also grateful to andrew kiley for comments that improved this paper this work was supported by the u s geological survey mendenhall research fellowship program 
25989,the kinematic runoff and erosion model version 2 kineros2 provides an urban component that is capable of modeling urban hydrology including green infrastructure gi practices at various scales the kineros2 model provides 15 parameters that can represent a variety of gi practices the goal of this study is to understand the sensitivity of the kineros2 model outputs infiltration peak flow runoff volume and outflow hydrograph to these 15 parameters across seven different precipitation events and at the watershed as well as the parcel scale a global sensitivity analysis gsa was performed using the variogram analysis of response surfaces vars framework to produce sensitivity metrics results from the time aggregate gsa indicate that parameter sensitivities follow trends based on precipitation intensity and duration time varying gsa indicate sensitivity of parameters to hyetograph shape especially the duration to maximum precipitation intensity the variation in sensitivity is influenced by the peaks and valleys in the precipitation hyetograph keywords kineros2 urban hydrology green infrastructure variogram analysis of response surfaces global sensitivity analysis 1 introduction green infrastructure gi or low impact development was pioneered by the department of environmental resources of prince george s county maryland to mitigate the urbanization impact of increasing impervious surfaces county and june 1999 as opposed to traditional stormwater management practices gi aims to preserve the pre development hydrology using a variety of cost effective on suite design techniques that store infiltrate evaporate and detain runoff in addition to stormwater management benefits gi practices are being used to mitigate a number of other urban issues such as heat island effects li et al 2014 norton et al 2015 santamouris 2012 yang and bou zeid 2019 and air pollution abhijith et al 2017 bottalico et al 2016 jayasooriya et al 2017 ortolani and vitale 2016 factors such as co benefits availability of space public interest government policies and funding influence the decision making process behind the type of gi its design and implementation zhang and chui 2018 in the context of hydrology gi is implemented to mitigate flood and water quality problems and for water augmentation dietz 2007 different gi practices can be implemented to achieve different benefits hopton et al 2015 for example retention basins rain gardens curb cuts infiltrating chicanes permeable pavements can help mitigate flooding and safeguard water quality whereas roof rainwater harvesting can result in water stored for later use this variety of gi practices creates an inherent uncertainty concerning the potential designs and implementations in an urban watershed a number of hydrological models have been used to represent urban hydrology and gi in order to assist various decision making processes for gi implementation zoppou 2001 elliot and trowsdale 2007 bach et al 2014 jayasooriya and ng 2014 without real world implementation data hypothetical modeling scenarios are guilty of using best case or ideal conditions for gi designs without considering the implications of uncertainty in terms of gi design parameters on the model output these scenarios may not always inform practical implementation moreover some design parameters may or may not have any influence over the model output if these design parameters are the basis of decision making the entire modeling exercise can be irrelevant if the parameters show no effect on the model output for the purpose of this study we focus on the kinematic runoff and erosion kineros2 model kineros2 is a spatially distributed complex physically based event driven model that simulates runoff and erosion for small watersheds smith et al 1995 goodrich et al 2012 https www tucson ars ag gov kineros kinematic wave equations are used to simulate overland flow over variable width planar or curvilinear model elements and concentrated flow in trapezoidal channels while the parlange 3 parameter equation is used to simulate infiltration parlange et al 1982 kineros2 can be used to simulate typical urban configurations using the rectangular urban element kennedy et al 2013 korgaonkar et al 2018 the urban element provides a number of overland flow areas that can be used to represent small scale urban characteristics including green infrastructure fig 1 these include 1 directly connected pervious dcp area 2 directly connected impervious dci area 3 indirectly connected impervious ici area 4 connecting pervious cp area 5 non contributing nc area 6 infiltrating retention basin areas rb dcp rb dci and rb cp 7 street half on to which the aforementioned overland flow areas contribute runoff and 8 a curb cut cc area to simulate retention and infiltration of runoff flowing down the street runoff from ici can be captured in a cistern to simulate roof rainwater harvesting whereas permeable pavements can be simulated by using the dci or the street half components with a constant infiltration rate for a detailed description of the kineros2 urban element see korgaonkar et al 2018 a large number of parameters are used in kineros2 to represent various properties and characteristics the variability in each of these parameters is capable of contributing to the uncertainty in the model outputs prior to the development of the urban component in kineros2 a global sensitivity analysis gsa was conducted to explore the relative influences of various parameter uncertainties on simulated output used for flash flood forecasting yatheendradas et al 2008 rainfall was determined to be the most sensitive factor followed by soil saturated hydraulic conductivity soil volumetric rock fraction and soil surface roughness the sensitivity of the model output to soil saturated hydraulic conductivity was confirmed by kennedy et al 2013 which utilized the urban component to simulate a small subdivision in arizona the assessment of sensitivity of the parameters that represent gi is vital in order to have confidence in using kineros2 to model gi practices there are a number of methodologies available to conduct sensitivity analysis of parameters in the context of earth and environmental system models norton 2015 pianosi et al 2016 razavi and gupta 2015 saltelli et al 2008 out of all the available methods for sensitivity analysis the variance based sobol method sobol 1990 and the derivative based morris method morris 1991 are the most popular methods razavi and gupta 2015 conducted a review of all the different methodologies and outlined two main issues in sensitivity analysis ambiguous characterization of sensitivity and computational cost in order to address these issues razavi and gupta 2016a 2016b introduced a theoretical framework for sensitivity analysis called variogram analysis of response surfaces vars razavi et al 2019 this methodology uses directional variograms and covariograms to characterize the global sensitivity of model response with respect to a model parameter varying within the feasible parameter space vars is computationally efficient statistically robust provides stable estimates with relatively small number of sampled points and accounts for spatial correlation in model response as parameters are varied a detailed background of the vars framework can be found in razavi and gupta 2016a 2016b in this study we conduct a gsa using the vars framework with a star based sampling strategy star vars razavi and gupta 2016b which provides a reliable low computational cost strategy to estimate sensitivity across a range of scales our study follows the performance metric free global sensitivity analysis procedure presented by gupta and razavi 2018 it can be summarized as follows with steps 2 4 5 and 6 implemented as part of the vars tool 1 identify the parameters of interest and model outputs 2 select a representative statistical sample of parameter locations using the progressive latin hypercube sampling method plhs sheikholeslami and razavi 2017 these locations will be the star centers to increase the sample size using the star sampling strategy 3 run the hydrologic model using each set of parameters created in step 2 4 construct the generalized global sensitivity matrix ggsm razavi and gupta 2019 using each model output parameter sets and the number of time steps to analyze sensitivity analysis results 5 conduct the total period time aggregate parameter importance analysis to understand the sensitivity of model responses to the various parameters 6 conduct the time varying parameter importance analysis in order to understand the effect of parameter variance during various time steps of the model output the temporal nature and complexity of modeling rainfall runoff using kineros2 model makes it an ideal model for the use of the vars tool the total period time aggregate parameter importance can provide summary statistics regarding overall sensitivity of model responses to variations in parameters this is especially helpful to identify parameters that can be varied or fixed during model calibration the time varying parameter importance can be useful to understand how the model and its parameters perform at every time step of the simulation precipitation serves as the driving input to the kineros2 model and due to its inherent temporal variability the time varying parameter importance analysis can highlight the relationship between the model response and parameters for varying precipitation levels and storm hyetograph shapes i e high intensities early late in the storm the overall goal is to highlight the varying sensitivity of the model responses to the various parameters across each timestep of the simulation as well as the entire duration these sensitivity indices are particularly helpful in metric free applications as is the case in this study vars uses variogram and covariogram functions to generate a directional variograms associated with each model parameter the mathematical integration of this directional variogram creates a comprehensive set of global sensitivity indices called integrated variograms across a range of scales or ivars specifically the ivars50 index is the most comprehensive variogram based index for global sensitivity that captures the full range of perturbation scales for the parameters under consideration razavi et al 2019 using the vars tool to perform a gsa on the kineros2 model our research focuses on answering the following questions 1 how sensitive are the kineros2 model outputs to parameters representing gi practices 2 do different precipitation events influence the sensitivity of the kineros2 gi parameters 3 how does the sensitivity of the kineros2 gi parameters compare at the watershed scale and at an individual parcel scale 4 can a gsa on kineros2 parameters representing gi inform decision making 2 methods the la terraza subdivision located in sierra vista arizona fig 2 was selected as the study area based on previous research parameters from kennedy et al 2013 and validated model from korgaonkar et al 2018 sierra vista is located at an elevation of approximately 1300 m and has an average annual precipitation of 360 mm with an annual mean temperature of 17 4 c the subdivision consists of 66 housing lots spanning 14 ha with an average parcel size of 1780 m2 standard deviation of 265 m2 average roof area of 380 m2 standard deviation of 59 m2 and average driveway area of 108 m2 standard deviation of 43 m2 with 7 3 m wide asphalt streets the total watershed area can be broken down into 17 roof area 16 street area and 5 driveway area totaling to 38 impervious area on the watershed seven observed precipitation events were selected based on their intensity shape duration and seasonality table 1 names of the event are in the yyyymmdd format events 20071130 and 20100121 are representative of typical low intensity long duration events occurring in the winter months events 20080725 and 20140704 are medium intensity short duration events representing the summer monsoon months in this region events 20140826 20150707 and 20140711 have larger return periods for the purpose of comparison these events are categorized as low intensity 20071130 20100121 20080725 and 20140704 and high intensity 20140826 20150707 and 20140711 note that event 20140711 corresponds to a 200 year 3 hour event from noaa atlas 14 point precipitation frequency estimates for the sierra vista station and can be considered as an extreme event for the purpose of discussion fifteen parameters used to represent various green infrastructure practices in the kineros2 urban element table 2 were selected for the analysis peak flow runoff volume outflow hydrograph at the watershed outlet and infiltration over the entire watershed were selected as model responses to test against sensitivity of parameters due to the lack of physical gi implementations parameter ranges were determined based on available area given the physical characteristics of the parcels literature and back of the envelope values to test the maximum range of parameters each of these parameters are independent of each other and are assumed to be uniformly distributed this assumption is valid as any value specified in the range for any of the parameter has an equal probability of occurrence in an actual implementation scenario a brief description of each of the fifteen parameters and their ranges follows parameter nc represents fraction of dcp that is disconnected and allows isolation and infiltration of all of the precipitation that falls on this area an upper range of 1 was determined as nc can be the entire pervious area represented by dcp hcd and hcs represent permeable pavements on driveways and streets respectively and the range of 0 500 mm h was selected based on infiltration rates reported in selbig and buer 2018 hfrac represents the fraction of roof area that contributes to a cistern whose volume is represented by parameter harv hfrac range was determined to be 0 to 1 since it is possible for the entire roof to contribute to a cistern a range of 0 40 m3 was determined for harv based on commercially available cistern sizes parameters rb dcp rb cp and rb dci represent fractions of parcel pervious areas converted to retention basins and vol dcp vol cp and vol dci represent the retention capacities of these basins hcb represents the saturated hydraulic conductivity of the retention basins and its range was selected based on hydraulic conductivities representing various materials from impervious surfaces 0 mm h to sand 210 mm h rawls et al 1982 parameter ranges for rb dcp rb cp and rb dci were selected as 0 to 1 because the entire available area from dcp cp or dci can be converted into a retention basin since available area for basins can be limited the depth determines the volumes for each of these basins a range of 0 50 m3 was selected for the volume parameters as depth of basins can be highly variable based on the available area cc represents the fraction of the street area converted to a retention basin with a parameter range of 0 0 5 keeping in mind right of way restrictions on streets allowing traffic movement vol cc and hcc represent the retention basin capacity and saturated hydraulic conductivity respectively the ranges of hcc and vol cc were determined similar to the ranges of the aforementioned parameters hcb and those representing on parcel retention basins respectively in addition to the watershed scale sensitivity of the kineros2 gi parameters were also tested against the aforementioned model responses for a single representative parcel pid 65 fig 2 due to the small size of the watershed and the relative homogeneity of the dimensions of the various parcels construction of houses and landscaping pid 65 was considered a suitable candidate to represent the remaining parcels pid 65 has a parcel area of 1700 m2 with a roof area of 420 m2 and driveway area of 101 m2 these values are within one standard deviation of the aforementioned mean values for the entire watershed the only exception to the homogeneity of the parcels would be the corner parcels that have irregular or triangular shapes fig 2 additionally pid 65 is a flow initiating parcel and provides a good examination of water balance without any incoming upstream flow the agwa urban tool was used to setup the kineros2 model using geospatial data as discussed in korgaonkar et al 2018 this configuration acts as a template for use in the numerous simulations as part of the gsa next the vars tool was setup to run a time aggregate analysis using infiltration peak flow and runoff volume as well as a time varying analysis using the output hydrograph in both cases the first step was to generate sets of kineros2 gi parameters using the star vars sampling strategy in vars vars was run with 100 star centers and a sampling resolution of 0 1 step size equal to 10 of the parameter range for the parameter space in table 2 razavi and gupta 2016a 2016b and razavi et al 2019 recommend a smaller value of h typically 0 1 to prevent degradation of the constant mean assumption that is possible at larger scales the sampling resolution of resolution of 0 1 is the distance between the location of the pairs of points in the parameter space and ensures that no parameter ranges are excluded from the sampling process additionally razavi and gupta 2016b also discussed the robustness and probability of failure based on the number of model evaluations their results indicated a success rate of 100 for star centers as low as 35 considering the computational power available we decided to use 100 star centers to generate 13 600 parameter sets which we felt would effectively capture the variability in the parameter space the vars tool automatically rescaled the parameter ranges between zero and one the output from this step resulted in 13 600 parameter sets that were used to create and execute the kineros2 simulations results were compiled into total infiltration peak flow and volume and output hydrographs for both the entire watershed as well pid 65 to serve as input to the vars tool which generates and analyzes the ggcm finally results were compiled using the ivars50 index eq 1 was used to calculate relative sensitivity for each parameter i using the ivars50 metric 1 r e l a t i v e s e n s i t i v i t y i i v a r s i i 1 15 i v a r s i note that the magnitude of the sensitivity is not featured in the results instead we focus on the relative sensitivity of the parameters 3 results 3 1 time aggregate gsa the time aggregate relative sensitivity was computed for simulated infiltration fig 3 subplot a peak flow subplot b and total volume subplot c at the outlet of the watershed left and from pid 65 right for all 15 parameters and for 7 precipitation events precipitation events are arranged in increasing order of maximum intensity from left to right note the similar trends in parameter sensitivity for the lower intensity events as compared to the higher intensity events in general parameters nc hcd rb dci and vol dci were the least sensitive parameters for all model responses across all events parameters with relative sensitivity of less than 0 05 were grouped together under the category other 3 1 1 infiltration in general roof runoff harvesting parameters hfrac and harv influence infiltration at both scales with dominant sensitivity for the low intensity events fig 3 subplot a parameter hfrac controls the surface area of the roof that contributes water into the cistern whose size is determined by parameter harv both these parameters influence the amount of precipitation that is intercepted before it is available for infiltration on the ground for the low intensity events hfrac is the most sensitive parameter with harv and hcb showing sensitivity as well parameter harv is more sensitive than hcb for the winter events whereas the reverse is true for the summer events the rainfall volume is distributed over a longer duration for winter events as a result the cistern is not overwhelmed in a short amount of time as is the case for the short duration high intensity summer events for the high intensity events hcb is the dominant parameter affecting infiltration for both the entire watershed and pid 65 additionally parameters rb cp rb dcp and cc also have relative sensitivity greater than 0 05 these parameters control the surface area of the retention basins thereby the area available for the water to collect and infiltrate parameter hcb represents the saturated hydraulic conductivity of the three retention basins on the parcels it is expected that the parameter that influences the infiltration rate would influence the amount of water that infiltrates into the ground 3 1 2 peak flow in the case of peak flow fig 3 subplot b there is a distinct difference between the parameter sensitivity at the watershed scale and pid 65 at the watershed scale for low intensity events cc is the most sensitive parameter followed by hcs and vol cc all three of these parameters control gi practices on the street cc controls the surface area of the curb cut retention basin vol cc is the total retention volume available and hcs controls the infiltration rate if the street was pervious on the other hand for pid 65 hcs is the most dominant parameter with the relative parameter sensitivity varying by event the parameter sensitivity can be ranked as hcb vol cp vol dcp for the winter events hcb vol dcp vol cp for 20080725 and cc vol dcp vol cp for 20140704 in each of these cases the available volume of the retention basins influences the peak flow downstream at the watershed outlet for the high intensity events at the watershed scale hcb sensitivity increases with increase in event size retention basin volume parameters have sensitivity ranked as vol cc vol dcp vol cp however at the pid 65 scale vol dcp is more sensitive than hcb except for event 20150707 and vol cp is the third most sensitive parameter for these larger events the parameters controlling the retention capacity of the basins as well as the hydraulic conductivity influence the peak flow 3 1 3 volume runoff volume fig 3 subplot c is sensitive to street gi parameters cc vol cc and hcs for low intensity events with the addition of retention basin parameters hcb vol dcp and vol cp for high intensity events at the watershed scale parameter sensitivity to output volume can be ranked as hcs cc vol cc for the low intensity winter events and as cc hcs vol cc for the summer events for pid 65 cc is the dominant parameter with hcs and vol cc both have relative sensitivity greater than 0 05 runoff volume is most sensitive to street gi parameters at both scales for low intensity events for the high intensity events retention basin parameters show relative sensitivity in addition to the street practices hcb sensitivity increases with increase in event size whereas vol cc and cc show a decreasing trend vol dcp and vol cp both have similar relative sensitivity across the high intensity events we can also see a higher sensitivity of parameter hcs at the watershed scale as compared to pid 65 possibly due to the cumulative effect of the street area across the watershed 3 2 time varying gsa time varying results of the relative sensitivity were derived using the ivars50 metric for outflow hydrographs for the entire watershed left and pid 65 right for each of the seven precipitation events fig 4 note that the maximum intensity of precipitation increases from top to bottom in general the variation in sensitivity of the parameters is smoother for the watershed hydrographs as compared to those for pid 65 parameters display more sensitivity at the parcel scale where the time of concentration is less compared to the watershed scale i e the time between precipitation falling on the parcel and runoff leaving the parcel as a result parameters show more sensitivity to variation in precipitation at the parcel scale additionally variations in sensitivity for a given parameter consistently follow abrupt changes in precipitation rate for the low intensity events fig 4 subplots a b c and d hcs cc and vol cc are the only three parameters that display relative sensitivity for winter events fig 4 subplots a and b where the hyetograph peaks after almost 5 hours parameters hcs and cc mirror each other where one increases while the other decreases and vice versa this can be attributed to the fact that hcs and cc are the only two parameters displaying sensitivity and the mirroring is due to the normalization of the sensitivity results at the watershed scale hcs has relatively higher sensitivity throughout both events except at maximum precipitation intensity where cc is more sensitive on the other hand for pid 65 cc is more sensitive with smoother variations as compared to hcs whose sensitivity varies significantly with peaks and valleys in precipitation intensity a third parameter cc vol shows relatively smaller sensitivity and is visible later in the event at both scales summer events 20080725 and 20140704 are similar in maximum intensity fig 4 subplots c and d respectively but differ in shape where the maximum intensity is at the beginning of the event 20080725 and the latter has maximum intensity towards the end of the event at the watershed scale parameter cc is more sensitive than hcs and vol cc for 20080725 in contrast for the 20140704 event the sensitivity of cc shows a decreasing trend while hcs keeps increasing the two parameters crossover each other at around the 45 min mark where the hyetograph displays intensities less than 10 mm h note that vol cc increases in relative sensitivity after the peak precipitation intensity in both cases at the parcel scale cc is dominantly sensitive as compared to hcs and vol cc for event 20080725 this dominance is also visible for event 20140704 but only during the first 30 min of high intensity in the event high intensity event 20140826 fig 4 subplot e has peak intensity at the beginning of the event whereas event 20150707 fig 4 subplot f has maximum intensity towards the end of the event both events have similar durations and maximum intensities parameters cc vol cc and vol dcp compete as most sensitive parameters at different time steps of the hyetograph cc starts as a dominant parameter with a decreasing trend both vol cc and vol dcp increase in sensitivity as the event progresses with the latter showing more sensitivity later in 20150707 event other parameters such as rb dcp hcb and cc each have less than 0 2 relative sensitivity and are visible after the precipitation subsides parameters hcc and hcs are also sensitive less than 0 2 especially in the early time steps of the hyetograph for the extreme event 20140711 fig 4 subplot g sensitivity of parameter cc drops off rapidly with increase in intensity of the event at the same time we can see a sharp increase in sensitivity of vol cc as the event further subsides parameter hcb becomes the most sensitive parameter followed by hcs this pattern is seen at both the watershed as well as the parcel scale 4 discussion a gsa is able to test how model responses change when model parameters are changed thereby revealing the parameters that have the most influence on the model response one of the key insights from this gsa would be that different model responses can show different sensitivities for different parameters in other words not all parameters may be relevant for a particular model response additionally parameters display varying sensitivities at various time steps of the model simulation due to this it is vital to determine the model responses the are most important and relevant before any modeling exercise is conducted this will enable the modeler to identify which parameters require more attention in terms of parameter ranges data collection and evaluation of model predictions as the complexity of a model increases the number of model parameters likewise typically increases it may not necessarily indicate that all of these parameters are essential or required in the model the gsa reveals interesting parameter behaviors in terms of relative sensitivity of parameters relationship to different precipitation events as well as scale each of these aspects are discussed in the subsections below 4 1 how sensitive are the kineros2 model outputs to various parameters representing gi practices parameters representing some gi practices do influence kineros2 output more than the others do this is evident from the influence of roof rainwater harvesting parameters hfrac and harv on infiltration permeable pavement parameter hcs on parcel retention basin parameters hcb rb dcp vol dcp rb cp and vol cp and curb cut retention basin parameters cc and vol cc on peak flow and volume roof rainwater harvesting parameters are responsible for intercepting precipitation falling on 17 of the total watershed area lower hfrac and harv values would contribute more runoff to cp and subsequently rb cp for infiltration and vice versa thus explaining the higher sensitivity for the infiltration output of the model on parcel retention basin parameters try to capture the runoff from 22 roof and driveway of the impervious area as well as the total pervious area of the watershed showing significant influence on model outputs similarly curb cut retention basins cater to 16 of the total watershed with high connectedness and their respective parameters show high relative sensitivity on the other hand certain parameters nc hcd rb dci and vol dci had minimum relative influence on the model outputs nc represents fraction of the pervious area that is completely disconnected and does not intercept any runoff from any impervious area if nc were to represent disconnection from impervious areas we can envision a larger influence on the model outputs driveways constitute only 5 of the total watershed area and thus parameters representing permeable driveways hcd rb dci and vol dci show relatively low sensitivity 4 2 do different precipitation events influence the sensitivity of the kineros2 gi parameters relative sensitivity of the parameters follows certain patterns as far as the precipitation events are concerned for the infiltration output sensitivity of the parameters for low intensity events follow the same trend as the depth of the precipitation event this is particularly evident from the increasing sensitivity of parameter harv with increase in precipitation depth from event 20080725 to 20100121 there is a visible trend for the low intensity events and another for the high intensity events specifically the retention basin parameters show higher relative sensitivity for the high intensity events additionally the long duration winter events 20071130 and 20100121 have similar relative sensitivity patterns that differ from the short duration low intensity summer events 20080725 and 20140826 the largest event 20140704 displays different sensitivity trends as compared to the remaining two high intensity events 20140826 and 20150707 indicating that higher intensity and larger total volume of an event can impact the sensitivity of model results to the parameters the shape of the precipitation event also has an impact on the parameter sensitivity to model responses and is more evident in the time varying gsa results for the high intensity events the sensitivity of many parameters increases after the maximum precipitation intensity if we consider the winter events the variation in sensitivity of the parameters is smoother except at precipitation intensity peaks in the hyetograph comparing events 20080725 and 20140704 the sensitivity of vol dcp shows a distinctive trend the sensitivity of vol dcp increases after the maximum precipitation intensity for 20140704 whereas its sensitivity is higher just before the maximum precipitation intensity for 20080826 since vol dcp directly relates to the capacity of one of the retention basins it appears that the amount of cumulative precipitation volume has an effect on its sensitivity 4 3 how does the sensitivity of the kineros2 gi parameters compare at the watershed scale and at an individual parcel scale the time of concentration is less at the parcel scale as compared to the watershed scale as a result we can see certain differences in parameter sensitivity at these scales the most prominent difference is for the time aggregate gsa results for peak flow for the low intensity events hcs is most sensitivity at the parcel scale whereas cc is dominant at the watershed scale for the high intensity events parameters for street gi practices have minimum sensitivity at the parcel scale from the time varying gsa results it is evident that sensitivity has more variations in slope at the parcel scale as compared to the watershed scale this can be attributed once again to the time of concentration at the watershed scale it takes rainfall considerably longer to reach the outlet on the other hand the path followed by rainfall to reach the outlet of the parcel is considerably shorter and hence the time of concentration is small as well at the parcel scale the effects of the parameters are relatively quicker as compared to the watershed scale where aggregation effects from multiple parcels dampen the sensitivity trends in an influent environment where watershed runoff response and runoff to rainfall ratios decrease with increasing drainage area the watershed acts as an attenuating filter where input variability becomes dampened this is evident from the smoother variation in sensitivity at the watershed scale as compared to the parcel scale additionally peaks and valleys in the hyetograph have a more dramatic effect on the sensitivity of the parameters at the parcel scale in terms of the time aggregate infiltration and volume results there are no noticeable differences when comparing the two scales and results follow similar trends at both scales this observation and the differences in peak flow results at the two scales can be attributed to the cumulative effects of connectedness in the watershed which is absent at the parcel scale 4 4 can a gsa on kineros2 parameters representing gi inform decision making there are two perspectives concerning decision making based on a gsa on parameters representing gi the first represents a hydrologic modeler s perspective to utilize gsa results when representing watersheds using kineros2 the second represents stakeholders designing and implementing gi based on these gsa results a hydrologic modeler would use gsa results to determine parameters that should be represented accurately in the kineros2 model more effort would be spent in field measurements in order to represent these gi practices accurately in the absence of field measurements the modeler would undertake a sensitivity analysis and model calibration to ensure model reliability in simulating results for example development has been shown to cause soil compaction decreasing infiltration gregory et al 2006 kennedy et al 2013 which could affect the parameters hcb and hcc if the modeler estimated these parameters based on soil texture without compaction the effectiveness of these gi practices could be over estimated additionally the modeler can simplify the model representation process by identifying parameters that do not display relative sensitivity for these parameters simple assumptions for values can be made knowing that their influence on model output is minimal based on the gsa results the kineros2 model can be useful to model scenarios that involve on street gi practices represented by parameters cc vol cc and hcc such as permeable streets curb cut retention basins and infiltrating chicanes and on parcel gi practices such as retention basins and rain gardens represented by parameters rb cp vol cp rb dcp vol dcp and hcb as well as roof runoff harvesting represented by parameters hfrac and harv model outputs show a higher sensitivity to the parameters that represent these practices fig 3 as a result the effect of the model implementation focusing on these parameters will have a more direct impact on total infiltration and runoff volume and peak flow at the outlet of the watershed and for a single parcel from stakeholders perspectives it would be valuable to know the dominant gi parameters that they can control and modify during implementation as well as a more realistic estimate of costs for example a homeowner may not be able to control the fraction of roof area contributing to a cistern however they can control the size of the cistern that captures roof runoff thereby influencing the amount of rainwater infiltrating on their parcel fig 3 subplot a similarly in the case of retention basins the area of the basin is largely dependent on available space and the hydraulic conductivity is largely dependent on the underlying soil but the depth of the basin can be modified with excavation at the time of implementation and the hydraulic conductivity hcb can be preserved with maintenance over time runoff volume and peak flow are sensitive to the depth of the basin as controlled by the vol cp and vol dcp parameters these parameters along with hcb are shown to have considerable influence on model response fig 3 subplots b and c respectively based on the gsa results the modeler should recommend that stakeholders focus on permeable pavements represented by parameter hcs and on street basins represented by parameters cc hcc and vol cc for downstream flood mitigation as these were the most sensitive parameters influencing runoff volume and peak flow fig 3 subplot b and c respectively any change in the values of these parameters will be reflected in the model output thereby allowing a better understanding of the efficacy of implementation of these gi practices many gi practices are retrofitted based on available infrastructure for example curb cuts divert water into basins that are dug out in existing areas alongside the streets or infiltrating chicanes are created on streets based on available area after considering right of way requirements additionally practices such as green roofs and roof rainwater harvesting are dependent on the available roof area in these cases the available area alongside the street or on the street as well as roof areas act as decision making factors modeling scenarios are utilized to gage the impact and effectiveness of various gi implementations if a certain parameter in the model is directly related to one of these decision making factors the gsa can inform whether a change in the value of this parameter can translate to real world implications based on the model output wagener and pianosi 2019 in our analysis parameter nc had no impact on the model outputs if gi scenarios were designed around disconnection the modeling exercise in kineros2 would be completely futile and would provide no new information to support disconnection designs 5 conclusions the sensitivity of kineros2 model outputs infiltration peak flow runoff volume and outflow hydrograph to parameters representing gi practices was tested by performing a global sensitivity analysis gsa using the vars framework with a star based sampling strategy star vars fifteen parameters in kineros2 representing different gi practices were tested using the ivars50 metric the agwa urban tool was used to setup the kineros2 model to simulate 66 parcels in the la terraza subdivision in sierra vista az seven different precipitation events varying in maximum intensity duration shape and seasonality were used to compare sensitivities across events results were compiled at the watershed scale as well as a single parcel key results are summarized as follows 1 parameters representing on parcel retention basins hydraulic conductivity surface area and volume of basin have strong influence on infiltration peak flow and runoff volume especially for high intensity precipitation events 2 parameters representing on street practices hydraulic conductivity of permeable pavement surface area and volume of curb cut retention basin have strong influence on peak flow and runoff volume for low intensity precipitation events 3 roof runoff harvesting parameters fraction of area contributing to cistern and cistern volume have a strong influence on infiltration for low intensity precipitation events 4 the difference in time of concentration at the parcel scale as compared to the watershed scale does play a role for the parameters influencing model outputs especially peak flows for low intensity events the variance in sensitivity is much sharper at the parcel scale and smoother at the watershed scale additionally for peak flows hydraulic conductivity of the permeable street is dominant at the parcel scale whereas the parameter controlling the size of the on street retention basin is dominant at the watershed scale 5 the type of precipitation events has an influence on the variation in sensitivity of parameters intensity and duration effects are visible in the time aggregate gsa results whereas shape effects are visible in the time varying gsa results sensitivity analysis results report the parameters that have relative influence on the kineros2 model outputs data collection efforts and selection of parameters for calibration and validation for scenario modeling can be guided based on parameter sensitivity a parameter with higher sensitivity implies the importance of accurate representation in the model to convey reliable results in kineros2 these parameters are the hydraulic conductivity of pervious streets the fraction of the street area converted to a curb cut retention basin and its volume as well as on parcel basins and their hydraulic conductivities and fraction of roof contributing runoff to cisterns and their volumes these parameters translate to roof rainwater harvesting retention basins and permeable streets as gi practices our understanding is that if these practices are represented accurately in kineros2 then the results can be useful in guiding gi implementation and decision making different hydrological models use different parameters to represent gi practices it is important to understand the sensitivity of model predictions to the parameters and the uncertainty they can introduce not all parameters may be important nor have a significant influence on the model output it is important to understand these parameters and the impact they might have on the analysis being conducted before any modeling exercise moreover the decision making factors behind the gi scenarios should directly translate to the model parameters and these parameters should have some influence on the model outputs that are guiding the decisions if model predictions are highly insensitive to one or more parameters one should consider setting these parameters to a default value resulting in a parsimonious model a gsa can help understand some of these concerns and enable a more informed modeling exercise to guide decision making it is important to understand the assumptions made behind every gsa method and model implementation before its application to an analysis author contributions all authors designed the research yk performed the research all authors analyzed the results yk wrote the paper and all authors reviewed and edited the final draft funding sources this work was funded by nsf sustainability research network srn cooperative agreement 1444758 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to acknowledge usda ars and guillermo ponce campos for help with setting up a server to run the numerous simulations on the authors would also like to thank the anonymous reviewers and editors for their constructive criticism and suggestions to make this paper better 
25989,the kinematic runoff and erosion model version 2 kineros2 provides an urban component that is capable of modeling urban hydrology including green infrastructure gi practices at various scales the kineros2 model provides 15 parameters that can represent a variety of gi practices the goal of this study is to understand the sensitivity of the kineros2 model outputs infiltration peak flow runoff volume and outflow hydrograph to these 15 parameters across seven different precipitation events and at the watershed as well as the parcel scale a global sensitivity analysis gsa was performed using the variogram analysis of response surfaces vars framework to produce sensitivity metrics results from the time aggregate gsa indicate that parameter sensitivities follow trends based on precipitation intensity and duration time varying gsa indicate sensitivity of parameters to hyetograph shape especially the duration to maximum precipitation intensity the variation in sensitivity is influenced by the peaks and valleys in the precipitation hyetograph keywords kineros2 urban hydrology green infrastructure variogram analysis of response surfaces global sensitivity analysis 1 introduction green infrastructure gi or low impact development was pioneered by the department of environmental resources of prince george s county maryland to mitigate the urbanization impact of increasing impervious surfaces county and june 1999 as opposed to traditional stormwater management practices gi aims to preserve the pre development hydrology using a variety of cost effective on suite design techniques that store infiltrate evaporate and detain runoff in addition to stormwater management benefits gi practices are being used to mitigate a number of other urban issues such as heat island effects li et al 2014 norton et al 2015 santamouris 2012 yang and bou zeid 2019 and air pollution abhijith et al 2017 bottalico et al 2016 jayasooriya et al 2017 ortolani and vitale 2016 factors such as co benefits availability of space public interest government policies and funding influence the decision making process behind the type of gi its design and implementation zhang and chui 2018 in the context of hydrology gi is implemented to mitigate flood and water quality problems and for water augmentation dietz 2007 different gi practices can be implemented to achieve different benefits hopton et al 2015 for example retention basins rain gardens curb cuts infiltrating chicanes permeable pavements can help mitigate flooding and safeguard water quality whereas roof rainwater harvesting can result in water stored for later use this variety of gi practices creates an inherent uncertainty concerning the potential designs and implementations in an urban watershed a number of hydrological models have been used to represent urban hydrology and gi in order to assist various decision making processes for gi implementation zoppou 2001 elliot and trowsdale 2007 bach et al 2014 jayasooriya and ng 2014 without real world implementation data hypothetical modeling scenarios are guilty of using best case or ideal conditions for gi designs without considering the implications of uncertainty in terms of gi design parameters on the model output these scenarios may not always inform practical implementation moreover some design parameters may or may not have any influence over the model output if these design parameters are the basis of decision making the entire modeling exercise can be irrelevant if the parameters show no effect on the model output for the purpose of this study we focus on the kinematic runoff and erosion kineros2 model kineros2 is a spatially distributed complex physically based event driven model that simulates runoff and erosion for small watersheds smith et al 1995 goodrich et al 2012 https www tucson ars ag gov kineros kinematic wave equations are used to simulate overland flow over variable width planar or curvilinear model elements and concentrated flow in trapezoidal channels while the parlange 3 parameter equation is used to simulate infiltration parlange et al 1982 kineros2 can be used to simulate typical urban configurations using the rectangular urban element kennedy et al 2013 korgaonkar et al 2018 the urban element provides a number of overland flow areas that can be used to represent small scale urban characteristics including green infrastructure fig 1 these include 1 directly connected pervious dcp area 2 directly connected impervious dci area 3 indirectly connected impervious ici area 4 connecting pervious cp area 5 non contributing nc area 6 infiltrating retention basin areas rb dcp rb dci and rb cp 7 street half on to which the aforementioned overland flow areas contribute runoff and 8 a curb cut cc area to simulate retention and infiltration of runoff flowing down the street runoff from ici can be captured in a cistern to simulate roof rainwater harvesting whereas permeable pavements can be simulated by using the dci or the street half components with a constant infiltration rate for a detailed description of the kineros2 urban element see korgaonkar et al 2018 a large number of parameters are used in kineros2 to represent various properties and characteristics the variability in each of these parameters is capable of contributing to the uncertainty in the model outputs prior to the development of the urban component in kineros2 a global sensitivity analysis gsa was conducted to explore the relative influences of various parameter uncertainties on simulated output used for flash flood forecasting yatheendradas et al 2008 rainfall was determined to be the most sensitive factor followed by soil saturated hydraulic conductivity soil volumetric rock fraction and soil surface roughness the sensitivity of the model output to soil saturated hydraulic conductivity was confirmed by kennedy et al 2013 which utilized the urban component to simulate a small subdivision in arizona the assessment of sensitivity of the parameters that represent gi is vital in order to have confidence in using kineros2 to model gi practices there are a number of methodologies available to conduct sensitivity analysis of parameters in the context of earth and environmental system models norton 2015 pianosi et al 2016 razavi and gupta 2015 saltelli et al 2008 out of all the available methods for sensitivity analysis the variance based sobol method sobol 1990 and the derivative based morris method morris 1991 are the most popular methods razavi and gupta 2015 conducted a review of all the different methodologies and outlined two main issues in sensitivity analysis ambiguous characterization of sensitivity and computational cost in order to address these issues razavi and gupta 2016a 2016b introduced a theoretical framework for sensitivity analysis called variogram analysis of response surfaces vars razavi et al 2019 this methodology uses directional variograms and covariograms to characterize the global sensitivity of model response with respect to a model parameter varying within the feasible parameter space vars is computationally efficient statistically robust provides stable estimates with relatively small number of sampled points and accounts for spatial correlation in model response as parameters are varied a detailed background of the vars framework can be found in razavi and gupta 2016a 2016b in this study we conduct a gsa using the vars framework with a star based sampling strategy star vars razavi and gupta 2016b which provides a reliable low computational cost strategy to estimate sensitivity across a range of scales our study follows the performance metric free global sensitivity analysis procedure presented by gupta and razavi 2018 it can be summarized as follows with steps 2 4 5 and 6 implemented as part of the vars tool 1 identify the parameters of interest and model outputs 2 select a representative statistical sample of parameter locations using the progressive latin hypercube sampling method plhs sheikholeslami and razavi 2017 these locations will be the star centers to increase the sample size using the star sampling strategy 3 run the hydrologic model using each set of parameters created in step 2 4 construct the generalized global sensitivity matrix ggsm razavi and gupta 2019 using each model output parameter sets and the number of time steps to analyze sensitivity analysis results 5 conduct the total period time aggregate parameter importance analysis to understand the sensitivity of model responses to the various parameters 6 conduct the time varying parameter importance analysis in order to understand the effect of parameter variance during various time steps of the model output the temporal nature and complexity of modeling rainfall runoff using kineros2 model makes it an ideal model for the use of the vars tool the total period time aggregate parameter importance can provide summary statistics regarding overall sensitivity of model responses to variations in parameters this is especially helpful to identify parameters that can be varied or fixed during model calibration the time varying parameter importance can be useful to understand how the model and its parameters perform at every time step of the simulation precipitation serves as the driving input to the kineros2 model and due to its inherent temporal variability the time varying parameter importance analysis can highlight the relationship between the model response and parameters for varying precipitation levels and storm hyetograph shapes i e high intensities early late in the storm the overall goal is to highlight the varying sensitivity of the model responses to the various parameters across each timestep of the simulation as well as the entire duration these sensitivity indices are particularly helpful in metric free applications as is the case in this study vars uses variogram and covariogram functions to generate a directional variograms associated with each model parameter the mathematical integration of this directional variogram creates a comprehensive set of global sensitivity indices called integrated variograms across a range of scales or ivars specifically the ivars50 index is the most comprehensive variogram based index for global sensitivity that captures the full range of perturbation scales for the parameters under consideration razavi et al 2019 using the vars tool to perform a gsa on the kineros2 model our research focuses on answering the following questions 1 how sensitive are the kineros2 model outputs to parameters representing gi practices 2 do different precipitation events influence the sensitivity of the kineros2 gi parameters 3 how does the sensitivity of the kineros2 gi parameters compare at the watershed scale and at an individual parcel scale 4 can a gsa on kineros2 parameters representing gi inform decision making 2 methods the la terraza subdivision located in sierra vista arizona fig 2 was selected as the study area based on previous research parameters from kennedy et al 2013 and validated model from korgaonkar et al 2018 sierra vista is located at an elevation of approximately 1300 m and has an average annual precipitation of 360 mm with an annual mean temperature of 17 4 c the subdivision consists of 66 housing lots spanning 14 ha with an average parcel size of 1780 m2 standard deviation of 265 m2 average roof area of 380 m2 standard deviation of 59 m2 and average driveway area of 108 m2 standard deviation of 43 m2 with 7 3 m wide asphalt streets the total watershed area can be broken down into 17 roof area 16 street area and 5 driveway area totaling to 38 impervious area on the watershed seven observed precipitation events were selected based on their intensity shape duration and seasonality table 1 names of the event are in the yyyymmdd format events 20071130 and 20100121 are representative of typical low intensity long duration events occurring in the winter months events 20080725 and 20140704 are medium intensity short duration events representing the summer monsoon months in this region events 20140826 20150707 and 20140711 have larger return periods for the purpose of comparison these events are categorized as low intensity 20071130 20100121 20080725 and 20140704 and high intensity 20140826 20150707 and 20140711 note that event 20140711 corresponds to a 200 year 3 hour event from noaa atlas 14 point precipitation frequency estimates for the sierra vista station and can be considered as an extreme event for the purpose of discussion fifteen parameters used to represent various green infrastructure practices in the kineros2 urban element table 2 were selected for the analysis peak flow runoff volume outflow hydrograph at the watershed outlet and infiltration over the entire watershed were selected as model responses to test against sensitivity of parameters due to the lack of physical gi implementations parameter ranges were determined based on available area given the physical characteristics of the parcels literature and back of the envelope values to test the maximum range of parameters each of these parameters are independent of each other and are assumed to be uniformly distributed this assumption is valid as any value specified in the range for any of the parameter has an equal probability of occurrence in an actual implementation scenario a brief description of each of the fifteen parameters and their ranges follows parameter nc represents fraction of dcp that is disconnected and allows isolation and infiltration of all of the precipitation that falls on this area an upper range of 1 was determined as nc can be the entire pervious area represented by dcp hcd and hcs represent permeable pavements on driveways and streets respectively and the range of 0 500 mm h was selected based on infiltration rates reported in selbig and buer 2018 hfrac represents the fraction of roof area that contributes to a cistern whose volume is represented by parameter harv hfrac range was determined to be 0 to 1 since it is possible for the entire roof to contribute to a cistern a range of 0 40 m3 was determined for harv based on commercially available cistern sizes parameters rb dcp rb cp and rb dci represent fractions of parcel pervious areas converted to retention basins and vol dcp vol cp and vol dci represent the retention capacities of these basins hcb represents the saturated hydraulic conductivity of the retention basins and its range was selected based on hydraulic conductivities representing various materials from impervious surfaces 0 mm h to sand 210 mm h rawls et al 1982 parameter ranges for rb dcp rb cp and rb dci were selected as 0 to 1 because the entire available area from dcp cp or dci can be converted into a retention basin since available area for basins can be limited the depth determines the volumes for each of these basins a range of 0 50 m3 was selected for the volume parameters as depth of basins can be highly variable based on the available area cc represents the fraction of the street area converted to a retention basin with a parameter range of 0 0 5 keeping in mind right of way restrictions on streets allowing traffic movement vol cc and hcc represent the retention basin capacity and saturated hydraulic conductivity respectively the ranges of hcc and vol cc were determined similar to the ranges of the aforementioned parameters hcb and those representing on parcel retention basins respectively in addition to the watershed scale sensitivity of the kineros2 gi parameters were also tested against the aforementioned model responses for a single representative parcel pid 65 fig 2 due to the small size of the watershed and the relative homogeneity of the dimensions of the various parcels construction of houses and landscaping pid 65 was considered a suitable candidate to represent the remaining parcels pid 65 has a parcel area of 1700 m2 with a roof area of 420 m2 and driveway area of 101 m2 these values are within one standard deviation of the aforementioned mean values for the entire watershed the only exception to the homogeneity of the parcels would be the corner parcels that have irregular or triangular shapes fig 2 additionally pid 65 is a flow initiating parcel and provides a good examination of water balance without any incoming upstream flow the agwa urban tool was used to setup the kineros2 model using geospatial data as discussed in korgaonkar et al 2018 this configuration acts as a template for use in the numerous simulations as part of the gsa next the vars tool was setup to run a time aggregate analysis using infiltration peak flow and runoff volume as well as a time varying analysis using the output hydrograph in both cases the first step was to generate sets of kineros2 gi parameters using the star vars sampling strategy in vars vars was run with 100 star centers and a sampling resolution of 0 1 step size equal to 10 of the parameter range for the parameter space in table 2 razavi and gupta 2016a 2016b and razavi et al 2019 recommend a smaller value of h typically 0 1 to prevent degradation of the constant mean assumption that is possible at larger scales the sampling resolution of resolution of 0 1 is the distance between the location of the pairs of points in the parameter space and ensures that no parameter ranges are excluded from the sampling process additionally razavi and gupta 2016b also discussed the robustness and probability of failure based on the number of model evaluations their results indicated a success rate of 100 for star centers as low as 35 considering the computational power available we decided to use 100 star centers to generate 13 600 parameter sets which we felt would effectively capture the variability in the parameter space the vars tool automatically rescaled the parameter ranges between zero and one the output from this step resulted in 13 600 parameter sets that were used to create and execute the kineros2 simulations results were compiled into total infiltration peak flow and volume and output hydrographs for both the entire watershed as well pid 65 to serve as input to the vars tool which generates and analyzes the ggcm finally results were compiled using the ivars50 index eq 1 was used to calculate relative sensitivity for each parameter i using the ivars50 metric 1 r e l a t i v e s e n s i t i v i t y i i v a r s i i 1 15 i v a r s i note that the magnitude of the sensitivity is not featured in the results instead we focus on the relative sensitivity of the parameters 3 results 3 1 time aggregate gsa the time aggregate relative sensitivity was computed for simulated infiltration fig 3 subplot a peak flow subplot b and total volume subplot c at the outlet of the watershed left and from pid 65 right for all 15 parameters and for 7 precipitation events precipitation events are arranged in increasing order of maximum intensity from left to right note the similar trends in parameter sensitivity for the lower intensity events as compared to the higher intensity events in general parameters nc hcd rb dci and vol dci were the least sensitive parameters for all model responses across all events parameters with relative sensitivity of less than 0 05 were grouped together under the category other 3 1 1 infiltration in general roof runoff harvesting parameters hfrac and harv influence infiltration at both scales with dominant sensitivity for the low intensity events fig 3 subplot a parameter hfrac controls the surface area of the roof that contributes water into the cistern whose size is determined by parameter harv both these parameters influence the amount of precipitation that is intercepted before it is available for infiltration on the ground for the low intensity events hfrac is the most sensitive parameter with harv and hcb showing sensitivity as well parameter harv is more sensitive than hcb for the winter events whereas the reverse is true for the summer events the rainfall volume is distributed over a longer duration for winter events as a result the cistern is not overwhelmed in a short amount of time as is the case for the short duration high intensity summer events for the high intensity events hcb is the dominant parameter affecting infiltration for both the entire watershed and pid 65 additionally parameters rb cp rb dcp and cc also have relative sensitivity greater than 0 05 these parameters control the surface area of the retention basins thereby the area available for the water to collect and infiltrate parameter hcb represents the saturated hydraulic conductivity of the three retention basins on the parcels it is expected that the parameter that influences the infiltration rate would influence the amount of water that infiltrates into the ground 3 1 2 peak flow in the case of peak flow fig 3 subplot b there is a distinct difference between the parameter sensitivity at the watershed scale and pid 65 at the watershed scale for low intensity events cc is the most sensitive parameter followed by hcs and vol cc all three of these parameters control gi practices on the street cc controls the surface area of the curb cut retention basin vol cc is the total retention volume available and hcs controls the infiltration rate if the street was pervious on the other hand for pid 65 hcs is the most dominant parameter with the relative parameter sensitivity varying by event the parameter sensitivity can be ranked as hcb vol cp vol dcp for the winter events hcb vol dcp vol cp for 20080725 and cc vol dcp vol cp for 20140704 in each of these cases the available volume of the retention basins influences the peak flow downstream at the watershed outlet for the high intensity events at the watershed scale hcb sensitivity increases with increase in event size retention basin volume parameters have sensitivity ranked as vol cc vol dcp vol cp however at the pid 65 scale vol dcp is more sensitive than hcb except for event 20150707 and vol cp is the third most sensitive parameter for these larger events the parameters controlling the retention capacity of the basins as well as the hydraulic conductivity influence the peak flow 3 1 3 volume runoff volume fig 3 subplot c is sensitive to street gi parameters cc vol cc and hcs for low intensity events with the addition of retention basin parameters hcb vol dcp and vol cp for high intensity events at the watershed scale parameter sensitivity to output volume can be ranked as hcs cc vol cc for the low intensity winter events and as cc hcs vol cc for the summer events for pid 65 cc is the dominant parameter with hcs and vol cc both have relative sensitivity greater than 0 05 runoff volume is most sensitive to street gi parameters at both scales for low intensity events for the high intensity events retention basin parameters show relative sensitivity in addition to the street practices hcb sensitivity increases with increase in event size whereas vol cc and cc show a decreasing trend vol dcp and vol cp both have similar relative sensitivity across the high intensity events we can also see a higher sensitivity of parameter hcs at the watershed scale as compared to pid 65 possibly due to the cumulative effect of the street area across the watershed 3 2 time varying gsa time varying results of the relative sensitivity were derived using the ivars50 metric for outflow hydrographs for the entire watershed left and pid 65 right for each of the seven precipitation events fig 4 note that the maximum intensity of precipitation increases from top to bottom in general the variation in sensitivity of the parameters is smoother for the watershed hydrographs as compared to those for pid 65 parameters display more sensitivity at the parcel scale where the time of concentration is less compared to the watershed scale i e the time between precipitation falling on the parcel and runoff leaving the parcel as a result parameters show more sensitivity to variation in precipitation at the parcel scale additionally variations in sensitivity for a given parameter consistently follow abrupt changes in precipitation rate for the low intensity events fig 4 subplots a b c and d hcs cc and vol cc are the only three parameters that display relative sensitivity for winter events fig 4 subplots a and b where the hyetograph peaks after almost 5 hours parameters hcs and cc mirror each other where one increases while the other decreases and vice versa this can be attributed to the fact that hcs and cc are the only two parameters displaying sensitivity and the mirroring is due to the normalization of the sensitivity results at the watershed scale hcs has relatively higher sensitivity throughout both events except at maximum precipitation intensity where cc is more sensitive on the other hand for pid 65 cc is more sensitive with smoother variations as compared to hcs whose sensitivity varies significantly with peaks and valleys in precipitation intensity a third parameter cc vol shows relatively smaller sensitivity and is visible later in the event at both scales summer events 20080725 and 20140704 are similar in maximum intensity fig 4 subplots c and d respectively but differ in shape where the maximum intensity is at the beginning of the event 20080725 and the latter has maximum intensity towards the end of the event at the watershed scale parameter cc is more sensitive than hcs and vol cc for 20080725 in contrast for the 20140704 event the sensitivity of cc shows a decreasing trend while hcs keeps increasing the two parameters crossover each other at around the 45 min mark where the hyetograph displays intensities less than 10 mm h note that vol cc increases in relative sensitivity after the peak precipitation intensity in both cases at the parcel scale cc is dominantly sensitive as compared to hcs and vol cc for event 20080725 this dominance is also visible for event 20140704 but only during the first 30 min of high intensity in the event high intensity event 20140826 fig 4 subplot e has peak intensity at the beginning of the event whereas event 20150707 fig 4 subplot f has maximum intensity towards the end of the event both events have similar durations and maximum intensities parameters cc vol cc and vol dcp compete as most sensitive parameters at different time steps of the hyetograph cc starts as a dominant parameter with a decreasing trend both vol cc and vol dcp increase in sensitivity as the event progresses with the latter showing more sensitivity later in 20150707 event other parameters such as rb dcp hcb and cc each have less than 0 2 relative sensitivity and are visible after the precipitation subsides parameters hcc and hcs are also sensitive less than 0 2 especially in the early time steps of the hyetograph for the extreme event 20140711 fig 4 subplot g sensitivity of parameter cc drops off rapidly with increase in intensity of the event at the same time we can see a sharp increase in sensitivity of vol cc as the event further subsides parameter hcb becomes the most sensitive parameter followed by hcs this pattern is seen at both the watershed as well as the parcel scale 4 discussion a gsa is able to test how model responses change when model parameters are changed thereby revealing the parameters that have the most influence on the model response one of the key insights from this gsa would be that different model responses can show different sensitivities for different parameters in other words not all parameters may be relevant for a particular model response additionally parameters display varying sensitivities at various time steps of the model simulation due to this it is vital to determine the model responses the are most important and relevant before any modeling exercise is conducted this will enable the modeler to identify which parameters require more attention in terms of parameter ranges data collection and evaluation of model predictions as the complexity of a model increases the number of model parameters likewise typically increases it may not necessarily indicate that all of these parameters are essential or required in the model the gsa reveals interesting parameter behaviors in terms of relative sensitivity of parameters relationship to different precipitation events as well as scale each of these aspects are discussed in the subsections below 4 1 how sensitive are the kineros2 model outputs to various parameters representing gi practices parameters representing some gi practices do influence kineros2 output more than the others do this is evident from the influence of roof rainwater harvesting parameters hfrac and harv on infiltration permeable pavement parameter hcs on parcel retention basin parameters hcb rb dcp vol dcp rb cp and vol cp and curb cut retention basin parameters cc and vol cc on peak flow and volume roof rainwater harvesting parameters are responsible for intercepting precipitation falling on 17 of the total watershed area lower hfrac and harv values would contribute more runoff to cp and subsequently rb cp for infiltration and vice versa thus explaining the higher sensitivity for the infiltration output of the model on parcel retention basin parameters try to capture the runoff from 22 roof and driveway of the impervious area as well as the total pervious area of the watershed showing significant influence on model outputs similarly curb cut retention basins cater to 16 of the total watershed with high connectedness and their respective parameters show high relative sensitivity on the other hand certain parameters nc hcd rb dci and vol dci had minimum relative influence on the model outputs nc represents fraction of the pervious area that is completely disconnected and does not intercept any runoff from any impervious area if nc were to represent disconnection from impervious areas we can envision a larger influence on the model outputs driveways constitute only 5 of the total watershed area and thus parameters representing permeable driveways hcd rb dci and vol dci show relatively low sensitivity 4 2 do different precipitation events influence the sensitivity of the kineros2 gi parameters relative sensitivity of the parameters follows certain patterns as far as the precipitation events are concerned for the infiltration output sensitivity of the parameters for low intensity events follow the same trend as the depth of the precipitation event this is particularly evident from the increasing sensitivity of parameter harv with increase in precipitation depth from event 20080725 to 20100121 there is a visible trend for the low intensity events and another for the high intensity events specifically the retention basin parameters show higher relative sensitivity for the high intensity events additionally the long duration winter events 20071130 and 20100121 have similar relative sensitivity patterns that differ from the short duration low intensity summer events 20080725 and 20140826 the largest event 20140704 displays different sensitivity trends as compared to the remaining two high intensity events 20140826 and 20150707 indicating that higher intensity and larger total volume of an event can impact the sensitivity of model results to the parameters the shape of the precipitation event also has an impact on the parameter sensitivity to model responses and is more evident in the time varying gsa results for the high intensity events the sensitivity of many parameters increases after the maximum precipitation intensity if we consider the winter events the variation in sensitivity of the parameters is smoother except at precipitation intensity peaks in the hyetograph comparing events 20080725 and 20140704 the sensitivity of vol dcp shows a distinctive trend the sensitivity of vol dcp increases after the maximum precipitation intensity for 20140704 whereas its sensitivity is higher just before the maximum precipitation intensity for 20080826 since vol dcp directly relates to the capacity of one of the retention basins it appears that the amount of cumulative precipitation volume has an effect on its sensitivity 4 3 how does the sensitivity of the kineros2 gi parameters compare at the watershed scale and at an individual parcel scale the time of concentration is less at the parcel scale as compared to the watershed scale as a result we can see certain differences in parameter sensitivity at these scales the most prominent difference is for the time aggregate gsa results for peak flow for the low intensity events hcs is most sensitivity at the parcel scale whereas cc is dominant at the watershed scale for the high intensity events parameters for street gi practices have minimum sensitivity at the parcel scale from the time varying gsa results it is evident that sensitivity has more variations in slope at the parcel scale as compared to the watershed scale this can be attributed once again to the time of concentration at the watershed scale it takes rainfall considerably longer to reach the outlet on the other hand the path followed by rainfall to reach the outlet of the parcel is considerably shorter and hence the time of concentration is small as well at the parcel scale the effects of the parameters are relatively quicker as compared to the watershed scale where aggregation effects from multiple parcels dampen the sensitivity trends in an influent environment where watershed runoff response and runoff to rainfall ratios decrease with increasing drainage area the watershed acts as an attenuating filter where input variability becomes dampened this is evident from the smoother variation in sensitivity at the watershed scale as compared to the parcel scale additionally peaks and valleys in the hyetograph have a more dramatic effect on the sensitivity of the parameters at the parcel scale in terms of the time aggregate infiltration and volume results there are no noticeable differences when comparing the two scales and results follow similar trends at both scales this observation and the differences in peak flow results at the two scales can be attributed to the cumulative effects of connectedness in the watershed which is absent at the parcel scale 4 4 can a gsa on kineros2 parameters representing gi inform decision making there are two perspectives concerning decision making based on a gsa on parameters representing gi the first represents a hydrologic modeler s perspective to utilize gsa results when representing watersheds using kineros2 the second represents stakeholders designing and implementing gi based on these gsa results a hydrologic modeler would use gsa results to determine parameters that should be represented accurately in the kineros2 model more effort would be spent in field measurements in order to represent these gi practices accurately in the absence of field measurements the modeler would undertake a sensitivity analysis and model calibration to ensure model reliability in simulating results for example development has been shown to cause soil compaction decreasing infiltration gregory et al 2006 kennedy et al 2013 which could affect the parameters hcb and hcc if the modeler estimated these parameters based on soil texture without compaction the effectiveness of these gi practices could be over estimated additionally the modeler can simplify the model representation process by identifying parameters that do not display relative sensitivity for these parameters simple assumptions for values can be made knowing that their influence on model output is minimal based on the gsa results the kineros2 model can be useful to model scenarios that involve on street gi practices represented by parameters cc vol cc and hcc such as permeable streets curb cut retention basins and infiltrating chicanes and on parcel gi practices such as retention basins and rain gardens represented by parameters rb cp vol cp rb dcp vol dcp and hcb as well as roof runoff harvesting represented by parameters hfrac and harv model outputs show a higher sensitivity to the parameters that represent these practices fig 3 as a result the effect of the model implementation focusing on these parameters will have a more direct impact on total infiltration and runoff volume and peak flow at the outlet of the watershed and for a single parcel from stakeholders perspectives it would be valuable to know the dominant gi parameters that they can control and modify during implementation as well as a more realistic estimate of costs for example a homeowner may not be able to control the fraction of roof area contributing to a cistern however they can control the size of the cistern that captures roof runoff thereby influencing the amount of rainwater infiltrating on their parcel fig 3 subplot a similarly in the case of retention basins the area of the basin is largely dependent on available space and the hydraulic conductivity is largely dependent on the underlying soil but the depth of the basin can be modified with excavation at the time of implementation and the hydraulic conductivity hcb can be preserved with maintenance over time runoff volume and peak flow are sensitive to the depth of the basin as controlled by the vol cp and vol dcp parameters these parameters along with hcb are shown to have considerable influence on model response fig 3 subplots b and c respectively based on the gsa results the modeler should recommend that stakeholders focus on permeable pavements represented by parameter hcs and on street basins represented by parameters cc hcc and vol cc for downstream flood mitigation as these were the most sensitive parameters influencing runoff volume and peak flow fig 3 subplot b and c respectively any change in the values of these parameters will be reflected in the model output thereby allowing a better understanding of the efficacy of implementation of these gi practices many gi practices are retrofitted based on available infrastructure for example curb cuts divert water into basins that are dug out in existing areas alongside the streets or infiltrating chicanes are created on streets based on available area after considering right of way requirements additionally practices such as green roofs and roof rainwater harvesting are dependent on the available roof area in these cases the available area alongside the street or on the street as well as roof areas act as decision making factors modeling scenarios are utilized to gage the impact and effectiveness of various gi implementations if a certain parameter in the model is directly related to one of these decision making factors the gsa can inform whether a change in the value of this parameter can translate to real world implications based on the model output wagener and pianosi 2019 in our analysis parameter nc had no impact on the model outputs if gi scenarios were designed around disconnection the modeling exercise in kineros2 would be completely futile and would provide no new information to support disconnection designs 5 conclusions the sensitivity of kineros2 model outputs infiltration peak flow runoff volume and outflow hydrograph to parameters representing gi practices was tested by performing a global sensitivity analysis gsa using the vars framework with a star based sampling strategy star vars fifteen parameters in kineros2 representing different gi practices were tested using the ivars50 metric the agwa urban tool was used to setup the kineros2 model to simulate 66 parcels in the la terraza subdivision in sierra vista az seven different precipitation events varying in maximum intensity duration shape and seasonality were used to compare sensitivities across events results were compiled at the watershed scale as well as a single parcel key results are summarized as follows 1 parameters representing on parcel retention basins hydraulic conductivity surface area and volume of basin have strong influence on infiltration peak flow and runoff volume especially for high intensity precipitation events 2 parameters representing on street practices hydraulic conductivity of permeable pavement surface area and volume of curb cut retention basin have strong influence on peak flow and runoff volume for low intensity precipitation events 3 roof runoff harvesting parameters fraction of area contributing to cistern and cistern volume have a strong influence on infiltration for low intensity precipitation events 4 the difference in time of concentration at the parcel scale as compared to the watershed scale does play a role for the parameters influencing model outputs especially peak flows for low intensity events the variance in sensitivity is much sharper at the parcel scale and smoother at the watershed scale additionally for peak flows hydraulic conductivity of the permeable street is dominant at the parcel scale whereas the parameter controlling the size of the on street retention basin is dominant at the watershed scale 5 the type of precipitation events has an influence on the variation in sensitivity of parameters intensity and duration effects are visible in the time aggregate gsa results whereas shape effects are visible in the time varying gsa results sensitivity analysis results report the parameters that have relative influence on the kineros2 model outputs data collection efforts and selection of parameters for calibration and validation for scenario modeling can be guided based on parameter sensitivity a parameter with higher sensitivity implies the importance of accurate representation in the model to convey reliable results in kineros2 these parameters are the hydraulic conductivity of pervious streets the fraction of the street area converted to a curb cut retention basin and its volume as well as on parcel basins and their hydraulic conductivities and fraction of roof contributing runoff to cisterns and their volumes these parameters translate to roof rainwater harvesting retention basins and permeable streets as gi practices our understanding is that if these practices are represented accurately in kineros2 then the results can be useful in guiding gi implementation and decision making different hydrological models use different parameters to represent gi practices it is important to understand the sensitivity of model predictions to the parameters and the uncertainty they can introduce not all parameters may be important nor have a significant influence on the model output it is important to understand these parameters and the impact they might have on the analysis being conducted before any modeling exercise moreover the decision making factors behind the gi scenarios should directly translate to the model parameters and these parameters should have some influence on the model outputs that are guiding the decisions if model predictions are highly insensitive to one or more parameters one should consider setting these parameters to a default value resulting in a parsimonious model a gsa can help understand some of these concerns and enable a more informed modeling exercise to guide decision making it is important to understand the assumptions made behind every gsa method and model implementation before its application to an analysis author contributions all authors designed the research yk performed the research all authors analyzed the results yk wrote the paper and all authors reviewed and edited the final draft funding sources this work was funded by nsf sustainability research network srn cooperative agreement 1444758 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to acknowledge usda ars and guillermo ponce campos for help with setting up a server to run the numerous simulations on the authors would also like to thank the anonymous reviewers and editors for their constructive criticism and suggestions to make this paper better 
