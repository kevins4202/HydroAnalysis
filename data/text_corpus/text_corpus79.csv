index,text
395,urban pluvial flooding is a threatening natural hazard in urban areas all over the world especially in recent years given its increasing frequency of occurrence in order to prevent flood occurrence and mitigate the subsequent aftermath urban water managers aim to predict precipitation characteristics including peak intensity arrival time and duration so that they can further warn inhabitants in risky areas and take emergency actions when forecasting a pluvial flood previous studies that dealt with the prediction of urban pluvial flooding are mainly based on hydrological or hydraulic models requiring a large volume of data for simulation accuracy these methods are computationally expensive using a rainfall threshold to predict flooding based on a data driven approach can decrease the computational complexity to a great extent in order to prepare cities for frequent pluvial flood events especially in the future climate this paper uses a rainfall threshold for classifying flood vs non flood events based on machine learning ml approaches applied to a case study of shenzhen city in china in doing so ml models can determine several rainfall threshold lines projected in a plane spanned by two principal components which provides a binary result flood or no flood compared to the conventional critical rainfall curve the proposed models especially the subspace discriminant analysis can classify flooding and non flooding by different combinations of multiple resolution rainfall intensities greatly raising the accuracy to 96 5 and lowering the false alert rate to 25 compared to the conventional model the critical indices of accuracy and true positive rate tpr were 5 15 higher in ml models such models are applicable to other urban catchments as well the results are expected to be used to assist early warning systems and provide rational information for contingency and emergency planning keywords urban pluvial flooding rainfall threshold machine learning shenzhen city 1 introduction 1 1 background urban pluvial flooding is a threat to a great number of cities worldwide especially given its increasing frequency of occurrence in recent years martina et al 2006 atta ur rahman et al 2016 ziegler 2012 its impact including loss of life and damages to both public and private properties can be further deepened by climate change and accelerated urbanization falconer et al 2009 this type of flooding usually occurs when intense rainfall exceeds the capacity of an urban drainage system recent extreme precipitation events have raised awareness from both authorities and citizens to the challenges of predicting and managing urban pluvial floods in july 2019 heavy rain caused at least 18 deaths and triggered massive transport disruptions in mumbai india bbc 2019 in the uk about 40 of damages and associated economic losses in cities are estimated to result from pluvial flooding douglas et al 2010 in china 98 of cities are exposed or vulnerable to frequent floods jiang et al 2017 a survey conducted between 2008 and 2010 showed that 218 chinese cities suffered severe urban pluvial flooding at least once and more than 100 cities experienced it more than three times jiang et al 2018 therefore urban pluvial flood prediction and management is a critical topic in the context of urban water management in order to prevent pluvial flooding and its consequences city authorities e g meteorological offices emergency agency offices or water authorities usually need to make predictions of pluvial floods this is based on good prediction of precipitation characteristics such as peak intensity arrival time and duration many studies investigate the prediction of pluvial flooding by hydraulic models li 2020 li and willems 2019 by simulating the inundated area and depth given certain historical or predicted rainfall scenarios babaei et al 2018 thorndahl et al 2016 xing et al 2019 however hydraulic models need a large volume of data and computational resources as a result the output of a hydraulic model is usually case specific in other words we have to run the model to make predictions for flooding duringeachseparate rainfall scenario on the other hand using a rainfall threshold based on data driven models can provide an easy and intuitive solution by comparing the current predicted rainfall with the threshold one can straightforwardly estimate the likelihood of the city being flooded martina et al 2006 montesarchio et al 2011 tian et al 2019 yang et al 2016 specifically a rainfall threshold specifies one or several rainfall depth s over certain time windows above which a pluvial flood is likely to occur moreover rainfall threshold based hazard prediction is widely applied to landslides garcia urquia and axelsson 2015 giannecchini et al 2012 golian et al 2015 hong et al 2018 martelloni et al 2012 debris flow nikolopoulos et al 2014 pan et al 2018 van asch et al 2014 and flash floods montesarchio et al 2011 norbiato et al 2008 zhai et al 2018 to determine a cumulative rainfall threshold a physically based model is usually needed to compute critical rainfall thresholds over time norbiato et al 2008 yang et al 2016 or a statistical data driven analysis can be applied carpenter et al 1999 golian et al 2010 martina et al 2006 montesarchio et al 2011 however on the one hand there is a gap in short term prediction capability of physical models costabile and macchione 2015 short lead time flood prediction is of crucial importance for highly urbanized areas in order to provide timely warnings to residents zhang et al 2018 on the other hand statistical models have a limitation on the accuracy of prediction fawcett and stone 2010 furthermore urban catchments often lack sufficient data on both the drainage network and topography complicating the estimate of rainfall threshold yang et al 2016 machine learning ml models can deal with data scarcity based on an ensemble method breiman 2001 therefore in this paper we use ml approaches to derive the flooding thresholds for different rainfall duration periods ml is a family of algorithms derived from statistics and computer science which aims to train mathematical models to make predictions or decisions based on observed samples ml is suggested as an effective tool to explore the connectedness between human and water systems shen et al 2018 the latter is anticipated to be a key interdisciplinary issue to deal with in future hydrological studies vogel et al 2015 moreover ml models can numerically reproduce flood nonlinearity solely based on historical data without requiring knowledge about the underlying physical processes mosavi et al 2018 therefore this study utilizes ml algorithms to attempt to classify the presence or absence of flooding based on rainfall characteristics although ml algorithms have shown powerful applicability to flood prediction and forecasting liu et al 2017 mosavi et al 2018 noymanee et al 2017 tayfur et al 2018 there are still very few studies that utilize ml to classify or predict urban pluvial flooding which is a challenge due to lack of flood inundation data drainage system data and fine resolution topography data yang et al 2016 therefore we aim to test ml algorithms for classifying urban flooding in the city of shenzhen which is frequently flooded a sudden rainstorm event claimed 11 lives in april 2019 in shenzhen hua 2019 attracting great attention for the local authorities to reconsider the early warning system for pluvial flooding in the city moreover shenzhen is a pioneer city in terms of high technology development socio cultural development and disaster emergency management this experience can be shared with other cities in china and abroad the paper is organized as follows section 2 describes the study area and data used for this study and introduces the conventional and ml methods for flood prediction section 3 shows the results of the models and proposes the rainfall threshold for shenzhen section 4 compares the ml results for rainfall thresholds to the current rainfall threshold and cumulative rainfall threshold in shenzhen section 5 presents the conclusions and recommendations 2 materials and methodology 2 1 study area in the past decades shenzhen has grown rapidly from a rural area to a prosperous economic zone and an important industrial city in southern china it is located on the central coast of guangdong province which is the passageway from mainland china to hong kong see fig 1 it is also an important city in the pearl river delta prd it has a total land area of 1 948 km2 the average elevation is 3 4 m above mean sea level rainstorm induced catastrophes in shenzhen city are mostly caused by persistent short duration heavy rainfall in the summer zhou et al 2017 pluvial flooding is one of the primary natural hazards in shenzhen in recent years urbanization has increased the surface runoff and intensified the flood frequency shi et al 2007 yan et al 2019 shenzhen is identified as an area under a high flood risk since many properties are built in flood prone areas such as the harbour front area chan et al 2014 the total average annual precipitation is 1 900 mm y of which rainstorms caused by typhoons july september make up 36 i e 689 mm y and approximately 85 of precipitation occurs from april to september see fig 2 data source meteorological bureau of shenzhen smb convective march june and typhoon rainstorms july october are the two main rainfall sources in this region as of 2019 shenzhen has a population of 13 million with a population density of 6 234 people km2 most of the city is drained by a separated storm sewer system 4 883 92 km whereas the remaining area 1 693 km is drained by a combined sewer system i e wastewater combined with rainwater sewer system with a drainage pipe density of 12 5km km2 ssb 2019 in total 126 municipal pumps with a capacity of 671 m3 s are used to drain stormwater out of the city ssb 2019 short duration high intensity rainfall is the main driver of pluvial flooding in shenzhen due to the rapid pace of urbanization the impervious area has significantly increased while the water storage area such as rivers lakes and wetlands has decreased with climate change increasing frequency of typhoon occurrence and intensity of torrential rainfall tracy et al 2007 pluvial flooding has a high likelihood of occurrence in the paved area on may 11 2014 for instance the daily rainfall volume reached 233 mm and some districts experienced a peak rainfall intensity of 310 mm in 6 hours cai 2014 currently smb uses a rainfall threshold for predicting urban pluvial flooding only based on 30 min rainfall depth i e 20 mm or 3 h rainfall depth i e 80 mm smb 2019 in the subsequent sections we will further testify and compare this threshold with that from the proposed ml models 2 2 records of flood events records of historical flood events from 1 june 2014 to 14 june 2017 consisting of 1 110 days and 663 records in total were retrieved from the water sector of shenzhen municipality http swj sz gov cn which has developed and implemented a disaster reporting system i e a flood report app named shenzhensanfang since 2014 citizens of shenzhen can report flood events via this system at any time these records register the date the location geotagging and a description as most of the records indicating pluvial flood events fall in the period between june and september 640 records i e 96 5 we only consider data points in the summer of each year namely 413 days in total over the 3 year study period in doing so we can exclude hundreds of non flooding events to lower the imbalance of the dataset too many non flooding events and too few flooding events note that the high frequency of the flooding record corresponds to the precipitation characteristics in shenzhen the 640 records were registered over 24 days c a 27 records d which are regarded as days with floods the remaining 389 days of the study period are regarded as days without floods these records are spatially distributed throughout the whole city see fig 3 it should be noted that as the inundation records were submitted by citizens socio economic background such as age education level and experience with previous pluvial flooding may affect the recording this may cause false alerts or missed alerts 2 3 rainfall observations the rainfall intensity each minute at 25 rainfall gauges see fig 3 from 1 june 2014 to 14 june 2017 was retrieved from smb http weather sz gov cn we used areal average rainfall intensity to represent the study area which stands for the mean value of rainfall intensities of all study sub areas districts the original database consisted of 1 min rainfall intensity these1 min rainfall intensities were aggregated to rainfall volumes of longer temporal scale namely 5 10 15 30 60 120 360 720 and 1440 mins each day the maximum rainfall volume at each temporal scale denoted as rdx in mm is calculated by eq 1 tian et al 2019 1 r d x max j k 1 x r 1 k k 1 j x j 1 x r 1 k k 1441 x 1440 r 1 k where j 0 1 1440 x min x 1 5 10 15 30 60 120 360 720 1440 min note that each item in the bracket of eq 1 stands for x min rainfall volume accumulated from 1 min rainfall intensity in the interval 1 j x j 1 x 2 4 flood classification models in this study we first apply a conventional rainfall curve method as a benchmark then we further develop multiple parametric and non parametric ml models to classify flooding and non flooding events based on rainfall intensities with respect to a binary classification problem four possible predicted outcomes are expected see table 1 namely true positives tp or correctly classified flooding events false positives fp or falsely classified flooding events true negatives tn or correctly classified non flooding events and false negatives fn or missed flooding events ideally an urban flood classification model should achieve a high true positive rate tpr a high true negative rate tnr and high overall accuracy acc on the other hand a prediction model with a low positive predictive rate ppr or a low tpr implies that a number of actual flood events are wrongly labeled or unexpectedly missed acc is also called the proportion of correct forecasts wilks 2005 2 5 conventional model with cumulative rainfall volume thresholds the cumulative rainfall volume threshold is a reference curve representing a cumulative amount of rainfall over a certain time window see fig 4 when the observed cumulative rainfall exceeds the threshold at a given moment flooding is expected to occur we propose a way to determine a threshold curve via the following steps 1 calculating the cumulative rainfall max in 24 hours based on the 1 min rainfall intensity for all flooding and non flooding events 2 computing the lower α percentile of the 1 min rainfall for all flooding events denoted as t α note α is to be determined in step 5 in doing so t α depicts a curve that a certain number of cumulative rainfall curves for flooding events stay above for instance all curves of flooding events are above the curve t α α 0 3 computing the upper β percentile of the 1 min rainfall for all non flooding events denoted as t β note β is also to be determined in step 5 the definition of t β is analogous to that of t α t β depicts a curve that a certain number of cumulative rainfall curves for flooding events stay below 4 constituting a linear combination of t α and t β based on a weight µ namely t α β µ µ t α 1 µ t β as a result we obtain a rainfall threshold based on three variables α β and µ any assigned values can result in a given cumulative rainfall threshold curve and its corresponding model performance 5 solving an optimization problem that maximizes the model performance by tuning α β and µ three optimal combinations for α β and µ were pursued aiming forthe maximum tpr the highest tnr and the highest acc 2 max α β μ perfofmance determined by t α β μ perfofmance tpr tnr or accuracy 2 5 1 machine learning ml algorithms machine learning ml algorithms are a collection of computational data driven methods without utilizing a pre defined equation as the basic model ml algorithms train a model using a certain type of algorithms fully based on known data whereas the trained model can be applied to new data as the number of training data sets increases the performance of ml algorithmscan improve ml consists of two families namely supervised learning and unsupervised learning specifically supervised learning algorithms aim to find functions that are able to map inputs to labeled outputs also including two categories classification and regression flooding prediction is commonly an application of classification jhong et al 2018 tayfur et al 2018 zhou et al 2018 which aims to distinguish flood events vs no flood events based on hydrological variables i e a binary classification problem given the size of the database available we adopt a collection of models in this study that usually show good performance for small to medium sized data sets 14 classification algorithms from 5 major ml families are considered to classify urban pluvial flooding based on rainfall intensities of multiple temporal scales table 2 brief introductions of these algorithms follow decision trees decision trees build a tree shaped top down structure from the roof at the top to leaf nodes at the bottom breiman et al 2017 each leaf node represents a predicted response given the fact that we focus on a binary classification problem the bifurcation starts from one parent node of a given layer to two child nodes of a subsequent lower layer relying on different values of variables specifically to find the optimal bifurcation we maximize gini s diversity index but stop maximization when i a node only contains a single class of data ii a child node to be generated contains fewer than five data points or iii the number of layers exceeds a pre defined criterion five for a coarse decision tree and twenty for a medium decision tree in general decision tree learning is one of the fastest algorithms its resultsare also easy to interpret we built the decision tree model in matlab by using the function fitctree discriminant analysis discriminant analysis da classifiers assumes a gaussian distribution for data of each class the gaussian distribution is determined by the sample mean of each class and the identical covariance matrix for linear da or different class based covariance matrices for quadratic da under this assumption linear or quadratic das make predictions by minimizing prediction costs based on bayes theorem note that the prediction costs are the sum of the multiplication of the posterior probability of a given class k for a data sample and the cost of classifying a sample as y but its actual class is k 0 for accurate classification and 1 for misclassification readers can refer to ledoit and wolf 2004 t hastie r tibshirani 2008 for more details note that this study considers both linear and quadratic discriminant analyses as their names suggest linear discriminant analysis can only learn linear boundaries while quadratic discriminant analysis can learn quadratic boundaries both of which are fast to run and easy to interpret we build the discriminant analysis model in matlab by using the function fitcdiscr support vector machine linear support vector machine svm applied to binary classification aims to find an optimal hyperplane that separates two classes with a margin of the maximal width in other words we look for the maximum margin widthwhile keeping the data of two classes on each side of the margin samples that are misclassified are penalized using kernel functions such as quadratic and cubic kernels can turn a linear svm into a non linear svm the latter is more flexible but also requires more computational resources and becomes less straightforward to explain readers can refer to ng 2000 for more details we build the support vector machine model in matlab using the function fitcsvm k nearest neighbor k nearest neighbor is a distance based learning technique that determines the predicted response of a given point by checking the major class of the k closest points cover and hart 1967 note that we use the euclidean and cosine distance as the metric to measure the closeness between points the knn algorithm is one of the easiest and most intuitive learning techniques widely used in many applications cheng et al 2014 zhang 2016 however it is also very sensitive to outliers ramaswamy et al 2000 which we may encounter frequently when predicting urban flooding based on rainfall intensities we build the k nn model in matlab using the function fitcknn bagged trees breiman et al 1984 breiman 2001 bagging stands for a type of ensemble learning which is used to reduce the variance of a single decision tree to build a bagged tree model we create multiple subsets of new data from original samples which are chosen randomly with replacement as a result we obtain an ensemble of decision trees also referred to as weak learners and they are proven to be more robust than a single decision tree we build the bagged trees model in matlab using the function fitcensemble subspace ensembles the random subspace method is also an ensemble technique to increase the accuracy of the discriminant classifier and knn classifier the subspace ensemble aims to train random sample features rather than the entire feature set it is proven to be an effective method to deal with the issue of high dimensional feature sets and small training sets as the name suggests classifiers are constructed in a random subspace of data feature space and then combined by simple majority voting readers can refer to tin kam ho 1998 for details garcía pedrajas and ortiz boyer 2009 skurichina and duin 2002 also prove that the random subspace method can be further used for da and knn which are applied in our study we build the subspace da and subspace knn models in matlab using the function fitcensemble 2 5 2 feature selection and model validation all the models listed in table 2 are first tested on ten features which are the ten multi temporal rainfall accumulationsrdx see eq 1 later we also run a principal component analysis pca based on the singular value decomposition method madsen et al 2004 to reduce the number of dimensions and find the most meaningful components for predicting flooding events as we only have a small dataset with 413 data points it is difficult to divide the whole dataset into several subsets for building calibrating and validating the model instead we use the 10 fold cross validation technique bengio and grandvalet 2004 to deal with this issue we randomly partition the dataset into 10 subsets of an equal size then we compute the mean value of the model performance for each subset if the 10 fold cross validation error is close to the error using the entire dataset it means the model built from the entire dataset is unlikely to be over fitted in doing so we are able to examine the performances of all models in the subsequent section the accuracy of the model indicates the mean value of the accuracies of 10 models based on all data subsets 3 results 3 1 conventional model the conventional method is based on a linear combination of the lower percentile of the cumulative rainfall volumes of flooding events and the upper percentile of the cumulative rainfall volumes of non flooding events fig 5 a shows all the cumulative rainfall curves of wet days daily rainfall depth 0 1 mm from 1 june 2014 to 14 june 2017 more than 60 of flooding events occur with intensive rainfall of short temporal scale e g 60 min to 360 min but also with larger accumulation blue dashed lines more than 90 of non flooding events have small rainfall volumes for instance daily accumulation being less than 20 mm however there are also exceptions where events with large rainfall volumes were reported as non flooding and vice versa we conducted an exhaustive search for all possible values of α β and µ between 0 and 1 and derived 112 pareto optimal threshold curves shown in fig 5 b four representative rainfall threshold curves are selected which havethe highest rate for at least one of the five model quality metrics threshold 1 has the highest values in terms of tnr 0 98 ppr 0 73 and acc 0 91 but also the lowest value of tpr 0 46 threshold 2 has the highest acc 0 91 but medium tpr 0 5 threshold 3 has the highest npr 0 98 and threshold 4 has the highest tpr 0 96 which are presented in fig 5 c and table 3 we can see that thresholds 1 and 2 ensure more non flooding events are correctly classified but also miss many flooding events thresholds 3 and 4 are more inclined to correctly classify flooding events which implies that many non flooding events can be labeled as flooding events based on these two thresholds threshold curves 1 and 2 are based on the lower 0 percentile α 0 of the rainfall depth for all the flooding events and the upper 100 percentile β 1 of the rainfall depth for all the non flooding events these curves use a coefficient of 0 and 0 2 to make the linear combination both curves have a low tpr meaning many actual flooding events are missed and a high tnr meaning non flooding is well captured acc is thus relatively high at 0 91 curves 3 and 4 have the highest npr and tpr respectively but very low acc this is because the threshold is low in fig 6 b ensuring flooding events are correctly classified but missing non flooding events in general it is difficult to find a threshold curve that can robustly indicate both flooding and non flooding events based on only the cumulative rainfall depth therefore we need other variables rather than only the cumulative rainfall depth to make a better classification 3 2 machine learning ml 3 2 1 prediction results with 10 features the first collection of ml models was trained based on ten rainfall volumes at 1 5 10 15 30 60 120 360 720 and 1440 min temporal resolutions with definitions given in eq 1 all of the ml models have an acc between 0 94 and 0 96 fig 6 except for one model with an acc of 0 92 this implies that only 16 to 25 events out of 413 events were misclassified in thirteen of the ml models used this shows a slightly better performance than that of the conventional model on the other hand the tpr has a larger variation ranging from 0 29 to 0 75 in other words the miss rate ranges from 0 25 to 0 71 among all the fourteen models the da family shows the most satisfactory performance specifically the quadratic da model 4 has the highest tpr 0 75 implying that 18 out of 24 actual flooding events can be well predicted while the ensemble da model 13 has the highest acc of 0 96 see fig 6 all the performance metrics are listed in appendix x1 although each ml model is easy to run with the complete set of all ten features the result cannot be visualized in a ten dimensional space resulting in difficulty interpreting results therefore we need to further reduce the number of dimensions to three or even fewer as shown in the subsequent section 3 2 2 prediction results with 2 features the second collection of ml models were trained by using two principal components which were derived from ten rainfall accumulations by running a principal component analysis the new features are linear combinations of the ten daily peak rainfall intensities at different temporal resolutions with a set of coefficients given in table 4 the first feature is a weighted sum with larger temporal scales receiving more weight explaining 97 5 of the total variance while the second feature has more weight at time scales between 30 and 120 min explaining 2 of the total variance therefore using these features can explain 99 5 of the original dataset when classifying the labeled events in principle one can easily compute the values of the two features for present or future events based on the combination of rainfall accumulation volumes if a study area has a coarser temporal resolution of rainfall measurement than that used here principal component analysis can be run based on historical data of coarser resolution to generate two new sets of weights all models using two features have a performance that is slightly worse than that of the ten feature models presented in section 3 2 1 the acc only drops by 0 01 to 0 02 for some models such as the medium decision trees model 2 and the fine subspace knn models 8 and 14 while other models model 3 4 5 7 and 13 do not see reduced acc in terms of the tpr fewer models reach 0 5 or higher compared to the models in section 3 2 1 however as seen from fig 7 the subspace da model 13 is still one of the best performing models two linear models namely the linear da model 3 and the linear svm model 5 also show a pareto optimal performance in terms of acc and tpr it should be noted that pareto optimality is a situation that cannot be modified so as to make any one individual or preference criterion better off without making at least one individual or preference criterion worse off models 3 5 and 13 are adopted for further discussion because they have the best performance regarding either tpr or acc as shown in fig 7 these three models in red perform better than other models in blue for both performance indicators the performance metrics of other models are listed in appendix x2 with two decision variables i e features we are able to visualize the outcome of the models in a two dimensional plane as shown in fig 8 models 3 5 and 13 determine rainfall threshold lines based on combinations of principle component feature 1 and feature 2 flooding and non flooding events occur to the right hand and left hand sides of each line respectively among these three models the threshold line from the linear da model is furthest left so classifies more events as flooding while the linear svm is the furthest right so classifies fewer events as flooding the subspace da provides a threshold line in between the other two note that fig 8 offers an intuitive look up graph that one can easily tell whether an event is flooding or not based on the values of two features for instance a combination of feature 1 of 60 mm and feature 2 of 10 mm is predicted to not be a flooding event but a combination of feature 1 of 100 mm and feature 2 of 10 mm is predicted to be a flooding event according to all models however further effort is still required to classify an event falling in the area between the lines of model 3 and model 5 as the three models may give different answers we further elaborate on the fact that the nature of the data can lead to different thresholds from each of the three ml models in the discussion section below 4 discussion 4 1 ml model compared to current rainfall threshold and cumulative rainfall threshold we first elaborate on how the proposed ml model estimates the rainfall threshold better than the current empirical threshold provided by the local authority smb 2019 the threshold suggests any event is regarded as a pluvial flood if either 30 min rainfall depth is over 20 mm or 3 h rainfall depth is over 80 mm this threshold and the historical data points are shown in fig 9 as the 3 h rainfall threshold is placed too high many flooding events are missed resulting in a bad result for the tpr only 0 25 although the overall acc is good 0 95 as a large number of non flooding events are correctly predicted in other words the miss rate for flooding events is very high i e 0 75 even if the ml model is built based on a single feature namely 30 min or 3 h rainfall depth the ml model is still able to explore the dataset and find thresholds we used one of the proposed ml models specifically the subspace da model as one of the models with the best performance to test the performance when using the same feature s of 30 min rainfall depth 3 h rainfall depth or their combination the da model suggests that the threshold should be either 30 min rainfall depth of 12 5 mm fig 10 a 3 h rainfall depth of 29 1 mm fig 10 b or a combination of these fig 10 c performance tpr s are all higher than 0 54 which is more than twice the tpr using the empirical rainfall threshold detailed metrics are shown in appendix x3 this means that the machine learning models can improve the current empirical rainfall threshold to a great extent next we compare the performance of conventional cumulative critical rainfall curves to those derived from the ml models the results show that ml models especially linear discriminant analysis can classify flooding and non flooding by two principle components raising the acc and tpr to 96 and 58 respectively and lowering the false alert rate to 25 compared to the conventional model the critical indices of acc and tpr were 5 15 higher in ml models therefore in general ml models can better classify flooding and non flooding events than the conventional empirical method based on different temporal resolutions of rainfall measurements the minimum temporal resolution for the input of our ml models is 1 minute however the method is generic the minimum temporal resolution can also be 5 min or 10 min to re train the model to train the ml model the user needs reports or observations of flooding and non flooding events these inputs rainfall and flood reports are identical to the inputs needed by the conventional method 4 2 pros and cons of the machine learning ml model ml models can successfully produce rainfall thresholds for urban pluvial flooding the model only needs to be run once and the water system manager operator can simply use a look up graph to determine whether a pluvial flood is likely to occur the features can be flexibly selected using either the entire 10 features or fewer representative features by running a pca however ml is a data derived method which largely relies on the quantity and the quality of data available for example five points which are circled in fig 11 regarded as tricky events can influence the output when using different models these events have similar rainfall conditions but they are categorized by the ml models differently in reality three are flooding events and two are non flooding events the models only make decisions based on data resulting indifferent threshold lines for models 3 5 and 13 model 3 includes these five points in the set of flooding events thereby making two predictions incorrectly model 5 excludes these five points from the set flooding events thereby making three predictions incorrectly model 13 draws a threshold line in between thereby making only one prediction incorrectly potentially more data points lying in between the threshold line of model 3 and the threshold line of model 5 can improve the model to make predictions more precisely in this work historical inundation records were collected through a flood report system a smart phone application however not all the municipality s citizens are aware of this reporting system this limits the number of the records thus affecting the tpr i e increased missed alerts in addition each citizen s socio economic background education level and experience with pluvial flooding influence the records as well for instance inundation caused by blockage of sewers pipes at home can be wrongly reported as inundation caused by rainfall this undoubtedly increased the number of false positives since the current flood report system does not provide information on the reasons for inundation false inundation records cannot be filtered out it should also be noted that our ml models were applied over the entire city of shenzhen in this study due to the limited number of data points if more data become available the model can be further refined to a district a community or a street similarly it can also be applied to other urban rural catchments given an available rainfall flooding database as more available data can be collected in the future even with images and text descriptions we also aim to test deep learning algorithms to increase the accuracy of the flood prediction model 5 conclusion despite uncertainty about the inundation records and ml models this data driven method provides a basis for generating rainfall thresholds for flood early warning and emergency response in shenzhen the objective of this paper is to predict the occurrence of urban pluvial flooding by ml approaches it concludes that ml models can determine the rainfall flooding threshold as a line projected in a plane spanned by two principal components thereby providing a binary result flood or no flood compared to the conventional empirical critical rainfall curve the proposed models especially the subspace discriminant analysis algorithm can better classify flooding and non flooding events by different combinations of multi resolution rainfall intensities greatly raising the acc to 96 5 and lowering the false alert rate to 25 such models are applicable to other urban catchments as well extreme weather events in the future due to global climate change will bring high intensity rainfall of short duration westra et al 2014 advanced techniques such as radar observations can efficiently improve very short range rainfall forecasts which are essential for accurate flood prediction yang et al 2016 precipitation is the dominant input influencing the flood prediction result other factors like soil characteristics drainage capacity and topography e g land subsidence would affect the result as well emphasizing the need for updated data driven flooding thresholds since rainfall threshold based flood prediction can be executed rapidly and simply this method allows decision makers e g emergency managers time for a high level assessment of flood risk providing valuable lead time for citizens in the flood prone areas to be warned probability thresholds which can help understand the uncertainties involved need to be investigated further although the inundation records contain information about occurrence locations and estimated inundation depths these data were not utilized analysed in this study further study on the correlation of spatial distribution of inundation and inundation depth with the spatially varying rainfall records will be valuable as well credit authorship contribution statement qian ke conceptualization methodology data curation writing original draft xin tian conceptualization methodology data curation writing original draft jeremy bricker supervision writing review editing zhan tian conceptualization methodology supervision writing review editing guanghua guan visualization investigation huayang cai visualization investigation xinxing huang visualization investigation honglong yang data curation junguo liu writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to thank the sponsors of this work the national key r d program of china grant no 2018yfe0206200 the national natural science foundation of china grant no 41671113 and 51761135024 the netherlands organisation for scientific research nwo grant no alwsd 2016 007 jpi urban europe era net co fund smart urban futures project no 646453 nwo verdus smart urban regions of the future surf and the engineering and physical sciences research council of uk grant no r034214 1 the high level special funding of the southern university of science and technology grant no g02296302 g02296402 we sincerely appreciate three reviewer s comments and suggestions which improved the quality of the article we also would like to show our gratitude to meteorological bureau of shenzhen municipality and water sector of shenzhen municipality for providing the rainfall data and inundation records in this paper supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103719 appendix b supplementary materials image application 1 appendices x1 performance metrics of 14 trained machine learning models based on 10 features no models tp fn fp tn tpr tnr ppr npr accuracy 1 coarse tree 13 11 7 382 0 54 0 98 0 65 0 97 0 956 2 medium tree 14 10 11 378 0 58 0 97 0 56 0 97 0 949 3 linear discriminant 14 10 7 382 0 58 0 98 0 67 0 97 0 959 4 quadratic discriminant 18 6 29 360 0 75 0 93 0 38 0 98 0 919 5 linear svm 11 13 2 387 0 46 0 99 0 85 0 97 0 964 6 quadratic svm 11 13 2 387 0 46 0 99 0 85 0 97 0 964 7 cubic svm 10 14 6 383 0 42 0 98 0 63 0 97 0 952 8 fine knn 11 13 9 380 0 46 0 98 0 55 0 97 0 946 9 medium knn 10 14 3 386 0 42 0 99 0 77 0 97 0 959 10 cos knn 7 17 4 385 0 29 0 99 0 64 0 96 0 950 11 cubic knn 10 14 3 386 0 42 0 99 0 77 0 97 0 959 12 bagged trees 12 12 6 383 0 50 0 98 0 67 0 97 0 956 13 subspace discriminant 14 10 5 384 0 58 0 99 0 74 0 97 0 964 14 subspace knn 14 10 10 379 0 58 0 97 0 58 0 97 0 951 x2 performance metrics of 14 trained machine learning models based on 2 features derived from a pca no models tp fn fp tn tpr tnr ppr npr accuracy 1 coarse tree 10 14 8 381 0 42 0 98 0 56 0 96 0 947 2 medium tree 10 14 16 373 0 42 0 96 0 38 0 96 0 927 3 linear discriminant 14 10 6 383 0 58 0 98 0 70 0 97 0 961 4 quadratic discriminant 14 10 10 379 0 58 0 97 0 58 0 97 0 952 5 linear svm 12 12 3 386 0 50 0 99 0 80 0 97 0 964 6 quadratic svm 9 15 4 385 0 38 0 99 0 69 0 96 0 954 7 cubic svm 7 17 3 386 0 29 0 99 0 70 0 96 0 952 8 fine knn 8 16 12 377 0 33 0 97 0 40 0 96 0 932 9 medium knn 7 17 1 388 0 29 1 00 0 88 0 96 0 956 10 cos knn 0 24 1 388 0 00 1 00 0 00 0 94 0 939 11 cubic knn 5 19 1 388 0 21 1 00 0 83 0 95 0 952 12 bagged trees 10 14 10 379 0 42 0 97 0 50 0 96 0 942 13 subspace discriminant 13 11 4 385 0 54 0 99 0 76 0 97 0 964 14 subspace knn 7 17 11 378 0 29 0 97 0 39 0 96 0 932 x3 performance metrics of 14 trained machine learning models based on a 30 min rainfall b 3 h rainfall c both 30 min and 3 h rainfall no models tp fn fp tn tpr tnr ppr npr accuracy a based on 30 min rainfall 13 11 9 380 0 54 0 98 0 59 0 97 0 952 b based on 3 h rainfall 15 9 7 382 0 63 0 98 0 68 0 98 0 961 c based on 30 min and 3 h rainfall 14 10 5 384 0 58 0 99 0 74 0 97 0 964 
395,urban pluvial flooding is a threatening natural hazard in urban areas all over the world especially in recent years given its increasing frequency of occurrence in order to prevent flood occurrence and mitigate the subsequent aftermath urban water managers aim to predict precipitation characteristics including peak intensity arrival time and duration so that they can further warn inhabitants in risky areas and take emergency actions when forecasting a pluvial flood previous studies that dealt with the prediction of urban pluvial flooding are mainly based on hydrological or hydraulic models requiring a large volume of data for simulation accuracy these methods are computationally expensive using a rainfall threshold to predict flooding based on a data driven approach can decrease the computational complexity to a great extent in order to prepare cities for frequent pluvial flood events especially in the future climate this paper uses a rainfall threshold for classifying flood vs non flood events based on machine learning ml approaches applied to a case study of shenzhen city in china in doing so ml models can determine several rainfall threshold lines projected in a plane spanned by two principal components which provides a binary result flood or no flood compared to the conventional critical rainfall curve the proposed models especially the subspace discriminant analysis can classify flooding and non flooding by different combinations of multiple resolution rainfall intensities greatly raising the accuracy to 96 5 and lowering the false alert rate to 25 compared to the conventional model the critical indices of accuracy and true positive rate tpr were 5 15 higher in ml models such models are applicable to other urban catchments as well the results are expected to be used to assist early warning systems and provide rational information for contingency and emergency planning keywords urban pluvial flooding rainfall threshold machine learning shenzhen city 1 introduction 1 1 background urban pluvial flooding is a threat to a great number of cities worldwide especially given its increasing frequency of occurrence in recent years martina et al 2006 atta ur rahman et al 2016 ziegler 2012 its impact including loss of life and damages to both public and private properties can be further deepened by climate change and accelerated urbanization falconer et al 2009 this type of flooding usually occurs when intense rainfall exceeds the capacity of an urban drainage system recent extreme precipitation events have raised awareness from both authorities and citizens to the challenges of predicting and managing urban pluvial floods in july 2019 heavy rain caused at least 18 deaths and triggered massive transport disruptions in mumbai india bbc 2019 in the uk about 40 of damages and associated economic losses in cities are estimated to result from pluvial flooding douglas et al 2010 in china 98 of cities are exposed or vulnerable to frequent floods jiang et al 2017 a survey conducted between 2008 and 2010 showed that 218 chinese cities suffered severe urban pluvial flooding at least once and more than 100 cities experienced it more than three times jiang et al 2018 therefore urban pluvial flood prediction and management is a critical topic in the context of urban water management in order to prevent pluvial flooding and its consequences city authorities e g meteorological offices emergency agency offices or water authorities usually need to make predictions of pluvial floods this is based on good prediction of precipitation characteristics such as peak intensity arrival time and duration many studies investigate the prediction of pluvial flooding by hydraulic models li 2020 li and willems 2019 by simulating the inundated area and depth given certain historical or predicted rainfall scenarios babaei et al 2018 thorndahl et al 2016 xing et al 2019 however hydraulic models need a large volume of data and computational resources as a result the output of a hydraulic model is usually case specific in other words we have to run the model to make predictions for flooding duringeachseparate rainfall scenario on the other hand using a rainfall threshold based on data driven models can provide an easy and intuitive solution by comparing the current predicted rainfall with the threshold one can straightforwardly estimate the likelihood of the city being flooded martina et al 2006 montesarchio et al 2011 tian et al 2019 yang et al 2016 specifically a rainfall threshold specifies one or several rainfall depth s over certain time windows above which a pluvial flood is likely to occur moreover rainfall threshold based hazard prediction is widely applied to landslides garcia urquia and axelsson 2015 giannecchini et al 2012 golian et al 2015 hong et al 2018 martelloni et al 2012 debris flow nikolopoulos et al 2014 pan et al 2018 van asch et al 2014 and flash floods montesarchio et al 2011 norbiato et al 2008 zhai et al 2018 to determine a cumulative rainfall threshold a physically based model is usually needed to compute critical rainfall thresholds over time norbiato et al 2008 yang et al 2016 or a statistical data driven analysis can be applied carpenter et al 1999 golian et al 2010 martina et al 2006 montesarchio et al 2011 however on the one hand there is a gap in short term prediction capability of physical models costabile and macchione 2015 short lead time flood prediction is of crucial importance for highly urbanized areas in order to provide timely warnings to residents zhang et al 2018 on the other hand statistical models have a limitation on the accuracy of prediction fawcett and stone 2010 furthermore urban catchments often lack sufficient data on both the drainage network and topography complicating the estimate of rainfall threshold yang et al 2016 machine learning ml models can deal with data scarcity based on an ensemble method breiman 2001 therefore in this paper we use ml approaches to derive the flooding thresholds for different rainfall duration periods ml is a family of algorithms derived from statistics and computer science which aims to train mathematical models to make predictions or decisions based on observed samples ml is suggested as an effective tool to explore the connectedness between human and water systems shen et al 2018 the latter is anticipated to be a key interdisciplinary issue to deal with in future hydrological studies vogel et al 2015 moreover ml models can numerically reproduce flood nonlinearity solely based on historical data without requiring knowledge about the underlying physical processes mosavi et al 2018 therefore this study utilizes ml algorithms to attempt to classify the presence or absence of flooding based on rainfall characteristics although ml algorithms have shown powerful applicability to flood prediction and forecasting liu et al 2017 mosavi et al 2018 noymanee et al 2017 tayfur et al 2018 there are still very few studies that utilize ml to classify or predict urban pluvial flooding which is a challenge due to lack of flood inundation data drainage system data and fine resolution topography data yang et al 2016 therefore we aim to test ml algorithms for classifying urban flooding in the city of shenzhen which is frequently flooded a sudden rainstorm event claimed 11 lives in april 2019 in shenzhen hua 2019 attracting great attention for the local authorities to reconsider the early warning system for pluvial flooding in the city moreover shenzhen is a pioneer city in terms of high technology development socio cultural development and disaster emergency management this experience can be shared with other cities in china and abroad the paper is organized as follows section 2 describes the study area and data used for this study and introduces the conventional and ml methods for flood prediction section 3 shows the results of the models and proposes the rainfall threshold for shenzhen section 4 compares the ml results for rainfall thresholds to the current rainfall threshold and cumulative rainfall threshold in shenzhen section 5 presents the conclusions and recommendations 2 materials and methodology 2 1 study area in the past decades shenzhen has grown rapidly from a rural area to a prosperous economic zone and an important industrial city in southern china it is located on the central coast of guangdong province which is the passageway from mainland china to hong kong see fig 1 it is also an important city in the pearl river delta prd it has a total land area of 1 948 km2 the average elevation is 3 4 m above mean sea level rainstorm induced catastrophes in shenzhen city are mostly caused by persistent short duration heavy rainfall in the summer zhou et al 2017 pluvial flooding is one of the primary natural hazards in shenzhen in recent years urbanization has increased the surface runoff and intensified the flood frequency shi et al 2007 yan et al 2019 shenzhen is identified as an area under a high flood risk since many properties are built in flood prone areas such as the harbour front area chan et al 2014 the total average annual precipitation is 1 900 mm y of which rainstorms caused by typhoons july september make up 36 i e 689 mm y and approximately 85 of precipitation occurs from april to september see fig 2 data source meteorological bureau of shenzhen smb convective march june and typhoon rainstorms july october are the two main rainfall sources in this region as of 2019 shenzhen has a population of 13 million with a population density of 6 234 people km2 most of the city is drained by a separated storm sewer system 4 883 92 km whereas the remaining area 1 693 km is drained by a combined sewer system i e wastewater combined with rainwater sewer system with a drainage pipe density of 12 5km km2 ssb 2019 in total 126 municipal pumps with a capacity of 671 m3 s are used to drain stormwater out of the city ssb 2019 short duration high intensity rainfall is the main driver of pluvial flooding in shenzhen due to the rapid pace of urbanization the impervious area has significantly increased while the water storage area such as rivers lakes and wetlands has decreased with climate change increasing frequency of typhoon occurrence and intensity of torrential rainfall tracy et al 2007 pluvial flooding has a high likelihood of occurrence in the paved area on may 11 2014 for instance the daily rainfall volume reached 233 mm and some districts experienced a peak rainfall intensity of 310 mm in 6 hours cai 2014 currently smb uses a rainfall threshold for predicting urban pluvial flooding only based on 30 min rainfall depth i e 20 mm or 3 h rainfall depth i e 80 mm smb 2019 in the subsequent sections we will further testify and compare this threshold with that from the proposed ml models 2 2 records of flood events records of historical flood events from 1 june 2014 to 14 june 2017 consisting of 1 110 days and 663 records in total were retrieved from the water sector of shenzhen municipality http swj sz gov cn which has developed and implemented a disaster reporting system i e a flood report app named shenzhensanfang since 2014 citizens of shenzhen can report flood events via this system at any time these records register the date the location geotagging and a description as most of the records indicating pluvial flood events fall in the period between june and september 640 records i e 96 5 we only consider data points in the summer of each year namely 413 days in total over the 3 year study period in doing so we can exclude hundreds of non flooding events to lower the imbalance of the dataset too many non flooding events and too few flooding events note that the high frequency of the flooding record corresponds to the precipitation characteristics in shenzhen the 640 records were registered over 24 days c a 27 records d which are regarded as days with floods the remaining 389 days of the study period are regarded as days without floods these records are spatially distributed throughout the whole city see fig 3 it should be noted that as the inundation records were submitted by citizens socio economic background such as age education level and experience with previous pluvial flooding may affect the recording this may cause false alerts or missed alerts 2 3 rainfall observations the rainfall intensity each minute at 25 rainfall gauges see fig 3 from 1 june 2014 to 14 june 2017 was retrieved from smb http weather sz gov cn we used areal average rainfall intensity to represent the study area which stands for the mean value of rainfall intensities of all study sub areas districts the original database consisted of 1 min rainfall intensity these1 min rainfall intensities were aggregated to rainfall volumes of longer temporal scale namely 5 10 15 30 60 120 360 720 and 1440 mins each day the maximum rainfall volume at each temporal scale denoted as rdx in mm is calculated by eq 1 tian et al 2019 1 r d x max j k 1 x r 1 k k 1 j x j 1 x r 1 k k 1441 x 1440 r 1 k where j 0 1 1440 x min x 1 5 10 15 30 60 120 360 720 1440 min note that each item in the bracket of eq 1 stands for x min rainfall volume accumulated from 1 min rainfall intensity in the interval 1 j x j 1 x 2 4 flood classification models in this study we first apply a conventional rainfall curve method as a benchmark then we further develop multiple parametric and non parametric ml models to classify flooding and non flooding events based on rainfall intensities with respect to a binary classification problem four possible predicted outcomes are expected see table 1 namely true positives tp or correctly classified flooding events false positives fp or falsely classified flooding events true negatives tn or correctly classified non flooding events and false negatives fn or missed flooding events ideally an urban flood classification model should achieve a high true positive rate tpr a high true negative rate tnr and high overall accuracy acc on the other hand a prediction model with a low positive predictive rate ppr or a low tpr implies that a number of actual flood events are wrongly labeled or unexpectedly missed acc is also called the proportion of correct forecasts wilks 2005 2 5 conventional model with cumulative rainfall volume thresholds the cumulative rainfall volume threshold is a reference curve representing a cumulative amount of rainfall over a certain time window see fig 4 when the observed cumulative rainfall exceeds the threshold at a given moment flooding is expected to occur we propose a way to determine a threshold curve via the following steps 1 calculating the cumulative rainfall max in 24 hours based on the 1 min rainfall intensity for all flooding and non flooding events 2 computing the lower α percentile of the 1 min rainfall for all flooding events denoted as t α note α is to be determined in step 5 in doing so t α depicts a curve that a certain number of cumulative rainfall curves for flooding events stay above for instance all curves of flooding events are above the curve t α α 0 3 computing the upper β percentile of the 1 min rainfall for all non flooding events denoted as t β note β is also to be determined in step 5 the definition of t β is analogous to that of t α t β depicts a curve that a certain number of cumulative rainfall curves for flooding events stay below 4 constituting a linear combination of t α and t β based on a weight µ namely t α β µ µ t α 1 µ t β as a result we obtain a rainfall threshold based on three variables α β and µ any assigned values can result in a given cumulative rainfall threshold curve and its corresponding model performance 5 solving an optimization problem that maximizes the model performance by tuning α β and µ three optimal combinations for α β and µ were pursued aiming forthe maximum tpr the highest tnr and the highest acc 2 max α β μ perfofmance determined by t α β μ perfofmance tpr tnr or accuracy 2 5 1 machine learning ml algorithms machine learning ml algorithms are a collection of computational data driven methods without utilizing a pre defined equation as the basic model ml algorithms train a model using a certain type of algorithms fully based on known data whereas the trained model can be applied to new data as the number of training data sets increases the performance of ml algorithmscan improve ml consists of two families namely supervised learning and unsupervised learning specifically supervised learning algorithms aim to find functions that are able to map inputs to labeled outputs also including two categories classification and regression flooding prediction is commonly an application of classification jhong et al 2018 tayfur et al 2018 zhou et al 2018 which aims to distinguish flood events vs no flood events based on hydrological variables i e a binary classification problem given the size of the database available we adopt a collection of models in this study that usually show good performance for small to medium sized data sets 14 classification algorithms from 5 major ml families are considered to classify urban pluvial flooding based on rainfall intensities of multiple temporal scales table 2 brief introductions of these algorithms follow decision trees decision trees build a tree shaped top down structure from the roof at the top to leaf nodes at the bottom breiman et al 2017 each leaf node represents a predicted response given the fact that we focus on a binary classification problem the bifurcation starts from one parent node of a given layer to two child nodes of a subsequent lower layer relying on different values of variables specifically to find the optimal bifurcation we maximize gini s diversity index but stop maximization when i a node only contains a single class of data ii a child node to be generated contains fewer than five data points or iii the number of layers exceeds a pre defined criterion five for a coarse decision tree and twenty for a medium decision tree in general decision tree learning is one of the fastest algorithms its resultsare also easy to interpret we built the decision tree model in matlab by using the function fitctree discriminant analysis discriminant analysis da classifiers assumes a gaussian distribution for data of each class the gaussian distribution is determined by the sample mean of each class and the identical covariance matrix for linear da or different class based covariance matrices for quadratic da under this assumption linear or quadratic das make predictions by minimizing prediction costs based on bayes theorem note that the prediction costs are the sum of the multiplication of the posterior probability of a given class k for a data sample and the cost of classifying a sample as y but its actual class is k 0 for accurate classification and 1 for misclassification readers can refer to ledoit and wolf 2004 t hastie r tibshirani 2008 for more details note that this study considers both linear and quadratic discriminant analyses as their names suggest linear discriminant analysis can only learn linear boundaries while quadratic discriminant analysis can learn quadratic boundaries both of which are fast to run and easy to interpret we build the discriminant analysis model in matlab by using the function fitcdiscr support vector machine linear support vector machine svm applied to binary classification aims to find an optimal hyperplane that separates two classes with a margin of the maximal width in other words we look for the maximum margin widthwhile keeping the data of two classes on each side of the margin samples that are misclassified are penalized using kernel functions such as quadratic and cubic kernels can turn a linear svm into a non linear svm the latter is more flexible but also requires more computational resources and becomes less straightforward to explain readers can refer to ng 2000 for more details we build the support vector machine model in matlab using the function fitcsvm k nearest neighbor k nearest neighbor is a distance based learning technique that determines the predicted response of a given point by checking the major class of the k closest points cover and hart 1967 note that we use the euclidean and cosine distance as the metric to measure the closeness between points the knn algorithm is one of the easiest and most intuitive learning techniques widely used in many applications cheng et al 2014 zhang 2016 however it is also very sensitive to outliers ramaswamy et al 2000 which we may encounter frequently when predicting urban flooding based on rainfall intensities we build the k nn model in matlab using the function fitcknn bagged trees breiman et al 1984 breiman 2001 bagging stands for a type of ensemble learning which is used to reduce the variance of a single decision tree to build a bagged tree model we create multiple subsets of new data from original samples which are chosen randomly with replacement as a result we obtain an ensemble of decision trees also referred to as weak learners and they are proven to be more robust than a single decision tree we build the bagged trees model in matlab using the function fitcensemble subspace ensembles the random subspace method is also an ensemble technique to increase the accuracy of the discriminant classifier and knn classifier the subspace ensemble aims to train random sample features rather than the entire feature set it is proven to be an effective method to deal with the issue of high dimensional feature sets and small training sets as the name suggests classifiers are constructed in a random subspace of data feature space and then combined by simple majority voting readers can refer to tin kam ho 1998 for details garcía pedrajas and ortiz boyer 2009 skurichina and duin 2002 also prove that the random subspace method can be further used for da and knn which are applied in our study we build the subspace da and subspace knn models in matlab using the function fitcensemble 2 5 2 feature selection and model validation all the models listed in table 2 are first tested on ten features which are the ten multi temporal rainfall accumulationsrdx see eq 1 later we also run a principal component analysis pca based on the singular value decomposition method madsen et al 2004 to reduce the number of dimensions and find the most meaningful components for predicting flooding events as we only have a small dataset with 413 data points it is difficult to divide the whole dataset into several subsets for building calibrating and validating the model instead we use the 10 fold cross validation technique bengio and grandvalet 2004 to deal with this issue we randomly partition the dataset into 10 subsets of an equal size then we compute the mean value of the model performance for each subset if the 10 fold cross validation error is close to the error using the entire dataset it means the model built from the entire dataset is unlikely to be over fitted in doing so we are able to examine the performances of all models in the subsequent section the accuracy of the model indicates the mean value of the accuracies of 10 models based on all data subsets 3 results 3 1 conventional model the conventional method is based on a linear combination of the lower percentile of the cumulative rainfall volumes of flooding events and the upper percentile of the cumulative rainfall volumes of non flooding events fig 5 a shows all the cumulative rainfall curves of wet days daily rainfall depth 0 1 mm from 1 june 2014 to 14 june 2017 more than 60 of flooding events occur with intensive rainfall of short temporal scale e g 60 min to 360 min but also with larger accumulation blue dashed lines more than 90 of non flooding events have small rainfall volumes for instance daily accumulation being less than 20 mm however there are also exceptions where events with large rainfall volumes were reported as non flooding and vice versa we conducted an exhaustive search for all possible values of α β and µ between 0 and 1 and derived 112 pareto optimal threshold curves shown in fig 5 b four representative rainfall threshold curves are selected which havethe highest rate for at least one of the five model quality metrics threshold 1 has the highest values in terms of tnr 0 98 ppr 0 73 and acc 0 91 but also the lowest value of tpr 0 46 threshold 2 has the highest acc 0 91 but medium tpr 0 5 threshold 3 has the highest npr 0 98 and threshold 4 has the highest tpr 0 96 which are presented in fig 5 c and table 3 we can see that thresholds 1 and 2 ensure more non flooding events are correctly classified but also miss many flooding events thresholds 3 and 4 are more inclined to correctly classify flooding events which implies that many non flooding events can be labeled as flooding events based on these two thresholds threshold curves 1 and 2 are based on the lower 0 percentile α 0 of the rainfall depth for all the flooding events and the upper 100 percentile β 1 of the rainfall depth for all the non flooding events these curves use a coefficient of 0 and 0 2 to make the linear combination both curves have a low tpr meaning many actual flooding events are missed and a high tnr meaning non flooding is well captured acc is thus relatively high at 0 91 curves 3 and 4 have the highest npr and tpr respectively but very low acc this is because the threshold is low in fig 6 b ensuring flooding events are correctly classified but missing non flooding events in general it is difficult to find a threshold curve that can robustly indicate both flooding and non flooding events based on only the cumulative rainfall depth therefore we need other variables rather than only the cumulative rainfall depth to make a better classification 3 2 machine learning ml 3 2 1 prediction results with 10 features the first collection of ml models was trained based on ten rainfall volumes at 1 5 10 15 30 60 120 360 720 and 1440 min temporal resolutions with definitions given in eq 1 all of the ml models have an acc between 0 94 and 0 96 fig 6 except for one model with an acc of 0 92 this implies that only 16 to 25 events out of 413 events were misclassified in thirteen of the ml models used this shows a slightly better performance than that of the conventional model on the other hand the tpr has a larger variation ranging from 0 29 to 0 75 in other words the miss rate ranges from 0 25 to 0 71 among all the fourteen models the da family shows the most satisfactory performance specifically the quadratic da model 4 has the highest tpr 0 75 implying that 18 out of 24 actual flooding events can be well predicted while the ensemble da model 13 has the highest acc of 0 96 see fig 6 all the performance metrics are listed in appendix x1 although each ml model is easy to run with the complete set of all ten features the result cannot be visualized in a ten dimensional space resulting in difficulty interpreting results therefore we need to further reduce the number of dimensions to three or even fewer as shown in the subsequent section 3 2 2 prediction results with 2 features the second collection of ml models were trained by using two principal components which were derived from ten rainfall accumulations by running a principal component analysis the new features are linear combinations of the ten daily peak rainfall intensities at different temporal resolutions with a set of coefficients given in table 4 the first feature is a weighted sum with larger temporal scales receiving more weight explaining 97 5 of the total variance while the second feature has more weight at time scales between 30 and 120 min explaining 2 of the total variance therefore using these features can explain 99 5 of the original dataset when classifying the labeled events in principle one can easily compute the values of the two features for present or future events based on the combination of rainfall accumulation volumes if a study area has a coarser temporal resolution of rainfall measurement than that used here principal component analysis can be run based on historical data of coarser resolution to generate two new sets of weights all models using two features have a performance that is slightly worse than that of the ten feature models presented in section 3 2 1 the acc only drops by 0 01 to 0 02 for some models such as the medium decision trees model 2 and the fine subspace knn models 8 and 14 while other models model 3 4 5 7 and 13 do not see reduced acc in terms of the tpr fewer models reach 0 5 or higher compared to the models in section 3 2 1 however as seen from fig 7 the subspace da model 13 is still one of the best performing models two linear models namely the linear da model 3 and the linear svm model 5 also show a pareto optimal performance in terms of acc and tpr it should be noted that pareto optimality is a situation that cannot be modified so as to make any one individual or preference criterion better off without making at least one individual or preference criterion worse off models 3 5 and 13 are adopted for further discussion because they have the best performance regarding either tpr or acc as shown in fig 7 these three models in red perform better than other models in blue for both performance indicators the performance metrics of other models are listed in appendix x2 with two decision variables i e features we are able to visualize the outcome of the models in a two dimensional plane as shown in fig 8 models 3 5 and 13 determine rainfall threshold lines based on combinations of principle component feature 1 and feature 2 flooding and non flooding events occur to the right hand and left hand sides of each line respectively among these three models the threshold line from the linear da model is furthest left so classifies more events as flooding while the linear svm is the furthest right so classifies fewer events as flooding the subspace da provides a threshold line in between the other two note that fig 8 offers an intuitive look up graph that one can easily tell whether an event is flooding or not based on the values of two features for instance a combination of feature 1 of 60 mm and feature 2 of 10 mm is predicted to not be a flooding event but a combination of feature 1 of 100 mm and feature 2 of 10 mm is predicted to be a flooding event according to all models however further effort is still required to classify an event falling in the area between the lines of model 3 and model 5 as the three models may give different answers we further elaborate on the fact that the nature of the data can lead to different thresholds from each of the three ml models in the discussion section below 4 discussion 4 1 ml model compared to current rainfall threshold and cumulative rainfall threshold we first elaborate on how the proposed ml model estimates the rainfall threshold better than the current empirical threshold provided by the local authority smb 2019 the threshold suggests any event is regarded as a pluvial flood if either 30 min rainfall depth is over 20 mm or 3 h rainfall depth is over 80 mm this threshold and the historical data points are shown in fig 9 as the 3 h rainfall threshold is placed too high many flooding events are missed resulting in a bad result for the tpr only 0 25 although the overall acc is good 0 95 as a large number of non flooding events are correctly predicted in other words the miss rate for flooding events is very high i e 0 75 even if the ml model is built based on a single feature namely 30 min or 3 h rainfall depth the ml model is still able to explore the dataset and find thresholds we used one of the proposed ml models specifically the subspace da model as one of the models with the best performance to test the performance when using the same feature s of 30 min rainfall depth 3 h rainfall depth or their combination the da model suggests that the threshold should be either 30 min rainfall depth of 12 5 mm fig 10 a 3 h rainfall depth of 29 1 mm fig 10 b or a combination of these fig 10 c performance tpr s are all higher than 0 54 which is more than twice the tpr using the empirical rainfall threshold detailed metrics are shown in appendix x3 this means that the machine learning models can improve the current empirical rainfall threshold to a great extent next we compare the performance of conventional cumulative critical rainfall curves to those derived from the ml models the results show that ml models especially linear discriminant analysis can classify flooding and non flooding by two principle components raising the acc and tpr to 96 and 58 respectively and lowering the false alert rate to 25 compared to the conventional model the critical indices of acc and tpr were 5 15 higher in ml models therefore in general ml models can better classify flooding and non flooding events than the conventional empirical method based on different temporal resolutions of rainfall measurements the minimum temporal resolution for the input of our ml models is 1 minute however the method is generic the minimum temporal resolution can also be 5 min or 10 min to re train the model to train the ml model the user needs reports or observations of flooding and non flooding events these inputs rainfall and flood reports are identical to the inputs needed by the conventional method 4 2 pros and cons of the machine learning ml model ml models can successfully produce rainfall thresholds for urban pluvial flooding the model only needs to be run once and the water system manager operator can simply use a look up graph to determine whether a pluvial flood is likely to occur the features can be flexibly selected using either the entire 10 features or fewer representative features by running a pca however ml is a data derived method which largely relies on the quantity and the quality of data available for example five points which are circled in fig 11 regarded as tricky events can influence the output when using different models these events have similar rainfall conditions but they are categorized by the ml models differently in reality three are flooding events and two are non flooding events the models only make decisions based on data resulting indifferent threshold lines for models 3 5 and 13 model 3 includes these five points in the set of flooding events thereby making two predictions incorrectly model 5 excludes these five points from the set flooding events thereby making three predictions incorrectly model 13 draws a threshold line in between thereby making only one prediction incorrectly potentially more data points lying in between the threshold line of model 3 and the threshold line of model 5 can improve the model to make predictions more precisely in this work historical inundation records were collected through a flood report system a smart phone application however not all the municipality s citizens are aware of this reporting system this limits the number of the records thus affecting the tpr i e increased missed alerts in addition each citizen s socio economic background education level and experience with pluvial flooding influence the records as well for instance inundation caused by blockage of sewers pipes at home can be wrongly reported as inundation caused by rainfall this undoubtedly increased the number of false positives since the current flood report system does not provide information on the reasons for inundation false inundation records cannot be filtered out it should also be noted that our ml models were applied over the entire city of shenzhen in this study due to the limited number of data points if more data become available the model can be further refined to a district a community or a street similarly it can also be applied to other urban rural catchments given an available rainfall flooding database as more available data can be collected in the future even with images and text descriptions we also aim to test deep learning algorithms to increase the accuracy of the flood prediction model 5 conclusion despite uncertainty about the inundation records and ml models this data driven method provides a basis for generating rainfall thresholds for flood early warning and emergency response in shenzhen the objective of this paper is to predict the occurrence of urban pluvial flooding by ml approaches it concludes that ml models can determine the rainfall flooding threshold as a line projected in a plane spanned by two principal components thereby providing a binary result flood or no flood compared to the conventional empirical critical rainfall curve the proposed models especially the subspace discriminant analysis algorithm can better classify flooding and non flooding events by different combinations of multi resolution rainfall intensities greatly raising the acc to 96 5 and lowering the false alert rate to 25 such models are applicable to other urban catchments as well extreme weather events in the future due to global climate change will bring high intensity rainfall of short duration westra et al 2014 advanced techniques such as radar observations can efficiently improve very short range rainfall forecasts which are essential for accurate flood prediction yang et al 2016 precipitation is the dominant input influencing the flood prediction result other factors like soil characteristics drainage capacity and topography e g land subsidence would affect the result as well emphasizing the need for updated data driven flooding thresholds since rainfall threshold based flood prediction can be executed rapidly and simply this method allows decision makers e g emergency managers time for a high level assessment of flood risk providing valuable lead time for citizens in the flood prone areas to be warned probability thresholds which can help understand the uncertainties involved need to be investigated further although the inundation records contain information about occurrence locations and estimated inundation depths these data were not utilized analysed in this study further study on the correlation of spatial distribution of inundation and inundation depth with the spatially varying rainfall records will be valuable as well credit authorship contribution statement qian ke conceptualization methodology data curation writing original draft xin tian conceptualization methodology data curation writing original draft jeremy bricker supervision writing review editing zhan tian conceptualization methodology supervision writing review editing guanghua guan visualization investigation huayang cai visualization investigation xinxing huang visualization investigation honglong yang data curation junguo liu writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to thank the sponsors of this work the national key r d program of china grant no 2018yfe0206200 the national natural science foundation of china grant no 41671113 and 51761135024 the netherlands organisation for scientific research nwo grant no alwsd 2016 007 jpi urban europe era net co fund smart urban futures project no 646453 nwo verdus smart urban regions of the future surf and the engineering and physical sciences research council of uk grant no r034214 1 the high level special funding of the southern university of science and technology grant no g02296302 g02296402 we sincerely appreciate three reviewer s comments and suggestions which improved the quality of the article we also would like to show our gratitude to meteorological bureau of shenzhen municipality and water sector of shenzhen municipality for providing the rainfall data and inundation records in this paper supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103719 appendix b supplementary materials image application 1 appendices x1 performance metrics of 14 trained machine learning models based on 10 features no models tp fn fp tn tpr tnr ppr npr accuracy 1 coarse tree 13 11 7 382 0 54 0 98 0 65 0 97 0 956 2 medium tree 14 10 11 378 0 58 0 97 0 56 0 97 0 949 3 linear discriminant 14 10 7 382 0 58 0 98 0 67 0 97 0 959 4 quadratic discriminant 18 6 29 360 0 75 0 93 0 38 0 98 0 919 5 linear svm 11 13 2 387 0 46 0 99 0 85 0 97 0 964 6 quadratic svm 11 13 2 387 0 46 0 99 0 85 0 97 0 964 7 cubic svm 10 14 6 383 0 42 0 98 0 63 0 97 0 952 8 fine knn 11 13 9 380 0 46 0 98 0 55 0 97 0 946 9 medium knn 10 14 3 386 0 42 0 99 0 77 0 97 0 959 10 cos knn 7 17 4 385 0 29 0 99 0 64 0 96 0 950 11 cubic knn 10 14 3 386 0 42 0 99 0 77 0 97 0 959 12 bagged trees 12 12 6 383 0 50 0 98 0 67 0 97 0 956 13 subspace discriminant 14 10 5 384 0 58 0 99 0 74 0 97 0 964 14 subspace knn 14 10 10 379 0 58 0 97 0 58 0 97 0 951 x2 performance metrics of 14 trained machine learning models based on 2 features derived from a pca no models tp fn fp tn tpr tnr ppr npr accuracy 1 coarse tree 10 14 8 381 0 42 0 98 0 56 0 96 0 947 2 medium tree 10 14 16 373 0 42 0 96 0 38 0 96 0 927 3 linear discriminant 14 10 6 383 0 58 0 98 0 70 0 97 0 961 4 quadratic discriminant 14 10 10 379 0 58 0 97 0 58 0 97 0 952 5 linear svm 12 12 3 386 0 50 0 99 0 80 0 97 0 964 6 quadratic svm 9 15 4 385 0 38 0 99 0 69 0 96 0 954 7 cubic svm 7 17 3 386 0 29 0 99 0 70 0 96 0 952 8 fine knn 8 16 12 377 0 33 0 97 0 40 0 96 0 932 9 medium knn 7 17 1 388 0 29 1 00 0 88 0 96 0 956 10 cos knn 0 24 1 388 0 00 1 00 0 00 0 94 0 939 11 cubic knn 5 19 1 388 0 21 1 00 0 83 0 95 0 952 12 bagged trees 10 14 10 379 0 42 0 97 0 50 0 96 0 942 13 subspace discriminant 13 11 4 385 0 54 0 99 0 76 0 97 0 964 14 subspace knn 7 17 11 378 0 29 0 97 0 39 0 96 0 932 x3 performance metrics of 14 trained machine learning models based on a 30 min rainfall b 3 h rainfall c both 30 min and 3 h rainfall no models tp fn fp tn tpr tnr ppr npr accuracy a based on 30 min rainfall 13 11 9 380 0 54 0 98 0 59 0 97 0 952 b based on 3 h rainfall 15 9 7 382 0 63 0 98 0 68 0 98 0 961 c based on 30 min and 3 h rainfall 14 10 5 384 0 58 0 99 0 74 0 97 0 964 
396,flood is one of the most catastrophic natural disasters in the united states particularly in the southeast states where hurricanes and tropical storms are most prevalent causing billions of dollars in damage annually and significant losses of life and property the weather research and forecasting hydrological model wrf hydro is a community based hydrologic model designed to improve the skill of hydrometeorological forecasts such as river discharge through simulating hydrologic prognostic e g soil moisture and diagnostic e g energy fluxes variables these quantities are potentially biased or erroneous due to the uncertainties involved in all layers of hydrologic predictions in this study we use an ensemble based data assimilation da approach to explore the benefit of independently and jointly assimilating remotely sensed smap soil moisture active passive soil moisture at different spatial resolutions and usgs streamflow observations to improve the accuracy and reliability of wrf hydro model predictions while accounting for uncertainties this study is conducted over a large region near to houston texas where heavy rainfall from hurricane harvey caused flooding in 2017 before implementing da we first calibrated the wrf hydro model parameters using four united states geological survey usgs stream gauges installed within the watershed in this step we identified the most dominant model parameters which were used later in the development of joint state parameter da the findings of this study showed that the multivariate assimilation of soil moisture and streamflow observations results in improved prediction of streamflow as compared to univariate assimilation configurations and regardless of the watershed s streamflow regime the results also revealed that during the normal streamflow condition assimilation of downscaled smap soil moisture at 1 km spatial resolution would improve the accuracy of streamflow simulation more than the assimilation of coarse resolution products i e the native smap at 36 km spatial resolution and its interpolated version at 9 km spatial resolution however during the period of hurricane harvey the soil moisture observations at different resolutions showed a similar impact on improving the streamflow prediction keywords flood prediction wrf hydro data assimilation smap soil moisture 1 introduction tropical storms and hurricanes in the southeast united states have become more frequent and intense over the past decades mainly due to the effects of climate change and climate variability chen et al 2015 donnelly and woodruff 2007 foltz et al 2018 kossin et al 2013 michener et al 1997 they often produce torrential rains that may result in deadly and destructive floods depending on orographic liu and smith 2016 smith et al 2009 geomorphologic kourgialas and karatzas 2011 naylor et al 2017 and hydrologic eisenbies et al 2007 kundzewicz et al 2014 characteristics of the region according to the annual natural hazard report severe rainfall and hurricane induced flooding were among the leading calamities that caused billions of dollars in damage and dozens of fatalities throughout the nation in 2018 in addition urbanization and anthropogenic activities are expected to further increase flood vulnerability and risk in the coming decades doocy et al 2013 huong and pathirana 2013 flood forecasting systems are widely used to mitigate the impacts of such hydrometeorological extremes by providing decision makers and the public with early warning information the most central part of a flood forecasting system is the hydrological model one of the latest advances of hydrological models is the weather research and forecasting model hydrological modeling system wrf hydro an open source community model developed by the national center for atmospheric research ncar for a range of studies including flash flood prediction regional hydroclimate impacts assessment and water resources management gochis and chen 2003 gochis et al 2015 hydrological models irrespective of their types most often do not provide accurate and reliable estimates of prognostic variables e g soil moisture and streamflow as they are subject to large uncertainties stemming from different sources including hydrometeorological forcing inputs model parameters boundary or initial condition and model structure abbaszadeh et al 2019a bi et al 2015 leach et al 2018 matgen et al 2010 these uncertainties can be accounted for when the hydrologic predictions are produced within a probabilistic framework kuczera and parent 1998 marshall et al 2004 smith and marshall 2008 typically this is performed through bayesian inference over the last couple of decades data assimilation da has garnered increasing attention among researchers and practitioners as an effective and reliable method based on bayesian theory to incorporate the hydrometeorological observations from in situ and remotely sensed measurements into hydrological models for enhancing their forecasting skills while taking into account the associated uncertainties reichle et al 2002 seo et al 2014 particle filter pf is a class of bayesian data assimilation approach that has been widely used in hydrologic studies dechant and moradkhani 2012 dong et al 2015 montzka et al 2013 evolutionary particle filter and markov chain monte carlo epfm recently developed by abbaszadeh et al 2018a is a successor version of pf mcmc moradkhani et al 2012 to improve both the state and parameter estimation of a high dimensional system in this study we will use this data assimilation approach for more information about data assimilation techniques in hydrometeorological studies we refer the interested readers to moradkhani et al 2018 soil moisture is a key hydrological variable that plays an imperative role in the terrestrial water cycle through controlling the water and energy fluxes in the land atmosphere interface many studies have assimilated soil moisture data collected from ground based stations or retrieved from satellites into hydrological models to improve their predictive skills these include studies ranging from drought monitoring bolten et al 2010 and flood forecasting massari et al 2018 wanders et al 2014 to irrigation and water resources management el sharif et al 2015 felfelani et al 2018 moreover significant efforts have been made over the past few years to assimilate downscaled and disaggregated soil moisture products into land surface models although their impact on hydrological model predictions has not yet been entirely explored lópez lópez et al 2017 malbéteau et al 2018 sahoo et al 2013 on the other hand streamflow is the main component of the hydrologic cycle generated through a combination of land surface and sub surfaces processes that incorporate the information of hydrologic states and fluxes streamflow observations measured by in situ sensors most often are assimilated into hydrological models to update the model states and or parameters and hence to improve the estimation of predictive uncertainty abbaszadeh et al 2018a clark et al 2008 while these prognostic variables soil moisture and streamflow are often independently assimilated into the hydrological models many other studies have shown that their joint assimilation is a more effective approach in improving model predictions and quantifying associated uncertainties aubert et al 2003 lee et al 2011 lópez lópez et al 2017 however such experiments typically are based on lumped or semi distributed hydrological models due to their simplicity and low computational requirements with recent advances in satellite remote sensing technologies soil moisture data have become available at a global scale with decent spatial and temporal resolutions while these products are promoting a wide range of global and continental land atmosphere studies their spatial resolutions impede their use in regional and local studies that require data with a resolution of 1 km or less abbaszadeh et al 2019 entekhabi et al 2010 peng et al 2017 particle filtering data assimilation in conjunction with fully distributed hydrologic models is still at infancy mostly due to its complexity and sophistication to the best of our knowledge this subject has not been explored in depth in the literature therefore the present study aims to examine the benefit of independently and jointly assimilating satellite soil moisture data at different spatial resolutions and streamflow observations into a fully distributed wrf hydro model we also aimed to understand the extent to which these observations can contribute to improving the model forecasts particularly during extreme events therefore we conducted our study over a region in southeast texas where heavy rainfall from hurricane harvey caused flooding in 2017 model calibration and da play complementary roles in improving hydrological model simulations model calibration addresses the deficiencies in the model s representation of physical processes through tuning the model parameters and finding the best set which maximizes the agreement between the model simulation and observation koster et al 2018 however da addresses such deficiencies by rectifying the model state variables depending on the availability of its corresponding observation data at a certain time and location previous studies mostly relied on parsimonious hydrological models and results showed that using both approaches could lead to acceptable accuracy of model estimates koster et al 2018 vrugt et al 2005 xu et al 2014 yucel et al 2015 in this study we will examine this conjecture over a very large basin and using a high resolution fully distributed hydrological model and advanced particle filter based data assimilation approach the remainder of the paper is organized as follows section 2 describes the study area and the datasets including the satellite soil moisture products united states geological survey usgs observations and north american land data assimilation system nldas data section 3 briefly explains the wrf hydro hydrologic model and section 4 outlines the proposed research framework section 5 provides a thorough explanation of the wrf hydro model setup and summarizes the model calibration and simulation results section 6 discusses different da scenarios including independent or joint assimilation of satellite soil moisture and streamflow observation into the wrf hydro model and their contributions to improving the model simulations section 7 summarizes the findings of the paper 2 study area and datasets following hurricane rita in 2005 and ike in 2008 hurricane harvey with more than 50 inches of total rainfall in 2017 was the strongest category 4 tropical cyclone to strike the united states in more than a decade resulting in catastrophic flooding dozens of fatalities and more than 200 million in crop losses hurricane harvey with 125 billion in damage was one of the costliest natural disasters in the history of the united states comparable to hurricane katrina in 2005 in this paper we conduct our study over a region with an area of 995 16 square miles located in southeast texas eastern part of san jacinto basin this area is one of the fastest growing population regions in the country this population growth has led to increased urbanization within the basin this along with the proximity to the gulf of mexico and associated tropical storms and hurricanes has made this watershed more vulnerable to flooding from both intensified precipitation due to climate change and increased runoff because of rapid urbanization zhu et al 2015 fig 1 demonstrates a detailed map of the study area such as the watershed boundary topography lakes stream networks major rivers and usgs streamflow gauges san jacinto is the main river in the region where flooding from hurricane harvey damaged the protective barrier at the san jacinto river waste pits site and polluted the river resulting in a 115 million cleanup program for more information please see https www epa gov newsreleases san jacinto waste pits superfund site cleanup plan approved san jacinto river flows across montgomery county to the south and forms lake houston san jacinto basin has a warm and humid climate the annual average rainfall in this region is about 51 inches while the snow is insignificant throughout the year the topography of the san jacinto basin is slightly hilly except along the san jacinto river where there are extensive areas of flood plains https webapps usgs gov harvey 2 1 smap soil moisture products the soil moisture active passive smap satellite equipped with two instruments a radar active and a radiometer passive was launched on january 2015 to provide high resolution soil moisture and detect frozen and thawed soils on a global scale the primary goal of this national aeronautics and space administration nasa satellite mission is to understand the links between earth s water energy and carbon cycles reduce the uncertainties in predicting the weather and climate and improve monitoring and predicting of natural disasters such as floods and droughts the smap satellite was originally designed to measure topsoil layer moisture 0 5 cm with a 9 km spatial resolution by combining l band brightness temperatures at 36 km resolution and 3 km high resolution l band radar backscatter data entekhabi et al 2010 unfortunately on july 7 2015 smap s radar stopped transmitting due to the irrecoverable hardware failure of the radar instrument and since then smap s radiometer became the only operational instrument providing soil moisture at 36 km spatial resolution chan et al 2016 although this product has a decent spatial resolution for continental or global studies it cannot be directly used for local or regional studies that require a finer resolution data entekhabi et al 2010 to circumvent this problem in january 2017 nasa announced a product named the enhanced smap radiometer in this dataset the standard smap data gridded at 36 km were interpolated into the global cylindrical ease grid 2 0 projection with 9 km spacing using the backus gilbert optimal interpolation algorithm one year later in 2018 nasa proclaimed a new soil moisture product with 3 km spatial resolution in this dataset smap l band brightness temperatures and copernicus sentinel 1 c band backscatter coefficients were used to generate soil moisture data which is then resampled to an earth fixed cylindrical 3 km equal area scalable earth grid version 2 0 das et al 2018 it should be noted that these soil moisture products are available on both ascending and descending orbit despite such advances some land surface applications such as water management agricultural production drought monitoring and flood forecasting still require soil moisture at finer resolutions from a kilometer to a sub kilometer scale for this purpose a few studies have rescaled the smap soil moisture data to a finer resolution abbaszadeh et al 2019b alemohammad et al 2018 zhao et al 2018 in this paper we evaluate the impact of assimilating smap soil moisture observations at different spatial resolutions into the wrf hydro model the descriptions of all smap soil moisture datasets used in this study are summarized in fig 2 this figure used as an example for conceptualization illustrates smap soil moisture data at three different spatial resolutions across the state of texas on 26 august 2017 when the hurricane harvey hit the southeast region of this state 2 2 nldas 2 forcing data the north american land data assimilation system nldas 2 provides quality controlled and spatiotemporally consistent datasets from best available observations to support modeling activities in this paper nldas 2 datasets including incoming shortwave radiation incoming longwave radiation specific humidity air temperature surface pressure near surface wind in the u and v components and precipitation rate are used to force wrf hydro model nldas 2 has a 12 5 km spatial resolution hourly temporal resolution and the data ranges from january 1979 to present nldas 2 forcing data is re gridded to match the geogrid domain of the wrf hydro model 2 3 usgs streamflow gauges in this study we used four usgs gauges installed within the basin fig 1 to calibrate the wrf hydro model assess the streamflow simulation results and perform data assimilation the usgs hydrograph not shown here shows that the streamflow peak caused by hurricane harvey in the river was well captured by all four gauges the study area is hence well situated for a study aimed at understanding the efficacy of the wrf hydro model in simulating floods triggered by heavy rainfall from hurricanes such as harvey usgs station 08070500 caney ck nr splendora is located in the caney creek river in caney creek watershed this perennial river is one of the tributaries of the san jacinto river that flows 155 miles southeast and drains into the gulf of mexico usgs station 08071000 peach ck at splendora is installed in the peach creek river in peach creek watershed this river is 27 miles long and is one of the tributaries of the san bernard river caney creek and peach creek are both located about 40 miles north of houston texas and the landuse of both the watersheds is dominated by pasture and forest the average annual rainfall in this area is 46 inches with uneven distribution of rainfall throughout the year usgs station 08070200 e fk san jacinto rv nr new caney is located in san jacinto river in san jacinto watershed this river flows southeast for about 28 miles and eventually drains into the galveston bay usgs station 08071280 luce bayou abv lk houston nr huffman is located in the luce bayou river in luce bayou watershed this river with 35 miles length is a single primary stream of this watershed that drains into the east fork san jacinto river the prevailing climate in this watershed is humid subtropical and cropland rangeland and pasture are the dominant land uses 3 wrf hydro hydrological model wrf hydro model was initially designed to facilitate coupling between the weather research and forecasting wrf and different elements of terrestrial hydrological systems while accounting for the discrepancy between the resolutions of atmospheric and hydrological models gochis and chen 2003 multiple land surface models lsms have been configured with the wrf hydro to simulate land surface processes in this study we compiled wrf hydro in the uncoupled mode offline with noah land surface model with multi parameterization options noah mp in offline mode there is no interaction between the lsm and wrf atmospheric model wrf hydro model is a fully distributed system that integrates different hydrological and hydraulic modules to simulate the surface overland flow subsurface saturated flow channel routing and baseflow processes the wrf hydro model is fully parallelized and runs on cluster and high performance computing hpc systems wrf hydro consists of different water routing modules that enable diffusive wave surface routing and saturated subsurface flow routing on a 250 m grid and muskingum cunge channel routing down national hydrography dataset plus version 2 nhdplusv2 stream reaches noah mp is available as a column lsm option in wrf hydro that accounts for soil infiltration and redistribution processes in addition to this wrf hydro with its routing module uses high resolution topography data to further help redistribute terrestrial moisture one of the significant enhancements of the wrf hydro model is its ability to route infiltration capacity excess and saturated subsurface water to simulate the surface runoff the routed infiltration capacity excess in the 1 d soil column along with additional exfiltration from saturated soil is allowed to remain on the land surface as ponded water which is available for lateral redistribution if suitable conditions are met ryu et al 2017 senatore et al 2015 yucel and onen 2014 although this model has recently been successfully applied in several regions throughout the world arnault et al 2016 lahmers et al 2019 lin et al 2018 silver et al 2017 wehbe et al 2018 the integration of state of the art data assimilation approaches such as the particle filtering with this model has not yet been implemented 4 the proposed research framework the data assimilation method in this study utilizes sequential monte carlo techniques to generate the replicates of model forcing and states and through a formal bayesian approach obtains a full probability distribution of the variables of interests and characterizes the predictive uncertainty the sequential assimilation techniques have been widely used in hydrologic prediction studies and provide effective means to assimilate ground based and remotely sensed observations into an lsm in this study we use the evolutionary particle filter with markov chain monte carlo hereafter epfm to improve the wrf hydro model predictions and quantify the associated uncertainties we refer the readers to our previous study abbaszadeh et al 2018a where we presented a detailed description about epfm approach its implementation and benefits in this study we used the noah mp land surface model which contains a set of canopy and surface energy fluxes to represent the exchanges of energy and water between the multi layer land surface and atmosphere when this land surface model is combined with the high resolution routing module of wrf hydro 250 m grid it enables producing soil moisture information to the noah mp 1 km grid fig 3 in this study we considered 250 meter grid resolution for flow routing and 1 km grid resolution for noah mp model run nldas meteorological forcing data are re gridded to noah mp 1 km grid resolution that are required by the wrf hydro modeling system uncertainties that arise from this disaggregation process are accounted for in the data assimilation framework in this research the model is first set up over a geogrid domain covering the study region the dynamically dimensioned search dds algorithm tolson and shoemaker 2007 is then used to calibrate the wrf hydro model parameters and finally the calibrated model is used to simulate streamflow at four usgs gauges installed within the basin we will provide a complete description of the wrf hydro model setup calibration and simulation in section 7 in this study we assimilate smap soil moisture observations ascending and descending overpasses at different resolutions 36 km native footprint and 9 km backus gilbert interpolated along with the recently downscaled smap product at 1 km abbaszadeh et al 2019b into the wrf hydro model with the aim to improve the accuracy and reliability of wrf hydro model predictions in this study we bias corrected the downscaled soil moisture data using the cumulative distribution function cdf matching approach reichle and koster 2004 in all four da scenarios designed we assimilate the soil moisture observation independently as well as in conjunction with the streamflow data to explore the best da configuration leading to the most improved model estimates in this research the wrf hydro model is equipped with an enhanced ensemble da approach epfm abbaszadeh et al 2018a which is appropriate da involving high dimension of state variables and parameters 5 wrf hydro modeling this section provides a progressive outline for the wrf hydro model setup and discusses in detail the model calibration process and assessment of the model the first step to setup the wrf hydro model is to define the geogrid domain file using the wrf preprocessing system wps the next step is to prepare the initial condition file using the geogrid file created earlier a high resolution digital elevation model dem and other optional inputs i e shapefiles of forecast points and lakes the wrf hydro gis pre processing tool is used to create the geospatial and tabular data layers that define the terrestrial overland flow subsurface flow and channel routing processes necessary for wrf hydro modeling in this study we used the nhdplusv2 to extract the dem for the study area wrf hydro model has three configurations national water model nwm gridded and reach in this study we selected gridded configuration that uses diffusive wave routing the earth system modeling framework esmf script is used to re grid the nldas meteorological forcing data to match the wrf hydro geogrid domain grid in order to run the wrf hydro executable wrf hydro exe two namelist files namelist hrldas noah mp land surface model namelist and hydro namelist should be carefully configured 5 1 wrf hydro model calibration and simulation results in this section we first describe how the wrf hydro model parameters are calibrated the model parameters are categorized into six groups including soil parameters runoff parameters groundwater parameters vegetation parameters snow parameter and channel parameters table 1 lists a subset of wrf hydro model parameters defined by ncar along with their minimum and maximum values these numbers are suggested in the automated ncar calibration tool fig 4 shows the procedure used to calibrate the wrf hydro model phase i and implement the data assimilation approach phase ii we split the entire period into three sub periods first nine months from 01 01 2016 to 09 30 2016 is used as a spin up warm up period to initialize the system and then the water year 2016 2017 from 10 01 2016 to 09 30 2017 is used to calibrate the model parameters the last three months of the year 2017 is also considered to validate the calibrated model in the spin up period we first employed a manual calibration to identify the most relevant model parameters thereafter we excluded some of the parameters including channel side slope chsslp initial channel depth hlink melt factor for snow depletion curve mfsno and the exponent in the resistance equation for soil evaporation rsurfexp from the parameter set due to their insignificant contribution to improving the model calibration performance then we ran the model using cold start initial condition in the spin up period to obtain warm state variables of the noah mp model and terrain routing module of the wrf hydro system see fig 3 these state variables are then used as initial conditions in the main model calibration process this process is consistent with the practices of the ncar wrf hydro development team senatore et al 2015 it is also important to note that wrf hydro model calibration is performed by optimizing hourly streamflow using dynamically dimension search dds algorithm tolson and shoemaker 2007 this optimization algorithm is able to converge to a near optimal solution with approximately 100 500 iterations lespinas et al 2018 which is much faster than the widely used shuffled complex evolution sce algorithm that requires 10 000 iterations to converge to a solution with the same accuracy duan et al 1992 therefore dds is better suited for calibration of the computationally intensive fully distributed physically based hydrological models like wrf hydro lahmers et al 2019 in this study we used all four usgs stations operated within the watershed to calibrate the wrf hydro model parameters fig 5 shows the model calibration process using 140 iterations for the water year 2016 2017 further investigations also revealed that increasing the number of dds iterations beyond 140 does not improve the calibration effectiveness and only adds to the computational burden we used the kling gupta efficiency kge gupta et al 2009 abbaszadeh et al 2018b as an objective function metric for all calibrations performed in this study kge along with other performance measures used in this study are described in appendix a of the paper as seen in fig 5 almost all the model simulations captured the high flows caused by heavy rainfall from hurricane harvey from august 17 2017 to september 2 2017 this is because the streamflow is much more controlled by the torrential rain during an extreme event rather than the parameters variabilities however model parameters tend to be more determinant than the forcing data for low to medium flows for the model calibration the noah mp time step was set to one hour which is the standard in the operational nwm after the model calibration we performed a simple one at a time parameter sensitivity analysis on the model parameters and the results revealed that channel parameters i e channel bottom width and manning s roughness and groundwater parameters i e maximum groundwater bucket depth and bucket model exponent along with surface runoff parameter refkdt had the greatest impact on the wrf hydro model performance refkdt is a runoff parameter that significantly impacts surface infiltration and hence the partitioning of total runoff into surface and subsurface runoff zmax is the maximum storage of the bucket to represent and conceptualize the groundwater process wrf hydro uses an exponential function which includes expon parameter to estimate the bucket discharge as a function of water depth in the bucket bw and mannn are the channel bottom width and manning s roughness coefficient that collectively shape the overland flow process it is important to note that these parameters have no real physical meaning and are only used for model conceptualization our sensitivity analysis also indicated that in our case study the performance of the wrf hydro model is more sensitive to the runoff parameters compared to the vegetation and soil parameters therefore in the next section the most relevant model parameters i e refkdt zmax expon bw and mannn will be used in the development of the joint state parameter data assimilation however the rest of the parameters will be used at their calibrated values to assess the effectiveness of the model we computed three deterministic performance measures including kge root mean square error rmse and pearson correlation coefficient pcc table 2 summarizes the model performance at four usgs stations for both calibrated blue line in fig 5 and un calibrated red line in fig 5 model scenarios as illustrated in fig 5 in all cases the model exhibits either positive or negative bias in the simulations indicating overestimation and underestimation of the streamflows this is most likely due to the model structure and parameterization errors in model calibration which is expected to be accounted for later in the joint state parameter data assimilation framework it is also important to note that although the wrf hydro model estimated the peak streamflow values well specifically those measured by usgs stations during hurricane harvey from august 17 2017 to september 2 2017 it did not provide accurate estimates during recession periods and low flow conditions zmax and expon are the two key groundwater parameters that affect such flow regimes by controlling the baseflow and the groundwater storage refkdt is also an important parameter that influences the amount of water that flows into the channel which in turn affects the runoff process the simultaneous adjustment of these model parameters together with model state variables through the dual state parameter data assimilation approach introduced in section 5 may improve the effectiveness and usefulness of the wrf hydro model in estimating all possible streamflow regimes this conjecture will be discussed in detail in the next section 6 assimilation of in situ and remotely sensed observations in sections 6 1 and 6 2 we demonstrate independent assimilation of usgs streamflow and smap soil moisture observations at different spatial resolutions into the wrf hydro model following these in section 6 3 we conduct the multivariate assimilation of these two observations to show its usefulness in comparison to independent assimilation configuration in this study all the assimilation runs are performed at the daily time scale which is identical to the temporal scale of the meteorological forcing satellite observations smap soil moisture at 36 km 9 km and 1 km and the daily aggregated streamflow time series multivariate joint data assimilation refers to the simultaneous assimilation of multiple observation data for different model state variables into a hydrological model it is noted that all the assimilation experiments are based on joint state parameter estimation as discussed earlier we only use the most sensitive model parameters i e refkdt zmax expon bw and mannn in the joint sequential data assimilation framework however for other model parameters the calibrated values are directly used 6 1 assimilation of streamflow observations into the wrf hydro model here we briefly explain the data assimilation settings given the multiplicative nature of error in precipitation we assumed a lognormal error distribution with mean zero and relative error of 25 to perturb the precipitation this is in fact a heteroscedastic error meaning that the error is proportional to the rain intensity for temperature the error is assumed to be homoscedastic and normally distributed with mean zero and standard deviation of 5 c uncertainty in other forcing data including incoming shortwave radiation incoming longwave radiation specific humidity surface pressure near surface wind in the u and v components were assumed insignificant to characterize uncertainty in the initial condition we assumed a normal error distribution with mean zero and a standard deviation of 0 04 m3 m3 for the soil moisture at the topsoil layer note that wrf hydro model has two restart files that represent the initial condition for the land surface model and routing module respectively in this study we only added white noise to the soil moisture at the top 10 cm soil layer in the land surface model it is also assumed that the streamflow observation errors follow a heteroscedastic normal distribution with a relative error of 15 percent in real case experiments the other major part of the errors arises from the model structural uncertainty which herein is represented by adding white noise with a relative error of 25 percent to the model outputs in this study all the errors were assumed uncorrelated and used with the same magnitude in all assimilation runs these error values are chosen following our previous studies abbaszadeh et al 2018b dechant and moradkhani 2012 and also trial and error to provide a comprehensive assessment of the effectiveness and usefulness of the epfm data assimilation approach we computed multiple performance measures i e kge and rmse fig 6 illustrates the performance of the wrf hydro model with and without da for the period of hurricane harvey august 17 2017 to september 02 2017 the results show that both da and open loop ol model runs had similar performance in identifying the onset of flooding however da showed better results for the recession period as compared to the ol when comparing the results of the ol and da runs it is concluded that assimilating streamflow into the wrf hydro model is quite useful in improving its performance although this improvement at gauge 8070200 remains limited this is may be due to sparse point observations that are assimilated into the gridded model such that the spatial discrepancy between the model outputs and observations results in suboptimal model performance this deficiency is expected to be alleviated by assimilating satellite soil moisture data into the wrf hydro model this hypothesis will be examined in the next section it is also important to mention that in this study all the assimilation runs are based on ensemble size of 90 the findings of this study also revealed that in ensemble data assimilation with wrf hydro model based on particle filtering adjusting the model state variables in conjunction with model parameters provides more accurate and reliable posterior distributions compared to state estimation only figure s1 in supplementary file illustrates the prior streamflow versus observation which is critical in interpreting the usefulness of the assimilation in providing the best possible set of model states at forecast time figures s2 and s3 respectively show the temporal evolution of model parameters over the study period and the distribution of particle weights at several time steps these are important information as they indicate the stable behavior of model parameter after nearly 1 year of assimilation also the particle weights distribution over time shows a tendency toward normal distribution and stability of the weights that with stable parameters result in improved model predictions 6 2 assimilation of smap soil moisture products into the wrf hydro model in this section we examine the influence of assimilating smap soil moisture observations at different spatial scales on the wrf hydro model performance fig 7 is a taylor diagram that simultaneously illustrates the deterministic i e time series correlation mean bias and normalized root mean square difference rmsd and probabilistic i e normalized root mean square error ratio nrr performance measures calculated at four usgs stations based on four assimilation configurations including assimilation of all streamflow observations within the watershed and assimilation of smap soil moisture at three different spatial resolutions the results reported in fig 9 are calculated for the year 2017 figures s4 s5 and s6 in the supplementary file show the contribution of assimilating smap soil moisture at 36 km 9 km and 1 km spatial resolutions to enhance the streamflow prediction during the hurricane harvey nrr dechant and moradkhani 2012 is calculated to measure the ensemble spread and assess how confidently the ensemble mean is statistically distinguishable from the ensemble spread these performance measures are fully described in appendix a the results revealed that the assimilation of smap soil moisture at 1 km spatial resolution contributes more to the improvement of wrf hydro model prediction than the coarser scale smap products 36 and 9 km although the assimilation of smap soil moisture at 1 km scale to some extent could improve the streamflow simulation accuracy see figure s4 in supplementary file this improvement was not as good as that achieved by assimilating streamflow observation into the model it can therefore be concluded that assimilating streamflow data at the usgs locations results in better representation of posterior distribution compared to assimilating soil moisture data at even higher resolution it is important to note that the standard deviation ratios are always around the arc of one unit that indicates similar variability bias in the variance between the simulated streamflows and usgs observations regardless of the type of assimilation configuration further analysis revealed that there is a complementary relationship between the nrr probabilistic measure and correlation coefficient normalized rmsd deterministic measures almost in all assimilation runs such that when the correlation coefficient normalized rmsd improves the nrr gets closer to 1 which is an ideal value of nrr and when the deterministic measures deteriorate resulting in a suboptimal value for nrr based on the aforementioned discussion we will use the streamflow observations at all four usgs gauges and smap soil moisture at 1 km spatial resolution in the implementation of multivariate data assimilation this will be discussed in the next section comparing the univariate assimilation with ol simulations at four usgs stations over the entire period of study confirmed that although the assimilation of streamflow and soil moisture at 1 km spatial resolution could significantly outperform the ol predictions assimilation of soil moisture at coarser spatial resolutions i e 9 km or 36 km had marginal performance compared to ol simulation the results also indicated that although assimilating the smap soil moisture into wrf hydro model would partially improve the streamflow estimates assimilation of streamflow has relatively small impact on the improvement of soil moisture estimates this may be due to the fact that streamflow observations are assimilated into the model only at certain points therefore while improving the streamflow simulation soil moisture estimates that are spatially variable are not fully corrected in streamflow assimilation the likelihood values and consequently the particle weights are calculated based on the simulated and observed streamflow at those points where the usgs stations are located this information although helps improve the streamflow prediction it is not that beneficial for updating the soil moisture at each grid cell across the entire domain in fact the observed streamflow is the result of hydrologic processes over a longer period of time across the watershed as a result it is often difficult to merely assimilate the outlet discharge at a time step and expect the best trajectories of watershed state gichamo and tarboton 2019 this problem is expected to be addressed when the streamflow and soil moisture observations are jointly assimilated into the model fig 8 demonstrates the spatial pattern of the updated soil moisture posterior mean versus the smap soil moisture data at 1 km spatial resolution the upper panel shows the average smap and downscaled soil moisture for the period of hurricane harvey from august 17 2017 to september 2 2017 across the study area it is clear that on average the downscaled soil moisture at 1 km spatial resolution is dryer than the original smap soil moisture at 36 km grid cell the lower panel shows an acceptable pearson s correlation coefficient between the updated soil moisture and smap at 1 km spatial resolution across the entire domain except for the northeast region where the lake livingston is located this is because the footprint of original smap soil moisture at 36 km over this region is heavily affected by the lake that consequently leads to the overestimation of soil moisture this wet bias in observational data when assimilated into the wrf hydro model results in inaccurate model predictions both the pcc and rmse confirm that the assimilation performance is degraded across the regions where 1 km grid cells are dominated by water bodies on average the rmse between the updated and smap soil moisture data at 1 km spatial resolution is below 0 04 m3 m3 that corroborates with the accuracy requirement of smap soil moisture retrieval it is worthy to mention that unlike the normal streamflow condition where assimilation of smap soil moisture at 1 km spatial resolution better than other coarse scale soil moisture observations would contribute to improving the streamflow prediction all the soil moisture data irrespective of their spatial resolutions would similarly improve the streamflow simulation during the hurricane harvey see figures s4 s5 and s6 in the supplementary file 6 3 multivariate assimilation of smap soil moisture and streamflow observations in this section we will jointly assimilate the streamflow observations and 1 km smap soil moisture into the wrf hydro model to further improve the estimation of the topsoil layer moisture content and quality of ensemble streamflow prediction we examined the results for the usgs gauges within the watershed and noticed that all the deterministic measures indicate the superior performance while conducting the joint assimilation as compared to a univariate assimilation configuration fig 9 illustrates the benefit of multivariate assimilation of satellite soil moisture and streamflow observation in improving the wrf hydro streamflow simulation during the period of hurricane harvey it is also important to note that the updated soil moisture at 1 km spatial resolution was in good agreement with the smap soil moisture observation at 1 km spatial resolution almost for the entire domain except the region where the lake livingston is located this is similar to the results of the univariate assimilation of soil moisture data and streamflow observation discussed in fig 8 the low performance of the model over this region is potentially due to the original smap soil moisture observation at 36 km spatial resolution that involves high retrieval error over this area comparing both univariate and multivariate assimilation configurations also revealed that in all scenarios the data assimilation results in significant improvement of medium to high streamflow predictions this may be attributed to the inherent characteristic of the wrf hydro model whose performance during the wet periods is higher than the dry periods table 3 summarizes the results of all data assimilation experiments conducted in this study these results are obtained for the period of hurricane harvey indicating the significance of each assimilation scenario in improving the streamflow prediction during an extreme event the univariate and multivariate assimilation results as shown in table 3 always outperform the ol simulation during the period of hurricane harvey high flow condition however as shown earlier in fig 7 the results of the open loop model simulations under low to medium flow conditions were similar to those obtained by univariate assimilation of soil moisture at 9 km and 36 km spatial resolutions the findings also show that the multivariate data assimilation results in better streamflow simulation regardless of the watershed s streamflow regime it should be noted that during the hurricane harvey assimilation of soil moisture at different spatial resolutions had similar impact on improving the streamflow simulation however during the normal flow condition the finer the spatial resolution of soil moisture the higher the accuracy of streamflow prediction note that the computational cost depends on the high performance computing hpc configuration and parallelization of the ensemble model simulations in this study we used a linux cluster node including 2 cpus 28 cores per cpu commensurate with the computational requirements of the parallel epfm data assimilation framework that works based on an ensemble size of 90 the runtime of the ensemble data assimilation for each day was approximately 6 minutes it is also important to note that all the assimilation experiments irrespective of their types i e univariate or multivariate had almost similar computational time this is due to the parallelization of hydrological data assimilation system and its operation on the hpc cluster which meets the computational demand of the ensemble simulations also in this data assimilation experiment we noticed that almost 97 percent of the runtime at each time step is associated with running the wrf hydro model itself 7 summary and conclusion over the last decade tropical storms and hurricanes have become more destructive and frequent in the southeast us mainly due to climate change and climate variability lim et al 2018 they most often are accompanied by violent winds and torrential rains which can lead to catastrophic flooding severe rainfall and hurricane induced flooding annually cause billions of dollars in damages property losses and a significant number of fatalities throughout the nation although hydrological models are widely used as an efficient means to estimate such floods their predicted values most often are not accurate and reliable as the model prognostic variables such as soil moisture and streamflow are subject to large uncertainties stemming from hydrometeorological forcing model parameters boundary or initial condition and model structure data assimilation has been recognized as an effective method that takes these uncertainties into account and improves hydrologic prediction with the recent advances in the satellite remote sensing technologies and ground based observation observational data is becoming increasingly available leading to the widespread use of hydrologic data assimilation in various applications despite such developments assimilation of remotely sensed soil moisture data at fine resolutions and streamflow observations into hyper resolution hydrologic models e g wrf hydro has been rarely done therefore as a prototype study we implemented hydrologic data assimilation using an evolutionary particle filter over a region in southeast texas where heavy rainfall from hurricane harvey caused fatal flooding the findings of this study showed that during the normal streamflow condition univariate assimilation of smap soil moisture at 1 km improves the streamflow simulation results more than the assimilation of coarse resolution products including the smap native product of 36 km spatial resolution smap l3 sm p and its interpolated version at 9 km spatial resolution smap l3 sm p e however during the period of hurricane harvey the soil moisture observations at different resolutions have similar impact on improving the streamflow prediction the results also indicated that multivariate assimilation of soil moisture and streamflow observations results in more desirable posterior distribution than any independent assimilation configuration we investigated the model performance during the hurricane harvey and post harvey periods and realized that although da whether univariate or multivariate and ol model runs have shown similar results in characterizing the onset of flooding the da performed better in predicting the termination of flooding streamflow recession period author statement credit authorship contribution statement peyman abbaszadeh conceptualization methodology software writing original draft data curation keyhan gavahi software methodology visualization hamid moradkhani supervision conceptualization methodology writing review editing declaration of competing interest the authors declare no competing interests supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103721 appendix b supplementary materials image application 1 appendix a summary of performance measures used in this study is tabulated in table a1 these performances measures are fully described in abbaszadeh et al 2019a dechant and moradkhani 2012 
396,flood is one of the most catastrophic natural disasters in the united states particularly in the southeast states where hurricanes and tropical storms are most prevalent causing billions of dollars in damage annually and significant losses of life and property the weather research and forecasting hydrological model wrf hydro is a community based hydrologic model designed to improve the skill of hydrometeorological forecasts such as river discharge through simulating hydrologic prognostic e g soil moisture and diagnostic e g energy fluxes variables these quantities are potentially biased or erroneous due to the uncertainties involved in all layers of hydrologic predictions in this study we use an ensemble based data assimilation da approach to explore the benefit of independently and jointly assimilating remotely sensed smap soil moisture active passive soil moisture at different spatial resolutions and usgs streamflow observations to improve the accuracy and reliability of wrf hydro model predictions while accounting for uncertainties this study is conducted over a large region near to houston texas where heavy rainfall from hurricane harvey caused flooding in 2017 before implementing da we first calibrated the wrf hydro model parameters using four united states geological survey usgs stream gauges installed within the watershed in this step we identified the most dominant model parameters which were used later in the development of joint state parameter da the findings of this study showed that the multivariate assimilation of soil moisture and streamflow observations results in improved prediction of streamflow as compared to univariate assimilation configurations and regardless of the watershed s streamflow regime the results also revealed that during the normal streamflow condition assimilation of downscaled smap soil moisture at 1 km spatial resolution would improve the accuracy of streamflow simulation more than the assimilation of coarse resolution products i e the native smap at 36 km spatial resolution and its interpolated version at 9 km spatial resolution however during the period of hurricane harvey the soil moisture observations at different resolutions showed a similar impact on improving the streamflow prediction keywords flood prediction wrf hydro data assimilation smap soil moisture 1 introduction tropical storms and hurricanes in the southeast united states have become more frequent and intense over the past decades mainly due to the effects of climate change and climate variability chen et al 2015 donnelly and woodruff 2007 foltz et al 2018 kossin et al 2013 michener et al 1997 they often produce torrential rains that may result in deadly and destructive floods depending on orographic liu and smith 2016 smith et al 2009 geomorphologic kourgialas and karatzas 2011 naylor et al 2017 and hydrologic eisenbies et al 2007 kundzewicz et al 2014 characteristics of the region according to the annual natural hazard report severe rainfall and hurricane induced flooding were among the leading calamities that caused billions of dollars in damage and dozens of fatalities throughout the nation in 2018 in addition urbanization and anthropogenic activities are expected to further increase flood vulnerability and risk in the coming decades doocy et al 2013 huong and pathirana 2013 flood forecasting systems are widely used to mitigate the impacts of such hydrometeorological extremes by providing decision makers and the public with early warning information the most central part of a flood forecasting system is the hydrological model one of the latest advances of hydrological models is the weather research and forecasting model hydrological modeling system wrf hydro an open source community model developed by the national center for atmospheric research ncar for a range of studies including flash flood prediction regional hydroclimate impacts assessment and water resources management gochis and chen 2003 gochis et al 2015 hydrological models irrespective of their types most often do not provide accurate and reliable estimates of prognostic variables e g soil moisture and streamflow as they are subject to large uncertainties stemming from different sources including hydrometeorological forcing inputs model parameters boundary or initial condition and model structure abbaszadeh et al 2019a bi et al 2015 leach et al 2018 matgen et al 2010 these uncertainties can be accounted for when the hydrologic predictions are produced within a probabilistic framework kuczera and parent 1998 marshall et al 2004 smith and marshall 2008 typically this is performed through bayesian inference over the last couple of decades data assimilation da has garnered increasing attention among researchers and practitioners as an effective and reliable method based on bayesian theory to incorporate the hydrometeorological observations from in situ and remotely sensed measurements into hydrological models for enhancing their forecasting skills while taking into account the associated uncertainties reichle et al 2002 seo et al 2014 particle filter pf is a class of bayesian data assimilation approach that has been widely used in hydrologic studies dechant and moradkhani 2012 dong et al 2015 montzka et al 2013 evolutionary particle filter and markov chain monte carlo epfm recently developed by abbaszadeh et al 2018a is a successor version of pf mcmc moradkhani et al 2012 to improve both the state and parameter estimation of a high dimensional system in this study we will use this data assimilation approach for more information about data assimilation techniques in hydrometeorological studies we refer the interested readers to moradkhani et al 2018 soil moisture is a key hydrological variable that plays an imperative role in the terrestrial water cycle through controlling the water and energy fluxes in the land atmosphere interface many studies have assimilated soil moisture data collected from ground based stations or retrieved from satellites into hydrological models to improve their predictive skills these include studies ranging from drought monitoring bolten et al 2010 and flood forecasting massari et al 2018 wanders et al 2014 to irrigation and water resources management el sharif et al 2015 felfelani et al 2018 moreover significant efforts have been made over the past few years to assimilate downscaled and disaggregated soil moisture products into land surface models although their impact on hydrological model predictions has not yet been entirely explored lópez lópez et al 2017 malbéteau et al 2018 sahoo et al 2013 on the other hand streamflow is the main component of the hydrologic cycle generated through a combination of land surface and sub surfaces processes that incorporate the information of hydrologic states and fluxes streamflow observations measured by in situ sensors most often are assimilated into hydrological models to update the model states and or parameters and hence to improve the estimation of predictive uncertainty abbaszadeh et al 2018a clark et al 2008 while these prognostic variables soil moisture and streamflow are often independently assimilated into the hydrological models many other studies have shown that their joint assimilation is a more effective approach in improving model predictions and quantifying associated uncertainties aubert et al 2003 lee et al 2011 lópez lópez et al 2017 however such experiments typically are based on lumped or semi distributed hydrological models due to their simplicity and low computational requirements with recent advances in satellite remote sensing technologies soil moisture data have become available at a global scale with decent spatial and temporal resolutions while these products are promoting a wide range of global and continental land atmosphere studies their spatial resolutions impede their use in regional and local studies that require data with a resolution of 1 km or less abbaszadeh et al 2019 entekhabi et al 2010 peng et al 2017 particle filtering data assimilation in conjunction with fully distributed hydrologic models is still at infancy mostly due to its complexity and sophistication to the best of our knowledge this subject has not been explored in depth in the literature therefore the present study aims to examine the benefit of independently and jointly assimilating satellite soil moisture data at different spatial resolutions and streamflow observations into a fully distributed wrf hydro model we also aimed to understand the extent to which these observations can contribute to improving the model forecasts particularly during extreme events therefore we conducted our study over a region in southeast texas where heavy rainfall from hurricane harvey caused flooding in 2017 model calibration and da play complementary roles in improving hydrological model simulations model calibration addresses the deficiencies in the model s representation of physical processes through tuning the model parameters and finding the best set which maximizes the agreement between the model simulation and observation koster et al 2018 however da addresses such deficiencies by rectifying the model state variables depending on the availability of its corresponding observation data at a certain time and location previous studies mostly relied on parsimonious hydrological models and results showed that using both approaches could lead to acceptable accuracy of model estimates koster et al 2018 vrugt et al 2005 xu et al 2014 yucel et al 2015 in this study we will examine this conjecture over a very large basin and using a high resolution fully distributed hydrological model and advanced particle filter based data assimilation approach the remainder of the paper is organized as follows section 2 describes the study area and the datasets including the satellite soil moisture products united states geological survey usgs observations and north american land data assimilation system nldas data section 3 briefly explains the wrf hydro hydrologic model and section 4 outlines the proposed research framework section 5 provides a thorough explanation of the wrf hydro model setup and summarizes the model calibration and simulation results section 6 discusses different da scenarios including independent or joint assimilation of satellite soil moisture and streamflow observation into the wrf hydro model and their contributions to improving the model simulations section 7 summarizes the findings of the paper 2 study area and datasets following hurricane rita in 2005 and ike in 2008 hurricane harvey with more than 50 inches of total rainfall in 2017 was the strongest category 4 tropical cyclone to strike the united states in more than a decade resulting in catastrophic flooding dozens of fatalities and more than 200 million in crop losses hurricane harvey with 125 billion in damage was one of the costliest natural disasters in the history of the united states comparable to hurricane katrina in 2005 in this paper we conduct our study over a region with an area of 995 16 square miles located in southeast texas eastern part of san jacinto basin this area is one of the fastest growing population regions in the country this population growth has led to increased urbanization within the basin this along with the proximity to the gulf of mexico and associated tropical storms and hurricanes has made this watershed more vulnerable to flooding from both intensified precipitation due to climate change and increased runoff because of rapid urbanization zhu et al 2015 fig 1 demonstrates a detailed map of the study area such as the watershed boundary topography lakes stream networks major rivers and usgs streamflow gauges san jacinto is the main river in the region where flooding from hurricane harvey damaged the protective barrier at the san jacinto river waste pits site and polluted the river resulting in a 115 million cleanup program for more information please see https www epa gov newsreleases san jacinto waste pits superfund site cleanup plan approved san jacinto river flows across montgomery county to the south and forms lake houston san jacinto basin has a warm and humid climate the annual average rainfall in this region is about 51 inches while the snow is insignificant throughout the year the topography of the san jacinto basin is slightly hilly except along the san jacinto river where there are extensive areas of flood plains https webapps usgs gov harvey 2 1 smap soil moisture products the soil moisture active passive smap satellite equipped with two instruments a radar active and a radiometer passive was launched on january 2015 to provide high resolution soil moisture and detect frozen and thawed soils on a global scale the primary goal of this national aeronautics and space administration nasa satellite mission is to understand the links between earth s water energy and carbon cycles reduce the uncertainties in predicting the weather and climate and improve monitoring and predicting of natural disasters such as floods and droughts the smap satellite was originally designed to measure topsoil layer moisture 0 5 cm with a 9 km spatial resolution by combining l band brightness temperatures at 36 km resolution and 3 km high resolution l band radar backscatter data entekhabi et al 2010 unfortunately on july 7 2015 smap s radar stopped transmitting due to the irrecoverable hardware failure of the radar instrument and since then smap s radiometer became the only operational instrument providing soil moisture at 36 km spatial resolution chan et al 2016 although this product has a decent spatial resolution for continental or global studies it cannot be directly used for local or regional studies that require a finer resolution data entekhabi et al 2010 to circumvent this problem in january 2017 nasa announced a product named the enhanced smap radiometer in this dataset the standard smap data gridded at 36 km were interpolated into the global cylindrical ease grid 2 0 projection with 9 km spacing using the backus gilbert optimal interpolation algorithm one year later in 2018 nasa proclaimed a new soil moisture product with 3 km spatial resolution in this dataset smap l band brightness temperatures and copernicus sentinel 1 c band backscatter coefficients were used to generate soil moisture data which is then resampled to an earth fixed cylindrical 3 km equal area scalable earth grid version 2 0 das et al 2018 it should be noted that these soil moisture products are available on both ascending and descending orbit despite such advances some land surface applications such as water management agricultural production drought monitoring and flood forecasting still require soil moisture at finer resolutions from a kilometer to a sub kilometer scale for this purpose a few studies have rescaled the smap soil moisture data to a finer resolution abbaszadeh et al 2019b alemohammad et al 2018 zhao et al 2018 in this paper we evaluate the impact of assimilating smap soil moisture observations at different spatial resolutions into the wrf hydro model the descriptions of all smap soil moisture datasets used in this study are summarized in fig 2 this figure used as an example for conceptualization illustrates smap soil moisture data at three different spatial resolutions across the state of texas on 26 august 2017 when the hurricane harvey hit the southeast region of this state 2 2 nldas 2 forcing data the north american land data assimilation system nldas 2 provides quality controlled and spatiotemporally consistent datasets from best available observations to support modeling activities in this paper nldas 2 datasets including incoming shortwave radiation incoming longwave radiation specific humidity air temperature surface pressure near surface wind in the u and v components and precipitation rate are used to force wrf hydro model nldas 2 has a 12 5 km spatial resolution hourly temporal resolution and the data ranges from january 1979 to present nldas 2 forcing data is re gridded to match the geogrid domain of the wrf hydro model 2 3 usgs streamflow gauges in this study we used four usgs gauges installed within the basin fig 1 to calibrate the wrf hydro model assess the streamflow simulation results and perform data assimilation the usgs hydrograph not shown here shows that the streamflow peak caused by hurricane harvey in the river was well captured by all four gauges the study area is hence well situated for a study aimed at understanding the efficacy of the wrf hydro model in simulating floods triggered by heavy rainfall from hurricanes such as harvey usgs station 08070500 caney ck nr splendora is located in the caney creek river in caney creek watershed this perennial river is one of the tributaries of the san jacinto river that flows 155 miles southeast and drains into the gulf of mexico usgs station 08071000 peach ck at splendora is installed in the peach creek river in peach creek watershed this river is 27 miles long and is one of the tributaries of the san bernard river caney creek and peach creek are both located about 40 miles north of houston texas and the landuse of both the watersheds is dominated by pasture and forest the average annual rainfall in this area is 46 inches with uneven distribution of rainfall throughout the year usgs station 08070200 e fk san jacinto rv nr new caney is located in san jacinto river in san jacinto watershed this river flows southeast for about 28 miles and eventually drains into the galveston bay usgs station 08071280 luce bayou abv lk houston nr huffman is located in the luce bayou river in luce bayou watershed this river with 35 miles length is a single primary stream of this watershed that drains into the east fork san jacinto river the prevailing climate in this watershed is humid subtropical and cropland rangeland and pasture are the dominant land uses 3 wrf hydro hydrological model wrf hydro model was initially designed to facilitate coupling between the weather research and forecasting wrf and different elements of terrestrial hydrological systems while accounting for the discrepancy between the resolutions of atmospheric and hydrological models gochis and chen 2003 multiple land surface models lsms have been configured with the wrf hydro to simulate land surface processes in this study we compiled wrf hydro in the uncoupled mode offline with noah land surface model with multi parameterization options noah mp in offline mode there is no interaction between the lsm and wrf atmospheric model wrf hydro model is a fully distributed system that integrates different hydrological and hydraulic modules to simulate the surface overland flow subsurface saturated flow channel routing and baseflow processes the wrf hydro model is fully parallelized and runs on cluster and high performance computing hpc systems wrf hydro consists of different water routing modules that enable diffusive wave surface routing and saturated subsurface flow routing on a 250 m grid and muskingum cunge channel routing down national hydrography dataset plus version 2 nhdplusv2 stream reaches noah mp is available as a column lsm option in wrf hydro that accounts for soil infiltration and redistribution processes in addition to this wrf hydro with its routing module uses high resolution topography data to further help redistribute terrestrial moisture one of the significant enhancements of the wrf hydro model is its ability to route infiltration capacity excess and saturated subsurface water to simulate the surface runoff the routed infiltration capacity excess in the 1 d soil column along with additional exfiltration from saturated soil is allowed to remain on the land surface as ponded water which is available for lateral redistribution if suitable conditions are met ryu et al 2017 senatore et al 2015 yucel and onen 2014 although this model has recently been successfully applied in several regions throughout the world arnault et al 2016 lahmers et al 2019 lin et al 2018 silver et al 2017 wehbe et al 2018 the integration of state of the art data assimilation approaches such as the particle filtering with this model has not yet been implemented 4 the proposed research framework the data assimilation method in this study utilizes sequential monte carlo techniques to generate the replicates of model forcing and states and through a formal bayesian approach obtains a full probability distribution of the variables of interests and characterizes the predictive uncertainty the sequential assimilation techniques have been widely used in hydrologic prediction studies and provide effective means to assimilate ground based and remotely sensed observations into an lsm in this study we use the evolutionary particle filter with markov chain monte carlo hereafter epfm to improve the wrf hydro model predictions and quantify the associated uncertainties we refer the readers to our previous study abbaszadeh et al 2018a where we presented a detailed description about epfm approach its implementation and benefits in this study we used the noah mp land surface model which contains a set of canopy and surface energy fluxes to represent the exchanges of energy and water between the multi layer land surface and atmosphere when this land surface model is combined with the high resolution routing module of wrf hydro 250 m grid it enables producing soil moisture information to the noah mp 1 km grid fig 3 in this study we considered 250 meter grid resolution for flow routing and 1 km grid resolution for noah mp model run nldas meteorological forcing data are re gridded to noah mp 1 km grid resolution that are required by the wrf hydro modeling system uncertainties that arise from this disaggregation process are accounted for in the data assimilation framework in this research the model is first set up over a geogrid domain covering the study region the dynamically dimensioned search dds algorithm tolson and shoemaker 2007 is then used to calibrate the wrf hydro model parameters and finally the calibrated model is used to simulate streamflow at four usgs gauges installed within the basin we will provide a complete description of the wrf hydro model setup calibration and simulation in section 7 in this study we assimilate smap soil moisture observations ascending and descending overpasses at different resolutions 36 km native footprint and 9 km backus gilbert interpolated along with the recently downscaled smap product at 1 km abbaszadeh et al 2019b into the wrf hydro model with the aim to improve the accuracy and reliability of wrf hydro model predictions in this study we bias corrected the downscaled soil moisture data using the cumulative distribution function cdf matching approach reichle and koster 2004 in all four da scenarios designed we assimilate the soil moisture observation independently as well as in conjunction with the streamflow data to explore the best da configuration leading to the most improved model estimates in this research the wrf hydro model is equipped with an enhanced ensemble da approach epfm abbaszadeh et al 2018a which is appropriate da involving high dimension of state variables and parameters 5 wrf hydro modeling this section provides a progressive outline for the wrf hydro model setup and discusses in detail the model calibration process and assessment of the model the first step to setup the wrf hydro model is to define the geogrid domain file using the wrf preprocessing system wps the next step is to prepare the initial condition file using the geogrid file created earlier a high resolution digital elevation model dem and other optional inputs i e shapefiles of forecast points and lakes the wrf hydro gis pre processing tool is used to create the geospatial and tabular data layers that define the terrestrial overland flow subsurface flow and channel routing processes necessary for wrf hydro modeling in this study we used the nhdplusv2 to extract the dem for the study area wrf hydro model has three configurations national water model nwm gridded and reach in this study we selected gridded configuration that uses diffusive wave routing the earth system modeling framework esmf script is used to re grid the nldas meteorological forcing data to match the wrf hydro geogrid domain grid in order to run the wrf hydro executable wrf hydro exe two namelist files namelist hrldas noah mp land surface model namelist and hydro namelist should be carefully configured 5 1 wrf hydro model calibration and simulation results in this section we first describe how the wrf hydro model parameters are calibrated the model parameters are categorized into six groups including soil parameters runoff parameters groundwater parameters vegetation parameters snow parameter and channel parameters table 1 lists a subset of wrf hydro model parameters defined by ncar along with their minimum and maximum values these numbers are suggested in the automated ncar calibration tool fig 4 shows the procedure used to calibrate the wrf hydro model phase i and implement the data assimilation approach phase ii we split the entire period into three sub periods first nine months from 01 01 2016 to 09 30 2016 is used as a spin up warm up period to initialize the system and then the water year 2016 2017 from 10 01 2016 to 09 30 2017 is used to calibrate the model parameters the last three months of the year 2017 is also considered to validate the calibrated model in the spin up period we first employed a manual calibration to identify the most relevant model parameters thereafter we excluded some of the parameters including channel side slope chsslp initial channel depth hlink melt factor for snow depletion curve mfsno and the exponent in the resistance equation for soil evaporation rsurfexp from the parameter set due to their insignificant contribution to improving the model calibration performance then we ran the model using cold start initial condition in the spin up period to obtain warm state variables of the noah mp model and terrain routing module of the wrf hydro system see fig 3 these state variables are then used as initial conditions in the main model calibration process this process is consistent with the practices of the ncar wrf hydro development team senatore et al 2015 it is also important to note that wrf hydro model calibration is performed by optimizing hourly streamflow using dynamically dimension search dds algorithm tolson and shoemaker 2007 this optimization algorithm is able to converge to a near optimal solution with approximately 100 500 iterations lespinas et al 2018 which is much faster than the widely used shuffled complex evolution sce algorithm that requires 10 000 iterations to converge to a solution with the same accuracy duan et al 1992 therefore dds is better suited for calibration of the computationally intensive fully distributed physically based hydrological models like wrf hydro lahmers et al 2019 in this study we used all four usgs stations operated within the watershed to calibrate the wrf hydro model parameters fig 5 shows the model calibration process using 140 iterations for the water year 2016 2017 further investigations also revealed that increasing the number of dds iterations beyond 140 does not improve the calibration effectiveness and only adds to the computational burden we used the kling gupta efficiency kge gupta et al 2009 abbaszadeh et al 2018b as an objective function metric for all calibrations performed in this study kge along with other performance measures used in this study are described in appendix a of the paper as seen in fig 5 almost all the model simulations captured the high flows caused by heavy rainfall from hurricane harvey from august 17 2017 to september 2 2017 this is because the streamflow is much more controlled by the torrential rain during an extreme event rather than the parameters variabilities however model parameters tend to be more determinant than the forcing data for low to medium flows for the model calibration the noah mp time step was set to one hour which is the standard in the operational nwm after the model calibration we performed a simple one at a time parameter sensitivity analysis on the model parameters and the results revealed that channel parameters i e channel bottom width and manning s roughness and groundwater parameters i e maximum groundwater bucket depth and bucket model exponent along with surface runoff parameter refkdt had the greatest impact on the wrf hydro model performance refkdt is a runoff parameter that significantly impacts surface infiltration and hence the partitioning of total runoff into surface and subsurface runoff zmax is the maximum storage of the bucket to represent and conceptualize the groundwater process wrf hydro uses an exponential function which includes expon parameter to estimate the bucket discharge as a function of water depth in the bucket bw and mannn are the channel bottom width and manning s roughness coefficient that collectively shape the overland flow process it is important to note that these parameters have no real physical meaning and are only used for model conceptualization our sensitivity analysis also indicated that in our case study the performance of the wrf hydro model is more sensitive to the runoff parameters compared to the vegetation and soil parameters therefore in the next section the most relevant model parameters i e refkdt zmax expon bw and mannn will be used in the development of the joint state parameter data assimilation however the rest of the parameters will be used at their calibrated values to assess the effectiveness of the model we computed three deterministic performance measures including kge root mean square error rmse and pearson correlation coefficient pcc table 2 summarizes the model performance at four usgs stations for both calibrated blue line in fig 5 and un calibrated red line in fig 5 model scenarios as illustrated in fig 5 in all cases the model exhibits either positive or negative bias in the simulations indicating overestimation and underestimation of the streamflows this is most likely due to the model structure and parameterization errors in model calibration which is expected to be accounted for later in the joint state parameter data assimilation framework it is also important to note that although the wrf hydro model estimated the peak streamflow values well specifically those measured by usgs stations during hurricane harvey from august 17 2017 to september 2 2017 it did not provide accurate estimates during recession periods and low flow conditions zmax and expon are the two key groundwater parameters that affect such flow regimes by controlling the baseflow and the groundwater storage refkdt is also an important parameter that influences the amount of water that flows into the channel which in turn affects the runoff process the simultaneous adjustment of these model parameters together with model state variables through the dual state parameter data assimilation approach introduced in section 5 may improve the effectiveness and usefulness of the wrf hydro model in estimating all possible streamflow regimes this conjecture will be discussed in detail in the next section 6 assimilation of in situ and remotely sensed observations in sections 6 1 and 6 2 we demonstrate independent assimilation of usgs streamflow and smap soil moisture observations at different spatial resolutions into the wrf hydro model following these in section 6 3 we conduct the multivariate assimilation of these two observations to show its usefulness in comparison to independent assimilation configuration in this study all the assimilation runs are performed at the daily time scale which is identical to the temporal scale of the meteorological forcing satellite observations smap soil moisture at 36 km 9 km and 1 km and the daily aggregated streamflow time series multivariate joint data assimilation refers to the simultaneous assimilation of multiple observation data for different model state variables into a hydrological model it is noted that all the assimilation experiments are based on joint state parameter estimation as discussed earlier we only use the most sensitive model parameters i e refkdt zmax expon bw and mannn in the joint sequential data assimilation framework however for other model parameters the calibrated values are directly used 6 1 assimilation of streamflow observations into the wrf hydro model here we briefly explain the data assimilation settings given the multiplicative nature of error in precipitation we assumed a lognormal error distribution with mean zero and relative error of 25 to perturb the precipitation this is in fact a heteroscedastic error meaning that the error is proportional to the rain intensity for temperature the error is assumed to be homoscedastic and normally distributed with mean zero and standard deviation of 5 c uncertainty in other forcing data including incoming shortwave radiation incoming longwave radiation specific humidity surface pressure near surface wind in the u and v components were assumed insignificant to characterize uncertainty in the initial condition we assumed a normal error distribution with mean zero and a standard deviation of 0 04 m3 m3 for the soil moisture at the topsoil layer note that wrf hydro model has two restart files that represent the initial condition for the land surface model and routing module respectively in this study we only added white noise to the soil moisture at the top 10 cm soil layer in the land surface model it is also assumed that the streamflow observation errors follow a heteroscedastic normal distribution with a relative error of 15 percent in real case experiments the other major part of the errors arises from the model structural uncertainty which herein is represented by adding white noise with a relative error of 25 percent to the model outputs in this study all the errors were assumed uncorrelated and used with the same magnitude in all assimilation runs these error values are chosen following our previous studies abbaszadeh et al 2018b dechant and moradkhani 2012 and also trial and error to provide a comprehensive assessment of the effectiveness and usefulness of the epfm data assimilation approach we computed multiple performance measures i e kge and rmse fig 6 illustrates the performance of the wrf hydro model with and without da for the period of hurricane harvey august 17 2017 to september 02 2017 the results show that both da and open loop ol model runs had similar performance in identifying the onset of flooding however da showed better results for the recession period as compared to the ol when comparing the results of the ol and da runs it is concluded that assimilating streamflow into the wrf hydro model is quite useful in improving its performance although this improvement at gauge 8070200 remains limited this is may be due to sparse point observations that are assimilated into the gridded model such that the spatial discrepancy between the model outputs and observations results in suboptimal model performance this deficiency is expected to be alleviated by assimilating satellite soil moisture data into the wrf hydro model this hypothesis will be examined in the next section it is also important to mention that in this study all the assimilation runs are based on ensemble size of 90 the findings of this study also revealed that in ensemble data assimilation with wrf hydro model based on particle filtering adjusting the model state variables in conjunction with model parameters provides more accurate and reliable posterior distributions compared to state estimation only figure s1 in supplementary file illustrates the prior streamflow versus observation which is critical in interpreting the usefulness of the assimilation in providing the best possible set of model states at forecast time figures s2 and s3 respectively show the temporal evolution of model parameters over the study period and the distribution of particle weights at several time steps these are important information as they indicate the stable behavior of model parameter after nearly 1 year of assimilation also the particle weights distribution over time shows a tendency toward normal distribution and stability of the weights that with stable parameters result in improved model predictions 6 2 assimilation of smap soil moisture products into the wrf hydro model in this section we examine the influence of assimilating smap soil moisture observations at different spatial scales on the wrf hydro model performance fig 7 is a taylor diagram that simultaneously illustrates the deterministic i e time series correlation mean bias and normalized root mean square difference rmsd and probabilistic i e normalized root mean square error ratio nrr performance measures calculated at four usgs stations based on four assimilation configurations including assimilation of all streamflow observations within the watershed and assimilation of smap soil moisture at three different spatial resolutions the results reported in fig 9 are calculated for the year 2017 figures s4 s5 and s6 in the supplementary file show the contribution of assimilating smap soil moisture at 36 km 9 km and 1 km spatial resolutions to enhance the streamflow prediction during the hurricane harvey nrr dechant and moradkhani 2012 is calculated to measure the ensemble spread and assess how confidently the ensemble mean is statistically distinguishable from the ensemble spread these performance measures are fully described in appendix a the results revealed that the assimilation of smap soil moisture at 1 km spatial resolution contributes more to the improvement of wrf hydro model prediction than the coarser scale smap products 36 and 9 km although the assimilation of smap soil moisture at 1 km scale to some extent could improve the streamflow simulation accuracy see figure s4 in supplementary file this improvement was not as good as that achieved by assimilating streamflow observation into the model it can therefore be concluded that assimilating streamflow data at the usgs locations results in better representation of posterior distribution compared to assimilating soil moisture data at even higher resolution it is important to note that the standard deviation ratios are always around the arc of one unit that indicates similar variability bias in the variance between the simulated streamflows and usgs observations regardless of the type of assimilation configuration further analysis revealed that there is a complementary relationship between the nrr probabilistic measure and correlation coefficient normalized rmsd deterministic measures almost in all assimilation runs such that when the correlation coefficient normalized rmsd improves the nrr gets closer to 1 which is an ideal value of nrr and when the deterministic measures deteriorate resulting in a suboptimal value for nrr based on the aforementioned discussion we will use the streamflow observations at all four usgs gauges and smap soil moisture at 1 km spatial resolution in the implementation of multivariate data assimilation this will be discussed in the next section comparing the univariate assimilation with ol simulations at four usgs stations over the entire period of study confirmed that although the assimilation of streamflow and soil moisture at 1 km spatial resolution could significantly outperform the ol predictions assimilation of soil moisture at coarser spatial resolutions i e 9 km or 36 km had marginal performance compared to ol simulation the results also indicated that although assimilating the smap soil moisture into wrf hydro model would partially improve the streamflow estimates assimilation of streamflow has relatively small impact on the improvement of soil moisture estimates this may be due to the fact that streamflow observations are assimilated into the model only at certain points therefore while improving the streamflow simulation soil moisture estimates that are spatially variable are not fully corrected in streamflow assimilation the likelihood values and consequently the particle weights are calculated based on the simulated and observed streamflow at those points where the usgs stations are located this information although helps improve the streamflow prediction it is not that beneficial for updating the soil moisture at each grid cell across the entire domain in fact the observed streamflow is the result of hydrologic processes over a longer period of time across the watershed as a result it is often difficult to merely assimilate the outlet discharge at a time step and expect the best trajectories of watershed state gichamo and tarboton 2019 this problem is expected to be addressed when the streamflow and soil moisture observations are jointly assimilated into the model fig 8 demonstrates the spatial pattern of the updated soil moisture posterior mean versus the smap soil moisture data at 1 km spatial resolution the upper panel shows the average smap and downscaled soil moisture for the period of hurricane harvey from august 17 2017 to september 2 2017 across the study area it is clear that on average the downscaled soil moisture at 1 km spatial resolution is dryer than the original smap soil moisture at 36 km grid cell the lower panel shows an acceptable pearson s correlation coefficient between the updated soil moisture and smap at 1 km spatial resolution across the entire domain except for the northeast region where the lake livingston is located this is because the footprint of original smap soil moisture at 36 km over this region is heavily affected by the lake that consequently leads to the overestimation of soil moisture this wet bias in observational data when assimilated into the wrf hydro model results in inaccurate model predictions both the pcc and rmse confirm that the assimilation performance is degraded across the regions where 1 km grid cells are dominated by water bodies on average the rmse between the updated and smap soil moisture data at 1 km spatial resolution is below 0 04 m3 m3 that corroborates with the accuracy requirement of smap soil moisture retrieval it is worthy to mention that unlike the normal streamflow condition where assimilation of smap soil moisture at 1 km spatial resolution better than other coarse scale soil moisture observations would contribute to improving the streamflow prediction all the soil moisture data irrespective of their spatial resolutions would similarly improve the streamflow simulation during the hurricane harvey see figures s4 s5 and s6 in the supplementary file 6 3 multivariate assimilation of smap soil moisture and streamflow observations in this section we will jointly assimilate the streamflow observations and 1 km smap soil moisture into the wrf hydro model to further improve the estimation of the topsoil layer moisture content and quality of ensemble streamflow prediction we examined the results for the usgs gauges within the watershed and noticed that all the deterministic measures indicate the superior performance while conducting the joint assimilation as compared to a univariate assimilation configuration fig 9 illustrates the benefit of multivariate assimilation of satellite soil moisture and streamflow observation in improving the wrf hydro streamflow simulation during the period of hurricane harvey it is also important to note that the updated soil moisture at 1 km spatial resolution was in good agreement with the smap soil moisture observation at 1 km spatial resolution almost for the entire domain except the region where the lake livingston is located this is similar to the results of the univariate assimilation of soil moisture data and streamflow observation discussed in fig 8 the low performance of the model over this region is potentially due to the original smap soil moisture observation at 36 km spatial resolution that involves high retrieval error over this area comparing both univariate and multivariate assimilation configurations also revealed that in all scenarios the data assimilation results in significant improvement of medium to high streamflow predictions this may be attributed to the inherent characteristic of the wrf hydro model whose performance during the wet periods is higher than the dry periods table 3 summarizes the results of all data assimilation experiments conducted in this study these results are obtained for the period of hurricane harvey indicating the significance of each assimilation scenario in improving the streamflow prediction during an extreme event the univariate and multivariate assimilation results as shown in table 3 always outperform the ol simulation during the period of hurricane harvey high flow condition however as shown earlier in fig 7 the results of the open loop model simulations under low to medium flow conditions were similar to those obtained by univariate assimilation of soil moisture at 9 km and 36 km spatial resolutions the findings also show that the multivariate data assimilation results in better streamflow simulation regardless of the watershed s streamflow regime it should be noted that during the hurricane harvey assimilation of soil moisture at different spatial resolutions had similar impact on improving the streamflow simulation however during the normal flow condition the finer the spatial resolution of soil moisture the higher the accuracy of streamflow prediction note that the computational cost depends on the high performance computing hpc configuration and parallelization of the ensemble model simulations in this study we used a linux cluster node including 2 cpus 28 cores per cpu commensurate with the computational requirements of the parallel epfm data assimilation framework that works based on an ensemble size of 90 the runtime of the ensemble data assimilation for each day was approximately 6 minutes it is also important to note that all the assimilation experiments irrespective of their types i e univariate or multivariate had almost similar computational time this is due to the parallelization of hydrological data assimilation system and its operation on the hpc cluster which meets the computational demand of the ensemble simulations also in this data assimilation experiment we noticed that almost 97 percent of the runtime at each time step is associated with running the wrf hydro model itself 7 summary and conclusion over the last decade tropical storms and hurricanes have become more destructive and frequent in the southeast us mainly due to climate change and climate variability lim et al 2018 they most often are accompanied by violent winds and torrential rains which can lead to catastrophic flooding severe rainfall and hurricane induced flooding annually cause billions of dollars in damages property losses and a significant number of fatalities throughout the nation although hydrological models are widely used as an efficient means to estimate such floods their predicted values most often are not accurate and reliable as the model prognostic variables such as soil moisture and streamflow are subject to large uncertainties stemming from hydrometeorological forcing model parameters boundary or initial condition and model structure data assimilation has been recognized as an effective method that takes these uncertainties into account and improves hydrologic prediction with the recent advances in the satellite remote sensing technologies and ground based observation observational data is becoming increasingly available leading to the widespread use of hydrologic data assimilation in various applications despite such developments assimilation of remotely sensed soil moisture data at fine resolutions and streamflow observations into hyper resolution hydrologic models e g wrf hydro has been rarely done therefore as a prototype study we implemented hydrologic data assimilation using an evolutionary particle filter over a region in southeast texas where heavy rainfall from hurricane harvey caused fatal flooding the findings of this study showed that during the normal streamflow condition univariate assimilation of smap soil moisture at 1 km improves the streamflow simulation results more than the assimilation of coarse resolution products including the smap native product of 36 km spatial resolution smap l3 sm p and its interpolated version at 9 km spatial resolution smap l3 sm p e however during the period of hurricane harvey the soil moisture observations at different resolutions have similar impact on improving the streamflow prediction the results also indicated that multivariate assimilation of soil moisture and streamflow observations results in more desirable posterior distribution than any independent assimilation configuration we investigated the model performance during the hurricane harvey and post harvey periods and realized that although da whether univariate or multivariate and ol model runs have shown similar results in characterizing the onset of flooding the da performed better in predicting the termination of flooding streamflow recession period author statement credit authorship contribution statement peyman abbaszadeh conceptualization methodology software writing original draft data curation keyhan gavahi software methodology visualization hamid moradkhani supervision conceptualization methodology writing review editing declaration of competing interest the authors declare no competing interests supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103721 appendix b supplementary materials image application 1 appendix a summary of performance measures used in this study is tabulated in table a1 these performances measures are fully described in abbaszadeh et al 2019a dechant and moradkhani 2012 
397,safe and cost effective design of infrastructures such as dams bridges highways often requires knowing the magnitude and frequency of peak floods the generalized extreme value distribution g e v prevailed in flood frequency analysis along with distributions comprising location scale and shape parameters here we explore alternative models and propose power type models having one scale and two shape parameters the burr type iii ɓriii and xii ɓrxii distributions are compared against the g e v in 1088 streamflow records of annual peaks across canada a generic l moment algorithm is devised to fit the distributions also applicable to distributions without analytical l moment expressions the analysis shows 1 the models perform equally well when describing the observed annual peaks 2 the right tail appears heavier in the ɓriii and ɓrxii models leading to larger streamflow predictions when compared to those of g e v 3 the g e v predicts upper streamflow limits in 39 1 of the records these limits have realistic exceedance probabilities based on the other two models 4 the tail heaviness estimation seems not robust in the g e v case when compared to the ɓriii and ɓrxii models and this could challenge g e v s reliability in predicting streamflow at large return periods and 5 regional variation is observed in the behaviour of flood peaks across different climatic regions of canada the findings of this study reveal potential limitations in using the g e v for flood frequency analysis and suggest the ɓriii and ɓrxii as consistent alternatives worth exploring keywords flood peaks extreme value theory gev distribution burr type iii distribution 1 introduction estimation of streamflow magnitude at large return periods is essential in engineering practice and has applications in infrastructure design water resources management flood risk estimation etc the typical procedure uses historical streamflow records to assess and fit appropriate probability distributions the reliability of streamflow estimates depends on the suitability of the selected distribution suitability does not necessarily imply just good fitting to the observed data many different distributions may provide this the selected model should be also theoretically consistent and in agreement with the broader knowledge we have on the probabilistic behaviour of a process for example exponential and power type distributions might fit equally well to observations in a specific streamflow record yet power type models heavier tails predict larger streamflow magnitude for large return periods if the regional knowledge of the process under study indicates a specific probabilistic behaviour then it should be considered in the selection of the model even if a different type of model fits equally well or even better the literature is rich in studies assessing and suggesting distributions for streamflow analysis in various regions of the world the most popular probability distribution models are the generalized extreme value g e v the three parameter lognormal l n 3 and the three parameter log pearson l p 3 however many more have been used including the two parameter lognormal l n two component extreme value distribution t c e v generalized logistic g l o pearson type iii p 3 etc for example arnell gabriele 1988 tested the t c e v g e v and wakeby w a distributions in the united kingdom and italy pearson 1991 described annual maxima in south canterbury new zealand with the fréchet distribution g e v with positive shape parameter vogel et al 1993 analysed flood data from 383 sites in the southwestern u s using l moment diagrams for model selection and showed that the g e v l p 3 l n and l n 3 describe well the flows in this region rao hamed 1994 suggested the g e v and l n 3 distributions for annual maxima at cauvery river in india önöz bayazit 1995 fitted and compared the l n l n 3 p 3 and l p 3 in large samples worldwide karim chowdhury 1995 indicated the g e v as the best model for streamflow in bangladesh after testing the l n gumbel g υ l p 3 and g e v vogel wilson 1996 based on l moment ratio diagrams investigated the performance of the g e v and l p 3 distributions for the annual maxima in the united states in more than 1455 sites kumar et al 2003 used the g e v distribution for annual maxima in the middle ganga plains in india ferro porto 2006 used the t c e v in sicily italy saf 2009 described annual maxima in the west mediterranean region in turkey using the p 3 and g l o distributions villarini and smith 2010 and villarini et al 2011 investigated annual peaks in eastern midewest united states using the g e v and mixtures of flood peak distributions kochanek et al 2014 recommended the use of g e v for annual maxima in france but stressed the fitting should be based on regional estimates salinas et al 2014 assessed the performance of g e v g l o l n 3 and p 3 in europe implying that the g e v cannot describe samples with medium and high skewness rutkowska et al 2017 and hassan et al 2019 proposed different distributions for different regions for poland and the torne riven in sweden respectively and recently miniussi et al 2020 used the metastatistical extreme value distribution m e v to interpret streamflow peaks observed across the continental united states in canada the intense topographic and climate variations mahmood et al 2017 make identifying a single distribution for streamflow frequency analysis in all regions a challenging task such variations result in streamflow with different characteristics buttle et al 2016 gullett skinner 1992 in different hydrographic regions in canada generally evidence shows that flood generation mechanisms vary regionally and seasonally pomeroy et al 2016 shook 2015 stadnyk et al 2016 yue wang 2004a 2004b also emphasized the importance of selecting an appropriate distribution for streamflow based on the regional climatic conditions previous studies focusing on canada used and tested several distributions to describe annual streamflow peaks for example the l n was proposed for the prairies spence 1972 the g e v was used in nova scotia pilon adamowski 1992 southern ontario glaves waylen 1997 and quebec yue et al 1999 the g e v l n 3 and l p 3 were used in several canadian regions yue wang 2004a and recently the g e v was proposed as a suitable model across canada zhang et al 2019 the previous literature review reveals 1 many different distributions have been proposed and used to describe streamflow peaks 2 the prevailing model is the g e v distribution and 3 the most well performing models are three parameter models that include one location one scale and one shape parameter exception is l p 3 with two shape and one scale parameter but the parameters interact in a complex way all these three parameter distributions are not consistent with the potential range of streamflow peaks streamflow can have any positive value and these models have lower bounds that can be even negative or worse upper bounds additionally a single one shape parameter model cannot reproduce in general the different distribution shapes that are observed in reality which led to testing many different models in order to find the best fit one this approach offers ad hoc choices that conceptually are not appealing for example it is scientifically sound to assume that streamflow peaks in homogenous regions can be described by the same probability law however fitting many different models might result in suggesting structurally different models for different sites within the same region of course this can be due to sample variations and leads to algorithmic treatment of probability laws this study revisits flood peak distributions by proposing structurally different models than those that have been typically used in the literature that is power type distributions defined in the positive real axis and comprising one scale and two shape parameters it aims in finding a single and consistent distribution to describe peaks in any region in canada consistent distributions should be able to describe the exceedance probability of any positive streamflow value and be parsimonious yet flexible enough to be used in any region two such models that are explored here are the burr type iii ɓriii burr 1942 papalexiou koutsoyiannis 2012 and burr type xii ɓrxii burr 1942 papalexiou 2018 other structurally similar models exist such as a reparameterization and generalization of the beta of the second kind b ii which was proposed recently by papalexiou serinaldi 2020 these models which have been neglected in hydrologic practice are tested against the g e v distribution using annual peaks from 1088 gauged sites across canada the significant climatic and topographic variability found in canada and the large number of available records offer an ideal place to stress test these models the theoretical arguments we provide indicate that such models especially the ɓriii distribution can be a suitable choice for annual peaks the empirical evidence of the analysis suggest excellent performance of the ɓriii model and reveals potential limitations for the g e v distribution 2 streamflow data the hydat database of environment and climate change canada eccc version 1 0 http collaboration cmc ec gc ca cmc hydrometrics www provides historical daily average streamflow records at 6371 sites in canada to ensure the quality of the data we used sites with natural flow unregulated by any weir levee dam etc and selected those having daily records of more than 20 years over the 1979 2016 period this screening resulted in 1119 sites also 31 more sites were eliminated as their streamflow time series showed suspicious trends and abrupt changes the final 1088 sites are spread all over canada with the higher density of sites observed in the south fig 1 the climatic classification of canada identifies 11 unique climatic regions based on meteorology topography altitude land exposure existence of large water bodies and other features that affect the hydrology of the region gullett skinner 1992 each region has a unique climate pattern and hence can potentially affect peak streamflow characteristics gullett skinner 1992 the selected 1088 hydat sites overlay 10 of these regions as presented in fig 1 the spatial density of the sites differs among the regions and generally is decreasing from south to north the number of sites in these regions varies from 196 in the canadian prairies pr to 13 sites in the arctic tundra at fig 1 the arctic mountains region does not include any sites 3 methods 3 1 probability distributions for streamflow extremes the extreme value theory evt focusing on block maxima is the prevailing method in engineering practice for flood frequency analysis the origins of evt date back in the works of fréchet 1927 fisher and tippett 1928 and gnedenko 1943 while gumbel 1958 applied and popularized such methods in engineering practice classical evt is derived based on the consideration that the maximum of a random sample of independent and identically distributed i i d random variables r v s with xi fx x parent distribution has distribution function 1 g x f x n in layman s terms and according to the fisher tippett gnedenko theorem if n and depending on the form of the parent distribution f x e g power type exponential etc the g x distribution converges either to the gumbel the fréchet or the reversed weibull distributions see e g papalexiou koutsoyiannis 2013 these three distributions can be unified in a single expression jenkinson 1955 von mises 1936 the so called generalized extreme value g e v distribution with cumulative distribution function 2 f g e v x α β γ exp 1 γ x α β 1 γ where 1 γ x α β 0 α β and γ denote respectively the location scale and shape parameters the g e v for γ 0 is defined in α β γ and thus it has an upper limit for γ 0 is defined in α β γ while for γ 0 it simplifies to the gumbel distribution that is f g u x α β exp exp x α β the g e v distribution for γ 0 despite the exponential function found in its expression is power type distribution heavy tail having exceedance probabilities with asymptotic behaviour p x x x 1 γ although the g e v distribution has been the standard choice in flood frequency analysis the assumptions that justify g e v as a theoretically consistent model are not necessarily valid for streamflow maxima as 1 a typical hydrograph includes rising and recession limbs and thus the r v s describing the daily flows cannot be considered identical equally streamflow series are not identically distributed because they exhibit clear deterministic seasonal behaviour 2 daily flows are highly autocorrelated thus the r v s are not independent and 3 the maximum is not extracted from a sample of very large size thus convergence is not guaranteed of course these arguments are valid for other hydroclimatic variables too in practice apart from the g e v many other distributions have been used to describe annual streamflow maxima see references in the introduction the number of well known distributions in the literature that could be tested as alternative distributions to g e v is vast see e g johnson et al 1994 1995 yet selecting and evaluating their performance without any justification can be subjective and from a practical viewpoint not efficient here two non extreme value distributions are tested that 1 are defined in 0 and thus are consistent with the potential range of extreme streamflow 2 are parsimonious but flexible that is they comprise one scale parameter and two shape parameters to control the left and the right tails 3 have analytical and simple distribution and quantile expressions and 4 have the same asymptotic behaviour as the g e v that is are power type distribution having the potential to model heavy tails the last point is crucial as the nature of streamflow extremes shows evidence of heavy tails this does not imply yet that heavy tails should be taken for granted everywhere identifying the tail type is not trivial and many methods have been invented see e g el adlouni et al 2008 nerantzaki papalexiou 2019 wietzke et al 2020 the alternative models tested are the burr type iii ɓriii and type xii ɓrxii from the burr system burr 1942 using the generalizations and reparameterizations proposed in papalexiou 2018 and papalexiou and koutsoyiannis 2012 respectively these distributions have been neglected in streamflow frequency analysis and only recently have been used for precipitation or suggested as generic models for hydrometeorological variables papalexiou 2018 the cumulative distribution functions of the ɓriii and ɓrxii are given by 3 f b r iii x β γ 1 γ 2 1 1 γ 1 x β 1 γ 2 γ 1 γ 2 4 f b r xii x β γ 1 γ 2 1 1 γ 2 x β γ 1 1 γ 1 γ 2 where β is scale parameter and γ1 and γ2 are shape parameters controlling respectively the left and the right tail the asymptotic behaviour of both distributions is x 1 γ 2 and thus equivalent with that of the g e v note that well known two parameter distributions emerge as special cases of these distributions for specific parameter values specifically the ɓrxii for γ1 1 becomes the pareto ii p ii for γ2 0 limiting case collapses to the weibull distribution w and for γ1 1 and γ2 0 becomes the celebrated exponential distribution the ɓriii for γ2 1 γ1 coincides with the log logistic distribution and for γ1 becomes the inverse weibull distribution from a mathematical viewpoint these two distributions are not limiting extreme value distributions yet the ɓriii has a very interesting property we can show that the maximum max x 1 xn of n i i d random variables with x i f b r iii x β γ 1 γ 2 parent distribution follows also a ɓriii distribution that is 5 f b r iii x β γ 1 γ 2 n f b r iii x β n γ 2 n γ 1 γ 2 this implies that if the daily streamflow can be described by a ɓriii distribution then using the ɓriii to describe annual maxima is theoretically more justified than using the g e v which assumes that the maximum is extracted from an infinitely large sample such an infinitely large sample is of course not available in practice to support the hypothesis that the ɓriii is theoretically justified we investigated its performance in describing the daily flows of the dataset used here we used the methodological approach described in papalexiou and koutsoyiannis 2016 and compared the sample l points of daily flows stratified in a daily basis with the theoretical l moments space of the ɓriii fig 2 the results show that 90 9 of the points lie within the theoretical l space of the ɓriii revealing that it is a competent model for daily flows 3 2 generic l moments parameter estimation the method of l moments greenwood et al 1979 hosking 1990 sillitto 1951 is a robust technique for fitting distributions and exploring their properties l moments of order q λ q are linear combinations of order statistics see e g hosking 1990 and have well documented advantages over the conventional product moments e g royston 1992 vogel fennessey 1993 similarly to product moments λ1 is a measure of central tendency coincides with the mean λ2 is a measure of dispersion and higher order l moments such as λ3 and λ4 relate to distributional shape properties their most important application is in identifying probability distributions that could describe observed samples e g hosking 1992 papalexiou koutsoyiannis 2013 vogel wilson 1996 vogel fennessey 1993 ye et al 2018 specifically for positively skewed samples as hosking 1990 notes l moments provide a more accurate method to identify underlying skewed distributions compared to product moments this is crucial in the analysis of maximum streamflow samples that have positive skewness and potentially heavy tails in practice heavy tails typically are manifested by sample values that appear as outliers much larger values than mean this makes the use of product moments estimates unreliable l moments is a robust fitting method but it has not been applied in general in fitting distributions with two shape parameters the main reason which also explains the infrequent use of such distributions is technical difficulties for many such distributions closed form expressions relating the parameters with l moments and thus leading to straight forward estimation do not exist in some cases for example for the ɓrxii distribution l moments can be expressed in terms of the distribution s parameters see papalexiou koutsoyiannis 2016 yet the emerging system of equations cannot be solved analytically and numerical methods have to be used here building on the results shown in papalexiou and koutsoyiannis 2016 2012 and exploiting the computational advances we propose a general numerical approach to fit any two shape parameter distribution using the method of l moments the expressions of l variation τ2 λ2 λ1 and l skewness τ3 λ3 λ2 of distributions having one scale and two shape parameters are functions only of the shape parameters thus the l moment definition integrals hosking 1990 can be written as 6 τ 2 γ 1 γ 2 λ 2 λ 1 0 1 q u 1 γ 1 γ 2 2 u 1 d u 0 1 q u 1 γ 1 γ 2 d u 7 τ 3 γ 1 γ 2 λ 3 λ 2 0 1 q u 1 γ 1 γ 2 6 u 2 6 u 1 d u 0 1 q u 1 γ 1 γ 2 2 u 1 d u where q u 1 γ1 γ2 denotes the quantile function inverse distribution function of any distribution with scale parameter β 1 by definition β is a scale parameter if f x β θ f x β 1 θ where θ is an arbitrary parameter vector expressing non scale parameters in terms of quantile functions and in this case q u β γ1 γ2 β q u 1 γ1 γ2 which leads to cancelling β from eqs 6 and 7 or else set β 1 now the two shape parameters γ1 and γ2 can be estimated by numerically minimizing the squared difference between theoretical and sample l moment ratios as 8 γ 1 γ 2 argmin γ 1 γ 2 i 2 3 τ i γ 1 γ 2 τ i 2 where τ i denote the empirical l ratios and can be estimated using the formulas given for example in hosking 1990 once γ1 and γ2 are estimated the theoretical first l moment λ1 depends solely on the scale parameter β thus it can be also numerically estimated by 9 β argmin β λ 1 β λ 1 2 with λ 1 β 0 1 q u β γ 1 γ 2 d u this process involves numerical integration and minimization yet modern computers make it feasible fast and accurate note that l moments for the ɓriii and ɓrxii can be expressed in terms of beta functions and their use could speed up the minimization yet this option is not preferred here as the aim is to offer a method as generic as possible and applicable for all distributions irrespective of existing or non existing analytical l moments expressions the g e v distribution can also be fitted using the previous approach but simple analytical expressions can be used hosking 1990 as 10 λ 1 α β γ 1 γ γ 11 λ 2 β 1 2 γ γ γ 12 τ 3 2 1 3 γ 1 2 γ 3 where γ is the gamma function the shape parameter can be numerically estimated by finding the root of eq 12 substituting τ3 with sample τ3 counterpart α and β can be explicitly estimated by the λ1 and λ2 equations 3 3 measures for evaluating fitting to compare the performance of the g e v and the two non extreme value distributions in describing the observed annual maxima streamflow samples four error measures are used papalexiou koutsoyiannis 2016 that is 13 e r i 1 n i 1 n δ x i 14 e r ii 1 m i n m 1 n δ x i 15 e r iii max δ x 1 δ x n 16 e r iv δ x n x n 100 where i is the rank of x i in the ordered sample x 1 x n and δ x i x i x i is the difference between the predicted x i and the observed x i values predicted values are estimated by using the quantile function of the fitted distribution with probability of non exceedance equal to i n 1 eri quantifies the overall fit by estimating the average absolute difference of all points is also known as the absolute error and the largest sample values are expected to contribute more erii focuses on the highest m extreme values here m 10 eriii calculates the maximum difference between the fitted and the observed values which does not necessarily occur at the sample s maximum yet it is typically observed among the sample s largest values and eriv quantifies the percentage overestimation or underestimation of the maximum observed value by the fitted distributions 4 results and discussion 4 1 tail heaviness describing streamflow maxima in most stations the annual peaks are spotted within a period smaller than three or four months yet the actual length depends on the region fig 3 this challenges the third assumption supporting the use of g e v section 3 1 by forcing the selection of the annual maximum from a limited sample size for example in the arctic tundra at the annual maximum events are observed within just one and half month during august and september in the north british colombia mountains nbcm annual maximum streamflow typically occurs between mid of june and end of july in other regions annual peaks can be observed in different seasons for example in south british colombia mountains sbcm and mackenzie district md peaks occur between may and july in the prairies pr northwestern forest nwf and northeastern forest nef peaks occur in spring and summer while in the great lakes gl and atlantic canada ac most peaks occur between mid of february and mid of may fig 3 since the assumptions supporting the g e v distribution as a standard model to describe annual peaks may be violated its selection as a standard model against other distributions should not be taken for granted for example in the analysis of extreme precipitation metastatistical approaches have been used see e g marani ignaccolo 2015 marra et al 2018 in order to relax the assumptions necessitated by the g e v here the performance of the non extreme distributions ɓriii and ɓrxii is evaluated in comparison to the g e v distribution recall that the ɓriii has the interesting property see eq 5 to describe maxima from finite samples a more realistic assumption than the one used by g e v which theoretically demands infinite size samples given that the parent distribution is also a ɓriii this does not imply that it cannot describe maxima emerging from a different parent distribution the flexibility of the ɓriii and ɓrxii might offer more robust performance compared to that of g e v we stress yet that while the mathematical formulation of g e v emerges as a limiting law assuming the maxima are extracted from samples with size tending to infinity in some cases this assumption can be relaxed or neglected for example if the parent distribution is power type then the maximum converges to g e v even if extracted from samples with very small size even just a few values the three distributions are fitted using the l moments method to the annual maxima of the 1088 stations and the behaviour of their right tail shape parameter is investigated the asymptotic behaviour of the right tail controls the frequency and the magnitude of extremes and thus is the most crucial component for extrapolating robustly streamflow at large return periods we remind that the g e v distribution with shape parameter γ 0 is a power type distribution and has the same right tail asymptotic behaviour as the ɓriii and ɓrxii for the latter two distribution the heaviness of the right tail is quantified by the parameter γ2 the analysis shows that the tail heaviness of the ɓriii and ɓrxii as quantified by their shape parameters γ2 does not match in general fig 4 a however most of the points in the form of density regions and for γ 2 b r xii 0 15 scatter approximately symmetrically over the γ 2 b r xii γ 2 b r iii line dashed line fig 4a indicating similar asymptotic behaviour this does not imply that two power type distributions having the same asymptotic tail behaviour perform the same the comparison follows in the next section the main differences are observed for γ 2 b r xii 0 15 where it is clear that the ɓrxii tails are less heavy compared to the ɓriii tails see especially the density for γ 2 b r xii close to zero in fig 4a this is also reflected by the medians of γ 2 b r iii and γ 2 b r xii which are 0 20 and 0 15 respectively the comparison however of the g e v and ɓriii right tails reveals crucial differences in the whole estimated parameter range most of the points are concentrated below the γ g e v γ 2 b r iii line fig 4b suggesting that the g e v tails are notably lighter this is reflected also by the median of the γ g e v which is just 0 05 additionally 39 1 of the sites have γ g e v 0 which implies that the g e v describing the streamflow in these locations has an upper limit at α g e v β g e v γ g e v conclusions are similar when the g e v and ɓrxii right tails are investigated fig 4c with the exception of the cloud of points observed for γ 2 b r xii 0 note that the reparameterization of the ɓrxii used here papalexiou 2018 leads for γ 2 b r xii 0 to the celebrated two parameter weibull w the w tail can be lighter or heavier than the exponential distribution but always lighter than a power type tail at least asymptotically thus the cloud of points emerging for γ 2 b r xii 0 essentially implies that weibull tails fit better than the power type tail of the ɓrxii 4 2 fitting performance and extrapolation the previous analysis reveals crucial and contradictory from a statistical viewpoint results three power type distributions having the same asymptotic behavior that is x 1 γ fitted by the same method show in practice different tail heaviness this becomes even more puzzling when the fitted distributions are compared in terms of error measures section 3 3 box plots depicting the four error measures of the fitted distributions to the observed annual maximum flows at the 1088 canadian sites show identical performance fig 5 interestingly the ɓriii ɓrxii and g e v distributions although in general are heavy tailed distributions underestimated the observed sample maximum streamflow at 76 8 77 4 and 75 9 of the sites respectively these numbers are obviously statistically indistinguishable this indicates that it is more likely that the observed maximum to be underestimated than overestimated which is a crucial issue in engineering design this is reflected also by the median of the er iv which is 7 6 for the ɓriii and ɓrxii and 7 1 for the g e v fig 5d we also investigated the goodness of fit gof for all distributions using the anderson darling test the null hypothesis that the maxima samples were drawn from the fitted distributions was rejected at 5 significance among the 1088 samples only 3 12 and 1 times for the ɓriii ɓrxii and g e v distributions respectively indicating the good fit of all distributions these surprising results naturally lead to the question of how distributions with different tail heaviness and properties perform equally with regard to fitting peak flows in order to reveal the difference we investigate the return periods of peak flows beyond the observed sample length streamflow in terms of return period in years here can be easily estimated by the relationship x t f 1 1 1 t where f 1 is the quantile function of any fitted distribution to the annual maxima two characteristic examples fig 6 showing the fitted ɓriii ɓrxii and g e v distributions to annual maxima streamflow and projecting streamflow to larger return periods reveal the difference that is for large return periods the ɓriii and ɓrxii predict notably larger streamflow than the g e v particularly the analysis shows that for t 1000 years the ɓriii ɓrxii and g e v predict the largest value in 63 1 25 2 and 11 7 of the stations respectively the results are only slightly different even when considering t 100 years as the ɓriii ɓrxii and g e v predict the largest magnitude in 57 2 24 2 and 18 7 of the stations respectively the practical implications of this finding could be of importance the previous analysis showed that all distributions underestimated the observed maximum in approximately 76 of the stations implying that even these heavy tailed distributions might also underestimate maxima corresponding to larger return periods thus it might be rational to assume that the distribution predicting higher values for large return periods underestimates less the actual future magnitudes therefore a hypothesis based on the previous arguments could be that the g e v may underestimate the streamflow beyond the observed values more than the other distributions of course we stress that this is a hypothesis that needs further exploration the flexibility of the g e v offers a good fit to the observed maxima but this does not imply that extrapolations for large return periods are robust to be fair the same argument holds for the other distributions to quantify the difference between the models predictions we calculate the percentage difference pd between predictions estimated by the non extreme value distributions ɓriii and ɓrxii and those estimated by the g e v we use return periods of 50 100 200 500 and 1000 years the percentage difference for example between the ɓriii and g e v and for return period t is pd 100 f b r iii 1 1 1 t f g e v 1 1 1 t 1 similarly it is estimated for the ɓrxii at 50 years fig 7 the prediction difference is small for example the medians show that the ɓriii and ɓrxii predict larger flood magnitude compared to the g e v by 1 1 and 0 6 respectively this is anticipated since the performance of these distributions for the observed maxima that correspond to approximately 35 years is almost identical fig 5d however this difference is markedly increased at larger return periods at 100 years which is the most popular return period in engineering practice the ɓriii predicts higher flood magnitude in 80 8 of the stations while the median shows larger predictions by 3 9 yet the significant difference is revealed at 1000 years a return period that might be used in the engineering design for large hydraulic infrastructures e g spillways in this case the ɓriii and ɓrxii based on the median values predict larger flood magnitude by 19 3 and 11 2 respectively also the ɓriii and ɓrxii predict larger values in 87 9 and 74 8 of the stations studied across canada respectively another crucial topic is related to the risk of extremes predicted by the g e v for estimated γ g e v 0 to be exceeded to clarify in 39 1 of the stations the g e v shape parameter was negative this implies that for these stations the g e v predicts an upper streamflow limit ul which can be estimated by ul α g e v β g e v γ g e v hence we can use the fitted non extreme value distributions which are unbounded to calculate the return period or risk corresponding to the upper limits predicted by the g e v distribution for the ɓriii the return period is t ul 1 f b r iii ul 1 of course the ɓrxii can be used too results show that in many stations the hypothetical upper limits predicted by the g e v correspond to return periods that in some cases are even less than 100 years based on ɓriii estimates fig 8 a the majority of g e v ul s seem to correspond to return periods ranging from 100 to 10 000 years recall that these return periods can be sometimes used in hydrological design the same results hold for the ɓrxii but in general larger return periods are predicted compared to those of ɓriii e g the high density of points is spotted for t ranging from 103 to 105 years fig 8b this analysis suggests that the ul predicted by the g e v might potentially correspond to realistic return periods or exceedance risks thus in some cases the g e v could underestimate the risk of extremes and therefore it should not be chosen blindly for flood frequency analysis 4 3 regional variation in tail heaviness the climatically homogenous regions of canada section 2 fig 1 and their unique physical features might potentially affect the behaviour of peak streamflow investigation on the season when annual maxima typically occur shows notable differences within each region fig 3 however we stress that these regions which are assumed climatically homogeneous are not necessarily homogeneous regarding streamflow peaks based on strict metrics identifying neighboring stations that form homogeneous clusters based for example on l moment metrics proposed by hosking and wallis 2005 is out of the scope of this study we deem yet that it is still valuable to study and reveal potential variations in the tail behaviour in these climatic regions given their significance for canada stratifying the estimated shape parameters that control the heaviness of the right tail by region and computing the median values reveals marked differences among the regions and among the three distributions table 1 the average value of the three medians 5th row in table 1 and the ranking of the regions based on it 6th row show the prairies pr northwestern forests nwf and atlantic canada ac as having the heaviest tails a second group with moderate tail heaviness includes the south bc mountains sbcm great lakes gl mackenzie district md and north bc mountains nbcm regions with lowest tail heaviness are the arctic tundra at pacific coast pc and northeastern forest nef in some cases plausible reasoning exists for the observed regional differences for example the topographies of the pr and nwf are dominated by ponds and lakes respectively that reduce the mean flows however when ponds and lakes are connected fill and spill mechanism spence woo 2003 basins can generate very large peaks that might be triggered unexpectedly and not necessarily concurrent with heavy precipitation shook et al 2015 furthermore in the semi arid prairies where runoff coefficient is typically low short duration convective storms can cause large instantaneous peak flows and large scale frontal systems can generate massive amounts of precipitation especially when combined with snow melt dumanski et al 2015 leading to extreme peaks and heavy tails the heavy tails in ac might have a different origin that is the atlantic coast is notorious for its intense storms and hurricanes which can cause extreme flooding events e g berg avila 2010 thompson et al 2009 on the other hand lighter tails of the distribution in pc nbcm and nef might be attributed to the fact that these regions are wetter with relatively high mean annual runoff values leading to a less positively skewed flow distribution the fact is that in canada several different processes can affect flooding and the nature of the peaks including extreme rainfall snowmelt rain on snow groundwater ice jams storm surges etc see buttle et al 2016 and references therein additionally other factors such as catchment size aridity index could affect in general stream flow characteristics see e g markonis et al 2018 a detailed investigation on which processes and why lead to heavy or less heavy tails is very challenging and out of the scope of this study the median values of the shape parameters vary regionally but also differ significantly among the three distributions a more detailed investigation of the right tail shape parameter in the form of box plots fig 9 shows not only differences in the median values but also in the variance of the estimated parameters the smaller the variance the more robust the results thus in this case the ɓriii seem to perform better than the g e v this fact can also be verified with monte carlo simulations and synthetic series for example if we generate synthetic time series with a parent marginal distribution resembling streamflow time series were generated using the cosmos r package available at cran papalexiou et al 2019 such as a bell shapedɓriii or ɓrxii then we extract the maxima and fit the three distributions tested here we observe that the standard deviation of the tail estimate in the g e v is more than double compared to the other distributions thus we deem the g e v performance is not ideal with the box plots indicating shape estimates varying in very large ranges within the regions studied for example for the sbcm the interquartile range for the ɓriii ɓrxii and g e v is 0 09 0 11 and 0 24 while the 95 empirical interval outer fences in the box plots is 0 25 0 35 and 0 74 respectively on average actual values depend on the region the standard deviation of the g e v shape parameter estimates is larger by 147 8 and 70 6 compared with the ɓriii and ɓrxii respectively another finding is that the g e v in most regions and sites predicts streamflow having upper limit exceptions are the ac nwf and pr regions where g e v is unbounded for most of the stations fig 9 the nature of streamflow and the well documented flooding peaks that in many cases are two orders of magnitude larger than the mean streamflow might be an indication of heavy tails this in combination with previous results showing that negative g e v shape parameters are artefacts for precipitation maxima papalexiou koutsoyiannis 2013 and with the results presented in this study fig 8 challenge the tail behaviour as quantified by the g e v especially when the fitted g e v suggests an upper bound we believe that this behaviour might be an artefact that can also be verified by monte carlo simulations for example we use a bell shaped ɓriii 100 2 0 1 distribution to generate 50 years of daily flows independent or correlated see papalexiou 2018 papalexiou et al 2018 and papalexiou and serinaldi 2020 on how to generate non gaussian time series with any distribution and correlation and extract maxima from blocks of 50 values assuming maxima occur within 50 days we repeat this process and form 10 000 samples of annual maxima fitting the g e v in each sample of maxima we observe that approximately 40 of times its shape parameter is negative for the independent case in the presence of autocorrelation for example assuming lag 1 equal to 0 6 this percentage increases to 48 while in both cases the true value is 0 1 the structure of g e v allows negative shape parameters while the location parameter offers additional flexibility which however is not theoretically consistent with the nature of streamflow recall that for γ 0 and 0 the g e v is defined in α β γ and α β γ respectively while the potential range of streamflow is 0 this flexibility allows good fit in the observed data yet this does not imply that the model s parameters are consistent with the nature of streamflow the previous analysis indicates that some of the climatic regions might have different tail behaviour but these regions are not necessarily homogeneous regarding streamflow peaks we applied a well known regional heterogeneity test by hosking and wallis 2005 and as anticipated these vast regions do not pass such a strict test regional homogeneity in this test is assessed by evaluating the stations l variation here we assess the tail behaviour by grouping the stations based on their sample l variation aiming to mimic regional analyses clearly the stations in each group are not necessarily neighbouring but have similar l variation values particularly we 1 sorted the 1088 stations based on their sample l variation 2 discarded the first and last 44 stations to avoid potential outliers and formed 10 groups g1 g10 using the remaining 1000 stations 100 stations each group the l variation range in each group is narrow fig 10 and it is assumed that the stations in each group are homogeneous in terms of l variation 3 the estimated right tail shape parameter of the ɓriii ɓrxii and g e v distributions for each group are shown as box plots fig 10 each box plots is constructed from 100 values 4 we merged the time series of each group after standardizing with their mean merged sample size for each group 3490 values and estimated the tail parameter dots in fig 10 the results indicate a large difference between the tails of the g e v and the ɓriii and ɓrxii specifically for low l variation values either the median of each group or the estimates of the merged sample almost coincide in most groups show that the g e v is bounded from above while the ɓriii and ɓrxii are not note that the results do not differ even if 20 groups are considered resulting in much smaller l variation range for each group the results also do not change if the standardization before the merging is done in terms of standard deviation dividing with standard deviation it is interesting to note that for large l variations groups g8 10 the ɓrxii shows decreasing values for the tail this is statistically possible as the increase in l variation does not imply analogous increase in l skewness and thus such τ2 τ3 points can lie in the lower part of the theoretical l space of the ɓrxii distribution resulting in less heavy right tails see the l space of ɓrxii in papalexiou and koutsoyiannis 2016 5 summary and conclusions analysis of streamflow peaks is the cornerstone of statistical hydrology and crucial in engineering practice the generalized extreme value g e v distribution has been the prevailing model in describing streamflow extremes for more than half a century its extensive use is supported by theoretical arguments it emerges as limiting law describing maxima and yet these arguments are not always consistent in real world cases other than the g e v other three parameter models have been used that typically comprise a location a scale and a shape parameter here we revisit flood peak distributions and demonstrate the framework through a pan canadian investigation we provide theoretical arguments on why some assumptions used by the g e v might be challenged in real world cases and we suggest consistent and flexible alternative distributions that have been neglected in describing flood peaks particularly we use the burr type iii ɓriii and xii ɓrxii distributions as alternative models and compare their performance with that of the g e v these distributions are defined for any positive value and thus are consistent with streamflow s potential range their two shape parameters allow to fully control the left and right tails providing flexibility to describe samples that differ markedly with each other the study provides theoretical arguments that justify specifically the ɓriii distribution to describe annual maxima the performance of these distributions is tested in more than 1000 records of annual maximum streamflow across canada that are representative of different climatic and topographic conditions the distributions are fitted using the method of l moments based on a generic algorithm that we devised and can be applied for any distribution even those that do not have analytical l moment expressions this extensive pan canadian analysis reveals 1 identical performance based on several error measures and the anderson darling gof test of the ɓriii ɓrxii and g e v distributions in describing the observed streamflow annual maxima fig 5 2 notable differences in the heaviness of the right tail that controls the frequency and the magnitude of extremes these models have theoretically the same power type tail and yet despite the identical performance in the observed samples the estimated shape parameters show in general heavier tails for the ɓriii followed by the ɓrxii and finally by the g e v fig 4 and fig 6 3 the previous finding might be important for estimating streamflow peaks for large return periods used in engineering practice for example the ɓriii and ɓrxii predict respectively 19 3 and 11 2 median values larger streamflow at the 1000 year return period compared to the g e v fig 7 this might be applicable to mega projects such as large dams 4 in 39 1 of the stations the g e v predicts an upper limit for streamflow maxima this might be inconsistent with the nature of streamflow it is controversial that distributions used to describe daily streamflow or precipitation in the literature are never bounded from above but distributions used for the annual maxima can have an upper bound here the analysis of these upper limits based on the other fitted distributions that do not have such limitations indicates that these upper limits might have realistic exceedance probabilities fig 8 5 the estimation of the right tail shape parameters critical component for extrapolating at large return periods is more robust for the ɓriii and ɓrxiii distributions the standard deviation of the g e v shape parameter estimates is larger on average by 147 8 and 70 6 compared to the ɓriii and ɓrxii cases respectively fig 9 this suggests that the tail heaviness might not be estimated reliably by the g e v in some cases in turn this could affect the accuracy of extrapolated flood magnitudes 6 regional variation across the climatic regions of canada is observed the tail heaviness as quantified by the right tail shape parameter shows the prairies northwestern forests and atlantic canada as having the heavier tails while less heavy tails are observed for the arctic tundra pacific coast and northeastern forests fig 9 the previous findings suggest that power type distributions with one scale and two shape parameters such as the ɓriii and ɓrxii distributions offer robust and consistent alternatives for the analysis of streamflow maxima credit authorship contribution statement mohanad zaghloul data curation formal analysis methodology visualization writing original draft simon michael papalexiou conceptualization formal analysis methodology supervision writing original draft amin elshorbagy conceptualization funding acquisition supervision writing review editing paulin coulibaly writing review editing declaration of competing interest none acknowledgements this research is financially supported by the integrated modelling program for canada impc project under the umbrella of the global water futures gwf program at the university of saskatchewan canada s m p acknowledges the support of the natural sciences and engineering research council of canada nserc discovery grant rgpin 2019 06894 in addition m z acknowledges the department of civil geological and environmental engineering for the financial support of research through the departmental devolved scholarship the data used in this study are freely available at http collaboration cmc ec gc ca cmc hydrometrics www finally we thank the associate editor and the three reviewers for their comprehensive remarks and criticisms that helped improve markedly the original manuscript 
397,safe and cost effective design of infrastructures such as dams bridges highways often requires knowing the magnitude and frequency of peak floods the generalized extreme value distribution g e v prevailed in flood frequency analysis along with distributions comprising location scale and shape parameters here we explore alternative models and propose power type models having one scale and two shape parameters the burr type iii ɓriii and xii ɓrxii distributions are compared against the g e v in 1088 streamflow records of annual peaks across canada a generic l moment algorithm is devised to fit the distributions also applicable to distributions without analytical l moment expressions the analysis shows 1 the models perform equally well when describing the observed annual peaks 2 the right tail appears heavier in the ɓriii and ɓrxii models leading to larger streamflow predictions when compared to those of g e v 3 the g e v predicts upper streamflow limits in 39 1 of the records these limits have realistic exceedance probabilities based on the other two models 4 the tail heaviness estimation seems not robust in the g e v case when compared to the ɓriii and ɓrxii models and this could challenge g e v s reliability in predicting streamflow at large return periods and 5 regional variation is observed in the behaviour of flood peaks across different climatic regions of canada the findings of this study reveal potential limitations in using the g e v for flood frequency analysis and suggest the ɓriii and ɓrxii as consistent alternatives worth exploring keywords flood peaks extreme value theory gev distribution burr type iii distribution 1 introduction estimation of streamflow magnitude at large return periods is essential in engineering practice and has applications in infrastructure design water resources management flood risk estimation etc the typical procedure uses historical streamflow records to assess and fit appropriate probability distributions the reliability of streamflow estimates depends on the suitability of the selected distribution suitability does not necessarily imply just good fitting to the observed data many different distributions may provide this the selected model should be also theoretically consistent and in agreement with the broader knowledge we have on the probabilistic behaviour of a process for example exponential and power type distributions might fit equally well to observations in a specific streamflow record yet power type models heavier tails predict larger streamflow magnitude for large return periods if the regional knowledge of the process under study indicates a specific probabilistic behaviour then it should be considered in the selection of the model even if a different type of model fits equally well or even better the literature is rich in studies assessing and suggesting distributions for streamflow analysis in various regions of the world the most popular probability distribution models are the generalized extreme value g e v the three parameter lognormal l n 3 and the three parameter log pearson l p 3 however many more have been used including the two parameter lognormal l n two component extreme value distribution t c e v generalized logistic g l o pearson type iii p 3 etc for example arnell gabriele 1988 tested the t c e v g e v and wakeby w a distributions in the united kingdom and italy pearson 1991 described annual maxima in south canterbury new zealand with the fréchet distribution g e v with positive shape parameter vogel et al 1993 analysed flood data from 383 sites in the southwestern u s using l moment diagrams for model selection and showed that the g e v l p 3 l n and l n 3 describe well the flows in this region rao hamed 1994 suggested the g e v and l n 3 distributions for annual maxima at cauvery river in india önöz bayazit 1995 fitted and compared the l n l n 3 p 3 and l p 3 in large samples worldwide karim chowdhury 1995 indicated the g e v as the best model for streamflow in bangladesh after testing the l n gumbel g υ l p 3 and g e v vogel wilson 1996 based on l moment ratio diagrams investigated the performance of the g e v and l p 3 distributions for the annual maxima in the united states in more than 1455 sites kumar et al 2003 used the g e v distribution for annual maxima in the middle ganga plains in india ferro porto 2006 used the t c e v in sicily italy saf 2009 described annual maxima in the west mediterranean region in turkey using the p 3 and g l o distributions villarini and smith 2010 and villarini et al 2011 investigated annual peaks in eastern midewest united states using the g e v and mixtures of flood peak distributions kochanek et al 2014 recommended the use of g e v for annual maxima in france but stressed the fitting should be based on regional estimates salinas et al 2014 assessed the performance of g e v g l o l n 3 and p 3 in europe implying that the g e v cannot describe samples with medium and high skewness rutkowska et al 2017 and hassan et al 2019 proposed different distributions for different regions for poland and the torne riven in sweden respectively and recently miniussi et al 2020 used the metastatistical extreme value distribution m e v to interpret streamflow peaks observed across the continental united states in canada the intense topographic and climate variations mahmood et al 2017 make identifying a single distribution for streamflow frequency analysis in all regions a challenging task such variations result in streamflow with different characteristics buttle et al 2016 gullett skinner 1992 in different hydrographic regions in canada generally evidence shows that flood generation mechanisms vary regionally and seasonally pomeroy et al 2016 shook 2015 stadnyk et al 2016 yue wang 2004a 2004b also emphasized the importance of selecting an appropriate distribution for streamflow based on the regional climatic conditions previous studies focusing on canada used and tested several distributions to describe annual streamflow peaks for example the l n was proposed for the prairies spence 1972 the g e v was used in nova scotia pilon adamowski 1992 southern ontario glaves waylen 1997 and quebec yue et al 1999 the g e v l n 3 and l p 3 were used in several canadian regions yue wang 2004a and recently the g e v was proposed as a suitable model across canada zhang et al 2019 the previous literature review reveals 1 many different distributions have been proposed and used to describe streamflow peaks 2 the prevailing model is the g e v distribution and 3 the most well performing models are three parameter models that include one location one scale and one shape parameter exception is l p 3 with two shape and one scale parameter but the parameters interact in a complex way all these three parameter distributions are not consistent with the potential range of streamflow peaks streamflow can have any positive value and these models have lower bounds that can be even negative or worse upper bounds additionally a single one shape parameter model cannot reproduce in general the different distribution shapes that are observed in reality which led to testing many different models in order to find the best fit one this approach offers ad hoc choices that conceptually are not appealing for example it is scientifically sound to assume that streamflow peaks in homogenous regions can be described by the same probability law however fitting many different models might result in suggesting structurally different models for different sites within the same region of course this can be due to sample variations and leads to algorithmic treatment of probability laws this study revisits flood peak distributions by proposing structurally different models than those that have been typically used in the literature that is power type distributions defined in the positive real axis and comprising one scale and two shape parameters it aims in finding a single and consistent distribution to describe peaks in any region in canada consistent distributions should be able to describe the exceedance probability of any positive streamflow value and be parsimonious yet flexible enough to be used in any region two such models that are explored here are the burr type iii ɓriii burr 1942 papalexiou koutsoyiannis 2012 and burr type xii ɓrxii burr 1942 papalexiou 2018 other structurally similar models exist such as a reparameterization and generalization of the beta of the second kind b ii which was proposed recently by papalexiou serinaldi 2020 these models which have been neglected in hydrologic practice are tested against the g e v distribution using annual peaks from 1088 gauged sites across canada the significant climatic and topographic variability found in canada and the large number of available records offer an ideal place to stress test these models the theoretical arguments we provide indicate that such models especially the ɓriii distribution can be a suitable choice for annual peaks the empirical evidence of the analysis suggest excellent performance of the ɓriii model and reveals potential limitations for the g e v distribution 2 streamflow data the hydat database of environment and climate change canada eccc version 1 0 http collaboration cmc ec gc ca cmc hydrometrics www provides historical daily average streamflow records at 6371 sites in canada to ensure the quality of the data we used sites with natural flow unregulated by any weir levee dam etc and selected those having daily records of more than 20 years over the 1979 2016 period this screening resulted in 1119 sites also 31 more sites were eliminated as their streamflow time series showed suspicious trends and abrupt changes the final 1088 sites are spread all over canada with the higher density of sites observed in the south fig 1 the climatic classification of canada identifies 11 unique climatic regions based on meteorology topography altitude land exposure existence of large water bodies and other features that affect the hydrology of the region gullett skinner 1992 each region has a unique climate pattern and hence can potentially affect peak streamflow characteristics gullett skinner 1992 the selected 1088 hydat sites overlay 10 of these regions as presented in fig 1 the spatial density of the sites differs among the regions and generally is decreasing from south to north the number of sites in these regions varies from 196 in the canadian prairies pr to 13 sites in the arctic tundra at fig 1 the arctic mountains region does not include any sites 3 methods 3 1 probability distributions for streamflow extremes the extreme value theory evt focusing on block maxima is the prevailing method in engineering practice for flood frequency analysis the origins of evt date back in the works of fréchet 1927 fisher and tippett 1928 and gnedenko 1943 while gumbel 1958 applied and popularized such methods in engineering practice classical evt is derived based on the consideration that the maximum of a random sample of independent and identically distributed i i d random variables r v s with xi fx x parent distribution has distribution function 1 g x f x n in layman s terms and according to the fisher tippett gnedenko theorem if n and depending on the form of the parent distribution f x e g power type exponential etc the g x distribution converges either to the gumbel the fréchet or the reversed weibull distributions see e g papalexiou koutsoyiannis 2013 these three distributions can be unified in a single expression jenkinson 1955 von mises 1936 the so called generalized extreme value g e v distribution with cumulative distribution function 2 f g e v x α β γ exp 1 γ x α β 1 γ where 1 γ x α β 0 α β and γ denote respectively the location scale and shape parameters the g e v for γ 0 is defined in α β γ and thus it has an upper limit for γ 0 is defined in α β γ while for γ 0 it simplifies to the gumbel distribution that is f g u x α β exp exp x α β the g e v distribution for γ 0 despite the exponential function found in its expression is power type distribution heavy tail having exceedance probabilities with asymptotic behaviour p x x x 1 γ although the g e v distribution has been the standard choice in flood frequency analysis the assumptions that justify g e v as a theoretically consistent model are not necessarily valid for streamflow maxima as 1 a typical hydrograph includes rising and recession limbs and thus the r v s describing the daily flows cannot be considered identical equally streamflow series are not identically distributed because they exhibit clear deterministic seasonal behaviour 2 daily flows are highly autocorrelated thus the r v s are not independent and 3 the maximum is not extracted from a sample of very large size thus convergence is not guaranteed of course these arguments are valid for other hydroclimatic variables too in practice apart from the g e v many other distributions have been used to describe annual streamflow maxima see references in the introduction the number of well known distributions in the literature that could be tested as alternative distributions to g e v is vast see e g johnson et al 1994 1995 yet selecting and evaluating their performance without any justification can be subjective and from a practical viewpoint not efficient here two non extreme value distributions are tested that 1 are defined in 0 and thus are consistent with the potential range of extreme streamflow 2 are parsimonious but flexible that is they comprise one scale parameter and two shape parameters to control the left and the right tails 3 have analytical and simple distribution and quantile expressions and 4 have the same asymptotic behaviour as the g e v that is are power type distribution having the potential to model heavy tails the last point is crucial as the nature of streamflow extremes shows evidence of heavy tails this does not imply yet that heavy tails should be taken for granted everywhere identifying the tail type is not trivial and many methods have been invented see e g el adlouni et al 2008 nerantzaki papalexiou 2019 wietzke et al 2020 the alternative models tested are the burr type iii ɓriii and type xii ɓrxii from the burr system burr 1942 using the generalizations and reparameterizations proposed in papalexiou 2018 and papalexiou and koutsoyiannis 2012 respectively these distributions have been neglected in streamflow frequency analysis and only recently have been used for precipitation or suggested as generic models for hydrometeorological variables papalexiou 2018 the cumulative distribution functions of the ɓriii and ɓrxii are given by 3 f b r iii x β γ 1 γ 2 1 1 γ 1 x β 1 γ 2 γ 1 γ 2 4 f b r xii x β γ 1 γ 2 1 1 γ 2 x β γ 1 1 γ 1 γ 2 where β is scale parameter and γ1 and γ2 are shape parameters controlling respectively the left and the right tail the asymptotic behaviour of both distributions is x 1 γ 2 and thus equivalent with that of the g e v note that well known two parameter distributions emerge as special cases of these distributions for specific parameter values specifically the ɓrxii for γ1 1 becomes the pareto ii p ii for γ2 0 limiting case collapses to the weibull distribution w and for γ1 1 and γ2 0 becomes the celebrated exponential distribution the ɓriii for γ2 1 γ1 coincides with the log logistic distribution and for γ1 becomes the inverse weibull distribution from a mathematical viewpoint these two distributions are not limiting extreme value distributions yet the ɓriii has a very interesting property we can show that the maximum max x 1 xn of n i i d random variables with x i f b r iii x β γ 1 γ 2 parent distribution follows also a ɓriii distribution that is 5 f b r iii x β γ 1 γ 2 n f b r iii x β n γ 2 n γ 1 γ 2 this implies that if the daily streamflow can be described by a ɓriii distribution then using the ɓriii to describe annual maxima is theoretically more justified than using the g e v which assumes that the maximum is extracted from an infinitely large sample such an infinitely large sample is of course not available in practice to support the hypothesis that the ɓriii is theoretically justified we investigated its performance in describing the daily flows of the dataset used here we used the methodological approach described in papalexiou and koutsoyiannis 2016 and compared the sample l points of daily flows stratified in a daily basis with the theoretical l moments space of the ɓriii fig 2 the results show that 90 9 of the points lie within the theoretical l space of the ɓriii revealing that it is a competent model for daily flows 3 2 generic l moments parameter estimation the method of l moments greenwood et al 1979 hosking 1990 sillitto 1951 is a robust technique for fitting distributions and exploring their properties l moments of order q λ q are linear combinations of order statistics see e g hosking 1990 and have well documented advantages over the conventional product moments e g royston 1992 vogel fennessey 1993 similarly to product moments λ1 is a measure of central tendency coincides with the mean λ2 is a measure of dispersion and higher order l moments such as λ3 and λ4 relate to distributional shape properties their most important application is in identifying probability distributions that could describe observed samples e g hosking 1992 papalexiou koutsoyiannis 2013 vogel wilson 1996 vogel fennessey 1993 ye et al 2018 specifically for positively skewed samples as hosking 1990 notes l moments provide a more accurate method to identify underlying skewed distributions compared to product moments this is crucial in the analysis of maximum streamflow samples that have positive skewness and potentially heavy tails in practice heavy tails typically are manifested by sample values that appear as outliers much larger values than mean this makes the use of product moments estimates unreliable l moments is a robust fitting method but it has not been applied in general in fitting distributions with two shape parameters the main reason which also explains the infrequent use of such distributions is technical difficulties for many such distributions closed form expressions relating the parameters with l moments and thus leading to straight forward estimation do not exist in some cases for example for the ɓrxii distribution l moments can be expressed in terms of the distribution s parameters see papalexiou koutsoyiannis 2016 yet the emerging system of equations cannot be solved analytically and numerical methods have to be used here building on the results shown in papalexiou and koutsoyiannis 2016 2012 and exploiting the computational advances we propose a general numerical approach to fit any two shape parameter distribution using the method of l moments the expressions of l variation τ2 λ2 λ1 and l skewness τ3 λ3 λ2 of distributions having one scale and two shape parameters are functions only of the shape parameters thus the l moment definition integrals hosking 1990 can be written as 6 τ 2 γ 1 γ 2 λ 2 λ 1 0 1 q u 1 γ 1 γ 2 2 u 1 d u 0 1 q u 1 γ 1 γ 2 d u 7 τ 3 γ 1 γ 2 λ 3 λ 2 0 1 q u 1 γ 1 γ 2 6 u 2 6 u 1 d u 0 1 q u 1 γ 1 γ 2 2 u 1 d u where q u 1 γ1 γ2 denotes the quantile function inverse distribution function of any distribution with scale parameter β 1 by definition β is a scale parameter if f x β θ f x β 1 θ where θ is an arbitrary parameter vector expressing non scale parameters in terms of quantile functions and in this case q u β γ1 γ2 β q u 1 γ1 γ2 which leads to cancelling β from eqs 6 and 7 or else set β 1 now the two shape parameters γ1 and γ2 can be estimated by numerically minimizing the squared difference between theoretical and sample l moment ratios as 8 γ 1 γ 2 argmin γ 1 γ 2 i 2 3 τ i γ 1 γ 2 τ i 2 where τ i denote the empirical l ratios and can be estimated using the formulas given for example in hosking 1990 once γ1 and γ2 are estimated the theoretical first l moment λ1 depends solely on the scale parameter β thus it can be also numerically estimated by 9 β argmin β λ 1 β λ 1 2 with λ 1 β 0 1 q u β γ 1 γ 2 d u this process involves numerical integration and minimization yet modern computers make it feasible fast and accurate note that l moments for the ɓriii and ɓrxii can be expressed in terms of beta functions and their use could speed up the minimization yet this option is not preferred here as the aim is to offer a method as generic as possible and applicable for all distributions irrespective of existing or non existing analytical l moments expressions the g e v distribution can also be fitted using the previous approach but simple analytical expressions can be used hosking 1990 as 10 λ 1 α β γ 1 γ γ 11 λ 2 β 1 2 γ γ γ 12 τ 3 2 1 3 γ 1 2 γ 3 where γ is the gamma function the shape parameter can be numerically estimated by finding the root of eq 12 substituting τ3 with sample τ3 counterpart α and β can be explicitly estimated by the λ1 and λ2 equations 3 3 measures for evaluating fitting to compare the performance of the g e v and the two non extreme value distributions in describing the observed annual maxima streamflow samples four error measures are used papalexiou koutsoyiannis 2016 that is 13 e r i 1 n i 1 n δ x i 14 e r ii 1 m i n m 1 n δ x i 15 e r iii max δ x 1 δ x n 16 e r iv δ x n x n 100 where i is the rank of x i in the ordered sample x 1 x n and δ x i x i x i is the difference between the predicted x i and the observed x i values predicted values are estimated by using the quantile function of the fitted distribution with probability of non exceedance equal to i n 1 eri quantifies the overall fit by estimating the average absolute difference of all points is also known as the absolute error and the largest sample values are expected to contribute more erii focuses on the highest m extreme values here m 10 eriii calculates the maximum difference between the fitted and the observed values which does not necessarily occur at the sample s maximum yet it is typically observed among the sample s largest values and eriv quantifies the percentage overestimation or underestimation of the maximum observed value by the fitted distributions 4 results and discussion 4 1 tail heaviness describing streamflow maxima in most stations the annual peaks are spotted within a period smaller than three or four months yet the actual length depends on the region fig 3 this challenges the third assumption supporting the use of g e v section 3 1 by forcing the selection of the annual maximum from a limited sample size for example in the arctic tundra at the annual maximum events are observed within just one and half month during august and september in the north british colombia mountains nbcm annual maximum streamflow typically occurs between mid of june and end of july in other regions annual peaks can be observed in different seasons for example in south british colombia mountains sbcm and mackenzie district md peaks occur between may and july in the prairies pr northwestern forest nwf and northeastern forest nef peaks occur in spring and summer while in the great lakes gl and atlantic canada ac most peaks occur between mid of february and mid of may fig 3 since the assumptions supporting the g e v distribution as a standard model to describe annual peaks may be violated its selection as a standard model against other distributions should not be taken for granted for example in the analysis of extreme precipitation metastatistical approaches have been used see e g marani ignaccolo 2015 marra et al 2018 in order to relax the assumptions necessitated by the g e v here the performance of the non extreme distributions ɓriii and ɓrxii is evaluated in comparison to the g e v distribution recall that the ɓriii has the interesting property see eq 5 to describe maxima from finite samples a more realistic assumption than the one used by g e v which theoretically demands infinite size samples given that the parent distribution is also a ɓriii this does not imply that it cannot describe maxima emerging from a different parent distribution the flexibility of the ɓriii and ɓrxii might offer more robust performance compared to that of g e v we stress yet that while the mathematical formulation of g e v emerges as a limiting law assuming the maxima are extracted from samples with size tending to infinity in some cases this assumption can be relaxed or neglected for example if the parent distribution is power type then the maximum converges to g e v even if extracted from samples with very small size even just a few values the three distributions are fitted using the l moments method to the annual maxima of the 1088 stations and the behaviour of their right tail shape parameter is investigated the asymptotic behaviour of the right tail controls the frequency and the magnitude of extremes and thus is the most crucial component for extrapolating robustly streamflow at large return periods we remind that the g e v distribution with shape parameter γ 0 is a power type distribution and has the same right tail asymptotic behaviour as the ɓriii and ɓrxii for the latter two distribution the heaviness of the right tail is quantified by the parameter γ2 the analysis shows that the tail heaviness of the ɓriii and ɓrxii as quantified by their shape parameters γ2 does not match in general fig 4 a however most of the points in the form of density regions and for γ 2 b r xii 0 15 scatter approximately symmetrically over the γ 2 b r xii γ 2 b r iii line dashed line fig 4a indicating similar asymptotic behaviour this does not imply that two power type distributions having the same asymptotic tail behaviour perform the same the comparison follows in the next section the main differences are observed for γ 2 b r xii 0 15 where it is clear that the ɓrxii tails are less heavy compared to the ɓriii tails see especially the density for γ 2 b r xii close to zero in fig 4a this is also reflected by the medians of γ 2 b r iii and γ 2 b r xii which are 0 20 and 0 15 respectively the comparison however of the g e v and ɓriii right tails reveals crucial differences in the whole estimated parameter range most of the points are concentrated below the γ g e v γ 2 b r iii line fig 4b suggesting that the g e v tails are notably lighter this is reflected also by the median of the γ g e v which is just 0 05 additionally 39 1 of the sites have γ g e v 0 which implies that the g e v describing the streamflow in these locations has an upper limit at α g e v β g e v γ g e v conclusions are similar when the g e v and ɓrxii right tails are investigated fig 4c with the exception of the cloud of points observed for γ 2 b r xii 0 note that the reparameterization of the ɓrxii used here papalexiou 2018 leads for γ 2 b r xii 0 to the celebrated two parameter weibull w the w tail can be lighter or heavier than the exponential distribution but always lighter than a power type tail at least asymptotically thus the cloud of points emerging for γ 2 b r xii 0 essentially implies that weibull tails fit better than the power type tail of the ɓrxii 4 2 fitting performance and extrapolation the previous analysis reveals crucial and contradictory from a statistical viewpoint results three power type distributions having the same asymptotic behavior that is x 1 γ fitted by the same method show in practice different tail heaviness this becomes even more puzzling when the fitted distributions are compared in terms of error measures section 3 3 box plots depicting the four error measures of the fitted distributions to the observed annual maximum flows at the 1088 canadian sites show identical performance fig 5 interestingly the ɓriii ɓrxii and g e v distributions although in general are heavy tailed distributions underestimated the observed sample maximum streamflow at 76 8 77 4 and 75 9 of the sites respectively these numbers are obviously statistically indistinguishable this indicates that it is more likely that the observed maximum to be underestimated than overestimated which is a crucial issue in engineering design this is reflected also by the median of the er iv which is 7 6 for the ɓriii and ɓrxii and 7 1 for the g e v fig 5d we also investigated the goodness of fit gof for all distributions using the anderson darling test the null hypothesis that the maxima samples were drawn from the fitted distributions was rejected at 5 significance among the 1088 samples only 3 12 and 1 times for the ɓriii ɓrxii and g e v distributions respectively indicating the good fit of all distributions these surprising results naturally lead to the question of how distributions with different tail heaviness and properties perform equally with regard to fitting peak flows in order to reveal the difference we investigate the return periods of peak flows beyond the observed sample length streamflow in terms of return period in years here can be easily estimated by the relationship x t f 1 1 1 t where f 1 is the quantile function of any fitted distribution to the annual maxima two characteristic examples fig 6 showing the fitted ɓriii ɓrxii and g e v distributions to annual maxima streamflow and projecting streamflow to larger return periods reveal the difference that is for large return periods the ɓriii and ɓrxii predict notably larger streamflow than the g e v particularly the analysis shows that for t 1000 years the ɓriii ɓrxii and g e v predict the largest value in 63 1 25 2 and 11 7 of the stations respectively the results are only slightly different even when considering t 100 years as the ɓriii ɓrxii and g e v predict the largest magnitude in 57 2 24 2 and 18 7 of the stations respectively the practical implications of this finding could be of importance the previous analysis showed that all distributions underestimated the observed maximum in approximately 76 of the stations implying that even these heavy tailed distributions might also underestimate maxima corresponding to larger return periods thus it might be rational to assume that the distribution predicting higher values for large return periods underestimates less the actual future magnitudes therefore a hypothesis based on the previous arguments could be that the g e v may underestimate the streamflow beyond the observed values more than the other distributions of course we stress that this is a hypothesis that needs further exploration the flexibility of the g e v offers a good fit to the observed maxima but this does not imply that extrapolations for large return periods are robust to be fair the same argument holds for the other distributions to quantify the difference between the models predictions we calculate the percentage difference pd between predictions estimated by the non extreme value distributions ɓriii and ɓrxii and those estimated by the g e v we use return periods of 50 100 200 500 and 1000 years the percentage difference for example between the ɓriii and g e v and for return period t is pd 100 f b r iii 1 1 1 t f g e v 1 1 1 t 1 similarly it is estimated for the ɓrxii at 50 years fig 7 the prediction difference is small for example the medians show that the ɓriii and ɓrxii predict larger flood magnitude compared to the g e v by 1 1 and 0 6 respectively this is anticipated since the performance of these distributions for the observed maxima that correspond to approximately 35 years is almost identical fig 5d however this difference is markedly increased at larger return periods at 100 years which is the most popular return period in engineering practice the ɓriii predicts higher flood magnitude in 80 8 of the stations while the median shows larger predictions by 3 9 yet the significant difference is revealed at 1000 years a return period that might be used in the engineering design for large hydraulic infrastructures e g spillways in this case the ɓriii and ɓrxii based on the median values predict larger flood magnitude by 19 3 and 11 2 respectively also the ɓriii and ɓrxii predict larger values in 87 9 and 74 8 of the stations studied across canada respectively another crucial topic is related to the risk of extremes predicted by the g e v for estimated γ g e v 0 to be exceeded to clarify in 39 1 of the stations the g e v shape parameter was negative this implies that for these stations the g e v predicts an upper streamflow limit ul which can be estimated by ul α g e v β g e v γ g e v hence we can use the fitted non extreme value distributions which are unbounded to calculate the return period or risk corresponding to the upper limits predicted by the g e v distribution for the ɓriii the return period is t ul 1 f b r iii ul 1 of course the ɓrxii can be used too results show that in many stations the hypothetical upper limits predicted by the g e v correspond to return periods that in some cases are even less than 100 years based on ɓriii estimates fig 8 a the majority of g e v ul s seem to correspond to return periods ranging from 100 to 10 000 years recall that these return periods can be sometimes used in hydrological design the same results hold for the ɓrxii but in general larger return periods are predicted compared to those of ɓriii e g the high density of points is spotted for t ranging from 103 to 105 years fig 8b this analysis suggests that the ul predicted by the g e v might potentially correspond to realistic return periods or exceedance risks thus in some cases the g e v could underestimate the risk of extremes and therefore it should not be chosen blindly for flood frequency analysis 4 3 regional variation in tail heaviness the climatically homogenous regions of canada section 2 fig 1 and their unique physical features might potentially affect the behaviour of peak streamflow investigation on the season when annual maxima typically occur shows notable differences within each region fig 3 however we stress that these regions which are assumed climatically homogeneous are not necessarily homogeneous regarding streamflow peaks based on strict metrics identifying neighboring stations that form homogeneous clusters based for example on l moment metrics proposed by hosking and wallis 2005 is out of the scope of this study we deem yet that it is still valuable to study and reveal potential variations in the tail behaviour in these climatic regions given their significance for canada stratifying the estimated shape parameters that control the heaviness of the right tail by region and computing the median values reveals marked differences among the regions and among the three distributions table 1 the average value of the three medians 5th row in table 1 and the ranking of the regions based on it 6th row show the prairies pr northwestern forests nwf and atlantic canada ac as having the heaviest tails a second group with moderate tail heaviness includes the south bc mountains sbcm great lakes gl mackenzie district md and north bc mountains nbcm regions with lowest tail heaviness are the arctic tundra at pacific coast pc and northeastern forest nef in some cases plausible reasoning exists for the observed regional differences for example the topographies of the pr and nwf are dominated by ponds and lakes respectively that reduce the mean flows however when ponds and lakes are connected fill and spill mechanism spence woo 2003 basins can generate very large peaks that might be triggered unexpectedly and not necessarily concurrent with heavy precipitation shook et al 2015 furthermore in the semi arid prairies where runoff coefficient is typically low short duration convective storms can cause large instantaneous peak flows and large scale frontal systems can generate massive amounts of precipitation especially when combined with snow melt dumanski et al 2015 leading to extreme peaks and heavy tails the heavy tails in ac might have a different origin that is the atlantic coast is notorious for its intense storms and hurricanes which can cause extreme flooding events e g berg avila 2010 thompson et al 2009 on the other hand lighter tails of the distribution in pc nbcm and nef might be attributed to the fact that these regions are wetter with relatively high mean annual runoff values leading to a less positively skewed flow distribution the fact is that in canada several different processes can affect flooding and the nature of the peaks including extreme rainfall snowmelt rain on snow groundwater ice jams storm surges etc see buttle et al 2016 and references therein additionally other factors such as catchment size aridity index could affect in general stream flow characteristics see e g markonis et al 2018 a detailed investigation on which processes and why lead to heavy or less heavy tails is very challenging and out of the scope of this study the median values of the shape parameters vary regionally but also differ significantly among the three distributions a more detailed investigation of the right tail shape parameter in the form of box plots fig 9 shows not only differences in the median values but also in the variance of the estimated parameters the smaller the variance the more robust the results thus in this case the ɓriii seem to perform better than the g e v this fact can also be verified with monte carlo simulations and synthetic series for example if we generate synthetic time series with a parent marginal distribution resembling streamflow time series were generated using the cosmos r package available at cran papalexiou et al 2019 such as a bell shapedɓriii or ɓrxii then we extract the maxima and fit the three distributions tested here we observe that the standard deviation of the tail estimate in the g e v is more than double compared to the other distributions thus we deem the g e v performance is not ideal with the box plots indicating shape estimates varying in very large ranges within the regions studied for example for the sbcm the interquartile range for the ɓriii ɓrxii and g e v is 0 09 0 11 and 0 24 while the 95 empirical interval outer fences in the box plots is 0 25 0 35 and 0 74 respectively on average actual values depend on the region the standard deviation of the g e v shape parameter estimates is larger by 147 8 and 70 6 compared with the ɓriii and ɓrxii respectively another finding is that the g e v in most regions and sites predicts streamflow having upper limit exceptions are the ac nwf and pr regions where g e v is unbounded for most of the stations fig 9 the nature of streamflow and the well documented flooding peaks that in many cases are two orders of magnitude larger than the mean streamflow might be an indication of heavy tails this in combination with previous results showing that negative g e v shape parameters are artefacts for precipitation maxima papalexiou koutsoyiannis 2013 and with the results presented in this study fig 8 challenge the tail behaviour as quantified by the g e v especially when the fitted g e v suggests an upper bound we believe that this behaviour might be an artefact that can also be verified by monte carlo simulations for example we use a bell shaped ɓriii 100 2 0 1 distribution to generate 50 years of daily flows independent or correlated see papalexiou 2018 papalexiou et al 2018 and papalexiou and serinaldi 2020 on how to generate non gaussian time series with any distribution and correlation and extract maxima from blocks of 50 values assuming maxima occur within 50 days we repeat this process and form 10 000 samples of annual maxima fitting the g e v in each sample of maxima we observe that approximately 40 of times its shape parameter is negative for the independent case in the presence of autocorrelation for example assuming lag 1 equal to 0 6 this percentage increases to 48 while in both cases the true value is 0 1 the structure of g e v allows negative shape parameters while the location parameter offers additional flexibility which however is not theoretically consistent with the nature of streamflow recall that for γ 0 and 0 the g e v is defined in α β γ and α β γ respectively while the potential range of streamflow is 0 this flexibility allows good fit in the observed data yet this does not imply that the model s parameters are consistent with the nature of streamflow the previous analysis indicates that some of the climatic regions might have different tail behaviour but these regions are not necessarily homogeneous regarding streamflow peaks we applied a well known regional heterogeneity test by hosking and wallis 2005 and as anticipated these vast regions do not pass such a strict test regional homogeneity in this test is assessed by evaluating the stations l variation here we assess the tail behaviour by grouping the stations based on their sample l variation aiming to mimic regional analyses clearly the stations in each group are not necessarily neighbouring but have similar l variation values particularly we 1 sorted the 1088 stations based on their sample l variation 2 discarded the first and last 44 stations to avoid potential outliers and formed 10 groups g1 g10 using the remaining 1000 stations 100 stations each group the l variation range in each group is narrow fig 10 and it is assumed that the stations in each group are homogeneous in terms of l variation 3 the estimated right tail shape parameter of the ɓriii ɓrxii and g e v distributions for each group are shown as box plots fig 10 each box plots is constructed from 100 values 4 we merged the time series of each group after standardizing with their mean merged sample size for each group 3490 values and estimated the tail parameter dots in fig 10 the results indicate a large difference between the tails of the g e v and the ɓriii and ɓrxii specifically for low l variation values either the median of each group or the estimates of the merged sample almost coincide in most groups show that the g e v is bounded from above while the ɓriii and ɓrxii are not note that the results do not differ even if 20 groups are considered resulting in much smaller l variation range for each group the results also do not change if the standardization before the merging is done in terms of standard deviation dividing with standard deviation it is interesting to note that for large l variations groups g8 10 the ɓrxii shows decreasing values for the tail this is statistically possible as the increase in l variation does not imply analogous increase in l skewness and thus such τ2 τ3 points can lie in the lower part of the theoretical l space of the ɓrxii distribution resulting in less heavy right tails see the l space of ɓrxii in papalexiou and koutsoyiannis 2016 5 summary and conclusions analysis of streamflow peaks is the cornerstone of statistical hydrology and crucial in engineering practice the generalized extreme value g e v distribution has been the prevailing model in describing streamflow extremes for more than half a century its extensive use is supported by theoretical arguments it emerges as limiting law describing maxima and yet these arguments are not always consistent in real world cases other than the g e v other three parameter models have been used that typically comprise a location a scale and a shape parameter here we revisit flood peak distributions and demonstrate the framework through a pan canadian investigation we provide theoretical arguments on why some assumptions used by the g e v might be challenged in real world cases and we suggest consistent and flexible alternative distributions that have been neglected in describing flood peaks particularly we use the burr type iii ɓriii and xii ɓrxii distributions as alternative models and compare their performance with that of the g e v these distributions are defined for any positive value and thus are consistent with streamflow s potential range their two shape parameters allow to fully control the left and right tails providing flexibility to describe samples that differ markedly with each other the study provides theoretical arguments that justify specifically the ɓriii distribution to describe annual maxima the performance of these distributions is tested in more than 1000 records of annual maximum streamflow across canada that are representative of different climatic and topographic conditions the distributions are fitted using the method of l moments based on a generic algorithm that we devised and can be applied for any distribution even those that do not have analytical l moment expressions this extensive pan canadian analysis reveals 1 identical performance based on several error measures and the anderson darling gof test of the ɓriii ɓrxii and g e v distributions in describing the observed streamflow annual maxima fig 5 2 notable differences in the heaviness of the right tail that controls the frequency and the magnitude of extremes these models have theoretically the same power type tail and yet despite the identical performance in the observed samples the estimated shape parameters show in general heavier tails for the ɓriii followed by the ɓrxii and finally by the g e v fig 4 and fig 6 3 the previous finding might be important for estimating streamflow peaks for large return periods used in engineering practice for example the ɓriii and ɓrxii predict respectively 19 3 and 11 2 median values larger streamflow at the 1000 year return period compared to the g e v fig 7 this might be applicable to mega projects such as large dams 4 in 39 1 of the stations the g e v predicts an upper limit for streamflow maxima this might be inconsistent with the nature of streamflow it is controversial that distributions used to describe daily streamflow or precipitation in the literature are never bounded from above but distributions used for the annual maxima can have an upper bound here the analysis of these upper limits based on the other fitted distributions that do not have such limitations indicates that these upper limits might have realistic exceedance probabilities fig 8 5 the estimation of the right tail shape parameters critical component for extrapolating at large return periods is more robust for the ɓriii and ɓrxiii distributions the standard deviation of the g e v shape parameter estimates is larger on average by 147 8 and 70 6 compared to the ɓriii and ɓrxii cases respectively fig 9 this suggests that the tail heaviness might not be estimated reliably by the g e v in some cases in turn this could affect the accuracy of extrapolated flood magnitudes 6 regional variation across the climatic regions of canada is observed the tail heaviness as quantified by the right tail shape parameter shows the prairies northwestern forests and atlantic canada as having the heavier tails while less heavy tails are observed for the arctic tundra pacific coast and northeastern forests fig 9 the previous findings suggest that power type distributions with one scale and two shape parameters such as the ɓriii and ɓrxii distributions offer robust and consistent alternatives for the analysis of streamflow maxima credit authorship contribution statement mohanad zaghloul data curation formal analysis methodology visualization writing original draft simon michael papalexiou conceptualization formal analysis methodology supervision writing original draft amin elshorbagy conceptualization funding acquisition supervision writing review editing paulin coulibaly writing review editing declaration of competing interest none acknowledgements this research is financially supported by the integrated modelling program for canada impc project under the umbrella of the global water futures gwf program at the university of saskatchewan canada s m p acknowledges the support of the natural sciences and engineering research council of canada nserc discovery grant rgpin 2019 06894 in addition m z acknowledges the department of civil geological and environmental engineering for the financial support of research through the departmental devolved scholarship the data used in this study are freely available at http collaboration cmc ec gc ca cmc hydrometrics www finally we thank the associate editor and the three reviewers for their comprehensive remarks and criticisms that helped improve markedly the original manuscript 
398,water demands for power generation within the energy water nexus focus on both consumptive and withdrawn water for thermoelectric power plant cooling however the consumptive based approach of water footprinting is incongruent with withdrawn water grey water footprints of thermoelectric power plants associated with thermal pollution offer a proxy method to integrate the consumptive blue water footprint concept with withdrawn water in this study we compute the monthly grey water footprints of thermoelectric power plants from 2010 2016 in the united states the calculation of grey water footprint relies on return flow and temperature effluent data which are available through the energy information administration however in cases where these data are unavailable we present a model for estimating grey water footprints based on fuel type return flow and generation grey water footprints show a peak in the winter and summer months with lower volumes in the spring and fall additionally the national grey water footprint was 18 greater in 2016 than 2010 408 km3 versus 347 km3 peaking in 2015 at 505 km3 grey water footprints of electricity generally occur in the eastern area of the united states where once through cooling systems are most prevalent we discuss the potential of grey water footprints as a policy tool for assessing aquatic ecosystem impacts of thermal pollution our study provides the first quantification of grey water footprints due to thermoelectric power plant pollution in the united states and provides a means of estimating grey water footprints with limited data keywords grey water footprints energy water nexus thermal pollution 1 introduction thermoelectric power plants accounted for 87 of total electricity generation between the years of 2010 to 2016 in the united states eia 2019 and these facilities require water for cooling the intersection of electricity production and water resources is part of the energy water nexus sanders 2015 grubert and sanders 2018 the impacts of thermoelectric power plants on water resources are largely studied from a perspective of water quantity withdrawals and consumption based on fuel source cooling system type and location e g macknick et al 2012a 2012b peer and sanders 2018 peer et al 2020 however there have been relatively few quantitative studies that have characterized the impact of return flow on the local environment resulting in degraded water quality via thermal pollution in this study we utilize water footprinting terminology specifically grey water footprints to characterize the impacts of thermal pollution across the united states from 2010 2016 and provide insight into the usefulness of such characterization as a tool for ecosystem assessment in general there are two different water intensive cooling systems associated with thermoelectric power generation open loop once through and closed loop cooling represented conceptually in fig 1 closed loop systems withdraw water from a source and then evaporatively cool the system using cooling towers or reservoirs in open loop cooled systems water is withdrawn from a water source generally a river or lake and then discharged back into the body of water at an elevated temperature grey water footprints predominantly apply to once through or open loop systems the elevated temperature return flow is governed by the clean water act 316 a with enforcement led by the united states environmental protection agency the amount of thermal pollution and its temperature gradient have important implications for the surrounding aquatic ecosystem logan and stillwell 2018a the volume of water consumed for the production of another good is often labeled the water footprint allan 1998 however as the energy water nexus considers both consumed and withdrawn water for thermoelectric power plants there is often a disconnect in linking the two research spaces as withdrawn water is not directly accounted for in water footprinting analyses water footprints can be disaggregated into three distinct types blue water green water and grey water hoekstra et al 2011 blue water refers to surface water and groundwater resources and is directly comparable to the consumed water for the energy water nexus green water refers to the amount of directly consumed rainwater which is negligible for thermoelectric power generation finally grey water refers to the amount of water predominantly surface water that is required to assimilate pollutants back into the environment the grey water footprint provides an opportunity to indirectly account for withdrawn water for thermoelectric power generation within the energy water nexus integrating with water footprint analysis in this study we provide an analysis of grey water at a basin scale to understand the variations of thermal pollution across multiple years throughout the united states the term grey water footprint was originally referred to as dilution water chapagain et al 2006 and references a difficult to quantify volume of water that is polluted morrison and schulte 2010 this volume of water is dependent on local hydrology and water quality standards nazer et al 2008 and is sometimes criticized as being less meaningful than the other two types of water footprints green and blue chenoweth et al 2014 one significant challenge to grey water footprints is the lack of available data to adequately capture and quantify this volume mekonnen et al 2015 2016 despite these challenges and concerns there are several studies that point to the relevance of grey water footprints especially in the energy sector grey water footprints have also been calculated for some agricultural products mekonnen and hoekstra 2011 and construction materials gerbens leenes et al 2018 a study in brazil focused on biofuel production with respect to grey blue and green water footprints castillo et al 2017 additionally a recent study categorized grey water imports and exports of 40 countries and regions across multiple economic categories zhao et al 2019 with respect to energy two studies evaluated the grey water footprint of china s energy sources ding et al 2018 liao et al 2019 but only considered the grey water footprint of chemical oxygen demand and not thermal pollution as a result these studies came to the conclusion that the blue water footprint was larger than the grey water footprint for energy however by not including thermal pollution as part of these calculations a significant portion of the grey water footprint was excluded a study that quantified both grey and blue virtual water transfers of the u s electricity grid showed a much larger grey water footprint than blue water footprint as a result of thermal pollution chini et al 2018 grey water e g polluted and not necessarily usable in another context can have an impact on water scarcity jamshidi 2019 warranting a more complete picture of grey water associated with power generation in this study we characterize total grey water footprints specifically using thermal pollution loads in the united states we build off previous research by chini et al 2018 and utilize existing data from the u s energy information administration to compute the grey water footprint for thermoelectric power plants across the united states using data from 2010 2016 to remain congruent with chini et al 2018 we identify intra annual and inter annual trends of grey water footprint at both national and basin scales through this work we highlight the variations in acceptable thermal pollution limits across the country and provide insights into how grey water footprints can be utilized to identify potential ecological vulnerabilities additionally we provide a discussion on utilization of grey water in a policy and decision making framework with emphasis on grey water footprints as quantitative proxies for assessing thermally impacted aquatic populations 2 background 2 1 energy water nexus in recent years energy and water scarcity and security concerns have fueled energy water nexus studies from local lubega and stillwell 2018 to global scales mekonnen et al 2015 the energy water nexus describes the relationship between energy and water resources regarding energy demand for water lam et al 2017 chini and stillwell 2018 and the water demand of energy resources we focus on the latter part of the energy water nexus in this study particularly regarding the water demands for electricity the dependence of electricity generation on water resources has been investigated across multiple geographic and supply chain scales for example water demands of electricity have been assessed both at the production scale peer and sanders 2016 and considering the extraction of fuels peer et al 2020 studies of the water footprint of electricity have considered the state e g stillwell et al 2011 denooyer et al 2016 regional e g ruddell et al 2014 and continental e g chini and stillwell 2020 scales climate impact assessment and planning and management strategies have also developed within the energy water nexus dai et al 2018 no longer are water decisions considered discrete as associated energy food and climate implications emerge swatuk and cash 2018 unfortunately water and energy policy goals can at times be in direct conflict hoffman 2019 furthermore the uncertainty associated with climate change intensity creates mutual vulnerabilities across energy and water sectors vliet et al 2012 2016 swatuk and cash 2018 with energy production linked to climate change hoffman 2019 these mutual vulnerabilities will require further attention from researchers and decision makers 2 2 national thermal pollution limits the united states governs the effluent limits from thermoelectric power plants under the clean water act while there are national limits to thermal pollution individual states have the authority to impose more stringent requirements on thermal discharge in general there are two types of thermal pollution laws related to power plants absolute temperatures e g daily average maximum temperature or instantaneous temperature should not exceed a temperature threshold often 32 c or changes in temperature e g temperature difference should not exceed 3 c the prevalence of these laws varies across the country with a majority of u s states having a combination of both structures see fig 2 additionally there are several states that have some variations of these laws either spatially or temporally many states impose seasonal temperature thresholds or have laws that vary by basin in particular wisconsin and ohio have a multitude of thermal pollution regulations for the various rivers within each state often states have different classes of streams and rivers with further restrictions to protect endangered species or critical ecology in the case where states have varying restrictions on stream classes size of stream river we assume the regulations from the largest class of rivers to be the governing rules for thermal pollution other variations in thermal pollution governance involve regulatory mixing zones with end of mixing zone criteria rather than end of pipe criteria epa 1991 finally some states have different pollution restrictions between lakes and rivers with differing maximum temperature change thresholds with the relationship between power plant thermal pollution and biologic impact becoming more apparent e g arieli et al 2011 particularly in light of climate change as a compounding factor regulation as an aquatic health mitigation strategy might become more prevalent visualizing the variation in thermal regulations across the united states as shown in fig 2 provides an opportunity to survey the various approaches states have implemented to meet the regulations imposed by the clean water act each power plant outfall has a national pollutant discharge elimination system npdes permit on occasion power plants need to seek a variance when temperature limits cannot be met for example the 2012 drought in the midwest led many power plants to curtail operations or request variances to limits set in npdes permits idnr 2013 additionally several studies have noted that climate change might increase water temperatures while decreasing water availability exacerbating the impacts of thermal pollution on surrounding ecosystems vliet et al 2012 miara et al 2018 understanding these regulations is important for not only computing grey water footprints but also for understanding the potential policy implications of using grey water footprints as a decision making framework if grey water were incorporated into regulations it could be used as an indicator for aquatic ecosystem health especially on an intra annual basis 3 methods 3 1 computing the grey water footprint grey water refers to the amount of water required to assimilate pollutants back into the natural environment hoekstra et al 2011 grey water footprints for thermal pollutants such as those from thermoelectric power plants are dictated by return flows ambient water temperatures discharge water temperatures and local water quality policy eq 1 the u s energy information administration eia reports monthly cooling water return flows average ambient water temperatures and discharge water temperature via form 923 eia these monthly averages might potentially underestimate peaks in grey water footprints at a sub monthly scale to determine the monthly thermal limits for return flow it is necessary to take a nationwide inventory of water quality regulations set by each state as enforcement of the clean water act in this study we focus on the contiguous united states excluding alaska and hawaii see the supporting information for sources and temperature limits in the national inventory and fig 2 for a visualization of the regulating approach to thermal discharge using these allowable temperatures and the data from the eia we compute the grey water footprint of power plants in each month from 2010 to 2016 using a previously established metric see eq 1 hoekstra et al 2011 1 grey water return flow t e f f l t a m b t a l l o w t a m b return flow is the amount of water withdrawn from a water body minus the consumption blue water within the power plant withdrawn water has a temperature of tamb and after being used as a heat sink is returned to the environment at an elevated temperature teffl see fig 1 for a visual schematic of power plant water demand and temperature considerations the allowable temperature increase tallow is determined by each state in accordance with the clean water act 3 2 regression analysis the grey water footprints for each power plant were aggregated based on three predominant thermal fuel sources coal natural gas or nuclear accounting for a majority of the grey water footprints across the country for each of these three fuel sources we generate regression models of grey water footprint volume based on return flow and electricity generation in the event that some power plants used multiple fuel types the fuel source that produced the largest portion of electricity dictated the fuel type similar to peer and sanders 2016 we only consider power plants where greater than 90 of total generation is attributed to a single fuel source for the regression analysis these regression models for the three different fuels offer opportunities to estimate grey water footprints with a lack of site specific temperature data 4 results of the total electricity generated at thermoelectric power plants where a grey water footprint was present nearly 43 was produced via coal and 17 was produced via natural gas from 2010 to 2016 the remaining electricity portfolio was generated by nuclear generation 34 and other or mixed fuel sources the total electricity generated from these power plants is approximately 1 8 million gwh year this total represents over 45 of total electricity generation for the united states during this period the significant portion of electricity generation that contains a grey water footprint in the united states indicates the need for assessing thermal pollution and aquatic ecosystem health at a national scale 4 1 seasonal and annual trends there has been a general increase in total grey water footprint in the united states between the years 2010 and 2016 this increase of grey water footprint is opposite the trend of decreasing withdrawn water volumes across the united states by means of retiring open loop cooled coal plants as identified by peer and sanders 2018 the national grey water footprint was 18 greater in 2016 than 2010 408 km3 versus 347 km3 peaking in 2015 at 505 km3 as shown in fig 3 this increase accompanies a seasonal pattern of grey water footprint partially dictated by the cyclical average intake and discharge temperatures of thermoelectric power plants across the country data for fig 3 are included in the supporting information in the summer of 2012 we see a peak in overall grey water footprints that is greater than all other years this peak in grey water footprint coincides with a drought in the midwestern united states where a large amount of open loop cooled power plants exist indicating the response of grey water footprints to droughts and elevated water temperatures we acknowledge there is uncertainty associated with these grey water footprints for example we utilize an average monthly temperature to compute grey water footprints instantaneous temperatures will fall on either side of this mean creating uncertainty in the footprint calculations however based on available data only at the monthly scale we are unable to capture uncertainty in these results and suggest these grey water footprints as a best approximation which are likely conservative estimates beyond peaks in the summer months there is a consistent pattern in the seasonal grey water footprint of thermoelectric power generation in the united states the average monthly grey water footprint for the seven year period is shown in fig 4 there is a relatively consistent spread of uncertainty in the form of one standard deviation across the year as one would expect peaks in the grey water footprint occur during the hot summer months however a second smaller peak occurs in the winter months with the lowest water footprints occurring in the early summer and late fall months the peak of grey water footprint in winter months is due to a large difference in intake versus discharge temperature 4 2 spatial variations in grey water in addition to quantifying overall aggregates of grey water footprints we define the spatial context of grey water using hydrologic unit codes huc using huc 8 designations we compute the grey water footprint spatially for the years 2010 2013 and 2016 fig 5 a 5 b and 5 c respectively in general a majority of basins have a grey water footprint of less than 1 km3 year the basins with the largest grey water footprints remain relatively constant over time a majority of the grey water footprints occur in the eastern area of the united states which corresponds to wetter climates and riparian water rights systems to understand the seasonality of grey water footprints fig 6 displays the grey water footprint of the four seasons as a percentage of the total water footprint in most locations the previously identified trend of intra annual water footprints holds true however there are some hucs where a different pattern emerges with larger water footprints in the spring months these seasonal variations support the need for seasonal thermal limits on discharge temperatures a small number of the states in the eastern and central area of the united states illinois wisconsin kentucky indiana ohio missouri and pennsylvania have seasonally varying laws to further characterize the spatial variation of grey water footprints it is necessary to put them in context of the available water in the catchment tidwell et al 2018 provides surface water availability values for hydrologic catchments across the united states using these annual water availability values we create an average water pollution level wpl defined as the ratio between the average annual grey water footprint in catchments from 2010 to 2016 and water availability see fig 7 nearly 70 of catchments have wpl less than 1 meaning that grey water footprints are less than available water however there are several catchments with wpl greater than 10 n 38 the distribution of wpl in catchments is highly positively skewed with several catchments having wpls of nearly 2000 these catchments would be particularly vulnerable to the environmental impacts of thermal pollution from power plants 4 3 modeling the grey water footprint a lack of data often limits the calculation of grey water footprints computing the grey water footprint of thermal pollution from power plants is no exception as described in eq 1 grey water relies on several factors including ambient water temperature and discharge water temperature however these data are often unavailable in the absence of these data we develop regression models based on monthly return flow already considered in eq 1 and monthly generated electricity for three different fuel types coal natural gas and nuclear only power plants that had a non zero discharge volume were considered in the analysis 2 l o g 10 g w f f β 0 β 1 l o g 10 q r f β 2 l o g 10 e ε where gwff m3 month is the monthly grey water footprint for each fuel type qrf m3 month is the monthly return flow of a power plant e mwh is the monthly electricity generation and ε is the error a log log model was determined to best represent the trend of the data table 1 shows the values of the model coefficients for example a 1 increase in electricity generation β 2 is associated with a 0 16 and 0 17 increase in grey water footprint for coal and natural gas thermoelectric power plants respectively and 0 04 increase in the grey water footprint of nuclear power plants additionally a 1 increase in return flow is associated with approximately 1 increase in grey water footprint for all fuel types these models capture a significant portion of the variability of the system with model goodness of fit r2 values greater than 0 80 for all fuel types however these models are only relevant for open loop cooled power plants both of the determinants were statistically relevant in all three models with p values less than 0 001 except for monthly generation at the nuclear fuel type variance inflation factors vif to test for multicolinearity were computed for each model multicolinearity was assumed to exist if vif 10 for all variables in each model vif was less than the threshold figure s1 in the supporting information shows the performance of each model through plots of predicted versus observed values additionally the supporting information contain plots to visualize the normality figure s2 and residuals of the models figures s3 s5 for each of these provided models a k folds cross validation test was performed to validate the predictive capabilities of the model the k folds test was performed with 10 folds using the caret package in r kuhn 2008 the results of these k folds test showed equivalent root mean squared errors rmse and coefficients when compared to the original log log regression model the k folds approach eliminates bias by ensuring that each datum is considered in a test set exactly one time additionally we performed a data splitting analysis to further verify the results using an 80 20 split in the data for coal the resulting rmse was slightly higher for the testing data than for the original values 0 473 compared to 0 459 additionally the modeled coefficients in the training set were relatively similar to the values reported in table 1 the supporting information table s1 provides the difference in rmse between the full model k folds validation and data splitting validation the results of these validation tests suggest high predictive power for the given log log models 5 discussion 5 1 fish kills and winter grey water footprints mapping grey water footprints to season provides additional emphasis on known trends in aquatic ecosystems downstream of power plant thermal effluent long term exposure to sub lethal environmental changes e g sustained high water temperature poses a great risk to aquatic ecosystems kulkarni et al 2011 while such sub lethal high temperature conditions are typically associated with summer wintertime changes in water temperature can lead to ecosystem changes as well notably thermally polluted waters downstream of power plants can serve as locations of refuge for fish and other species during the winter months gallaway 1974 competition for temperature as a resource can impact aquatic ecosystem diversity and health as well as individual organism health magnuson et al 1979 logan and stillwell 2018a when warm water or tropical species compete for and win habitat space e g at power plant outfalls against native species overall diversity patterns can shift power plant outfalls are known hot spots for such habitat competition emde et al 2016 furthermore warm temperatures during the winter months can lead to altered reproductive cycles in aquatic species firkus et al 2018 leaving juvenile individuals susceptible to impacts from unnatural water conditions should a power plant change operational conditions related to the difference in intake and effluent temperature e g response to changing energy demand scheduled or unscheduled maintenance fish and other organisms in the vicinity become vulnerable to rapidly changing water temperature a sudden decrease in power plant outfall temperature during the wintertime could cause the non native warm water species to find themselves in cold water leading to shock or death for example a pennsylvania power plant killed an estimated 12 000 fish during a plant shutdown in 2016 leading to a revision of cold weather shutdown protocol crable 2018 with this power plant water temperature and species response relationship in mind better accounting of ecosystem conditions could prove beneficial in maintaining and conserving aquatic species populations grey water footprints could serve as volumetric and thus quantitative proxies for potentially impacted habitat space in waterways combining the biological aspects of predictive models for the effect of temperature changes on fish such as logan and stillwell 2018a with specific power plant grey water footprints a more thorough accounting of the potential for aquatic species impact could be completed volumetric grey water footprints provide an opportunity to assess aquatic ecosystem quality and allow for species specific information such as lethal temperatures particularly the corollaries are relevant for warm water species seeking refuge in thermal pollution hot spots during winter months to serve as biological impact assessment criteria for power plant operations and or regulatory mixing zones as regulatory mixing zones tend to provide edge of zone rather than end of pipe water quality requirements epa 1991 there is the potential to underestimate the true biologic impact that thermal pollution has on species within mixing zones logan and stillwell 2018b combining grey water footprints and biological impacts has promising potential for advancing definitions of regulatory mixing zones and providing more adaptive management in areas with known sensitive and or at risk species 5 2 temperature duration curves and grey water footprints when assessing water quality biodiversity and ecosystem health are important components of overall watershed health in research areas such as river management for hydropower maintaining natural flow conditions is viewed as one of the best ways to protect biodiversity dutta et al 2020 in a similar fashion maintaining or at least mimicking natural temperature conditions is one way to encourage biodiversity downstream of thermoelectric power plants logan and stillwell 2018b introduced the concept and creation of temperature duration curves for use in modeling thermal pollution impacts from thermoelectric power plants such targeted temperature modeling can be used to assess associated ecosystem service outputs using known biodiversity and temperature linkages by adding the grey water footprint to water resource accountability and prediction temperature conditions and water consumption trends for a given watershed can be used in decision making contexts future work could investigate the incorporation of volumetric water demands in the form of grey water footprints with temperature conditions and profiles for modeling thermal pollution impacts 5 3 grey water footprints and policy watershed assessments e g evaluations of the processes influences and problems within a watershed with the intent to develop an action plan debarry 2004 serve as a planning and management space for which grey water footprints become vital to be successful with water management strategies all values and perspectives across stakeholders are required to be integrated heathcote 2009 proper accounting of thermal pollution in grey water footprints provides a proxy by which the ecosystem has a quantitative stake to watershed management oftentimes in stream beneficial use permits are where ecosystems are considered direct stakeholders in water use and management in stream beneficial use monitoring aims to identify and track water impairments as well as water quality trends oklahoma for example initiated a beneficial use monitoring program in 1998 to provide a comprehensive unified approach to water quality monitoring okl 2014 such monitoring programs in oklahoma and elsewhere could be another application of grey water footprints as a water quality metric seasonal trends in grey water footprints could be used to inform local water policies at the huc scale such trends shed light on potential inadequacies of blanket thermal regulations e g cwa 316 a as shown in fig 6 winter has a higher grey water footprint than spring and fall for most hucs due in part to the large temperature differential between intake and effluent temperatures typical seasonal npdes permits categorize temperature regulations into summer and winter seasons further consideration of seasonality might provide more accurate accounting and mitigation of thermal pollution impacts leading to a more even spread of grey water footprint by season at individual power plants when feasible furthermore specific life cycle stages of ecologically and or economically important species and holistic ecosystem health could become part of temperature metrics discussions based on ecosystem data and grey water footprint data as discussed previously winter fish kill events can be a serious issue given the alteration of normal seasonal conditions at power plant outfalls providing power plants with appropriate temperature limits and or procedures for cold weather shutdown informed by grey water footprints could reduce the severity of ecosystem damages found near power plant outfalls 6 conclusions grey water footprints provide an important component to the discussion of the energy water nexus by acting as a proxy for withdrawn water impacts on the surrounding environment however in many instances these water volumes are challenging to calculate due to a lack of available data especially source and effluent temperature therefore we present models to estimate the grey water footprint of electricity based on fuel generation and return flow these models have relatively high explanatory potential and could be used to estimate within an order of magnitude grey water footprints in environments with a lack of data while often overlooked grey water footprints have an important and growing role in understanding ecosystem impacts of the energy water nexus while this study focuses on the united states it has global implications much of the global data on water for electricity generation utilizes fuel specific information from the united states spang et al 2014 vandecasteele et al 2016 chini and stillwell 2020 in this study we found substantial evidence of seasonality with respect to grey water footprints these seasonal trends showed peaks of grey water in both the summer and winter months however in many jurisdictions there are no seasonal regulations of thermal effluent with many states only providing a singular threshold for thermal pollution limits in most locations the previously identified trend of intra annual water footprints holds true with peaks in the winter and summer months however there are some river basins where a different pattern emerges with larger water footprints in the spring months these seasonal variations support the need for seasonal thermal limits on discharge grey water footprints can inform the relative magnitude of these ecosystem impacts to facilitate seasonal rules and regulations surrounding thermal pollution finally there are significant opportunities to combine grey water with previous studies of thermal pollution to identify ecosystem impacts the grey water concept alone does not provide detailed information with respect to the impact of polluted water on downstream ecosystem service delivery launiainen et al 2014 therefore it is necessary to further evaluate grey water footprints in conjunction with other metrics we suggest further analysis of grey water footprints with large scale ecosystem impacts such as fish kills or pairing the water footprints with quantitative concepts of thermal pollution such as temperature duration curves targeted temperature modeling can be used to assess associated ecosystem service outputs using known biodiversity and temperature linkages by including the grey water footprint concepts with these water resource ecosystem models temperature conditions and water consumption trends for a given watershed can be used in decision making contexts credit authorship contribution statement christopher m chini conceptualization methodology data curation formal analysis writing original draft writing review editing visualization lauren h logan formal analysis writing original draft writing review editing ashlynn s stillwell conceptualization writing original draft writing review editing supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors thank lucas a djehdian and william n lubega for their insight and support in the initial project formulation supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103733 appendix a supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 supplementary data s2 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s2 supplementary data s3 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s3 
398,water demands for power generation within the energy water nexus focus on both consumptive and withdrawn water for thermoelectric power plant cooling however the consumptive based approach of water footprinting is incongruent with withdrawn water grey water footprints of thermoelectric power plants associated with thermal pollution offer a proxy method to integrate the consumptive blue water footprint concept with withdrawn water in this study we compute the monthly grey water footprints of thermoelectric power plants from 2010 2016 in the united states the calculation of grey water footprint relies on return flow and temperature effluent data which are available through the energy information administration however in cases where these data are unavailable we present a model for estimating grey water footprints based on fuel type return flow and generation grey water footprints show a peak in the winter and summer months with lower volumes in the spring and fall additionally the national grey water footprint was 18 greater in 2016 than 2010 408 km3 versus 347 km3 peaking in 2015 at 505 km3 grey water footprints of electricity generally occur in the eastern area of the united states where once through cooling systems are most prevalent we discuss the potential of grey water footprints as a policy tool for assessing aquatic ecosystem impacts of thermal pollution our study provides the first quantification of grey water footprints due to thermoelectric power plant pollution in the united states and provides a means of estimating grey water footprints with limited data keywords grey water footprints energy water nexus thermal pollution 1 introduction thermoelectric power plants accounted for 87 of total electricity generation between the years of 2010 to 2016 in the united states eia 2019 and these facilities require water for cooling the intersection of electricity production and water resources is part of the energy water nexus sanders 2015 grubert and sanders 2018 the impacts of thermoelectric power plants on water resources are largely studied from a perspective of water quantity withdrawals and consumption based on fuel source cooling system type and location e g macknick et al 2012a 2012b peer and sanders 2018 peer et al 2020 however there have been relatively few quantitative studies that have characterized the impact of return flow on the local environment resulting in degraded water quality via thermal pollution in this study we utilize water footprinting terminology specifically grey water footprints to characterize the impacts of thermal pollution across the united states from 2010 2016 and provide insight into the usefulness of such characterization as a tool for ecosystem assessment in general there are two different water intensive cooling systems associated with thermoelectric power generation open loop once through and closed loop cooling represented conceptually in fig 1 closed loop systems withdraw water from a source and then evaporatively cool the system using cooling towers or reservoirs in open loop cooled systems water is withdrawn from a water source generally a river or lake and then discharged back into the body of water at an elevated temperature grey water footprints predominantly apply to once through or open loop systems the elevated temperature return flow is governed by the clean water act 316 a with enforcement led by the united states environmental protection agency the amount of thermal pollution and its temperature gradient have important implications for the surrounding aquatic ecosystem logan and stillwell 2018a the volume of water consumed for the production of another good is often labeled the water footprint allan 1998 however as the energy water nexus considers both consumed and withdrawn water for thermoelectric power plants there is often a disconnect in linking the two research spaces as withdrawn water is not directly accounted for in water footprinting analyses water footprints can be disaggregated into three distinct types blue water green water and grey water hoekstra et al 2011 blue water refers to surface water and groundwater resources and is directly comparable to the consumed water for the energy water nexus green water refers to the amount of directly consumed rainwater which is negligible for thermoelectric power generation finally grey water refers to the amount of water predominantly surface water that is required to assimilate pollutants back into the environment the grey water footprint provides an opportunity to indirectly account for withdrawn water for thermoelectric power generation within the energy water nexus integrating with water footprint analysis in this study we provide an analysis of grey water at a basin scale to understand the variations of thermal pollution across multiple years throughout the united states the term grey water footprint was originally referred to as dilution water chapagain et al 2006 and references a difficult to quantify volume of water that is polluted morrison and schulte 2010 this volume of water is dependent on local hydrology and water quality standards nazer et al 2008 and is sometimes criticized as being less meaningful than the other two types of water footprints green and blue chenoweth et al 2014 one significant challenge to grey water footprints is the lack of available data to adequately capture and quantify this volume mekonnen et al 2015 2016 despite these challenges and concerns there are several studies that point to the relevance of grey water footprints especially in the energy sector grey water footprints have also been calculated for some agricultural products mekonnen and hoekstra 2011 and construction materials gerbens leenes et al 2018 a study in brazil focused on biofuel production with respect to grey blue and green water footprints castillo et al 2017 additionally a recent study categorized grey water imports and exports of 40 countries and regions across multiple economic categories zhao et al 2019 with respect to energy two studies evaluated the grey water footprint of china s energy sources ding et al 2018 liao et al 2019 but only considered the grey water footprint of chemical oxygen demand and not thermal pollution as a result these studies came to the conclusion that the blue water footprint was larger than the grey water footprint for energy however by not including thermal pollution as part of these calculations a significant portion of the grey water footprint was excluded a study that quantified both grey and blue virtual water transfers of the u s electricity grid showed a much larger grey water footprint than blue water footprint as a result of thermal pollution chini et al 2018 grey water e g polluted and not necessarily usable in another context can have an impact on water scarcity jamshidi 2019 warranting a more complete picture of grey water associated with power generation in this study we characterize total grey water footprints specifically using thermal pollution loads in the united states we build off previous research by chini et al 2018 and utilize existing data from the u s energy information administration to compute the grey water footprint for thermoelectric power plants across the united states using data from 2010 2016 to remain congruent with chini et al 2018 we identify intra annual and inter annual trends of grey water footprint at both national and basin scales through this work we highlight the variations in acceptable thermal pollution limits across the country and provide insights into how grey water footprints can be utilized to identify potential ecological vulnerabilities additionally we provide a discussion on utilization of grey water in a policy and decision making framework with emphasis on grey water footprints as quantitative proxies for assessing thermally impacted aquatic populations 2 background 2 1 energy water nexus in recent years energy and water scarcity and security concerns have fueled energy water nexus studies from local lubega and stillwell 2018 to global scales mekonnen et al 2015 the energy water nexus describes the relationship between energy and water resources regarding energy demand for water lam et al 2017 chini and stillwell 2018 and the water demand of energy resources we focus on the latter part of the energy water nexus in this study particularly regarding the water demands for electricity the dependence of electricity generation on water resources has been investigated across multiple geographic and supply chain scales for example water demands of electricity have been assessed both at the production scale peer and sanders 2016 and considering the extraction of fuels peer et al 2020 studies of the water footprint of electricity have considered the state e g stillwell et al 2011 denooyer et al 2016 regional e g ruddell et al 2014 and continental e g chini and stillwell 2020 scales climate impact assessment and planning and management strategies have also developed within the energy water nexus dai et al 2018 no longer are water decisions considered discrete as associated energy food and climate implications emerge swatuk and cash 2018 unfortunately water and energy policy goals can at times be in direct conflict hoffman 2019 furthermore the uncertainty associated with climate change intensity creates mutual vulnerabilities across energy and water sectors vliet et al 2012 2016 swatuk and cash 2018 with energy production linked to climate change hoffman 2019 these mutual vulnerabilities will require further attention from researchers and decision makers 2 2 national thermal pollution limits the united states governs the effluent limits from thermoelectric power plants under the clean water act while there are national limits to thermal pollution individual states have the authority to impose more stringent requirements on thermal discharge in general there are two types of thermal pollution laws related to power plants absolute temperatures e g daily average maximum temperature or instantaneous temperature should not exceed a temperature threshold often 32 c or changes in temperature e g temperature difference should not exceed 3 c the prevalence of these laws varies across the country with a majority of u s states having a combination of both structures see fig 2 additionally there are several states that have some variations of these laws either spatially or temporally many states impose seasonal temperature thresholds or have laws that vary by basin in particular wisconsin and ohio have a multitude of thermal pollution regulations for the various rivers within each state often states have different classes of streams and rivers with further restrictions to protect endangered species or critical ecology in the case where states have varying restrictions on stream classes size of stream river we assume the regulations from the largest class of rivers to be the governing rules for thermal pollution other variations in thermal pollution governance involve regulatory mixing zones with end of mixing zone criteria rather than end of pipe criteria epa 1991 finally some states have different pollution restrictions between lakes and rivers with differing maximum temperature change thresholds with the relationship between power plant thermal pollution and biologic impact becoming more apparent e g arieli et al 2011 particularly in light of climate change as a compounding factor regulation as an aquatic health mitigation strategy might become more prevalent visualizing the variation in thermal regulations across the united states as shown in fig 2 provides an opportunity to survey the various approaches states have implemented to meet the regulations imposed by the clean water act each power plant outfall has a national pollutant discharge elimination system npdes permit on occasion power plants need to seek a variance when temperature limits cannot be met for example the 2012 drought in the midwest led many power plants to curtail operations or request variances to limits set in npdes permits idnr 2013 additionally several studies have noted that climate change might increase water temperatures while decreasing water availability exacerbating the impacts of thermal pollution on surrounding ecosystems vliet et al 2012 miara et al 2018 understanding these regulations is important for not only computing grey water footprints but also for understanding the potential policy implications of using grey water footprints as a decision making framework if grey water were incorporated into regulations it could be used as an indicator for aquatic ecosystem health especially on an intra annual basis 3 methods 3 1 computing the grey water footprint grey water refers to the amount of water required to assimilate pollutants back into the natural environment hoekstra et al 2011 grey water footprints for thermal pollutants such as those from thermoelectric power plants are dictated by return flows ambient water temperatures discharge water temperatures and local water quality policy eq 1 the u s energy information administration eia reports monthly cooling water return flows average ambient water temperatures and discharge water temperature via form 923 eia these monthly averages might potentially underestimate peaks in grey water footprints at a sub monthly scale to determine the monthly thermal limits for return flow it is necessary to take a nationwide inventory of water quality regulations set by each state as enforcement of the clean water act in this study we focus on the contiguous united states excluding alaska and hawaii see the supporting information for sources and temperature limits in the national inventory and fig 2 for a visualization of the regulating approach to thermal discharge using these allowable temperatures and the data from the eia we compute the grey water footprint of power plants in each month from 2010 to 2016 using a previously established metric see eq 1 hoekstra et al 2011 1 grey water return flow t e f f l t a m b t a l l o w t a m b return flow is the amount of water withdrawn from a water body minus the consumption blue water within the power plant withdrawn water has a temperature of tamb and after being used as a heat sink is returned to the environment at an elevated temperature teffl see fig 1 for a visual schematic of power plant water demand and temperature considerations the allowable temperature increase tallow is determined by each state in accordance with the clean water act 3 2 regression analysis the grey water footprints for each power plant were aggregated based on three predominant thermal fuel sources coal natural gas or nuclear accounting for a majority of the grey water footprints across the country for each of these three fuel sources we generate regression models of grey water footprint volume based on return flow and electricity generation in the event that some power plants used multiple fuel types the fuel source that produced the largest portion of electricity dictated the fuel type similar to peer and sanders 2016 we only consider power plants where greater than 90 of total generation is attributed to a single fuel source for the regression analysis these regression models for the three different fuels offer opportunities to estimate grey water footprints with a lack of site specific temperature data 4 results of the total electricity generated at thermoelectric power plants where a grey water footprint was present nearly 43 was produced via coal and 17 was produced via natural gas from 2010 to 2016 the remaining electricity portfolio was generated by nuclear generation 34 and other or mixed fuel sources the total electricity generated from these power plants is approximately 1 8 million gwh year this total represents over 45 of total electricity generation for the united states during this period the significant portion of electricity generation that contains a grey water footprint in the united states indicates the need for assessing thermal pollution and aquatic ecosystem health at a national scale 4 1 seasonal and annual trends there has been a general increase in total grey water footprint in the united states between the years 2010 and 2016 this increase of grey water footprint is opposite the trend of decreasing withdrawn water volumes across the united states by means of retiring open loop cooled coal plants as identified by peer and sanders 2018 the national grey water footprint was 18 greater in 2016 than 2010 408 km3 versus 347 km3 peaking in 2015 at 505 km3 as shown in fig 3 this increase accompanies a seasonal pattern of grey water footprint partially dictated by the cyclical average intake and discharge temperatures of thermoelectric power plants across the country data for fig 3 are included in the supporting information in the summer of 2012 we see a peak in overall grey water footprints that is greater than all other years this peak in grey water footprint coincides with a drought in the midwestern united states where a large amount of open loop cooled power plants exist indicating the response of grey water footprints to droughts and elevated water temperatures we acknowledge there is uncertainty associated with these grey water footprints for example we utilize an average monthly temperature to compute grey water footprints instantaneous temperatures will fall on either side of this mean creating uncertainty in the footprint calculations however based on available data only at the monthly scale we are unable to capture uncertainty in these results and suggest these grey water footprints as a best approximation which are likely conservative estimates beyond peaks in the summer months there is a consistent pattern in the seasonal grey water footprint of thermoelectric power generation in the united states the average monthly grey water footprint for the seven year period is shown in fig 4 there is a relatively consistent spread of uncertainty in the form of one standard deviation across the year as one would expect peaks in the grey water footprint occur during the hot summer months however a second smaller peak occurs in the winter months with the lowest water footprints occurring in the early summer and late fall months the peak of grey water footprint in winter months is due to a large difference in intake versus discharge temperature 4 2 spatial variations in grey water in addition to quantifying overall aggregates of grey water footprints we define the spatial context of grey water using hydrologic unit codes huc using huc 8 designations we compute the grey water footprint spatially for the years 2010 2013 and 2016 fig 5 a 5 b and 5 c respectively in general a majority of basins have a grey water footprint of less than 1 km3 year the basins with the largest grey water footprints remain relatively constant over time a majority of the grey water footprints occur in the eastern area of the united states which corresponds to wetter climates and riparian water rights systems to understand the seasonality of grey water footprints fig 6 displays the grey water footprint of the four seasons as a percentage of the total water footprint in most locations the previously identified trend of intra annual water footprints holds true however there are some hucs where a different pattern emerges with larger water footprints in the spring months these seasonal variations support the need for seasonal thermal limits on discharge temperatures a small number of the states in the eastern and central area of the united states illinois wisconsin kentucky indiana ohio missouri and pennsylvania have seasonally varying laws to further characterize the spatial variation of grey water footprints it is necessary to put them in context of the available water in the catchment tidwell et al 2018 provides surface water availability values for hydrologic catchments across the united states using these annual water availability values we create an average water pollution level wpl defined as the ratio between the average annual grey water footprint in catchments from 2010 to 2016 and water availability see fig 7 nearly 70 of catchments have wpl less than 1 meaning that grey water footprints are less than available water however there are several catchments with wpl greater than 10 n 38 the distribution of wpl in catchments is highly positively skewed with several catchments having wpls of nearly 2000 these catchments would be particularly vulnerable to the environmental impacts of thermal pollution from power plants 4 3 modeling the grey water footprint a lack of data often limits the calculation of grey water footprints computing the grey water footprint of thermal pollution from power plants is no exception as described in eq 1 grey water relies on several factors including ambient water temperature and discharge water temperature however these data are often unavailable in the absence of these data we develop regression models based on monthly return flow already considered in eq 1 and monthly generated electricity for three different fuel types coal natural gas and nuclear only power plants that had a non zero discharge volume were considered in the analysis 2 l o g 10 g w f f β 0 β 1 l o g 10 q r f β 2 l o g 10 e ε where gwff m3 month is the monthly grey water footprint for each fuel type qrf m3 month is the monthly return flow of a power plant e mwh is the monthly electricity generation and ε is the error a log log model was determined to best represent the trend of the data table 1 shows the values of the model coefficients for example a 1 increase in electricity generation β 2 is associated with a 0 16 and 0 17 increase in grey water footprint for coal and natural gas thermoelectric power plants respectively and 0 04 increase in the grey water footprint of nuclear power plants additionally a 1 increase in return flow is associated with approximately 1 increase in grey water footprint for all fuel types these models capture a significant portion of the variability of the system with model goodness of fit r2 values greater than 0 80 for all fuel types however these models are only relevant for open loop cooled power plants both of the determinants were statistically relevant in all three models with p values less than 0 001 except for monthly generation at the nuclear fuel type variance inflation factors vif to test for multicolinearity were computed for each model multicolinearity was assumed to exist if vif 10 for all variables in each model vif was less than the threshold figure s1 in the supporting information shows the performance of each model through plots of predicted versus observed values additionally the supporting information contain plots to visualize the normality figure s2 and residuals of the models figures s3 s5 for each of these provided models a k folds cross validation test was performed to validate the predictive capabilities of the model the k folds test was performed with 10 folds using the caret package in r kuhn 2008 the results of these k folds test showed equivalent root mean squared errors rmse and coefficients when compared to the original log log regression model the k folds approach eliminates bias by ensuring that each datum is considered in a test set exactly one time additionally we performed a data splitting analysis to further verify the results using an 80 20 split in the data for coal the resulting rmse was slightly higher for the testing data than for the original values 0 473 compared to 0 459 additionally the modeled coefficients in the training set were relatively similar to the values reported in table 1 the supporting information table s1 provides the difference in rmse between the full model k folds validation and data splitting validation the results of these validation tests suggest high predictive power for the given log log models 5 discussion 5 1 fish kills and winter grey water footprints mapping grey water footprints to season provides additional emphasis on known trends in aquatic ecosystems downstream of power plant thermal effluent long term exposure to sub lethal environmental changes e g sustained high water temperature poses a great risk to aquatic ecosystems kulkarni et al 2011 while such sub lethal high temperature conditions are typically associated with summer wintertime changes in water temperature can lead to ecosystem changes as well notably thermally polluted waters downstream of power plants can serve as locations of refuge for fish and other species during the winter months gallaway 1974 competition for temperature as a resource can impact aquatic ecosystem diversity and health as well as individual organism health magnuson et al 1979 logan and stillwell 2018a when warm water or tropical species compete for and win habitat space e g at power plant outfalls against native species overall diversity patterns can shift power plant outfalls are known hot spots for such habitat competition emde et al 2016 furthermore warm temperatures during the winter months can lead to altered reproductive cycles in aquatic species firkus et al 2018 leaving juvenile individuals susceptible to impacts from unnatural water conditions should a power plant change operational conditions related to the difference in intake and effluent temperature e g response to changing energy demand scheduled or unscheduled maintenance fish and other organisms in the vicinity become vulnerable to rapidly changing water temperature a sudden decrease in power plant outfall temperature during the wintertime could cause the non native warm water species to find themselves in cold water leading to shock or death for example a pennsylvania power plant killed an estimated 12 000 fish during a plant shutdown in 2016 leading to a revision of cold weather shutdown protocol crable 2018 with this power plant water temperature and species response relationship in mind better accounting of ecosystem conditions could prove beneficial in maintaining and conserving aquatic species populations grey water footprints could serve as volumetric and thus quantitative proxies for potentially impacted habitat space in waterways combining the biological aspects of predictive models for the effect of temperature changes on fish such as logan and stillwell 2018a with specific power plant grey water footprints a more thorough accounting of the potential for aquatic species impact could be completed volumetric grey water footprints provide an opportunity to assess aquatic ecosystem quality and allow for species specific information such as lethal temperatures particularly the corollaries are relevant for warm water species seeking refuge in thermal pollution hot spots during winter months to serve as biological impact assessment criteria for power plant operations and or regulatory mixing zones as regulatory mixing zones tend to provide edge of zone rather than end of pipe water quality requirements epa 1991 there is the potential to underestimate the true biologic impact that thermal pollution has on species within mixing zones logan and stillwell 2018b combining grey water footprints and biological impacts has promising potential for advancing definitions of regulatory mixing zones and providing more adaptive management in areas with known sensitive and or at risk species 5 2 temperature duration curves and grey water footprints when assessing water quality biodiversity and ecosystem health are important components of overall watershed health in research areas such as river management for hydropower maintaining natural flow conditions is viewed as one of the best ways to protect biodiversity dutta et al 2020 in a similar fashion maintaining or at least mimicking natural temperature conditions is one way to encourage biodiversity downstream of thermoelectric power plants logan and stillwell 2018b introduced the concept and creation of temperature duration curves for use in modeling thermal pollution impacts from thermoelectric power plants such targeted temperature modeling can be used to assess associated ecosystem service outputs using known biodiversity and temperature linkages by adding the grey water footprint to water resource accountability and prediction temperature conditions and water consumption trends for a given watershed can be used in decision making contexts future work could investigate the incorporation of volumetric water demands in the form of grey water footprints with temperature conditions and profiles for modeling thermal pollution impacts 5 3 grey water footprints and policy watershed assessments e g evaluations of the processes influences and problems within a watershed with the intent to develop an action plan debarry 2004 serve as a planning and management space for which grey water footprints become vital to be successful with water management strategies all values and perspectives across stakeholders are required to be integrated heathcote 2009 proper accounting of thermal pollution in grey water footprints provides a proxy by which the ecosystem has a quantitative stake to watershed management oftentimes in stream beneficial use permits are where ecosystems are considered direct stakeholders in water use and management in stream beneficial use monitoring aims to identify and track water impairments as well as water quality trends oklahoma for example initiated a beneficial use monitoring program in 1998 to provide a comprehensive unified approach to water quality monitoring okl 2014 such monitoring programs in oklahoma and elsewhere could be another application of grey water footprints as a water quality metric seasonal trends in grey water footprints could be used to inform local water policies at the huc scale such trends shed light on potential inadequacies of blanket thermal regulations e g cwa 316 a as shown in fig 6 winter has a higher grey water footprint than spring and fall for most hucs due in part to the large temperature differential between intake and effluent temperatures typical seasonal npdes permits categorize temperature regulations into summer and winter seasons further consideration of seasonality might provide more accurate accounting and mitigation of thermal pollution impacts leading to a more even spread of grey water footprint by season at individual power plants when feasible furthermore specific life cycle stages of ecologically and or economically important species and holistic ecosystem health could become part of temperature metrics discussions based on ecosystem data and grey water footprint data as discussed previously winter fish kill events can be a serious issue given the alteration of normal seasonal conditions at power plant outfalls providing power plants with appropriate temperature limits and or procedures for cold weather shutdown informed by grey water footprints could reduce the severity of ecosystem damages found near power plant outfalls 6 conclusions grey water footprints provide an important component to the discussion of the energy water nexus by acting as a proxy for withdrawn water impacts on the surrounding environment however in many instances these water volumes are challenging to calculate due to a lack of available data especially source and effluent temperature therefore we present models to estimate the grey water footprint of electricity based on fuel generation and return flow these models have relatively high explanatory potential and could be used to estimate within an order of magnitude grey water footprints in environments with a lack of data while often overlooked grey water footprints have an important and growing role in understanding ecosystem impacts of the energy water nexus while this study focuses on the united states it has global implications much of the global data on water for electricity generation utilizes fuel specific information from the united states spang et al 2014 vandecasteele et al 2016 chini and stillwell 2020 in this study we found substantial evidence of seasonality with respect to grey water footprints these seasonal trends showed peaks of grey water in both the summer and winter months however in many jurisdictions there are no seasonal regulations of thermal effluent with many states only providing a singular threshold for thermal pollution limits in most locations the previously identified trend of intra annual water footprints holds true with peaks in the winter and summer months however there are some river basins where a different pattern emerges with larger water footprints in the spring months these seasonal variations support the need for seasonal thermal limits on discharge grey water footprints can inform the relative magnitude of these ecosystem impacts to facilitate seasonal rules and regulations surrounding thermal pollution finally there are significant opportunities to combine grey water with previous studies of thermal pollution to identify ecosystem impacts the grey water concept alone does not provide detailed information with respect to the impact of polluted water on downstream ecosystem service delivery launiainen et al 2014 therefore it is necessary to further evaluate grey water footprints in conjunction with other metrics we suggest further analysis of grey water footprints with large scale ecosystem impacts such as fish kills or pairing the water footprints with quantitative concepts of thermal pollution such as temperature duration curves targeted temperature modeling can be used to assess associated ecosystem service outputs using known biodiversity and temperature linkages by including the grey water footprint concepts with these water resource ecosystem models temperature conditions and water consumption trends for a given watershed can be used in decision making contexts credit authorship contribution statement christopher m chini conceptualization methodology data curation formal analysis writing original draft writing review editing visualization lauren h logan formal analysis writing original draft writing review editing ashlynn s stillwell conceptualization writing original draft writing review editing supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors thank lucas a djehdian and william n lubega for their insight and support in the initial project formulation supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103733 appendix a supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 supplementary data s2 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s2 supplementary data s3 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s3 
399,in this paper different pore network models to simulate single phase flow in porous media are built and their accuracy is evaluated in addition to the conventional pore network model cpnm which consists of regular pore bodies and throat bonds three improved pore network models ipnms are developed allowing to better describing the real pore and throat geometry the first improved pore network model ipnm1 replaces the regular throat bond with a throat bond showing the real throat cross section the second improvement ipnm2 uses a series of sub throat bonds with varying cross sections to better describe the real throat geometry which is firstly proposed in this paper the third model ipnm3 extracts the real pore throat pore geometry without simplification the conductance of fluid flow through these more realistic throat bonds is calculated by the lattice boltzmann method lbm the accuracy and computational efficiency of the different pore network models are evaluated taking the lbm simulation over the whole porous medium as reference solution the global permeability and detailed pressure distributions in the pores for the different pore network models are validated the results show that the accuracy of the pore network model increases from cpnm to ipnm3 but at the expense of increasing computational cost this study suggests that ipnm3 can replace a whole domain lbm simulation with similar accuracy but much lower computational cost as a first order approximation the newly proposed ipnm2 is suggested as good compromise between accuracy and computational cost keywords permeability porous media lattice boltzmann method pore network model watershed method 1 introduction permeability is an important property characterizing fluid flow in porous media it can be obtained mainly in three ways by experiments using analytical models or numerical simulation at pore scale although experimental testing is straightforward it has some limitations gerke et al 2018 such as measurement errors and the inability to determine the real flow regime in the porous medium analytical models predict the permeability based on physical properties of the porous media the kozeny carman k c equation carman 1939 is the most used analytical model for permeability prediction which is derived through simplifying the complex porous medium into a simple capillary bundle model the original k c model has many limitations as it only considers porosity and specific surface area later it has been modified for different kinds of porous media taking into account more detailed pore geometry information nishiyama and yokoyama 2017 xu and yu 2008 however the topology and heterogeneity of the pore structure which have a significant influence on permeability cannot be properly accounted for in these analytical models in recent years the development of imaging techniques namely x ray computer tomography ct schlüter et al 2014 has enabled to document the real pore structure of porous media with this information pore scale numerical methods can be used to calculate permeability and study the influence of pore geometry on permeability there are different kinds of numerical methods at pore scale to simulate single phase flow in porous media mainly including direct numerical methods dnm pore network models pnm and hybrid methods combining these two in some manner dnm simulates fluid flow directly in the real porous structure the lattice boltzmann method lbm is a commonly used direct numerical method for fluid flow in porous media because of the following advantages a ease of programming b natural suitability for parallel computing c ease of dealing with complex solid boundaries qin et al 2019 in pioneering work succi et al 1989 firstly adopted lbm to simulate single phase flow in three dimensional complex porous media as the simulation could recover darcy s law this work was groundbreaking towards using lbm for rocks and other porous media since then lbm has been more widely adopted to simulate fluid flow in different kinds of porous geometries nabovati et al 2009 okabe and blunt 2004 the relationships between permeability and pore structures were widely investigated bosl et al 1998 kang et al 2003 matyka et al 2008 nabovati et al 2009 van doormaal and pharoah 2009 zhao et al 2020a 2016 which can be used for macroscale flow simulations or analysis of flow mechanisms for example sun et al 2017 proposed to analyzed the gas transport patterns in shale rocks based on lbm simulation results noteworthy is the work of pan et al 2006 reporting that the intrinsic permeability obtained by single relaxation time srt lbm is strongly dependent on fluid viscosity which is unphysical this deficiency can be significantly controlled by multi relaxation time mrt lbm lbm is in fact a navier stokes n s solver as the n s equation can be recovered by lbm he and luo 1997 for low speed creeping flow which usually happens in porous media the n s equation can be simplified to the stokes equation another direct numerical method is to solve the n s equation or stokes equation directly in porous media using the finite element fem borujeni et al 2013 finite volume fvm guibert et al 2015 or finite difference mostaghimi et al 2013 zhu et al 2019 fdm methods the governing equation can be further simplified in order to avoid solving the n s or stokes equations and improve the computational efficiency shabro et al 2012 and chung et al 2019 characterized the fluid flow resistance or conductivity in each pore voxel using two parameters namely the distance to the pore wall and the largest inscribed radius of the corresponding pore then the simplified elliptic flow equation is solved using the fdm or fvm directly on the micro ct images the obtained permeability is within acceptable error compared with that of other more accurate solvers arns et al 2018 used a laplace solver using only the euclidean distance map to calculate the local conductivity of each voxel which is a fast and stable approach even under low spatial resolution these methods were lately combined with dual grid domain decomposition method da wang et al 2019 or voxel agglomeration method chung et al 2019a to further improve the computational efficiency although such numerical methods are promising in calculating the permeability of single phase flow their applicability to two phase flow simulation is still unclear pore network models pnm belong to another type of pore scale numerical method which are much more computationally efficient than direct numerical methods blunt et al 2013 instead of simulating fluid flow directly in real pore structures pnm simulates fluid flow in networks of simplified pores which are constructed based on real pore geometry of porous media the complex pore structures are simplified into regular pore bodies connected by throat bonds in conventional pnm the cross sections of the pore bodies and throat bonds are described by circles squares triangles or other regular geometries the fluid transport properties in such simple pore bodies and throat bonds can be calculated by analytical or empirical expressions lal et al 2019 øren et al 1998 rajaram et al 1997 valvatne 2004 this significantly reduces the computational cost on the other hand the conductance predicted with simplified pore bodies and throat bonds is less accurate than with a description based on the real pore geometry leading to errors in the predicted transport properties of the porous medium as described above dnms can accurately capture the details of pore structures at the expense of computational efficiency on the contrary pnm is computationally efficient but less accurate in accounting for the effect of real pore geometry on fluid flow in recent years some hybrid methods have been developed which combine the strengths of both dnm and pnm for single phase flow in porous media the flow resistance increases in more constricted channels which means that the throats with smaller cross sections account for most of the flow resistance therefore it is essential to describe the throat geometry as accurate as possible as a result different improvements of pore network models have been proposed using a more realistic representation of the throat bonds miao et al 2017 and rabbani et al 2019 extracted throat bonds with real throat cross sections instead of using regular shapes for pore network simulation then dnm was used to calculate the conductance of these real throat bonds in addition neural network models are developed to derive correlations between throat geometry information and conductance based on a large number of simulations sholokhova et al 2009 and van marcke et al 2010 extracted the pore throat pore elements without any geometry simplification and used dnm to simulate single phase flow and calculate the conductance in each element the obtained conductances are in turn introduced into pnm to calculate the permeability of the whole porous medium this kind of pore network models which uses more accurate geometry information yields a more accurate prediction of the permeability compared with cpnm while the computational cost is lower compared to dnm in addition the improved pore network models can be easily extended to two phase flow simulations recently zhao et al 2020b extended the improved pore network model with real throat cross section to two phase drainage flow simulation which showed better prediction results than the cpnm it is true that with modern high performance clusters a highly optimized lbm code can simulate single phase flow in real pore structures rather fast one exception is that when the porous structure is very tight it is hard for the lbm to converge at such conditions an accurate pnm becomes a good option to calculate the permeability for example song et al 2016 proposed a unified apparent permeability calculation model for shale gas reservoir by incorporating various transport mechanisms which was used in pnm simulations in addition for two phase flow in porous media lbm can be much more computational expensive and the advantage of pnm becomes more significant the dynamic pnm can be used to simulate two phase flow in porous media taking into account both the capillary force and viscous force joekar niasar et al 2010 in dynamic pnm for two phase flow single phase pnm is used to calculate the pressure distributions in the wetting and non wetting phases while maintaining a capillary pressure between the two phases e g joekar niasar et al 2010 accurate pressure distribution is needed as this information is used to update the phase distribution however to our knowledge there is still no research reporting on the accuracy of using single phase pnm to simulate pressure distribution therefore we focus on improved pore network models to simulate single phase flow in porous media in this work we not only evaluate the improvements in terms of calculating the permeability but also the detailed fluid pressure distribution four pore network models are evaluated in this work including the conventional pnm and three improved pnms for the improved pore network models in addition to using throat bonds showing real throat cross section and the real pore throat pore geometry without simplification we propose a third option with a new hybrid pnm approach which uses serial sub throat bonds with real throat cross sections to describe the cross section variation between two pore bodies the paper is arranged as follows in section 2 we introduce different pore network extraction methods in section 3 the numerical methods are introduced including the multi relaxation time lattice boltzmann model the conventional pore network model and the three improved pore network models the coupling procedure between lbm and ipnms is described in detail in section 4 the performance of the different pnms to predict the permeability and pressure distributions is discussed in detail finally conclusions are summarized in section 5 2 pore network extraction there are different methods to extract a pore network from a real porous geometry such as the medial axis based method lindquist et al 1996 maximum ball based method dong and blunt 2009 watershed based method gostick 2017 rabbani et al 2014 in this work we use the watershed method to decompose the porous media into sub pore regions and extract the pore network in addition to the conventional pore network model three different improved pore network models are presented in this work as shown in fig 1 firstly the porous medium is decomposed into sub pore regions using the watershed method which has been implemented in the open source algorithm snow gostick 2017 this algorithm includes four steps first step is to obtain the distance map and apply a gaussian filter to remove some fake watershed ridge lines then a maximum filter is applied to eliminate peaks on saddles and plateaus we set the standard deviation of gaussian filter as 0 4 and the radius of structuring element as 5 in this work next the peaks close to each other are merged into one peak finally the porous medium is segmented into sub pore regions more details about this algorithm can be found in gostick 2017 using the watershed method the pore space is split into sub pore regions as illustrated in fig 1 b each identified sub pore region corresponds to a pore body in the extracted pore network the sub pore regions are connected with narrow interfaces which correspond to the throat bonds in the pore network 2 1 conventional pore network extraction with simplified pore and throat geometry in the conventional pore network model cpnm both pore bodies and throat bonds are simplified into regular geometries under some constraints as shown in fig 1 c in this work all the pore bodies are taken as cubes with the volume equal to that of the corresponding sub pore region throats usually account for most of the flow resistance as they have much smaller cross sections than the pore bodies therefore it is assumed that the pore bodies have no flow resistance and that all flow resistance is located at the throat bonds similar assumptions have been used in other research joekar niasar et al 2010 the inscribed radius rin and shape factor g of the throat cross sections are adopted to simplify the real throat cross sections into regular geometries such as circle square and triangle valvatne 2004 the inscribed radius rin is defined as the radius of inscribed circle in the throat cross section the shape factor g is defined as a p 2 where a and p are the area and perimeter of the throat cross section respectively based on g the throat cross sections can be simplified into triangle g 3 36 square 3 36 g 1 16 and circle g 1 16 respectively after the geometry simplification the parameters a and p of the throat bonds are recalculated based on the simplified cross sections the length of throat bonds is calculated by lij r in i r in j where lij is the distance between the centroids of the connecting two pore bodies r in i and r in j are the inscribed radii of these two pore bodies 2 2 improved pore network extraction with real throat cross section one way to improve the accuracy of pore network model is to replace the regular throat bonds with throat bonds with real throat cross sections as shown in fig 1 d this improvement has also been studied recently by miao et al 2017 rabbani and babaei 2019 we refer to this first improved pore network model as ipnm1 hereafter to obtain the real cross section of the throat bonds we firstly connect the centroids of the connecting two pore bodies with a straight line shown as dashed blue line in fig 1 b then a plane perpendicular to this line is inserted and moved from centroid 2 to 3 and the varying cross sections along this line are obtained we select the cross section with minimum inscribed radius as the real throat cross section because the minimum cross section has most significant influence on the flow resistance the lengths of the throat bonds are calculated in the same manner as those in cpnm as there is no analytical or empirical equations to calculate the conductances of these real throat bonds lbm is used to calculate such conductance as described in section 3 2 2 3 improved pore network extraction with real pore and throat geometry to retain as much geometrical information as possible the best way is to extract the real pore throat pore element without any geometry simplification as shown in fig 1 f to extract the pore throat pore element first a straight line connecting the two pore bodies is also determined then two planes perpendicular to this line are inserted at the pore centroids 2 and 3 and all pore voxels in between these two planes are extracted while the other voxels between these two planes are considered as solids the extracted pore throat pore element is placed in a cubic simulation domain to calculate the conductance as shown in figs 1 f and 4 2 4 improved pore network extraction with serial sub throat bonds as more geometrical information is maintained in the improved pore network model in previous section the computational efficiency becomes much lower as demonstrated in section 4 2 to reach a balance between the accuracy and computational efficiency we propose a new improved pore network model for a real throat connecting two pore bodies the shape of the cross section varies from one pore centroid to the other however both the cpnm and ipnm1 use only one throat cross section to calculate the flow resistance which means that the variation of throat cross section between the two pore bodies is not captured to capture this information we propose a new pore network model which uses several throat bonds in series between the two pore bodies as shown in fig 1 e to determine the shapes of the bonds the procedure is similar to the one in section 2 2 first a straight line is set connecting two pore bodies then the total throat bond is divided into several sub throat bonds each with length lsub the cross section retained for each sub throat bond is the one with minimum inscribed radius as shown in fig 1 e and its flow resistance is determined the overall flow resistance is calculated by the harmonic mean of the conductances of all sub throat bonds which will be further elaborated in section 3 2 3 as the accuracy and the computer efficiency of this model lie in between those of ipnm1 and of the ipnm presented in section 2 3 we call this improved pore network model 2 ipnm2 in the following analysis while the model described in section 2 3 as improved pore network model 3 ipnm3 3 numerical methods 3 1 lattice boltzmann method a multi relaxation time mrt single phase lb model is used to simulate fluid flow in the throat bonds and calculate their conductances it is also used to simulate fluid flow in the whole porous medium to provide a reference solution for evaluating the accuracy of the different pore network models here the lb model is briefly introduced the evolution equation is shown below 1 f r e α δ t t δ t f r t m 1 s m r t m e q r t m 1 i s 2 m f δ t where f f 0 f 1 f 18 t andf α is the density distribution function of α direction ris the spatial location e α is the velocity of the α direction tis the time i is the identity matrix δ t is the time step the d3q19 velocity model is adopted in this work f is the external body force which acts as the driven force in ipnm1 and 2 to calculate the throat conductance and it can be expressed as 2 f f 0 f 1 f 18 t f α ρ w α e α a c s 2 ua e α e α c s 2 i c s 4 δ t where w α is the weight factor in the α direction ρ is the macroscopic density a is the acceleration due to externalforces c s 1 3 is the speed of sound m is a 19 19 transformation matrix to map the vector f in the discrete velocity space to the vector m in the moment space s is the diagonal collision matrix the halfway bounce back boundary condition is adopted on solid surfaces which will generate no slip velocities the accuracy of using lbm to simulate single phase flow is verified by comparing simulation results with analytical solutions in simple geometries as illustrated in the appendix in all simulations and analyses of the following paragraphs lattice units are used unless otherwise specified the lattice units can be easily converted into physical units given the time scale t 0 length scale l 0 and mass scale m 0 zhao et al 2018 for example the viscosity in lattice units can be converted into physical units by μ p h y μ l b m m 0 l 0 t 0 3 2 pore network model for single phase flow through a throat bond connecting two pore bodies i and j the volume flux qij is proportional to the pressure difference in the two pore bodies which can be expressed by 3 q i j g i j p i p j where gij is the throat conductance pi and pj are the pressures in pores i and j respectively under steady state the mass balance equation in each pore body i should be satisfied which means 4 j q i j j g i j p i p j 0 for pressure driven fluid flow the pressures at the inlet and outlet pores are constant 5 p i i n l e t p i n p i o u t l e t p o u t combining eqs 3 5 we can solve the pressure field in the whole pore network then the volume flux across the pore network and the corresponding permeability can be calculated it is obvious that the accuracy of single phase pore network model depends on the throat conductance gij the difference of different pore network models lies in the calculation of gij which will be described in following paragraphs 3 2 1 conventional pore network model cpnm as introduced in section 2 2 in cpnm the complex throat cross sections are simplified into regular shapes such as circle square and triangle for single phase flow in such regular throat bonds the throat conductance can be calculated by valvatne 2004 6 g c a 2 g μ l where c is a modification coefficient and c 0 5 0 5623 and 0 6 respectively for circular square and triangular throat bonds l is the throat length a is the cross section area of the throat bond and µ is the dynamic fluid viscosity as there is no analytical or empirical expressions to calculate the throat conductance in the improved pore network models lbm is used to simulate single phase flow in these real throat bonds and calculate their conductances 3 2 2 improved pore network model 1 ipnm1 in ipnm1 the throat bond with real cross section is considered to calculate the conductance the model shown in fig 2 a is developed by extending the real throat cross section in x direction with a short distance namely 3 lattices in this work this throat cross section is extracted from the pore throat pore element shown in fig 4 in the lbm simulation the fluid density is set as 1 0 and the kinetic viscosity is set as 0 5 a small external acceleration ax 1 0 10 4 is imposed in x direction and periodic boundary conditions are considered the volume flux q across this throat bond can be calculated by integrating the velocity distribution in the slice at x 0 when the relative volume flux change in 100 consecutive iteration time steps is less than 10 6 conditions referred to as the stop criterion the simulation is assumed to have reached the steady state this simulation can reach steady state in a short time as there is no velocity gradient in x direction and the simulation domain is small the throat conductance is then given by 6 g μ q ρ a x l μ l b m μ where ρ is the fluid density in the lbm simulation l is the length of throat bond as the throat conductance is related to the fluid viscosity the last term of eq 6 above is used to rescale the throat conductance calculated in lbm simulation with the dynamic viscosity µ lbm to the one in pnm simulation with the dynamic viscosity µ as this work considers single phase flow the dynamic viscosity has no influence on the calculation of permeability and pressure distribution and we set µ µ lbm 0 5 in lattice units 3 2 3 improved pore network model 2 ipnm2 in ipnm2 we assume two pore bodies are connected through several sub throat bonds in series the cross section of each sub throat bond is extracted from the digital rock with its real geometry the length lij between two pore centroids of connecting pore bodies i and j is divided into nsub lij lsub sub throat bonds to check the influence of lsub on the throat conductance we calculate the conductances of different pore throat pore elements with different lsub ranging from 8 to 3 it is found that there is very minor change in throat conductance after lsub 5 therefore we set lsub 4 in the following simulation and analysis as an example for the pore throat pore element shown in fig 4 we obtain 6 sub throat bonds with lsub 4 the cross section of each sub throat bond is shown in fig 3 then lbm is used to simulate single phase flow and flow resistance for each sub throat bond eq 7b the overall flow resistance is calculated as the harmonic mean of the conductances of all sub throat bonds 7 g μ 1 k 1 g k μ g k μ q k ρ a x l k μ l b m μ where g k µ is the conductance of sub throat bond k lk is the length of sub throat bond k 3 2 4 improved pore network model 3 ipnm3 in ipnm3 the complete pore throat pore element is extracted fig 4 shows one pore throat pore element randomly selected and extracted from a fontainebleau sandstone digital rock shown in fig 5 e lbm is used to determine the pressure driven single phase flow imposing an inlet pressure pin at the left boundary and an outlet pressure pout at the right boundary the volume flux q across this pore throat pore element is calculated by integrating the velocity at the inlet outlet boundary the same stop criterion as in section 3 2 2 is adopted to minimize the compressible effect we use a small inlet outlet pressure ratio 1 01 in the simulation in addition the average volume flux across the inlet and outlet boundaries is used in this simulation more time steps are needed to reach steady state as the simulation domain is much larger and more complex the conductance of this pore throat pore element is calculated by 8 g μ q p i n p o u t μ l b m μ 4 results and analysis the three different models combining pnm and lbm are run on three different digital porous materials and the results of permeability pressure distribution and throat conductance are compared with results of simulations with conventional pnm and whole domain lbm 4 1 digital rocks and pore networks the different pnms used to simulate single phase flow are based on three different digital rocks a berea sandstone berea sandstone 2008 a fontainebleau sandstone berg 2016 and a bentheimer sandstone muljadi 2015 as shown in fig 5 a c the three digital rocks are available from online resources to save computational time and for better comparison a subdomain of size of 300 200 200 voxels is extracted from each of the digital rock voxels are then directly considered as lattices grids to build the computational domain in the lbm simulation two buffer zones of 10 lattices are added at the inlet and outlet of each domain the corresponding digital rock samples are shown in fig 5 d f and figures g i show the extracted pore networks for each pore network the number of pore bodies are 854 653 and 812 and the number of throat bonds are 1791 1480 and 1899 respectively it should be mentioned that the bentheimer sandstone digital rock is upscaled from 1000 1000 1000 voxels to 500 500 500 voxels in order to include more pore bodies in the extracted pore network which means eight neighboring voxels in the original dataset are merged into one voxel in the rescaled digital rock in the rescaling process the upscaled voxel is assigned as pore voxel if the fraction of pore voxels in original dataset is larger than 50 the main goal of this work is to evaluate the accuracy and efficiency of different pnms to simulate single phase flow in porous media through comparing with whole domain lbm simulation if the prediction results of pnm are already accurate for smaller domains it will produce even more accurate results for larger domains therefore we do not implement the representative element volume rev test here first lbm is used to simulate fluid flow in the whole digital rocks shown in fig 5 d f the left boundary is considered as inlet and right as outlet and pressure boundary conditions are imposed on both sides at initial condition a linear pressure distribution from inlet to outlet is considered when the same stop criterion as above is met it is assumed that the steady state is reached then the permeability is calculated based on darcy equation these lbm simulation results are considered as reference solutions in the following analysis next the different pore network models are extracted the conductance of each throat bond is calculated and the fluid flow and permeability of the porous medium is determined the results are analyzed in following sections when comparing the simulation results of different models we use the same digital rock thus the same resolution 4 2 performance comparison of different numerical methods table 1 summarizes the performances of the different pnms introduced in section 3 the relative error of different pore network models is calculated by e r k p n m k l b m k l b m 100 with k the permeability all the lbm simulationsare executed on the piz daint from the swiss national supercomputing centre the computational times shown in the table are transformed to equivalent one cpu time by multiplying the time for simulation with the number of cpus in general for the improved pore network models the simulation time increases with the throat number it should be noted that both the time for pore network extraction and the time for conductance calculation using lbm are included in the computational time for the different pore network models as shown in table 1 all the ipnms improve the prediction of permeability of the digital rocks compared to the cpnm results because they capture more accurate information on the pore geometry than cpnm both the cpnm and ipnm1 underestimate the permeability although ipnm1 shows a better performance than cpnm in predicting permeability its relative error is still around 25 to 45 the main reason for the underestimation is that the first two pore network models assume only the minimum cross section of the throat between the two pore bodies when considering also the cross section variation of the throats between the two pore bodies in ipnm2 the accuracy significantly improves with only a little increase in computational cost compared to ipnm1 as ipnm3 considers the real pore throat pore geometry it shows the best accuracy predicting similar results as the reference lbm however at a much higher computational cost compared to ipnm1 and ipnm2 compared with the reference lbm ipnm3 shows a much lower computational cost therefore ipnm3 can be considered to replace whole domain lbm simulation for predicting the permeability of a digital rock with almost the same accuracy while requiring much lower computational cost however to obtain a first order prediction of the permeability the newly proposed ipnm2 is considered as the best choice showing the best compromise between accuracy and computational cost we also observe that the predicted permeability of ipnm2 for the fontainebleau sandstone is less accurate than those predicted for the berea and bentheimer sandstones analyzing the porous structures and the pore network extraction processes for the different digital rocks we found that the accuracy of ipnm2 is related to the structure of the extracted pore network the numbers of extracted pore bodies and throat bonds change with the standard deviation of gaussian filter and the radius of structuring element in the watershed method the accuracy of ipnm2 can be improved if optimal pore network extraction parameters are selected however even with a less favorable extracted pore network as shown in table 1 the accuracy of ipnm2 is still much higher than those of cpnm and ipnm1 an analysis of the sensitivity of pore network extraction process on the accuracy of the permeability calculation by the different pore network models is although an important topic not the focus of the present paper and should be studied in future 4 3 pressure distribution comparison of different models as stated in the introduction it is important to obtain accurate prediction of the pressure distribution when simulating dynamic two phase flow in porous media in this section we evaluate the accuracy of the different pore network models to simulate the pressure distribution fig 6 a c show the dimensionless pressure distributions in different digital rocks obtained by lbm simulations fig 6 d f show the corresponding simulation results obtained by ipnm3 as all the pore network models yield visually similar results we only plot the pressure distribution for ipnm3 for illustration to quantitatively compare the results for lbm simulations the fluid pressure in each pore body is calculated by volume averaging as 9 p i l b m r r i p l b m r v i where p i l b m is the volume averaged fluid pressure in pore body i plbm r is the fluid pressure in position r r i is the coordinate set of region i vi is the total volume of pore region i we take the lbm simulation results as reference value and compare the pressure distributions for the different rocks and pnms as shown in fig 7 in all the figures x represents the fluid pressure obtained by lbm y represents the corresponding pressure obtained by the pore network model from top to bottom the pore network models are cpnm ipnm1 ipnm2 and ipnm3 respectively from left to right the rock types are berea sandstone fontainebleau sandstone and bentheimer sandstone respectively the mean absolute relative error mare for each case is calculated as 10 mare 1 n p i 1 n p p i p n m p i l b m p i n p o u t 100 where p i l b m is the reference pressure of pore i obtained by lbm p i p n m is the pressure of pore i simulated by pnm np is the total number of pore bodies for evaluation pin and pout are the inlet and outlet pressures respectively the mare values for different cases are shown in table 2 as can be seen from the top two rows of fig 7 the pressure distributions obtained by both cpnm and ipnm1 deviate more from the reference lbm results compared to ipnm2 and ipnm3 shown in the bottom two rows although ipnm1 was found to be more accurate in predicting the permeability compared to cpnm the mares of pressure for ipnm1 are even worse as shown in table 2 meaning more errors for ipnm1 in predicting the pressure distribution the reason for this observation will be analyzed in the next section as the ipnm2 and ipnm3 capture more geometry information they yield more accurate pressure distributions the mares for these two pore network models are much smaller especially for ipnm3 this comparison also confirms the conclusion obtained in previous section that the ipnm3 can replace whole domain lbm simulation for single phase flow in porous media while the ipnm2 is a good first order approximation for single phase flow in porous media 4 4 throat conductance comparison of different pore network models as mentioned in section 3 2 the accuracy of single phase pore network model depends highly on the throat conductance according to the results in sections 4 2 and 4 3 ipnm3 can produce results similar to the ones of the reference lbm in terms of both permeability and pressure distribution this means that the prediction of the throat conductance by ipnm3 is most accurate compared with the other three pore network models reason being that ipnm3 uses the real pore throat pore geometry in this section we consider the throat conductance of ipnm3 as reference values and compare these with the throat conductances predicted by the other pore network models the comparison results are shown in fig 8 as can be seen from fig 8 for cpnm most throat bonds yield an underestimated value of the throat conductance especially for small throat bonds accordingly the permeability of the whole porous medium is underestimated in general the throat conductances calculated by ipnm1 are higher than those obtained by cpnm therefore ipnm1 predicts a higher permeability than cpnm obtaining values closer to the reference values obtained by lbm however there is a crossover for the conductance calculated by ipnm1 and the reference values calculated by ipnm3 for small throat bonds ipnm1 underestimates the throat conductance while for large throat bonds ipnm1 overestimates the throat conductance due to this phenomenon the pressure distribution obtained by ipnm1 deviates substantially from those obtained by ipnm3 and lbm compared with the first two pore network models the data obtained by ipnm2 are less scattered predicting throat conductances similar to those of ipnm3 for both small throat bonds and large throat bonds therefore ipnm2 gives much better prediction results than cpnm and ipnm1 in terms of both permeability and pressure distribution we take the fontainebleau sandstone as an example to further analyze the crossover of throat conductance for ipnm1 observed in fig 8 in ipnm1 the flow resistance between two pore bodies is determined only by the throat conductance as the flow resistances in the pore bodies are neglected for a certain throat bond its conductance depends on two parameters its cross section and length fig 9 shows schematically two different pore throat pore structures and the resulting extracted pore network in ipnm1 the cross section of a throat bond is taken as the minimum cross section between two connecting pore bodies and the length of the throat bond is calculated by lij r in i r in j as stated in section 2 1 we use three parameters to describe the structure of the throat bond and its relationship with the connecting pore bodies which are the radius ratio between throat bond and the connecting pore bodies r throat r pore the ratio between throat length and the distance connecting the centroids of the two pore bodies l throat lij and the ratio between throat length and throat diameter l throat d throat these throat characteristics are plotted for all throat bonds of the fontainebleau sandstone pore network versus throat conductance in fig 10 in fig 10 a the ratio r throat r pore generally increases with throat conductance meaning that throat bond much smaller that connecting pore bodies results in lower conductance in contrast both the ratio s l throat lij and l throat dij generally decrease with increasing throat conductance thus a relatively shorter throat bond results in larger throat conductance as shown in fig 10 b and c using the schematic of fig 9 b for throat bonds with small conductance e g throat bond t1 2 the inscribed radius of this throat bond is much smaller than those of the connecting pore bodies p1 and p2 therefore the ratio r throat r pore is small and the flow resistance in the connecting pore bodies can be neglected remark that the cross section varies along the long throat bond while it is assumed in ipnm1 that it equals the minimum throat cross section in the throat bond t1 2 accordingly the throat bond t1 2 underestimates the real throat conductance while a large throat bond as illustrated by the throat bond t2 3 in fig 9 b has an inscribed radius similar to the radii of the connecting pore bodies p2 and p3 in addition the l throat lij is relative small meaning that the radii of the pore bodies are similar to the length of throat bond therefore the flow resistances in the throat bond and the connecting pore bodies are of the similar magnitude however the flow resistances in the connecting pore bodies are ignored in ipnm1 resulting in an overestimation of the throat conductance the above description provides an explanation for the crossover of throat conductance for ipnm1 as ipnm2 considers the cross section variation between the centroids of the connecting two pore bodies it actually adjust for this crossover and predicts throat conductance more accurately 4 5 velocity distribution comparison of different pore network models pore network models have been often used to simulate solute transport for which the single phase pnm is used to provide the flux or velocity distribution köhne et al 2011 qin and hassanizadeh 2015 in this section we analyze the velocity distributions obtained by different pnms we take the simulation results in the berea sandstone digital rock as an example the velocity distributions obtained by different models are shown in fig 11 the velocity in each throat bond in the pore network model is calculated by v t q t a t where qt and at are the volume flux across the throat bond and the area of throat cross section respectively we remark that the different pnms only display the mean velocities across the throats with minimum cross section which are larger than the ones in other cross sections therefore it is reasonable that the velocity magnitude of ipnm3 shown in fig 11 e is visually larger than that of the lbm simulation results shown in fig 11 a simulation results between lbm and pnm cannot be compared throat by throat as the lbm velocities are not uniform over the throat alternatively we take the simulation result of ipnm3 as the reference value and compare the velocity distributions obtained by the different pnms as shown in fig 11 b e the velocity distributions obtained by ipnm2 and ipnm3 are roughly similar but the velocity distributions obtained by ipnm1 and cpnm are significantly different with the ones obtained by ipnm3 especially for cpnm which produces much smaller velocities than the ipnms taking the velocity distribution obtained by ipnm3 as the reference value we further compare the velocity distributions obtained by the other pnms throat by throat in the plots of fig 12 we note that a few data points with values smaller than 1 0e 8 are excluded from this figure for better view according to the comparison results the cpnm significantly underestimates the velocity in each throat bond due to the fact that the cpnm predicts smaller throat conductance than ipnm3 the ipnm1 predicts better the velocities than cpnm the dataset of ipnm2 is least scattered and shows the best agreement with ipnm3 the comparison results are similar to those of throat conductance shown in fig 8 as the velocity distribution is determined by the throat conductance 5 conclusion in this paper we performed a comprehensive investigation of the accuracy of different pore network models to simulate single phase flow in porous media four different pore networks are extracted based on the watershed method the conventional pore network model cpnm uses regular shaped pore bodies and throat bonds the first improved pore network model ipnm1 replaces the regular shaped throat bonds with throat bonds showing their real minimal throat cross sections the second improved model ipnm2 which is firstly proposed in this work uses a series of sub throat bonds to describe the cross section variation between two pore bodies the improved pore network model 3 ipnm3 directly extracts the real pore throat pore element without any geometry simplification the throat conductance in the three improved pore network models is calculated by lbm the performance of different pnms to simulate single phase flow in porous media is evaluated taking the lbm simulation results in the whole porous media as reference solutions from cpnm to ipnm3 the accuracy to predict the permeability increases as more accurate information on pore geometry is used at the expense of computational efficiency in addition to the permeability the pressure distribution obtained by different pnms is compared pore by pore with the reference lbm results the results show that ipnm2 and ipnm3 achieve more accurate pressure distributions compared with those obtained by cpnm and ipnm1 although ipnm1 can predict more accurate permeability than cpnm the relative errors in predicting the pressure distributions are even worse it was shown that cpnm underestimates the throat conductance for most of the throat bonds as compared to those calculated by ipnm3 ipnm1 underestimates the throat conductance for small throat bonds and overestimates the throat conductance for large throat bonds due to the structure difference for small throat bonds and large ones ipnm2 which takes into account the cross section variation between the connecting pore bodies obtains very similar throat conductance values as those determined by ipnm3 in summary ipnm3 can replace lbm to simulate single phase flow in porous media with almost the same accuracy but at much lower computational cost however to obtain a first order approximation of single phase flow in porous media in terms of both permeability and pressure distribution the newly proposed ipnm2 is a better choice with the best balance between accuracy and computational cost the results presented in this work by comparing different pore network models can help researchers and engineers to choose the adequate pore network model for their specific purpose in addition the different improvements presented here for single phase pore network models can be incorporated into other pore network models for more complicated flow or transport processes such as multiphase flow solute transport etc which should be studied in future work declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper the authors declare the following financial interests personal relationships which may be considered as potential competing interests acknowledgements this work was supported by the swiss national science foundation project no 175793 and the swiss national supercomputing centre project no s823 q k acknowledges the support of lanl s ldrd program the data associated with this paper are made available from https www research collection ethz ch handle 20 500 11850 401218 appendix we conduct external force driven single phase flow through circular tubes with different radii to verify the accuracy of using lbm to calculate the conductance the simulation domain is similar to that shown in fig 2 and we replace the cross section with circles of different radii a small external acceleration ax 1 0 10 4 is imposed in x direction and periodic boundary condition is considered when the relative volume flux change in 100 consecutive iteration time steps is less than 10 6 the simulation is assumed to have reached the steady state then the permeability of fluid flow through this tube can be calculated by 11 k l b m q μ π r i n 2 ρ a x the calculated permeability by lbm is plotted in fig 13 as red squares where the solid line shows the theoretical solution which is k theory r i n 2 8 as can be seen the simulation results match well with theoretical solutions demonstrating the accuracy of using lbm to calculate conductance of single phase flow 
399,in this paper different pore network models to simulate single phase flow in porous media are built and their accuracy is evaluated in addition to the conventional pore network model cpnm which consists of regular pore bodies and throat bonds three improved pore network models ipnms are developed allowing to better describing the real pore and throat geometry the first improved pore network model ipnm1 replaces the regular throat bond with a throat bond showing the real throat cross section the second improvement ipnm2 uses a series of sub throat bonds with varying cross sections to better describe the real throat geometry which is firstly proposed in this paper the third model ipnm3 extracts the real pore throat pore geometry without simplification the conductance of fluid flow through these more realistic throat bonds is calculated by the lattice boltzmann method lbm the accuracy and computational efficiency of the different pore network models are evaluated taking the lbm simulation over the whole porous medium as reference solution the global permeability and detailed pressure distributions in the pores for the different pore network models are validated the results show that the accuracy of the pore network model increases from cpnm to ipnm3 but at the expense of increasing computational cost this study suggests that ipnm3 can replace a whole domain lbm simulation with similar accuracy but much lower computational cost as a first order approximation the newly proposed ipnm2 is suggested as good compromise between accuracy and computational cost keywords permeability porous media lattice boltzmann method pore network model watershed method 1 introduction permeability is an important property characterizing fluid flow in porous media it can be obtained mainly in three ways by experiments using analytical models or numerical simulation at pore scale although experimental testing is straightforward it has some limitations gerke et al 2018 such as measurement errors and the inability to determine the real flow regime in the porous medium analytical models predict the permeability based on physical properties of the porous media the kozeny carman k c equation carman 1939 is the most used analytical model for permeability prediction which is derived through simplifying the complex porous medium into a simple capillary bundle model the original k c model has many limitations as it only considers porosity and specific surface area later it has been modified for different kinds of porous media taking into account more detailed pore geometry information nishiyama and yokoyama 2017 xu and yu 2008 however the topology and heterogeneity of the pore structure which have a significant influence on permeability cannot be properly accounted for in these analytical models in recent years the development of imaging techniques namely x ray computer tomography ct schlüter et al 2014 has enabled to document the real pore structure of porous media with this information pore scale numerical methods can be used to calculate permeability and study the influence of pore geometry on permeability there are different kinds of numerical methods at pore scale to simulate single phase flow in porous media mainly including direct numerical methods dnm pore network models pnm and hybrid methods combining these two in some manner dnm simulates fluid flow directly in the real porous structure the lattice boltzmann method lbm is a commonly used direct numerical method for fluid flow in porous media because of the following advantages a ease of programming b natural suitability for parallel computing c ease of dealing with complex solid boundaries qin et al 2019 in pioneering work succi et al 1989 firstly adopted lbm to simulate single phase flow in three dimensional complex porous media as the simulation could recover darcy s law this work was groundbreaking towards using lbm for rocks and other porous media since then lbm has been more widely adopted to simulate fluid flow in different kinds of porous geometries nabovati et al 2009 okabe and blunt 2004 the relationships between permeability and pore structures were widely investigated bosl et al 1998 kang et al 2003 matyka et al 2008 nabovati et al 2009 van doormaal and pharoah 2009 zhao et al 2020a 2016 which can be used for macroscale flow simulations or analysis of flow mechanisms for example sun et al 2017 proposed to analyzed the gas transport patterns in shale rocks based on lbm simulation results noteworthy is the work of pan et al 2006 reporting that the intrinsic permeability obtained by single relaxation time srt lbm is strongly dependent on fluid viscosity which is unphysical this deficiency can be significantly controlled by multi relaxation time mrt lbm lbm is in fact a navier stokes n s solver as the n s equation can be recovered by lbm he and luo 1997 for low speed creeping flow which usually happens in porous media the n s equation can be simplified to the stokes equation another direct numerical method is to solve the n s equation or stokes equation directly in porous media using the finite element fem borujeni et al 2013 finite volume fvm guibert et al 2015 or finite difference mostaghimi et al 2013 zhu et al 2019 fdm methods the governing equation can be further simplified in order to avoid solving the n s or stokes equations and improve the computational efficiency shabro et al 2012 and chung et al 2019 characterized the fluid flow resistance or conductivity in each pore voxel using two parameters namely the distance to the pore wall and the largest inscribed radius of the corresponding pore then the simplified elliptic flow equation is solved using the fdm or fvm directly on the micro ct images the obtained permeability is within acceptable error compared with that of other more accurate solvers arns et al 2018 used a laplace solver using only the euclidean distance map to calculate the local conductivity of each voxel which is a fast and stable approach even under low spatial resolution these methods were lately combined with dual grid domain decomposition method da wang et al 2019 or voxel agglomeration method chung et al 2019a to further improve the computational efficiency although such numerical methods are promising in calculating the permeability of single phase flow their applicability to two phase flow simulation is still unclear pore network models pnm belong to another type of pore scale numerical method which are much more computationally efficient than direct numerical methods blunt et al 2013 instead of simulating fluid flow directly in real pore structures pnm simulates fluid flow in networks of simplified pores which are constructed based on real pore geometry of porous media the complex pore structures are simplified into regular pore bodies connected by throat bonds in conventional pnm the cross sections of the pore bodies and throat bonds are described by circles squares triangles or other regular geometries the fluid transport properties in such simple pore bodies and throat bonds can be calculated by analytical or empirical expressions lal et al 2019 øren et al 1998 rajaram et al 1997 valvatne 2004 this significantly reduces the computational cost on the other hand the conductance predicted with simplified pore bodies and throat bonds is less accurate than with a description based on the real pore geometry leading to errors in the predicted transport properties of the porous medium as described above dnms can accurately capture the details of pore structures at the expense of computational efficiency on the contrary pnm is computationally efficient but less accurate in accounting for the effect of real pore geometry on fluid flow in recent years some hybrid methods have been developed which combine the strengths of both dnm and pnm for single phase flow in porous media the flow resistance increases in more constricted channels which means that the throats with smaller cross sections account for most of the flow resistance therefore it is essential to describe the throat geometry as accurate as possible as a result different improvements of pore network models have been proposed using a more realistic representation of the throat bonds miao et al 2017 and rabbani et al 2019 extracted throat bonds with real throat cross sections instead of using regular shapes for pore network simulation then dnm was used to calculate the conductance of these real throat bonds in addition neural network models are developed to derive correlations between throat geometry information and conductance based on a large number of simulations sholokhova et al 2009 and van marcke et al 2010 extracted the pore throat pore elements without any geometry simplification and used dnm to simulate single phase flow and calculate the conductance in each element the obtained conductances are in turn introduced into pnm to calculate the permeability of the whole porous medium this kind of pore network models which uses more accurate geometry information yields a more accurate prediction of the permeability compared with cpnm while the computational cost is lower compared to dnm in addition the improved pore network models can be easily extended to two phase flow simulations recently zhao et al 2020b extended the improved pore network model with real throat cross section to two phase drainage flow simulation which showed better prediction results than the cpnm it is true that with modern high performance clusters a highly optimized lbm code can simulate single phase flow in real pore structures rather fast one exception is that when the porous structure is very tight it is hard for the lbm to converge at such conditions an accurate pnm becomes a good option to calculate the permeability for example song et al 2016 proposed a unified apparent permeability calculation model for shale gas reservoir by incorporating various transport mechanisms which was used in pnm simulations in addition for two phase flow in porous media lbm can be much more computational expensive and the advantage of pnm becomes more significant the dynamic pnm can be used to simulate two phase flow in porous media taking into account both the capillary force and viscous force joekar niasar et al 2010 in dynamic pnm for two phase flow single phase pnm is used to calculate the pressure distributions in the wetting and non wetting phases while maintaining a capillary pressure between the two phases e g joekar niasar et al 2010 accurate pressure distribution is needed as this information is used to update the phase distribution however to our knowledge there is still no research reporting on the accuracy of using single phase pnm to simulate pressure distribution therefore we focus on improved pore network models to simulate single phase flow in porous media in this work we not only evaluate the improvements in terms of calculating the permeability but also the detailed fluid pressure distribution four pore network models are evaluated in this work including the conventional pnm and three improved pnms for the improved pore network models in addition to using throat bonds showing real throat cross section and the real pore throat pore geometry without simplification we propose a third option with a new hybrid pnm approach which uses serial sub throat bonds with real throat cross sections to describe the cross section variation between two pore bodies the paper is arranged as follows in section 2 we introduce different pore network extraction methods in section 3 the numerical methods are introduced including the multi relaxation time lattice boltzmann model the conventional pore network model and the three improved pore network models the coupling procedure between lbm and ipnms is described in detail in section 4 the performance of the different pnms to predict the permeability and pressure distributions is discussed in detail finally conclusions are summarized in section 5 2 pore network extraction there are different methods to extract a pore network from a real porous geometry such as the medial axis based method lindquist et al 1996 maximum ball based method dong and blunt 2009 watershed based method gostick 2017 rabbani et al 2014 in this work we use the watershed method to decompose the porous media into sub pore regions and extract the pore network in addition to the conventional pore network model three different improved pore network models are presented in this work as shown in fig 1 firstly the porous medium is decomposed into sub pore regions using the watershed method which has been implemented in the open source algorithm snow gostick 2017 this algorithm includes four steps first step is to obtain the distance map and apply a gaussian filter to remove some fake watershed ridge lines then a maximum filter is applied to eliminate peaks on saddles and plateaus we set the standard deviation of gaussian filter as 0 4 and the radius of structuring element as 5 in this work next the peaks close to each other are merged into one peak finally the porous medium is segmented into sub pore regions more details about this algorithm can be found in gostick 2017 using the watershed method the pore space is split into sub pore regions as illustrated in fig 1 b each identified sub pore region corresponds to a pore body in the extracted pore network the sub pore regions are connected with narrow interfaces which correspond to the throat bonds in the pore network 2 1 conventional pore network extraction with simplified pore and throat geometry in the conventional pore network model cpnm both pore bodies and throat bonds are simplified into regular geometries under some constraints as shown in fig 1 c in this work all the pore bodies are taken as cubes with the volume equal to that of the corresponding sub pore region throats usually account for most of the flow resistance as they have much smaller cross sections than the pore bodies therefore it is assumed that the pore bodies have no flow resistance and that all flow resistance is located at the throat bonds similar assumptions have been used in other research joekar niasar et al 2010 the inscribed radius rin and shape factor g of the throat cross sections are adopted to simplify the real throat cross sections into regular geometries such as circle square and triangle valvatne 2004 the inscribed radius rin is defined as the radius of inscribed circle in the throat cross section the shape factor g is defined as a p 2 where a and p are the area and perimeter of the throat cross section respectively based on g the throat cross sections can be simplified into triangle g 3 36 square 3 36 g 1 16 and circle g 1 16 respectively after the geometry simplification the parameters a and p of the throat bonds are recalculated based on the simplified cross sections the length of throat bonds is calculated by lij r in i r in j where lij is the distance between the centroids of the connecting two pore bodies r in i and r in j are the inscribed radii of these two pore bodies 2 2 improved pore network extraction with real throat cross section one way to improve the accuracy of pore network model is to replace the regular throat bonds with throat bonds with real throat cross sections as shown in fig 1 d this improvement has also been studied recently by miao et al 2017 rabbani and babaei 2019 we refer to this first improved pore network model as ipnm1 hereafter to obtain the real cross section of the throat bonds we firstly connect the centroids of the connecting two pore bodies with a straight line shown as dashed blue line in fig 1 b then a plane perpendicular to this line is inserted and moved from centroid 2 to 3 and the varying cross sections along this line are obtained we select the cross section with minimum inscribed radius as the real throat cross section because the minimum cross section has most significant influence on the flow resistance the lengths of the throat bonds are calculated in the same manner as those in cpnm as there is no analytical or empirical equations to calculate the conductances of these real throat bonds lbm is used to calculate such conductance as described in section 3 2 2 3 improved pore network extraction with real pore and throat geometry to retain as much geometrical information as possible the best way is to extract the real pore throat pore element without any geometry simplification as shown in fig 1 f to extract the pore throat pore element first a straight line connecting the two pore bodies is also determined then two planes perpendicular to this line are inserted at the pore centroids 2 and 3 and all pore voxels in between these two planes are extracted while the other voxels between these two planes are considered as solids the extracted pore throat pore element is placed in a cubic simulation domain to calculate the conductance as shown in figs 1 f and 4 2 4 improved pore network extraction with serial sub throat bonds as more geometrical information is maintained in the improved pore network model in previous section the computational efficiency becomes much lower as demonstrated in section 4 2 to reach a balance between the accuracy and computational efficiency we propose a new improved pore network model for a real throat connecting two pore bodies the shape of the cross section varies from one pore centroid to the other however both the cpnm and ipnm1 use only one throat cross section to calculate the flow resistance which means that the variation of throat cross section between the two pore bodies is not captured to capture this information we propose a new pore network model which uses several throat bonds in series between the two pore bodies as shown in fig 1 e to determine the shapes of the bonds the procedure is similar to the one in section 2 2 first a straight line is set connecting two pore bodies then the total throat bond is divided into several sub throat bonds each with length lsub the cross section retained for each sub throat bond is the one with minimum inscribed radius as shown in fig 1 e and its flow resistance is determined the overall flow resistance is calculated by the harmonic mean of the conductances of all sub throat bonds which will be further elaborated in section 3 2 3 as the accuracy and the computer efficiency of this model lie in between those of ipnm1 and of the ipnm presented in section 2 3 we call this improved pore network model 2 ipnm2 in the following analysis while the model described in section 2 3 as improved pore network model 3 ipnm3 3 numerical methods 3 1 lattice boltzmann method a multi relaxation time mrt single phase lb model is used to simulate fluid flow in the throat bonds and calculate their conductances it is also used to simulate fluid flow in the whole porous medium to provide a reference solution for evaluating the accuracy of the different pore network models here the lb model is briefly introduced the evolution equation is shown below 1 f r e α δ t t δ t f r t m 1 s m r t m e q r t m 1 i s 2 m f δ t where f f 0 f 1 f 18 t andf α is the density distribution function of α direction ris the spatial location e α is the velocity of the α direction tis the time i is the identity matrix δ t is the time step the d3q19 velocity model is adopted in this work f is the external body force which acts as the driven force in ipnm1 and 2 to calculate the throat conductance and it can be expressed as 2 f f 0 f 1 f 18 t f α ρ w α e α a c s 2 ua e α e α c s 2 i c s 4 δ t where w α is the weight factor in the α direction ρ is the macroscopic density a is the acceleration due to externalforces c s 1 3 is the speed of sound m is a 19 19 transformation matrix to map the vector f in the discrete velocity space to the vector m in the moment space s is the diagonal collision matrix the halfway bounce back boundary condition is adopted on solid surfaces which will generate no slip velocities the accuracy of using lbm to simulate single phase flow is verified by comparing simulation results with analytical solutions in simple geometries as illustrated in the appendix in all simulations and analyses of the following paragraphs lattice units are used unless otherwise specified the lattice units can be easily converted into physical units given the time scale t 0 length scale l 0 and mass scale m 0 zhao et al 2018 for example the viscosity in lattice units can be converted into physical units by μ p h y μ l b m m 0 l 0 t 0 3 2 pore network model for single phase flow through a throat bond connecting two pore bodies i and j the volume flux qij is proportional to the pressure difference in the two pore bodies which can be expressed by 3 q i j g i j p i p j where gij is the throat conductance pi and pj are the pressures in pores i and j respectively under steady state the mass balance equation in each pore body i should be satisfied which means 4 j q i j j g i j p i p j 0 for pressure driven fluid flow the pressures at the inlet and outlet pores are constant 5 p i i n l e t p i n p i o u t l e t p o u t combining eqs 3 5 we can solve the pressure field in the whole pore network then the volume flux across the pore network and the corresponding permeability can be calculated it is obvious that the accuracy of single phase pore network model depends on the throat conductance gij the difference of different pore network models lies in the calculation of gij which will be described in following paragraphs 3 2 1 conventional pore network model cpnm as introduced in section 2 2 in cpnm the complex throat cross sections are simplified into regular shapes such as circle square and triangle for single phase flow in such regular throat bonds the throat conductance can be calculated by valvatne 2004 6 g c a 2 g μ l where c is a modification coefficient and c 0 5 0 5623 and 0 6 respectively for circular square and triangular throat bonds l is the throat length a is the cross section area of the throat bond and µ is the dynamic fluid viscosity as there is no analytical or empirical expressions to calculate the throat conductance in the improved pore network models lbm is used to simulate single phase flow in these real throat bonds and calculate their conductances 3 2 2 improved pore network model 1 ipnm1 in ipnm1 the throat bond with real cross section is considered to calculate the conductance the model shown in fig 2 a is developed by extending the real throat cross section in x direction with a short distance namely 3 lattices in this work this throat cross section is extracted from the pore throat pore element shown in fig 4 in the lbm simulation the fluid density is set as 1 0 and the kinetic viscosity is set as 0 5 a small external acceleration ax 1 0 10 4 is imposed in x direction and periodic boundary conditions are considered the volume flux q across this throat bond can be calculated by integrating the velocity distribution in the slice at x 0 when the relative volume flux change in 100 consecutive iteration time steps is less than 10 6 conditions referred to as the stop criterion the simulation is assumed to have reached the steady state this simulation can reach steady state in a short time as there is no velocity gradient in x direction and the simulation domain is small the throat conductance is then given by 6 g μ q ρ a x l μ l b m μ where ρ is the fluid density in the lbm simulation l is the length of throat bond as the throat conductance is related to the fluid viscosity the last term of eq 6 above is used to rescale the throat conductance calculated in lbm simulation with the dynamic viscosity µ lbm to the one in pnm simulation with the dynamic viscosity µ as this work considers single phase flow the dynamic viscosity has no influence on the calculation of permeability and pressure distribution and we set µ µ lbm 0 5 in lattice units 3 2 3 improved pore network model 2 ipnm2 in ipnm2 we assume two pore bodies are connected through several sub throat bonds in series the cross section of each sub throat bond is extracted from the digital rock with its real geometry the length lij between two pore centroids of connecting pore bodies i and j is divided into nsub lij lsub sub throat bonds to check the influence of lsub on the throat conductance we calculate the conductances of different pore throat pore elements with different lsub ranging from 8 to 3 it is found that there is very minor change in throat conductance after lsub 5 therefore we set lsub 4 in the following simulation and analysis as an example for the pore throat pore element shown in fig 4 we obtain 6 sub throat bonds with lsub 4 the cross section of each sub throat bond is shown in fig 3 then lbm is used to simulate single phase flow and flow resistance for each sub throat bond eq 7b the overall flow resistance is calculated as the harmonic mean of the conductances of all sub throat bonds 7 g μ 1 k 1 g k μ g k μ q k ρ a x l k μ l b m μ where g k µ is the conductance of sub throat bond k lk is the length of sub throat bond k 3 2 4 improved pore network model 3 ipnm3 in ipnm3 the complete pore throat pore element is extracted fig 4 shows one pore throat pore element randomly selected and extracted from a fontainebleau sandstone digital rock shown in fig 5 e lbm is used to determine the pressure driven single phase flow imposing an inlet pressure pin at the left boundary and an outlet pressure pout at the right boundary the volume flux q across this pore throat pore element is calculated by integrating the velocity at the inlet outlet boundary the same stop criterion as in section 3 2 2 is adopted to minimize the compressible effect we use a small inlet outlet pressure ratio 1 01 in the simulation in addition the average volume flux across the inlet and outlet boundaries is used in this simulation more time steps are needed to reach steady state as the simulation domain is much larger and more complex the conductance of this pore throat pore element is calculated by 8 g μ q p i n p o u t μ l b m μ 4 results and analysis the three different models combining pnm and lbm are run on three different digital porous materials and the results of permeability pressure distribution and throat conductance are compared with results of simulations with conventional pnm and whole domain lbm 4 1 digital rocks and pore networks the different pnms used to simulate single phase flow are based on three different digital rocks a berea sandstone berea sandstone 2008 a fontainebleau sandstone berg 2016 and a bentheimer sandstone muljadi 2015 as shown in fig 5 a c the three digital rocks are available from online resources to save computational time and for better comparison a subdomain of size of 300 200 200 voxels is extracted from each of the digital rock voxels are then directly considered as lattices grids to build the computational domain in the lbm simulation two buffer zones of 10 lattices are added at the inlet and outlet of each domain the corresponding digital rock samples are shown in fig 5 d f and figures g i show the extracted pore networks for each pore network the number of pore bodies are 854 653 and 812 and the number of throat bonds are 1791 1480 and 1899 respectively it should be mentioned that the bentheimer sandstone digital rock is upscaled from 1000 1000 1000 voxels to 500 500 500 voxels in order to include more pore bodies in the extracted pore network which means eight neighboring voxels in the original dataset are merged into one voxel in the rescaled digital rock in the rescaling process the upscaled voxel is assigned as pore voxel if the fraction of pore voxels in original dataset is larger than 50 the main goal of this work is to evaluate the accuracy and efficiency of different pnms to simulate single phase flow in porous media through comparing with whole domain lbm simulation if the prediction results of pnm are already accurate for smaller domains it will produce even more accurate results for larger domains therefore we do not implement the representative element volume rev test here first lbm is used to simulate fluid flow in the whole digital rocks shown in fig 5 d f the left boundary is considered as inlet and right as outlet and pressure boundary conditions are imposed on both sides at initial condition a linear pressure distribution from inlet to outlet is considered when the same stop criterion as above is met it is assumed that the steady state is reached then the permeability is calculated based on darcy equation these lbm simulation results are considered as reference solutions in the following analysis next the different pore network models are extracted the conductance of each throat bond is calculated and the fluid flow and permeability of the porous medium is determined the results are analyzed in following sections when comparing the simulation results of different models we use the same digital rock thus the same resolution 4 2 performance comparison of different numerical methods table 1 summarizes the performances of the different pnms introduced in section 3 the relative error of different pore network models is calculated by e r k p n m k l b m k l b m 100 with k the permeability all the lbm simulationsare executed on the piz daint from the swiss national supercomputing centre the computational times shown in the table are transformed to equivalent one cpu time by multiplying the time for simulation with the number of cpus in general for the improved pore network models the simulation time increases with the throat number it should be noted that both the time for pore network extraction and the time for conductance calculation using lbm are included in the computational time for the different pore network models as shown in table 1 all the ipnms improve the prediction of permeability of the digital rocks compared to the cpnm results because they capture more accurate information on the pore geometry than cpnm both the cpnm and ipnm1 underestimate the permeability although ipnm1 shows a better performance than cpnm in predicting permeability its relative error is still around 25 to 45 the main reason for the underestimation is that the first two pore network models assume only the minimum cross section of the throat between the two pore bodies when considering also the cross section variation of the throats between the two pore bodies in ipnm2 the accuracy significantly improves with only a little increase in computational cost compared to ipnm1 as ipnm3 considers the real pore throat pore geometry it shows the best accuracy predicting similar results as the reference lbm however at a much higher computational cost compared to ipnm1 and ipnm2 compared with the reference lbm ipnm3 shows a much lower computational cost therefore ipnm3 can be considered to replace whole domain lbm simulation for predicting the permeability of a digital rock with almost the same accuracy while requiring much lower computational cost however to obtain a first order prediction of the permeability the newly proposed ipnm2 is considered as the best choice showing the best compromise between accuracy and computational cost we also observe that the predicted permeability of ipnm2 for the fontainebleau sandstone is less accurate than those predicted for the berea and bentheimer sandstones analyzing the porous structures and the pore network extraction processes for the different digital rocks we found that the accuracy of ipnm2 is related to the structure of the extracted pore network the numbers of extracted pore bodies and throat bonds change with the standard deviation of gaussian filter and the radius of structuring element in the watershed method the accuracy of ipnm2 can be improved if optimal pore network extraction parameters are selected however even with a less favorable extracted pore network as shown in table 1 the accuracy of ipnm2 is still much higher than those of cpnm and ipnm1 an analysis of the sensitivity of pore network extraction process on the accuracy of the permeability calculation by the different pore network models is although an important topic not the focus of the present paper and should be studied in future 4 3 pressure distribution comparison of different models as stated in the introduction it is important to obtain accurate prediction of the pressure distribution when simulating dynamic two phase flow in porous media in this section we evaluate the accuracy of the different pore network models to simulate the pressure distribution fig 6 a c show the dimensionless pressure distributions in different digital rocks obtained by lbm simulations fig 6 d f show the corresponding simulation results obtained by ipnm3 as all the pore network models yield visually similar results we only plot the pressure distribution for ipnm3 for illustration to quantitatively compare the results for lbm simulations the fluid pressure in each pore body is calculated by volume averaging as 9 p i l b m r r i p l b m r v i where p i l b m is the volume averaged fluid pressure in pore body i plbm r is the fluid pressure in position r r i is the coordinate set of region i vi is the total volume of pore region i we take the lbm simulation results as reference value and compare the pressure distributions for the different rocks and pnms as shown in fig 7 in all the figures x represents the fluid pressure obtained by lbm y represents the corresponding pressure obtained by the pore network model from top to bottom the pore network models are cpnm ipnm1 ipnm2 and ipnm3 respectively from left to right the rock types are berea sandstone fontainebleau sandstone and bentheimer sandstone respectively the mean absolute relative error mare for each case is calculated as 10 mare 1 n p i 1 n p p i p n m p i l b m p i n p o u t 100 where p i l b m is the reference pressure of pore i obtained by lbm p i p n m is the pressure of pore i simulated by pnm np is the total number of pore bodies for evaluation pin and pout are the inlet and outlet pressures respectively the mare values for different cases are shown in table 2 as can be seen from the top two rows of fig 7 the pressure distributions obtained by both cpnm and ipnm1 deviate more from the reference lbm results compared to ipnm2 and ipnm3 shown in the bottom two rows although ipnm1 was found to be more accurate in predicting the permeability compared to cpnm the mares of pressure for ipnm1 are even worse as shown in table 2 meaning more errors for ipnm1 in predicting the pressure distribution the reason for this observation will be analyzed in the next section as the ipnm2 and ipnm3 capture more geometry information they yield more accurate pressure distributions the mares for these two pore network models are much smaller especially for ipnm3 this comparison also confirms the conclusion obtained in previous section that the ipnm3 can replace whole domain lbm simulation for single phase flow in porous media while the ipnm2 is a good first order approximation for single phase flow in porous media 4 4 throat conductance comparison of different pore network models as mentioned in section 3 2 the accuracy of single phase pore network model depends highly on the throat conductance according to the results in sections 4 2 and 4 3 ipnm3 can produce results similar to the ones of the reference lbm in terms of both permeability and pressure distribution this means that the prediction of the throat conductance by ipnm3 is most accurate compared with the other three pore network models reason being that ipnm3 uses the real pore throat pore geometry in this section we consider the throat conductance of ipnm3 as reference values and compare these with the throat conductances predicted by the other pore network models the comparison results are shown in fig 8 as can be seen from fig 8 for cpnm most throat bonds yield an underestimated value of the throat conductance especially for small throat bonds accordingly the permeability of the whole porous medium is underestimated in general the throat conductances calculated by ipnm1 are higher than those obtained by cpnm therefore ipnm1 predicts a higher permeability than cpnm obtaining values closer to the reference values obtained by lbm however there is a crossover for the conductance calculated by ipnm1 and the reference values calculated by ipnm3 for small throat bonds ipnm1 underestimates the throat conductance while for large throat bonds ipnm1 overestimates the throat conductance due to this phenomenon the pressure distribution obtained by ipnm1 deviates substantially from those obtained by ipnm3 and lbm compared with the first two pore network models the data obtained by ipnm2 are less scattered predicting throat conductances similar to those of ipnm3 for both small throat bonds and large throat bonds therefore ipnm2 gives much better prediction results than cpnm and ipnm1 in terms of both permeability and pressure distribution we take the fontainebleau sandstone as an example to further analyze the crossover of throat conductance for ipnm1 observed in fig 8 in ipnm1 the flow resistance between two pore bodies is determined only by the throat conductance as the flow resistances in the pore bodies are neglected for a certain throat bond its conductance depends on two parameters its cross section and length fig 9 shows schematically two different pore throat pore structures and the resulting extracted pore network in ipnm1 the cross section of a throat bond is taken as the minimum cross section between two connecting pore bodies and the length of the throat bond is calculated by lij r in i r in j as stated in section 2 1 we use three parameters to describe the structure of the throat bond and its relationship with the connecting pore bodies which are the radius ratio between throat bond and the connecting pore bodies r throat r pore the ratio between throat length and the distance connecting the centroids of the two pore bodies l throat lij and the ratio between throat length and throat diameter l throat d throat these throat characteristics are plotted for all throat bonds of the fontainebleau sandstone pore network versus throat conductance in fig 10 in fig 10 a the ratio r throat r pore generally increases with throat conductance meaning that throat bond much smaller that connecting pore bodies results in lower conductance in contrast both the ratio s l throat lij and l throat dij generally decrease with increasing throat conductance thus a relatively shorter throat bond results in larger throat conductance as shown in fig 10 b and c using the schematic of fig 9 b for throat bonds with small conductance e g throat bond t1 2 the inscribed radius of this throat bond is much smaller than those of the connecting pore bodies p1 and p2 therefore the ratio r throat r pore is small and the flow resistance in the connecting pore bodies can be neglected remark that the cross section varies along the long throat bond while it is assumed in ipnm1 that it equals the minimum throat cross section in the throat bond t1 2 accordingly the throat bond t1 2 underestimates the real throat conductance while a large throat bond as illustrated by the throat bond t2 3 in fig 9 b has an inscribed radius similar to the radii of the connecting pore bodies p2 and p3 in addition the l throat lij is relative small meaning that the radii of the pore bodies are similar to the length of throat bond therefore the flow resistances in the throat bond and the connecting pore bodies are of the similar magnitude however the flow resistances in the connecting pore bodies are ignored in ipnm1 resulting in an overestimation of the throat conductance the above description provides an explanation for the crossover of throat conductance for ipnm1 as ipnm2 considers the cross section variation between the centroids of the connecting two pore bodies it actually adjust for this crossover and predicts throat conductance more accurately 4 5 velocity distribution comparison of different pore network models pore network models have been often used to simulate solute transport for which the single phase pnm is used to provide the flux or velocity distribution köhne et al 2011 qin and hassanizadeh 2015 in this section we analyze the velocity distributions obtained by different pnms we take the simulation results in the berea sandstone digital rock as an example the velocity distributions obtained by different models are shown in fig 11 the velocity in each throat bond in the pore network model is calculated by v t q t a t where qt and at are the volume flux across the throat bond and the area of throat cross section respectively we remark that the different pnms only display the mean velocities across the throats with minimum cross section which are larger than the ones in other cross sections therefore it is reasonable that the velocity magnitude of ipnm3 shown in fig 11 e is visually larger than that of the lbm simulation results shown in fig 11 a simulation results between lbm and pnm cannot be compared throat by throat as the lbm velocities are not uniform over the throat alternatively we take the simulation result of ipnm3 as the reference value and compare the velocity distributions obtained by the different pnms as shown in fig 11 b e the velocity distributions obtained by ipnm2 and ipnm3 are roughly similar but the velocity distributions obtained by ipnm1 and cpnm are significantly different with the ones obtained by ipnm3 especially for cpnm which produces much smaller velocities than the ipnms taking the velocity distribution obtained by ipnm3 as the reference value we further compare the velocity distributions obtained by the other pnms throat by throat in the plots of fig 12 we note that a few data points with values smaller than 1 0e 8 are excluded from this figure for better view according to the comparison results the cpnm significantly underestimates the velocity in each throat bond due to the fact that the cpnm predicts smaller throat conductance than ipnm3 the ipnm1 predicts better the velocities than cpnm the dataset of ipnm2 is least scattered and shows the best agreement with ipnm3 the comparison results are similar to those of throat conductance shown in fig 8 as the velocity distribution is determined by the throat conductance 5 conclusion in this paper we performed a comprehensive investigation of the accuracy of different pore network models to simulate single phase flow in porous media four different pore networks are extracted based on the watershed method the conventional pore network model cpnm uses regular shaped pore bodies and throat bonds the first improved pore network model ipnm1 replaces the regular shaped throat bonds with throat bonds showing their real minimal throat cross sections the second improved model ipnm2 which is firstly proposed in this work uses a series of sub throat bonds to describe the cross section variation between two pore bodies the improved pore network model 3 ipnm3 directly extracts the real pore throat pore element without any geometry simplification the throat conductance in the three improved pore network models is calculated by lbm the performance of different pnms to simulate single phase flow in porous media is evaluated taking the lbm simulation results in the whole porous media as reference solutions from cpnm to ipnm3 the accuracy to predict the permeability increases as more accurate information on pore geometry is used at the expense of computational efficiency in addition to the permeability the pressure distribution obtained by different pnms is compared pore by pore with the reference lbm results the results show that ipnm2 and ipnm3 achieve more accurate pressure distributions compared with those obtained by cpnm and ipnm1 although ipnm1 can predict more accurate permeability than cpnm the relative errors in predicting the pressure distributions are even worse it was shown that cpnm underestimates the throat conductance for most of the throat bonds as compared to those calculated by ipnm3 ipnm1 underestimates the throat conductance for small throat bonds and overestimates the throat conductance for large throat bonds due to the structure difference for small throat bonds and large ones ipnm2 which takes into account the cross section variation between the connecting pore bodies obtains very similar throat conductance values as those determined by ipnm3 in summary ipnm3 can replace lbm to simulate single phase flow in porous media with almost the same accuracy but at much lower computational cost however to obtain a first order approximation of single phase flow in porous media in terms of both permeability and pressure distribution the newly proposed ipnm2 is a better choice with the best balance between accuracy and computational cost the results presented in this work by comparing different pore network models can help researchers and engineers to choose the adequate pore network model for their specific purpose in addition the different improvements presented here for single phase pore network models can be incorporated into other pore network models for more complicated flow or transport processes such as multiphase flow solute transport etc which should be studied in future work declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper the authors declare the following financial interests personal relationships which may be considered as potential competing interests acknowledgements this work was supported by the swiss national science foundation project no 175793 and the swiss national supercomputing centre project no s823 q k acknowledges the support of lanl s ldrd program the data associated with this paper are made available from https www research collection ethz ch handle 20 500 11850 401218 appendix we conduct external force driven single phase flow through circular tubes with different radii to verify the accuracy of using lbm to calculate the conductance the simulation domain is similar to that shown in fig 2 and we replace the cross section with circles of different radii a small external acceleration ax 1 0 10 4 is imposed in x direction and periodic boundary condition is considered when the relative volume flux change in 100 consecutive iteration time steps is less than 10 6 the simulation is assumed to have reached the steady state then the permeability of fluid flow through this tube can be calculated by 11 k l b m q μ π r i n 2 ρ a x the calculated permeability by lbm is plotted in fig 13 as red squares where the solid line shows the theoretical solution which is k theory r i n 2 8 as can be seen the simulation results match well with theoretical solutions demonstrating the accuracy of using lbm to calculate conductance of single phase flow 
