index,text
25990,financial risk access to capital regulatory processes and regional competition for limited water sources represent dominant concerns in the united states as well as the broader global water supply sector this work introduces the waterpaths simulation software a generalizable cloud compatible open source exploratory modeling system designed to inform long term regional investments in water infrastructure while simultaneously aiding regions to improve their short term weekly management decisions often made in response to droughts uniquely waterpaths has the capability to identify coordinated planning and management for groups of water utilities sharing water resources waterpaths exploits dynamic and adaptive risk of failure rof rules to trigger management and planning actions in temporally consistent pathways the compact and efficient rof based representation of decision pathways allows waterpaths to scale efficiently with the number of regional actors and their candidate actions lastly as a platform for supporting decision making under deep uncertainty waterpaths accounts for a broad range of uncertainties including hydrological or climate extremes permitting time demand growth effectiveness of water use restrictions construction costs and financing uncertainties to demonstrate the capabilities of waterpaths we introduce a new hypothetical water resources test case the sedento valley the sedento valley test case contains three resource sharing water utilities that seek to regionally coordinate their policies for drought mitigation and infrastructure investment the three utilities are challenged by a diverse set of deep uncertainties that encompass natural and human systems stressors the sedento valley test case contributes a new opportunity for benchmarking decision support tools and water resources systems simulation software keywords water supply decision making under deep uncertainty robustness infrastructure pathways simulation 1 introduction recent projections estimate that the united states us will require over one trillion dollars of investment in water supply infrastructure in the next 20 years asce 2017 this investment represents a difficult challenge as water fees which often fund water infrastructure projects are already rising above the consumer price index uswa 2019b a hall et al 2019 and initiatives such as the water infrastructure finance and innovation act commonly known as wifia copeland 2016 have only been able to address a fraction of the required investment epa 2019 this challenge is mirrored around the world as water managers face the task of maintaining supply reliability under climatic social and financial uncertainties bonzanigo et al 2018 awwa 2019 in many regions rapid shifts in seasonal climate drought patterns and the frequency of extreme events threaten to overwhelm existing water supply capacity shafer and fox 2017 these threats are felt more acutely in large metropolitan areas which have observed strong and sustained population growth in recent decades mcnabb 2019 census 2019 to face these challenges water managers must transition planning frameworks to jointly address growing uncertainties and to include the recent innovations in sustainable freshwater management gleick 2018 historically water managers have relied on infrastructure expansion to confront long term supply risks and utilized water use restrictions to manage short term drought crises gleick 2002b this approach often yields high financial burdens as utilities are forced to accumulate large amounts of debt revenue disruptions resulting from water use restrictions may cause utilities to miss payments on this debt threatening their financial stability and increasing the borrowing costs hughes and leurig 2013 moody s 2017 2019 additionally the land or water resources most suitable for new water supply infrastructure in developed countries has largely been developed and allocated lund 2013 and new water infrastructure projects such as large dams pose environmental concerns and face significant regulatory uncertainties these challenges have led to the proposal of soft path strategies to water resources management that seek to compliment centralized infrastructure with non structural measures to improve efficiency and manage water demand gleick 2002a exploring synergies between long term infrastructure investment pathways and non structural drought mitigation instruments holds promise for incorporating soft path strategies into water supply planning and management potential short term management instruments include demand management hall 2019 zeff et al 2014 borgomeo et al 2018 huskova et al 2016 haasnoot et al 2013 treated water transfers through regional shared infrastructure caldwell and characklis 2014 zeff et al 2014 watkins and mckinney 1997 hall 2019 and raw water transfers through upstream reservoir releases borgomeo et al 2018 gorelick et al 2019 integrating these short term management instruments with long term infrastructure sequencing allows utilities to create adaptive management strategies that maintain reliability under drought conditions while minimizing the need for infrastructure expansion zeff et al 2016 the coupling of long term infrastructure investment strategies with short term drought mitigation necessitates innovations in decision support systems for water resources management problems broadly decision support systems are composed of the suite of analytical mathematical and or simulation focused tools that aid planners in the design and operation of water resources systems loucks and da costa 2013 there is large historical body of literature on water resources planning and management where simulation models are a central focus for analysts developing insights on how a system behaves under varying system states or decision maker actions see review in loucks and van beek 2017 since the inception of the field in the early 1960s water systems models have typically consisted of networks of storage and junction nodes linked by conveyance structures such as pipelines canals and river reaches maass et al 1962 harou et al 2009 there is a large array of existing water systems simulation software that has contributed substantial benefits for decision support applications including modsim labadie 2011 water evaluation and planning weap sieber 2006 interactive river aquifer simulation iras 2010 matrosov et al 2011 wathnet kuczera 1992a ribasim hydraulics 2004 mike basin jha and gupta 2003 source welsh et al 2013 calvin draper et al 2003 oasis hydrologics 2009 riverware zagona et al 2001 and pywr tomlinson et al 2020 for a summarized feature comparison between waterpaths and other mentioned software see tables 1 and 2 more recently there has been a growing interest in multiobjective simulation optimization applications where models such as modsim iras riverware wathnet and pywr are coupled with multiobjective evolutionary algorithms moeas to aid in the discovery of key water supply planning and management tradeoffs matrosov et al 2015 borgomeo et al 2018 basdekas 2014 smith et al 2016 tomlinson et al 2020 moeas are global population based search algorithms that evolve sets of pareto approximate solutions to multiobjective problems through processes of mating mutation and selection for reviews see nicklow et al 2010 maier et al 2014 and coello et al 2007 however search based decision support tools such as moeas require an effective and efficient software coupling with simulation software moreover typical applications substantially increase the computational demands of analyses commonly requiring tens of thousands of simulation based evaluations of performance objectives to guide search processes in the context of simulation optimization applications modern water systems simulation software systems have key technical limitations 1 they often do not provide users with flexibility in representing complex state aware actions as an artifact of their approach to water balance modeling e g optimization or rule based allocations that cannot capture information feedbacks 2 the most widely used simulations do not provide access to their underlying source code bases e g commercial software packages 3 despite the growing recognition that a broad array of uncertainties strongly shape water resources systems e g financial risks behavioral responses changing hydro climatic conditions they are highly constrained in their representation and scalable computational support for including these concerns and 4 they lack the ability to perform planning exercises including simultaneous consideration of both short and long term decisions beyond purely software related technical challenges there are broader conceptual challenges in how decision support framework s treat uncertainties in particular deep uncertainties for recent reviews see moallemi et al 2019 dittrich et al 2016 kwakkel and haasnoot 2019 herman et al 2015 deep uncertainty refers to conditions when decision makers do not know or cannot agree upon probability distributions of key system parameters and or the system boundaries kwakkel et al 2016 lempert 2002 water supply planning and management under deep uncertainty transitions from a focus on discovering optimal management strategies to crafting robust and adaptive strategies that maintain performance across a wide array of potential future conditions walker et al 2013 dittrich et al 2016 frameworks such as decision scaling brown et al 2012 robust decision making rdm lempert et al 2006 many objective robust decision making mordm kasprzyk et al 2013 and info gap ben haim 2006 provide methods to aid decision makers in the discovery of key uncertainties control system vulnerability these methods often involve computationally demanding exploratory monte carlo simulation analysis that strongly benefits from scalable simulation software that can be adapted across a range of state of the art computing architectures the challenges and needs summarized above motivated this work s contribution of the open source waterpaths stochastic simulation software which has been specifically developed to support integrated infrastructure investment pathway planning and water portfolio management under deep uncertainty trindade et al 2019 waterpaths allows for a flexible and efficient representation of multi actor water resources systems while providing advanced computational support for multiobjective optimization algorithms and exploratory analyses of a broad range of uncertainties waterpaths supports water supply infrastructure and water portfolio management applications for systems confronting water scarcity or increasingly severe droughts waterpaths provides decision makers with ability to flexibly abstract water infrastructure portfolio problems that contain measures for addressing the short term supply and financial impacts of droughts along with long term capacity expansions the simulation system was developed to run on desktops cloud computing systems and on high performance computing resources it utilizes both shared memory and distributed memory parallelization to enable decision makers to efficiently design optimize and evaluate candidate water supply portfolios over large ensembles of potential future states of the world in this study we demonstrate waterpaths by introducing the sedento valley a detailed and realistic hypothetical multi actor regional water supply planning test case waterpaths is used to represent three hypothetical water utilities in the south eastern us that are facing the prospect of water shortage due to growing demand and changing climate the utilities seek to craft both short term drought mitigation responses and long term infrastructure investment pathways that maintain reliable supply and financial stability the utilities have the potential to cooperate using treated transfers through existing infrastructure and through shared infrastructure development 2 the waterpaths simulation system 2 1 overview waterpaths is an open source and modular model for the simulation and optimization of water infrastructure planning and management policies the simulation system is designed to facilitate decision making under deep uncertainty dmdu for interested readers supplement s1 provides a more detailed review of several commonly employed dmdu frameworks that center on planned adaptation we demonstrate waterpaths functionality using the recently proposed deeply uncertain pathways du pathways framework for designing dynamic and adaptive infrastructure investment and management policies trindade et al 2019 the du pathways framework builds upon many objective robust decision making mordm kasprzyk et al 2013 a methodology for deliberative decision support processes that explore deeply uncertain tradeoffs and vulnerabilities in complex environmental systems a schematic of du pathways methodology in fig 1 a showcases the iterative decision making process in four steps problem formulation generating alternatives reevaluation under deep uncertainty and policy diagnostics the iterative nature of the framework facilitates a constructive decision aiding approach tsoukias 2008 that enables decision makers to learn about the problem and their preferences through exploratory modeling bankes 1993 the du pathways framework builds upon mordm by explicitly incorporating the design of rule systems that react to observed system states du pathways provides a means of implementing closed loop control policies bertsekas et al 1995 for infrastructure sequencing and drought mitigation the simultaneous optimization of risk based infrastructure triggers and drought mitigation instruments effectively bridges the gap between long term infrastructure investment and short term water portfolio management zeff et al 2016 trindade et al 2019 demonstrate that the du pathways approach can produce robust and adaptive infrastructure pathways that perform well across broad sets of uncertainty and balance conflicting interests within multi actor systems successful implementation of the du pathways framework is a computationally intensive task due to the required stochastic simulations and large numbers of function evaluations during moea search these properties make it difficult implement with existing decision support systems the waterpaths modeling system has been specifically developed to aid the application and extension of the du pathways decision support framework fig 1b illustrates how waterpaths facilitates both multiobjective optimization and policy evaluation when in optimization mode which includes elements in fig 1b enclosed in the red dotted line waterpaths streamlines its output to efficiently run with a multiobjective evolutionary algorithm in policy evaluation mode which includes elements enclosed in the green dotted line in fig 1b waterpaths gathers and outputs detailed diagnostics on policy performance and system states waterpaths was designed for centralized system state information recording to allow for easy implementation of custom objective functions it currently has five objectives implemented but more can be easily added to the masterdatacollector class the currently implemented objectives are reliability restriction frequency net present value of infrastructure built during the life of the policy maximum annual financial cost of drought mitigation and infrastructure construction and worse first percentile annual financial cost of drought mitigation and infrastructure construction the details of which are provided in supplemental section s4 waterpaths is equipped with an input file parser a simple user interface that facilitates applications by users with limited to no programming capabilities due to waterpaths open source code base new features and other customizations can be added by users with a basic knowledge of c new features such as new types of water infrastructure customized reservoir control rules and new drought mitigation policies can be easily added by extending the already implemented abstract classes similarly any system metric for decision making purposes can be implemented with the addition of a single function to the existing code the mass balance model rof calculation model and other aspects of the systemwide simulation are designed to work with any child class of the existing abstract classes making it possible for a user to flexibly add features to waterpaths on a high level waterpaths organizes the simulation of a system of cooperating water utilities in terms of a mass balance performed for each piece of water infrastructure utilities drawing water from and building new water infrastructure as well as drought mitigation and financial policies that guide utilities supply and financial operations a diagram showing the object oriented structure of waterpaths can be found in supplementary section s11 for a given utility each time step day week or month the simulation follows the sequence shown in fig 2 1 calculation of long term rof metric if at first week of a calendar year 2 calculation of the short term rof 3 application of drought mitigation and financial policies 4 system mass balance modeling and 5 system state data collection each of waterpaths specific features are as illustrated in fig 2 discussed in the subsequent sections see fig 3 2 2 rof based decision rules the facilitation of decision making through state aware rule systems is a core component of the waterpaths simulation system in waterpaths the default formulation of investment and management policies exploits short and long term rofs triggers which constitute dynamic measures of a utility s capacity to demand ratio palmer and characklis 2009 zeff et al 2016 a graphical depiction of an example rof calculation can be found in fig 1 for a given utility at a given system state total combined storage level the rof is the estimated probability that the utility s storage will fall below a critical storage level in the next t r o f weeks if the system were subjected to hydrological conditions from the past 50 years of the historical record under default settings short term rof used to trigger drought mitigation actions is calculated each week during a simulation using t r o f 52 the calculation of short term rof at each time step allows policies to trigger drought mitigation instruments that respond to current reservoir storage levels the use of t r o f 52 is designed so that drought mitigation responds to single year droughts long term rof is calculated at the first day of each calendar year of a simulation assumes the utility s storage is at capacity and uses t r o f 78 this formulation provides information on the suitability of utility storage capacity to handle multi year droughts under default settings the critical storage level for all utilities is set to 20 of total storage a more mathematically detailed description of the rof metrics is available in section 2 of the supplement the state aware nature of rofs means that a water management and infrastructure investment policies generated by waterpaths generate contextually appropriate sequences of actions for the conditions being experienced in a given realization i e a policy will take different actions in wet versus dry futures or high demand versus low demand futures this closed loop style of integrated short term management and long term investment rules better account for the path dependencies of action sequences provide an overall higher degree of adaptivity and better exploit information feed backs relative to traditional abstractions of decisions in water supply literature see detailed discussion in trindade et al 2017 this is especially important for systems that are subject to deep uncertainties that challenge traditional fixed formulations that take the same actions regardless of the context of the conditions being experienced knox et al 2018 huskova et al 2016 borgomeo et al 2016 matrosov et al 2011 formulating decisions through state aware rof triggers is particularly powerful when combined with deep uncertainty optimization du optimization which confronts candidate policies with a broad array of challenging mild and middling future conditions allowing for the discovery of flexible policy sets that mitigate tail risks without incurring exorbitant baseline costs trindade et al 2017 for a water resources decision support system the inclusion of state aware rofs triggers is a non trivial aspect of the simulation optimization runtime waterpaths contains an efficient representation of both short and long term rofs that exploits shared memory parallelization to efficiently run on desktop computers cloud systems and hpc clusters for parallelization details see section 10 of the supplement additionally the rof metric can be customized to capture single year droughts as done by default for short term drought management instruments or multiple year droughts the default for long term infrastructure sequencing actions the modular design of waterpaths also makes it straightforward for users to implement and test their own state aware decision triggers 2 3 mass balance model the core of waterpaths is its mass balance model the mass balance is solved for all water infrastructure following an upstream to downstream order defined through infrastructure connectivity information provided by the user as a directed graph the mass balance equation for reservoirs is shown in eq 1 1 x s w 1 x s w n i w s e w u r o w e r w r a x s w e o w s w r d w where x s w 1 is the volume of water stored in the reservoir at the week after the current week w n i is the natural inflow into the reservoir from all its tributaries s e is a treated sewage effluent discharged either on a tributary or directly on the reservoir u r o is the upstream reservoir total outflow if such reservoir exists mandated outflow plus spillage e r is a non dimensional evaporation rate r a is the reservoir area as a function of stored volume e o is the environmental outflow r d w is the total municipal demand drawn from that reservoir by one or more independently modeled utilities and s is the reservoir spillage which is set to zero unless the reservoir is completely full if a reservoir is allocated for multiple uses municipal supply to multiple utilities water quality etc each with a designated percentage of the total storage capacity the inflow is split across all uses proportionally to their allocated volumes water in excess of any allocated capacity is redistributed among the others to allow for more realistic simulation of reservoirs waterpaths also provides an abstract class for reservoir control rules named minenvflowcontrol and one for controls of other discharges such as treated waste water discharges the latter flowing into a reservoir or stream named control rules waterpaths also simulates non storage water infrastructure such as water intakes and water re use stations other water infrastructure such as desalinization plants can be easily implemented by extending the abstract class water sources based on a custom mass balance function waterpaths also has implemented upgrades of existing infrastructure namely reservoir expansions and treatment capacity expansions 2 4 capturing regional decision making to support regional decision making waterpaths can simulate multiple water utilities potentially with interconnected networks shared infrastructure as separate elements all within the same regional system each utility in waterpaths divides its demand among its storage based infrastructure based on the allocated stored volume of water after drawing as much as possible from non storage infrastructure e g intakes and reuse stations the volume of water in storage owned by a given utility is also used to calculate its short and long term rof metrics as described in section 2 2 the rof dynamics are central to the timing and coordination of candidate water management decisions in waterpaths each candidate action is triggered based on a utility defined rof level i e the level of supply failure risk at which a given action is taken this abstraction is drawn from the common usage of risk tables in practice and it provides state aware adaptivity additionally water utilities in waterpaths track their own finances the finances as modeled as cost fluctuations due to drought mitigation and financial instruments and debt repayment the total annual revenue is also calculated based demand consumer tiers and corresponding tariffs and is used to support objectives calculations and financial instruments 2 5 drought mitigation instruments building from prior works zeff and characklis 2013 zeff et al 2014 2016 trindade et al 2017 palmer and characklis 2009 caldwell and characklis 2014 waterpaths has implemented water use restrictions and inter utility treated water transfers although other instruments can be easily added by creating a new child class of the drought mitigation instrument class water use restrictions as currently implemented in waterpaths can be implemented on multiple tiers each with different percentages of demand reduction based on stricter measures reductions in lawn irrigation car and sidewalk washing etc and triggered by a different value of the rof metric revenue losses due to restricted water sales can be mitigated by the use of higher contingency tariffs provided by the user to be used for the duration of the enactment of water use restriction treated water transfers in waterpaths are performed from a source utility to requesting utilities as water use restrictions requests for treated transfers are contingent on the current value of the short term rof metric for each requesting utility however treated water transfers often suffer from two constraints treatment capacity at the source utility and limited inter utility conveyance capacity to calculate the volume granted to each utility waterpaths solves a constrained allocation problem using a quadratic programming algorithm goldfarb and idnani 1983 gaspero 2007 the problem is set up to minimize the mean square error between the volumes requested by each utility adjusted for their rof values a utility with a higher rof receives proportionally more water and the volumes that can be transferred through the inter utility network subject to conveyance constraints funds are then transferred from all requesting to the source utility drought mitigation instruments however can be costly and utilities may benefit from setting in place financial instruments to absorb and stabilize such costs 2 6 financial instruments waterpaths allows utilities to hedge against the negative financial effects of the drought mitigation instruments by using financial instruments such as the drought insurance and contingency funds zeff et al 2014 both already implemented in waterpaths the drought insurance currently implemented in waterpaths triggers a payout of a percentage of the previous year s annual revenue whenever the value of the rof metric reaches a set value the policy is updated priced and bought by the utilities every year based on the most current values of annual revenue and on a premium of 20 of the expected cost of the policy for the following year other types of index insurance policies based on state variables other than the rof can be easily added to waterpaths by creating a child class of the drought mitigation policy class contingency funds are not dependent on the rof metric and instead are implemented as a fixed percentage of the annual revenue on the last week of every year each utility that has a contingency fund adds a fixed percentage of that year s total revenue to its contingency fund these funds are then used during the following year to pay for drought mitigation policies other financial policies and for debt issued to build new infrastructure 2 7 reservoir flow control the reservoir controls policies currently implemented in waterpaths are fixed release release based on inflow seasonal minimum release and storage based release while the mentioned standard control policies cover a broad range of application contexts the option of creating customized reservoir control rules may be used to reflect the particulars of a system as an example of a custom policy rule releases could be made to be contingent on current stored volume and on the flow at a gauge at a tributary merging downstream from the reservoir in question 2 8 infrastructure investment infrastructure construction is triggered based on an action threshold or trigger for the long term rof metric if a utility or region has several candidate infrastructure investment options users or search must provide a construction sequence i e an infrastructure pathway across time to allow for infrastructure construction over the course of the simulated planning period waterpaths relies on information passed by the user or optimization algorithm about all infrastructure options storage capacity treatment capacity cost etc and the sequence in which a utility is to build such options if the value of the long term rof metric reaches its trigger value this is a unique feature of waterpaths which allows the user to prioritize options in the near term and define requirements infrastructure options can be located anywhere in and outside of the reservoir connectivity network infrastructure options in waterpaths may take the form of altogether new projects such as new reservoirs water intakes treatment plants etc and of expansion of current infrastructure such as storage treatment capacity expansions and reservoir re allocations this allows waterpaths to account for flexible infrastructure development as emphasized in the real options analysis literature as important elements in the long term planning for water utilities fletcher et al 2017 2019 erfani et al 2018 wang and de neufville 2005 cox et al 1979 infrastructure development is often financed over decades rather than paid upfront waterpaths allows for multiple types of bonds to be issued for financing new infrastructure resulting in different possible debt repayment streams with different net present values for the same infrastructure option currently implemented in waterpaths are level debt service balloon payment and variable interest bonds however waterpaths allows for the design of creative finance mechanisms by allowing the user to create new types of bonds by creating children classes of the bond class and including those in optimization and simulation exercises 2 9 flexible representation and evaluation of uncertainties one of the key features that differentiates waterpaths from existing simulation systems is its ability to incorporate a broad array of uncertainties typically water systems simulation systems can only run one realization a scenario fully specified by one time series of stream flows evaporation rates and demands when pertinent and one value for other uncertainty factors at a time which implies in no explicit consideration of uncertainties see tables 1 and 2 others most notably the real options analysis frameworks include uncertainty in policy optimization in a bayesian fashion using decision trees and stochastic dynamic programming over expected costs fletcher et al 2017 2019 hui et al 2018 there are a growing number of example applications that consider uncertainty in a stochastic fashion by simulating a policy over hundreds or thousands of realizations every time the model is called with policy optimization being performed by attaching the model to a black box optimization algorithm zeff et al 2014 2016 kwakkel et al 2014 watson and kasprzyk 2016 borgomeo et al 2018 trindade et al 2019 waterpaths was designed to augment this latter approach by broadening the scope of actions and uncertainties that can be jointly evaluated waterpaths currently requires as minimum uncertainty related input one time series of inflows evaporation rates and demands for each piece of infrastructure and utility when applicable for each realization to be run in addition the user has the option of providing a series of multipliers representing deeply uncertainty factors which if not provided will assume a default value of the unit all time series and multipliers are to be sampled externally from any desired distribution and passed to waterpaths as input data the user can add as many deeply uncertain factors as desired when setting up a problem or in new classes of water infrastructure controls and drought mitigation policies the deep uncertainties currently included in waterpaths are presented in table 3 simulating and optimizing a policy using a stochastic approach to modeling uncertainty is a mathematically complex and can be computationally costly so care was taken to allow waterpaths to both be computationally efficient and to make use of any scale of computational resources available to the analyst a discussion of sampling strategies for optimization and exploratory modeling with the default version of waterpaths can be found in section s6 of the supplement 2 10 reducing runtime with local parallel computing waterpaths stochastic design allows for detailed uncertainty analysis that requires large ensembles of simulations however given that each realization can be simulated independently they can be easily distributed across multiple computational cores cpus within a processor of a modern desktop laptop or cloud computing instance the shared memory parallelization scheme in waterpaths several cores working on the same process while sharing the same block of memory is implemented using openmp 5 0 code directives klemm et al 2019 the openmp directives allow waterpaths to create a given number of computational threads and distribute them across available cores 2 11 facilitating decision analytics given the diversity and number of policy actions that can be evaluated with waterpaths the resulting water infrastructure investment and management pathways are typically composed of mixtures of discrete variables continuous values and permutations of infrastructure options as a consequence objective measuring system performance across different metrics are non convex and discontinuous functions of the policy variables moreover the uncertainty sampling enabled by waterpaths has the potential to make the performance metric functions noisy and highly heterogeneous in their behavior across water supply or financial concerns all these traits make the problem of manually or automatically designing policies for water infrastructure planning and management particularly difficult and attractive to researchers zeff et al 2016 fletcher et al 2019 borgomeo et al 2018 huskova et al 2016 kwakkel et al 2014 beh et al 2017 waterpaths policy input and objectives output was designed for easy integration with any black box multiobjective optimization algorithm and currently has out of the box integration with the master worker borg multiobjective optimization evolutionary algorithm ms borg moea hadka and reed 2013 2014 although support other algorithms written in c or other languages can be easily added 3 methodology 3 1 the sedento valley an illustrative test case we demonstrate the functionality of waterpaths using the sedento valley test case a novel hypothetical system that reflects many of the challenges facing regions with several independently operated urban water utilities we developed the sedento valley test case to contribute a benchmark problem for the evaluation of future methodologies for water infrastructure planning and management we designed the test case to encompass 1 a region with a growing population that stresses current water supplies 2 a system of multiple urban water utilities in close geographic proximity and asymmetric vulnerability to drought 3 utilities that face financial vulnerability to future drought risk 4 the potential for inter utility cooperation on short term drought mitigation planning and 5 the potential for cooperative infrastructure development between interconnected utilities the test case consists of three utilities in close geographic proximity facing increasing vulnerability to drought conditions due to climatic stressors population growth and changes in land use and land coverage gorelick et al 2020 the close proximity of the three urban utilities in the test case provides an opportunity for regional cooperation and shared infrastructure development but also creates the potential for inter utility conflicts over scarce resources this study demonstrates waterpaths through a demonstrative implementation of the deeply uncertain du pathways methodology trindade et al 2019 to examine infrastructure investment and drought management strategies for the sedento valley the du pathways analysis demonstrates key waterpaths capabilities 1 monte carlo evaluation of financial institutional and hydro climatic uncertainties 2 rof based adaptive infrastructure pathways zeff et al 2016 and 3 management of a mix of short term water supply portfolio instruments for drought mitigation characklis et al 2006 zeff et al 2014 the sedento valley shown in fig 4 is home to two medium sized cities dryville and fallsland and a smaller municipality watertown the population of the valley is near 1 5 million residents the three municipalities are each supplied by an independent water utility the cities of dryville and fallsland currently receive water from the autumn lake reservoir a large flood control reservoir owned and operated by the us army corps of engineers usace watertown owns and operates college rock reservoir and receives water from lake michael another large usace operated reservoir dryville and fallsland also have emergency allocations to lake michael where they have access by purchasing water from watertown s treatment plant and transferring via shared pipelines current supply capacities of each water utility can be found in table 4 the unallocated portion of lake michael is used by other utilities in the region that are not being modeled to meet environmental flow requirements see section 2 7 and as a water quality pool a growing population has reduced each of the utilities capacity to demand ratios increasing their vulnerability to drought conditions this problem is compounded by the increased difficulty of large infrastructure investments i e new reservoirs due to scarcity in feasible sites higher costs and reduced tolerance of environmental impacts increased hydro climatic variability and uncertainties stemming from land use and land cover changes that effect reservoir inflows pose additional stresses to the region currently the water utilities drought mitigation strategies rely entirely on the water use restrictions a measure that is expensive and deeply unpopular with local residents while the three utilities are each facing increased water stress their capacity to demand ratios and access to new supply options differ as shown in fig 5 and table 5 this demand to capacity ratio varies greatly even across regions which means that utilities facing the same hydrologic conditions can have very different vulnerability to drought the asymmetry across the utilities demand to capacity ratios in the sedento valley test case their close geographic proximity and interconnected infrastructure represents an opportunity for cooperative regional water management strategies there is an interest in cooperative infrastructure investment pathways and coordinated drought mitigation using a regionalized portfolio approach zeff et al 2014 to improve regional and individual performance of water supply systems each of the three utilities has identified potential new infrastructure investments to improve their water supply reliability watertown has the option to expand college rock reservoir this can either be a large expansion or a small expansion dryville has two small potential water supply options the development of granite quarry into a reservoir and the construction of sugar creek reservoir fallsland and watertown have also been investigating a joint infrastructure investment in the construction of the new river reservoir a listing of potential infrastructure improvements can be found in table 6 the three utilities are seeking to find a cooperative strategy that links infrastructure pathways planning with drought mitigation policy as part of this policy the utilities are interested in incorporating newly available financial tools to hedge against unexpected costs from drought mitigation the optimization problem formulation is presented in detail in section s6 of the supplementary information as mentioned we will illustrate the core functionality of waterpaths through an application of the du pathways framework trindade et al 2019 for the sedento valley example the du pathways framework as illustrated here has three core steps beyond the problem formulation all of which a deliberative iterative process that is repeated until a satisfactory solution has been found 1 identify tradeoffs 2 evaluating robustness and 3 infrastructure pathway analytics our application of waterpaths to support these steps for the sedento valley test is described below see trindade et al 2019 for more details on the du pathways framework all data pertaining to the sedento valley test case is available in the code repository mentioned in the software availability section 3 2 identifying tradeoffs the sedento valley water portfolio management and infrastructure investment pathway demonstration is a challenging high dimensional in terms of decisions and objectives stochastic multiobjective problem our demonstration of waterpaths capabilities and use for discovering the sedento valley test case s tradeoffs exploits the ms borg moea hadka et al 2013 the ms borg moea combines adaptive operator selection vrugt and robinson 2007 based on probabilities calculated from solutions in its ε dominance archive laumanns et al 2002 this combination makes it suitable to solve problems with a wide range of mathematical characteristics the borg moea has demonstrated superior performance over a diverse set of multiobjective problems such as benchmarking test problems water supply portfolio planning pollution control given ecological thresholds groundwater monitoring design and reservoir control hadka and reed 2012 reed et al 2013 zatarain salazar et al 2016 ward et al 2015 furthermore the ms borg moea has demonstrated that it is capable of solving infrastructure pathway problems zeff et al 2016 trindade et al 2019 without the need of parameter tuning which makes it the ideal choice for engineers not familiar with the technical aspects of evolutionary optimization the standard values of parameters of the borg moea v1 8 master worker were used in this work see hadka et al 2013 for the specific values although waterpaths can be readily used with any available modern moea or other search tools the ms borg moea parallel design facilitates our demonstration of the framework s ability to support state of the art massively parallel analytical capabilities involving input and output of potentially high amounts of data fig 1b shows the optimization loop and the single simulation input output flow for this work we performed all optimization runs on texas advanced computing center s tacc stampede 2 the runs were performed on the skx computing nodes comprised of two intel xeon platinum 8160 skylake with 48 cores 2 1 ghz and 192 ghz of ram we found that nine different optimization seeds ran with ms borg moea each with 125 000 function evaluations and a unique number of nodes was sufficient to converge to the best attainable approximate pareto set based on the evolution of the hypervolume metric seen in supplementary fig s5 each function evaluation was performed using the monte carlo functionality of waterpaths by sampling 1000 realizations each realizations fully defines one full draw of candidate values for all of the deeply uncertain factors of concern presented in table 7 each single draw of these du values are then coupled to one realization of a 45 year synthetically generated record for streamflows evaporation rates and demands overall we augment the limits of the 80 year historical record for the sedento valley system by creating 1000 cross correlated synthetic streamflows and evaporation rate time series realizations for each reservoir and stream gauge these natural times series are also used to inform the generation of 1000 synthetic demand time series for each utility which account for correlations with changing levels of water scarcity for each utility the simulation of all 1000 fully specified realizations and subsequent calculation of the specified objectives constitutes one function evaluation waterpaths software design permits significant flexibility for stochastic simulation as well as stochastic simulation optimization the resulting output pareto approximation sets from each of the ms borg moea random seed trials on tacc stampede 2 were combined and sorted using ε d o m i n a n c e sorting to build a best known reference set for the sedento valley demonstration problem the use of ε d o m i n a n c e provides a convenient means of preserving high quality representations of key tradeoffs sets while reducing solution sets to a user preferred size to improve interpretability of tradeoff assessments and or limit the computational demands of the robustness assessments described below readers interested in more details can reference reed et al 2013 kollat and reed 2007 for the sedento valley example presented here we filtered the reference set of solutions using two steps 1 ε dominance sorting of all solutions based with our specified ε values and 2 filtering solutions based on performance goals for specific objectives see discussion in the next section 3 3 evaluating robustness the next step was to further stress test solutions in the final reference set generated by ε dominance sorting all solutions based on specified ε values by re evaluating them against a larger independent sampling of 5000 vectors of the du factors presented in table 7 for each of the 5000 re evaluation runs we created one set of 1000 streamflows evaporation rate and demand time series based on the first three deeply uncertain factor in table 7 time series for each reservoir and gauge resulting in 5 000 000 sows the processes for the generation of inflows evaporation rate and demand time series is presented in the supplemental section s3 the process for choosing the number of monte carlo samples used to evaluate each solution in the reference set is explained in detail in the supplemental section s5 before proceeding with such computationally expensive re evaluation and robustness evaluation the initial reference set resulting in 830 non dominated solutions was reduced in size to 229 solutions before being re evaluated over 5000 re evaluation scenarios for computational tractability as mentioned in section 3 2 this was achieved by keeping in the reference set only solutions whose objective values as calculated during optimization displayed reliability greater than 98 restriction frequency on less than 30 of the years and worse first percentile cost smaller than 10 of the annual revenue these criteria were chosen for 1 being similar to minimum performance standards deemed acceptable based on the authors prior experience with real utilities and 2 for reducing the number of solutions limit computational demands the resulting 229 5 000 1 145 000 simulations required for re evaluation were split into 25 independent jobs on stampede 2 each job consisted in one python script using mpi4py distributed memory parallelization to use distribute blocks of 229 solutions 200 rdm scenarios 22 900 function evaluations across 50 nodes resulting in 229 solutions 4 rdm scenarios 916 function evaluations per node per job for each of the 25 jobs the reason for the split was to improve the fault tolerance of the re evaluation exercise by preventing a crashing function evaluation from crashing the entire re evaluation exercise as a crash in one function evaluation would crash the python script and in turn all its function evaluations each of the 50 nodes ran one function evaluation at a time distributing its 1000 realizations across all 48 cores with 2 realizations running simultaneously on each core the objective values for each of the 5000 re evaluation function evaluations for each of the policies in the abbreviated combined reference set were used to re calculate the policies objectives this time based on 1 000 5 000 5 000 000 realizations rather than the initial 1000 using during the moea search phase the re calculation of the objectives for each solution was performed by averaging the values of each objective across all 5000 runs except for the worse first percentile cost objective which was calculated as the worse first percentile across all 5000 runs additionally the 5000 sets of objective values obtained for each policy were used to calculate the value of the satisficing robustness metric starr 1962 herman et al 2014 trindade et al 2019 for each utility under each policy defined as the percentage of the 5000 sets that met the performance criteria defined by the utilities in short the satisficing measure of robustness measures the percent of sample worlds where decision makers deem performance acceptable based specified performance requirements readers interested in more details on the robustness assessment in the du pathways framework can reference trindade et al 2019 for more details 3 4 pathway analytics based on the robustness values a solution was selected for a detail scenario discovery analysis bryant and lempert 2010 performed with the boosted trees algorithm schapire 1999 as presented in trindade et al 2019 scenario discovery is an effort to determine the uncertainties factor prioritization and or combinations of their values factor mapping that most closely relate to performance and robustness boosted trees was needed for the sedento valley test case because of two advantages it holds over the other more easily interpretable but limited scenario discovery methods such as prim bryant and lempert 2010 friedman and fisher 1999 cart bryant and lempert 2010 breiman et al 1984 and logistic regression quinn et al 2018 gold et al 2019 1 boosted trees captures non differentiable boundaries typical from threshold based rules such as waterpaths rof based action triggers 2 it captures non linear boundaries without explicitly modeling variable interactions while being resistant to overfitting helping assure scenario discovery maps that are as simple as possible to interpret to obtain the maps and factor priorities a python analysis script provided with the distribution of waterpaths reads all samples 5000 of deeply uncertain factors and corresponding objectives and for each solution fits a boosted trees classifier to the uncertain factors and objective values for each utility for each policy the surface is then presented in maps indicating the regions of the space of uncertainties in which that policy is likely to fail for each utility as well as the sources of uncertainty that most impact the performance of each utility under that policy the script for fitting the boosted tree classifier and the plotting the maps is based on the scikit learn machine learning library pedregosa et al 2011 and provided in the data processing repository lastly all infrastructure pathway data output for each of the 5000 function evaluations performed for the policy being analyzed is used to create plots representing infrastructure construction as a function of time the data is comprised of the realization id the utility that triggered a project the infrastructure option that was built and the week in which the option became operational three pathway plots are created for each utility under the policy being analyzed one presenting infrastructure construction over time for all realizations under the least favorable sample of deeply uncertain factors one for the projected values for each deeply uncertain factor and one for the most favorable these plots can be used to understand which projects are likely to be suitable for construction in the near term and how dependent on uncertainty the need for construction is 3 5 scaling performance on clusters and in the cloud in this comparative assessment replicate optimization random seed trial runs were run on small and large traditional high performance computing clusters as well as a virtual cloud cluster scaling performance for shared memory parallelization on a single desktop computer can be found in supplementary section s9 cornell red cloud the virtual cloud cluster utilized in this study is similar to commercial cloud services that may be utilized by water utilities future decision support systems will need to support emerging subscription based computational support services with advances in scientific cloud computing in the scaling analysis all waterpaths model runs or function evaluations were distributed across computing nodes on stampede 2 with intel mpi for linux v2019u5 distributed memory parallelization library intel 2019 and on the cornell red cloud and on the research cluster using openmpi v3 1 4 gabriel et al 2004 while the sampled realizations that composed the function evaluations were distributed across cores within a node using openmp v5 0 shared memory parallelization library stallman and community 2017 klemm et al 2019 this hybrid parallelization scheme as discussed in section 2 11 and shown in detail in supplementary fig s4b has the advantage of allowing the user to scale optimization runs up to as many computing nodes as are available while avoiding core idle time due to memory access and size limitations within a node the within node shared memory parallelization of realizations within a function evaluation also has the advantage of allowing the user to run more realizations simultaneously this feature is particularly useful as the number of cores per processor is increasing and as cloud computing with nodes of various sizes are becoming more readily available in addition to optimization runs performed on stampede 2 performance was assessed on a research cluster termed the cube whose nodes are comprised of two intel xeon e5 2680 sandy bridge 2 7 ghz and 128 gb of ram the cloud results were attained on cornell red cloud with nodes with 28 cores and variable architecture the scaling analysis consists of two types of tests to evaluate search performance on each platform search scalability and cross platform wall clock search time comparisons our goal for the multi architecture comparison tests is to benchmark how performance varies for typical small clusters elite class high performance computing hpc systems and emerging cloud platforms our scalability benchmarking focuses on efficiency as a function of the number of nodes cores as defined in eq 2 derived from amdahl s law assuming negligible necessarily serial work our core demonstration results are based on nine random seed search trials on stampede 2 where the seed trial count allowed our experiment to exploit the maximum allowed by the system administrators the cube and red cloud analyses are based on three random seed trial runs for the sole purpose of informing this computational scalability analysis each seed had one ms borg moea master process and a number of worker processes based on the number of nodes and of processes per node 2 e n 0 t n 0 a r c h n t n n o d e s a r c h in eq 2 e is parallel efficiency t n 0 a r c h is the total execution time on the base case here eight nodes for a given architecture and n is the number of nodes to be compared against the base case eq 2 assumes the vast majority of the execution time is spent on tasks run in parallel table 8 shows which computer configurations were used for each test the configurations of each seed can be found in table 8 together with details about the architecture of the compute nodes used table 8 also shows that the exercise of optimizing a model built with waterpaths using ms borg moea is nearly perfectly scalable when running function evaluations in parallel for optimization on either low or high numbers of nodes in high performance computing applications these results highlight that when using waterpaths with ms borg moea investments in high performance computing power will yield returns linearly proportional to the number of nodes which corresponds to ideal scalability furthermore table 8 shows that the tested cloud finished a full optimization run in 3665 min when using 8 nodes versus 8274 min for the local traditional hpc system on 8 nodes showing that despite notoriously slower inter node communication clouds can potentially result in great time savings given the high costs of traditional hpc systems these results highlight the potential of cloud computing platforms for solving engineering problems at compared higher speed and lower costs specially with the increasing numbers of cores per instance in modern cloud systems up to 96 in current cloud platforms 4 results 4 1 performance and robustness tradeoffs fig 6 shows the objectives panel a and robustness panel b tradeoffs for the sedento valley utilities following re evaluation as output by waterpaths for details on re evaluation see supplementary section s6 in fig 6a each axis represents an objective and each line represents a policy the location where each line policy intersects a vertical axes represents it performance value in the corresponding objective the highlighted policies represent the policies with best robustness for each utility bro x with the best robustness compromise across utilities roc and best performance compromise pc the axes in fig 6a are oriented such that the ideal policy would be a horizontal line at the bottom of all axes compromise policies were chosen through unanimity fallback bargaining brams and kilgour 2001 which has been shown to select realistic compromises in environmental resource problems madani et al 2011 to conduct fallback bargaining each bargainer rank orders their preferences across potential policies if a single policy is the most preferable it is selected if the most preferable policy differs across bargainers they each fallback in lockstep to less desirable policies until a policy is found that is satisfactory to all bargainers brams and kilgour 2001 in this study roc was chosen by treating each utility as a bargainer who valued solutions based upon their robustness pc was chosen by treating each regional objective as a separate bargainer and choosing a solution that compromised across objectives by chance the best robustness solution for fallsland br f was also the robustness compromise solution selected through fallback bargaining roc fig 6a highlights tradeoffs between reliability and restriction frequency which is expected given restrictions are a supply reliability instrument likewise significant tradeoffs exist between infrastructure npv and financial cost for higher reliabilities indicating that the construction of infrastructure and soft path management options that increase drought mitigation costs and the need for financial instruments fig 6a also shows that the low reliability policies tend to generally have between 5 and 15 of restriction frequency and that policies with reduced demand management depend on higher investment levels a small group of policies with the highest reliability make use of moderate restrictions and investments in new infrastructure these solutions would be difficult to discover with existing frameworks and were made possible by waterpaths unique combination of supply and financial models furthermore policies are discovered instead of being prespecified and the triggered sequences of infrastructure are dynamically consistent with the sows as well as short term management actions transitioning to robustness tradeoffs fig 6b uses a similar parallel axes plot to display the percentages of sows where each utility meets their goal performance requirements reliability 98 restriction frequency 10 and annual worst first percentile cost 10 in fig 6b the direction of preference is upward so that the ideal policy would be represented by a horizontal line at the top of the axes i e 100 for all utilities the most robust policies for each utility and the best robustness and objectives performance compromises across all utilities are again highlighted in different colors in fig 6b the highlighted best robustness policies in fig 6b suggest inter utility robustness tradeoffs for the highest levels of attained robustness which indicate complex interdependencies between the utilities caused by utilities attempting to simultaneously use constrained regional resources furthermore performance compromise solution shows that focusing on performance levels may degrade robustness for all utilities which would likely go against the utilities risk aversion the capability of simulating resources shared by utilities to allow for regional conflict analysis is increasingly important given how the distance between areas serviced by water utilities is decreasing and is a distinct capability of waterpaths fig 7 supplements fig 6b by comparing utilities robustness profiles across all of the policies found through optimization one important result highlighted in figs 7 and 6b is the strong source of potential regional tension depending on if the utilities seek to focus on overall performance tradeoffs versus individual robustness tradeoffs for dryville and fallsland the illustrated distances between the blue robustness compromise and the fuchsia performance compromise solutions in both figures implies that seeking a regional management and investment policy compromise for performance tradeoffs could make dryville and fallsland more vulnerable individually fig 7 highlights that there are very few solutions that are simultaneously close to the highest attainable robustness for each individual utility as illustrated by the steep downward slope in bar heights on the leftmost region of all 3 bar charts which is concern that should be considered explicitly in any regional water portfolio management and infrastructure investment policy negotiation waterpaths provides detailed analyses that can be helpful for these types of regional decision making contexts the simulation system allows stakeholders to explore how the component policy decision variables themselves are driving these tradeoffs 4 2 policy rules in the robustness compromises fig 8 provides a detailed illustration of the rules that compose the water portfolio management and infrastructure investment policies highlighted in figs 6 and 7 panels a g show parallel axis plots of the decision variables across the three utilities the three utilities are shown on the x axis and each line represents a highlighted policy with its y axis value corresponding to its decision variable values recall that each policy is comprised of 1 rof triggers for short term mitigation instruments fig 8a b and 8c and for long term infrastructure investments fig 8e 2 lake michael allocations fig 8f 3 annual contingency fund contributions and insurance payouts defined as percentages of annual revenue fig 8d and g and 4 infrastructure construction order table within fig 8 the table included in fig 8 contains the infrastructure construction order for each policy and each utility listed in the order shown in the legend and colored according to each policy the first striking feature of fig 8 are the similarities between all of the highlighted solutions regarding the use of supply focused instruments that are strongly influential for reliability namely restrictions transfers infrastructure construction lake michael allocation and infrastructure construction order the latter for dryville and fallsland in all policies all utilities make extensive use of restrictions due to its low trigger value build considerable infrastructure and almost the entirety of the lake michael is allocated to watertown in addition dryville also has high reliance on transfers from watertown comparison of figs 8b to 7 highlights a possible relation between transfer obligations for watertown and its robustness as the most robust policy for watertown implies less treated water transfers higher triggers in fig 8 and robustness for the other two utilities smaller robustness for dryville and fallsland in fig 7 transitioning to the utilities use of financial instrument there are significant differences in their financial decisions across the highlighted solutions fig 7 shows that all of the highlighted policies except for watertown s most robust have values between 2 5 and 5 for the contingency fund variable suggesting this is an instrument to be explored by all utilities in addition fig 8 shows that dryville s insurance strategy tends to gravitate towards either unusually infrequent use of insurance and or low payments both effectively meaning insurance is less useful for dryville watertown s and fallsland s financial strategies on the other hand varied across policies depending their focus on performance robustness and alternative compromises a key feature of waterpaths is providing a self consistent means of simulating and evaluating how short term management actions shape sequences of infrastructure investments in fig 8 the rof based management and investment action triggers interact with the ordering or prioritization of infrastructure sequences for each of the utilities as fig 8 shows the infrastructure construction order is stable for dryville and fallsland across solutions but varies for watertown in all policies watertown builds early on the new river reservoir which is shared with fallsland and for whom it is also the first option to be built indicating the importance of this joint project also all utilities in all solutions prioritize the construction of added storage as opposed to water reuse stations except for watertown in its most robust policy which indicates high storage s higher efficacy in improving supply and financial performance the inversion in priority between expanding the college rock reservoir and building reuse stations for watertown in their most robust policy may also been connected to its high robustness in this policy and to the decrease of fallsland robustness given the college rock reservoir helps stabilize the flows into the new river reservoir the similarities in the use of reliability focused instruments across utilities also suggests that their differences in robustness may be substantially dependent on the utilities differences in their use of their financial instruments this leaves room for fine tuning of policies using waterpaths given that financial instruments are part of individual as opposed to regional planning and that the during optimization the financial instruments were optimized only for the worse performing utility for each objective with an understanding of the decisions being made by the utilities under each policy the next step is to understand which uncertainty factors and their values drive robustness 4 3 scenario discovery for compromise policies waterpaths as a flexible monte carlo simulation system enables careful exploratory modeling to better understand what deeply uncertain factors most strongly influence robustness each map in fig 9 shows the performance attained across scenarios for the two most important uncertainties for each of the three utilities when the sedento valley implements the policy with the best robustness compromise highlighted in figs 6 and 7 in each panel of fig 9 the percentages shown in the axes denote the importance of each source of uncertainty in determining whether the mentioned policy would or would not meet the utilities performance criteria in the samples scenarios as identified by the boosted trees algorithm as mentioned in section 3 4 trindade et al 2019 more specifically the percentages represent the decrease in impurity of the tree ensemble from splits on that factor with a higher percentage indicating a higher importance of that factor the red dots denote scenarios in which the utility fails to meet its performance targets and gray dots represent scenarios a utility performs as or better than required red shading represents regions of the uncertainty space where a utility under the mentioned policy will likely not meet the performance criteria with p meeting criteria scenario 0 5 white regions denote inconclusive regions in which 0 98 p meeting criteria scenario 0 5 inconclusive regions and gray regions represent scenario regions in which the utility will likely meet the performance criteria with p meeting criteria scenario 0 98 all probabilities were calculated using boosted trees with 500 trees of depth 4 in each panel of fig 9 the stars represent the most favorable scenario gray star the least favorable orange star and the baseline case a sow representing the future according to best estimate values that the utilities use in planning when not examining deep uncertainty for example a demand growth multiplier of 100 demands grow exactly according to forecasts and a restriction state effectiveness of 100 consumers respond to restrictions exactly as utilities assume they d blue star fig 9 shows that all of the utilities robustness are mostly contingent on demand growth but that contrary to expectations both low and high values of demand growth may cause the three utilities to fail however low and high demand failures happen for very different reasons failures due to low demand result for all three utilities when low long term rof values do not trigger infrastructure construction leading to over dependence on financially challenging restrictions i e high restriction frequencies and volatile cost swings the low demand failures result when restrictions and short term transfers are not sufficient for preventing supply failures causing low reliabilities and high restriction frequencies in low demand scenarios all of the utilities struggle with financial stability and fail to meet the robustness criterion for the worse first percentile cost objective which is worse first percentile cost smaller 10 this highlights the importance of the study of innovative financial instruments for water utilities which may mitigate such issues overall fig 9 provides some important regional insights for the sedento valley the dependence on failures in all three objectives on demand growth rates indicate an intimate relationship between supply reliability and financial stability overall keeping demand growth rates at 20 above the utilities projected value would be an important step for all of the utilities to ensure satisfactory regional performance while avoiding stranded assets which requires high demands because of their high costs other uncertainties such as permitting time construction cost overruns interest rates and others did not have a significant impact on the utilities performance the next section will explore how the scenarios highlighted above by the stars drive infrastructure construction as a way of understanding the relationship between infrastructure construction and robustness performance uncertainty factors 4 4 infrastructure ensemble pathway analysis fig 10 illustrates the detailed analyses that waterpaths enables for each of the utilities which includes the joint outputs for capacity expansions short term rof dynamics and pathways for each of the utilities under their individual most favorable projected and least favorable scenarios highlighted by starts in fig 9 figs 10a c plot watertown s storage capacity and short term rof dynamics for each of the three scenarios likewise figs 10d f shows watertown s resulting ensemble of infrastructure pathways across the three highlighted demand growth scenarios the same panel layout is repeated for the dryville and fallsland pathways the horizontal axes in the pathway panels represent time in weeks from 2015 to 2060 the planning horizon each infrastructure pathway plot displays a stack of results attained for 1000 sows and the horizontal multicolored lines each represent the construction sequence that would be triggered under a specific hydrological scenario on the vertical axis for each horizontal line in the stack the color of a segment represents the infrastructure option built last in time that being the case a transition in color indicates the time when an infrastructure option becomes operational for example the orange horizontal segment starting at the bottom center significant and early infrastructure year 26 of the panel p indicates that in that realization one with more build infrastructure options fallsland finished the construction of the new river reservoir 9 years after finishing the construction of its reuse station brought online in year 17 fig 10 panels d through f indicate that the amount of infrastructure built by watertown varies significantly with demand growth while the scenario with the highest demand growth triggers infrastructure investments for all hydrological realizations and still fails to minimize short term rof values the projected future triggers infrastructure investments in less than 10 of the hydrological realizations however the low investments in infrastructure observed in the projected scenario shown in panel e makes watertown more susceptible to swings in revenue in more challenging realizations due to the need for frequent restrictions such financial swings cause watertown to fail the worse first percentile cost target resulting in a red zone around the blue star in fig 9a on the other hand all three scenarios for dryville and fallsland see the same infrastructure investment patterns with small timing variations which combined with the short term drought mitigation and financial instruments are generally enough to make utilities meet their robustness performance criteria in the regions highlighted in gray in fig 9 an important aspect of the infrastructure investment problem is the effect of permitting times on the scheduled capacity expansions which may cause the construction of an infrastructure option with high permitting time to be postponed in favor of another option with already granted permits in the pathways illustrated in fig 10 panels d through f and p through r for watertown and fallsland the occurrence of purple high capacity college rock expansion and green lines fallsland reuse segments respectively before the orange regions new river reservoir shows that this issue often happen to the jointly built new river reservoir resulting in an observed infrastructure construction order that does not match the order specified in the corresponding policy robustness compromise policy in fig 8 more specifically in scenarios of higher stress when either watertown or fallsland trigger the news river reservoir its permit may be strongly delayed by the extended permitting period so watertown may trigger instead the college rock reservoir expansion and fallsland may trigger the construction of its water reuse station both with substantially reduced permitting times 5 conclusion recent projections estimate that the us will require over one trillion dollars of investment in water supply infrastructure in the next 20 years which can potentially be minimized by exploring synergies between infrastructure investment and non structural drought mitigation instruments both shared among regional actors the coupling of long term infrastructure investment strategies with short term drought mitigation necessitates a model with capabilities not all found in a single currently available software joint simulation of multiple utilities involved in joint planning and operations of shared water resources and infrastructure joint financial supply simulation broad array of uncertainties within a single simulation easy customizability and ready integration with multiobjective optimization algorithms for use with high performance computing waterpaths was designed and shown to fill these gaps we present the design of waterpaths an open source model written in the c programming language and designed to fit the du pathways in addition we demonstrate the application of waterpaths on a hypothetical water regionally coordinated infrastructure planning and management problem called the sedento valley using waterpaths we modeled the decision making and system upgrades of the regionally coordinating water utilities in the sedento valley under well characterized and deep uncertainty to explore policy options discovered inter dependencies between utilities and discovered vulnerabilities revealed under deep uncertainty additionally waterpaths facilitates stochastic tradeoff analyses by integrating state of the art parallel search that can exploit a broad array of high performance computing platforms the sedento valley test case was designed to be itself a contribution to the field of water systems engineering it was designed to be a benchmark test case for the direct comparison of competing regional decision making frameworks for the design and comparison of drought mitigation strategies and for the evaluation of decision making metrics while maintaining computational tractability and interpretability the sedento valley test case captures a high degree of complexity due to the number of utilities involved to the unbalanced distribution of resources and liabilities across utilities and to the utilities locations in relation to each other in the local shared basins the code repository provided in waterpaths code repository contains all the data needed to reproduce the test case in waterpaths or other frameworks and to compare new methodologies and policy instruments against the ones presented this paper overall waterpaths contributes a state of the art stochastic simulation and analytical platform that holds promise for addressing the challenges posed by water supply planning under deep uncertainty globally software availability name of software waterpaths description waterpaths is an open source c model for the stochastic simulation of decision making policies by water utilities developer b trindade bct52 cornell edu with contributions by p reed d gold contributed to the development of the sedento valley test case funding source funding for this work was provided by the national institute of food and agriculture u s department of agriculture wsc agreement no 2014 67003 22076 additional support was provided by the u s national science foundation s water sustainability and climate program award no 1360442 source language c supported systems unix linux windows mac license bsd 3 clause clear license availability https github com bernardoct waterpaths the code used for the waterpaths optimization runs can be found at https bit ly 3auj2bu the code used for the waterpaths re evaluation runs can be found at https bit ly 2o8ktxo the code used to process the output data from optimization and re evaluation can be found at https github com bernardoct paper3 the full paths to the github repositories including the commit hashes can be found in the supplemental section s12 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments funding for this work was provided by the national institute of food and agriculture u s department of agriculture wsc agreement no 2014 67003 22076 additional support was provided by the u s national science foundation s nsf water sustainability and climate program award no 1360442 the cloud computing components of the study were supported by the nsf s office of advanced cyberinfrastructure award no 1541215 the views expressed in this work represent those of the authors and do not necessarily reflect the views or policies of the nsf or the usda the authors would also like to acknowledge the cornell center for advanced computing as well as the texas advanced computing center appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104772 
25990,financial risk access to capital regulatory processes and regional competition for limited water sources represent dominant concerns in the united states as well as the broader global water supply sector this work introduces the waterpaths simulation software a generalizable cloud compatible open source exploratory modeling system designed to inform long term regional investments in water infrastructure while simultaneously aiding regions to improve their short term weekly management decisions often made in response to droughts uniquely waterpaths has the capability to identify coordinated planning and management for groups of water utilities sharing water resources waterpaths exploits dynamic and adaptive risk of failure rof rules to trigger management and planning actions in temporally consistent pathways the compact and efficient rof based representation of decision pathways allows waterpaths to scale efficiently with the number of regional actors and their candidate actions lastly as a platform for supporting decision making under deep uncertainty waterpaths accounts for a broad range of uncertainties including hydrological or climate extremes permitting time demand growth effectiveness of water use restrictions construction costs and financing uncertainties to demonstrate the capabilities of waterpaths we introduce a new hypothetical water resources test case the sedento valley the sedento valley test case contains three resource sharing water utilities that seek to regionally coordinate their policies for drought mitigation and infrastructure investment the three utilities are challenged by a diverse set of deep uncertainties that encompass natural and human systems stressors the sedento valley test case contributes a new opportunity for benchmarking decision support tools and water resources systems simulation software keywords water supply decision making under deep uncertainty robustness infrastructure pathways simulation 1 introduction recent projections estimate that the united states us will require over one trillion dollars of investment in water supply infrastructure in the next 20 years asce 2017 this investment represents a difficult challenge as water fees which often fund water infrastructure projects are already rising above the consumer price index uswa 2019b a hall et al 2019 and initiatives such as the water infrastructure finance and innovation act commonly known as wifia copeland 2016 have only been able to address a fraction of the required investment epa 2019 this challenge is mirrored around the world as water managers face the task of maintaining supply reliability under climatic social and financial uncertainties bonzanigo et al 2018 awwa 2019 in many regions rapid shifts in seasonal climate drought patterns and the frequency of extreme events threaten to overwhelm existing water supply capacity shafer and fox 2017 these threats are felt more acutely in large metropolitan areas which have observed strong and sustained population growth in recent decades mcnabb 2019 census 2019 to face these challenges water managers must transition planning frameworks to jointly address growing uncertainties and to include the recent innovations in sustainable freshwater management gleick 2018 historically water managers have relied on infrastructure expansion to confront long term supply risks and utilized water use restrictions to manage short term drought crises gleick 2002b this approach often yields high financial burdens as utilities are forced to accumulate large amounts of debt revenue disruptions resulting from water use restrictions may cause utilities to miss payments on this debt threatening their financial stability and increasing the borrowing costs hughes and leurig 2013 moody s 2017 2019 additionally the land or water resources most suitable for new water supply infrastructure in developed countries has largely been developed and allocated lund 2013 and new water infrastructure projects such as large dams pose environmental concerns and face significant regulatory uncertainties these challenges have led to the proposal of soft path strategies to water resources management that seek to compliment centralized infrastructure with non structural measures to improve efficiency and manage water demand gleick 2002a exploring synergies between long term infrastructure investment pathways and non structural drought mitigation instruments holds promise for incorporating soft path strategies into water supply planning and management potential short term management instruments include demand management hall 2019 zeff et al 2014 borgomeo et al 2018 huskova et al 2016 haasnoot et al 2013 treated water transfers through regional shared infrastructure caldwell and characklis 2014 zeff et al 2014 watkins and mckinney 1997 hall 2019 and raw water transfers through upstream reservoir releases borgomeo et al 2018 gorelick et al 2019 integrating these short term management instruments with long term infrastructure sequencing allows utilities to create adaptive management strategies that maintain reliability under drought conditions while minimizing the need for infrastructure expansion zeff et al 2016 the coupling of long term infrastructure investment strategies with short term drought mitigation necessitates innovations in decision support systems for water resources management problems broadly decision support systems are composed of the suite of analytical mathematical and or simulation focused tools that aid planners in the design and operation of water resources systems loucks and da costa 2013 there is large historical body of literature on water resources planning and management where simulation models are a central focus for analysts developing insights on how a system behaves under varying system states or decision maker actions see review in loucks and van beek 2017 since the inception of the field in the early 1960s water systems models have typically consisted of networks of storage and junction nodes linked by conveyance structures such as pipelines canals and river reaches maass et al 1962 harou et al 2009 there is a large array of existing water systems simulation software that has contributed substantial benefits for decision support applications including modsim labadie 2011 water evaluation and planning weap sieber 2006 interactive river aquifer simulation iras 2010 matrosov et al 2011 wathnet kuczera 1992a ribasim hydraulics 2004 mike basin jha and gupta 2003 source welsh et al 2013 calvin draper et al 2003 oasis hydrologics 2009 riverware zagona et al 2001 and pywr tomlinson et al 2020 for a summarized feature comparison between waterpaths and other mentioned software see tables 1 and 2 more recently there has been a growing interest in multiobjective simulation optimization applications where models such as modsim iras riverware wathnet and pywr are coupled with multiobjective evolutionary algorithms moeas to aid in the discovery of key water supply planning and management tradeoffs matrosov et al 2015 borgomeo et al 2018 basdekas 2014 smith et al 2016 tomlinson et al 2020 moeas are global population based search algorithms that evolve sets of pareto approximate solutions to multiobjective problems through processes of mating mutation and selection for reviews see nicklow et al 2010 maier et al 2014 and coello et al 2007 however search based decision support tools such as moeas require an effective and efficient software coupling with simulation software moreover typical applications substantially increase the computational demands of analyses commonly requiring tens of thousands of simulation based evaluations of performance objectives to guide search processes in the context of simulation optimization applications modern water systems simulation software systems have key technical limitations 1 they often do not provide users with flexibility in representing complex state aware actions as an artifact of their approach to water balance modeling e g optimization or rule based allocations that cannot capture information feedbacks 2 the most widely used simulations do not provide access to their underlying source code bases e g commercial software packages 3 despite the growing recognition that a broad array of uncertainties strongly shape water resources systems e g financial risks behavioral responses changing hydro climatic conditions they are highly constrained in their representation and scalable computational support for including these concerns and 4 they lack the ability to perform planning exercises including simultaneous consideration of both short and long term decisions beyond purely software related technical challenges there are broader conceptual challenges in how decision support framework s treat uncertainties in particular deep uncertainties for recent reviews see moallemi et al 2019 dittrich et al 2016 kwakkel and haasnoot 2019 herman et al 2015 deep uncertainty refers to conditions when decision makers do not know or cannot agree upon probability distributions of key system parameters and or the system boundaries kwakkel et al 2016 lempert 2002 water supply planning and management under deep uncertainty transitions from a focus on discovering optimal management strategies to crafting robust and adaptive strategies that maintain performance across a wide array of potential future conditions walker et al 2013 dittrich et al 2016 frameworks such as decision scaling brown et al 2012 robust decision making rdm lempert et al 2006 many objective robust decision making mordm kasprzyk et al 2013 and info gap ben haim 2006 provide methods to aid decision makers in the discovery of key uncertainties control system vulnerability these methods often involve computationally demanding exploratory monte carlo simulation analysis that strongly benefits from scalable simulation software that can be adapted across a range of state of the art computing architectures the challenges and needs summarized above motivated this work s contribution of the open source waterpaths stochastic simulation software which has been specifically developed to support integrated infrastructure investment pathway planning and water portfolio management under deep uncertainty trindade et al 2019 waterpaths allows for a flexible and efficient representation of multi actor water resources systems while providing advanced computational support for multiobjective optimization algorithms and exploratory analyses of a broad range of uncertainties waterpaths supports water supply infrastructure and water portfolio management applications for systems confronting water scarcity or increasingly severe droughts waterpaths provides decision makers with ability to flexibly abstract water infrastructure portfolio problems that contain measures for addressing the short term supply and financial impacts of droughts along with long term capacity expansions the simulation system was developed to run on desktops cloud computing systems and on high performance computing resources it utilizes both shared memory and distributed memory parallelization to enable decision makers to efficiently design optimize and evaluate candidate water supply portfolios over large ensembles of potential future states of the world in this study we demonstrate waterpaths by introducing the sedento valley a detailed and realistic hypothetical multi actor regional water supply planning test case waterpaths is used to represent three hypothetical water utilities in the south eastern us that are facing the prospect of water shortage due to growing demand and changing climate the utilities seek to craft both short term drought mitigation responses and long term infrastructure investment pathways that maintain reliable supply and financial stability the utilities have the potential to cooperate using treated transfers through existing infrastructure and through shared infrastructure development 2 the waterpaths simulation system 2 1 overview waterpaths is an open source and modular model for the simulation and optimization of water infrastructure planning and management policies the simulation system is designed to facilitate decision making under deep uncertainty dmdu for interested readers supplement s1 provides a more detailed review of several commonly employed dmdu frameworks that center on planned adaptation we demonstrate waterpaths functionality using the recently proposed deeply uncertain pathways du pathways framework for designing dynamic and adaptive infrastructure investment and management policies trindade et al 2019 the du pathways framework builds upon many objective robust decision making mordm kasprzyk et al 2013 a methodology for deliberative decision support processes that explore deeply uncertain tradeoffs and vulnerabilities in complex environmental systems a schematic of du pathways methodology in fig 1 a showcases the iterative decision making process in four steps problem formulation generating alternatives reevaluation under deep uncertainty and policy diagnostics the iterative nature of the framework facilitates a constructive decision aiding approach tsoukias 2008 that enables decision makers to learn about the problem and their preferences through exploratory modeling bankes 1993 the du pathways framework builds upon mordm by explicitly incorporating the design of rule systems that react to observed system states du pathways provides a means of implementing closed loop control policies bertsekas et al 1995 for infrastructure sequencing and drought mitigation the simultaneous optimization of risk based infrastructure triggers and drought mitigation instruments effectively bridges the gap between long term infrastructure investment and short term water portfolio management zeff et al 2016 trindade et al 2019 demonstrate that the du pathways approach can produce robust and adaptive infrastructure pathways that perform well across broad sets of uncertainty and balance conflicting interests within multi actor systems successful implementation of the du pathways framework is a computationally intensive task due to the required stochastic simulations and large numbers of function evaluations during moea search these properties make it difficult implement with existing decision support systems the waterpaths modeling system has been specifically developed to aid the application and extension of the du pathways decision support framework fig 1b illustrates how waterpaths facilitates both multiobjective optimization and policy evaluation when in optimization mode which includes elements in fig 1b enclosed in the red dotted line waterpaths streamlines its output to efficiently run with a multiobjective evolutionary algorithm in policy evaluation mode which includes elements enclosed in the green dotted line in fig 1b waterpaths gathers and outputs detailed diagnostics on policy performance and system states waterpaths was designed for centralized system state information recording to allow for easy implementation of custom objective functions it currently has five objectives implemented but more can be easily added to the masterdatacollector class the currently implemented objectives are reliability restriction frequency net present value of infrastructure built during the life of the policy maximum annual financial cost of drought mitigation and infrastructure construction and worse first percentile annual financial cost of drought mitigation and infrastructure construction the details of which are provided in supplemental section s4 waterpaths is equipped with an input file parser a simple user interface that facilitates applications by users with limited to no programming capabilities due to waterpaths open source code base new features and other customizations can be added by users with a basic knowledge of c new features such as new types of water infrastructure customized reservoir control rules and new drought mitigation policies can be easily added by extending the already implemented abstract classes similarly any system metric for decision making purposes can be implemented with the addition of a single function to the existing code the mass balance model rof calculation model and other aspects of the systemwide simulation are designed to work with any child class of the existing abstract classes making it possible for a user to flexibly add features to waterpaths on a high level waterpaths organizes the simulation of a system of cooperating water utilities in terms of a mass balance performed for each piece of water infrastructure utilities drawing water from and building new water infrastructure as well as drought mitigation and financial policies that guide utilities supply and financial operations a diagram showing the object oriented structure of waterpaths can be found in supplementary section s11 for a given utility each time step day week or month the simulation follows the sequence shown in fig 2 1 calculation of long term rof metric if at first week of a calendar year 2 calculation of the short term rof 3 application of drought mitigation and financial policies 4 system mass balance modeling and 5 system state data collection each of waterpaths specific features are as illustrated in fig 2 discussed in the subsequent sections see fig 3 2 2 rof based decision rules the facilitation of decision making through state aware rule systems is a core component of the waterpaths simulation system in waterpaths the default formulation of investment and management policies exploits short and long term rofs triggers which constitute dynamic measures of a utility s capacity to demand ratio palmer and characklis 2009 zeff et al 2016 a graphical depiction of an example rof calculation can be found in fig 1 for a given utility at a given system state total combined storage level the rof is the estimated probability that the utility s storage will fall below a critical storage level in the next t r o f weeks if the system were subjected to hydrological conditions from the past 50 years of the historical record under default settings short term rof used to trigger drought mitigation actions is calculated each week during a simulation using t r o f 52 the calculation of short term rof at each time step allows policies to trigger drought mitigation instruments that respond to current reservoir storage levels the use of t r o f 52 is designed so that drought mitigation responds to single year droughts long term rof is calculated at the first day of each calendar year of a simulation assumes the utility s storage is at capacity and uses t r o f 78 this formulation provides information on the suitability of utility storage capacity to handle multi year droughts under default settings the critical storage level for all utilities is set to 20 of total storage a more mathematically detailed description of the rof metrics is available in section 2 of the supplement the state aware nature of rofs means that a water management and infrastructure investment policies generated by waterpaths generate contextually appropriate sequences of actions for the conditions being experienced in a given realization i e a policy will take different actions in wet versus dry futures or high demand versus low demand futures this closed loop style of integrated short term management and long term investment rules better account for the path dependencies of action sequences provide an overall higher degree of adaptivity and better exploit information feed backs relative to traditional abstractions of decisions in water supply literature see detailed discussion in trindade et al 2017 this is especially important for systems that are subject to deep uncertainties that challenge traditional fixed formulations that take the same actions regardless of the context of the conditions being experienced knox et al 2018 huskova et al 2016 borgomeo et al 2016 matrosov et al 2011 formulating decisions through state aware rof triggers is particularly powerful when combined with deep uncertainty optimization du optimization which confronts candidate policies with a broad array of challenging mild and middling future conditions allowing for the discovery of flexible policy sets that mitigate tail risks without incurring exorbitant baseline costs trindade et al 2017 for a water resources decision support system the inclusion of state aware rofs triggers is a non trivial aspect of the simulation optimization runtime waterpaths contains an efficient representation of both short and long term rofs that exploits shared memory parallelization to efficiently run on desktop computers cloud systems and hpc clusters for parallelization details see section 10 of the supplement additionally the rof metric can be customized to capture single year droughts as done by default for short term drought management instruments or multiple year droughts the default for long term infrastructure sequencing actions the modular design of waterpaths also makes it straightforward for users to implement and test their own state aware decision triggers 2 3 mass balance model the core of waterpaths is its mass balance model the mass balance is solved for all water infrastructure following an upstream to downstream order defined through infrastructure connectivity information provided by the user as a directed graph the mass balance equation for reservoirs is shown in eq 1 1 x s w 1 x s w n i w s e w u r o w e r w r a x s w e o w s w r d w where x s w 1 is the volume of water stored in the reservoir at the week after the current week w n i is the natural inflow into the reservoir from all its tributaries s e is a treated sewage effluent discharged either on a tributary or directly on the reservoir u r o is the upstream reservoir total outflow if such reservoir exists mandated outflow plus spillage e r is a non dimensional evaporation rate r a is the reservoir area as a function of stored volume e o is the environmental outflow r d w is the total municipal demand drawn from that reservoir by one or more independently modeled utilities and s is the reservoir spillage which is set to zero unless the reservoir is completely full if a reservoir is allocated for multiple uses municipal supply to multiple utilities water quality etc each with a designated percentage of the total storage capacity the inflow is split across all uses proportionally to their allocated volumes water in excess of any allocated capacity is redistributed among the others to allow for more realistic simulation of reservoirs waterpaths also provides an abstract class for reservoir control rules named minenvflowcontrol and one for controls of other discharges such as treated waste water discharges the latter flowing into a reservoir or stream named control rules waterpaths also simulates non storage water infrastructure such as water intakes and water re use stations other water infrastructure such as desalinization plants can be easily implemented by extending the abstract class water sources based on a custom mass balance function waterpaths also has implemented upgrades of existing infrastructure namely reservoir expansions and treatment capacity expansions 2 4 capturing regional decision making to support regional decision making waterpaths can simulate multiple water utilities potentially with interconnected networks shared infrastructure as separate elements all within the same regional system each utility in waterpaths divides its demand among its storage based infrastructure based on the allocated stored volume of water after drawing as much as possible from non storage infrastructure e g intakes and reuse stations the volume of water in storage owned by a given utility is also used to calculate its short and long term rof metrics as described in section 2 2 the rof dynamics are central to the timing and coordination of candidate water management decisions in waterpaths each candidate action is triggered based on a utility defined rof level i e the level of supply failure risk at which a given action is taken this abstraction is drawn from the common usage of risk tables in practice and it provides state aware adaptivity additionally water utilities in waterpaths track their own finances the finances as modeled as cost fluctuations due to drought mitigation and financial instruments and debt repayment the total annual revenue is also calculated based demand consumer tiers and corresponding tariffs and is used to support objectives calculations and financial instruments 2 5 drought mitigation instruments building from prior works zeff and characklis 2013 zeff et al 2014 2016 trindade et al 2017 palmer and characklis 2009 caldwell and characklis 2014 waterpaths has implemented water use restrictions and inter utility treated water transfers although other instruments can be easily added by creating a new child class of the drought mitigation instrument class water use restrictions as currently implemented in waterpaths can be implemented on multiple tiers each with different percentages of demand reduction based on stricter measures reductions in lawn irrigation car and sidewalk washing etc and triggered by a different value of the rof metric revenue losses due to restricted water sales can be mitigated by the use of higher contingency tariffs provided by the user to be used for the duration of the enactment of water use restriction treated water transfers in waterpaths are performed from a source utility to requesting utilities as water use restrictions requests for treated transfers are contingent on the current value of the short term rof metric for each requesting utility however treated water transfers often suffer from two constraints treatment capacity at the source utility and limited inter utility conveyance capacity to calculate the volume granted to each utility waterpaths solves a constrained allocation problem using a quadratic programming algorithm goldfarb and idnani 1983 gaspero 2007 the problem is set up to minimize the mean square error between the volumes requested by each utility adjusted for their rof values a utility with a higher rof receives proportionally more water and the volumes that can be transferred through the inter utility network subject to conveyance constraints funds are then transferred from all requesting to the source utility drought mitigation instruments however can be costly and utilities may benefit from setting in place financial instruments to absorb and stabilize such costs 2 6 financial instruments waterpaths allows utilities to hedge against the negative financial effects of the drought mitigation instruments by using financial instruments such as the drought insurance and contingency funds zeff et al 2014 both already implemented in waterpaths the drought insurance currently implemented in waterpaths triggers a payout of a percentage of the previous year s annual revenue whenever the value of the rof metric reaches a set value the policy is updated priced and bought by the utilities every year based on the most current values of annual revenue and on a premium of 20 of the expected cost of the policy for the following year other types of index insurance policies based on state variables other than the rof can be easily added to waterpaths by creating a child class of the drought mitigation policy class contingency funds are not dependent on the rof metric and instead are implemented as a fixed percentage of the annual revenue on the last week of every year each utility that has a contingency fund adds a fixed percentage of that year s total revenue to its contingency fund these funds are then used during the following year to pay for drought mitigation policies other financial policies and for debt issued to build new infrastructure 2 7 reservoir flow control the reservoir controls policies currently implemented in waterpaths are fixed release release based on inflow seasonal minimum release and storage based release while the mentioned standard control policies cover a broad range of application contexts the option of creating customized reservoir control rules may be used to reflect the particulars of a system as an example of a custom policy rule releases could be made to be contingent on current stored volume and on the flow at a gauge at a tributary merging downstream from the reservoir in question 2 8 infrastructure investment infrastructure construction is triggered based on an action threshold or trigger for the long term rof metric if a utility or region has several candidate infrastructure investment options users or search must provide a construction sequence i e an infrastructure pathway across time to allow for infrastructure construction over the course of the simulated planning period waterpaths relies on information passed by the user or optimization algorithm about all infrastructure options storage capacity treatment capacity cost etc and the sequence in which a utility is to build such options if the value of the long term rof metric reaches its trigger value this is a unique feature of waterpaths which allows the user to prioritize options in the near term and define requirements infrastructure options can be located anywhere in and outside of the reservoir connectivity network infrastructure options in waterpaths may take the form of altogether new projects such as new reservoirs water intakes treatment plants etc and of expansion of current infrastructure such as storage treatment capacity expansions and reservoir re allocations this allows waterpaths to account for flexible infrastructure development as emphasized in the real options analysis literature as important elements in the long term planning for water utilities fletcher et al 2017 2019 erfani et al 2018 wang and de neufville 2005 cox et al 1979 infrastructure development is often financed over decades rather than paid upfront waterpaths allows for multiple types of bonds to be issued for financing new infrastructure resulting in different possible debt repayment streams with different net present values for the same infrastructure option currently implemented in waterpaths are level debt service balloon payment and variable interest bonds however waterpaths allows for the design of creative finance mechanisms by allowing the user to create new types of bonds by creating children classes of the bond class and including those in optimization and simulation exercises 2 9 flexible representation and evaluation of uncertainties one of the key features that differentiates waterpaths from existing simulation systems is its ability to incorporate a broad array of uncertainties typically water systems simulation systems can only run one realization a scenario fully specified by one time series of stream flows evaporation rates and demands when pertinent and one value for other uncertainty factors at a time which implies in no explicit consideration of uncertainties see tables 1 and 2 others most notably the real options analysis frameworks include uncertainty in policy optimization in a bayesian fashion using decision trees and stochastic dynamic programming over expected costs fletcher et al 2017 2019 hui et al 2018 there are a growing number of example applications that consider uncertainty in a stochastic fashion by simulating a policy over hundreds or thousands of realizations every time the model is called with policy optimization being performed by attaching the model to a black box optimization algorithm zeff et al 2014 2016 kwakkel et al 2014 watson and kasprzyk 2016 borgomeo et al 2018 trindade et al 2019 waterpaths was designed to augment this latter approach by broadening the scope of actions and uncertainties that can be jointly evaluated waterpaths currently requires as minimum uncertainty related input one time series of inflows evaporation rates and demands for each piece of infrastructure and utility when applicable for each realization to be run in addition the user has the option of providing a series of multipliers representing deeply uncertainty factors which if not provided will assume a default value of the unit all time series and multipliers are to be sampled externally from any desired distribution and passed to waterpaths as input data the user can add as many deeply uncertain factors as desired when setting up a problem or in new classes of water infrastructure controls and drought mitigation policies the deep uncertainties currently included in waterpaths are presented in table 3 simulating and optimizing a policy using a stochastic approach to modeling uncertainty is a mathematically complex and can be computationally costly so care was taken to allow waterpaths to both be computationally efficient and to make use of any scale of computational resources available to the analyst a discussion of sampling strategies for optimization and exploratory modeling with the default version of waterpaths can be found in section s6 of the supplement 2 10 reducing runtime with local parallel computing waterpaths stochastic design allows for detailed uncertainty analysis that requires large ensembles of simulations however given that each realization can be simulated independently they can be easily distributed across multiple computational cores cpus within a processor of a modern desktop laptop or cloud computing instance the shared memory parallelization scheme in waterpaths several cores working on the same process while sharing the same block of memory is implemented using openmp 5 0 code directives klemm et al 2019 the openmp directives allow waterpaths to create a given number of computational threads and distribute them across available cores 2 11 facilitating decision analytics given the diversity and number of policy actions that can be evaluated with waterpaths the resulting water infrastructure investment and management pathways are typically composed of mixtures of discrete variables continuous values and permutations of infrastructure options as a consequence objective measuring system performance across different metrics are non convex and discontinuous functions of the policy variables moreover the uncertainty sampling enabled by waterpaths has the potential to make the performance metric functions noisy and highly heterogeneous in their behavior across water supply or financial concerns all these traits make the problem of manually or automatically designing policies for water infrastructure planning and management particularly difficult and attractive to researchers zeff et al 2016 fletcher et al 2019 borgomeo et al 2018 huskova et al 2016 kwakkel et al 2014 beh et al 2017 waterpaths policy input and objectives output was designed for easy integration with any black box multiobjective optimization algorithm and currently has out of the box integration with the master worker borg multiobjective optimization evolutionary algorithm ms borg moea hadka and reed 2013 2014 although support other algorithms written in c or other languages can be easily added 3 methodology 3 1 the sedento valley an illustrative test case we demonstrate the functionality of waterpaths using the sedento valley test case a novel hypothetical system that reflects many of the challenges facing regions with several independently operated urban water utilities we developed the sedento valley test case to contribute a benchmark problem for the evaluation of future methodologies for water infrastructure planning and management we designed the test case to encompass 1 a region with a growing population that stresses current water supplies 2 a system of multiple urban water utilities in close geographic proximity and asymmetric vulnerability to drought 3 utilities that face financial vulnerability to future drought risk 4 the potential for inter utility cooperation on short term drought mitigation planning and 5 the potential for cooperative infrastructure development between interconnected utilities the test case consists of three utilities in close geographic proximity facing increasing vulnerability to drought conditions due to climatic stressors population growth and changes in land use and land coverage gorelick et al 2020 the close proximity of the three urban utilities in the test case provides an opportunity for regional cooperation and shared infrastructure development but also creates the potential for inter utility conflicts over scarce resources this study demonstrates waterpaths through a demonstrative implementation of the deeply uncertain du pathways methodology trindade et al 2019 to examine infrastructure investment and drought management strategies for the sedento valley the du pathways analysis demonstrates key waterpaths capabilities 1 monte carlo evaluation of financial institutional and hydro climatic uncertainties 2 rof based adaptive infrastructure pathways zeff et al 2016 and 3 management of a mix of short term water supply portfolio instruments for drought mitigation characklis et al 2006 zeff et al 2014 the sedento valley shown in fig 4 is home to two medium sized cities dryville and fallsland and a smaller municipality watertown the population of the valley is near 1 5 million residents the three municipalities are each supplied by an independent water utility the cities of dryville and fallsland currently receive water from the autumn lake reservoir a large flood control reservoir owned and operated by the us army corps of engineers usace watertown owns and operates college rock reservoir and receives water from lake michael another large usace operated reservoir dryville and fallsland also have emergency allocations to lake michael where they have access by purchasing water from watertown s treatment plant and transferring via shared pipelines current supply capacities of each water utility can be found in table 4 the unallocated portion of lake michael is used by other utilities in the region that are not being modeled to meet environmental flow requirements see section 2 7 and as a water quality pool a growing population has reduced each of the utilities capacity to demand ratios increasing their vulnerability to drought conditions this problem is compounded by the increased difficulty of large infrastructure investments i e new reservoirs due to scarcity in feasible sites higher costs and reduced tolerance of environmental impacts increased hydro climatic variability and uncertainties stemming from land use and land cover changes that effect reservoir inflows pose additional stresses to the region currently the water utilities drought mitigation strategies rely entirely on the water use restrictions a measure that is expensive and deeply unpopular with local residents while the three utilities are each facing increased water stress their capacity to demand ratios and access to new supply options differ as shown in fig 5 and table 5 this demand to capacity ratio varies greatly even across regions which means that utilities facing the same hydrologic conditions can have very different vulnerability to drought the asymmetry across the utilities demand to capacity ratios in the sedento valley test case their close geographic proximity and interconnected infrastructure represents an opportunity for cooperative regional water management strategies there is an interest in cooperative infrastructure investment pathways and coordinated drought mitigation using a regionalized portfolio approach zeff et al 2014 to improve regional and individual performance of water supply systems each of the three utilities has identified potential new infrastructure investments to improve their water supply reliability watertown has the option to expand college rock reservoir this can either be a large expansion or a small expansion dryville has two small potential water supply options the development of granite quarry into a reservoir and the construction of sugar creek reservoir fallsland and watertown have also been investigating a joint infrastructure investment in the construction of the new river reservoir a listing of potential infrastructure improvements can be found in table 6 the three utilities are seeking to find a cooperative strategy that links infrastructure pathways planning with drought mitigation policy as part of this policy the utilities are interested in incorporating newly available financial tools to hedge against unexpected costs from drought mitigation the optimization problem formulation is presented in detail in section s6 of the supplementary information as mentioned we will illustrate the core functionality of waterpaths through an application of the du pathways framework trindade et al 2019 for the sedento valley example the du pathways framework as illustrated here has three core steps beyond the problem formulation all of which a deliberative iterative process that is repeated until a satisfactory solution has been found 1 identify tradeoffs 2 evaluating robustness and 3 infrastructure pathway analytics our application of waterpaths to support these steps for the sedento valley test is described below see trindade et al 2019 for more details on the du pathways framework all data pertaining to the sedento valley test case is available in the code repository mentioned in the software availability section 3 2 identifying tradeoffs the sedento valley water portfolio management and infrastructure investment pathway demonstration is a challenging high dimensional in terms of decisions and objectives stochastic multiobjective problem our demonstration of waterpaths capabilities and use for discovering the sedento valley test case s tradeoffs exploits the ms borg moea hadka et al 2013 the ms borg moea combines adaptive operator selection vrugt and robinson 2007 based on probabilities calculated from solutions in its ε dominance archive laumanns et al 2002 this combination makes it suitable to solve problems with a wide range of mathematical characteristics the borg moea has demonstrated superior performance over a diverse set of multiobjective problems such as benchmarking test problems water supply portfolio planning pollution control given ecological thresholds groundwater monitoring design and reservoir control hadka and reed 2012 reed et al 2013 zatarain salazar et al 2016 ward et al 2015 furthermore the ms borg moea has demonstrated that it is capable of solving infrastructure pathway problems zeff et al 2016 trindade et al 2019 without the need of parameter tuning which makes it the ideal choice for engineers not familiar with the technical aspects of evolutionary optimization the standard values of parameters of the borg moea v1 8 master worker were used in this work see hadka et al 2013 for the specific values although waterpaths can be readily used with any available modern moea or other search tools the ms borg moea parallel design facilitates our demonstration of the framework s ability to support state of the art massively parallel analytical capabilities involving input and output of potentially high amounts of data fig 1b shows the optimization loop and the single simulation input output flow for this work we performed all optimization runs on texas advanced computing center s tacc stampede 2 the runs were performed on the skx computing nodes comprised of two intel xeon platinum 8160 skylake with 48 cores 2 1 ghz and 192 ghz of ram we found that nine different optimization seeds ran with ms borg moea each with 125 000 function evaluations and a unique number of nodes was sufficient to converge to the best attainable approximate pareto set based on the evolution of the hypervolume metric seen in supplementary fig s5 each function evaluation was performed using the monte carlo functionality of waterpaths by sampling 1000 realizations each realizations fully defines one full draw of candidate values for all of the deeply uncertain factors of concern presented in table 7 each single draw of these du values are then coupled to one realization of a 45 year synthetically generated record for streamflows evaporation rates and demands overall we augment the limits of the 80 year historical record for the sedento valley system by creating 1000 cross correlated synthetic streamflows and evaporation rate time series realizations for each reservoir and stream gauge these natural times series are also used to inform the generation of 1000 synthetic demand time series for each utility which account for correlations with changing levels of water scarcity for each utility the simulation of all 1000 fully specified realizations and subsequent calculation of the specified objectives constitutes one function evaluation waterpaths software design permits significant flexibility for stochastic simulation as well as stochastic simulation optimization the resulting output pareto approximation sets from each of the ms borg moea random seed trials on tacc stampede 2 were combined and sorted using ε d o m i n a n c e sorting to build a best known reference set for the sedento valley demonstration problem the use of ε d o m i n a n c e provides a convenient means of preserving high quality representations of key tradeoffs sets while reducing solution sets to a user preferred size to improve interpretability of tradeoff assessments and or limit the computational demands of the robustness assessments described below readers interested in more details can reference reed et al 2013 kollat and reed 2007 for the sedento valley example presented here we filtered the reference set of solutions using two steps 1 ε dominance sorting of all solutions based with our specified ε values and 2 filtering solutions based on performance goals for specific objectives see discussion in the next section 3 3 evaluating robustness the next step was to further stress test solutions in the final reference set generated by ε dominance sorting all solutions based on specified ε values by re evaluating them against a larger independent sampling of 5000 vectors of the du factors presented in table 7 for each of the 5000 re evaluation runs we created one set of 1000 streamflows evaporation rate and demand time series based on the first three deeply uncertain factor in table 7 time series for each reservoir and gauge resulting in 5 000 000 sows the processes for the generation of inflows evaporation rate and demand time series is presented in the supplemental section s3 the process for choosing the number of monte carlo samples used to evaluate each solution in the reference set is explained in detail in the supplemental section s5 before proceeding with such computationally expensive re evaluation and robustness evaluation the initial reference set resulting in 830 non dominated solutions was reduced in size to 229 solutions before being re evaluated over 5000 re evaluation scenarios for computational tractability as mentioned in section 3 2 this was achieved by keeping in the reference set only solutions whose objective values as calculated during optimization displayed reliability greater than 98 restriction frequency on less than 30 of the years and worse first percentile cost smaller than 10 of the annual revenue these criteria were chosen for 1 being similar to minimum performance standards deemed acceptable based on the authors prior experience with real utilities and 2 for reducing the number of solutions limit computational demands the resulting 229 5 000 1 145 000 simulations required for re evaluation were split into 25 independent jobs on stampede 2 each job consisted in one python script using mpi4py distributed memory parallelization to use distribute blocks of 229 solutions 200 rdm scenarios 22 900 function evaluations across 50 nodes resulting in 229 solutions 4 rdm scenarios 916 function evaluations per node per job for each of the 25 jobs the reason for the split was to improve the fault tolerance of the re evaluation exercise by preventing a crashing function evaluation from crashing the entire re evaluation exercise as a crash in one function evaluation would crash the python script and in turn all its function evaluations each of the 50 nodes ran one function evaluation at a time distributing its 1000 realizations across all 48 cores with 2 realizations running simultaneously on each core the objective values for each of the 5000 re evaluation function evaluations for each of the policies in the abbreviated combined reference set were used to re calculate the policies objectives this time based on 1 000 5 000 5 000 000 realizations rather than the initial 1000 using during the moea search phase the re calculation of the objectives for each solution was performed by averaging the values of each objective across all 5000 runs except for the worse first percentile cost objective which was calculated as the worse first percentile across all 5000 runs additionally the 5000 sets of objective values obtained for each policy were used to calculate the value of the satisficing robustness metric starr 1962 herman et al 2014 trindade et al 2019 for each utility under each policy defined as the percentage of the 5000 sets that met the performance criteria defined by the utilities in short the satisficing measure of robustness measures the percent of sample worlds where decision makers deem performance acceptable based specified performance requirements readers interested in more details on the robustness assessment in the du pathways framework can reference trindade et al 2019 for more details 3 4 pathway analytics based on the robustness values a solution was selected for a detail scenario discovery analysis bryant and lempert 2010 performed with the boosted trees algorithm schapire 1999 as presented in trindade et al 2019 scenario discovery is an effort to determine the uncertainties factor prioritization and or combinations of their values factor mapping that most closely relate to performance and robustness boosted trees was needed for the sedento valley test case because of two advantages it holds over the other more easily interpretable but limited scenario discovery methods such as prim bryant and lempert 2010 friedman and fisher 1999 cart bryant and lempert 2010 breiman et al 1984 and logistic regression quinn et al 2018 gold et al 2019 1 boosted trees captures non differentiable boundaries typical from threshold based rules such as waterpaths rof based action triggers 2 it captures non linear boundaries without explicitly modeling variable interactions while being resistant to overfitting helping assure scenario discovery maps that are as simple as possible to interpret to obtain the maps and factor priorities a python analysis script provided with the distribution of waterpaths reads all samples 5000 of deeply uncertain factors and corresponding objectives and for each solution fits a boosted trees classifier to the uncertain factors and objective values for each utility for each policy the surface is then presented in maps indicating the regions of the space of uncertainties in which that policy is likely to fail for each utility as well as the sources of uncertainty that most impact the performance of each utility under that policy the script for fitting the boosted tree classifier and the plotting the maps is based on the scikit learn machine learning library pedregosa et al 2011 and provided in the data processing repository lastly all infrastructure pathway data output for each of the 5000 function evaluations performed for the policy being analyzed is used to create plots representing infrastructure construction as a function of time the data is comprised of the realization id the utility that triggered a project the infrastructure option that was built and the week in which the option became operational three pathway plots are created for each utility under the policy being analyzed one presenting infrastructure construction over time for all realizations under the least favorable sample of deeply uncertain factors one for the projected values for each deeply uncertain factor and one for the most favorable these plots can be used to understand which projects are likely to be suitable for construction in the near term and how dependent on uncertainty the need for construction is 3 5 scaling performance on clusters and in the cloud in this comparative assessment replicate optimization random seed trial runs were run on small and large traditional high performance computing clusters as well as a virtual cloud cluster scaling performance for shared memory parallelization on a single desktop computer can be found in supplementary section s9 cornell red cloud the virtual cloud cluster utilized in this study is similar to commercial cloud services that may be utilized by water utilities future decision support systems will need to support emerging subscription based computational support services with advances in scientific cloud computing in the scaling analysis all waterpaths model runs or function evaluations were distributed across computing nodes on stampede 2 with intel mpi for linux v2019u5 distributed memory parallelization library intel 2019 and on the cornell red cloud and on the research cluster using openmpi v3 1 4 gabriel et al 2004 while the sampled realizations that composed the function evaluations were distributed across cores within a node using openmp v5 0 shared memory parallelization library stallman and community 2017 klemm et al 2019 this hybrid parallelization scheme as discussed in section 2 11 and shown in detail in supplementary fig s4b has the advantage of allowing the user to scale optimization runs up to as many computing nodes as are available while avoiding core idle time due to memory access and size limitations within a node the within node shared memory parallelization of realizations within a function evaluation also has the advantage of allowing the user to run more realizations simultaneously this feature is particularly useful as the number of cores per processor is increasing and as cloud computing with nodes of various sizes are becoming more readily available in addition to optimization runs performed on stampede 2 performance was assessed on a research cluster termed the cube whose nodes are comprised of two intel xeon e5 2680 sandy bridge 2 7 ghz and 128 gb of ram the cloud results were attained on cornell red cloud with nodes with 28 cores and variable architecture the scaling analysis consists of two types of tests to evaluate search performance on each platform search scalability and cross platform wall clock search time comparisons our goal for the multi architecture comparison tests is to benchmark how performance varies for typical small clusters elite class high performance computing hpc systems and emerging cloud platforms our scalability benchmarking focuses on efficiency as a function of the number of nodes cores as defined in eq 2 derived from amdahl s law assuming negligible necessarily serial work our core demonstration results are based on nine random seed search trials on stampede 2 where the seed trial count allowed our experiment to exploit the maximum allowed by the system administrators the cube and red cloud analyses are based on three random seed trial runs for the sole purpose of informing this computational scalability analysis each seed had one ms borg moea master process and a number of worker processes based on the number of nodes and of processes per node 2 e n 0 t n 0 a r c h n t n n o d e s a r c h in eq 2 e is parallel efficiency t n 0 a r c h is the total execution time on the base case here eight nodes for a given architecture and n is the number of nodes to be compared against the base case eq 2 assumes the vast majority of the execution time is spent on tasks run in parallel table 8 shows which computer configurations were used for each test the configurations of each seed can be found in table 8 together with details about the architecture of the compute nodes used table 8 also shows that the exercise of optimizing a model built with waterpaths using ms borg moea is nearly perfectly scalable when running function evaluations in parallel for optimization on either low or high numbers of nodes in high performance computing applications these results highlight that when using waterpaths with ms borg moea investments in high performance computing power will yield returns linearly proportional to the number of nodes which corresponds to ideal scalability furthermore table 8 shows that the tested cloud finished a full optimization run in 3665 min when using 8 nodes versus 8274 min for the local traditional hpc system on 8 nodes showing that despite notoriously slower inter node communication clouds can potentially result in great time savings given the high costs of traditional hpc systems these results highlight the potential of cloud computing platforms for solving engineering problems at compared higher speed and lower costs specially with the increasing numbers of cores per instance in modern cloud systems up to 96 in current cloud platforms 4 results 4 1 performance and robustness tradeoffs fig 6 shows the objectives panel a and robustness panel b tradeoffs for the sedento valley utilities following re evaluation as output by waterpaths for details on re evaluation see supplementary section s6 in fig 6a each axis represents an objective and each line represents a policy the location where each line policy intersects a vertical axes represents it performance value in the corresponding objective the highlighted policies represent the policies with best robustness for each utility bro x with the best robustness compromise across utilities roc and best performance compromise pc the axes in fig 6a are oriented such that the ideal policy would be a horizontal line at the bottom of all axes compromise policies were chosen through unanimity fallback bargaining brams and kilgour 2001 which has been shown to select realistic compromises in environmental resource problems madani et al 2011 to conduct fallback bargaining each bargainer rank orders their preferences across potential policies if a single policy is the most preferable it is selected if the most preferable policy differs across bargainers they each fallback in lockstep to less desirable policies until a policy is found that is satisfactory to all bargainers brams and kilgour 2001 in this study roc was chosen by treating each utility as a bargainer who valued solutions based upon their robustness pc was chosen by treating each regional objective as a separate bargainer and choosing a solution that compromised across objectives by chance the best robustness solution for fallsland br f was also the robustness compromise solution selected through fallback bargaining roc fig 6a highlights tradeoffs between reliability and restriction frequency which is expected given restrictions are a supply reliability instrument likewise significant tradeoffs exist between infrastructure npv and financial cost for higher reliabilities indicating that the construction of infrastructure and soft path management options that increase drought mitigation costs and the need for financial instruments fig 6a also shows that the low reliability policies tend to generally have between 5 and 15 of restriction frequency and that policies with reduced demand management depend on higher investment levels a small group of policies with the highest reliability make use of moderate restrictions and investments in new infrastructure these solutions would be difficult to discover with existing frameworks and were made possible by waterpaths unique combination of supply and financial models furthermore policies are discovered instead of being prespecified and the triggered sequences of infrastructure are dynamically consistent with the sows as well as short term management actions transitioning to robustness tradeoffs fig 6b uses a similar parallel axes plot to display the percentages of sows where each utility meets their goal performance requirements reliability 98 restriction frequency 10 and annual worst first percentile cost 10 in fig 6b the direction of preference is upward so that the ideal policy would be represented by a horizontal line at the top of the axes i e 100 for all utilities the most robust policies for each utility and the best robustness and objectives performance compromises across all utilities are again highlighted in different colors in fig 6b the highlighted best robustness policies in fig 6b suggest inter utility robustness tradeoffs for the highest levels of attained robustness which indicate complex interdependencies between the utilities caused by utilities attempting to simultaneously use constrained regional resources furthermore performance compromise solution shows that focusing on performance levels may degrade robustness for all utilities which would likely go against the utilities risk aversion the capability of simulating resources shared by utilities to allow for regional conflict analysis is increasingly important given how the distance between areas serviced by water utilities is decreasing and is a distinct capability of waterpaths fig 7 supplements fig 6b by comparing utilities robustness profiles across all of the policies found through optimization one important result highlighted in figs 7 and 6b is the strong source of potential regional tension depending on if the utilities seek to focus on overall performance tradeoffs versus individual robustness tradeoffs for dryville and fallsland the illustrated distances between the blue robustness compromise and the fuchsia performance compromise solutions in both figures implies that seeking a regional management and investment policy compromise for performance tradeoffs could make dryville and fallsland more vulnerable individually fig 7 highlights that there are very few solutions that are simultaneously close to the highest attainable robustness for each individual utility as illustrated by the steep downward slope in bar heights on the leftmost region of all 3 bar charts which is concern that should be considered explicitly in any regional water portfolio management and infrastructure investment policy negotiation waterpaths provides detailed analyses that can be helpful for these types of regional decision making contexts the simulation system allows stakeholders to explore how the component policy decision variables themselves are driving these tradeoffs 4 2 policy rules in the robustness compromises fig 8 provides a detailed illustration of the rules that compose the water portfolio management and infrastructure investment policies highlighted in figs 6 and 7 panels a g show parallel axis plots of the decision variables across the three utilities the three utilities are shown on the x axis and each line represents a highlighted policy with its y axis value corresponding to its decision variable values recall that each policy is comprised of 1 rof triggers for short term mitigation instruments fig 8a b and 8c and for long term infrastructure investments fig 8e 2 lake michael allocations fig 8f 3 annual contingency fund contributions and insurance payouts defined as percentages of annual revenue fig 8d and g and 4 infrastructure construction order table within fig 8 the table included in fig 8 contains the infrastructure construction order for each policy and each utility listed in the order shown in the legend and colored according to each policy the first striking feature of fig 8 are the similarities between all of the highlighted solutions regarding the use of supply focused instruments that are strongly influential for reliability namely restrictions transfers infrastructure construction lake michael allocation and infrastructure construction order the latter for dryville and fallsland in all policies all utilities make extensive use of restrictions due to its low trigger value build considerable infrastructure and almost the entirety of the lake michael is allocated to watertown in addition dryville also has high reliance on transfers from watertown comparison of figs 8b to 7 highlights a possible relation between transfer obligations for watertown and its robustness as the most robust policy for watertown implies less treated water transfers higher triggers in fig 8 and robustness for the other two utilities smaller robustness for dryville and fallsland in fig 7 transitioning to the utilities use of financial instrument there are significant differences in their financial decisions across the highlighted solutions fig 7 shows that all of the highlighted policies except for watertown s most robust have values between 2 5 and 5 for the contingency fund variable suggesting this is an instrument to be explored by all utilities in addition fig 8 shows that dryville s insurance strategy tends to gravitate towards either unusually infrequent use of insurance and or low payments both effectively meaning insurance is less useful for dryville watertown s and fallsland s financial strategies on the other hand varied across policies depending their focus on performance robustness and alternative compromises a key feature of waterpaths is providing a self consistent means of simulating and evaluating how short term management actions shape sequences of infrastructure investments in fig 8 the rof based management and investment action triggers interact with the ordering or prioritization of infrastructure sequences for each of the utilities as fig 8 shows the infrastructure construction order is stable for dryville and fallsland across solutions but varies for watertown in all policies watertown builds early on the new river reservoir which is shared with fallsland and for whom it is also the first option to be built indicating the importance of this joint project also all utilities in all solutions prioritize the construction of added storage as opposed to water reuse stations except for watertown in its most robust policy which indicates high storage s higher efficacy in improving supply and financial performance the inversion in priority between expanding the college rock reservoir and building reuse stations for watertown in their most robust policy may also been connected to its high robustness in this policy and to the decrease of fallsland robustness given the college rock reservoir helps stabilize the flows into the new river reservoir the similarities in the use of reliability focused instruments across utilities also suggests that their differences in robustness may be substantially dependent on the utilities differences in their use of their financial instruments this leaves room for fine tuning of policies using waterpaths given that financial instruments are part of individual as opposed to regional planning and that the during optimization the financial instruments were optimized only for the worse performing utility for each objective with an understanding of the decisions being made by the utilities under each policy the next step is to understand which uncertainty factors and their values drive robustness 4 3 scenario discovery for compromise policies waterpaths as a flexible monte carlo simulation system enables careful exploratory modeling to better understand what deeply uncertain factors most strongly influence robustness each map in fig 9 shows the performance attained across scenarios for the two most important uncertainties for each of the three utilities when the sedento valley implements the policy with the best robustness compromise highlighted in figs 6 and 7 in each panel of fig 9 the percentages shown in the axes denote the importance of each source of uncertainty in determining whether the mentioned policy would or would not meet the utilities performance criteria in the samples scenarios as identified by the boosted trees algorithm as mentioned in section 3 4 trindade et al 2019 more specifically the percentages represent the decrease in impurity of the tree ensemble from splits on that factor with a higher percentage indicating a higher importance of that factor the red dots denote scenarios in which the utility fails to meet its performance targets and gray dots represent scenarios a utility performs as or better than required red shading represents regions of the uncertainty space where a utility under the mentioned policy will likely not meet the performance criteria with p meeting criteria scenario 0 5 white regions denote inconclusive regions in which 0 98 p meeting criteria scenario 0 5 inconclusive regions and gray regions represent scenario regions in which the utility will likely meet the performance criteria with p meeting criteria scenario 0 98 all probabilities were calculated using boosted trees with 500 trees of depth 4 in each panel of fig 9 the stars represent the most favorable scenario gray star the least favorable orange star and the baseline case a sow representing the future according to best estimate values that the utilities use in planning when not examining deep uncertainty for example a demand growth multiplier of 100 demands grow exactly according to forecasts and a restriction state effectiveness of 100 consumers respond to restrictions exactly as utilities assume they d blue star fig 9 shows that all of the utilities robustness are mostly contingent on demand growth but that contrary to expectations both low and high values of demand growth may cause the three utilities to fail however low and high demand failures happen for very different reasons failures due to low demand result for all three utilities when low long term rof values do not trigger infrastructure construction leading to over dependence on financially challenging restrictions i e high restriction frequencies and volatile cost swings the low demand failures result when restrictions and short term transfers are not sufficient for preventing supply failures causing low reliabilities and high restriction frequencies in low demand scenarios all of the utilities struggle with financial stability and fail to meet the robustness criterion for the worse first percentile cost objective which is worse first percentile cost smaller 10 this highlights the importance of the study of innovative financial instruments for water utilities which may mitigate such issues overall fig 9 provides some important regional insights for the sedento valley the dependence on failures in all three objectives on demand growth rates indicate an intimate relationship between supply reliability and financial stability overall keeping demand growth rates at 20 above the utilities projected value would be an important step for all of the utilities to ensure satisfactory regional performance while avoiding stranded assets which requires high demands because of their high costs other uncertainties such as permitting time construction cost overruns interest rates and others did not have a significant impact on the utilities performance the next section will explore how the scenarios highlighted above by the stars drive infrastructure construction as a way of understanding the relationship between infrastructure construction and robustness performance uncertainty factors 4 4 infrastructure ensemble pathway analysis fig 10 illustrates the detailed analyses that waterpaths enables for each of the utilities which includes the joint outputs for capacity expansions short term rof dynamics and pathways for each of the utilities under their individual most favorable projected and least favorable scenarios highlighted by starts in fig 9 figs 10a c plot watertown s storage capacity and short term rof dynamics for each of the three scenarios likewise figs 10d f shows watertown s resulting ensemble of infrastructure pathways across the three highlighted demand growth scenarios the same panel layout is repeated for the dryville and fallsland pathways the horizontal axes in the pathway panels represent time in weeks from 2015 to 2060 the planning horizon each infrastructure pathway plot displays a stack of results attained for 1000 sows and the horizontal multicolored lines each represent the construction sequence that would be triggered under a specific hydrological scenario on the vertical axis for each horizontal line in the stack the color of a segment represents the infrastructure option built last in time that being the case a transition in color indicates the time when an infrastructure option becomes operational for example the orange horizontal segment starting at the bottom center significant and early infrastructure year 26 of the panel p indicates that in that realization one with more build infrastructure options fallsland finished the construction of the new river reservoir 9 years after finishing the construction of its reuse station brought online in year 17 fig 10 panels d through f indicate that the amount of infrastructure built by watertown varies significantly with demand growth while the scenario with the highest demand growth triggers infrastructure investments for all hydrological realizations and still fails to minimize short term rof values the projected future triggers infrastructure investments in less than 10 of the hydrological realizations however the low investments in infrastructure observed in the projected scenario shown in panel e makes watertown more susceptible to swings in revenue in more challenging realizations due to the need for frequent restrictions such financial swings cause watertown to fail the worse first percentile cost target resulting in a red zone around the blue star in fig 9a on the other hand all three scenarios for dryville and fallsland see the same infrastructure investment patterns with small timing variations which combined with the short term drought mitigation and financial instruments are generally enough to make utilities meet their robustness performance criteria in the regions highlighted in gray in fig 9 an important aspect of the infrastructure investment problem is the effect of permitting times on the scheduled capacity expansions which may cause the construction of an infrastructure option with high permitting time to be postponed in favor of another option with already granted permits in the pathways illustrated in fig 10 panels d through f and p through r for watertown and fallsland the occurrence of purple high capacity college rock expansion and green lines fallsland reuse segments respectively before the orange regions new river reservoir shows that this issue often happen to the jointly built new river reservoir resulting in an observed infrastructure construction order that does not match the order specified in the corresponding policy robustness compromise policy in fig 8 more specifically in scenarios of higher stress when either watertown or fallsland trigger the news river reservoir its permit may be strongly delayed by the extended permitting period so watertown may trigger instead the college rock reservoir expansion and fallsland may trigger the construction of its water reuse station both with substantially reduced permitting times 5 conclusion recent projections estimate that the us will require over one trillion dollars of investment in water supply infrastructure in the next 20 years which can potentially be minimized by exploring synergies between infrastructure investment and non structural drought mitigation instruments both shared among regional actors the coupling of long term infrastructure investment strategies with short term drought mitigation necessitates a model with capabilities not all found in a single currently available software joint simulation of multiple utilities involved in joint planning and operations of shared water resources and infrastructure joint financial supply simulation broad array of uncertainties within a single simulation easy customizability and ready integration with multiobjective optimization algorithms for use with high performance computing waterpaths was designed and shown to fill these gaps we present the design of waterpaths an open source model written in the c programming language and designed to fit the du pathways in addition we demonstrate the application of waterpaths on a hypothetical water regionally coordinated infrastructure planning and management problem called the sedento valley using waterpaths we modeled the decision making and system upgrades of the regionally coordinating water utilities in the sedento valley under well characterized and deep uncertainty to explore policy options discovered inter dependencies between utilities and discovered vulnerabilities revealed under deep uncertainty additionally waterpaths facilitates stochastic tradeoff analyses by integrating state of the art parallel search that can exploit a broad array of high performance computing platforms the sedento valley test case was designed to be itself a contribution to the field of water systems engineering it was designed to be a benchmark test case for the direct comparison of competing regional decision making frameworks for the design and comparison of drought mitigation strategies and for the evaluation of decision making metrics while maintaining computational tractability and interpretability the sedento valley test case captures a high degree of complexity due to the number of utilities involved to the unbalanced distribution of resources and liabilities across utilities and to the utilities locations in relation to each other in the local shared basins the code repository provided in waterpaths code repository contains all the data needed to reproduce the test case in waterpaths or other frameworks and to compare new methodologies and policy instruments against the ones presented this paper overall waterpaths contributes a state of the art stochastic simulation and analytical platform that holds promise for addressing the challenges posed by water supply planning under deep uncertainty globally software availability name of software waterpaths description waterpaths is an open source c model for the stochastic simulation of decision making policies by water utilities developer b trindade bct52 cornell edu with contributions by p reed d gold contributed to the development of the sedento valley test case funding source funding for this work was provided by the national institute of food and agriculture u s department of agriculture wsc agreement no 2014 67003 22076 additional support was provided by the u s national science foundation s water sustainability and climate program award no 1360442 source language c supported systems unix linux windows mac license bsd 3 clause clear license availability https github com bernardoct waterpaths the code used for the waterpaths optimization runs can be found at https bit ly 3auj2bu the code used for the waterpaths re evaluation runs can be found at https bit ly 2o8ktxo the code used to process the output data from optimization and re evaluation can be found at https github com bernardoct paper3 the full paths to the github repositories including the commit hashes can be found in the supplemental section s12 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments funding for this work was provided by the national institute of food and agriculture u s department of agriculture wsc agreement no 2014 67003 22076 additional support was provided by the u s national science foundation s nsf water sustainability and climate program award no 1360442 the cloud computing components of the study were supported by the nsf s office of advanced cyberinfrastructure award no 1541215 the views expressed in this work represent those of the authors and do not necessarily reflect the views or policies of the nsf or the usda the authors would also like to acknowledge the cornell center for advanced computing as well as the texas advanced computing center appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104772 
25991,the inherent complexity of environmental models is frequently a limiting factor in their usefulness and practical applicability this paper aims to demonstrate how scientific workflows can increase the reproducibility of environmental models by better managing this complexity specifically through the example of solar analyst solar radiation model the paper identifies three primary mechanisms for managing environmental modeling complexity using scientific workflows i increasing transparency and improving reproducibility in both the modeling process and the model itself ii integrating validation and improving warrantability of solar radiation model outputs and iii widening opportunities for supporting parameter setting decisions for a diversity of modelers using machine learning the results demonstrate how each of these mechanisms can be realized using a freely available and open source scientific workflow management system swfms called knime firstly our example knime workflows demonstrate increased transparency and improved reproducibility of solar radiation models and the entire modeling process in turn improving transparency and reproducibility can aid novice users in understanding and reusing solar radiation models secondly an extended knime workflow is used to integrate both modeling and validation into a single transparent workflow lastly using knime workflows facilitates integration with other decision support tools and techniques such as machine learning using decision trees an extended solar radiation knime workflow offers the capability to support more transparent and warrantable decisions around setting solar analyst parameter values ultimately better managing the complexity of environmental modeling contributes to wider uptake and scrutiny of environmental models and the outputs they generate both in scientific research and in applied evidence based decision making keywords scientific workflows environmental modeling reproducibility decision trees solar radiation 1 introduction reproducibility is fundamental to science reproducibility is vital for explaining validating and sharing the methods and results of scientific research reproducible science encompasses the communication of the methods procedures protocols techniques tools and observations that enable consistent results when the same work is repeated by others kitzes et al 2017 mesirov 2010 cohen boulakia et al 2017 however reproducibility presents significant challenges to scientists today a recent survey conducted by baker 2016 indicated that more than 70 researchers have tried and failed to successfully reproduce work completed by other scientists environmental scientists similarly face challenges of reproducibility in particular in the context of environmental modeling hutton et al 2016 refsgaard et al 2007 environmental models are frequently complex and often implemented as opaque black boxes with limited documentation e g jakeman et al 2006 the environmental modeling process too is frequently insufficiently transparent to enable scientific results that are easily reproducible explainable and warrantable even for domain scientists let alone novice users e g li et al 2015 to redress this gap this paper explores the use of scientific workflows to increase the reproducibility of environmental models and the environmental modeling process itself using scientific workflows involves capturing explicitly the entire process of scientific computing including all the computational assets to run an environmental model including data inputs transformations analyses and outputs gil et al 2007 the technology that underpins scientific workflows helps modellers organize and segment complex models into transparent and documented composable components each of which individually inspected and executed the resulting models are expected to be more easily reused with outputs more easily reproduced and modeling assumptions and errors more easily identified and checked this paper asks how and to what extent scientific workflows can be used to improve reproducibility in environmental modeling specifically this paper identifies and examines three key mechanisms for improving the management of complexity in environmental modeling using scientific workflows by 1 increasing transparency and improving reproducibility of environmental models 2 integrating validation and improving warrantability of environmental model outputs and 3 increasing opportunities for decision support for environmental modelers the remainder of the paper is organized as follows section 2 provides background to issues of complexity transparency and uncertainty in environmental modeling section 2 4 also provides a deeper introduction to scientific workflows and the associated technology of scientific workflows management systems swfms a scientific workflow management system swfms provides an environment for building and sharing executable scientific workflows swfms such as kepler knime workspace usually provide a graphical language and interface for representing of data flows and analysis steps as well as distributed or grid computing capabilities altintas et al 2004 berthold et al 2009 bakos 2013 ludäscher et al 2006 in section 3 the conventional approach to environmental modeling is reviewed with reference to a specific commonly used example environmental model solar radiation estimation section 4 demonstrates how scientific workflows can be used to manage complexity by improving transparency and warrantability of the environmental model section 5 goes a step further by exploring how scientific workflows and swfms can facilitate the integration of environmental models with other supporting tools such as machine learning the paper demonstrates how decision tree learning can be integrated with solar radiation modeling to enable more transparent and warrantable user support for setting key model parameters finally section 6 concludes with a review of the key lessons learned and a road map for future research and uptake of the approach 2 background the paper distinguishes two different aspects of environmental modeling i the modeling process and ii the environmental model itself the environmental modeling process also termed environmental modeling encompasses all the steps required to successfully design build apply and generate outputs from an environmental model the environmental modeling process may include tasks such as preparation and processing of data sets estimating and setting values for parameters model execution and validation and presentation of results on the other hand an environmental model is a core computational asset for the environmental modeling process that provides a precise mathematical approximation of a real physical system both the model itself and the wider modeling process present challenges to managing complexity and to enhancing reproducibility validation and opportunities for decision support fig 1 summarizes graphically three major challenges that affect both the modeling process and the environmental model itself the level of complexity of both process and model a lack of transparency in model and process and difficulties in handling uncertainty in both model and process arrows in fig 1 connect primary continuous arrow line and secondary dotted arrow line causes of each challenge for both the modeling process and the environmental model itself for example black box implementations are a primary cause of insufficient model transparency uncertainty about the model s inner workings is a secondary effect of black box implementations as the detailed model design is not exposed to users the different causes of each of the three major reproducibility challenges complexity transparency and uncertainty are discussed further in the following sections 2 1 complexity of model and process environmental systems are inherently complex environmental modeling attempts to balance the need to develop tractable and understandable models with the requirement to adequately capture the behavior of complex uncertain and often ill defined environmental systems young et al 1996 paola and leeder 2011 in most cases the physical systems being modelled are sufficiently complex that our understanding of them remains incomplete for example accurate prediction of solar radiation may be confounded by a lack of knowledge of precise sky conditions and cloud composition šúri and hofierka 2004 rich et al 1994 further simple interactions at small scales in environmental systems can give rise to complex behavior at larger scales werner 1999 paola and leeder 2011 the complexity of physical systems in turn gives rise to complexity in the modeling of environmental processes environmental modeling may involve numerous steps to generate the final results each step may require the integration of data sets setting multiple parameters and the choice between different options for analysis a failure to document and justify exhaustively each modeling step undermines the ability of other modelers to scrutinize or reproduce results from previous studies differences in modeling processes between different users are known to cause variations in model outputs for instance even when using the same solar radiation model and the same type of data sets different studies can still result in significantly different modeling results cf li et al 2015 redweik et al 2013 kodysh et al 2013 similarly problems arise when certain steps may be excluded from a detailed description of the modeling process one such example is the validation of model results often excluded or treated as a separate process in environmental modeling just as for other components of the environmental modeling process the model itself embodies increasing levels of complexity as models and modeling tools become increasingly sophisticated the complexity of a model may be driven by the developer s pursuit of ever more detailed representations of physical systems walker et al 2003 more detailed representations of physical systems tend to lead to an increasing number of model components data inputs and parameters this complexity may increase the likelihood of incorrect estimation and selection of appropriate parameter settings it may even make models inaccessible to novice users in either event reproducibility is decreased 2 2 transparency of model and process the complexity of environmental models and modeling processes frequently contributes to a lack of transparency limited descriptions and poor documentation of models and processes limit the information available to successfully reproduce the results of modeling lacking adequate information about model limitations and assumptions subsequent model users may apply models inappropriately or incorrectly jakeman et al 2006 the desire to manage high levels of model complexity frequently leads to black box components in environmental models black box components do make it easier for novice users to include those components in larger more sophisticated models however making the component details inaccessible or unknown for users makes it more likely those components will not be properly understood or applied gilbert et al 2018 holzworth et al 2010 for instance black boxes with low levels of transparency may prevent users selecting appropriate model parameter settings previous studies often do not provide sufficient information about correct parameter selection cf singh and banerjee 2015 santos et al 2014 li et al 2015 kodysh et al 2013 further black box components make errors much more likely to go unnoticed until they emerge as obvious blunders in output hutton et al 2016 thus modeling practice based on opaque black box model components tends to restrict knowledge sharing to narrow communities of experts from specific research domains a lack of clear descriptions of the modeling process together with the application of black box model components only reduces opportunities for sharing and understanding environmental models and modeling results 2 3 uncertainty in model and process uncertainty arises at every step of the environmental modeling process including model development and design model data and parameters and model application uncertainty comes in many forms but can frequently be traced back to two main causes a lack of correctness termed inaccuracy and a lack of detail termed imprecision worboys and duckham 2004 model developers must necessarily make multiple decisions in simplifying environmental systems all environmental models then involve a degree of subjectivity in their design walker et al 2003 jakeman et al 2006 model inaccuracy arises when modelers incorrectly define the mathematical relationship between model inputs and outputs model imprecision arises when models lack relevant detail about the environmental phenomenon under study models may also sometimes accord unwarranted levels of detail termed spurious or false precision the data assets required to execute environmental models including input data and parameters are also subject to imprecision and inaccuracy inaccuracy and imprecision are endemic in many environmental data sets particularly in connection with spatial data shi et al 2002 for instance a raster map with coarse resolution provides less information about a specific location than the same map at a finer resolution parameters too are subject to uncertainty parameter inaccuracy relates to errors in estimation measurement or calibration of a parameter value parameter imprecision refers to a lack of detail in a parameter setting such as estimating a value as 10 units rather than say a more precise value of 11 4 units these uncertainties can combine with a modeler s lack of experience or level of knowledge about the application domain refsgaard et al 2007 showed that the level of a model user s experience played a significant role in the results of an environmental modeling application users with more experience in a particular modeling method and or computational asset were more likely to follow their own preference in customizing and parameterizing the modeling process refsgaard et al 2007 in contrast novice users had difficulties to conduct the process at all because of lack of domain knowledge thus given the same modeling resources and data sets experienced users approached the solution in different ways while novice users had difficulties generating any appropriate output in all cases uncertainty at every level can propagated through to inaccuracy and imprecision in model outputs the data sets generated by our modeling may themselves be inaccurate and imprecise thus in addition to the demonstrated lack of reproducibility uncertainties ultimately lead to a reduction in the warrantability in our modeling outputs 2 4 scientific workflows scientific workflows are an approach to scientific computing that aims to explicitly capture and document all the steps involved in capturing transforming aggregating analyzing and presenting scientific data cf altintas 2008 ludäscher and goble 2005 according to gil et al 2007 granell et al 2013 and kitzes et al 2017 scientific workflows increase reproducibility and improve decision support of computational processes by allowing simple integration of multiple computational assets improving transparency through detailed description and documentation providing a flexible environment for design and execution of multiple tasks and increasing ease of sharing of models among users at their core scientific workflows can help manage the complexity of environmental modeling cf buahin and horsburgh 2018 cohen boulakia et al 2017 granell et al 2013 kitzes et al 2017 scientific workflow management systems swfms such as kepler www kepler project org knime www knime com and vistrails www vistrails org index php main page make documentation easier offer an integrated environment to capture and share all data sets parameters and computational assets and provide a consistent structure for computational processes based on data flows connecting operations with defined inputs and outputs cf ludäscher et al 2006 berthold et al 2009 bakos 2013 freire 2012 freire et al 2014 swfms also often offer a graphical interface to interact with workflows 1 1 figs 6 8 discussed further in later sections provide examples of the graphical representation of scientific workflows within the knime swfms distributed capabilities for grid or cloud computing with workflows altintas et al 2003 nesting of workflows at multiple levels of detail and abstraction and the ability to share the entire computational processes documentation and data through a single file bakos 2013 scientific workflows are already being applied in environmental modeling for example kaster et al 2005 applied scientific workflows to the development of an open and transparent tool for documenting monitoring and assessing the impacts of fertilizing application on soil and water in agriculture zyl et al 2012 designed and tested a scientific workflow for spatiotemporal wildfire with a particular focus on accessing and integrating geospatial data in the scientific workflows scientific workflows have also been successfully implemented for disaster response for processing location based social media cerutti et al 2019 the study demonstrated the potential for improvements in reproducibility and transparency of three disaster response analytics processes using knime scientific workflows fang et al 2008 have designed scientific workflows for multi satellite remote sensing knowledge extraction for disaster monitoring and early warning in the fishery industry the implementation of workflows enabled automated analysis of water quality and marine disasters such as detection of oil spill accidents another case study involved the design and implementation of scientific workflows for monitoring the water quality of east lake in wuhan hubei province yue et al 2015 a novel geospatial workflow system named geojmodelbuilder was developed to show how remotely sensed imagery could be integrated with available sensor data to generate turbidity maps in the same paper a more sophisticated use of scientific workflows was demonstrated through an additional case study which integrates two hydrological models topmodel and hargreaves together with spatial analysis algorithms the application arguably improves reproducibility as well as assisting in the management and efficient execution of the computational process of coupled watershed runoff modeling scientific workflows have also been used as a visualization tool for improving reproducibility and transparency of hydrological models leonard and duffy 2016 climate modeling is another established area of scientific workflow application turuncoglu et al 2013 developed kepler scientific workflows in which a coupled modeling system integrates regional ocean and atmospheric models for simulating sea surface temperature despite the manifest advantages of scientific workflows and swfms challenges undoubtedly remain in realizing those advantages scientific workflows can themselves become a complex system structure different software platforms programming languages standards and formats can cause issues with workflow performance and sharing cohen boulakia et al 2017 gil et al 2007 kitzes et al 2017 mcferren et al 2012 hence a key contribution of this work is to provide a blueprint for realizing the potential advantages of scientific workflows in environmental modeling however as a basis for comparison we first review in more detail the classical approach to environmental modeling through the specific example of solar radiation estimation 3 case study solar radiation estimation with solar analyst estimation of solar radiation is an important component of many environmental models solar radiation is also an important data product in its own right for applications in civil engineering urban planning and natural resource management the physics that underpin solar radiation are well understood and predictable making solar radiation an ideal base case study for using scientific workflows nevertheless despite this relative simplicity solar radiation estimation still embodies sufficient complexity to exhibit the challenges identified above 3 1 solar analyst solar analyst is one of the most commonly used environmental models for assessment of solar energy potential the popularity of this model is in part related to its availability in the widely used software arcgis www esri com en us arcgis about arcgis overview fu and rich 1999 2000 working with proprietorial close source models such as esri arcgis spatial analyst toolkit presents more challenges to reproducibility than open source alternatives such as r sun in grass gis https grass osgeo org and saga www saga gis org en index html hence the approach taken here could also be applied to other open source alternatives such as r sun with it expected that in most cases reproducibility can at least as easily if not more achieved working with open source components solar analyst requires a digital surface model dsm rich et al 1994 fu and rich 1999 2000 as its primary input for each raster cell of the dsm solar analyst creates an upward looking viewshed illustrated in fig 5 3a comparable to hemispherical photographs solar analyst then constructs the sky obstruction from surrounding topographic features for each location covered by the dsm in fig 5 3a the dark gray region depicts obstructed areas while light gray depicts the sun visible area of the upward looking viewshed the model outputs estimates of the combined direct and diffuse solar radiation at each location estimating solar energy potential with solar analyst entails data preparation and preprocessing setting parameters and configuring solar analyst and executing the model and collating or visualizing the outputs fig 2 shows an example study area in melbourne australia monash university clayton campus spatial data about spot heights contours and building footprints is combined in a data preprocessing step to generate a dsm of the study area as shown in fig 3 the dsm illustrated in fig 3 is the main input for the model integrated in esri s arcgis area solar radiation tool the model accepts data in all projected coordinate systems supported by arcgis software esri 2012 once the input data is assembled the next step is to determine appropriate spatial temporal and physical parameter settings for model execution setting parameter values is frequently a process that requires estimation experience and ultimately subjective decisions by the modeler table 1 in the following section 3 2 contains further details on parameters specific settings and data sources for the monash university case study finally the esri solar analyst area solar radiation tool was executed to provide estimated solar energy potential across the study area the main output from the model is a raster containing information about the global direct and diffuse solar radiation potential across the study area for example fig 4 maps the total annual solar energy radiation potential output by the model across building rooftops in the study area the estimated solar radiation potential in fig 4 provides information about the distribution of solar radiation across building rooftops at monash university this information can provide a basis for more accurate and economical installation of solar photovoltaic energy systems at the study area additionally estimated solar radiation potential helps in planning investment and development of strategic solar energy projects 3 2 model parameters an array of different model parameters must be set in order to estimate solar radiation using solar analyst despite its relative simplicity as a well understood physical model the parameters can be bewildering for modelers not already familiar with solar radiation modeling generally and solar analyst model specifically cf table 1 below amongst the simplest parameters are those that define the spatiotemporal location of the model run clearly an important factor in direct solar radiation specifically the model requires as parameters the geographic latitude and the time period under study two further definitional parameters configure the model itself for a spherical coordinate system a user needs to specify a z factor parameter for the given latitude of area of interest the z factor allows different units of length to be used in the z direction height when compares to the x y planar spatial units of the dsm the type parameter determines the underlying behavior and ultimately accuracy of the model using the simpler but less accurate uniform type diffuse radiation model assumes the same incoming diffuse radiation at any sky direction the standard overcast type model by contract additionally models the directions and intensity of incoming diffuse radiation fu and rich 2000 estimating two further physical parameters has a critical impact on the estimated incoming radiation the diffuse proportion estimates the proportion of total incoming solar radiation that is diffuse a typical diffuse proportion will vary between 0 2 20 for clear skies to 0 7 70 for dense cloud conversely transmission proportion is an estimate of the average proportion of incoming solar radiation reaching the ground typical transmission proportion values will vary between 0 7 for very clear skies to 0 4 for overcast the two parameters are inversely related by prevailing atmospheric conditions and so are not entirely independent finally the most complex and numerous parameters define the spatiotemporal granularity of the model computation in all cases specifying finer granularity offers the potential for greater precision and accuracy in the output solar radiation estimates however finer granularity also entails greater computational overheads with the computationally intensive nature of the model rapidly making the computation intractable as granularity becomes finer the zenith divisions and azimuth divisions specify the number of sky sectors used to compute diffuse radiation fu and rich 2000 typical values can vary from relatively coarse 8 sectors to arbitrarily fine granularity a sky size parameter sets the extent of the hemispherical raster grids used to estimate direct and diffuse radiation across different parts of the sky this parameter is expressed as the length of one side of the raster grid in unit cells e g 250 500 1000 the number of azimuth directions determines the number of rays used to compute the viewshed while minimum of 8 directions may be sufficient for smooth terrain irregular terrain or urban settings may require setting much higher levels of detail to adequately capture the shading effect of nearby features such as buildings finally temporal parameters also define the temporal granularity of the output i e whether the estimated solar radiation is averaged over minutes days months or even years fig 5 summarizes the main granularity parameters used in solar analyst in the example of model output in fig 4 32 zenith and azimuth divisions defined sky sectors for computing diffuse radiation fig 5 1d the setting value for sky size parameter was 250 fig 5 2a the number of azimuth directions was set to 32 fig 5 3c and the time interval was set to monthly fig 5 4b 3 3 summary the process of solar radiation modeling with solar analyst highlights number of issues with wider significance in environmental modeling firstly the modeling process is complex it includes multiple steps including preparing and processing spatial data sets setting parameter values executing the model and visualizing the outputs these steps are often not well documented it is also not possible to infer anything about the specific steps and inputs used from the output ultimately generated secondly several of these steps require domain expertise experience or judgment this feature makes using the model error prone or impractical for novice users even for experts the need for subjective decisions makes it harder to reproduce the modeling process either by another expert or indeed the same expert weeks or months later it also reduces the warrantability of the outputs how do we know if a parameter setting is appropriately selected thirdly the physical equations and relations used to generate the output are not accessible to the modeler solar analyst effectively operates as a black box modelers have little opportunity to interrogate the design and structure of the model directly aside from reading the manual finally solar analyst offers no support for validation of model output validation is a critical step in understanding the reliability of any environmental modeling outputs but a step that is all too frequently omitted in practice brito et al 2012 kodysh et al 2013 santos et al 2014 in summary high levels of complexity a lack of transparency and a lack of decision support for modelers all serve to reduce the reproducibility of the modeling process and the warrantability of the model outputs addressing these issues for solar analyst and for environmental modeling more generally is the focus of our scientific workflows approach in the following sections 4 scientific workflows for increased reproducibility and warrantability this section demonstrates how the capabilities of scientific workflows can be used to address the fundamental issues identified above of reproducibility of the modeling process and warrantability of the modeling outputs 4 1 knime solar analyst workflow the knime analytics platform was chosen as the swfms for this study potentially any swfms with similar capabilities might also be used in place of knime including those discussed in section 2 4 i e kepler vistrails geojmodelbuilder amongst others however knime offers a range of features which while not unique to knime do make it an ideal selection for exploring reproducibility in environmental modeling knime is an open source cross platform swfms with a graphic user interface compatible with in microsoft mac and linux operating systems some other options such as for example esri model builder are commercial software products with closed source and not supported across all common operating systems knime supports an array of external components including machine learning tools such as decision tree learning neural network and support vector machines while many other software packages do have graphical tools for workflow like automation of processes including esri model builder 2 2 https pro arcgis com en pro app help analysis geoprocessing modelbuilder what is modelbuilder htm grass gmodeler 3 3 https grass osgeo org grass78 manuals g gui gmodeler html and qgis graphical modeler 4 4 https docs qgis org 2 8 en docs user manual processing modeler html these tools are typically difficult to integrate with external components and software systems knime has a large established and active user community unlike some more research oriented tools such as geojmodelbuilder knime offers the full range of swfms features including parallelism of both tasks operations and pipelines data flows and nesting of tasks and data flows in subworkflows such features are essential to efficient execution and deployment of workflows and managing complexity in environmental models and are largely absent in automation tools such as esri model builder grass gmodeler and qgis graphical modeler complete knime workflows including process data and documentation are easily archived and shared at any stage of execution using a cross platform persistent file format in an swfms such as knime analysts can see workflows graphically and manipulate them on screen using the gui fig 6 shows a minimal knime workflow to reproduce the most basic elements of the modeling process already described above in section 3 the solar analyst model itself is integrated with the workflow as a python script central node in fig 6 the script executes an external instance of solar analyst in esri arcgis the computational performance of the workflow is approximately the same as executing solar analyst through esri arcgis graphic use interface solar analyst model execution dominates all other computational demands of the system whether initiated through knime or arcgis interface other operations such as inputs parameters and raster dsm and outputs simple tabular output in fig 6 are also represented with labeled boxes lines connecting boxes capture flows of data or parameters between operations left to right importantly the workflow is executable integrating both data and the operations upon data the traffic lights associated with operations in fig 6 summarize whether the underlying data has been processed green is ready to be processed yellow or is not yet ready to be processed red the swfms enables the entire workflow including data parameter settings and the dsm data set in fig 6 to be archived at any stage of development or execution the intuitive graphical layout together with simple to use labels for nodes and data flows provides a degree of self documentation even such a simple starting point provides some basic reproducibility capabilities when compared with the wholly manual process described in section 3 the knime workflow can be exported together with all workflow components nodes and metanodes from knime software into a persistent knwf file the file can then be shared with and opened by other users workflows can also be distributed and shared through a server version of knime as well as implemented on cloud based platforms such as aws or azure only external software components such as the solar analyst executable itself will be absent from the exported knwf file requiring users to have access to their own version of such software but see also section 4 3 the following subsections describe two important extensions to this minimal workflow to further enhance reproducibility integrating validation and exposing model details 4 2 integrating validation as argued above validating the model and outputs is important for warrantability but frequently omitted brito et al 2012 kodysh et al 2013 santos et al 2014 using a swfms helps make model validation easier to integrate fig 7 extends the knime workflow in fig 6 with additional validation of the model output through comparison with a ground truth data set data about expected solar radiation data is provided by the australian bureau of meteorology www bom gov au climate data captured using pyranometers at local meteorological stations for our study expected solar radiation took the form of monthly solar radiation potential kwh m2 for the study area averaged over the last 30 years in cases where ground based pyranometer data is unavailable average solar radiation can also be derived from satellite images for the area of interest https power larc nasa gov the additional validation steps and data sets can be seen by comparing figs 6 and 7 the lower portion of fig 7 extends the basic model in fig 6 by reading in and reformatting the ground truth data set calculating the differences between the model estimates and expected solar radiation and classifying those differences for ease of interpretation the solar analyst output provides an estimated solar radiation potential in kwh m2 for each raster cell in the input dsm in the example workflow in fig 7 model outputs are classified based on their difference with the monthly average solar radiation potential value avg rad derived from the ground truth data set model outputs with the maximum of 15 relative difference δ to the ground truth data set and within a range 15 of avg rad δ 15 of avg rad are included in expected solar radiation potential category model outputs with a negative or positive difference value outside of the range are included in low and high solar radiation potential category respectively the approach to validation depicted in fig 7 is simple but can fulfill two important functions first the approach provides a mechanism to report on and share prior validation of model results in an executable form that helps users understand model limitations second the workflow provides model users with and a ready and intuitive mechanism to check for errors in their own modeling results in the latter case users can easily insert their own ground truth data to validate their own or others model outputs more sophisticated validation workflows are naturally also possible such validation might aim to highlight the spatial distribution of errors for example mapping the occurrence of errors or computing spatial correlations in error irrespective of the specific validation method used explicitly capturing validation in the workflow and alongside the modeling process itself inherently increases reproducibility and transparency crucially once again the scientific workflow combines all analysis and validation steps and all the data sets that accompany those steps hence sharing modeling and validation process and data sets ensures that both modeling and validation are reproducible and increases the warrantability of model outputs 4 3 exposing model details a strength of swfms such as knime is the ease with which external components such as esri solar analyst grass machine learning tools or python scripts can be integrated with workflows at the same time reliance on external components also introduces a weakness in reproducibility the swfms enables both process and data in the workflows in figs 6 and 7 to be archived and shared however it cannot guarantee that external components in particular proprietorial software such as esri solar analyst will be available to other modelers further the external component may operate as a black box reducing the transparency of the model unpacking the details of the solar radiation as a workflow itself improves model transparency exposing the inner operations within the model hamilton et al 2019 and provides more comprehensive documentation of the model reducing uncertainty related to model application jakeman et al 2006 hamilton et al 2019 fig 8 shows an extended scientific workflow that again executes the same basic model as that in fig 6 but exposes all of the modeling steps within the process of solar radiation estimation by esri solar analyst the workflow in fig 8 begins with two main streams the lower workflow stream calculates direct and diffuse solar radiation components based on equations from solar analyst user manual fu and rich 2000 without including the sky obstruction from surrounding area the upper workflow stream takes point raster cell coordinates together with aspect and slope information derived from the dsm illustrated in fig 3 and calculates a sky obstruction factor by including shadowing from building rooftops in our specific case study we use a small cluster of points spatially contained withing the flat rooftops and with slope and aspect constant value equal to 0 i e sky facing in our case study local ground terrain does not have any impact on the estimation of rooftop solar radiation potential because the building is significantly higher than any surrounding topography see fig 3 however more complex examples might require that workflow accounts for the shadowing effect of local terrain upon estimation of solar radiation these two streams combined together calculate solar radiation potential for any given location within the dsm note that the uncertainty associated with solar radiation potential estimates from the workflow in fig 8 will be similar to that of the solar analyst model output uncertainty this is mainly because our workflow s solar radiation model is a simplified but otherwise faithful reproduction of the solar analyst model based on the same equations for calculating two main components direct and diffuse of global radiation fu and rich 2000 our aim in constructing the workflow in fig 8 is not to build a better or more accurate solar radiation estimator rather it is to illustrate the principle of unpacking black box environmental models and exposing their workings using swfms hence this example of knime workflow illustrated in fig 8 demonstrates a component based transparent tool for solar radiation modeling which operates completely independent from the esri solar analyst solar radiation tool unlike previous workflows depicted in figs 6 and 7 in the case of this extended workflow fig 8 the whole modeling process can be exposed stored shared and manipulated with no restrictions on transparency or need for external software components or licenses swfms such as knime also helps modelers manage the complexity of more exposed models through metanodes where a single logical modeling step comprises multiple operations thus complex workflows can be hierarchically nested to retain clarity ensuring transparency and reproducibility 5 parameter setting assistance using machine learning the results in the previous section demonstrate how swfms can be used to improve reproducibility and warrantability of environmental modeling through improved model clarity transparency documentation validation and integration of data and processing the ability additionally to integrate a range of analytics tools together in a single workflow offers further opportunities for improving model reproducibility and warrantability this section explores one such example the integration of machine learning tools to assist novice users in choosing appropriate parameter settings as discussed in section 3 setting appropriate parameters often relies on domain expertise experience or subjective judgment in most past published studies using solar analyst for example authors simply do not report parameter settings singh and banerjee 2015 santos et al 2014 li et al 2015 kodysh et al 2013 this reliance on expertise and judgment creates barriers to reproducibility especially for novice users fig 9 shows an extension of our solar analyst environmental model workflow that uses a machine learning technique decision trees to support estimation and selection of appropriate parameter settings the solar radiation estimation model itself is contained within a single metanode workflow loop the majority of the visible workflow is concerned with three main steps 1 generating model output using a range of different parameter settings choices fig 9 section with orange border line 2 comparing and classifying model output based on its difference with the ground truth dataset fig 9 section with yellow border line and 3 decision tree learning with classified model output to identify parameter settings fig 9 section with red border line decision tree learning is a class of machine learning that recursively classifies a data set based on its most important or salient features russell and norvig 2002 amongst the most famous examples of decision tree learning are the id3 and c4 5 algorithms by quinlan quinlan 1986 1993 both algorithms classify a data set by selecting at each node those attribute values that maximize the shannon s information gain shannon and weaver 1949 about the training data shannon s information gain is sometimes explained as quantifying the surprisal value of information how much a particular item of data surprises us in the context of decision tree learning those attributes of a data set that once known tell us most about the ultimate outcome can be said to decrease our surprise at the outcome hence these attributes are associated with the highest information gain and so are selected first in a decision tree fig 10 shows the detail of the decision tree learning process in the knime scientific workflow expanded from the decision tree model metanode in fig 9 in the context of environmental modeling decision tree learning can be used to classify different parameters and parameter settings based on the shannon s information gain about the model output in the example of our solar radiation estimation decision tree learning can be used to classify the parameters and parameter settings most important for model accuracy certain parameters and settings will have a significant impact on the accuracy of the resulting model execution others may have limited impact in the resulting decision tree parameters that have the most impact on model accuracy i e most reduce our surprise in output accuracy will be associated to the greatest shannon s information gained successively less important parameters and their setting values will be associated with lower shannon s information gain classifying parameters and parameter settings in this way provides simple guidance to novice users as to the importance and likely impact of different parameters and settings the following subsections step through the process of using decision tree learning to aid in parameter settings 5 1 parameter solution space table 2 shows six key parameters of solar analyst fu and rich 2000 of those identified in section 3 2 the selected parameters include the physical parameters that are amongst the most difficult to estimate diffuse and transmission proportion ruiz arias et al 2009 and the granularity parameters that are most reliant on experience sky size number of azimuth directions and divisions number of zenith divisions these parameters cannot easily be observed directly by sensors or other measurements they must be estimated or defined by users based on experience or estimation some expertise may still be required to determine even the range of sensible settings to be explored e g sky size of between 250 and 550 in table 2 however guidance for the novice on the broad range of settings can often be deduced from the documentation certainly this is the case for solar analyst documentation 5 2 training data set a brute force approach to explore this parameter solution space is to use decision tree learning across all 4 6 4096 different combinations of parameter settings to classify parameters and settings based on the accuracy of the model output computing the estimated solar radiation for the entire study area can take several hours hence for our training data set a small representative cluster of rooftops exhibiting a range of different building heights was selected from the northern side of the study area the buildings contained 392 pixels around 5 of the total study area for each parameter setting combination the solar radiation estimated in every raster cell of dsm by the model can be compared with expected solar radiation measured at local weather stations as a result 4096 model runs produced 4096 392 1 605 632 records individual solar radiation estimates an example of the results of comparing model outputs with expected ground truth radiation is shown in table 3 each row in the table provides one of the 4096 possible parameter setting combinations repeated for each of the 392 raster cells the final column in table 3 additionally classifies the solar radiation in kwh m2 estimated by solar analyst compared to the ground truth expected average monthly radiation each output cell is ranked according to the absolute difference between expected and estimated solar radiation high performing cells are those where estimates are in the top 25 of smallest absolute deviations from expected lower performing cell have estimates in the bottom 75 of smallest absolute deviations from expected the rationale for selecting the top quartile as high performing is connected with the resultant decision tree structure explained further in the following section 5 3 decision tree learning decision tree learning classifies the training data according to the information gained about estimation accuracy by each parameter setting in table 2 in the resulting decision tree each node in the tree details the number and proportion of high and lower performing output records cells each branch identifies a parameter value setting with the associated information gained through that setting for example figs 11 and 12 show the decision trees resulting from decision tree learning on four of the six parameters in table 2 taking tpi transmission proportion index as an example fig 11a the root node shows the entire training data set with the top 25 of settings designated high performing and the bottom 75 of settings designated lower performance at the first level a tpi setting of 0 4 right hand branch versus 0 4 left hand branch has the largest information gain of any tpi setting in the training data set i e 0 1226 none of those model runs with a tpi parameter setting of 0 4 led to high performing i e top quartile outputs in contrast for model runs with a tpi setting of 0 4 a further tpi setting of 0 5 versus 0 5 leads to further information gained about the high performing settings i e 0 1961 and in the case of tpi 0 5 further tpi parameter settings 0 6 versus 0 6 yield further information gained i e 0 0215 examining the resulting decision tree in fig 11a the highlighted leaf node with 204 746 records 51 0 of the data has the largest number of high performing top quartile model outcomes consequently the parameter setting associated with that node tpi of 0 5 and 0 6 is arguably the best default setting for that parameter hence the rationale for training the decision tree with the top quartile classified high performing ensures that in the ideal case where just two setting decisions for a parameter e g x and y perfectly describe the training data then the resulting decision tree will have less than three levels with all of the high performing model runs allocated to one leaf node in practice however usually only some lesser proportion of the high performance outputs are classified correctly by a single parameter in this way in such cases we select the parameter setting with the largest total number of high performance outputs from the top quartile this largest number of high performing records from the top 25 of data is used as a measure for accuracy of model output with a particular parameter setting decision for example in fig 11b the parameter setting of dpi diffusion proportion index of 0 1 versus 0 1 is identified with the greatest information gain at the first level i e 0 002 in this case the first level leaf node in left hand branch dpi 0 1 is associated with the largest number of high performing model outputs 116302 high 29 of the data of any leaf node and so represents the preferred default parameter setting similar decision trees can be constructed for other parameters and settings including nadr number of azimuth directions fig 12a where a setting of 8 leads to the greatest number of high performing model outputs 104819 ss sky size fig 12b where settings have very low impact on number of high performing outputs indicated by very low information content i e 0 0001 nadv and nzdv number of azimuth zenith divisions which have almost no influence on the high versus lower performing model runs with very low information content i e 0 0001 in all cases 5 4 summary advice for users the information from the decision trees for each parameter can be summarized in the form of advice to novice users as shown in table 4 such advice may only provide a guide and may be limited in application to the local vicinity which the training data set covers however in the case of solar radiation modeling this vicinity may be large as parameters such as tpi and dpi are not expected to vary significantly over small geographic distances further the method makes efficient use of a small number of expected ground truth measurements in our case one spatial location with a history of 30 years measurements applied to a larger number of spatial locations further parameters can be ranked in order of importance according to the associated shannon s information gained through that parameter setting default settings for each parameter can then be reported as ranges derived from the leaf node in the decision tree with the highest number of high top quartile model runs see table 4 for example from table 4 the tpi parameter has the greatest information gain i e 0 1226 at the root node of decision tree fig 11a it is in this sense the most important parameter for users to set the one with the greatest influence over the performance of model outputs users can be advised that a tpi parameter setting of 0 5 tpi 0 6 is associated with the largest number of high performing top quartile model runs based on the information gained the six key parameters can be divided into two groups the first group includes the three most important key parameters tpi dpi and nadr with information gain value i e 0 0001 the second group includes the bottom ranked parameters ss nzdv and nadv associated with very small information gains of i e 0 0001 users can be advised that this second parameter group has a very low impact on the performance of model runs 5 4 1 computational considerations in cases such as those above where parameters have minimal impact on model accuracy it may be advantageous to prioritize other considerations in setting those parameters such as computational efficiency or increasing model granularity the time required to execute our complete parameter learning workflow in fig 9 is dominated by executing the 4096 solar analyst runs with different parameter settings executing all 4096 model runs requires between 24 and 48 hours depending on parameter settings on a standard pc for the selected subset of building rooftops occupying about 5 of the total study area each individual solar analyst model run requires only a few minutes to execute on the rooftop subset whereas the full study area in fig 4 may require several hours for a single model execution all other workflow operations require less than 2 min total to execute specifically with reference to fig 9 about 45 seconds for workflow in the orange area excluding solar analyst model runs 30 seconds for the yellow area and 20 seconds for the red area hence our analysis of computational scalability below focuses on the impact of changing parameters on solar analyst model runs fig 13 summarizes the impact on model execution times of the six main parameter settings explored ss nzdv nadv nadr tpi dpi in all cases the model was executed on a standard personal computer with 8 gb ram memory and a 2 40 ghz processor with four cores each model execution was conducted using a finer spatial resolution 0 2 m cell size and a coarser spatial resolution 0 4 m cell size dsm fig 13 shows how the execution time increases scales as a function of parameter setting in general finer spatial granularity leads to increased execution time as would be expected due the larger number of raster cells involved in the computation increasing the sky size ss increases the time required to execute each model as a quadratic in the number of cells per side in computing terms a time complexity of o n 2 the total number of cells in the computation increases as function of the square of the sky size so this growth in execution time is as would be expected increasing nadv leads to quadratic increases in computation time for the same reasons regression on the observed results confirmed this expectation of quadratic growth r2 0 99 in all cases while the scalability remains the same o n 2 for both 0 2 m and 0 4 m dsm the absolute times required for the finer granularity dsm 0 2 m are significantly higher in all cases for the same reason in proportion to the increased number of cells all other parameters nzdv nadv tpi and dpi have no impact on the computation time while tpi and dpi are important factors used in the computing accurate output they have no effect on the computational efficiency finally although nzdv nadv are granularity parameters specifying the mesh size for computing shadow boundaries they do not affect the structure of the model computation and so have no effect on the time required for a model run combining the decision tree output in table 4 with the computational scalability of parameters in fig 13 leads to the following parameter setting advice tpi and dpi parameter settings have the most impact on output accuracy table 4 but no impact on computational time complexity fig 13d and e hence for those parameters model output accuracy is the sole consideration nadr parameter settings have a moderate impact on output accuracy table 4 but also a significant impact on computational time complexity fig 13b hence for the nadr parameter settings towards the lower end of the range are advisable as long as they do not degrade output accuracy i e nadr 8 in the range 8 32 table 4 a change in sky size ss parameter is associated with only minimal effect on the model output accuracy table 4 but significant effects on the computational scalability with higher values requiring significantly longer to process fig 13a hence for the ss parameter settings at the lower end of the accurate range are advisable i e ss 250 in the range 250 550 table 4 nzdv and nadv settings have no significant effect on model accuracy table 4 however higher settings do not require any additional runtime hence adopting the highest setting in the range may be the natural default nadv nzdv 32 in the range 8 32 table 4 5 5 workflow limitations and applications the primary limitation of the parameter learning workflow illustrated in fig 9 is the computational time required to repeatedly execute computationally intensive environmental model such as solar radiation estimation in order to explore a large parameter space in this work that computational cost was managed by applying our machine learning of parameters to only a small representative sample of building rooftops about 300 pixels and 5 of the total study area potentially there are a number of other approaches that might be taken to reduce or ameliorate computational overheads including high performance computing hpc hpc and shared cloud based computing resources could be used to efficiently precompute model outputs using a range of parameters especially in cases where some or all default parameter settings are expected to be common across different geographic regions or groups of applications users reducing parameter solution space omitting from decision tree learning any parameters known to have limited impact on output accuracy will significantly reduce the solution space to explore for example omitting nzdv nadv and ss from training in other areas would reduce the number of model runs from 4096 4 6 to 64 4 3 potentially reducing computational time by a factor of 64 using fractional factorial designs rather than exhaustively exploring the full factorial parameter space would further reduce computational overheads a fractional factorial design exploring only some combinations of parameter settings is likely to be effective because some parameter settings tend to co vary in their effects for example parameters tpi and dpi are inversely related by prevailing atmospheric conditions see section 3 2 incremental model algorithms in our naive approach each individual model run will need to regenerate all computational resources or steps even those common to other model runs an incremental algorithm is able to efficiently reuse previously computed intermediate data or steps in subsequent model runs designing an incremental version of a black box model such as solar analyst is not feasible due to its closed nature however incremental versions of open workflow based models such as that explored in section 4 3 are potentially possible for example associated with parallel branches of the workflow fig 8 although this work has focused on the specific case study of solar radiation modeling our machine learning workflow approach has potential wider applicability for selecting appropriate parameter settings in any environmental model such as aquatic ecosystem nielsen et al 2017 hydrological pietroniro et al 2007 terink et al 2015 and weather and climate mughal et al 2017 turuncoglu et al 2013 skamarock et al 2008 models for example for aquatic ecosystem modeling estimating and selecting appropriate values for meteorological parameters such as wind speed air pressure air temperature and cloud cover fraction are similarly challenging to tpi and dpi in solar analyst further this transparent workflow approach may also find utility for calibrating and tuning weather and climate models some climate models consistently underestimate important drivers of climate such as rainfall solar radiation or wind potential while other overestimate them hourdin et al 2017 hence the automated assistance with setting workflow parameters may aid in reducing errors due to inappropriate parameter settings in model outputs finally the approach would also be useful in deciding where to prioritize additional efforts to refine parameter settings table 4 indicates that for example efforts to more precisely determine tpi and dpi in the study area are likely to lead directly to improved accuracy by contrast nzdv nadv and ss are unlikely to impact model output accuracy and need not be a high priority for modelers developing the work further still it would be possible to explore out information gain approach to similarly exploring those inputs or spatial regions where more accurate fine granular or up to date data might have the largest impact on model accuracy 6 conclusions and further work through the example of knime and solar analyst this article has demonstrated the capacity of scientific workflows and swfms to improve reproducibility and warrantability of environmental modeling in addition swfms can lead to increased opportunities for transparent decision support for environmental modelers the results demonstrate that swfms based environmental modeling can be more reproducible more transparent and more easily validated and explained than the conventional approach to modeling swfms enable the model data and parameters to be captured in a reproducible and transparent way allow the integration of modeling and model validation expose the environmental model details to scrutiny and can be used to support integration of higher level capabilities such as parameter setting advice for novice users nevertheless this research lays the ground work for a range of further avenues to explore for example there are a range of options to extend our approach to machine learning for parameter setting advice different machine learning approaches such as genetic algorithms shapiro 2001 for efficient searching the model parameter solution space might be more appropriate than decision tree learning particularly for models with larger parameter sets and spaces such techniques are similarly easily integrated through swfms further our work focuses on just one example environmental model solar radiation with a particular focus on solar analyst using a swfms means other solar radiation models such as for example grass r sun or even native workflow models such as explored in section 4 3 can be easily substituted the broader principles exposed by this work are expected to hold true for other environmental models indeed as discussed in section 3 2 solar radiation is an example of a relatively simple environmental model with a well understood physical basis more complex environmental models may reveal further challenges and opportunities for swfms the increasing popularity of geospatially enabled scientific workflows means future applications should also help to improve reproducibility and extend warrantability of environmental modeling across more complex application domains such as hydrological climate or ecological modeling yue et al 2015 turuncoglu et al 2013 nielsen et al 2017 coll and steenbeek 2017 the benefits of improved reproducibility warrantability and transparency offered by swfms are only expected to increase as environmental models become more complex software and scientific workflows availability knime software is free and open source and can be accessed at www knime com downloads download knime and www knime com next steps further background information and support documentation is provided in berthold et al 2009 and bakos 2013 all knime workflows developed for this research are freely available on github at www github com nenad1984 knime workflows git solar analyst is closed source and proprietorial available as part of the spatial analyst toolkit in esri arcgis declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported through the provision of an australian government research training program scholarship the work was also partially supported through the australian research council s arc discovery projects funding scheme project dp120100072 
25991,the inherent complexity of environmental models is frequently a limiting factor in their usefulness and practical applicability this paper aims to demonstrate how scientific workflows can increase the reproducibility of environmental models by better managing this complexity specifically through the example of solar analyst solar radiation model the paper identifies three primary mechanisms for managing environmental modeling complexity using scientific workflows i increasing transparency and improving reproducibility in both the modeling process and the model itself ii integrating validation and improving warrantability of solar radiation model outputs and iii widening opportunities for supporting parameter setting decisions for a diversity of modelers using machine learning the results demonstrate how each of these mechanisms can be realized using a freely available and open source scientific workflow management system swfms called knime firstly our example knime workflows demonstrate increased transparency and improved reproducibility of solar radiation models and the entire modeling process in turn improving transparency and reproducibility can aid novice users in understanding and reusing solar radiation models secondly an extended knime workflow is used to integrate both modeling and validation into a single transparent workflow lastly using knime workflows facilitates integration with other decision support tools and techniques such as machine learning using decision trees an extended solar radiation knime workflow offers the capability to support more transparent and warrantable decisions around setting solar analyst parameter values ultimately better managing the complexity of environmental modeling contributes to wider uptake and scrutiny of environmental models and the outputs they generate both in scientific research and in applied evidence based decision making keywords scientific workflows environmental modeling reproducibility decision trees solar radiation 1 introduction reproducibility is fundamental to science reproducibility is vital for explaining validating and sharing the methods and results of scientific research reproducible science encompasses the communication of the methods procedures protocols techniques tools and observations that enable consistent results when the same work is repeated by others kitzes et al 2017 mesirov 2010 cohen boulakia et al 2017 however reproducibility presents significant challenges to scientists today a recent survey conducted by baker 2016 indicated that more than 70 researchers have tried and failed to successfully reproduce work completed by other scientists environmental scientists similarly face challenges of reproducibility in particular in the context of environmental modeling hutton et al 2016 refsgaard et al 2007 environmental models are frequently complex and often implemented as opaque black boxes with limited documentation e g jakeman et al 2006 the environmental modeling process too is frequently insufficiently transparent to enable scientific results that are easily reproducible explainable and warrantable even for domain scientists let alone novice users e g li et al 2015 to redress this gap this paper explores the use of scientific workflows to increase the reproducibility of environmental models and the environmental modeling process itself using scientific workflows involves capturing explicitly the entire process of scientific computing including all the computational assets to run an environmental model including data inputs transformations analyses and outputs gil et al 2007 the technology that underpins scientific workflows helps modellers organize and segment complex models into transparent and documented composable components each of which individually inspected and executed the resulting models are expected to be more easily reused with outputs more easily reproduced and modeling assumptions and errors more easily identified and checked this paper asks how and to what extent scientific workflows can be used to improve reproducibility in environmental modeling specifically this paper identifies and examines three key mechanisms for improving the management of complexity in environmental modeling using scientific workflows by 1 increasing transparency and improving reproducibility of environmental models 2 integrating validation and improving warrantability of environmental model outputs and 3 increasing opportunities for decision support for environmental modelers the remainder of the paper is organized as follows section 2 provides background to issues of complexity transparency and uncertainty in environmental modeling section 2 4 also provides a deeper introduction to scientific workflows and the associated technology of scientific workflows management systems swfms a scientific workflow management system swfms provides an environment for building and sharing executable scientific workflows swfms such as kepler knime workspace usually provide a graphical language and interface for representing of data flows and analysis steps as well as distributed or grid computing capabilities altintas et al 2004 berthold et al 2009 bakos 2013 ludäscher et al 2006 in section 3 the conventional approach to environmental modeling is reviewed with reference to a specific commonly used example environmental model solar radiation estimation section 4 demonstrates how scientific workflows can be used to manage complexity by improving transparency and warrantability of the environmental model section 5 goes a step further by exploring how scientific workflows and swfms can facilitate the integration of environmental models with other supporting tools such as machine learning the paper demonstrates how decision tree learning can be integrated with solar radiation modeling to enable more transparent and warrantable user support for setting key model parameters finally section 6 concludes with a review of the key lessons learned and a road map for future research and uptake of the approach 2 background the paper distinguishes two different aspects of environmental modeling i the modeling process and ii the environmental model itself the environmental modeling process also termed environmental modeling encompasses all the steps required to successfully design build apply and generate outputs from an environmental model the environmental modeling process may include tasks such as preparation and processing of data sets estimating and setting values for parameters model execution and validation and presentation of results on the other hand an environmental model is a core computational asset for the environmental modeling process that provides a precise mathematical approximation of a real physical system both the model itself and the wider modeling process present challenges to managing complexity and to enhancing reproducibility validation and opportunities for decision support fig 1 summarizes graphically three major challenges that affect both the modeling process and the environmental model itself the level of complexity of both process and model a lack of transparency in model and process and difficulties in handling uncertainty in both model and process arrows in fig 1 connect primary continuous arrow line and secondary dotted arrow line causes of each challenge for both the modeling process and the environmental model itself for example black box implementations are a primary cause of insufficient model transparency uncertainty about the model s inner workings is a secondary effect of black box implementations as the detailed model design is not exposed to users the different causes of each of the three major reproducibility challenges complexity transparency and uncertainty are discussed further in the following sections 2 1 complexity of model and process environmental systems are inherently complex environmental modeling attempts to balance the need to develop tractable and understandable models with the requirement to adequately capture the behavior of complex uncertain and often ill defined environmental systems young et al 1996 paola and leeder 2011 in most cases the physical systems being modelled are sufficiently complex that our understanding of them remains incomplete for example accurate prediction of solar radiation may be confounded by a lack of knowledge of precise sky conditions and cloud composition šúri and hofierka 2004 rich et al 1994 further simple interactions at small scales in environmental systems can give rise to complex behavior at larger scales werner 1999 paola and leeder 2011 the complexity of physical systems in turn gives rise to complexity in the modeling of environmental processes environmental modeling may involve numerous steps to generate the final results each step may require the integration of data sets setting multiple parameters and the choice between different options for analysis a failure to document and justify exhaustively each modeling step undermines the ability of other modelers to scrutinize or reproduce results from previous studies differences in modeling processes between different users are known to cause variations in model outputs for instance even when using the same solar radiation model and the same type of data sets different studies can still result in significantly different modeling results cf li et al 2015 redweik et al 2013 kodysh et al 2013 similarly problems arise when certain steps may be excluded from a detailed description of the modeling process one such example is the validation of model results often excluded or treated as a separate process in environmental modeling just as for other components of the environmental modeling process the model itself embodies increasing levels of complexity as models and modeling tools become increasingly sophisticated the complexity of a model may be driven by the developer s pursuit of ever more detailed representations of physical systems walker et al 2003 more detailed representations of physical systems tend to lead to an increasing number of model components data inputs and parameters this complexity may increase the likelihood of incorrect estimation and selection of appropriate parameter settings it may even make models inaccessible to novice users in either event reproducibility is decreased 2 2 transparency of model and process the complexity of environmental models and modeling processes frequently contributes to a lack of transparency limited descriptions and poor documentation of models and processes limit the information available to successfully reproduce the results of modeling lacking adequate information about model limitations and assumptions subsequent model users may apply models inappropriately or incorrectly jakeman et al 2006 the desire to manage high levels of model complexity frequently leads to black box components in environmental models black box components do make it easier for novice users to include those components in larger more sophisticated models however making the component details inaccessible or unknown for users makes it more likely those components will not be properly understood or applied gilbert et al 2018 holzworth et al 2010 for instance black boxes with low levels of transparency may prevent users selecting appropriate model parameter settings previous studies often do not provide sufficient information about correct parameter selection cf singh and banerjee 2015 santos et al 2014 li et al 2015 kodysh et al 2013 further black box components make errors much more likely to go unnoticed until they emerge as obvious blunders in output hutton et al 2016 thus modeling practice based on opaque black box model components tends to restrict knowledge sharing to narrow communities of experts from specific research domains a lack of clear descriptions of the modeling process together with the application of black box model components only reduces opportunities for sharing and understanding environmental models and modeling results 2 3 uncertainty in model and process uncertainty arises at every step of the environmental modeling process including model development and design model data and parameters and model application uncertainty comes in many forms but can frequently be traced back to two main causes a lack of correctness termed inaccuracy and a lack of detail termed imprecision worboys and duckham 2004 model developers must necessarily make multiple decisions in simplifying environmental systems all environmental models then involve a degree of subjectivity in their design walker et al 2003 jakeman et al 2006 model inaccuracy arises when modelers incorrectly define the mathematical relationship between model inputs and outputs model imprecision arises when models lack relevant detail about the environmental phenomenon under study models may also sometimes accord unwarranted levels of detail termed spurious or false precision the data assets required to execute environmental models including input data and parameters are also subject to imprecision and inaccuracy inaccuracy and imprecision are endemic in many environmental data sets particularly in connection with spatial data shi et al 2002 for instance a raster map with coarse resolution provides less information about a specific location than the same map at a finer resolution parameters too are subject to uncertainty parameter inaccuracy relates to errors in estimation measurement or calibration of a parameter value parameter imprecision refers to a lack of detail in a parameter setting such as estimating a value as 10 units rather than say a more precise value of 11 4 units these uncertainties can combine with a modeler s lack of experience or level of knowledge about the application domain refsgaard et al 2007 showed that the level of a model user s experience played a significant role in the results of an environmental modeling application users with more experience in a particular modeling method and or computational asset were more likely to follow their own preference in customizing and parameterizing the modeling process refsgaard et al 2007 in contrast novice users had difficulties to conduct the process at all because of lack of domain knowledge thus given the same modeling resources and data sets experienced users approached the solution in different ways while novice users had difficulties generating any appropriate output in all cases uncertainty at every level can propagated through to inaccuracy and imprecision in model outputs the data sets generated by our modeling may themselves be inaccurate and imprecise thus in addition to the demonstrated lack of reproducibility uncertainties ultimately lead to a reduction in the warrantability in our modeling outputs 2 4 scientific workflows scientific workflows are an approach to scientific computing that aims to explicitly capture and document all the steps involved in capturing transforming aggregating analyzing and presenting scientific data cf altintas 2008 ludäscher and goble 2005 according to gil et al 2007 granell et al 2013 and kitzes et al 2017 scientific workflows increase reproducibility and improve decision support of computational processes by allowing simple integration of multiple computational assets improving transparency through detailed description and documentation providing a flexible environment for design and execution of multiple tasks and increasing ease of sharing of models among users at their core scientific workflows can help manage the complexity of environmental modeling cf buahin and horsburgh 2018 cohen boulakia et al 2017 granell et al 2013 kitzes et al 2017 scientific workflow management systems swfms such as kepler www kepler project org knime www knime com and vistrails www vistrails org index php main page make documentation easier offer an integrated environment to capture and share all data sets parameters and computational assets and provide a consistent structure for computational processes based on data flows connecting operations with defined inputs and outputs cf ludäscher et al 2006 berthold et al 2009 bakos 2013 freire 2012 freire et al 2014 swfms also often offer a graphical interface to interact with workflows 1 1 figs 6 8 discussed further in later sections provide examples of the graphical representation of scientific workflows within the knime swfms distributed capabilities for grid or cloud computing with workflows altintas et al 2003 nesting of workflows at multiple levels of detail and abstraction and the ability to share the entire computational processes documentation and data through a single file bakos 2013 scientific workflows are already being applied in environmental modeling for example kaster et al 2005 applied scientific workflows to the development of an open and transparent tool for documenting monitoring and assessing the impacts of fertilizing application on soil and water in agriculture zyl et al 2012 designed and tested a scientific workflow for spatiotemporal wildfire with a particular focus on accessing and integrating geospatial data in the scientific workflows scientific workflows have also been successfully implemented for disaster response for processing location based social media cerutti et al 2019 the study demonstrated the potential for improvements in reproducibility and transparency of three disaster response analytics processes using knime scientific workflows fang et al 2008 have designed scientific workflows for multi satellite remote sensing knowledge extraction for disaster monitoring and early warning in the fishery industry the implementation of workflows enabled automated analysis of water quality and marine disasters such as detection of oil spill accidents another case study involved the design and implementation of scientific workflows for monitoring the water quality of east lake in wuhan hubei province yue et al 2015 a novel geospatial workflow system named geojmodelbuilder was developed to show how remotely sensed imagery could be integrated with available sensor data to generate turbidity maps in the same paper a more sophisticated use of scientific workflows was demonstrated through an additional case study which integrates two hydrological models topmodel and hargreaves together with spatial analysis algorithms the application arguably improves reproducibility as well as assisting in the management and efficient execution of the computational process of coupled watershed runoff modeling scientific workflows have also been used as a visualization tool for improving reproducibility and transparency of hydrological models leonard and duffy 2016 climate modeling is another established area of scientific workflow application turuncoglu et al 2013 developed kepler scientific workflows in which a coupled modeling system integrates regional ocean and atmospheric models for simulating sea surface temperature despite the manifest advantages of scientific workflows and swfms challenges undoubtedly remain in realizing those advantages scientific workflows can themselves become a complex system structure different software platforms programming languages standards and formats can cause issues with workflow performance and sharing cohen boulakia et al 2017 gil et al 2007 kitzes et al 2017 mcferren et al 2012 hence a key contribution of this work is to provide a blueprint for realizing the potential advantages of scientific workflows in environmental modeling however as a basis for comparison we first review in more detail the classical approach to environmental modeling through the specific example of solar radiation estimation 3 case study solar radiation estimation with solar analyst estimation of solar radiation is an important component of many environmental models solar radiation is also an important data product in its own right for applications in civil engineering urban planning and natural resource management the physics that underpin solar radiation are well understood and predictable making solar radiation an ideal base case study for using scientific workflows nevertheless despite this relative simplicity solar radiation estimation still embodies sufficient complexity to exhibit the challenges identified above 3 1 solar analyst solar analyst is one of the most commonly used environmental models for assessment of solar energy potential the popularity of this model is in part related to its availability in the widely used software arcgis www esri com en us arcgis about arcgis overview fu and rich 1999 2000 working with proprietorial close source models such as esri arcgis spatial analyst toolkit presents more challenges to reproducibility than open source alternatives such as r sun in grass gis https grass osgeo org and saga www saga gis org en index html hence the approach taken here could also be applied to other open source alternatives such as r sun with it expected that in most cases reproducibility can at least as easily if not more achieved working with open source components solar analyst requires a digital surface model dsm rich et al 1994 fu and rich 1999 2000 as its primary input for each raster cell of the dsm solar analyst creates an upward looking viewshed illustrated in fig 5 3a comparable to hemispherical photographs solar analyst then constructs the sky obstruction from surrounding topographic features for each location covered by the dsm in fig 5 3a the dark gray region depicts obstructed areas while light gray depicts the sun visible area of the upward looking viewshed the model outputs estimates of the combined direct and diffuse solar radiation at each location estimating solar energy potential with solar analyst entails data preparation and preprocessing setting parameters and configuring solar analyst and executing the model and collating or visualizing the outputs fig 2 shows an example study area in melbourne australia monash university clayton campus spatial data about spot heights contours and building footprints is combined in a data preprocessing step to generate a dsm of the study area as shown in fig 3 the dsm illustrated in fig 3 is the main input for the model integrated in esri s arcgis area solar radiation tool the model accepts data in all projected coordinate systems supported by arcgis software esri 2012 once the input data is assembled the next step is to determine appropriate spatial temporal and physical parameter settings for model execution setting parameter values is frequently a process that requires estimation experience and ultimately subjective decisions by the modeler table 1 in the following section 3 2 contains further details on parameters specific settings and data sources for the monash university case study finally the esri solar analyst area solar radiation tool was executed to provide estimated solar energy potential across the study area the main output from the model is a raster containing information about the global direct and diffuse solar radiation potential across the study area for example fig 4 maps the total annual solar energy radiation potential output by the model across building rooftops in the study area the estimated solar radiation potential in fig 4 provides information about the distribution of solar radiation across building rooftops at monash university this information can provide a basis for more accurate and economical installation of solar photovoltaic energy systems at the study area additionally estimated solar radiation potential helps in planning investment and development of strategic solar energy projects 3 2 model parameters an array of different model parameters must be set in order to estimate solar radiation using solar analyst despite its relative simplicity as a well understood physical model the parameters can be bewildering for modelers not already familiar with solar radiation modeling generally and solar analyst model specifically cf table 1 below amongst the simplest parameters are those that define the spatiotemporal location of the model run clearly an important factor in direct solar radiation specifically the model requires as parameters the geographic latitude and the time period under study two further definitional parameters configure the model itself for a spherical coordinate system a user needs to specify a z factor parameter for the given latitude of area of interest the z factor allows different units of length to be used in the z direction height when compares to the x y planar spatial units of the dsm the type parameter determines the underlying behavior and ultimately accuracy of the model using the simpler but less accurate uniform type diffuse radiation model assumes the same incoming diffuse radiation at any sky direction the standard overcast type model by contract additionally models the directions and intensity of incoming diffuse radiation fu and rich 2000 estimating two further physical parameters has a critical impact on the estimated incoming radiation the diffuse proportion estimates the proportion of total incoming solar radiation that is diffuse a typical diffuse proportion will vary between 0 2 20 for clear skies to 0 7 70 for dense cloud conversely transmission proportion is an estimate of the average proportion of incoming solar radiation reaching the ground typical transmission proportion values will vary between 0 7 for very clear skies to 0 4 for overcast the two parameters are inversely related by prevailing atmospheric conditions and so are not entirely independent finally the most complex and numerous parameters define the spatiotemporal granularity of the model computation in all cases specifying finer granularity offers the potential for greater precision and accuracy in the output solar radiation estimates however finer granularity also entails greater computational overheads with the computationally intensive nature of the model rapidly making the computation intractable as granularity becomes finer the zenith divisions and azimuth divisions specify the number of sky sectors used to compute diffuse radiation fu and rich 2000 typical values can vary from relatively coarse 8 sectors to arbitrarily fine granularity a sky size parameter sets the extent of the hemispherical raster grids used to estimate direct and diffuse radiation across different parts of the sky this parameter is expressed as the length of one side of the raster grid in unit cells e g 250 500 1000 the number of azimuth directions determines the number of rays used to compute the viewshed while minimum of 8 directions may be sufficient for smooth terrain irregular terrain or urban settings may require setting much higher levels of detail to adequately capture the shading effect of nearby features such as buildings finally temporal parameters also define the temporal granularity of the output i e whether the estimated solar radiation is averaged over minutes days months or even years fig 5 summarizes the main granularity parameters used in solar analyst in the example of model output in fig 4 32 zenith and azimuth divisions defined sky sectors for computing diffuse radiation fig 5 1d the setting value for sky size parameter was 250 fig 5 2a the number of azimuth directions was set to 32 fig 5 3c and the time interval was set to monthly fig 5 4b 3 3 summary the process of solar radiation modeling with solar analyst highlights number of issues with wider significance in environmental modeling firstly the modeling process is complex it includes multiple steps including preparing and processing spatial data sets setting parameter values executing the model and visualizing the outputs these steps are often not well documented it is also not possible to infer anything about the specific steps and inputs used from the output ultimately generated secondly several of these steps require domain expertise experience or judgment this feature makes using the model error prone or impractical for novice users even for experts the need for subjective decisions makes it harder to reproduce the modeling process either by another expert or indeed the same expert weeks or months later it also reduces the warrantability of the outputs how do we know if a parameter setting is appropriately selected thirdly the physical equations and relations used to generate the output are not accessible to the modeler solar analyst effectively operates as a black box modelers have little opportunity to interrogate the design and structure of the model directly aside from reading the manual finally solar analyst offers no support for validation of model output validation is a critical step in understanding the reliability of any environmental modeling outputs but a step that is all too frequently omitted in practice brito et al 2012 kodysh et al 2013 santos et al 2014 in summary high levels of complexity a lack of transparency and a lack of decision support for modelers all serve to reduce the reproducibility of the modeling process and the warrantability of the model outputs addressing these issues for solar analyst and for environmental modeling more generally is the focus of our scientific workflows approach in the following sections 4 scientific workflows for increased reproducibility and warrantability this section demonstrates how the capabilities of scientific workflows can be used to address the fundamental issues identified above of reproducibility of the modeling process and warrantability of the modeling outputs 4 1 knime solar analyst workflow the knime analytics platform was chosen as the swfms for this study potentially any swfms with similar capabilities might also be used in place of knime including those discussed in section 2 4 i e kepler vistrails geojmodelbuilder amongst others however knime offers a range of features which while not unique to knime do make it an ideal selection for exploring reproducibility in environmental modeling knime is an open source cross platform swfms with a graphic user interface compatible with in microsoft mac and linux operating systems some other options such as for example esri model builder are commercial software products with closed source and not supported across all common operating systems knime supports an array of external components including machine learning tools such as decision tree learning neural network and support vector machines while many other software packages do have graphical tools for workflow like automation of processes including esri model builder 2 2 https pro arcgis com en pro app help analysis geoprocessing modelbuilder what is modelbuilder htm grass gmodeler 3 3 https grass osgeo org grass78 manuals g gui gmodeler html and qgis graphical modeler 4 4 https docs qgis org 2 8 en docs user manual processing modeler html these tools are typically difficult to integrate with external components and software systems knime has a large established and active user community unlike some more research oriented tools such as geojmodelbuilder knime offers the full range of swfms features including parallelism of both tasks operations and pipelines data flows and nesting of tasks and data flows in subworkflows such features are essential to efficient execution and deployment of workflows and managing complexity in environmental models and are largely absent in automation tools such as esri model builder grass gmodeler and qgis graphical modeler complete knime workflows including process data and documentation are easily archived and shared at any stage of execution using a cross platform persistent file format in an swfms such as knime analysts can see workflows graphically and manipulate them on screen using the gui fig 6 shows a minimal knime workflow to reproduce the most basic elements of the modeling process already described above in section 3 the solar analyst model itself is integrated with the workflow as a python script central node in fig 6 the script executes an external instance of solar analyst in esri arcgis the computational performance of the workflow is approximately the same as executing solar analyst through esri arcgis graphic use interface solar analyst model execution dominates all other computational demands of the system whether initiated through knime or arcgis interface other operations such as inputs parameters and raster dsm and outputs simple tabular output in fig 6 are also represented with labeled boxes lines connecting boxes capture flows of data or parameters between operations left to right importantly the workflow is executable integrating both data and the operations upon data the traffic lights associated with operations in fig 6 summarize whether the underlying data has been processed green is ready to be processed yellow or is not yet ready to be processed red the swfms enables the entire workflow including data parameter settings and the dsm data set in fig 6 to be archived at any stage of development or execution the intuitive graphical layout together with simple to use labels for nodes and data flows provides a degree of self documentation even such a simple starting point provides some basic reproducibility capabilities when compared with the wholly manual process described in section 3 the knime workflow can be exported together with all workflow components nodes and metanodes from knime software into a persistent knwf file the file can then be shared with and opened by other users workflows can also be distributed and shared through a server version of knime as well as implemented on cloud based platforms such as aws or azure only external software components such as the solar analyst executable itself will be absent from the exported knwf file requiring users to have access to their own version of such software but see also section 4 3 the following subsections describe two important extensions to this minimal workflow to further enhance reproducibility integrating validation and exposing model details 4 2 integrating validation as argued above validating the model and outputs is important for warrantability but frequently omitted brito et al 2012 kodysh et al 2013 santos et al 2014 using a swfms helps make model validation easier to integrate fig 7 extends the knime workflow in fig 6 with additional validation of the model output through comparison with a ground truth data set data about expected solar radiation data is provided by the australian bureau of meteorology www bom gov au climate data captured using pyranometers at local meteorological stations for our study expected solar radiation took the form of monthly solar radiation potential kwh m2 for the study area averaged over the last 30 years in cases where ground based pyranometer data is unavailable average solar radiation can also be derived from satellite images for the area of interest https power larc nasa gov the additional validation steps and data sets can be seen by comparing figs 6 and 7 the lower portion of fig 7 extends the basic model in fig 6 by reading in and reformatting the ground truth data set calculating the differences between the model estimates and expected solar radiation and classifying those differences for ease of interpretation the solar analyst output provides an estimated solar radiation potential in kwh m2 for each raster cell in the input dsm in the example workflow in fig 7 model outputs are classified based on their difference with the monthly average solar radiation potential value avg rad derived from the ground truth data set model outputs with the maximum of 15 relative difference δ to the ground truth data set and within a range 15 of avg rad δ 15 of avg rad are included in expected solar radiation potential category model outputs with a negative or positive difference value outside of the range are included in low and high solar radiation potential category respectively the approach to validation depicted in fig 7 is simple but can fulfill two important functions first the approach provides a mechanism to report on and share prior validation of model results in an executable form that helps users understand model limitations second the workflow provides model users with and a ready and intuitive mechanism to check for errors in their own modeling results in the latter case users can easily insert their own ground truth data to validate their own or others model outputs more sophisticated validation workflows are naturally also possible such validation might aim to highlight the spatial distribution of errors for example mapping the occurrence of errors or computing spatial correlations in error irrespective of the specific validation method used explicitly capturing validation in the workflow and alongside the modeling process itself inherently increases reproducibility and transparency crucially once again the scientific workflow combines all analysis and validation steps and all the data sets that accompany those steps hence sharing modeling and validation process and data sets ensures that both modeling and validation are reproducible and increases the warrantability of model outputs 4 3 exposing model details a strength of swfms such as knime is the ease with which external components such as esri solar analyst grass machine learning tools or python scripts can be integrated with workflows at the same time reliance on external components also introduces a weakness in reproducibility the swfms enables both process and data in the workflows in figs 6 and 7 to be archived and shared however it cannot guarantee that external components in particular proprietorial software such as esri solar analyst will be available to other modelers further the external component may operate as a black box reducing the transparency of the model unpacking the details of the solar radiation as a workflow itself improves model transparency exposing the inner operations within the model hamilton et al 2019 and provides more comprehensive documentation of the model reducing uncertainty related to model application jakeman et al 2006 hamilton et al 2019 fig 8 shows an extended scientific workflow that again executes the same basic model as that in fig 6 but exposes all of the modeling steps within the process of solar radiation estimation by esri solar analyst the workflow in fig 8 begins with two main streams the lower workflow stream calculates direct and diffuse solar radiation components based on equations from solar analyst user manual fu and rich 2000 without including the sky obstruction from surrounding area the upper workflow stream takes point raster cell coordinates together with aspect and slope information derived from the dsm illustrated in fig 3 and calculates a sky obstruction factor by including shadowing from building rooftops in our specific case study we use a small cluster of points spatially contained withing the flat rooftops and with slope and aspect constant value equal to 0 i e sky facing in our case study local ground terrain does not have any impact on the estimation of rooftop solar radiation potential because the building is significantly higher than any surrounding topography see fig 3 however more complex examples might require that workflow accounts for the shadowing effect of local terrain upon estimation of solar radiation these two streams combined together calculate solar radiation potential for any given location within the dsm note that the uncertainty associated with solar radiation potential estimates from the workflow in fig 8 will be similar to that of the solar analyst model output uncertainty this is mainly because our workflow s solar radiation model is a simplified but otherwise faithful reproduction of the solar analyst model based on the same equations for calculating two main components direct and diffuse of global radiation fu and rich 2000 our aim in constructing the workflow in fig 8 is not to build a better or more accurate solar radiation estimator rather it is to illustrate the principle of unpacking black box environmental models and exposing their workings using swfms hence this example of knime workflow illustrated in fig 8 demonstrates a component based transparent tool for solar radiation modeling which operates completely independent from the esri solar analyst solar radiation tool unlike previous workflows depicted in figs 6 and 7 in the case of this extended workflow fig 8 the whole modeling process can be exposed stored shared and manipulated with no restrictions on transparency or need for external software components or licenses swfms such as knime also helps modelers manage the complexity of more exposed models through metanodes where a single logical modeling step comprises multiple operations thus complex workflows can be hierarchically nested to retain clarity ensuring transparency and reproducibility 5 parameter setting assistance using machine learning the results in the previous section demonstrate how swfms can be used to improve reproducibility and warrantability of environmental modeling through improved model clarity transparency documentation validation and integration of data and processing the ability additionally to integrate a range of analytics tools together in a single workflow offers further opportunities for improving model reproducibility and warrantability this section explores one such example the integration of machine learning tools to assist novice users in choosing appropriate parameter settings as discussed in section 3 setting appropriate parameters often relies on domain expertise experience or subjective judgment in most past published studies using solar analyst for example authors simply do not report parameter settings singh and banerjee 2015 santos et al 2014 li et al 2015 kodysh et al 2013 this reliance on expertise and judgment creates barriers to reproducibility especially for novice users fig 9 shows an extension of our solar analyst environmental model workflow that uses a machine learning technique decision trees to support estimation and selection of appropriate parameter settings the solar radiation estimation model itself is contained within a single metanode workflow loop the majority of the visible workflow is concerned with three main steps 1 generating model output using a range of different parameter settings choices fig 9 section with orange border line 2 comparing and classifying model output based on its difference with the ground truth dataset fig 9 section with yellow border line and 3 decision tree learning with classified model output to identify parameter settings fig 9 section with red border line decision tree learning is a class of machine learning that recursively classifies a data set based on its most important or salient features russell and norvig 2002 amongst the most famous examples of decision tree learning are the id3 and c4 5 algorithms by quinlan quinlan 1986 1993 both algorithms classify a data set by selecting at each node those attribute values that maximize the shannon s information gain shannon and weaver 1949 about the training data shannon s information gain is sometimes explained as quantifying the surprisal value of information how much a particular item of data surprises us in the context of decision tree learning those attributes of a data set that once known tell us most about the ultimate outcome can be said to decrease our surprise at the outcome hence these attributes are associated with the highest information gain and so are selected first in a decision tree fig 10 shows the detail of the decision tree learning process in the knime scientific workflow expanded from the decision tree model metanode in fig 9 in the context of environmental modeling decision tree learning can be used to classify different parameters and parameter settings based on the shannon s information gain about the model output in the example of our solar radiation estimation decision tree learning can be used to classify the parameters and parameter settings most important for model accuracy certain parameters and settings will have a significant impact on the accuracy of the resulting model execution others may have limited impact in the resulting decision tree parameters that have the most impact on model accuracy i e most reduce our surprise in output accuracy will be associated to the greatest shannon s information gained successively less important parameters and their setting values will be associated with lower shannon s information gain classifying parameters and parameter settings in this way provides simple guidance to novice users as to the importance and likely impact of different parameters and settings the following subsections step through the process of using decision tree learning to aid in parameter settings 5 1 parameter solution space table 2 shows six key parameters of solar analyst fu and rich 2000 of those identified in section 3 2 the selected parameters include the physical parameters that are amongst the most difficult to estimate diffuse and transmission proportion ruiz arias et al 2009 and the granularity parameters that are most reliant on experience sky size number of azimuth directions and divisions number of zenith divisions these parameters cannot easily be observed directly by sensors or other measurements they must be estimated or defined by users based on experience or estimation some expertise may still be required to determine even the range of sensible settings to be explored e g sky size of between 250 and 550 in table 2 however guidance for the novice on the broad range of settings can often be deduced from the documentation certainly this is the case for solar analyst documentation 5 2 training data set a brute force approach to explore this parameter solution space is to use decision tree learning across all 4 6 4096 different combinations of parameter settings to classify parameters and settings based on the accuracy of the model output computing the estimated solar radiation for the entire study area can take several hours hence for our training data set a small representative cluster of rooftops exhibiting a range of different building heights was selected from the northern side of the study area the buildings contained 392 pixels around 5 of the total study area for each parameter setting combination the solar radiation estimated in every raster cell of dsm by the model can be compared with expected solar radiation measured at local weather stations as a result 4096 model runs produced 4096 392 1 605 632 records individual solar radiation estimates an example of the results of comparing model outputs with expected ground truth radiation is shown in table 3 each row in the table provides one of the 4096 possible parameter setting combinations repeated for each of the 392 raster cells the final column in table 3 additionally classifies the solar radiation in kwh m2 estimated by solar analyst compared to the ground truth expected average monthly radiation each output cell is ranked according to the absolute difference between expected and estimated solar radiation high performing cells are those where estimates are in the top 25 of smallest absolute deviations from expected lower performing cell have estimates in the bottom 75 of smallest absolute deviations from expected the rationale for selecting the top quartile as high performing is connected with the resultant decision tree structure explained further in the following section 5 3 decision tree learning decision tree learning classifies the training data according to the information gained about estimation accuracy by each parameter setting in table 2 in the resulting decision tree each node in the tree details the number and proportion of high and lower performing output records cells each branch identifies a parameter value setting with the associated information gained through that setting for example figs 11 and 12 show the decision trees resulting from decision tree learning on four of the six parameters in table 2 taking tpi transmission proportion index as an example fig 11a the root node shows the entire training data set with the top 25 of settings designated high performing and the bottom 75 of settings designated lower performance at the first level a tpi setting of 0 4 right hand branch versus 0 4 left hand branch has the largest information gain of any tpi setting in the training data set i e 0 1226 none of those model runs with a tpi parameter setting of 0 4 led to high performing i e top quartile outputs in contrast for model runs with a tpi setting of 0 4 a further tpi setting of 0 5 versus 0 5 leads to further information gained about the high performing settings i e 0 1961 and in the case of tpi 0 5 further tpi parameter settings 0 6 versus 0 6 yield further information gained i e 0 0215 examining the resulting decision tree in fig 11a the highlighted leaf node with 204 746 records 51 0 of the data has the largest number of high performing top quartile model outcomes consequently the parameter setting associated with that node tpi of 0 5 and 0 6 is arguably the best default setting for that parameter hence the rationale for training the decision tree with the top quartile classified high performing ensures that in the ideal case where just two setting decisions for a parameter e g x and y perfectly describe the training data then the resulting decision tree will have less than three levels with all of the high performing model runs allocated to one leaf node in practice however usually only some lesser proportion of the high performance outputs are classified correctly by a single parameter in this way in such cases we select the parameter setting with the largest total number of high performance outputs from the top quartile this largest number of high performing records from the top 25 of data is used as a measure for accuracy of model output with a particular parameter setting decision for example in fig 11b the parameter setting of dpi diffusion proportion index of 0 1 versus 0 1 is identified with the greatest information gain at the first level i e 0 002 in this case the first level leaf node in left hand branch dpi 0 1 is associated with the largest number of high performing model outputs 116302 high 29 of the data of any leaf node and so represents the preferred default parameter setting similar decision trees can be constructed for other parameters and settings including nadr number of azimuth directions fig 12a where a setting of 8 leads to the greatest number of high performing model outputs 104819 ss sky size fig 12b where settings have very low impact on number of high performing outputs indicated by very low information content i e 0 0001 nadv and nzdv number of azimuth zenith divisions which have almost no influence on the high versus lower performing model runs with very low information content i e 0 0001 in all cases 5 4 summary advice for users the information from the decision trees for each parameter can be summarized in the form of advice to novice users as shown in table 4 such advice may only provide a guide and may be limited in application to the local vicinity which the training data set covers however in the case of solar radiation modeling this vicinity may be large as parameters such as tpi and dpi are not expected to vary significantly over small geographic distances further the method makes efficient use of a small number of expected ground truth measurements in our case one spatial location with a history of 30 years measurements applied to a larger number of spatial locations further parameters can be ranked in order of importance according to the associated shannon s information gained through that parameter setting default settings for each parameter can then be reported as ranges derived from the leaf node in the decision tree with the highest number of high top quartile model runs see table 4 for example from table 4 the tpi parameter has the greatest information gain i e 0 1226 at the root node of decision tree fig 11a it is in this sense the most important parameter for users to set the one with the greatest influence over the performance of model outputs users can be advised that a tpi parameter setting of 0 5 tpi 0 6 is associated with the largest number of high performing top quartile model runs based on the information gained the six key parameters can be divided into two groups the first group includes the three most important key parameters tpi dpi and nadr with information gain value i e 0 0001 the second group includes the bottom ranked parameters ss nzdv and nadv associated with very small information gains of i e 0 0001 users can be advised that this second parameter group has a very low impact on the performance of model runs 5 4 1 computational considerations in cases such as those above where parameters have minimal impact on model accuracy it may be advantageous to prioritize other considerations in setting those parameters such as computational efficiency or increasing model granularity the time required to execute our complete parameter learning workflow in fig 9 is dominated by executing the 4096 solar analyst runs with different parameter settings executing all 4096 model runs requires between 24 and 48 hours depending on parameter settings on a standard pc for the selected subset of building rooftops occupying about 5 of the total study area each individual solar analyst model run requires only a few minutes to execute on the rooftop subset whereas the full study area in fig 4 may require several hours for a single model execution all other workflow operations require less than 2 min total to execute specifically with reference to fig 9 about 45 seconds for workflow in the orange area excluding solar analyst model runs 30 seconds for the yellow area and 20 seconds for the red area hence our analysis of computational scalability below focuses on the impact of changing parameters on solar analyst model runs fig 13 summarizes the impact on model execution times of the six main parameter settings explored ss nzdv nadv nadr tpi dpi in all cases the model was executed on a standard personal computer with 8 gb ram memory and a 2 40 ghz processor with four cores each model execution was conducted using a finer spatial resolution 0 2 m cell size and a coarser spatial resolution 0 4 m cell size dsm fig 13 shows how the execution time increases scales as a function of parameter setting in general finer spatial granularity leads to increased execution time as would be expected due the larger number of raster cells involved in the computation increasing the sky size ss increases the time required to execute each model as a quadratic in the number of cells per side in computing terms a time complexity of o n 2 the total number of cells in the computation increases as function of the square of the sky size so this growth in execution time is as would be expected increasing nadv leads to quadratic increases in computation time for the same reasons regression on the observed results confirmed this expectation of quadratic growth r2 0 99 in all cases while the scalability remains the same o n 2 for both 0 2 m and 0 4 m dsm the absolute times required for the finer granularity dsm 0 2 m are significantly higher in all cases for the same reason in proportion to the increased number of cells all other parameters nzdv nadv tpi and dpi have no impact on the computation time while tpi and dpi are important factors used in the computing accurate output they have no effect on the computational efficiency finally although nzdv nadv are granularity parameters specifying the mesh size for computing shadow boundaries they do not affect the structure of the model computation and so have no effect on the time required for a model run combining the decision tree output in table 4 with the computational scalability of parameters in fig 13 leads to the following parameter setting advice tpi and dpi parameter settings have the most impact on output accuracy table 4 but no impact on computational time complexity fig 13d and e hence for those parameters model output accuracy is the sole consideration nadr parameter settings have a moderate impact on output accuracy table 4 but also a significant impact on computational time complexity fig 13b hence for the nadr parameter settings towards the lower end of the range are advisable as long as they do not degrade output accuracy i e nadr 8 in the range 8 32 table 4 a change in sky size ss parameter is associated with only minimal effect on the model output accuracy table 4 but significant effects on the computational scalability with higher values requiring significantly longer to process fig 13a hence for the ss parameter settings at the lower end of the accurate range are advisable i e ss 250 in the range 250 550 table 4 nzdv and nadv settings have no significant effect on model accuracy table 4 however higher settings do not require any additional runtime hence adopting the highest setting in the range may be the natural default nadv nzdv 32 in the range 8 32 table 4 5 5 workflow limitations and applications the primary limitation of the parameter learning workflow illustrated in fig 9 is the computational time required to repeatedly execute computationally intensive environmental model such as solar radiation estimation in order to explore a large parameter space in this work that computational cost was managed by applying our machine learning of parameters to only a small representative sample of building rooftops about 300 pixels and 5 of the total study area potentially there are a number of other approaches that might be taken to reduce or ameliorate computational overheads including high performance computing hpc hpc and shared cloud based computing resources could be used to efficiently precompute model outputs using a range of parameters especially in cases where some or all default parameter settings are expected to be common across different geographic regions or groups of applications users reducing parameter solution space omitting from decision tree learning any parameters known to have limited impact on output accuracy will significantly reduce the solution space to explore for example omitting nzdv nadv and ss from training in other areas would reduce the number of model runs from 4096 4 6 to 64 4 3 potentially reducing computational time by a factor of 64 using fractional factorial designs rather than exhaustively exploring the full factorial parameter space would further reduce computational overheads a fractional factorial design exploring only some combinations of parameter settings is likely to be effective because some parameter settings tend to co vary in their effects for example parameters tpi and dpi are inversely related by prevailing atmospheric conditions see section 3 2 incremental model algorithms in our naive approach each individual model run will need to regenerate all computational resources or steps even those common to other model runs an incremental algorithm is able to efficiently reuse previously computed intermediate data or steps in subsequent model runs designing an incremental version of a black box model such as solar analyst is not feasible due to its closed nature however incremental versions of open workflow based models such as that explored in section 4 3 are potentially possible for example associated with parallel branches of the workflow fig 8 although this work has focused on the specific case study of solar radiation modeling our machine learning workflow approach has potential wider applicability for selecting appropriate parameter settings in any environmental model such as aquatic ecosystem nielsen et al 2017 hydrological pietroniro et al 2007 terink et al 2015 and weather and climate mughal et al 2017 turuncoglu et al 2013 skamarock et al 2008 models for example for aquatic ecosystem modeling estimating and selecting appropriate values for meteorological parameters such as wind speed air pressure air temperature and cloud cover fraction are similarly challenging to tpi and dpi in solar analyst further this transparent workflow approach may also find utility for calibrating and tuning weather and climate models some climate models consistently underestimate important drivers of climate such as rainfall solar radiation or wind potential while other overestimate them hourdin et al 2017 hence the automated assistance with setting workflow parameters may aid in reducing errors due to inappropriate parameter settings in model outputs finally the approach would also be useful in deciding where to prioritize additional efforts to refine parameter settings table 4 indicates that for example efforts to more precisely determine tpi and dpi in the study area are likely to lead directly to improved accuracy by contrast nzdv nadv and ss are unlikely to impact model output accuracy and need not be a high priority for modelers developing the work further still it would be possible to explore out information gain approach to similarly exploring those inputs or spatial regions where more accurate fine granular or up to date data might have the largest impact on model accuracy 6 conclusions and further work through the example of knime and solar analyst this article has demonstrated the capacity of scientific workflows and swfms to improve reproducibility and warrantability of environmental modeling in addition swfms can lead to increased opportunities for transparent decision support for environmental modelers the results demonstrate that swfms based environmental modeling can be more reproducible more transparent and more easily validated and explained than the conventional approach to modeling swfms enable the model data and parameters to be captured in a reproducible and transparent way allow the integration of modeling and model validation expose the environmental model details to scrutiny and can be used to support integration of higher level capabilities such as parameter setting advice for novice users nevertheless this research lays the ground work for a range of further avenues to explore for example there are a range of options to extend our approach to machine learning for parameter setting advice different machine learning approaches such as genetic algorithms shapiro 2001 for efficient searching the model parameter solution space might be more appropriate than decision tree learning particularly for models with larger parameter sets and spaces such techniques are similarly easily integrated through swfms further our work focuses on just one example environmental model solar radiation with a particular focus on solar analyst using a swfms means other solar radiation models such as for example grass r sun or even native workflow models such as explored in section 4 3 can be easily substituted the broader principles exposed by this work are expected to hold true for other environmental models indeed as discussed in section 3 2 solar radiation is an example of a relatively simple environmental model with a well understood physical basis more complex environmental models may reveal further challenges and opportunities for swfms the increasing popularity of geospatially enabled scientific workflows means future applications should also help to improve reproducibility and extend warrantability of environmental modeling across more complex application domains such as hydrological climate or ecological modeling yue et al 2015 turuncoglu et al 2013 nielsen et al 2017 coll and steenbeek 2017 the benefits of improved reproducibility warrantability and transparency offered by swfms are only expected to increase as environmental models become more complex software and scientific workflows availability knime software is free and open source and can be accessed at www knime com downloads download knime and www knime com next steps further background information and support documentation is provided in berthold et al 2009 and bakos 2013 all knime workflows developed for this research are freely available on github at www github com nenad1984 knime workflows git solar analyst is closed source and proprietorial available as part of the spatial analyst toolkit in esri arcgis declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported through the provision of an australian government research training program scholarship the work was also partially supported through the australian research council s arc discovery projects funding scheme project dp120100072 
25992,this study aims to improve the estimates of the economic impacts of climate change by developing a river flood risk model climrisk river and introducing it into an existing climate economy integrated assessment model iam it operates on a local scale and can project climate change related river flood damage for various socioeconomic climate and flood adaptation scenarios whereas other iams have relied on temperature as a climate change proxy we show that precipitation is a key variable in projecting river flood damage the way adaptation is accounted for in our flood damage functions has a large influence on the results highlighting the relevance of modelling local level adaptation in iams results presented at different spatial scales demonstrate the relevance of river flood damage functions for estimating the economic impacts of climate change and allows for exploration of the spatial distribution of impacts through local estimates keywords integrated assessment modelling climate change river flood risk river flood adaptation 1 introduction integrated assessment models iams of climate and the economy are commonly used to project the future economic impacts of climate change while some iams estimate the climate impacts on specific physical systems eg energy land use bosetti et al 2007 bouwman et al 2006 raoet al 2008 others paint a more general picture of climate impacts on the economy stern 2008 nordhaus 2013 anthoff and tol 2014 the latter set usually provides monetized climate impacts through climate damage functions and or estimates of benefits of climate policy aimed at reducing the greenhouse gas emissions such models estimate the complex relationship between the anthropogenic and climate system using transparent and simplified reduced form functions since the original version of the dice model nordhaus 1992 one of the first iams of climate change and the economy environmental economists have made several attempts to improve these models to aid policymakers in making decisions about climate policy in the face of climate change uncertainty tol 2018 nevertheless climate economy iams have received various criticisms including the aggregated spatial dimension of the models farmer et al 2015 the incomplete representation of climate change risks stern 2013 van den bergh and botzen 2014 and the fact that the damage functions that translate global warming into economic impacts are outdated and require an improvement diaz and moore 2017 in this respect it should be noted that in most cases these iams work on an aggregated spatial scale meaning that there is either one region the earth or several larger regions for example the well known rice model which is the regional version of the global dice model estimates the economic impacts from climate change for 12 world regions nordhaus and yang 1996 nordhaus 2017 global information about the projected economic impacts of climate change is increasingly available at a refined spatial resolution for specific impact categories which can serve as a basis for updating the iam impact functions if this improves iam estimates of the economic costs of climate change the effects of extreme weather and natural hazards on the economy due to climate change have been included in climate economy iams to a very limited extent and estimates from catastrophe models of how natural disaster risks are expected to develop under climate change have become increasingly available botzen et al 2019 for instance river flooding is an important damage category that must be better represented in iams because it poses a significant economic impact and sophisticated global modelling approaches to estimate river flood risk at a detailed spatial resolution are increasingly available wardet al 2015 about 50 trillion 1 1 all units are in us dollars and 0 8 billion people are subjected to a 1 in 100 years river flood event kundzewicz et al 2013 and the direct economic losses from river floods between 1980 and 2013 exceeded 1 trillion and caused a loss of more than 220 000 lives munich et al 2014 the river flood risk is expected to increase in many regions in the world wardet al 2013 and it is important to take this into account in economic iams of climate change the main goal of this paper is to improve the local estimates of the economic impacts of climate change by developing climrisk river a spatially explicit model of river flood risk that is introduced into a broader climate economy iam climrisk there are four main reasons why climrisk is the preferred iam for this study first other iams including rice do not provide precipitation projections which we find to be an important explanatory variable of future changes in flood damages second with climrisk we can model spatial heterogeneity in climate change impacts the results of climrisk river show that spatial heterogeneity in projected flood damages is substantial and modelling this provides insights into which areas face a high river flood risk third climrisk allows us to model climate projections probabilistically which gives insights into uncertainty of climate change impacts fourth with climrisk we are flexible with exploring how climate change impacts develop under many different scenarios with limited computing time climrisk river also accounts for local human adaptation through the use of recently developed local flood protection standard database flopros scussoliniet al 2016 accounting for local flood adaptation in estimating the river flood damage functions for the iam is important as flood protection standards that reduce the probability of a river flooding already exist in many areas in the world governments are likely to update these standards if flood risk increases as a result of climate change most studies that use iams such as dice account for adaptation implicitly through the reduction in the climate damage used in estimating the damage function nordhaus 2017 we follow this approach by estimating the river flood damage functions implicitly using flood damage estimates that account for different flood protection standard scenarios these scenarios range from maintaining current flood protection standards to implementing economically optimal protection given a climate and socio economic scenario through creating unique flood damage functions under each flood protection scenario we contribute to the small body of literature that has made adaptation explicit in damage functions in climate economy iams de bruin et al 2009 hope 2011 dumas and ha duong 2013 our approach of accounting for river flood risk in a climate economy iam is currently the closest to the work of kuik 2017 in introducing the river flood damage functions in the climate framework for uncertainty negotiation and distribution fund model a climate economy iam anthoff and tol 2014 the flood damage functions in fund generate estimates for 16 regions as a product of a regional damage coefficient regional temperature and gross domestic product gdp moreover the functions make use of ad hoc medium and high adaptation scenarios from jongmanet al 2015 however with gridded flood damage and flood protection standard data available it is now possible to produce accurate local estimates of expected annual flood damage climrisk river flood damage functions are spatially more detailed more complete as they account for wet and dry regions by means of gridded precipitation projections as an additional explanatory variable and have a more realistic representation of adaptation policy the reason for the latter is that flood adaptation scenarios are not implicitly assumed but are modelled using data on existing and future economically optimal flood protection standards scussoliniet al 2016 wardet al 2017 the end product of climrisk river is a set of flood damage function estimates which are introduced into a general framework of climrisk which allows the user to explore climate change related damage including the expected damage due to river flooding the model works for any user defined climate change and socioeconomic scenario combination presented in the ipcc 5th assessment report the model can be used by policymakers interested in expected future changes in flood risk and implementing a flood adaptation policy in a particular geographic area for this audience climrisk river can serve as a preliminary quick scan that gives insights into how flood risk is expected to develop under future scenarios and the effectiveness of flood protection infrastructure to limit this risk which can be a motivation for conducting additional higher resolution studies to identify appropriate local flood risk management measures other users are members of the academic community who are interested in mapping river flood hotspots under different future scenarios to motivate further local scale flood adaptation or climate hazard research moreover the integration of flood risk in a general climate economy iam is likely to appeal to the broader community of researchers and policymakers who are interested in understanding global regional or local economic impacts from climate change 2 methodology and data in this section we present the methodology used to develop climrisk river this section follows a downstream flow that resembles the flow of methods presented in the flowchart fig 1 firstly we outline the ingredients required for developing the new river flood damage functions that emulate glofris subsection 2 1 next we develop 6 regression models and evaluate their performance based on in sample and out of sample glofris data subsection 2 2 once the most suitable model model 6 is selected it is introduced into the climrisk model by feeding it downscaled climate and socioeconomic data from a spatially explicit iam climrisk explained in appendix subsection a finally the results of climrisk river and the resulting total climate damage of climrisk after integration are presented in section 3 2 1 flood risk ingredients the main goal in developing climrisk river is to produce validated river flood damage functions for different protection standard assumptions that can be fed with temperature precipitation and gdp estimates from any climate or socioeconomic scenario combination the input for the flood risk emulator climrisk river is based on the glofris model a global framework for flood risk assessment that works on a detailed spatial scale 30 30 and includes all main river basins worldwide winsemius et al 2012 the main ingredients of the new flood damage functions for climrisk river are as follows risk expected annual damage ead vulnerability flood protection standards hazard climate projections exposure economic projections each of the above listed ingredients are explained in more detail in the following sections 2 1 1 risk expected annual damage ead formally flood risk can be defined as the expected annual damage ead or the damage of a hazardous event weighted by the probability of its occurrence over a smooth probability curve it can be written as 1 e a d 0 1 p d p d p where ead represents the ead of the particular cell d p is the damage in that cell caused by a flooding event of probability p which is related to a flooding with return period r p in the following way 2 r p 1 p the ead estimates are made using current and future flood hazard layers built up area gdp estimates country level maximum damage estimates and depth damage curves based on the occupancy type ward et al 2017 tiggeloven et al 2020 winsemius et al 2016 to estimate the flood damage functions for climrisk river using glofris data we must retrace the steps of how the data were created in the first place in glofris flood damage is estimated for flood return periods of 2 5 10 25 50 100 250 500 and 1000 years for illustration the damage of a 2 year return period represents the loss that would be caused by relatively small floods with annual exceedance probability of 0 5 the resulting flood probabilities were calculated using equation 2 and a smooth probability curve was obtained for the purpose of ead estimation between the above mentioned points the ead in climrisk river for any particular cell can be calculated using the trapezoidal rule for approximating definite integrals 3 e a d g f i 1 10000 d g f x p 1 d g f x p 2 δ x p where ead gf is the approximation of ead eq 1 in the glofris gf model x 0 0 0001 1 is a vector of equally spaced 10 000 points and δx p δx is the length of equal spacing these loss data are estimated in glofris for current conditions but also for future climate and socioeconomic scenarios and are available for different time periods centered around three years 2030 2050 and 2080 the glofris modelling cascade uses forcing data from eu watch weedonet al 2011 over the period of 1960 1999 to force the hydrological model pcr globwb sutanudjajaet al 2018 which is used for the flood inundation modelling as baseline conditions as baseline for the socioeconomic data glofris uses the hyde database which consists of gridded percentages of built up area population and gdp projections as a starting point damage estimates for rivers of different return periods for the baseline period are collected and these do not depend on different emissions or socioeconomic scenarios hence the baseline represents current climate and socioeconomic conditions the future projections of flood damage however are available for various climate 2 2 climate scenarios rcp2 6 rcp4 5 rcp6 0 rcp8 5 and socioeconomic scenarios 3 3 socioeconomic scenarios ssp1 ssp2 ssp3 ssp4 ssp5 finally the data are available for five different earth system model esms 4 4 esms used hadgem2 es gfdl esm2m noresm1 m ipsl cm5a lr and mirocesm chem simulations that were used to force the glofris model projections for each rcp scenario see fig 2 each of the above mentioned combinations of climate and socioeconomic scenarios for different esms in different time periods produces a single observation of ead for each cell therefore each cell ideally contains 305 observations of ead with no missing data 5 5 the number 305 comes from the fact that there are 5esms 5ssps 4rcps 3years 300 plus 5 additional baseline period observations for each esm by pooling estimates from different esms in our damage function estimation we are effectively averaging across all five available esms and making our emulator esm independent with respect to climate input the user would only need to specify the rcp scenario and global average temperature percentile realization through magicc appendix a 1 which would naturally correspond to a particular esm all the ead estimates are expressed in billions of us dollars 2005 ppp and are available for 30 x30 cells to integrate climrisk river into the climrisk model all the glofris river flood data must be upscaled to 0 5 0 5 by aggregating the impacts over the 30 30 cells prior to integration with climrisk river the emulated damage estimates were translated into us dollars 2010 ppp to match the climrisk monetary impact estimates when providing the total climate change damages 2 1 2 vulnerability flood protection standards to obtain more reliable estimates of annual expected river flood damage the current flood protection standards must be taken into account the flood protection standards enter the flood risk model through the ead function whereby damages of rivers with return periods lower than the available protection standards are assumed to be zero for example if an area is protected against a 500 year return period flood this means that the sum in equation 3 only contains damage estimates of rivers with return periods greater than 500 years a comprehensive global database flopros of observed and modelled current river protection standards has recently been compiled scussoliniet al 2016 the flood protection data are available at the state level and any given cell receives protection equal to its estimated state protection level the river protection data are currently available for 2683 states in the world in the form of river return periods against which the state is protected however in estimating the annual expected flood damage in the future we also need estimates of future flood protection standards that depend on adaptation decisions about flood protection investments flopros data consists of two options for future protection standards 6 6 the names assigned to the protection standards in this paper are arbitrary and do not correspond to the names given by the original authors of the flood protection standards baseline height standards baseheightstd which assume that the protection infrastructure is maintained at the baseline year height in the future and allow the river flood risk to vary over the course of the century this scenario does not imply any additional river flood adaptation baseline probability standards baseprobstd which assume that the protection standards are updated so as to keep the baseline flood probability constant this scenario does imply additional river flood adaptation as the flood protection standards are upgraded according to the varying natural factors in order to maintain constant flood probability regardless of the constant flood probability the flood related damage could still vary with the amount of exposed assets and the severity of flooding taking this research a step forward through cost benefit analysis policymakers desired economic decision making has been taken into account when designing the optimal level of river flood protection kuik 2017 optimal standards optimalstd which assume that all states behave in an economically optimal manner and invest today in the level of protection that would yield the highest net present value npv over the twenty first century in addition to the three adaptation scenarios a fourth scenario no standards nostd will be used for comparison with the other assumptions as suggested by the name this scenario assumes that no cells are protected against any potential river floods thereby ignoring existing protection standards as modelled in flopros this is true for both the baseline year estimates and future projections although this is not a very realistic assumption it helps us understand how important flood protection standards are in estimating the flood damages in an iam the data for optimal protection standards is not available for some cells due to for example missing future projections of socioeconomic data in such cases we assume that the protection is maintained at the baseheightstd level if available 2 1 3 hazard climate change projections as in many climate iams annual surface air temperature is among the primary climate variables of interest and serves as the main proxy for climate change we are also interested in the effect of annual precipitation on the river flood risk precipitation has until now not been introduced into an iam damage function and it is important to assess its impact on river flood risk in light of newly available local precipitation data although floods are generally short lived events that are mainly driven by extremes in precipitation we follow the common approach in iams to use annual climate indicators in our case annual temperature and precipitation as a proxy of climate change this is in line with the purpose of climrisk river to project changes in long term annual expected flood damage in response to long term trends in economic exposure and climate change which as our results show can be approximated by changes in annual temperature and precipitation section 3 1 whereas the climate forcing data in climrisk is generated through magicc with the use of pattern scaling see appendix a 1 the forcing data in glofris contains daily gridded estimates of surface temperature and precipitation these are generated using different esms interpolated to 0 5 0 5 and they are bias corrected using observations from 1960 to 1999 for the eu watch project these same estimates are then used to force the pcr globwb global water and hydrological model sutanudjajaet al 2018 the eu eu watch forcing observations are also used to generate the baseline flood risk the reason why 2010 is not used as the baseline period for the climate forcing data in glofris is the fact that the authors used a 40 year interval around the year of interest in extreme value analysis as observed data is not available for 1990 2030 the authors assume that global hydrological processes did not drastically change between 1980 and 2010 after accounting for climate therefore the eu watch baseline period estimates are used in the year 2010 as the baseline period to create the climate input variables necessary for the river flood damage functions daily temperatures were converted to annual mean temperature and daily precipitation to total annual precipitation since the glofris model produces estimates for three periods centered around years 2030 2050 and 2080 mean annual surface temperature and total annual precipitation estimates were averaged over the years 2010 2050 2030 2070 and 2060 2100 finally differences of precipitation and temperature with respect to the baseline period average climate were taken in order to fit the magicc climate projection units for temperature the difference is expressed in absolute value of degrees celsius while for precipitation the percentage change of precipitation with respect to baseline period is required 2 1 4 exposure economic projections exposure is another important determinant of flood damage because it captures the extent of assets that are prone to flooding gdp ppp was extracted from the ssp database with the oecd env growth gdp projections the gdp data used to estimate the flood damages in glofris are derived from the iiasa ssp database riahiet al 2017 next the future development of urban areas is estimated using the 2up model van huijstee et al 2018 in this model urban and rural populations are distributed according to a map of urban areas which helps in determining the exposure of a certain area in climrisk river we proxy the glofris exposure data using iiasa projections for various ssp scenarios combined with the compatible sres scenarios to create a spatially explicit 0 5 0 5 grid of local gdp estimates more information about exposure ssp and sres data can be found in appendix a 2 2 2 climrisk river an important step in climrisk river development is the formulation of the flood damage functions the main decisions involved in this step are geographic scale of model parameters choice of explanatory variables the geographic scale of model parameters refers to the geographic area that a particular damage function coefficient covers depending on input data availability the scale can range between highly local 30 30 grid cells and global since the scale of downscaled inputs in our iam is somewhere in between the primary candidates for the model scale in climrisk river are the 0 5 0 5 grid cell and river basin level the main advantage of river basin level functions is simplicity each cell within a river basin would inherit the set of parameters corresponding to the basin the main disadvantage however is the loss of model fit as heterogeneous grid cell data are aggregated into a single function over a potentially large basin area the opposite is true of a grid cell level function as we prefer the higher explanatory power of the grid cell level functions over the smaller total number of function estimates we set the scale to grid cell level to make full use of local input data the scale of results is still entirely up to the user who can explore various scenario combinations at different levels of aggregation the next step concerns the choice of the dependent and explanatory variables the main goal of the regressions that follow is to project the δead t that is the change in ead with respect to the baseline period for the available data t represents a time period mid point for which the glofris expected annual damage estimates are made namely t 2010 2030 2050 2080 in order to find the most suitable statistical model for climrisk river we take a nested model approach and evaluate several functional forms with increasing number of terms the following nested model consisting of six functional forms is evaluated model 1 δ e a d g f t β 1 δ g d p t model 3 δ e a d g f t β 1 δ g d p t β 2 δ t t where δ t t represents the absolute change in annual mean surface air temperature in year t 7 7 all changes in temperature and precipitation are with respect to 1980 observed climatology from eu watch weedonet al 2011 model 3 δ e a d g f t β 1 δ g d p t β 2 δ t t β 3 δ t t 2 model 4 δ e a d g f t β 1 δ g d p t β 2 δ t t β 3 δ t t 2 β 4 δ p t where δ p t represents the percentage point change in precipitation in year t model 5 δ e a d g f t β 1 δ g d p t β 2 δ t t β 3 δ t t 2 β 4 δ p t β 5 δ p t 2 model 6 δ e a d g f t β 1 δ g d p t β 2 δ t t β 3 δ t t 2 β 4 δ p t β 5 δ p t 2 β 6 δ t t δ p t where δ t t δ p t represents the interaction term between temperature and precipitation change after evaluating the six different emulator models model 6 proved to have the best predictive power across different rcp ssp scenario combinations in other words model 6 presents the best emulator of our chosen river flood risk model glofris and is chosen as the climrisk river model it is defined as 4 δ e a d g f t β 1 δ g d p t β 2 δ t t β 3 δ t t 2 β 4 δ p t β 5 δ p t 2 β 6 δ t t δ p t where β 1 is the effect of a 1 billion increase in gdp 8 8 all the listed effects refer the change in expected flood damage δd t δgdp t is the difference in gdp between year t and 2010 β 2 is the effect of a 1 c increase in surface air temperature δt t represents the change in mean surface air temperature at t β 3 is the squared term of surface air temperature δ t t 2 is the change in squared surface air temperature at t β 4 is the effect of a 1 increase in total annual precipitation δp t is the percentage point change total annual precipitation at t β 5 is the squared term of total annual precipitation δp t 2 is the percentage point change in total squared annual precipitation at t β 6 is the effect of a 1 increase in total annual precipitation conditional on a 1 c increase in surface air temperature and δ t t δ p t is the interaction term between the change in mean surface air temperature and the percentage point change in mean total annual precipitation at t this model was selected so as to take advantage of the explanatory power of precipitation when estimating δead precipitation could capture the effect that wetter or drier regions could have on the frequency of flooding in addition the interaction between temperature and precipitation could capture the interaction between for example hotter and wetter regions both of which could lead to an increase in flood risk higher than estimated by temperature and precipitation alone this functional form is similar to the functional form of the rice damage function where quadratic climatic terms are used to capture nonlinear effects on damage of high temperature and in our case also precipitation that is excluded in rice nordhaus 2014 there is no constant term in the regression as the function passes through the origin and the left and right hand side terms are zero in the baseline year while the gdp estimate in rice is multiplied by the impact function eq 11 the δ g d p t in this model is an explanatory variable the specific units for temperature and precipitation explained above were selected so as to match the magicc model output appendix a 1 magicc generates differences in annual surface temperature in degrees celsius and percentage difference in annual precipitation with respect to any particular base year since the precipitation data used for the fitting was originally expressed in k g m 2 s a unit conversion is necessary to obtain the percentage difference 5 δ p t 100 p t p 0 p 0 where δp t is the percentage change in total annual precipitation in year t p t is the total annual precipitation estimate at time t and p 0 is the baseline total annual precipitation estimate due to the large number of possible scenario combinations that can be fed to the climrisk river model a select few are presented in this paper as climate and economic trajectories are uncertain it is useful to consider the middle of the road case of either flood risk driver and to explore the impact of varying the other in this paper we chose to keep the socioeconomic aspect of development fixed by opting for the ssp2 scenario therefore we vary the climate projection and mainly focus on the following two scenario combinations 9 9 in addition to these two consistent climate economy scenarios we also present some results for other rcp scenarios when we explore the advantages of climate change mitigation in section 3 2 1 unsustainable world in the middle of the road rcp6 0 ssp2 in this scenario the carbon emissions peak around year 2080 declining thereafter in combination with the ssp2 scenario of intermediate challenges for both adaptation and mitigation this scenario combination allows us to explore the impact of an intermediate baseline scenario without any emission reduction policies 2 sustainable world in the middle of the road rcp2 6 ssp2 abiding by the paris climate agreement rcp2 6 and strongly curbing carbon emissions well below 2 c this scenario is consistent with the sustainable future assumption whereby countries are abiding by the paris climate agreement schleussneret al 2016 given these two contrasting representations of carbon emission development expected economic damages from not mitigating climate change to 2 c can be calculated in addition we will also compare ssp1 and other ssps in the results section to explore the uncertainty arising from economic development since the model can project annual damages up to 2100 it is common practice to calculate the net present value npv of those damages using a discount rate tol 2008 this estimate is referred to as discounted climate damages this rate is set at 3 as is common in other iams tol 2018 nordhaus 2017 therefore whenever we refer to discounted total δead we are in fact referring to the change in total ead in a particular area in the twenty first century between the years 2010 and 2100 discounted at 3 annually after the proposed model 6 is selected as the climrisk river model the next step is to introduce it into climrisk in the following manner see appendix a 1 magicc model is used to generate probabilistic estimates of global temperatures for the period 2010 2100 2 climrisk s pre defined downscaling factors for each cell are used to obtain grid cell level annual mean temperature and precipitation data 3 the downscaled magicc climate and sres iiasa socio economic data is fed to the climrisk river damage function to obtain grid cell level river flood damage estimates 4 the absolute expected river flood damage is added to the climrisk damage estimates making up the improved climrisk estimate of climate related damages in the following section the expected river flood damage estimates of the climrisk river are presented first followed by the total expected climate related damage from the integrated climrisk model 3 results in this section we present some key results from the climrisk river model first the model is validated using in sample and out of sample δead estimates from the original glofris model next the flood damage estimates produced by the climrisk river model are presented on an aggregate level for several large regions in the world here we also present some key country level results finally particularly interesting local damage estimates are presented to illustrate the relevance of estimating climate impacts at a spatially explicit level here we specifically focus on grid cells most affected by river flooding and identify cities that fall under these areas in the world and in europe 3 1 climrisk river model evaluation the performance of the climate change related river flood damage functions must first be evaluated before being introduced into the climrisk framework first we evaluate the adjusted r 2 value for each model across all grid cells to see how much of the variation in δead it can explain next we evaluate the in sample and out of sample forecasting performance of the model by computing the mean normalized root mean squared error nrmse for each cell 10 10 see appendix c 1 for the description of nrmse and appendix c 2 for more nrmse results the results in the main text focus on two protection standard assumptions baseheightstd and optimalstd which were selected because of the need to capture the most realistic range of adaptation policies focused on maintaining the current height of flood protection baseheightstd and moving towards protection standards that are economically optimal given socio economic development and climate change optimalstd the nostd assumption can lead to misleading flood risk predictions and is used solely to illustrate the importance of including flood protection standards in flood risk models baseprobstd is also set aside as it assumes that countries monitor the flood probability level and upgrade their standards to keep the flood probability constant a policy that many developing countries are currently not pursuing nevertheless the comparison of regression estimates for the nostd and baseprobstd case can be found in table 5 and are available in the climrisk river model since there are over 16 000 cells each with its own adjusted r 2 model estimate table 1 reports only the average adjusted r 2 values from table 1 it is clear that the model with the most explanatory power is model 6 which yields the highest average adjusted r 2 across all flood protection standard assumptions this is to be expected as model 6 contains more predictors and unlike the other models is able to capture the interaction between the climate regressors unlike the r 2 the adjusted r 2 penalizes the use of additional explanatory variables and is therefore comparable across different model specifications it is also important to note that the explanatory power increases by 0 08 on average between models 3 and 4 across all protection standard assumptions model 4 is the first proposed model to contain precipitation as an additional explanatory variable precipitation has often been ignored in integrated assessment models where the sole focus is placed on temperature change see table 5 in the appendix for the complete regression coefficients of the climrisk river model for all protection standard assumptions as we are evaluating a nested model with 6 functional forms of increasing number of predictors we will only focus here on comparisons between model 3 and model 6 the main reason for this is that model 3 represents a functional form similar to that of rice and climrisk models with quadratic function of temperature change this makes model 3 consistent and comparable to other iams unlike model 1 which ignores climate factors or models 2 4 and 5 which include them but either ignore the quadratic terms of temperature precipitation or the interaction between the two the model fit to glofris data is shown in fig 3 two main conclusions can be drawn from these figures first model 6 has a better fit to actual data than model 3 as is measured by the mean normalized rmse and adjusted r 2 the variance of the mean normalized adjusted r 2 is lower under model 6 whereas model 3 only accounts for gdp and first order temperature change model 6 also accounts for precipitation and the squared and interaction terms of temperature and precipitation second model 6 is able to predict future projected glofris estimates reasonably well with the majority of predictions not over or underestimating by more than 1 5 times the mean value of the projected glofris damage the spatial distribution of adjusted r 2 and nrmse in europe and south the spatial distribution of adjusted r 2 and nrmse is presented for europe and south east asia in fig 4 along with their respective histograms in fig 5 the two regions were selected for presentation as europe is expected to experience the lowest amount of flood damage and south east asia the highest south east asia is also among the least flood protected regions in the world whereas the opposite is true of europe it is apparent from fig 4 that in the northern region in india around the ganges river where damage estimates are among the highest in the world the model forecasts damage reasonably well with nrmse of around 0 5 the same is true of the north eastern part of china in the yangtze river basin in europe the adjusted r 2 is higher in central and western parts than in north eastern europe in the baltic region there is no clear spatial pattern with respect to nrmse from the histogram plots it is apparent that most adjusted r 2 values are greater than 0 5 and that most nrmse values are lower than 2 the model fit is good in europe with a mean of 0 65 0 61 and a variance 0 04 for the baseheightstd optimalstd in south east asia the fit is even better with a mean adjusted r 2 of 0 75 0 74 for the baseheightstd optimalstd and variance of 0 02 the out of sample out explanatory power of baseheightstd seems slightly higher than that of optimalstd in both regions this can be explained by the fact that protection standards play an important role in forecasting future flood related damage weakening the climate signal that feeds into climrisk river 11 11 the glofris modelling framework uses a global hydrology and water resource model at 5 x 5 resolution and downscaled to 30 x 30 resolution which simulates fluvial flooding based on a 2d volume river routing model this flood model shows the best results for europe and north america where the meteorological forcing is generally more accurate as a result of availability of station data and reanalysis products sutanudjajaet al 2018 this is in line with the projection accuracy of climrisk river which shows the highest values of r2 and the lowest rmse on average in europe and the us furthermore due to the strong seasonality in forcing and discharge monsoon dominated areas are also well simulated the least accurate results are obtained in african rivers due to groundwater dynamics snow dominated areas and continental eastern europe because of overestimations in groundwater recession constants sutanudjajaet al 2018 for readers who are interested in more information about the physical reasoning behind the glofris flood risk results and strengths and weaknesses we refer to the original gloris paper having performed various model validity checks we can now explore the regression results using the regression coefficients from the out of sample validation for different flood protection standard assumptions table 5 in the appendix illustrates coefficient distributions for each protection standard assumption for model 6 an important conclusion that can be drawn from these results is that protection standards play an important role in producing the annual expected damage estimates as the flood protection assumption is changed from nostd towards optimalstd the climate change signal temperature and precipitation weakens while the gdp signal remains strong but with a smaller effect therefore the key determinant of climate change related flood risk under the economically optimal protection standard is not the climate change but the standard itself and the total economic output increased economic output mostly leads to higher river flood damage across all protection standards with the median independent impact of a 1 billion increase in gdp leading to a 0 1 2 4 million increase in ead across all scenarios table 5 column 4 temperature increase is expected to increase flood damage but the median effect is zero across all cells for most protection standard assumptions the effect of precipitation is also in the direction that it leads to more river flood damage for most cells the median effect of a 1 increase in annual total precipitation on additional flood damage across all cells is around 0 02 million for the nostd and 0 million for optimalstd the mean effect of squared terms varies but overall indicates that many cells experience increasing damage to temperature and precipitation increase this property of the damage functions is important as it helps in capturing nonlinear responses of flood damage to higher temperature and precipitation nordhaus 1992 the interaction term is positive on average indicating that temperature and precipitation reinforce each other s impact on total damage in other words wet hot regions are subject to more severe flooding than dry hot regions 3 2 aggregate flood risk next we look at the development of climate change related flood risk on an aggregate level firstly on a regional and continental and then on a country level fig 6 shows the evolution of river flood risk on a global scale and in several regions united states europe africa south east asia and latin america the results do not vary significantly between different rcp scenarios in the first half of the century but start to diverge in the second for both adaptation assumptions the reduction in river flood risk is much greater with improved protection standards than with a stricter emission reduction policy regardless of the rcp scenario or region this is evident from the spread between the red and blue lines corresponding to baseheightstd and optimalstd under the baseheightstd on the global scale the expected river flood damage could reach 3 5 trillion in 2100 the evolution of climate related river flood damage is similar in europe with damage reaching 25 30 billion in 2100 south east asia and africa are expected to experience the bulk of the total global damage 1 5 2 5 trillion and 1 2 trillion respectively in 2100 when optimal adaptation is introduced the expected damage is reduced drastically on a global scale to around 250 billion in 2100 however the damage reduction is relatively lower in europe around 5 10 billion a 66 reduction when compared to south east asia around 200 billion 90 reduction and africa around 140 billion a 90 reduction the main reason for this drastic regional difference is the fact that the protection standards already in place in the developed world are close to or at the optimal level of protection standards introducing the optimal level of protection standards ensures that the river flood damage due to climate change does not exceed 30 and 10 of the business as usual potential damage in europe and south east asia africa respectively to put climate change related river flood risk into the perspective of total damage from climate change we compare the results of our model with the damage in climrisk 12 12 the plotted climrisk values do not account for climrisk river estimates i e before climrisk river integration fig 6 presents the evolution of climrisk and climrisk river for different regions in the world 13 13 more results for other protection standard assumptions can be found in appendix c fig 18 on a global scale top left the δead under the baseheightstd assumption is equivalent to 50 of the climrisk model damage projections for the rcp 2 6 ssp2 south east asia and africa are expected to experience a large increase in flood risk because adequate flood protection standards are lacking or non existent in many areas the opposite is true in europe and the united states where climrisk river projects river flood damage of much lower magnitude than the climrisk damage projections on an aggregate level the river flood damage under the baseheightstd assumption presents a relatively lesser threat to europe and the united states than to the less developed world of south east asia and africa another important conclusion that can be drawn from fig 6 pertains to the relative impact of climate mitigation under the two models namely the climrisk model is more sensitive to climate change mitigation than climrisk river as indicated by the relative difference in damage estimates under the rcp 2 6 and rcp 6 0 climate scenarios socio economic development plays an important role in future flood risk as it determines the number of exposed assets fig 7 illustrates flood risk estimates under different ssp scenarios and the rcp 2 6 climate scenario in all regions the ssp5 scenario of fossil fuel development would universally lead to the highest expected river flood damages the ssp3 and ssp4 scenarios both involving a relatively high level of climate adaptation compared to that of other ssps would lead to the lowest expected river flood damages in most regions 3 2 1 country level climrisk river in this section we present the climrisk river damage estimates on a country level the damage estimates are presented in terms of discounted δead relative to the discounted gdp for the period 2010 2100 fig 8 below presents a world map of country level damage estimates for the baseheightstd adaptation scenario table 2 shows the top 10 countries in the world 14 14 rounded to the nearest integer and in europe 15 15 rounded to 2 significant digits in terms of highest discounted total δead in the climrisk river model eight of the ten global highest are in south east asia and two in africa specifically the democratic republic of congo dr congo and egypt the discounted climate change related river flood damage in the top 10 most affected countries in the world is around 16 trillion which could be brought down to about 300 billion by implementing the optimal level of flood protection standards under the rcp 2 6 scenario in europe damages are much lower with a combined total of around 105 billion dollars of discounted total δead in the top 10 countries under the rcp 2 6 scenario and with 110 billion under rcp 6 0 this damage could be reduced by adopting an optimal flood protection policy bringing them down to around 25 billion under both rcp 2 6 and rcp 6 0 to conclude european countries stand to gain relatively less from additional adaptation than the most affected countries in the world because the adaptation standards already in place in many european countries are high please note that it is possible for the damage under rcp 2 6 to be higher than under rcp 6 0 scenario due to varying natural factors accounted for in glofris precipitation temperature evaporation etc some regions could become wetter or drier with increasing temperature or could switch between wet and dry periods throughout the twenty first century depending on varying natural factors this effect could ultimately determine the total damage due to river flooding 3 3 local flood risk in this section we analyze the local impacts of river flooding due to climate change this involves the estimation of the discounted value of river flood damage due to climate change at the grid cell level along with city level estimates fig 9 illustrates the effect that flood adaptation standards have on the expected flood damage due to climate change the selected scenario combination in this figure is rcp 6 0 ssp2 and the damages are expressed as a fraction of discounted gdp in europe the area around the rhine river basin would benefit from additional flood adaptation there are also clear benefits to river flood adaptation in eastern europe around the northern coast of the black sea ukraine the greatest benefits from optimal protection standards can be observed in south east asia and africa the northern region of india is expected to experience serious flooding due to climate change along the ganges river basin under the current level protection in china the yangtze river is also expected to cause significant damage to the built environment in africa the nile river poses a flood threat under the baseline protection standards central africa also stands to benefit greatly from river flood adaptation along the congo river basin as most of these areas currently have low or no river flood protection standards in the following subsection we focus on cells most severely affected by future flood risk and identify cities that fall within their boundaries 3 3 1 city level climrisk river river flooding represents a serious threat to the built environment especially in areas wherein flood protection standards are lacking and many assets are exposed most cities are built around rivers or other available bodies of water which makes them vulnerable to flooding and this is particularly the case in overpopulated cities that are less developed and lack flood protection although climrisk river does not produce city level estimates we identify in table 3 the cities in the world residing in cells that are expected to experience the most discounted total δead between 2010 and 2100 this is not a perfect measure of city level flood risk but helps us identify the cities located in most flood risk prone areas the first half of the table consists of city cells that are expected to experience the most discounted damage in the world here eight of the ten city cells belong to india and two in dr congo the reasons for this include relatively low flood protection standards as witnessed by the benefits of flood adaptation under the optimalstd assumption a high potential for flooding around the ganges river basin and an abundance of exposed assets the top 10 european city cells are expected to experience relatively less damage with a total discounted δead of around 80 45 billion under rcp 2 6 and the baseheightstd assumption still there are benefits from optimalstd that result in around 31 55 billion discounted δead 16 16 please note that this estimate is heavily influenced by the fact that the city of porto is facing a high flood risk under both baseheightstd and optimalstd meaning that there is insufficient space for flood adaptation improvement under rcp 6 0 the situation is similar with baseheightstd leading to 33 billion and optimalstd to 5 6 billion additional damage the bulk of this damage would be experienced around the danube river basin in eastern europe as was also evident from fig 9 we are interested in the difference in expected flood risk among city cells and express the damage projections as a fraction of gdp 17 discounted δead and gdp are both for the period 2010 2100 at 3 17 using this method we define the following four arbitrary risk categories 1 low risk city cells discounted δead of less than 5 discounted gdp 2 medium risk city cells discounted δead between 5 and 25 of discounted gdp 3 high risk city cells discounted δead between 25 and 100 of discounted gdp 4 very high risk city cells discounted δead of more than 100 discounted gdp fig 10 presents low risk city cells in particular those that could experience less than 5 gdp in discounted total δead most cities globally fall within this category the medium risk category is presented in fig 11 fewer city cells in the developed world belong in this category and most cells can be found in latin america africa and south east asia the high risk category is presented in fig 12 and isolates city cells almost exclusively in africa latin america and south east asia gyor in hungary is the cell in europe belonging to a high risk category fig 13 presents city cells which belong to a very high risk group of cells as expected the most affected cells could be found exclusively in central africa dr kongo and in latin america in areas with low river flood protection standards 4 discussion the climrisk river model was developed using the spatially explicit damage estimates and flood protection standards from the glofris and flopros models several model specifications were tested and the best performing statistical model model 6 was selected as the climrisk river model the process of selecting the most suitable model among different candidates was based on the measure of statistical fit to the glofris model data however the proposed model candidates were chosen with general climate and economic predictors in mind to remain intuitive and easy to use thereby avoiding the issue of overfitting and going beyond simple emulation several validation methods were applied to the newly estimated model we first looked at the spatial and frequency distributions of the adjusted r 2 statistics for all cells the model performs reasonably well on a global scale the mean adjusted r 2 of the model being 0 69 and 0 66 for the baseheightstd and optimalstd assumptions respectively we also measured the model forecasting performance through within sample and out of sample nrmse with an average out of sample 5 fold nrmse value of 0 69 and a variance of 0 17 across all cells we can confirm that the model performs reasonably well on previously unseen data this grants confidence to our approach to gain a better understanding of the magnitude of climrisk river damage estimates within the total climate change related damage the results are plotted against total climrisk damage estimates on a global scale under the baseheightstd flood protection scenario the climate related damage due to river flooding could exceed previously estimated total climate damage in climrisk 18 18 rcp 2 6 ssp2 fig 6 of 2 trillion in year 2100 however when the economically optimal flood protection policy is introduced the estimated damage drops down to just around 250 billion an estimated ten fold decrease in global river flood damage at the end of the century these results highlight the importance of accounting for adaptation in estimating the damage functions for iams however the divergent estimates under the different adaptation assumptions also reveal that adaptation policies introduce an important uncertainty in forecasting future flood risk there are also some limitations of the model which require attention firstly we are focusing on emulation of a complex river flood risk model and approximate the underlying physical and socio economic processes to gain on both model and computational simplicity which results in structural model uncertainties for instance flood damage development depends on the severity of flooding which is approximated with annual resolution climatic variables used in this study to emulate damage changes in weather extremes are not accounted for in the current work for the sake of ease of model implementation within the broader climrisk model moreover it should be realized that the flood damage functions are based on glofris results forced with gcm simulations rather than recorded climatic data in which annual trends may be less clearly aligned with occurrence of extremes climate economy iams inevitably require reduced form damage functions like we estimated for climrisk river and despite the limitations we have confidence in our approach given the overall good forecast performance of our model there are possibilities for improving the presented model in future work one point of improvement could be the function estimation method for example different model specifications could be used including non linear interpolation or even less orthodox non parametric methods splines bootstrapping etc however adding more degrees of freedom to the model should be done with care as this could lead to overfitting to the glofris estimates and complex damage functions both of which climrisk river attempts to avoid 5 conclusion in this paper we presented climrisk river a climate change related river flood damage assessment model that is integrated in a spatially explicit climate economy iam called climrisk the newly developed model operates on a local spatially explicit scale and can project future climate change related river flood damage estimates for various socioeconomic climate and flood adaptation scenarios the main idea behind the development of such a model was the need for generic damage functions that can be used with any climate and socioeconomic scenarios this means that the user will be able to select any climate and socioeconomic scenario combination including future to be developed versions of scenario projections as long as they have local temperature precipitation and gdp estimates to feed into the model two traits of the climrisk river model are unique in the existing literature on climate economy iams first its local nature allows the user to explore river flood damage projections for 0 5 0 5 cells anywhere in the world this is much more detailed than most climate economy iams that apply global damage functions or estimate the economic impacts of climate change for several world regions second its inclusion of current and future adaptation scenarios that account for local river flood protection standards in generating the damage estimates is unique in existing literature of iams few existing climate economy iams explicitly account for adaptation using policy relevant scenarios four main conclusions can be drawn from our analysis first our validation exercise shows that our reduced form flood damage functions estimated at the grid cell level perform reasonably well in capturing estimates produced by the more detailed flood damage model glofris second while iams have relied on temperature as a proxy for climate change in generating economic impacts we show that precipitation is a relevant additional explanatory variable in projecting changes in river flood damages third our aggregated results for several large regions show that the additional flood damages from climate change are non negligible compared to the total economic impacts estimated by climrisk although this depends on the flood adaptation scenario finally the spatially heterogeneous results that we presented for countries and cities reveal the importance of introducing local scale estimates like glofris into a more local iam like climrisk these findings demonstrate the advantages of the local nature of our model which allows the user to observe the spatial distribution of river flood damage that are otherwise lost through aggregation funding this research has received funding from the european union s horizon 2020 research and innovation programme under grant agreement no 776479 philip j ward received additional funding from the netherlands organisation for scientific research nwo through vidi grant 016 161 324 the authors also acknowledge the financial support received from the united nations development programme méxico the national institute of ecology and climate change grant ic 2017 068 as part of mexico s sixth national communication to the united nations framework convention on climate change declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements observed temperature and precipitation data used for training the climrisk river model was kindly provided by edwin sutanudjaja of utrecht university appendix h supplementary data the following is the supplementary data related to this article multimedia component 1 multimedia component 1 appendix h supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2020 104784 appendix a climrisk model the flood risk model climrisk river climrisk river developed in this paper is introduced into the climrisk model that operates on a local 0 5 0 5 or larger scale estrada and botzen 2018 climrisk is a global model that assesses the dynamic economic impacts of climate change at the local scale 0 5 x 0 5 for various socioeconomic and climate change projections this is done by combining local gdp exposure information grübleret al 2007 with climate projections obtained using the magicc climate model and scaling patterns from the climate models included in the cmip5 lynch et al 2017 kravitz et al 2017 regional impact functions are taken from the rice model and encompass a broad range of economic sectors nordhaus 2017 estrada et al 2017 the result is a global dynamic integrated assessment model that projects climate damages on a local regional and global scale the climrisk model is composed of the following interlinked components 1 hazard climate change projections 2 exposure economic projections 3 vulnerability impact functions these modules build upon and integrate well established models datasets techniques and procedures that have been previously evaluated and published as will be explained next 1hazard climate projections climrisk combines probabilistically generated global temperatures with the regional precipitation and temperature scaling patterns from 41 general circulation models gcm in the cmip5 database lynch et al 2017 kravitz et al 2017 for the global and regional probabilistic estimates of climate projections climrisk uses the magicc version 6 software magicc represents a reduced complexity model of climate change and is widely used in the research community to project future climate impacts meinshausen et al 2011 meinshausen et al 2011 van vuuren and carter 2014 barker 2007 it also relies on the technique of pattern scaling to produce local estimates of temperature and precipitation change the magicc projections used in climrisk are the difference in annual mean temperatures in degrees celsius and precipitation in with respect to 1900 the spatial patterns of different gcms are randomly selected using a uniform distribution weigel et al 2010 knutti 2010 climrisk imposes a triangular probability distribution for the climate sensitivity parameter this particular distribution is centered around a lower limit of 1 5 c an upper limit of 4 5 c and a mean value of 3 c for climate sensitivity stockeret al 2013 the triangular distribution of temperature realizations is a measure of climate sensitivity in the climrisk model by representing it using a probability distribution the iam is able to produce temperature rise estimates that encompass the likely ranges of climate change as precipitation realizations directly depend on temperature realizations in magicc the triangular distribution covers both climate variables in the model for the purposes of this paper only the median 50th percentile realizations are used for all scenario combinations 2exposure economic projections scenarios of gdp determine the economic exposure to the climate change hazard a 1 which is an important input in the damage functions for the climate impacts calculation a 3 the economic projection data used in climrisk and climrisk river relies on the gdp and population projections of the ssp public database version 1 1 19 19 https tntcat iiasa ac at sspdb in order to construct spatially explicit information climrisk combines the shared socio economic pathways ssp and special report on emissions scenarios sres projections available through the ggi scenario database version 2 0 1 http www iiasa ac at research ggi db raoet al 2008 riahiet al 2017 grübleret al 2007 these databases are commonly used in climate impact studies and gather a variety of scenarios about gdp and population that have been produced by different modeling groups this procedure of combining ssp and sres data can be done due to the fact that certain sres and ssp scenarios are consistent with each other riahiet al 2017 for example a1 or b1 and the ssp1 ssp5 narratives represent a similar pattern of socio economic development the a2 scenario would make ssp3 and ssp4 scenarios plausible while the b2 sres is consistent with ssp2 van vuuren and carter 2014 the pattern scaling method used in climrisk is similar to that in other general circulation models kravitz et al 2017 tebaldi and arblaster 2014 the grid cell i s pattern spatial pattern labelled as p i t represents the proportion of gdp or population in cell i within region r to the regional total across all cells i 6 p i t y i t s r e s t 1 i y i t s r e s where cell i belongs to a particular region i r y i r t r is derived from sres and for every sres storyline these patterns are not assumed to be constant over time and are updated every ten years period the regional total of the particular ssp scenario can then be scaled by the sres consistent regional p i r t pattern to obtain i s grid cell level estimate 7 y i t s s p p i t y r t s s p by construction the downscaled scenario projections match exactly the quantifications of the ssp narratives produced by different modeling groups at the aggregated 13 world regions and the global totals climrisk attempts to account for socioeconomic uncertainty by including all the ssp narratives ssp1 ssp2 ssp3 ssp4 ssp5 in addition uncertainty in gdp quantification is tackled through the use of economic data provided by three different modelling groups oecd env growth iiasa pik crespo cuaresma 2017 dellink et al 2017 leimbach et al 2017 the iiasa scenarios were selected for the use in climrisk river because they correspond to the socio economic projections used in the glofris model making the model validation easier however using any of the above mentioned sources is possible when generating river flood damage projections 3vulnerability impact functions climrisk relies on the regional damage functions improved estimates of the rice model nordhaus 2017 these regional damage functions encompass a broad range of economic impacts following the rice model the impact functions take into account the losses suffered by major economic sectors such as agriculture but also the cost of sea level rise adverse impacts on health non market damages and catastrophic damages nordhaus 2014 the damage function in rice is as follows 8 d r t r i c e α r t t β r t t 2 where α r and β r represent the regional temperature coefficients that measure the impact of global mean annual temperature increase on climate related regional economic damage d r t rice measured in billions of dollars 2015 ppp table 4 presents the climrisk rice regions along with their respective damage coefficients based on nordhaus 2017 the damage function defined above lets us derive the rice regional climate impacts 9 i r t r i c e y r t d r t r i c e where y r t is the annual regional gdp in region r at time t when using pattern down scaled inputs as defined in a 2 the regional damage function can be converted into a local 10 d i t c l i m r i s k α r t i t β r t i t 2 where as previously defined α r and β r represent the regional temperature coefficients that measure the impact of local mean annual temperature increase t i t on climate related local economic damage d i t c l i m r i s k in cell i the total economic impact of climate change in year t in cell i is therefore 11 i i t y i t d i t c l i m r i s k s r t r i c e where y i t represents the projected output for cell in year t as defined in equation 7 and s r t represents the scaling factor that ensures that projected damages are exactly the same regardless of whether the regional damage functions are driven by global or grid cell temperatures the scaling factor s r t is defined as 12 s r t r i c e i r t r i c e i i i i t where i r t r i c e represents the rice regional impacts and cell i belongs to particular region i r consistent with the rice model that assumes the same damage function can be applied across areas within a region across ssp scenarios and over time we apply the same rice damage function to grid cells within a region this assumption may be seen as restrictive which is why climrisk river introduces damage functions for river flood risk that vary between grid cells see section 2 in addition to the local impacts based on the rice damage functions climrisk also introduces urban damages into the model given that the rice model relies on regional mean surface air temperatures it is not possible to account for the effect of urbanization on local temperatures in cities using the rice model an effect commonly known as urban heat island uhi however the local nature of climrisk makes the modeling of uhi possible namely urban areas 1 million inhabitants are likely to experience higher damage due to climate change than non urban areas as cities account for about 80 of global gdp and about 50 of global population re 2004 dobbs et al 2011 in addition urban areas are expected to experience higher local temperatures due to the urbanization process for example the replacement of natural surfaces with structures of higher thermal capacity concrete asphalt etc leads to local climate change effects estrada et al 2017 since the grid cell scale is still likely to cover an area larger than a particular urban city the total output in an urban cell is divided between the non urban 20 and urban 80 part of the cell the damage function for the non urban part of a particular urban cell i follows a recently used damage function for urban areas in an iam for cities estrada et al 2017 13 d i t t u 0 9 t i t u i t 2 5 2 where t i t and u i t represent changes in annual temperature in cell i due to global and local climate change respectively the combined damage from the non urban and urban part of the urban cell represents the total expected damage for cell i in a given year t the local temperature increase u i t in equation 13 is commonly estimated in the following way 14 u i t a p o p i t where p o p i t represents the urban population of cell i at time t and parameters a and b are commonly used in literature for estimating the urban temperature increase due to urbanization oke 1973 karl et al 1988 mills 2014 appendix b regressions the coefficients of regressions for all cells in model 6 are presented in figure 14 15 16 and 17 for various protection standard assumptions in table 5 the distribution of different coefficients is presented for model 6 the chosen climrisk river model for presentation purposes the coefficients in this table are converted to yield damages in millions of usd instead of billions as presented in the paper from the figures we can observe that the effect of an increase in economic output gdp leads to increased exposure and higher river flood damages the effect is the strongest with no existing flood protection standards median 2 3 million and drops significantly when optimal standards are employed median 0 1 million for every 1 billion increase in gdp we can also observe that the effect of precipitation is stronger with no existing standards median 1 1 million than in other cases median 0 1million for a 1 point increase in precipitation the effect of temperature remains ambiguous with median value of 0 across all flood protection assumptions fig 14 model 6 coefficient distribution nostd fig 14 fig 15 model 6 coefficient distribution baseheightstd fig 15 fig 16 model 6 coefficient distribution baseprobstd fig 16 fig 17 model 6 coefficient distribution optimalstd fig 17 table 5 regression results table for climrisk river the coefficients refer to impacts on δead in millions of dollars 2005 ppp the effects of temperature and precipitation change grow stronger as the protection standard assumption moves from nostd towards optimalstd however the effect of δgdp on total damage decreases dramatically with improved protection standards indicating the reduction in river flood vulnerability table 5 coefficient p value variable mean 2 5 50 97 5 mean 2 5 50 97 5 nostd β gdp 80 84 21 65 2 34 345 61 0 002 0 0 0 β ᴛ 31 62 272 64 0 08 76 51 0 007 0 0 0 001 β ᴛ 2 7 95 9 16 0 59 76 0 010 5 0 0 0 038 β p 1 57 5 89 0 02 19 99 0 011 0 0 0 052 β p 2 0 04 0 46 0 0 8 0 012 0 0 0 098 β p t 0 9 3 13 0 9 84 0 011 0 0 0 069 nrmse out 1 44 0 0 0 4 observations cells 16503 adj r 2 in 0 83 0 53 0 86 0 97 baseheightstd β gdp 92 02 7 61 0 57 201 75 0 003 0 0 0 β ᴛ 29 82 198 53 0 14 55 0 009 0 0 0 027 β ᴛ 2 6 62 2 26 0 35 58 0 012 0 0 0 079 β p 0 02 6 66 0 6 19 0 012 0 0 0 091 β p 2 0 05 0 22 0 0 56 0 011 0 0 0 057 β p t 1 0 89 0 8 89 0 010 0 0 0 045 nrmse out 4 5 0 0 0 64 observations cells 16503 adj r 2 in 0 7 0 25 0 75 0 91 baseprobstd β gdp 96 43 7 23 0 53 158 39 0 002 0 0 0 β ᴛ 16 22 117 83 0 14 16 0 007 0 0 0 002 β ᴛ 2 3 64 1 75 0 23 36 0 010 0 0 0 045 β p 0 1 2 14 0 3 08 0 013 0 0 0 119 β p 2 0 01 0 1 0 0 17 0 014 0 0 0 158 β p t 0 44 0 44 0 3 19 0 014 0 0 0 139 nrmse out 3 34 0 0 0 43 observations cells 16502 adj r 2 in 0 81 0 36 0 85 0 98 optimalstd β gdp 131 87 5 73 0 1 42 19 0 004 0 0 0 β ᴛ 2 2 3 49 0 4 57 0 009 0 0 0 018 β ᴛ 2 0 63 0 45 0 1 43 0 012 0 0 0 084 β p 0 02 0 43 0 0 59 0 014 0 0 0 141 β p 2 0 01 0 03 0 0 04 0 013 0 0 0 136 β p t 0 12 0 14 0 0 39 0 013 0 0 0 106 nrmse out 4 62 0 0 0 67 observations cells 16503 adj r 2 in 0 66 0 2 0 7 0 91 appendix c baseprobstd and nostd results fig 18 annual δead in climrisk river under different protection standards rcp 2 6 and ssp2 scenario combinations fig 18 appendix d model forecasting performance nrmse introduction to evaluate the forecasting performance of the climrisk river model we use a statistical method of normalized root mean squared error nrmse first we compute the within sample nrmse by normalizing the root mean squared error rmse eq 15 with the mean value of the observed response variable eq 16 to compute the out of sample nrmse statistic for each cell we run a five fold stratified cross validation check where we split the data into four stratified training sets and one test set each subset containing points from a single esm next we evaluate the out of sample rmse on the test fold we repeat the process with a different training test split set until every fold has been used in testing exactly once finally the mean rmse value across all five folds is taken and is normalized using the mean value of observed response variable eq 16 to yield the nrmse eq 17 15 r m s e n 1 n δ e a d g f δ e a d c l r v 2 n 16 δ e a d g f n 1 n δ e a d g f n n 17 n r m s e r m s e δ e a d g f where δ e a d g f is the mean value of all the δ e a d g f observations used to fit the damage function for a particular cell the reason for this normalization is that it gives a more intuitive interpretation of the rmse by indicating how large the forecasting error is relative to the mean δ e a d g f observed in glofris nrmse results in this section we compare the performance of models 3 and 6 as potential candidates for climrisk river fig 19 presents the out of sample predictions 20 20 the out of sample predictions labelled as out refer to stratified 5 fold cross validated rmse estimates normalized using the mean value of the response variable for the two models of interest for the estimates of the remaining models please refer to fig 20 and 21 fig 19 forecasting performance of models 3 and 6 the climrisk river model 6 right panel has a better forecasting performance than model 3 left panel as measured by out of sample nrmse for presented regions fig 19 fig 20 out of sample nrmse for alternative model candidates fig 20 fig 21 out of sample adjusted r2 for alternative model candidates fig 21 appendix e computational performance considerations due to the large amount of data required for processing in climrisk various improvements to performance of the model have been made the gridded nature of both the flood and climate damage data is suitable for parallel processing two api s for parallel processing have been used in the model multiprocessing library in python which is able to get around the global interpreter lock gil allowing computations to be split into many processes which can be run concurrently utilizing the full potential of a multi core system pycuda library in python which utilizes the potential of a graphical processing unit gpu present in the system the cellular architecture of a gpu makes solving relatively simple operations over a large grid convenient and fast please note that an nvidia gpu with cuda support is required 
25992,this study aims to improve the estimates of the economic impacts of climate change by developing a river flood risk model climrisk river and introducing it into an existing climate economy integrated assessment model iam it operates on a local scale and can project climate change related river flood damage for various socioeconomic climate and flood adaptation scenarios whereas other iams have relied on temperature as a climate change proxy we show that precipitation is a key variable in projecting river flood damage the way adaptation is accounted for in our flood damage functions has a large influence on the results highlighting the relevance of modelling local level adaptation in iams results presented at different spatial scales demonstrate the relevance of river flood damage functions for estimating the economic impacts of climate change and allows for exploration of the spatial distribution of impacts through local estimates keywords integrated assessment modelling climate change river flood risk river flood adaptation 1 introduction integrated assessment models iams of climate and the economy are commonly used to project the future economic impacts of climate change while some iams estimate the climate impacts on specific physical systems eg energy land use bosetti et al 2007 bouwman et al 2006 raoet al 2008 others paint a more general picture of climate impacts on the economy stern 2008 nordhaus 2013 anthoff and tol 2014 the latter set usually provides monetized climate impacts through climate damage functions and or estimates of benefits of climate policy aimed at reducing the greenhouse gas emissions such models estimate the complex relationship between the anthropogenic and climate system using transparent and simplified reduced form functions since the original version of the dice model nordhaus 1992 one of the first iams of climate change and the economy environmental economists have made several attempts to improve these models to aid policymakers in making decisions about climate policy in the face of climate change uncertainty tol 2018 nevertheless climate economy iams have received various criticisms including the aggregated spatial dimension of the models farmer et al 2015 the incomplete representation of climate change risks stern 2013 van den bergh and botzen 2014 and the fact that the damage functions that translate global warming into economic impacts are outdated and require an improvement diaz and moore 2017 in this respect it should be noted that in most cases these iams work on an aggregated spatial scale meaning that there is either one region the earth or several larger regions for example the well known rice model which is the regional version of the global dice model estimates the economic impacts from climate change for 12 world regions nordhaus and yang 1996 nordhaus 2017 global information about the projected economic impacts of climate change is increasingly available at a refined spatial resolution for specific impact categories which can serve as a basis for updating the iam impact functions if this improves iam estimates of the economic costs of climate change the effects of extreme weather and natural hazards on the economy due to climate change have been included in climate economy iams to a very limited extent and estimates from catastrophe models of how natural disaster risks are expected to develop under climate change have become increasingly available botzen et al 2019 for instance river flooding is an important damage category that must be better represented in iams because it poses a significant economic impact and sophisticated global modelling approaches to estimate river flood risk at a detailed spatial resolution are increasingly available wardet al 2015 about 50 trillion 1 1 all units are in us dollars and 0 8 billion people are subjected to a 1 in 100 years river flood event kundzewicz et al 2013 and the direct economic losses from river floods between 1980 and 2013 exceeded 1 trillion and caused a loss of more than 220 000 lives munich et al 2014 the river flood risk is expected to increase in many regions in the world wardet al 2013 and it is important to take this into account in economic iams of climate change the main goal of this paper is to improve the local estimates of the economic impacts of climate change by developing climrisk river a spatially explicit model of river flood risk that is introduced into a broader climate economy iam climrisk there are four main reasons why climrisk is the preferred iam for this study first other iams including rice do not provide precipitation projections which we find to be an important explanatory variable of future changes in flood damages second with climrisk we can model spatial heterogeneity in climate change impacts the results of climrisk river show that spatial heterogeneity in projected flood damages is substantial and modelling this provides insights into which areas face a high river flood risk third climrisk allows us to model climate projections probabilistically which gives insights into uncertainty of climate change impacts fourth with climrisk we are flexible with exploring how climate change impacts develop under many different scenarios with limited computing time climrisk river also accounts for local human adaptation through the use of recently developed local flood protection standard database flopros scussoliniet al 2016 accounting for local flood adaptation in estimating the river flood damage functions for the iam is important as flood protection standards that reduce the probability of a river flooding already exist in many areas in the world governments are likely to update these standards if flood risk increases as a result of climate change most studies that use iams such as dice account for adaptation implicitly through the reduction in the climate damage used in estimating the damage function nordhaus 2017 we follow this approach by estimating the river flood damage functions implicitly using flood damage estimates that account for different flood protection standard scenarios these scenarios range from maintaining current flood protection standards to implementing economically optimal protection given a climate and socio economic scenario through creating unique flood damage functions under each flood protection scenario we contribute to the small body of literature that has made adaptation explicit in damage functions in climate economy iams de bruin et al 2009 hope 2011 dumas and ha duong 2013 our approach of accounting for river flood risk in a climate economy iam is currently the closest to the work of kuik 2017 in introducing the river flood damage functions in the climate framework for uncertainty negotiation and distribution fund model a climate economy iam anthoff and tol 2014 the flood damage functions in fund generate estimates for 16 regions as a product of a regional damage coefficient regional temperature and gross domestic product gdp moreover the functions make use of ad hoc medium and high adaptation scenarios from jongmanet al 2015 however with gridded flood damage and flood protection standard data available it is now possible to produce accurate local estimates of expected annual flood damage climrisk river flood damage functions are spatially more detailed more complete as they account for wet and dry regions by means of gridded precipitation projections as an additional explanatory variable and have a more realistic representation of adaptation policy the reason for the latter is that flood adaptation scenarios are not implicitly assumed but are modelled using data on existing and future economically optimal flood protection standards scussoliniet al 2016 wardet al 2017 the end product of climrisk river is a set of flood damage function estimates which are introduced into a general framework of climrisk which allows the user to explore climate change related damage including the expected damage due to river flooding the model works for any user defined climate change and socioeconomic scenario combination presented in the ipcc 5th assessment report the model can be used by policymakers interested in expected future changes in flood risk and implementing a flood adaptation policy in a particular geographic area for this audience climrisk river can serve as a preliminary quick scan that gives insights into how flood risk is expected to develop under future scenarios and the effectiveness of flood protection infrastructure to limit this risk which can be a motivation for conducting additional higher resolution studies to identify appropriate local flood risk management measures other users are members of the academic community who are interested in mapping river flood hotspots under different future scenarios to motivate further local scale flood adaptation or climate hazard research moreover the integration of flood risk in a general climate economy iam is likely to appeal to the broader community of researchers and policymakers who are interested in understanding global regional or local economic impacts from climate change 2 methodology and data in this section we present the methodology used to develop climrisk river this section follows a downstream flow that resembles the flow of methods presented in the flowchart fig 1 firstly we outline the ingredients required for developing the new river flood damage functions that emulate glofris subsection 2 1 next we develop 6 regression models and evaluate their performance based on in sample and out of sample glofris data subsection 2 2 once the most suitable model model 6 is selected it is introduced into the climrisk model by feeding it downscaled climate and socioeconomic data from a spatially explicit iam climrisk explained in appendix subsection a finally the results of climrisk river and the resulting total climate damage of climrisk after integration are presented in section 3 2 1 flood risk ingredients the main goal in developing climrisk river is to produce validated river flood damage functions for different protection standard assumptions that can be fed with temperature precipitation and gdp estimates from any climate or socioeconomic scenario combination the input for the flood risk emulator climrisk river is based on the glofris model a global framework for flood risk assessment that works on a detailed spatial scale 30 30 and includes all main river basins worldwide winsemius et al 2012 the main ingredients of the new flood damage functions for climrisk river are as follows risk expected annual damage ead vulnerability flood protection standards hazard climate projections exposure economic projections each of the above listed ingredients are explained in more detail in the following sections 2 1 1 risk expected annual damage ead formally flood risk can be defined as the expected annual damage ead or the damage of a hazardous event weighted by the probability of its occurrence over a smooth probability curve it can be written as 1 e a d 0 1 p d p d p where ead represents the ead of the particular cell d p is the damage in that cell caused by a flooding event of probability p which is related to a flooding with return period r p in the following way 2 r p 1 p the ead estimates are made using current and future flood hazard layers built up area gdp estimates country level maximum damage estimates and depth damage curves based on the occupancy type ward et al 2017 tiggeloven et al 2020 winsemius et al 2016 to estimate the flood damage functions for climrisk river using glofris data we must retrace the steps of how the data were created in the first place in glofris flood damage is estimated for flood return periods of 2 5 10 25 50 100 250 500 and 1000 years for illustration the damage of a 2 year return period represents the loss that would be caused by relatively small floods with annual exceedance probability of 0 5 the resulting flood probabilities were calculated using equation 2 and a smooth probability curve was obtained for the purpose of ead estimation between the above mentioned points the ead in climrisk river for any particular cell can be calculated using the trapezoidal rule for approximating definite integrals 3 e a d g f i 1 10000 d g f x p 1 d g f x p 2 δ x p where ead gf is the approximation of ead eq 1 in the glofris gf model x 0 0 0001 1 is a vector of equally spaced 10 000 points and δx p δx is the length of equal spacing these loss data are estimated in glofris for current conditions but also for future climate and socioeconomic scenarios and are available for different time periods centered around three years 2030 2050 and 2080 the glofris modelling cascade uses forcing data from eu watch weedonet al 2011 over the period of 1960 1999 to force the hydrological model pcr globwb sutanudjajaet al 2018 which is used for the flood inundation modelling as baseline conditions as baseline for the socioeconomic data glofris uses the hyde database which consists of gridded percentages of built up area population and gdp projections as a starting point damage estimates for rivers of different return periods for the baseline period are collected and these do not depend on different emissions or socioeconomic scenarios hence the baseline represents current climate and socioeconomic conditions the future projections of flood damage however are available for various climate 2 2 climate scenarios rcp2 6 rcp4 5 rcp6 0 rcp8 5 and socioeconomic scenarios 3 3 socioeconomic scenarios ssp1 ssp2 ssp3 ssp4 ssp5 finally the data are available for five different earth system model esms 4 4 esms used hadgem2 es gfdl esm2m noresm1 m ipsl cm5a lr and mirocesm chem simulations that were used to force the glofris model projections for each rcp scenario see fig 2 each of the above mentioned combinations of climate and socioeconomic scenarios for different esms in different time periods produces a single observation of ead for each cell therefore each cell ideally contains 305 observations of ead with no missing data 5 5 the number 305 comes from the fact that there are 5esms 5ssps 4rcps 3years 300 plus 5 additional baseline period observations for each esm by pooling estimates from different esms in our damage function estimation we are effectively averaging across all five available esms and making our emulator esm independent with respect to climate input the user would only need to specify the rcp scenario and global average temperature percentile realization through magicc appendix a 1 which would naturally correspond to a particular esm all the ead estimates are expressed in billions of us dollars 2005 ppp and are available for 30 x30 cells to integrate climrisk river into the climrisk model all the glofris river flood data must be upscaled to 0 5 0 5 by aggregating the impacts over the 30 30 cells prior to integration with climrisk river the emulated damage estimates were translated into us dollars 2010 ppp to match the climrisk monetary impact estimates when providing the total climate change damages 2 1 2 vulnerability flood protection standards to obtain more reliable estimates of annual expected river flood damage the current flood protection standards must be taken into account the flood protection standards enter the flood risk model through the ead function whereby damages of rivers with return periods lower than the available protection standards are assumed to be zero for example if an area is protected against a 500 year return period flood this means that the sum in equation 3 only contains damage estimates of rivers with return periods greater than 500 years a comprehensive global database flopros of observed and modelled current river protection standards has recently been compiled scussoliniet al 2016 the flood protection data are available at the state level and any given cell receives protection equal to its estimated state protection level the river protection data are currently available for 2683 states in the world in the form of river return periods against which the state is protected however in estimating the annual expected flood damage in the future we also need estimates of future flood protection standards that depend on adaptation decisions about flood protection investments flopros data consists of two options for future protection standards 6 6 the names assigned to the protection standards in this paper are arbitrary and do not correspond to the names given by the original authors of the flood protection standards baseline height standards baseheightstd which assume that the protection infrastructure is maintained at the baseline year height in the future and allow the river flood risk to vary over the course of the century this scenario does not imply any additional river flood adaptation baseline probability standards baseprobstd which assume that the protection standards are updated so as to keep the baseline flood probability constant this scenario does imply additional river flood adaptation as the flood protection standards are upgraded according to the varying natural factors in order to maintain constant flood probability regardless of the constant flood probability the flood related damage could still vary with the amount of exposed assets and the severity of flooding taking this research a step forward through cost benefit analysis policymakers desired economic decision making has been taken into account when designing the optimal level of river flood protection kuik 2017 optimal standards optimalstd which assume that all states behave in an economically optimal manner and invest today in the level of protection that would yield the highest net present value npv over the twenty first century in addition to the three adaptation scenarios a fourth scenario no standards nostd will be used for comparison with the other assumptions as suggested by the name this scenario assumes that no cells are protected against any potential river floods thereby ignoring existing protection standards as modelled in flopros this is true for both the baseline year estimates and future projections although this is not a very realistic assumption it helps us understand how important flood protection standards are in estimating the flood damages in an iam the data for optimal protection standards is not available for some cells due to for example missing future projections of socioeconomic data in such cases we assume that the protection is maintained at the baseheightstd level if available 2 1 3 hazard climate change projections as in many climate iams annual surface air temperature is among the primary climate variables of interest and serves as the main proxy for climate change we are also interested in the effect of annual precipitation on the river flood risk precipitation has until now not been introduced into an iam damage function and it is important to assess its impact on river flood risk in light of newly available local precipitation data although floods are generally short lived events that are mainly driven by extremes in precipitation we follow the common approach in iams to use annual climate indicators in our case annual temperature and precipitation as a proxy of climate change this is in line with the purpose of climrisk river to project changes in long term annual expected flood damage in response to long term trends in economic exposure and climate change which as our results show can be approximated by changes in annual temperature and precipitation section 3 1 whereas the climate forcing data in climrisk is generated through magicc with the use of pattern scaling see appendix a 1 the forcing data in glofris contains daily gridded estimates of surface temperature and precipitation these are generated using different esms interpolated to 0 5 0 5 and they are bias corrected using observations from 1960 to 1999 for the eu watch project these same estimates are then used to force the pcr globwb global water and hydrological model sutanudjajaet al 2018 the eu eu watch forcing observations are also used to generate the baseline flood risk the reason why 2010 is not used as the baseline period for the climate forcing data in glofris is the fact that the authors used a 40 year interval around the year of interest in extreme value analysis as observed data is not available for 1990 2030 the authors assume that global hydrological processes did not drastically change between 1980 and 2010 after accounting for climate therefore the eu watch baseline period estimates are used in the year 2010 as the baseline period to create the climate input variables necessary for the river flood damage functions daily temperatures were converted to annual mean temperature and daily precipitation to total annual precipitation since the glofris model produces estimates for three periods centered around years 2030 2050 and 2080 mean annual surface temperature and total annual precipitation estimates were averaged over the years 2010 2050 2030 2070 and 2060 2100 finally differences of precipitation and temperature with respect to the baseline period average climate were taken in order to fit the magicc climate projection units for temperature the difference is expressed in absolute value of degrees celsius while for precipitation the percentage change of precipitation with respect to baseline period is required 2 1 4 exposure economic projections exposure is another important determinant of flood damage because it captures the extent of assets that are prone to flooding gdp ppp was extracted from the ssp database with the oecd env growth gdp projections the gdp data used to estimate the flood damages in glofris are derived from the iiasa ssp database riahiet al 2017 next the future development of urban areas is estimated using the 2up model van huijstee et al 2018 in this model urban and rural populations are distributed according to a map of urban areas which helps in determining the exposure of a certain area in climrisk river we proxy the glofris exposure data using iiasa projections for various ssp scenarios combined with the compatible sres scenarios to create a spatially explicit 0 5 0 5 grid of local gdp estimates more information about exposure ssp and sres data can be found in appendix a 2 2 2 climrisk river an important step in climrisk river development is the formulation of the flood damage functions the main decisions involved in this step are geographic scale of model parameters choice of explanatory variables the geographic scale of model parameters refers to the geographic area that a particular damage function coefficient covers depending on input data availability the scale can range between highly local 30 30 grid cells and global since the scale of downscaled inputs in our iam is somewhere in between the primary candidates for the model scale in climrisk river are the 0 5 0 5 grid cell and river basin level the main advantage of river basin level functions is simplicity each cell within a river basin would inherit the set of parameters corresponding to the basin the main disadvantage however is the loss of model fit as heterogeneous grid cell data are aggregated into a single function over a potentially large basin area the opposite is true of a grid cell level function as we prefer the higher explanatory power of the grid cell level functions over the smaller total number of function estimates we set the scale to grid cell level to make full use of local input data the scale of results is still entirely up to the user who can explore various scenario combinations at different levels of aggregation the next step concerns the choice of the dependent and explanatory variables the main goal of the regressions that follow is to project the δead t that is the change in ead with respect to the baseline period for the available data t represents a time period mid point for which the glofris expected annual damage estimates are made namely t 2010 2030 2050 2080 in order to find the most suitable statistical model for climrisk river we take a nested model approach and evaluate several functional forms with increasing number of terms the following nested model consisting of six functional forms is evaluated model 1 δ e a d g f t β 1 δ g d p t model 3 δ e a d g f t β 1 δ g d p t β 2 δ t t where δ t t represents the absolute change in annual mean surface air temperature in year t 7 7 all changes in temperature and precipitation are with respect to 1980 observed climatology from eu watch weedonet al 2011 model 3 δ e a d g f t β 1 δ g d p t β 2 δ t t β 3 δ t t 2 model 4 δ e a d g f t β 1 δ g d p t β 2 δ t t β 3 δ t t 2 β 4 δ p t where δ p t represents the percentage point change in precipitation in year t model 5 δ e a d g f t β 1 δ g d p t β 2 δ t t β 3 δ t t 2 β 4 δ p t β 5 δ p t 2 model 6 δ e a d g f t β 1 δ g d p t β 2 δ t t β 3 δ t t 2 β 4 δ p t β 5 δ p t 2 β 6 δ t t δ p t where δ t t δ p t represents the interaction term between temperature and precipitation change after evaluating the six different emulator models model 6 proved to have the best predictive power across different rcp ssp scenario combinations in other words model 6 presents the best emulator of our chosen river flood risk model glofris and is chosen as the climrisk river model it is defined as 4 δ e a d g f t β 1 δ g d p t β 2 δ t t β 3 δ t t 2 β 4 δ p t β 5 δ p t 2 β 6 δ t t δ p t where β 1 is the effect of a 1 billion increase in gdp 8 8 all the listed effects refer the change in expected flood damage δd t δgdp t is the difference in gdp between year t and 2010 β 2 is the effect of a 1 c increase in surface air temperature δt t represents the change in mean surface air temperature at t β 3 is the squared term of surface air temperature δ t t 2 is the change in squared surface air temperature at t β 4 is the effect of a 1 increase in total annual precipitation δp t is the percentage point change total annual precipitation at t β 5 is the squared term of total annual precipitation δp t 2 is the percentage point change in total squared annual precipitation at t β 6 is the effect of a 1 increase in total annual precipitation conditional on a 1 c increase in surface air temperature and δ t t δ p t is the interaction term between the change in mean surface air temperature and the percentage point change in mean total annual precipitation at t this model was selected so as to take advantage of the explanatory power of precipitation when estimating δead precipitation could capture the effect that wetter or drier regions could have on the frequency of flooding in addition the interaction between temperature and precipitation could capture the interaction between for example hotter and wetter regions both of which could lead to an increase in flood risk higher than estimated by temperature and precipitation alone this functional form is similar to the functional form of the rice damage function where quadratic climatic terms are used to capture nonlinear effects on damage of high temperature and in our case also precipitation that is excluded in rice nordhaus 2014 there is no constant term in the regression as the function passes through the origin and the left and right hand side terms are zero in the baseline year while the gdp estimate in rice is multiplied by the impact function eq 11 the δ g d p t in this model is an explanatory variable the specific units for temperature and precipitation explained above were selected so as to match the magicc model output appendix a 1 magicc generates differences in annual surface temperature in degrees celsius and percentage difference in annual precipitation with respect to any particular base year since the precipitation data used for the fitting was originally expressed in k g m 2 s a unit conversion is necessary to obtain the percentage difference 5 δ p t 100 p t p 0 p 0 where δp t is the percentage change in total annual precipitation in year t p t is the total annual precipitation estimate at time t and p 0 is the baseline total annual precipitation estimate due to the large number of possible scenario combinations that can be fed to the climrisk river model a select few are presented in this paper as climate and economic trajectories are uncertain it is useful to consider the middle of the road case of either flood risk driver and to explore the impact of varying the other in this paper we chose to keep the socioeconomic aspect of development fixed by opting for the ssp2 scenario therefore we vary the climate projection and mainly focus on the following two scenario combinations 9 9 in addition to these two consistent climate economy scenarios we also present some results for other rcp scenarios when we explore the advantages of climate change mitigation in section 3 2 1 unsustainable world in the middle of the road rcp6 0 ssp2 in this scenario the carbon emissions peak around year 2080 declining thereafter in combination with the ssp2 scenario of intermediate challenges for both adaptation and mitigation this scenario combination allows us to explore the impact of an intermediate baseline scenario without any emission reduction policies 2 sustainable world in the middle of the road rcp2 6 ssp2 abiding by the paris climate agreement rcp2 6 and strongly curbing carbon emissions well below 2 c this scenario is consistent with the sustainable future assumption whereby countries are abiding by the paris climate agreement schleussneret al 2016 given these two contrasting representations of carbon emission development expected economic damages from not mitigating climate change to 2 c can be calculated in addition we will also compare ssp1 and other ssps in the results section to explore the uncertainty arising from economic development since the model can project annual damages up to 2100 it is common practice to calculate the net present value npv of those damages using a discount rate tol 2008 this estimate is referred to as discounted climate damages this rate is set at 3 as is common in other iams tol 2018 nordhaus 2017 therefore whenever we refer to discounted total δead we are in fact referring to the change in total ead in a particular area in the twenty first century between the years 2010 and 2100 discounted at 3 annually after the proposed model 6 is selected as the climrisk river model the next step is to introduce it into climrisk in the following manner see appendix a 1 magicc model is used to generate probabilistic estimates of global temperatures for the period 2010 2100 2 climrisk s pre defined downscaling factors for each cell are used to obtain grid cell level annual mean temperature and precipitation data 3 the downscaled magicc climate and sres iiasa socio economic data is fed to the climrisk river damage function to obtain grid cell level river flood damage estimates 4 the absolute expected river flood damage is added to the climrisk damage estimates making up the improved climrisk estimate of climate related damages in the following section the expected river flood damage estimates of the climrisk river are presented first followed by the total expected climate related damage from the integrated climrisk model 3 results in this section we present some key results from the climrisk river model first the model is validated using in sample and out of sample δead estimates from the original glofris model next the flood damage estimates produced by the climrisk river model are presented on an aggregate level for several large regions in the world here we also present some key country level results finally particularly interesting local damage estimates are presented to illustrate the relevance of estimating climate impacts at a spatially explicit level here we specifically focus on grid cells most affected by river flooding and identify cities that fall under these areas in the world and in europe 3 1 climrisk river model evaluation the performance of the climate change related river flood damage functions must first be evaluated before being introduced into the climrisk framework first we evaluate the adjusted r 2 value for each model across all grid cells to see how much of the variation in δead it can explain next we evaluate the in sample and out of sample forecasting performance of the model by computing the mean normalized root mean squared error nrmse for each cell 10 10 see appendix c 1 for the description of nrmse and appendix c 2 for more nrmse results the results in the main text focus on two protection standard assumptions baseheightstd and optimalstd which were selected because of the need to capture the most realistic range of adaptation policies focused on maintaining the current height of flood protection baseheightstd and moving towards protection standards that are economically optimal given socio economic development and climate change optimalstd the nostd assumption can lead to misleading flood risk predictions and is used solely to illustrate the importance of including flood protection standards in flood risk models baseprobstd is also set aside as it assumes that countries monitor the flood probability level and upgrade their standards to keep the flood probability constant a policy that many developing countries are currently not pursuing nevertheless the comparison of regression estimates for the nostd and baseprobstd case can be found in table 5 and are available in the climrisk river model since there are over 16 000 cells each with its own adjusted r 2 model estimate table 1 reports only the average adjusted r 2 values from table 1 it is clear that the model with the most explanatory power is model 6 which yields the highest average adjusted r 2 across all flood protection standard assumptions this is to be expected as model 6 contains more predictors and unlike the other models is able to capture the interaction between the climate regressors unlike the r 2 the adjusted r 2 penalizes the use of additional explanatory variables and is therefore comparable across different model specifications it is also important to note that the explanatory power increases by 0 08 on average between models 3 and 4 across all protection standard assumptions model 4 is the first proposed model to contain precipitation as an additional explanatory variable precipitation has often been ignored in integrated assessment models where the sole focus is placed on temperature change see table 5 in the appendix for the complete regression coefficients of the climrisk river model for all protection standard assumptions as we are evaluating a nested model with 6 functional forms of increasing number of predictors we will only focus here on comparisons between model 3 and model 6 the main reason for this is that model 3 represents a functional form similar to that of rice and climrisk models with quadratic function of temperature change this makes model 3 consistent and comparable to other iams unlike model 1 which ignores climate factors or models 2 4 and 5 which include them but either ignore the quadratic terms of temperature precipitation or the interaction between the two the model fit to glofris data is shown in fig 3 two main conclusions can be drawn from these figures first model 6 has a better fit to actual data than model 3 as is measured by the mean normalized rmse and adjusted r 2 the variance of the mean normalized adjusted r 2 is lower under model 6 whereas model 3 only accounts for gdp and first order temperature change model 6 also accounts for precipitation and the squared and interaction terms of temperature and precipitation second model 6 is able to predict future projected glofris estimates reasonably well with the majority of predictions not over or underestimating by more than 1 5 times the mean value of the projected glofris damage the spatial distribution of adjusted r 2 and nrmse in europe and south the spatial distribution of adjusted r 2 and nrmse is presented for europe and south east asia in fig 4 along with their respective histograms in fig 5 the two regions were selected for presentation as europe is expected to experience the lowest amount of flood damage and south east asia the highest south east asia is also among the least flood protected regions in the world whereas the opposite is true of europe it is apparent from fig 4 that in the northern region in india around the ganges river where damage estimates are among the highest in the world the model forecasts damage reasonably well with nrmse of around 0 5 the same is true of the north eastern part of china in the yangtze river basin in europe the adjusted r 2 is higher in central and western parts than in north eastern europe in the baltic region there is no clear spatial pattern with respect to nrmse from the histogram plots it is apparent that most adjusted r 2 values are greater than 0 5 and that most nrmse values are lower than 2 the model fit is good in europe with a mean of 0 65 0 61 and a variance 0 04 for the baseheightstd optimalstd in south east asia the fit is even better with a mean adjusted r 2 of 0 75 0 74 for the baseheightstd optimalstd and variance of 0 02 the out of sample out explanatory power of baseheightstd seems slightly higher than that of optimalstd in both regions this can be explained by the fact that protection standards play an important role in forecasting future flood related damage weakening the climate signal that feeds into climrisk river 11 11 the glofris modelling framework uses a global hydrology and water resource model at 5 x 5 resolution and downscaled to 30 x 30 resolution which simulates fluvial flooding based on a 2d volume river routing model this flood model shows the best results for europe and north america where the meteorological forcing is generally more accurate as a result of availability of station data and reanalysis products sutanudjajaet al 2018 this is in line with the projection accuracy of climrisk river which shows the highest values of r2 and the lowest rmse on average in europe and the us furthermore due to the strong seasonality in forcing and discharge monsoon dominated areas are also well simulated the least accurate results are obtained in african rivers due to groundwater dynamics snow dominated areas and continental eastern europe because of overestimations in groundwater recession constants sutanudjajaet al 2018 for readers who are interested in more information about the physical reasoning behind the glofris flood risk results and strengths and weaknesses we refer to the original gloris paper having performed various model validity checks we can now explore the regression results using the regression coefficients from the out of sample validation for different flood protection standard assumptions table 5 in the appendix illustrates coefficient distributions for each protection standard assumption for model 6 an important conclusion that can be drawn from these results is that protection standards play an important role in producing the annual expected damage estimates as the flood protection assumption is changed from nostd towards optimalstd the climate change signal temperature and precipitation weakens while the gdp signal remains strong but with a smaller effect therefore the key determinant of climate change related flood risk under the economically optimal protection standard is not the climate change but the standard itself and the total economic output increased economic output mostly leads to higher river flood damage across all protection standards with the median independent impact of a 1 billion increase in gdp leading to a 0 1 2 4 million increase in ead across all scenarios table 5 column 4 temperature increase is expected to increase flood damage but the median effect is zero across all cells for most protection standard assumptions the effect of precipitation is also in the direction that it leads to more river flood damage for most cells the median effect of a 1 increase in annual total precipitation on additional flood damage across all cells is around 0 02 million for the nostd and 0 million for optimalstd the mean effect of squared terms varies but overall indicates that many cells experience increasing damage to temperature and precipitation increase this property of the damage functions is important as it helps in capturing nonlinear responses of flood damage to higher temperature and precipitation nordhaus 1992 the interaction term is positive on average indicating that temperature and precipitation reinforce each other s impact on total damage in other words wet hot regions are subject to more severe flooding than dry hot regions 3 2 aggregate flood risk next we look at the development of climate change related flood risk on an aggregate level firstly on a regional and continental and then on a country level fig 6 shows the evolution of river flood risk on a global scale and in several regions united states europe africa south east asia and latin america the results do not vary significantly between different rcp scenarios in the first half of the century but start to diverge in the second for both adaptation assumptions the reduction in river flood risk is much greater with improved protection standards than with a stricter emission reduction policy regardless of the rcp scenario or region this is evident from the spread between the red and blue lines corresponding to baseheightstd and optimalstd under the baseheightstd on the global scale the expected river flood damage could reach 3 5 trillion in 2100 the evolution of climate related river flood damage is similar in europe with damage reaching 25 30 billion in 2100 south east asia and africa are expected to experience the bulk of the total global damage 1 5 2 5 trillion and 1 2 trillion respectively in 2100 when optimal adaptation is introduced the expected damage is reduced drastically on a global scale to around 250 billion in 2100 however the damage reduction is relatively lower in europe around 5 10 billion a 66 reduction when compared to south east asia around 200 billion 90 reduction and africa around 140 billion a 90 reduction the main reason for this drastic regional difference is the fact that the protection standards already in place in the developed world are close to or at the optimal level of protection standards introducing the optimal level of protection standards ensures that the river flood damage due to climate change does not exceed 30 and 10 of the business as usual potential damage in europe and south east asia africa respectively to put climate change related river flood risk into the perspective of total damage from climate change we compare the results of our model with the damage in climrisk 12 12 the plotted climrisk values do not account for climrisk river estimates i e before climrisk river integration fig 6 presents the evolution of climrisk and climrisk river for different regions in the world 13 13 more results for other protection standard assumptions can be found in appendix c fig 18 on a global scale top left the δead under the baseheightstd assumption is equivalent to 50 of the climrisk model damage projections for the rcp 2 6 ssp2 south east asia and africa are expected to experience a large increase in flood risk because adequate flood protection standards are lacking or non existent in many areas the opposite is true in europe and the united states where climrisk river projects river flood damage of much lower magnitude than the climrisk damage projections on an aggregate level the river flood damage under the baseheightstd assumption presents a relatively lesser threat to europe and the united states than to the less developed world of south east asia and africa another important conclusion that can be drawn from fig 6 pertains to the relative impact of climate mitigation under the two models namely the climrisk model is more sensitive to climate change mitigation than climrisk river as indicated by the relative difference in damage estimates under the rcp 2 6 and rcp 6 0 climate scenarios socio economic development plays an important role in future flood risk as it determines the number of exposed assets fig 7 illustrates flood risk estimates under different ssp scenarios and the rcp 2 6 climate scenario in all regions the ssp5 scenario of fossil fuel development would universally lead to the highest expected river flood damages the ssp3 and ssp4 scenarios both involving a relatively high level of climate adaptation compared to that of other ssps would lead to the lowest expected river flood damages in most regions 3 2 1 country level climrisk river in this section we present the climrisk river damage estimates on a country level the damage estimates are presented in terms of discounted δead relative to the discounted gdp for the period 2010 2100 fig 8 below presents a world map of country level damage estimates for the baseheightstd adaptation scenario table 2 shows the top 10 countries in the world 14 14 rounded to the nearest integer and in europe 15 15 rounded to 2 significant digits in terms of highest discounted total δead in the climrisk river model eight of the ten global highest are in south east asia and two in africa specifically the democratic republic of congo dr congo and egypt the discounted climate change related river flood damage in the top 10 most affected countries in the world is around 16 trillion which could be brought down to about 300 billion by implementing the optimal level of flood protection standards under the rcp 2 6 scenario in europe damages are much lower with a combined total of around 105 billion dollars of discounted total δead in the top 10 countries under the rcp 2 6 scenario and with 110 billion under rcp 6 0 this damage could be reduced by adopting an optimal flood protection policy bringing them down to around 25 billion under both rcp 2 6 and rcp 6 0 to conclude european countries stand to gain relatively less from additional adaptation than the most affected countries in the world because the adaptation standards already in place in many european countries are high please note that it is possible for the damage under rcp 2 6 to be higher than under rcp 6 0 scenario due to varying natural factors accounted for in glofris precipitation temperature evaporation etc some regions could become wetter or drier with increasing temperature or could switch between wet and dry periods throughout the twenty first century depending on varying natural factors this effect could ultimately determine the total damage due to river flooding 3 3 local flood risk in this section we analyze the local impacts of river flooding due to climate change this involves the estimation of the discounted value of river flood damage due to climate change at the grid cell level along with city level estimates fig 9 illustrates the effect that flood adaptation standards have on the expected flood damage due to climate change the selected scenario combination in this figure is rcp 6 0 ssp2 and the damages are expressed as a fraction of discounted gdp in europe the area around the rhine river basin would benefit from additional flood adaptation there are also clear benefits to river flood adaptation in eastern europe around the northern coast of the black sea ukraine the greatest benefits from optimal protection standards can be observed in south east asia and africa the northern region of india is expected to experience serious flooding due to climate change along the ganges river basin under the current level protection in china the yangtze river is also expected to cause significant damage to the built environment in africa the nile river poses a flood threat under the baseline protection standards central africa also stands to benefit greatly from river flood adaptation along the congo river basin as most of these areas currently have low or no river flood protection standards in the following subsection we focus on cells most severely affected by future flood risk and identify cities that fall within their boundaries 3 3 1 city level climrisk river river flooding represents a serious threat to the built environment especially in areas wherein flood protection standards are lacking and many assets are exposed most cities are built around rivers or other available bodies of water which makes them vulnerable to flooding and this is particularly the case in overpopulated cities that are less developed and lack flood protection although climrisk river does not produce city level estimates we identify in table 3 the cities in the world residing in cells that are expected to experience the most discounted total δead between 2010 and 2100 this is not a perfect measure of city level flood risk but helps us identify the cities located in most flood risk prone areas the first half of the table consists of city cells that are expected to experience the most discounted damage in the world here eight of the ten city cells belong to india and two in dr congo the reasons for this include relatively low flood protection standards as witnessed by the benefits of flood adaptation under the optimalstd assumption a high potential for flooding around the ganges river basin and an abundance of exposed assets the top 10 european city cells are expected to experience relatively less damage with a total discounted δead of around 80 45 billion under rcp 2 6 and the baseheightstd assumption still there are benefits from optimalstd that result in around 31 55 billion discounted δead 16 16 please note that this estimate is heavily influenced by the fact that the city of porto is facing a high flood risk under both baseheightstd and optimalstd meaning that there is insufficient space for flood adaptation improvement under rcp 6 0 the situation is similar with baseheightstd leading to 33 billion and optimalstd to 5 6 billion additional damage the bulk of this damage would be experienced around the danube river basin in eastern europe as was also evident from fig 9 we are interested in the difference in expected flood risk among city cells and express the damage projections as a fraction of gdp 17 discounted δead and gdp are both for the period 2010 2100 at 3 17 using this method we define the following four arbitrary risk categories 1 low risk city cells discounted δead of less than 5 discounted gdp 2 medium risk city cells discounted δead between 5 and 25 of discounted gdp 3 high risk city cells discounted δead between 25 and 100 of discounted gdp 4 very high risk city cells discounted δead of more than 100 discounted gdp fig 10 presents low risk city cells in particular those that could experience less than 5 gdp in discounted total δead most cities globally fall within this category the medium risk category is presented in fig 11 fewer city cells in the developed world belong in this category and most cells can be found in latin america africa and south east asia the high risk category is presented in fig 12 and isolates city cells almost exclusively in africa latin america and south east asia gyor in hungary is the cell in europe belonging to a high risk category fig 13 presents city cells which belong to a very high risk group of cells as expected the most affected cells could be found exclusively in central africa dr kongo and in latin america in areas with low river flood protection standards 4 discussion the climrisk river model was developed using the spatially explicit damage estimates and flood protection standards from the glofris and flopros models several model specifications were tested and the best performing statistical model model 6 was selected as the climrisk river model the process of selecting the most suitable model among different candidates was based on the measure of statistical fit to the glofris model data however the proposed model candidates were chosen with general climate and economic predictors in mind to remain intuitive and easy to use thereby avoiding the issue of overfitting and going beyond simple emulation several validation methods were applied to the newly estimated model we first looked at the spatial and frequency distributions of the adjusted r 2 statistics for all cells the model performs reasonably well on a global scale the mean adjusted r 2 of the model being 0 69 and 0 66 for the baseheightstd and optimalstd assumptions respectively we also measured the model forecasting performance through within sample and out of sample nrmse with an average out of sample 5 fold nrmse value of 0 69 and a variance of 0 17 across all cells we can confirm that the model performs reasonably well on previously unseen data this grants confidence to our approach to gain a better understanding of the magnitude of climrisk river damage estimates within the total climate change related damage the results are plotted against total climrisk damage estimates on a global scale under the baseheightstd flood protection scenario the climate related damage due to river flooding could exceed previously estimated total climate damage in climrisk 18 18 rcp 2 6 ssp2 fig 6 of 2 trillion in year 2100 however when the economically optimal flood protection policy is introduced the estimated damage drops down to just around 250 billion an estimated ten fold decrease in global river flood damage at the end of the century these results highlight the importance of accounting for adaptation in estimating the damage functions for iams however the divergent estimates under the different adaptation assumptions also reveal that adaptation policies introduce an important uncertainty in forecasting future flood risk there are also some limitations of the model which require attention firstly we are focusing on emulation of a complex river flood risk model and approximate the underlying physical and socio economic processes to gain on both model and computational simplicity which results in structural model uncertainties for instance flood damage development depends on the severity of flooding which is approximated with annual resolution climatic variables used in this study to emulate damage changes in weather extremes are not accounted for in the current work for the sake of ease of model implementation within the broader climrisk model moreover it should be realized that the flood damage functions are based on glofris results forced with gcm simulations rather than recorded climatic data in which annual trends may be less clearly aligned with occurrence of extremes climate economy iams inevitably require reduced form damage functions like we estimated for climrisk river and despite the limitations we have confidence in our approach given the overall good forecast performance of our model there are possibilities for improving the presented model in future work one point of improvement could be the function estimation method for example different model specifications could be used including non linear interpolation or even less orthodox non parametric methods splines bootstrapping etc however adding more degrees of freedom to the model should be done with care as this could lead to overfitting to the glofris estimates and complex damage functions both of which climrisk river attempts to avoid 5 conclusion in this paper we presented climrisk river a climate change related river flood damage assessment model that is integrated in a spatially explicit climate economy iam called climrisk the newly developed model operates on a local spatially explicit scale and can project future climate change related river flood damage estimates for various socioeconomic climate and flood adaptation scenarios the main idea behind the development of such a model was the need for generic damage functions that can be used with any climate and socioeconomic scenarios this means that the user will be able to select any climate and socioeconomic scenario combination including future to be developed versions of scenario projections as long as they have local temperature precipitation and gdp estimates to feed into the model two traits of the climrisk river model are unique in the existing literature on climate economy iams first its local nature allows the user to explore river flood damage projections for 0 5 0 5 cells anywhere in the world this is much more detailed than most climate economy iams that apply global damage functions or estimate the economic impacts of climate change for several world regions second its inclusion of current and future adaptation scenarios that account for local river flood protection standards in generating the damage estimates is unique in existing literature of iams few existing climate economy iams explicitly account for adaptation using policy relevant scenarios four main conclusions can be drawn from our analysis first our validation exercise shows that our reduced form flood damage functions estimated at the grid cell level perform reasonably well in capturing estimates produced by the more detailed flood damage model glofris second while iams have relied on temperature as a proxy for climate change in generating economic impacts we show that precipitation is a relevant additional explanatory variable in projecting changes in river flood damages third our aggregated results for several large regions show that the additional flood damages from climate change are non negligible compared to the total economic impacts estimated by climrisk although this depends on the flood adaptation scenario finally the spatially heterogeneous results that we presented for countries and cities reveal the importance of introducing local scale estimates like glofris into a more local iam like climrisk these findings demonstrate the advantages of the local nature of our model which allows the user to observe the spatial distribution of river flood damage that are otherwise lost through aggregation funding this research has received funding from the european union s horizon 2020 research and innovation programme under grant agreement no 776479 philip j ward received additional funding from the netherlands organisation for scientific research nwo through vidi grant 016 161 324 the authors also acknowledge the financial support received from the united nations development programme méxico the national institute of ecology and climate change grant ic 2017 068 as part of mexico s sixth national communication to the united nations framework convention on climate change declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements observed temperature and precipitation data used for training the climrisk river model was kindly provided by edwin sutanudjaja of utrecht university appendix h supplementary data the following is the supplementary data related to this article multimedia component 1 multimedia component 1 appendix h supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2020 104784 appendix a climrisk model the flood risk model climrisk river climrisk river developed in this paper is introduced into the climrisk model that operates on a local 0 5 0 5 or larger scale estrada and botzen 2018 climrisk is a global model that assesses the dynamic economic impacts of climate change at the local scale 0 5 x 0 5 for various socioeconomic and climate change projections this is done by combining local gdp exposure information grübleret al 2007 with climate projections obtained using the magicc climate model and scaling patterns from the climate models included in the cmip5 lynch et al 2017 kravitz et al 2017 regional impact functions are taken from the rice model and encompass a broad range of economic sectors nordhaus 2017 estrada et al 2017 the result is a global dynamic integrated assessment model that projects climate damages on a local regional and global scale the climrisk model is composed of the following interlinked components 1 hazard climate change projections 2 exposure economic projections 3 vulnerability impact functions these modules build upon and integrate well established models datasets techniques and procedures that have been previously evaluated and published as will be explained next 1hazard climate projections climrisk combines probabilistically generated global temperatures with the regional precipitation and temperature scaling patterns from 41 general circulation models gcm in the cmip5 database lynch et al 2017 kravitz et al 2017 for the global and regional probabilistic estimates of climate projections climrisk uses the magicc version 6 software magicc represents a reduced complexity model of climate change and is widely used in the research community to project future climate impacts meinshausen et al 2011 meinshausen et al 2011 van vuuren and carter 2014 barker 2007 it also relies on the technique of pattern scaling to produce local estimates of temperature and precipitation change the magicc projections used in climrisk are the difference in annual mean temperatures in degrees celsius and precipitation in with respect to 1900 the spatial patterns of different gcms are randomly selected using a uniform distribution weigel et al 2010 knutti 2010 climrisk imposes a triangular probability distribution for the climate sensitivity parameter this particular distribution is centered around a lower limit of 1 5 c an upper limit of 4 5 c and a mean value of 3 c for climate sensitivity stockeret al 2013 the triangular distribution of temperature realizations is a measure of climate sensitivity in the climrisk model by representing it using a probability distribution the iam is able to produce temperature rise estimates that encompass the likely ranges of climate change as precipitation realizations directly depend on temperature realizations in magicc the triangular distribution covers both climate variables in the model for the purposes of this paper only the median 50th percentile realizations are used for all scenario combinations 2exposure economic projections scenarios of gdp determine the economic exposure to the climate change hazard a 1 which is an important input in the damage functions for the climate impacts calculation a 3 the economic projection data used in climrisk and climrisk river relies on the gdp and population projections of the ssp public database version 1 1 19 19 https tntcat iiasa ac at sspdb in order to construct spatially explicit information climrisk combines the shared socio economic pathways ssp and special report on emissions scenarios sres projections available through the ggi scenario database version 2 0 1 http www iiasa ac at research ggi db raoet al 2008 riahiet al 2017 grübleret al 2007 these databases are commonly used in climate impact studies and gather a variety of scenarios about gdp and population that have been produced by different modeling groups this procedure of combining ssp and sres data can be done due to the fact that certain sres and ssp scenarios are consistent with each other riahiet al 2017 for example a1 or b1 and the ssp1 ssp5 narratives represent a similar pattern of socio economic development the a2 scenario would make ssp3 and ssp4 scenarios plausible while the b2 sres is consistent with ssp2 van vuuren and carter 2014 the pattern scaling method used in climrisk is similar to that in other general circulation models kravitz et al 2017 tebaldi and arblaster 2014 the grid cell i s pattern spatial pattern labelled as p i t represents the proportion of gdp or population in cell i within region r to the regional total across all cells i 6 p i t y i t s r e s t 1 i y i t s r e s where cell i belongs to a particular region i r y i r t r is derived from sres and for every sres storyline these patterns are not assumed to be constant over time and are updated every ten years period the regional total of the particular ssp scenario can then be scaled by the sres consistent regional p i r t pattern to obtain i s grid cell level estimate 7 y i t s s p p i t y r t s s p by construction the downscaled scenario projections match exactly the quantifications of the ssp narratives produced by different modeling groups at the aggregated 13 world regions and the global totals climrisk attempts to account for socioeconomic uncertainty by including all the ssp narratives ssp1 ssp2 ssp3 ssp4 ssp5 in addition uncertainty in gdp quantification is tackled through the use of economic data provided by three different modelling groups oecd env growth iiasa pik crespo cuaresma 2017 dellink et al 2017 leimbach et al 2017 the iiasa scenarios were selected for the use in climrisk river because they correspond to the socio economic projections used in the glofris model making the model validation easier however using any of the above mentioned sources is possible when generating river flood damage projections 3vulnerability impact functions climrisk relies on the regional damage functions improved estimates of the rice model nordhaus 2017 these regional damage functions encompass a broad range of economic impacts following the rice model the impact functions take into account the losses suffered by major economic sectors such as agriculture but also the cost of sea level rise adverse impacts on health non market damages and catastrophic damages nordhaus 2014 the damage function in rice is as follows 8 d r t r i c e α r t t β r t t 2 where α r and β r represent the regional temperature coefficients that measure the impact of global mean annual temperature increase on climate related regional economic damage d r t rice measured in billions of dollars 2015 ppp table 4 presents the climrisk rice regions along with their respective damage coefficients based on nordhaus 2017 the damage function defined above lets us derive the rice regional climate impacts 9 i r t r i c e y r t d r t r i c e where y r t is the annual regional gdp in region r at time t when using pattern down scaled inputs as defined in a 2 the regional damage function can be converted into a local 10 d i t c l i m r i s k α r t i t β r t i t 2 where as previously defined α r and β r represent the regional temperature coefficients that measure the impact of local mean annual temperature increase t i t on climate related local economic damage d i t c l i m r i s k in cell i the total economic impact of climate change in year t in cell i is therefore 11 i i t y i t d i t c l i m r i s k s r t r i c e where y i t represents the projected output for cell in year t as defined in equation 7 and s r t represents the scaling factor that ensures that projected damages are exactly the same regardless of whether the regional damage functions are driven by global or grid cell temperatures the scaling factor s r t is defined as 12 s r t r i c e i r t r i c e i i i i t where i r t r i c e represents the rice regional impacts and cell i belongs to particular region i r consistent with the rice model that assumes the same damage function can be applied across areas within a region across ssp scenarios and over time we apply the same rice damage function to grid cells within a region this assumption may be seen as restrictive which is why climrisk river introduces damage functions for river flood risk that vary between grid cells see section 2 in addition to the local impacts based on the rice damage functions climrisk also introduces urban damages into the model given that the rice model relies on regional mean surface air temperatures it is not possible to account for the effect of urbanization on local temperatures in cities using the rice model an effect commonly known as urban heat island uhi however the local nature of climrisk makes the modeling of uhi possible namely urban areas 1 million inhabitants are likely to experience higher damage due to climate change than non urban areas as cities account for about 80 of global gdp and about 50 of global population re 2004 dobbs et al 2011 in addition urban areas are expected to experience higher local temperatures due to the urbanization process for example the replacement of natural surfaces with structures of higher thermal capacity concrete asphalt etc leads to local climate change effects estrada et al 2017 since the grid cell scale is still likely to cover an area larger than a particular urban city the total output in an urban cell is divided between the non urban 20 and urban 80 part of the cell the damage function for the non urban part of a particular urban cell i follows a recently used damage function for urban areas in an iam for cities estrada et al 2017 13 d i t t u 0 9 t i t u i t 2 5 2 where t i t and u i t represent changes in annual temperature in cell i due to global and local climate change respectively the combined damage from the non urban and urban part of the urban cell represents the total expected damage for cell i in a given year t the local temperature increase u i t in equation 13 is commonly estimated in the following way 14 u i t a p o p i t where p o p i t represents the urban population of cell i at time t and parameters a and b are commonly used in literature for estimating the urban temperature increase due to urbanization oke 1973 karl et al 1988 mills 2014 appendix b regressions the coefficients of regressions for all cells in model 6 are presented in figure 14 15 16 and 17 for various protection standard assumptions in table 5 the distribution of different coefficients is presented for model 6 the chosen climrisk river model for presentation purposes the coefficients in this table are converted to yield damages in millions of usd instead of billions as presented in the paper from the figures we can observe that the effect of an increase in economic output gdp leads to increased exposure and higher river flood damages the effect is the strongest with no existing flood protection standards median 2 3 million and drops significantly when optimal standards are employed median 0 1 million for every 1 billion increase in gdp we can also observe that the effect of precipitation is stronger with no existing standards median 1 1 million than in other cases median 0 1million for a 1 point increase in precipitation the effect of temperature remains ambiguous with median value of 0 across all flood protection assumptions fig 14 model 6 coefficient distribution nostd fig 14 fig 15 model 6 coefficient distribution baseheightstd fig 15 fig 16 model 6 coefficient distribution baseprobstd fig 16 fig 17 model 6 coefficient distribution optimalstd fig 17 table 5 regression results table for climrisk river the coefficients refer to impacts on δead in millions of dollars 2005 ppp the effects of temperature and precipitation change grow stronger as the protection standard assumption moves from nostd towards optimalstd however the effect of δgdp on total damage decreases dramatically with improved protection standards indicating the reduction in river flood vulnerability table 5 coefficient p value variable mean 2 5 50 97 5 mean 2 5 50 97 5 nostd β gdp 80 84 21 65 2 34 345 61 0 002 0 0 0 β ᴛ 31 62 272 64 0 08 76 51 0 007 0 0 0 001 β ᴛ 2 7 95 9 16 0 59 76 0 010 5 0 0 0 038 β p 1 57 5 89 0 02 19 99 0 011 0 0 0 052 β p 2 0 04 0 46 0 0 8 0 012 0 0 0 098 β p t 0 9 3 13 0 9 84 0 011 0 0 0 069 nrmse out 1 44 0 0 0 4 observations cells 16503 adj r 2 in 0 83 0 53 0 86 0 97 baseheightstd β gdp 92 02 7 61 0 57 201 75 0 003 0 0 0 β ᴛ 29 82 198 53 0 14 55 0 009 0 0 0 027 β ᴛ 2 6 62 2 26 0 35 58 0 012 0 0 0 079 β p 0 02 6 66 0 6 19 0 012 0 0 0 091 β p 2 0 05 0 22 0 0 56 0 011 0 0 0 057 β p t 1 0 89 0 8 89 0 010 0 0 0 045 nrmse out 4 5 0 0 0 64 observations cells 16503 adj r 2 in 0 7 0 25 0 75 0 91 baseprobstd β gdp 96 43 7 23 0 53 158 39 0 002 0 0 0 β ᴛ 16 22 117 83 0 14 16 0 007 0 0 0 002 β ᴛ 2 3 64 1 75 0 23 36 0 010 0 0 0 045 β p 0 1 2 14 0 3 08 0 013 0 0 0 119 β p 2 0 01 0 1 0 0 17 0 014 0 0 0 158 β p t 0 44 0 44 0 3 19 0 014 0 0 0 139 nrmse out 3 34 0 0 0 43 observations cells 16502 adj r 2 in 0 81 0 36 0 85 0 98 optimalstd β gdp 131 87 5 73 0 1 42 19 0 004 0 0 0 β ᴛ 2 2 3 49 0 4 57 0 009 0 0 0 018 β ᴛ 2 0 63 0 45 0 1 43 0 012 0 0 0 084 β p 0 02 0 43 0 0 59 0 014 0 0 0 141 β p 2 0 01 0 03 0 0 04 0 013 0 0 0 136 β p t 0 12 0 14 0 0 39 0 013 0 0 0 106 nrmse out 4 62 0 0 0 67 observations cells 16503 adj r 2 in 0 66 0 2 0 7 0 91 appendix c baseprobstd and nostd results fig 18 annual δead in climrisk river under different protection standards rcp 2 6 and ssp2 scenario combinations fig 18 appendix d model forecasting performance nrmse introduction to evaluate the forecasting performance of the climrisk river model we use a statistical method of normalized root mean squared error nrmse first we compute the within sample nrmse by normalizing the root mean squared error rmse eq 15 with the mean value of the observed response variable eq 16 to compute the out of sample nrmse statistic for each cell we run a five fold stratified cross validation check where we split the data into four stratified training sets and one test set each subset containing points from a single esm next we evaluate the out of sample rmse on the test fold we repeat the process with a different training test split set until every fold has been used in testing exactly once finally the mean rmse value across all five folds is taken and is normalized using the mean value of observed response variable eq 16 to yield the nrmse eq 17 15 r m s e n 1 n δ e a d g f δ e a d c l r v 2 n 16 δ e a d g f n 1 n δ e a d g f n n 17 n r m s e r m s e δ e a d g f where δ e a d g f is the mean value of all the δ e a d g f observations used to fit the damage function for a particular cell the reason for this normalization is that it gives a more intuitive interpretation of the rmse by indicating how large the forecasting error is relative to the mean δ e a d g f observed in glofris nrmse results in this section we compare the performance of models 3 and 6 as potential candidates for climrisk river fig 19 presents the out of sample predictions 20 20 the out of sample predictions labelled as out refer to stratified 5 fold cross validated rmse estimates normalized using the mean value of the response variable for the two models of interest for the estimates of the remaining models please refer to fig 20 and 21 fig 19 forecasting performance of models 3 and 6 the climrisk river model 6 right panel has a better forecasting performance than model 3 left panel as measured by out of sample nrmse for presented regions fig 19 fig 20 out of sample nrmse for alternative model candidates fig 20 fig 21 out of sample adjusted r2 for alternative model candidates fig 21 appendix e computational performance considerations due to the large amount of data required for processing in climrisk various improvements to performance of the model have been made the gridded nature of both the flood and climate damage data is suitable for parallel processing two api s for parallel processing have been used in the model multiprocessing library in python which is able to get around the global interpreter lock gil allowing computations to be split into many processes which can be run concurrently utilizing the full potential of a multi core system pycuda library in python which utilizes the potential of a graphical processing unit gpu present in the system the cellular architecture of a gpu makes solving relatively simple operations over a large grid convenient and fast please note that an nvidia gpu with cuda support is required 
25993,intensive agricultural practices represent a major threat to aquatic ecosystems because they impair water quality however this can be ameliorated by farmers improving crop management provided they are aware of their contribution to declining water quality water quality information systems can increase farmer awareness but most were developed to assess water quality targets set in regulations rather than inform farmers we developed the 1622wq application using user centred design principles to provide farmers with real time information on nitrate and other contextual variables in their local creeks and rivers the design process identified barriers to uptake of the application such as a limited internet connection b poor data quality and c operational issues once these barriers were addressed there was substantial uptake nevertheless providing real time information to farmers is only part of the solution due to legacy issues caused by a digital divide between traditional industries and those that are digitally enabled keywords participatory water quality monitoring great barrier reef nitrogen sugarcane pollution human centred design 1 introduction the use of nitrogen fertilisers in agricultural systems has allowed farmers to increase crop production per unit of land sustaining increasing human populations zhang et al 2015 however a large fraction of the nitrogen applied in agricultural lands is lost to aquatic ecosystems mekonnen and hoekstra 2015 agriculture is responsible for most of the global nitrogen input to freshwater and marine ecosystems fowler et al 2013 mekonnen and hoekstra 2015 which represents a threat to water security via the eutrophication of water bodies elser et al 2007 groundwater degradation rosenstock et al 2014 thorburn et al 2003 and increased water treatment costs mcdonald et al 2016 climate change is expected to intensify nitrogen pollution due to increased frequency and intensity of extreme weather events jeppesen et al 2011 to protect aquatic ecosystems from nitrogen and other agricultural pollutants governments have introduced regulations that define targets for water quality kroon et al 2016 nainggolan et al 2018 national research council 2010 van grinsven et al 2012 achieving these targets requires farmers changing crop management so that nitrogen losses are minimised kroon et al 2016 nainggolan et al 2018 wulff et al 2014 voluntary actions have been shown to lead to long lasting behavioural change ayer 1997 because of the complexities associated with regulatory approaches bohman 2018 farmer behaviour can be influenced by a range of factors such as economic incentives legal requirements perceptions personal factors and beliefs gachango et al 2015 greiner and gregg 2011 pannell 2017 taylor and eberhard 2020 for farmers to engage in voluntary actions to improve water quality they need to be aware of the link between crop management and water quality glavan et al 2019 macgregor and warren 2006 unfortunately farmers are often unaware of their contribution to declining water quality benn et al 2010 glavan et al 2019 macgregor and warren 2006 okumah et al 2018 or are sceptical about the impact of agriculture on the environment macgregor and warren 2006 pannell 2017 stuart et al 2014 despite this evidence many targets for water and land management assume that impacts from farming practices are well understood by farmers blackstock et al 2010 clearly for these targets to be reached farmers need to be more aware of the link between farming practices and water quality barnes et al 2009 and there needs to be better dissemination of water quality information to farmers gachango et al 2015 several nitrate water quality information systems exist such as the land air and water aotearoa lawa nz https www lawa org nz the iowa water quality information system iwqis https iwqis iowawis org the usgs water quality information system https www usgs gov products data and tools real time data water and the eyes on the chesapeake bay http eyesonthebay dnr maryland gov these systems were primarily developed to assess water quality improvements coming from government programs and report results to governments and the public jones et al 2018 rather than raise awareness among farmers a different approach is needed to maximise the impact of this information on farmers awareness mcintosh et al 2007 specifically to help farmers understand the link between crop management and nitrogen pollution water quality information systems need to be real time high frequency and provide contextual information that can assist farmers interpret the data glavan et al 2019 jones et al 2018 real time water quality information systems can facilitate the immediate evaluation of the impact of a recent farming practice e g fertiliser application on water quality moreover because discharge i e triggered by rainfall may increase nitrogen loads in running waters over short periods jones et al 2018 high frequency monitoring systems are needed to demonstrate the link between crop management river discharge and nitrogen pollution in addition contextual information e g rainfall can help farmers understand the connection between weather events and nitrogen pollution with the aim of increasing awareness of the impact of cropping on water quality we developed 1622tmwq a web based application that provides real time data on stream water quality we utilised principles from social science and human centred design in an effort to maximise the uptake of the application jakku and thorburn 2010 rose et al 2016 stitzlein and mooij 2019 ultimately changes resulting from use of the technology and sharing of the lessons with peer networks of farmers is hoped to increase the likelihood of achieving water quality targets glavan et al 2019 our contribution to knowledge is to describe the pathway of the development process of the application through to descriptions of stakeholder interaction to evaluate the extent to which 1622wq increased farmer awareness as such we report on attempts to mitigate issues of poor internet connectivity lack of engagement with technologies in general and poor data quality to emphasise lessons for technology development aiming to achieve impact 2 study site 2 1 regional setting our study site comprises some of the river catchments that discharge into the great barrier reef gbr world heritage listed site particularly sugarcane farming in the russel mulgrave johnston and tully catchments of the wet tropics fig 1 river flows in these catchments are characterised by high intensity flood events during the wet season november april which lead to substantial export of nutrients pesticides and sediments to the gbr mitchell et al 1997 across all catchments discharging into the gbr ecosystems agricultural land use is the major source of nutrients pesticides and sediments kroon et al 2016 grazing systems occupy 80 of the area and account for most of the sediments loads to the gbr waterhouse et al 2012 cropping systems banana sugarcane occupy 5 or the area but account for most of the dissolved inorganic nitrogen din nitrate nitrite ammonium discharged from the catchment 14 940 t year 1 napel et al 2019 among cropping systems sugarcane is the main contributor to din load on an annual basis thorburn et al 2013 thorburn and wilkinson 2013 waterhouse et al 2012 because it is a widespread crop 40 of cropping area grown in areas of high rainfall 1000 4000 mm year 1 and it receives large amounts 130 200 kg n ha 1 of synthetic nitrogen fertiliser thorburn et al 2013 high din availability in coastal waters promotes outbreaks of the coral eating crown of thorns starfish responsible for much of the decline in coral de ath et al 2012 to protect the gbr the australian and queensland governments released the reef 2050 long term sustainability plan which sets a target of 60 reduction of anthropogenic end of catchment din loads by 2025 commonwealth of australia 2018 to achieve this target farmers in the gbr catchments need to alter crop management so that nitrogen losses to the environment are minimised these could include reducing n fertiliser application rates or replacing conventional fertilisers with ones that better synchronise n supply with crop uptake thorburn et al 2017 despite considerable investment by governments in incentives to promote practice change kroon et al 2016 the adoption of improved practices has been very slow the government of queensland 2018 innovative ways of promoting farming practice change have been called for great barrier reef water science taskforce department of environment and heritage protection 2016 we recognise that agricultural innovation is not only about adopting new digital technologies but requires collaboration between scientists and stakeholders throughout the innovation process and through the development of new technologies jakku and thorburn 2010 klerkx et al 2012 as such the design and development of 1622wq was guided by interactions with farmers for practical reasons we focused on the russell mulgrave catchment fig 1 2 2 water quality monitoring within the study area there are a number of water quality monitoring efforts the most extensive is the great barrier reef catchment loads monitoring program gbrclmp a queensland government program that collects data across multiple catchments to track long term trends in water quality entering the great barrier reef lagoon to both validate catchment water quality models and track changes in water quality relative to government water quality targets https www reefplan qld gov au there are also projects aiming to inform farmers about variation in water quality in specific localities and how that relates to their farm the first project established was project 25 farmers water quality and on farm decision making hereafter referred to as p25 davis 2019 a farmer led water quality monitoring program located in the russell mulgrave catchment fig 1 the second was the cane to creek c2c project an extension officer led project working with farmers to understand water quality in the figtree creek sub catchment of the mulgrave river catchment billing 2017 the final project is the wet tropics major integrated project wetmip https terrain org au projects wet tropics major integrated project measuring water quality at two locations in the tully river catchment and one location in the johnstone river catchment fig 1 prior to collaboration with our developments described below water quality data collected in these projects was provided to farmers by project staff at individual or group meetings the time between collection and these meetings were in the order of months with the gbrclmp data are made publicly available through reports napel et al 2019 and an interactive story map http qgsp maps arcgis com apps mapseries index html appid 9d1aad1e2b444ec6a1890e4032284147 the delay between collection and display is in the order of years 3 1622wq application the 1622wq application is named after the highest mountain in queensland australia which is 1622 m in height 3 1 data sources and storage high frequency nitrate concentrations no3 n mg n l 1 were obtained from opus and nico sensors trios https www trios de en sensors html accuracy 5 0 1 mg l 1 no3 n in deployed in tidal and non tidal waters respectively in the water quality monitoring programs and projects described above other parameters are also monitored in these programs river or creek height m being of particular relevance here to date the gbrclmp program monitors 10 of the 35 river catchments draining into the gbr through 11 real time stations measuring water level and or water quality variables including nitrate the water sampling intervals range from 10 to 60 min data from the program are stored in a cloud based platform named eagle io https eagle io where water flow m3 s 1 and n loads t d 1 are calculated for the two locations that have reliable rating curves tully river at euramo and tully gorge national park as well as water level and nitrate concentration data all data from this program are referred to as queensland government in the app p25 has to date deployed four monitoring stations collecting nitrate concentrations every 60 min in the c2c project nitrate concentration is collected every 60 min at one monitoring station in the wetmip project nitrate concentrations are recorded every 30 min at two locations in the tully river catchment and one location in the johnstone river catchment data from these projects are also stored in cloud based platforms including eagle io they are identified in the app by their project names our early user research described below in section 4 3 2 highlighted that farmers in gbr catchments saw rainfall as an important factor in both their farm performance and management and the driver of nitrogen losses from their farm thus rainfall data is also integrated into the 1622wq application to enable farmers to establish the link between rainfall fertiliser application and nitrogen pollution rainfall data are obtained from the australian bureau of meteorology http www bom gov au which collects rainfall data every 60 min on the gbr catchments in addition 10 rainfall gauges were deployed by the project team in the russell mulgrave catchment collecting data every 10 min all data streams are stored in eagle io 3 2 prototype different versions of the 1622wq prototype were developed using the shiny framework https shiny rstudio com an open source r package that provides a web framework for building web applications using r https cran r project org shiny facilitates turning analyses into interactive web applications without requiring html css or javascript knowledge beeley 2013 prototypes were deployed on the shinyapps io platform https www shinyapps io and access was restricted by user identification preventing anonymous visitors from accessing the application the user interface consisted of three tabs see supplementary material figures a1 3 1 a locn tab that displays a list of all locations and allows users to filter and select locations 2 a map tab that allows users to select a location and 3 a data tab that displays data streams for the selected location the prototype obtained data from csv files and did not leverage any form of caching which caused latencies in data loading times as described in section 4 3 social science and user experience research identified that the latencies in data loading times and the need for a two step user registration discouraged farmers from using the application these barriers for technological uptake were exacerbated by poor internet connectivity the additional complexity needed for addressing these issues meant that it was more effective to transition from shiny to a single page application spa 3 3 application 3 3 1 description the application accessible through https wq 1622 farm was designed as an online mobile first single page application spa with no installations required on desktop or mobile allowing us to partially address internet connectivity issues by keeping network requests to a minimum internet connectivity issues were also alleviated by allowing for local caching of data streams and locations as previously indicated the complexity of registration and logging in acted as a barrier to uptake thus the web based application was designed so that publicly available data streams are accessible without the need for registration registration and login systems were continued to access private data streams which are password protected when opening the application the user is presented with an annotated map displaying the locations where data streams are available fig 2 when a location is selected some key figures i e latest nitrate concentration load are displayed the user can browse and search specific locations by clicking on the locations button located in the top navigation area fig 2 locations can be bookmarked by clicking on the favourite button allowing for quick data access in future interactions fig 3 when selecting a location charts displaying data streams for that location are shown on desktop both the charts and the map are visible to facilitate navigation fig 3 on mobile an exit button was implemented to return to the map see supplementary material figure a4 the charts are easy to navigate i e scrolling on desktop or pinching on mobile readjusts the time span to facilitate the interpretation of the data contextual information such as yearly and monthly average values are also displayed fig 3 3 3 2 architecture the 1622wq application was developed as a react spa with a django microservice based backend https www djangoproject com and a postgresql database https www postgresql org these were implemented for speed and utilize all open source frameworks and libraries fig 4 the django microservices expose authorized access to the data index stored in a postgresql database the backend consists of a a data service that acts as an intermediary between the frontend the database and eagle io b a discovery service that retrieves a list of data streams from eagle io and c an access service that manages access to the data streams the data service checks that the user has permission to access the data and requests data from eagle io by searching for the details stored in the data index a management system allows administrators to manage data index permissions through the 1622wq management interface as well as import new data sources into the index through the discovery service interface the discovery service crawls eagle io for new data sources and displays them through an interface where administrators can select the new data sources to be added to the data index once data sources are added the access service can be used to restrict access to users or groups of users data filtering and infilling is conducted in amazon web service aws lambda see section 4 1 and 4 2 the web based interface was written in typescript utilising the react javascript library https reactjs org and redux framework https redux js org for state management allowing for fast page loading times the map was managed through the leaflet library https leafletjs com and the imagery obtained from mapbox https www mapbox com the interactive charting library echarts https www echartsjs com was used for displaying time series of the data streams 4 testing and evaluation 4 1 erroneous data errors and anomalies are commonly found in real time water quality data zhang et al 2019 erroneous measurements are a result of sensor malfunction lack of sensor maintenance physical interference with the sensors communication failures among others this is a particularly acute problem for water quality monitoring in gbr catchments the sensors accessed for 1622wq are commonly deployed amongst vegetation on river or creek banks in remote locations e g up to 1000 km from a water quality technician in these settings debris from vegetation often causes physical interference with the sensors which can take some time to repair a malfunctioning sensor erroneous measurements described below make it difficult to interpret the data and can also lead to lack of confidence by stakeholders in the measurements to tackle these issues we introduced a process to filter out obviously erroneous data the most common problems with nitrate data that we encountered were 1 the presence of negative values and 2 unrealistically rapid change in nitrate concentrations negative values were removed by the data filter changes in nitrate concentrations that exceeded the 98 quantile for a specific location were also considered unrealistic for the environments being monitored and removed helsel and hirsch 2002 in addition to data issues we identified problems with the sensors related to the need for inspection and maintenance of the sensors these problems can be recognized through reference variables related to light intensity which are implemented by the sensor manufacturers and indicate that the quality of the measurement is compromised trios optical sensors 2019a 2019b in our data set we removed nitrate values when the sensor reference variables were below or above the thresholds defined by the sensor manufacturers we also removed data that had been labelled as erroneous data by the water quality program project managers the filtering system is implemented in aws using the architecture shown in fig 5 firstly individual data streams are retrieved from eagle io using the http api and cache into aws then the three filtering modules are applied sequentially using the aws lambda service finally the filtered data streams are written back to eagle io filtering removes values from the data stream small gaps less than 5 data points missing created by the filtering algorithms are filled by linear interpolation which is implemented in the filtering module however linear interpolation is not appropriate when multiple data points are missing and a more sophisticated infilling method described below is used 4 2 missing data large gaps more than 5 data points in data streams can be produced during the filtering e g as a result of sensor malfunction or due to network communication outage multiple approaches have been developed to infill gaps in the data box et al 2013 pankratz 1983 however they usually give poor estimates when applied to large data gaps thus to infill large gaps we implemented a new sequence to sequence imputation model ssim based on a deep neural network zhang et al 2019 the ssim uses the state of the art sequence to sequence deep learning architecture and the long short term memory network lstm to utilize information that varies over time the model provides superior performance in recovering missing data sequences reducing a range of error metrics root mean square error mean absolute error mean absolute percentage error and symmetric mean absolute percentage error by 70 98 compared with six established data infilling techniques arima seasonal arima matrix factorization multivariate imputation by chained equations and expectation maximization zhang et al 2019 the ssim model has been implemented for water level streams in the three locations where it was tested russel river mulgrave river at deeral and johnstone river we did not implement the infilling algorithm on nitrate data streams as there are challenges associated with infilling data streams with large gaps we intend to expand the functionality to other sensors and data streams as more data becomes available and deep learning models are better developed 4 3 social science and user experience for the research and design team to learn as the project progressed we considered the socio technical aspects of the 1622wq application jakku and thorburn 2010 rose et al 2016 stitzlein and mooij 2019 the addition of a social research component allowed us to more appropriately justify shifts in priorities during the development of the application and identify structural and functional limitations i e the initial low levels of digital literacy and potential existing stakeholder conflict as they respond to different incentives jakku et al 2018 shepherd et al 2018 most importantly our social research and participatory design process provided grounds to recognise such issues and respond to them so enhancing impact by balancing the priorities of the different stakeholders involved in the application king et al 2019 4 3 1 social science considerations of the farmer and need for a monitoring framework due to existing knowledge of the context within which regional agricultural stakeholders operate what has been termed the digital divide in terms of lack of access to use and understanding of digital technologies rotz et al 2019 shepherd et al 2018 a co development approach was encouraged by social researchers advising the project team a combination of social research and human centred design and more specifically user experience were involved from the early stages of the project in an attempt to operationalise learning from existing research on agricultural technology development ditzler et al 2018 glover et al 2019 see fig 6 the challenge of innovation evaluation in such an impact focused project was addressed by embedded social research and development of a novel conceptual framework to capture changes in stakeholders perceptions of technology overtime the digi mast framework fielke et al 2020 this framework was built on the notion of digi grasping and the different engagements that stakeholders have with specific technologies at given points in time dufva and dufva 2019 digi mast characterises the level of people s understanding of a digital technology through four modes mystery m no knowledge of the relevant digital technology or dismissive if aware not interested in engaging further due to various factors aware a have heard of the technology but are still unsure of the implications and how it might influence their behaviour spark s see the potential value of the technology and wants to understand how it contributes in the specific social and physical context and transform t using the digital technology confidently and sharing it with others to change the way stakeholders do things following innovation system research protocol the social researcher conducted semi structured interviews with 12 farmers from the russell mulgrave catchment during okumah et al 2018 fig 6 of whom had been interviewed prior to the development of 1622wq and had previously interacted with the prototype see section 4 3 2 and fig 6 we also interviewed farm management advisors n 3 and researchers n 5 working in this region or scientific area to get a better understanding of the system into which the technology was to be deployed we undertook a total of 20 formal interviews this number is not unusual for highly qualitative methods fielke and wilson 2017 king et al 2019 thorburn et al 2011 turner et al 2020 based on the interview responses we assessed the mode of digi mast in which the interviewees were operating in relation to 1622wq most farmers were in one of the first two modes of digi mast mystery and aware with little evidence of them being in the spark or transform modes at that time on the other hand advisors and researcher responses fell in the spark and transform modes at the same point in time these results were in part due to pre existing barriers to uptake and the flow of the technology transfer approach specifically there were farmers that either did not have or want access to a smart phone or had limited internet service there were other barriers to uptake of the application related to data quality and design issues that were addressed in version 2 1 subsequently semi structured interviews using this version were held with 3 leading farmers and there was evidence of transition to the spark or transform modes confirming the benefits of our human centred design approach in addition the transition to the spark or transform modes was further supported by a project collaborator who indicated since its recent launch i am regularly contacted and queried by influential growers in my study catchment as to particular water quality events occurring in their local waterways often before i am even aware of them myself this development demonstrates a major paradigm shift in how farmers can engage with water quality monitoring in their local environments and one that cannot be understated 4 3 2 participatory user experience design process and plan to evaluate impact the 1622wq application development process followed a user experience design approach stitzlein and mooij 2019 to maximise the likely use of the application rose et al 2018 2016 social scientists and human computer interaction experts in the project team conducted both formal and informal user experience workshops and hallway testing testing by random individuals over a 20 month period fig 6 the design process started in 2017 when a context assessment was undertaken through structured interviews with 10 farmers from the russell mulgrave catchment to identify their desires and needs regarding water quality information these interviews confirmed their desire for readily accessible information and revealed the basic features the farmers wanted in the application such as the format of data display e g line graphs the ability to compare data across time and or locations and the preferences about information displayed on the main page these interviews also revealed that farmers were interested in rainfall data so that they could track farm performance and adjust management version 1 0 of the prototype prototype v1 0 was developed based on this feedback with a major development being that rainfall data was added as bar charts prototype v1 0 was then tested with a group of 7 farmers in this interaction we identified the need for a list of sensor locations to be presented to users as well as a map interface to facilitate the selection of the sensors prototype v1 1 incorporated these features and was released to a group of 12 farmers through one on one interactions in october 2018 google analytics use data and subsequent informal interviews 3 months later revealed that most of the 12 farmers had not accessed the prototype v1 1 and those that had did so rarely when asked for specific feedback on the prototype these farmers indicated that the two step registration activation process which involved contacting the app manager to register and then logging in to activate the account was a substantial barrier to its use they also specified the need for contextual information to help them interpret the data e g mean nitrate concentrations in addition latency when loading data and data errors e g negative nitrate values further diminished their interest in visualising the data version 1 0 of the application app v1 0 was developed in february 2019 fig 6 considering the feedback provided on the prototypes specifically app v1 0 was developed so that data could be visualised rapidly facilitated by the caching and architecture of the application and the need for the two step registration activation process was removed overcoming significant barriers that were present earlier also the filtering and infilling techniques described above were implemented in this version the app v1 0 also included the mean nitrate concentration for the past 30 days to help farmers interpret the data this information was provided below the charts rather than within the charts to avoid the addition of a legend that would make the visualisation of the data in mobile platforms less neat wireframes model for the next development of the application were designed then tested with a limited number of stakeholders 10 hallway testing the application was re designed based on this feedback app v2 0 additional hallway testing was undertaken and app v2 0 was updated to app v2 1 to improve the interaction with the map interface also at this time the 10 rainfall gauges were deployed in the catchment and data from these were include in the app v2 1 as requested by the farmers version 2 1 was completed in december 2019 the application was publicly released in january 2020 with coverage of the launch in print and television media in gbr catchments google analytics data showed there were 1100 users in the 5 months after the public launch compared with 1400 sugarcane farms in the region australian bureau of statistics abs 2010 with a bounce rate of 24 29 these use statistics starkly contrast the experience following the release of prototype v1 1 highlighting the benefit of the process leading to the design changes to overcome the barriers to uptake of the earlier versions 5 discussion improving water quality in agricultural catchments is a global priority mekonnen and hoekstra 2015 zhang et al 2015 it is crucial for farmers to be aware of water quality near their farms and understanding the influence of their farm management on water quality in order for them to willingly change their management practices glavan et al 2019 macgregor and warren 2006 further providing this information to farmers in real time will better show the link between farm management and water quality glavan et al 2019 and facilitate changed management okumah et al 2018 these are important objectives in our study area as improving farm management in catchments adjacent to the gbr is a critical action to protect this world heritage listed area de ath et al 2012 kroon et al 2016 yet as described in section 2 2 in water quality monitoring programs in the study region even in those specifically aiming to influence farmer behaviour there was a delay months to years between measurement and availability to farmers of water quality results sometimes the feedback to farmers was facilitated by project staff p25 c2c and wtmip projects section 2 2 other times information was published on a website of the gbrclmp that farmers could view if they both knew of its existence and were motivated the situation in our study region is not unusual current information systems for water quality in agricultural catchments mainly focus on evaluating progress towards water quality targets jones et al 2018 or on evaluating nutrient mitigation scenarios strömbäck et al 2019 they are not real time and or tailored to being accessed by farmers and thus are likely to be ineffective in increasing awareness of water quality among farmers there are few exceptions one being the nitrate app developed by deltares https www deltares nl en software nitrate app that presents results from nitrate test strips collected by farmers to encourage them to adopt best management practices however the reliance on nitrate test strips restricts the use of this system to 1 places where nitrate concentrations high enough to be within the detection limits of test strips and 2 locations where manual and thus infrequent sampling yields useful information neither of these pre conditions are met in our study area and thus water quality monitoring is done with sensors that have low detection limits and measure at high frequency e g hourly we have overcome the data access barriers with the 1622wq app which shows nitrate concentrations and other relevant information from high frequency measurement campaigns in real time the use statistics section 4 3 2 also show a clear demand for this information our evaluations section 4 3 1 further show that exposure to 16222wq and the data it displayed moved stakeholders along the digi mast framework fielke et al 2020 from mystery about the technology and data displayed to transform ation where they shared it with others to change their actions thus the 16222wq app clearly fulfills a necessary condition awareness of water quality for reducing farmers barriers to change practice to improve water quality after 1622wq has been in operation for longer periods its impact on farmers attitudes and their farm management will become clearer through information reported by the queensland government s paddock to reef integrated monitoring modelling and reporting program the government of queensland 2018 in developing the 16222wq app we instituted a deliberate design methodology to maximise the impact of the software to maximise the benefit from research investment new technologies need to be developed in concert with stakeholders rather than through purely top down technology transfer approaches where the social and design aspects are left unaccounted for glavan et al 2019 mcintosh et al 2007 we developed 1622wq a web based application that delivers real time high frequency data on stream nitrate to farmers as well as contextual information that facilitates the interpretation of the data these characteristics are needed to increase farmer awareness of the impacts of crop management on nitrogen pollution glavan et al 2019 jones et al 2018 however through stakeholder engagement we identified that there were barriers to farmers accessing the app that needed to be overcome before 1622wq could have meaningful impact the first barrier was related to the quality of the data data quality issues are common in sensor networks zhang et al 2019 and can occur because of breakdowns lack of sensor maintenance or a range of other reasons this issue is particularly important in our case study because the sensors are in remote areas e g up to 1000 km from a water quality technician so timely maintenance is not always feasible the second barrier was related to the design of the application the prototype required user registration and logging in to access the data which discouraged farmers from using the prototype farmers were also discouraged by the latencies in data loading times the last and most problematic barrier was the unease some farmers had in interfacing with digital technology while limited internet connection and or lack of the appropriate technology to support web applications e g smart phone may be part of the underlying problem we are also concerned about the possibility of a digital divide between urbanised and technology proficient locations and more regional or remote areas where traditional industries dominate rotz et al 2019 while the pre existing barriers to technological uptake e g lack of appropriate technology were not possible to address in this study data quality and design issues were there were various errors in the data output from the water quality sensors and correcting these data prior to end user presentation is important to build trust in the tools eastwood et al 2019 and thereby influence farmers behaviour especially if the new behaviour e g reducing fertiliser application rate carries a level of risk musvoto et al 2015 pannell 2017 to address data quality issues we created filtering algorithms that remove erroneous data and data imputation models zhang et al 2019 that fill data gaps these imputation algorithms were only implemented on a few water level data streams because many sensors in the application area had only been recently installed and did not have enough data to train the imputation models additionally only data gaps shorter than 1 day were infilled in 1622wq as imputation accuracy is strongly affected by the size of the data gaps cao et al 2018 tang et al 2019 our informal interactions with farmers lead us to believe that filtering and imputation will be accepted provided imputed data are clearly marked as farmers accept there can be problems with monitoring and appreciate transparency about data management future interactions will more formally evaluate farmers attitudes towards the pre processing of data design issues that limited the use of the prototype were also addressed specifically we designed the application so that no registration or logging in is required and keeping network requests to a minimum while we tested these changes on a small group of farmers the number of users since the launch of 1622wq 1000 in four months suggest these changes have facilitated scaling up of the technology future work will continue to ensure the application is accessible by and relevant to all farmers in the relevant locations not just the digital literate rotz et al 2019 to enhance the extent to which 1622wq can change farmer behaviour utilising a novel social impact framework in digi mast fielke et al 2020 rose and chilvers 2018 during the development of the application it became clear that the different water quality programs projects used different protocols to calibrate and maintain sensors and different protocols for assigning quality codes to data thus limiting comparison of results between programs projects these differences were revealed by bringing these data streams into the single platform provided by 1622wq to achieve a common data delivery platform for these disparate programs projects a community of practice was established for real time nitrate monitoring the main objective of this community was to develop agreed protocols and standards for sensor maintenance calibration and data quality coding that would enable the comparison of nitrate measurements across different projects thus the development of 1622wq was instrumental in ensuring consistency and promoting collaboration across the different programs the broader contribution of this technological case study 1622wq lies in clearly demonstrating the value of transdisciplinary research and in reducing the digital divide that creates diverging and unequal relationships between stakeholders and digital technologies salemink et al 2017 although this divide is widely recognized billon et al 2010 pant and hambly odame 2017 roberts et al 2017 examples like our work with close interaction between technological development social science and user experience highlight empirical methods that will help overcome at least some of the barriers discussed as such this paper supports the role of science openly combined in various innovative forms to increase impact in the form of individual practice change and ultimately the environmental sustainability of agriculture via digital technological development 6 conclusion this study describes the development of a web based application in which we used human centred design to identify the way in which farmers engage with and use technologies the human centred design as well as the social science component of the project enabled us to identify barriers to uptake and co develop the application with target end users we showed that failure to consider the systemic context within which new technologies are developed and adapted will prevent widespread use of technologies adoption and therefore decrease the likelihood of achieving water quality targets impact we also showed how the application through providing a common data delivery platform created a community or practice across disparate water quality programs and projects our work highlights the importance of integrating different disciplinary expertise over time and the potential for longer term added value created through collaborative approaches to technology development that are not possible in traditional technology transfer efforts software availability the 1622wq application is freely available through https wq 1622 farm declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we thank the water quality monitoring teams that shared our vision of democratising water quality data and willingly allowed us to access their data the various farmers that freely gave their feedback on the various stages of development we thank drs takuya iwanaga joachim rozemeijer philip smethurst and robert brown for their constructive comments on the paper we are also grateful to emma jakku and the csiro social dimensions project team for their ongoing advice during this work this work was supported by the csiro digiscape future science platform appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104816 
25993,intensive agricultural practices represent a major threat to aquatic ecosystems because they impair water quality however this can be ameliorated by farmers improving crop management provided they are aware of their contribution to declining water quality water quality information systems can increase farmer awareness but most were developed to assess water quality targets set in regulations rather than inform farmers we developed the 1622wq application using user centred design principles to provide farmers with real time information on nitrate and other contextual variables in their local creeks and rivers the design process identified barriers to uptake of the application such as a limited internet connection b poor data quality and c operational issues once these barriers were addressed there was substantial uptake nevertheless providing real time information to farmers is only part of the solution due to legacy issues caused by a digital divide between traditional industries and those that are digitally enabled keywords participatory water quality monitoring great barrier reef nitrogen sugarcane pollution human centred design 1 introduction the use of nitrogen fertilisers in agricultural systems has allowed farmers to increase crop production per unit of land sustaining increasing human populations zhang et al 2015 however a large fraction of the nitrogen applied in agricultural lands is lost to aquatic ecosystems mekonnen and hoekstra 2015 agriculture is responsible for most of the global nitrogen input to freshwater and marine ecosystems fowler et al 2013 mekonnen and hoekstra 2015 which represents a threat to water security via the eutrophication of water bodies elser et al 2007 groundwater degradation rosenstock et al 2014 thorburn et al 2003 and increased water treatment costs mcdonald et al 2016 climate change is expected to intensify nitrogen pollution due to increased frequency and intensity of extreme weather events jeppesen et al 2011 to protect aquatic ecosystems from nitrogen and other agricultural pollutants governments have introduced regulations that define targets for water quality kroon et al 2016 nainggolan et al 2018 national research council 2010 van grinsven et al 2012 achieving these targets requires farmers changing crop management so that nitrogen losses are minimised kroon et al 2016 nainggolan et al 2018 wulff et al 2014 voluntary actions have been shown to lead to long lasting behavioural change ayer 1997 because of the complexities associated with regulatory approaches bohman 2018 farmer behaviour can be influenced by a range of factors such as economic incentives legal requirements perceptions personal factors and beliefs gachango et al 2015 greiner and gregg 2011 pannell 2017 taylor and eberhard 2020 for farmers to engage in voluntary actions to improve water quality they need to be aware of the link between crop management and water quality glavan et al 2019 macgregor and warren 2006 unfortunately farmers are often unaware of their contribution to declining water quality benn et al 2010 glavan et al 2019 macgregor and warren 2006 okumah et al 2018 or are sceptical about the impact of agriculture on the environment macgregor and warren 2006 pannell 2017 stuart et al 2014 despite this evidence many targets for water and land management assume that impacts from farming practices are well understood by farmers blackstock et al 2010 clearly for these targets to be reached farmers need to be more aware of the link between farming practices and water quality barnes et al 2009 and there needs to be better dissemination of water quality information to farmers gachango et al 2015 several nitrate water quality information systems exist such as the land air and water aotearoa lawa nz https www lawa org nz the iowa water quality information system iwqis https iwqis iowawis org the usgs water quality information system https www usgs gov products data and tools real time data water and the eyes on the chesapeake bay http eyesonthebay dnr maryland gov these systems were primarily developed to assess water quality improvements coming from government programs and report results to governments and the public jones et al 2018 rather than raise awareness among farmers a different approach is needed to maximise the impact of this information on farmers awareness mcintosh et al 2007 specifically to help farmers understand the link between crop management and nitrogen pollution water quality information systems need to be real time high frequency and provide contextual information that can assist farmers interpret the data glavan et al 2019 jones et al 2018 real time water quality information systems can facilitate the immediate evaluation of the impact of a recent farming practice e g fertiliser application on water quality moreover because discharge i e triggered by rainfall may increase nitrogen loads in running waters over short periods jones et al 2018 high frequency monitoring systems are needed to demonstrate the link between crop management river discharge and nitrogen pollution in addition contextual information e g rainfall can help farmers understand the connection between weather events and nitrogen pollution with the aim of increasing awareness of the impact of cropping on water quality we developed 1622tmwq a web based application that provides real time data on stream water quality we utilised principles from social science and human centred design in an effort to maximise the uptake of the application jakku and thorburn 2010 rose et al 2016 stitzlein and mooij 2019 ultimately changes resulting from use of the technology and sharing of the lessons with peer networks of farmers is hoped to increase the likelihood of achieving water quality targets glavan et al 2019 our contribution to knowledge is to describe the pathway of the development process of the application through to descriptions of stakeholder interaction to evaluate the extent to which 1622wq increased farmer awareness as such we report on attempts to mitigate issues of poor internet connectivity lack of engagement with technologies in general and poor data quality to emphasise lessons for technology development aiming to achieve impact 2 study site 2 1 regional setting our study site comprises some of the river catchments that discharge into the great barrier reef gbr world heritage listed site particularly sugarcane farming in the russel mulgrave johnston and tully catchments of the wet tropics fig 1 river flows in these catchments are characterised by high intensity flood events during the wet season november april which lead to substantial export of nutrients pesticides and sediments to the gbr mitchell et al 1997 across all catchments discharging into the gbr ecosystems agricultural land use is the major source of nutrients pesticides and sediments kroon et al 2016 grazing systems occupy 80 of the area and account for most of the sediments loads to the gbr waterhouse et al 2012 cropping systems banana sugarcane occupy 5 or the area but account for most of the dissolved inorganic nitrogen din nitrate nitrite ammonium discharged from the catchment 14 940 t year 1 napel et al 2019 among cropping systems sugarcane is the main contributor to din load on an annual basis thorburn et al 2013 thorburn and wilkinson 2013 waterhouse et al 2012 because it is a widespread crop 40 of cropping area grown in areas of high rainfall 1000 4000 mm year 1 and it receives large amounts 130 200 kg n ha 1 of synthetic nitrogen fertiliser thorburn et al 2013 high din availability in coastal waters promotes outbreaks of the coral eating crown of thorns starfish responsible for much of the decline in coral de ath et al 2012 to protect the gbr the australian and queensland governments released the reef 2050 long term sustainability plan which sets a target of 60 reduction of anthropogenic end of catchment din loads by 2025 commonwealth of australia 2018 to achieve this target farmers in the gbr catchments need to alter crop management so that nitrogen losses to the environment are minimised these could include reducing n fertiliser application rates or replacing conventional fertilisers with ones that better synchronise n supply with crop uptake thorburn et al 2017 despite considerable investment by governments in incentives to promote practice change kroon et al 2016 the adoption of improved practices has been very slow the government of queensland 2018 innovative ways of promoting farming practice change have been called for great barrier reef water science taskforce department of environment and heritage protection 2016 we recognise that agricultural innovation is not only about adopting new digital technologies but requires collaboration between scientists and stakeholders throughout the innovation process and through the development of new technologies jakku and thorburn 2010 klerkx et al 2012 as such the design and development of 1622wq was guided by interactions with farmers for practical reasons we focused on the russell mulgrave catchment fig 1 2 2 water quality monitoring within the study area there are a number of water quality monitoring efforts the most extensive is the great barrier reef catchment loads monitoring program gbrclmp a queensland government program that collects data across multiple catchments to track long term trends in water quality entering the great barrier reef lagoon to both validate catchment water quality models and track changes in water quality relative to government water quality targets https www reefplan qld gov au there are also projects aiming to inform farmers about variation in water quality in specific localities and how that relates to their farm the first project established was project 25 farmers water quality and on farm decision making hereafter referred to as p25 davis 2019 a farmer led water quality monitoring program located in the russell mulgrave catchment fig 1 the second was the cane to creek c2c project an extension officer led project working with farmers to understand water quality in the figtree creek sub catchment of the mulgrave river catchment billing 2017 the final project is the wet tropics major integrated project wetmip https terrain org au projects wet tropics major integrated project measuring water quality at two locations in the tully river catchment and one location in the johnstone river catchment fig 1 prior to collaboration with our developments described below water quality data collected in these projects was provided to farmers by project staff at individual or group meetings the time between collection and these meetings were in the order of months with the gbrclmp data are made publicly available through reports napel et al 2019 and an interactive story map http qgsp maps arcgis com apps mapseries index html appid 9d1aad1e2b444ec6a1890e4032284147 the delay between collection and display is in the order of years 3 1622wq application the 1622wq application is named after the highest mountain in queensland australia which is 1622 m in height 3 1 data sources and storage high frequency nitrate concentrations no3 n mg n l 1 were obtained from opus and nico sensors trios https www trios de en sensors html accuracy 5 0 1 mg l 1 no3 n in deployed in tidal and non tidal waters respectively in the water quality monitoring programs and projects described above other parameters are also monitored in these programs river or creek height m being of particular relevance here to date the gbrclmp program monitors 10 of the 35 river catchments draining into the gbr through 11 real time stations measuring water level and or water quality variables including nitrate the water sampling intervals range from 10 to 60 min data from the program are stored in a cloud based platform named eagle io https eagle io where water flow m3 s 1 and n loads t d 1 are calculated for the two locations that have reliable rating curves tully river at euramo and tully gorge national park as well as water level and nitrate concentration data all data from this program are referred to as queensland government in the app p25 has to date deployed four monitoring stations collecting nitrate concentrations every 60 min in the c2c project nitrate concentration is collected every 60 min at one monitoring station in the wetmip project nitrate concentrations are recorded every 30 min at two locations in the tully river catchment and one location in the johnstone river catchment data from these projects are also stored in cloud based platforms including eagle io they are identified in the app by their project names our early user research described below in section 4 3 2 highlighted that farmers in gbr catchments saw rainfall as an important factor in both their farm performance and management and the driver of nitrogen losses from their farm thus rainfall data is also integrated into the 1622wq application to enable farmers to establish the link between rainfall fertiliser application and nitrogen pollution rainfall data are obtained from the australian bureau of meteorology http www bom gov au which collects rainfall data every 60 min on the gbr catchments in addition 10 rainfall gauges were deployed by the project team in the russell mulgrave catchment collecting data every 10 min all data streams are stored in eagle io 3 2 prototype different versions of the 1622wq prototype were developed using the shiny framework https shiny rstudio com an open source r package that provides a web framework for building web applications using r https cran r project org shiny facilitates turning analyses into interactive web applications without requiring html css or javascript knowledge beeley 2013 prototypes were deployed on the shinyapps io platform https www shinyapps io and access was restricted by user identification preventing anonymous visitors from accessing the application the user interface consisted of three tabs see supplementary material figures a1 3 1 a locn tab that displays a list of all locations and allows users to filter and select locations 2 a map tab that allows users to select a location and 3 a data tab that displays data streams for the selected location the prototype obtained data from csv files and did not leverage any form of caching which caused latencies in data loading times as described in section 4 3 social science and user experience research identified that the latencies in data loading times and the need for a two step user registration discouraged farmers from using the application these barriers for technological uptake were exacerbated by poor internet connectivity the additional complexity needed for addressing these issues meant that it was more effective to transition from shiny to a single page application spa 3 3 application 3 3 1 description the application accessible through https wq 1622 farm was designed as an online mobile first single page application spa with no installations required on desktop or mobile allowing us to partially address internet connectivity issues by keeping network requests to a minimum internet connectivity issues were also alleviated by allowing for local caching of data streams and locations as previously indicated the complexity of registration and logging in acted as a barrier to uptake thus the web based application was designed so that publicly available data streams are accessible without the need for registration registration and login systems were continued to access private data streams which are password protected when opening the application the user is presented with an annotated map displaying the locations where data streams are available fig 2 when a location is selected some key figures i e latest nitrate concentration load are displayed the user can browse and search specific locations by clicking on the locations button located in the top navigation area fig 2 locations can be bookmarked by clicking on the favourite button allowing for quick data access in future interactions fig 3 when selecting a location charts displaying data streams for that location are shown on desktop both the charts and the map are visible to facilitate navigation fig 3 on mobile an exit button was implemented to return to the map see supplementary material figure a4 the charts are easy to navigate i e scrolling on desktop or pinching on mobile readjusts the time span to facilitate the interpretation of the data contextual information such as yearly and monthly average values are also displayed fig 3 3 3 2 architecture the 1622wq application was developed as a react spa with a django microservice based backend https www djangoproject com and a postgresql database https www postgresql org these were implemented for speed and utilize all open source frameworks and libraries fig 4 the django microservices expose authorized access to the data index stored in a postgresql database the backend consists of a a data service that acts as an intermediary between the frontend the database and eagle io b a discovery service that retrieves a list of data streams from eagle io and c an access service that manages access to the data streams the data service checks that the user has permission to access the data and requests data from eagle io by searching for the details stored in the data index a management system allows administrators to manage data index permissions through the 1622wq management interface as well as import new data sources into the index through the discovery service interface the discovery service crawls eagle io for new data sources and displays them through an interface where administrators can select the new data sources to be added to the data index once data sources are added the access service can be used to restrict access to users or groups of users data filtering and infilling is conducted in amazon web service aws lambda see section 4 1 and 4 2 the web based interface was written in typescript utilising the react javascript library https reactjs org and redux framework https redux js org for state management allowing for fast page loading times the map was managed through the leaflet library https leafletjs com and the imagery obtained from mapbox https www mapbox com the interactive charting library echarts https www echartsjs com was used for displaying time series of the data streams 4 testing and evaluation 4 1 erroneous data errors and anomalies are commonly found in real time water quality data zhang et al 2019 erroneous measurements are a result of sensor malfunction lack of sensor maintenance physical interference with the sensors communication failures among others this is a particularly acute problem for water quality monitoring in gbr catchments the sensors accessed for 1622wq are commonly deployed amongst vegetation on river or creek banks in remote locations e g up to 1000 km from a water quality technician in these settings debris from vegetation often causes physical interference with the sensors which can take some time to repair a malfunctioning sensor erroneous measurements described below make it difficult to interpret the data and can also lead to lack of confidence by stakeholders in the measurements to tackle these issues we introduced a process to filter out obviously erroneous data the most common problems with nitrate data that we encountered were 1 the presence of negative values and 2 unrealistically rapid change in nitrate concentrations negative values were removed by the data filter changes in nitrate concentrations that exceeded the 98 quantile for a specific location were also considered unrealistic for the environments being monitored and removed helsel and hirsch 2002 in addition to data issues we identified problems with the sensors related to the need for inspection and maintenance of the sensors these problems can be recognized through reference variables related to light intensity which are implemented by the sensor manufacturers and indicate that the quality of the measurement is compromised trios optical sensors 2019a 2019b in our data set we removed nitrate values when the sensor reference variables were below or above the thresholds defined by the sensor manufacturers we also removed data that had been labelled as erroneous data by the water quality program project managers the filtering system is implemented in aws using the architecture shown in fig 5 firstly individual data streams are retrieved from eagle io using the http api and cache into aws then the three filtering modules are applied sequentially using the aws lambda service finally the filtered data streams are written back to eagle io filtering removes values from the data stream small gaps less than 5 data points missing created by the filtering algorithms are filled by linear interpolation which is implemented in the filtering module however linear interpolation is not appropriate when multiple data points are missing and a more sophisticated infilling method described below is used 4 2 missing data large gaps more than 5 data points in data streams can be produced during the filtering e g as a result of sensor malfunction or due to network communication outage multiple approaches have been developed to infill gaps in the data box et al 2013 pankratz 1983 however they usually give poor estimates when applied to large data gaps thus to infill large gaps we implemented a new sequence to sequence imputation model ssim based on a deep neural network zhang et al 2019 the ssim uses the state of the art sequence to sequence deep learning architecture and the long short term memory network lstm to utilize information that varies over time the model provides superior performance in recovering missing data sequences reducing a range of error metrics root mean square error mean absolute error mean absolute percentage error and symmetric mean absolute percentage error by 70 98 compared with six established data infilling techniques arima seasonal arima matrix factorization multivariate imputation by chained equations and expectation maximization zhang et al 2019 the ssim model has been implemented for water level streams in the three locations where it was tested russel river mulgrave river at deeral and johnstone river we did not implement the infilling algorithm on nitrate data streams as there are challenges associated with infilling data streams with large gaps we intend to expand the functionality to other sensors and data streams as more data becomes available and deep learning models are better developed 4 3 social science and user experience for the research and design team to learn as the project progressed we considered the socio technical aspects of the 1622wq application jakku and thorburn 2010 rose et al 2016 stitzlein and mooij 2019 the addition of a social research component allowed us to more appropriately justify shifts in priorities during the development of the application and identify structural and functional limitations i e the initial low levels of digital literacy and potential existing stakeholder conflict as they respond to different incentives jakku et al 2018 shepherd et al 2018 most importantly our social research and participatory design process provided grounds to recognise such issues and respond to them so enhancing impact by balancing the priorities of the different stakeholders involved in the application king et al 2019 4 3 1 social science considerations of the farmer and need for a monitoring framework due to existing knowledge of the context within which regional agricultural stakeholders operate what has been termed the digital divide in terms of lack of access to use and understanding of digital technologies rotz et al 2019 shepherd et al 2018 a co development approach was encouraged by social researchers advising the project team a combination of social research and human centred design and more specifically user experience were involved from the early stages of the project in an attempt to operationalise learning from existing research on agricultural technology development ditzler et al 2018 glover et al 2019 see fig 6 the challenge of innovation evaluation in such an impact focused project was addressed by embedded social research and development of a novel conceptual framework to capture changes in stakeholders perceptions of technology overtime the digi mast framework fielke et al 2020 this framework was built on the notion of digi grasping and the different engagements that stakeholders have with specific technologies at given points in time dufva and dufva 2019 digi mast characterises the level of people s understanding of a digital technology through four modes mystery m no knowledge of the relevant digital technology or dismissive if aware not interested in engaging further due to various factors aware a have heard of the technology but are still unsure of the implications and how it might influence their behaviour spark s see the potential value of the technology and wants to understand how it contributes in the specific social and physical context and transform t using the digital technology confidently and sharing it with others to change the way stakeholders do things following innovation system research protocol the social researcher conducted semi structured interviews with 12 farmers from the russell mulgrave catchment during okumah et al 2018 fig 6 of whom had been interviewed prior to the development of 1622wq and had previously interacted with the prototype see section 4 3 2 and fig 6 we also interviewed farm management advisors n 3 and researchers n 5 working in this region or scientific area to get a better understanding of the system into which the technology was to be deployed we undertook a total of 20 formal interviews this number is not unusual for highly qualitative methods fielke and wilson 2017 king et al 2019 thorburn et al 2011 turner et al 2020 based on the interview responses we assessed the mode of digi mast in which the interviewees were operating in relation to 1622wq most farmers were in one of the first two modes of digi mast mystery and aware with little evidence of them being in the spark or transform modes at that time on the other hand advisors and researcher responses fell in the spark and transform modes at the same point in time these results were in part due to pre existing barriers to uptake and the flow of the technology transfer approach specifically there were farmers that either did not have or want access to a smart phone or had limited internet service there were other barriers to uptake of the application related to data quality and design issues that were addressed in version 2 1 subsequently semi structured interviews using this version were held with 3 leading farmers and there was evidence of transition to the spark or transform modes confirming the benefits of our human centred design approach in addition the transition to the spark or transform modes was further supported by a project collaborator who indicated since its recent launch i am regularly contacted and queried by influential growers in my study catchment as to particular water quality events occurring in their local waterways often before i am even aware of them myself this development demonstrates a major paradigm shift in how farmers can engage with water quality monitoring in their local environments and one that cannot be understated 4 3 2 participatory user experience design process and plan to evaluate impact the 1622wq application development process followed a user experience design approach stitzlein and mooij 2019 to maximise the likely use of the application rose et al 2018 2016 social scientists and human computer interaction experts in the project team conducted both formal and informal user experience workshops and hallway testing testing by random individuals over a 20 month period fig 6 the design process started in 2017 when a context assessment was undertaken through structured interviews with 10 farmers from the russell mulgrave catchment to identify their desires and needs regarding water quality information these interviews confirmed their desire for readily accessible information and revealed the basic features the farmers wanted in the application such as the format of data display e g line graphs the ability to compare data across time and or locations and the preferences about information displayed on the main page these interviews also revealed that farmers were interested in rainfall data so that they could track farm performance and adjust management version 1 0 of the prototype prototype v1 0 was developed based on this feedback with a major development being that rainfall data was added as bar charts prototype v1 0 was then tested with a group of 7 farmers in this interaction we identified the need for a list of sensor locations to be presented to users as well as a map interface to facilitate the selection of the sensors prototype v1 1 incorporated these features and was released to a group of 12 farmers through one on one interactions in october 2018 google analytics use data and subsequent informal interviews 3 months later revealed that most of the 12 farmers had not accessed the prototype v1 1 and those that had did so rarely when asked for specific feedback on the prototype these farmers indicated that the two step registration activation process which involved contacting the app manager to register and then logging in to activate the account was a substantial barrier to its use they also specified the need for contextual information to help them interpret the data e g mean nitrate concentrations in addition latency when loading data and data errors e g negative nitrate values further diminished their interest in visualising the data version 1 0 of the application app v1 0 was developed in february 2019 fig 6 considering the feedback provided on the prototypes specifically app v1 0 was developed so that data could be visualised rapidly facilitated by the caching and architecture of the application and the need for the two step registration activation process was removed overcoming significant barriers that were present earlier also the filtering and infilling techniques described above were implemented in this version the app v1 0 also included the mean nitrate concentration for the past 30 days to help farmers interpret the data this information was provided below the charts rather than within the charts to avoid the addition of a legend that would make the visualisation of the data in mobile platforms less neat wireframes model for the next development of the application were designed then tested with a limited number of stakeholders 10 hallway testing the application was re designed based on this feedback app v2 0 additional hallway testing was undertaken and app v2 0 was updated to app v2 1 to improve the interaction with the map interface also at this time the 10 rainfall gauges were deployed in the catchment and data from these were include in the app v2 1 as requested by the farmers version 2 1 was completed in december 2019 the application was publicly released in january 2020 with coverage of the launch in print and television media in gbr catchments google analytics data showed there were 1100 users in the 5 months after the public launch compared with 1400 sugarcane farms in the region australian bureau of statistics abs 2010 with a bounce rate of 24 29 these use statistics starkly contrast the experience following the release of prototype v1 1 highlighting the benefit of the process leading to the design changes to overcome the barriers to uptake of the earlier versions 5 discussion improving water quality in agricultural catchments is a global priority mekonnen and hoekstra 2015 zhang et al 2015 it is crucial for farmers to be aware of water quality near their farms and understanding the influence of their farm management on water quality in order for them to willingly change their management practices glavan et al 2019 macgregor and warren 2006 further providing this information to farmers in real time will better show the link between farm management and water quality glavan et al 2019 and facilitate changed management okumah et al 2018 these are important objectives in our study area as improving farm management in catchments adjacent to the gbr is a critical action to protect this world heritage listed area de ath et al 2012 kroon et al 2016 yet as described in section 2 2 in water quality monitoring programs in the study region even in those specifically aiming to influence farmer behaviour there was a delay months to years between measurement and availability to farmers of water quality results sometimes the feedback to farmers was facilitated by project staff p25 c2c and wtmip projects section 2 2 other times information was published on a website of the gbrclmp that farmers could view if they both knew of its existence and were motivated the situation in our study region is not unusual current information systems for water quality in agricultural catchments mainly focus on evaluating progress towards water quality targets jones et al 2018 or on evaluating nutrient mitigation scenarios strömbäck et al 2019 they are not real time and or tailored to being accessed by farmers and thus are likely to be ineffective in increasing awareness of water quality among farmers there are few exceptions one being the nitrate app developed by deltares https www deltares nl en software nitrate app that presents results from nitrate test strips collected by farmers to encourage them to adopt best management practices however the reliance on nitrate test strips restricts the use of this system to 1 places where nitrate concentrations high enough to be within the detection limits of test strips and 2 locations where manual and thus infrequent sampling yields useful information neither of these pre conditions are met in our study area and thus water quality monitoring is done with sensors that have low detection limits and measure at high frequency e g hourly we have overcome the data access barriers with the 1622wq app which shows nitrate concentrations and other relevant information from high frequency measurement campaigns in real time the use statistics section 4 3 2 also show a clear demand for this information our evaluations section 4 3 1 further show that exposure to 16222wq and the data it displayed moved stakeholders along the digi mast framework fielke et al 2020 from mystery about the technology and data displayed to transform ation where they shared it with others to change their actions thus the 16222wq app clearly fulfills a necessary condition awareness of water quality for reducing farmers barriers to change practice to improve water quality after 1622wq has been in operation for longer periods its impact on farmers attitudes and their farm management will become clearer through information reported by the queensland government s paddock to reef integrated monitoring modelling and reporting program the government of queensland 2018 in developing the 16222wq app we instituted a deliberate design methodology to maximise the impact of the software to maximise the benefit from research investment new technologies need to be developed in concert with stakeholders rather than through purely top down technology transfer approaches where the social and design aspects are left unaccounted for glavan et al 2019 mcintosh et al 2007 we developed 1622wq a web based application that delivers real time high frequency data on stream nitrate to farmers as well as contextual information that facilitates the interpretation of the data these characteristics are needed to increase farmer awareness of the impacts of crop management on nitrogen pollution glavan et al 2019 jones et al 2018 however through stakeholder engagement we identified that there were barriers to farmers accessing the app that needed to be overcome before 1622wq could have meaningful impact the first barrier was related to the quality of the data data quality issues are common in sensor networks zhang et al 2019 and can occur because of breakdowns lack of sensor maintenance or a range of other reasons this issue is particularly important in our case study because the sensors are in remote areas e g up to 1000 km from a water quality technician so timely maintenance is not always feasible the second barrier was related to the design of the application the prototype required user registration and logging in to access the data which discouraged farmers from using the prototype farmers were also discouraged by the latencies in data loading times the last and most problematic barrier was the unease some farmers had in interfacing with digital technology while limited internet connection and or lack of the appropriate technology to support web applications e g smart phone may be part of the underlying problem we are also concerned about the possibility of a digital divide between urbanised and technology proficient locations and more regional or remote areas where traditional industries dominate rotz et al 2019 while the pre existing barriers to technological uptake e g lack of appropriate technology were not possible to address in this study data quality and design issues were there were various errors in the data output from the water quality sensors and correcting these data prior to end user presentation is important to build trust in the tools eastwood et al 2019 and thereby influence farmers behaviour especially if the new behaviour e g reducing fertiliser application rate carries a level of risk musvoto et al 2015 pannell 2017 to address data quality issues we created filtering algorithms that remove erroneous data and data imputation models zhang et al 2019 that fill data gaps these imputation algorithms were only implemented on a few water level data streams because many sensors in the application area had only been recently installed and did not have enough data to train the imputation models additionally only data gaps shorter than 1 day were infilled in 1622wq as imputation accuracy is strongly affected by the size of the data gaps cao et al 2018 tang et al 2019 our informal interactions with farmers lead us to believe that filtering and imputation will be accepted provided imputed data are clearly marked as farmers accept there can be problems with monitoring and appreciate transparency about data management future interactions will more formally evaluate farmers attitudes towards the pre processing of data design issues that limited the use of the prototype were also addressed specifically we designed the application so that no registration or logging in is required and keeping network requests to a minimum while we tested these changes on a small group of farmers the number of users since the launch of 1622wq 1000 in four months suggest these changes have facilitated scaling up of the technology future work will continue to ensure the application is accessible by and relevant to all farmers in the relevant locations not just the digital literate rotz et al 2019 to enhance the extent to which 1622wq can change farmer behaviour utilising a novel social impact framework in digi mast fielke et al 2020 rose and chilvers 2018 during the development of the application it became clear that the different water quality programs projects used different protocols to calibrate and maintain sensors and different protocols for assigning quality codes to data thus limiting comparison of results between programs projects these differences were revealed by bringing these data streams into the single platform provided by 1622wq to achieve a common data delivery platform for these disparate programs projects a community of practice was established for real time nitrate monitoring the main objective of this community was to develop agreed protocols and standards for sensor maintenance calibration and data quality coding that would enable the comparison of nitrate measurements across different projects thus the development of 1622wq was instrumental in ensuring consistency and promoting collaboration across the different programs the broader contribution of this technological case study 1622wq lies in clearly demonstrating the value of transdisciplinary research and in reducing the digital divide that creates diverging and unequal relationships between stakeholders and digital technologies salemink et al 2017 although this divide is widely recognized billon et al 2010 pant and hambly odame 2017 roberts et al 2017 examples like our work with close interaction between technological development social science and user experience highlight empirical methods that will help overcome at least some of the barriers discussed as such this paper supports the role of science openly combined in various innovative forms to increase impact in the form of individual practice change and ultimately the environmental sustainability of agriculture via digital technological development 6 conclusion this study describes the development of a web based application in which we used human centred design to identify the way in which farmers engage with and use technologies the human centred design as well as the social science component of the project enabled us to identify barriers to uptake and co develop the application with target end users we showed that failure to consider the systemic context within which new technologies are developed and adapted will prevent widespread use of technologies adoption and therefore decrease the likelihood of achieving water quality targets impact we also showed how the application through providing a common data delivery platform created a community or practice across disparate water quality programs and projects our work highlights the importance of integrating different disciplinary expertise over time and the potential for longer term added value created through collaborative approaches to technology development that are not possible in traditional technology transfer efforts software availability the 1622wq application is freely available through https wq 1622 farm declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we thank the water quality monitoring teams that shared our vision of democratising water quality data and willingly allowed us to access their data the various farmers that freely gave their feedback on the various stages of development we thank drs takuya iwanaga joachim rozemeijer philip smethurst and robert brown for their constructive comments on the paper we are also grateful to emma jakku and the csiro social dimensions project team for their ongoing advice during this work this work was supported by the csiro digiscape future science platform appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104816 
25994,there are a many reasons for creating quantitative models of deforestation supported by a variety of modelling techniques for doing so we examine the suitability of four different modelling techniques for predicting deforestation bayesian networks bns artificial neural networks anns gaussian processes gps and generalised linear mixed models glmms the analysis is provided in the context of the verified carbon standard approved methodology for avoided unplanned deforestation to illustrate specific examples of scenarios where each technique would be suitable anns potentially improve predictions over glmms while gps were better able to generalise to other geographic areas anns were sensitive to design decisions which may limit their applications where results are required within short time frames bns easily facilitate scenario analysis but are also sensitive to design decisions with machine learning becoming easier each day we encourage researchers to consider carefully which technique is suitable for their situation prior to beginning implementation keywords machine learning deforestation generalized linear mixed models bayesian networks artificial neural networks 1 introduction assessing deforestation risk requires an understanding of complex phenomena involving many different and interrelated variables while statistical techniques have traditionally been the tool of choice machine learning ml techniques are now gaining popularity this shift is supported by a wide range of techniques jakeman et al 2006 kelly et al 2013 and modelling tools soares filho et al 2002 hallgren et al 2016 however not every technique will necessarily meet the requirements of every analysis which will vary depending on the overall purpose of the study be it protected area pa management planning robinson and lokina 2011 implementing a scheme for reduced emissions from avoided deforestation and degradation redd di lallo et al 2017 or analysing biodiversity offset schemes virah sawmy et al 2014 for example protected area management studies may be looking to either evaluate the effectiveness of a pa by comparing against some counterfactual scenario mas 2005 or predict which areas are most at risk allnutt et al 2013 a data model for a redd scheme simonet et al 2019 on the other hand may be designed primarily as a decision support tool to establish a deforestation baseline for calculating payments regardless of the purpose deforestation studies generally share one or some combination of three specific objectives determining the factors that affect deforestation freitas et al 2010 wyman and stein 2010 predicting where deforestation will occur müller and mburu 2009 gómez ossa and botero fernández 2017 and predicting how much forest will be lost singh et al 2017 factors affecting deforestation can be examined on several levels in some instances an assessment of the factors that predispose an area to deforestation such as road density freitas et al 2010 soil type or rainfall bax and francesconi 2018 may be appropriate whereas other situations may require measures of direct causes such as the potential of the area for fuel wood extraction geist and lambin 2001 predicting the amount of forest loss either for a business as usual bau or for some counterfactual scenario is often more difficult than predicting the location of deforestation because deforestation rates are often influenced by economic and political factors such as international treaties or local policies veldkamp and lambin 2001 these factors are often outside of the model scope and are liable to change within the study period each modelling technique has different strengths and weaknesses when addressing these three objectives in addition organisations such as governments environmental groups non governmental organisations ngos and private enterprise will each have differing motives expertise resources and regulations which will restrict which techniques can or should be implemented constraints can also potentially arise from the design and specifications of the study including the content and structure of the data and the size of the datasets in particular data imbalance haibo and garcia 2009 where one data class is much rarer than another may disadvantage techniques that require a large number of examples to learn from this is especially relevant in areas with low rates of deforestation this paper compares the suitability of four data analysis techniques for two common objectives of deforestation studies predicting the location of areas at risk of deforestation and analysing the variables that drive this risk three ml techniques are considered bayesian networks bns artificial neural networks anns and gaussian processes gps as well as a common statistical method generalised linear mixed models glmms results evaluating the usefulness of the datasets and the accuracy of the modelling technique have been published previously mayfield et al 2017 here we extend this work by placing each technique in the context of a real world decision making situation using the verified carbon standard vcs as a case study vcs 2012 we consider the impact of both user capacity and the study constraints on the suitability of each method fig 1 by also illustrating each technique with a detailed example this analysis is designed to assist practitioners with little or no experience in ml to decide which if any of the listed techniques are appropriate to their situation 1 1 verified carbon standard case study the example of the verified carbon standard vcs approved methodology for avoided unplanned deforestation serves as a good illustration of how different techniques suit different situations the vcs details the accepted procedure for conducting an analysis prior to the introduction of a redd program vcs 2012 the methodology outlines several distinct stages in the process step 1 definition of spatial and temporal boundaries carbon pools and sources of emissions step 2 analysis of historical land cover change base rate step 3 analysis of agents drivers and underlying causes of deforestation and their likely future development step 4 projection of future deforestation allows for either a function approach extrapolating historical trends using regression techniques or a modelling approach modelling deforestation as a function of the driver variables includes four steps o preparation of factor maps o preparation of risk maps for deforestation o selection of the most accurate deforestation risk map o mapping locations of future deforestation step 5 definition of the land use and land cover change component of the baseline data analysis is an essential tool for steps 2 through to 5 but the requirement for the analysis will differ at each step for example in step 3 a method is needed that allows for a comparison of the hierarchy of importance for the drivers and underlying causes of the deforestation mapping future deforestation in step 4 would not necessarily require this interpretability rather focusing on the accuracy of predicting at risk locations 1 2 machine learning in deforestation statistical regression models are commonly used for analysing deforestation and have been previously implemented for examining the drivers of deforestation freitas et al 2010 wyman and stein 2010 predicting the magnitude of deforestation rosa et al 2013 and examining the effectiveness of protected areas in avoiding deforestation green et al 2013 more advanced ml models are also becoming widely applied anns have been successfully used to forecast deforestation hotspots mas et al 2004 müller and mburu 2009 gómez ossa and botero fernández 2017 singh et al 2017 predict the location of forest fire maeda et al 2009 and more generally to model land use change tayyebi et al 2014 samardzic petrovic et al 2017 advances in deep learning have also shown particular promise for predicting high risk areas ye et al 2019 other ml classifiers that have been used to model land use change include support vector machines samardzic petrovic et al 2017 regression trees tayyebi et al 2014 random forests bax and francesconi 2018 and bns silva et al 2020 bns designed by experts rather than learnt solely from quantitative data have also been used to integrate stakeholder knowledge and preferences for modelling drivers of land use change celio et al 2014 specialised methods from other fields such as presence absence models used in species distribution are also being re purposed to model deforestation distribution de souza and de marco 2018 comparisons of multiple methods often report only model performance and with little context of how each might be applicable in different decision making scenarios pérez vega et al 2012 tayyebi et al 2014 samardzic petrovic et al 2017 exceptions to this include studies that also report on comprehensibility and method intricacy kampichler et al 2010 and model calibration time rodrigues and de la riva 2014 this study helps to address this gap by discussing the models in terms of both performance and the practical aspects of their implementation the three ml techniques chosen represent different model families as identified in jakeman et al 2006 anns are used as an example of empirical data based models bns are used as an example of conceptual models and gps are used as an example of spatial based models glmms are included as a representative statistical technique based on results reported in mayfield et al 2017 generalised linear models glms are regression models that estimate a single coefficient for each predictor variable and combine these using a covariance function to predict the value of the dependant variable hastie et al 2009 interactions between the predictor variables are not taken into account unless they are explicitly specified in the model glmms build on glms by allowing certain variables to be modelled as random rather than fixed effects meaning that its coefficient is defined by a probability distribution defining a variable as a random effect accounts for levels within a dataset and is often included to account for spatial autocorrelation within the model green et al 2013 stepwise procedures that systematically remove variables to find more parsimonious models are available in standard r packages such as mass venables and ripley 2002 and can be used to address issues with collinearity between predictor variables options also exist to run glms that will attempt to find significant interactions between predictors calcagno 2013 however these are both computationally expensive restricted to linear relationships between a small subset of predictors and do not always improve model performance mayfield et al 2017 anns are a non linear extension of glms which learn relationships in the data by repeatedly passing values through a network of nodes and links haykin 2009 ann structure is defined by the number of layers in the network and the number of nodes in each layer fig 2 the most common network structure is a fully connected network meaning that each hidden node those not directly representing inputs or outputs is linked to every other node in the preceding and following layers haykin 2009 weights roughly equivalent to the covariant in a glm on each link are adjusted during each pass epoch of the training cycle in the direction that brings the network s predicted values closer to the observed values hastie et al 2009 as well as the network structure the rate at which the network learns and the conditions under which learning is terminated to avoid overfitting to the training data also need to be specified in each case often through trial and error the increased complexity of an ann over a glm allows it to learn interactions between the predictor variables rather having to make them explicit a priori bns fenton and neil 2013 marcot and penman 2019 are directed acyclic graphs that can represent the cause and effect relationships among variables fig 3 bns represent variables as nodes which can be continuous numerical or discrete categorical numeric variables are often discretised although there is recent work looking to remove this requirement zhu and collette 2015 conditional probabilities are used to quantify the relationships between nodes both the network structure and conditional probabilities can be either user defined or learnt from data using bn software packages appendix a a key feature of bns is that they are often represented with a graphical user interface gui that allow users to directly interact with the network to explore relationships and scenarios the simplest bn structure is a naïve network fig 3 in which the response variable is modelled as a parent node predictors are modelled as child nodes and there are no causal links between the predictors i e predictors are considered independent a tree augmented network tan structure expands on a naïve network by allowing each node to have at most one other parent in addition to the target node fenton and neil 2013 the final model described gps are a spatial model which rather than learning a single function representing the relationship between the response and predictor variables instead learn a probability distribution over a range of functions rasmussen and williams 2006 from this the model is able to calculate the value of the response variable for a previously unseen combination of predictor values based on the average values at known surrounding points formulas for gps are given in appendix b 2 methods 2 1 study area two study areas were selected in the yucatan peninsula of southern mexico fig 4 the yucatan peninsula was selected because of its importance as a conservation area and its historically high deforestation rates perez verdin et al 2009 the amount of data available mayfield et al 2017 also allows for a feasible comparison of the selected modelling techniques the main study area 100 km 200 km is located in the state of campeche and overlaps the calakmul biosphere reserve the second study area 130 km 90 km is located in the state of quintana roo 2 2 datasets and variable design to implement the example models we obtained several freely available or low cost datasets described fully in mayfield et al 2017 the datasets used were the conservation international land use change data vaca et al 2012 the world database on protected areas wdpa 2010 landscan population pressure ut battelle 2013 landsat slope and elevation data nasa 2015 mapability road locations mapability 2012 and the natural earth city locations ne 2013 predictor variables were extracted from these datasets using esri arcmap esri 2013 and python python software foundation 2012 predictor variables describing the fragmentation of forest cover were calculated using the fragstats software program mcgarigal et al 2012 datasets were generated by selecting random points spaced a minimum of 250 m apart from within each of the study areas the response variable was whether or not any deforestation had occurred between the years 2000 and 2005 within a 500 m 500 m target area surrounding each point predictors included the geographic coordinates of the sample point surrounding deforestation between 1990 and 2000 fragmentation of surrounding forest proximity to forest edge surrounding pas elevation slope ruggedness proximity to road population pressure and proximity to the nearest city to avoid strongly correlated predictors we generated a correlation matrix and removed one variable of any pair with a pearson s correlation coefficient above 0 8 full variable definitions are available in appendix c we generated four datasets using either random sampling or stratified random sampling table 1 the stratified sample involved selecting an equal number of points with and without observed deforestation any sample points with no forest in the target area at the start of the prediction time period 2000 were replaced until each sample contained 8000 points we set aside 1000 points from each sample to form a validation set for independent model testing i e none of these points were used in the training of the models we then used repeated random subsampling to split the remaining 7000 points into 20 training and testing data set pairs with replacement of sample points between each subsample for each trial 50 of points were allocated to each of the testing and training sets sampling the areas with and without deforestation separately and hence maintaining the original deforestation prevalence rate in each sample 2 3 model design and implementation glmms were implemented using the mass package in r venables and ripley 2002 using the binomial link function design considerations for the models included the transformation of predictor variables and the choice of random effects we elected not to transform the variables to avoid variation in the datasets used between models for the random effects we used the geographic coordinates of the sample point rounded to create a 10 11 grid on the study area cell size approximately 10 km 20 km for main study area for models tested on the study area other than the one they were trained on the x and y coordinates were excluded from the list of predictor variables resulting in glms rather than glmms for the statistical models anns were implemented in matlab mathworks 2014 using a resilient backpropagation algorithm preliminary trials were used to guide the design decisions and select the number of hidden layers and nodes initial values for the weights and suitable stopping clauses to make sure models had sufficient runs to learn without overfitting to the training set the same network structure was used for models trained on either the standard random sampling or the stratified random sampling bns were implemented in netica norsys software corp 2013 the design decisions were network structure i e the cause and effect relationships between nodes and the discretisation method for the continuous variables three network structures were tested naïve tan and expert structured bns the structure for the tan bns was learnt from the training dataset using netica software norsys software corp 2013 conditional probability tables cpts for all three bn designs were learnt from the data using the entropy maximisation algorithm available in the netica for the expert defined models five deforestation experts were drafted to individually suggest a structure each structure was implemented as a bn trained on the data to learn the cpts and then tested against unseen data the expert structure with the highest true skill statistic tss score allouche et al 2006 was compared to the naïve and tan bns full details on the creation of the expert structured bns are given in appendix d for discretisation of continuous nodes we used a custom algorithm mayfield et al 2017 that ensured every node state contained a minimum number of sample points gps were implemented using the gp toolbox in matlab rasmussen and nickisch 2015 while several settings can be altered within the package rasmussen and williams 2006 defaults are suitable for general model implementation to reduce the high runtime of the gp models we tested the effect of learning the hyperparameters kernel parameters which are estimated prior to the main training with sub samples of various sizes for the models implemented in this study we translate the outputs to be the probability that deforestation will occur for the glmms anns and gps this equates to converting the log odds of presence for the bns this value is given as a direct output of the model 2 4 evaluation we carried out four series of tests to evaluate each modelling technique s performance in different circumstances table 2 models in series 1 evaluating the overall performance of the models under either balanced or imbalanced data and series 2 evaluating how well each model generalises to a nearby region were evaluated using sensitivity the proportion of observed presences correctly predicted specificity the proportion of observed absences correctly predicted and tss tss provides a single value combining sensitivity and specificity allouche et al 2006 and is commonly used in testing presence absence models for example lu et al 2012 and aben et al 2014 we used the most probable outcome as the cut off for evaluation i e if the model predicted a probability of 50 for deforestation then deforestation was predicted to be present if not the prediction was assigned as forest persistence model sensitivity specificity and tss were then calculated for each trial to evaluate the models independently of the cut off required for the other metrics we also used the area under the receiver operator curve auc fielding 1997 in series 1 we also examined the effect of correcting for deforestation rates by adjusting the model predictions to reflect the observed amount of deforestation this was done by selecting the points with the highest chance of deforestation until the number of predicted points matched the observed number of points deforested predictions from series 3 designed to evaluate how well each technique predicted the location of high risk areas were mapped and compared to the known outcomes for these 1000 points this allowed the predicted spatial distribution of deforestation for each model to be compared against the observed spatial distribution we also conducted a spearman s rank correlation test on these results to examine whether there were significant differences in predictions between models for series 4 bns and glmms were assessed for how well they described relative importance of deforestation predictors the explanatory ability of glmms was evaluated by looking at the coefficient standard error and p value for each predictor variable for the bns a sensitivity to findings analysis was carried out this analysis indicates the relative influence of a predictor variable by reporting the percent variance reduction in the response variable a measure of how much the variance in the response variable in this case deforestation presence is reduced by knowing the state of the predictor variable while the two analyses are not strictly equivalent they serve to demonstrate the potential interpretability of each model the bn was also used to conduct a scenario analysis by setting the node states of the three most influential predictors to high low and medium values and examining the effect on the target node i e the probability of deforestation 3 results as a result of the preliminary trials the anns were structured with a single hidden layer with 60 nodes and set to a maximum run of 3000 epochs per trial with at most 300 epochs without improvement before training was terminated for the bns both the naïve bns and tan bns have been included for comparison because they performed differently when the different sampling methods were used however both outperformed the expert structured models the structure of the tan bn is given in appendix e gps were run using 1500 points for learning the hyperparameters 3 1 series 1 overall performance tss and auc results are presented here as a measure of overall technique performance fig 5 sensitivity and specificity results are provided in appendix f the naïve bns were the most stable over the sample methods with high scores on the standard sampling as a result of high sensitivity scores see appendix f while the tss scores were generally low they reflect in part the models tendency to under predict the amount of deforestation when defined according to a 50 probability cut off when predictions were adjusted to reflect the observed amount of deforestation tss results for models trained on standard data generally improved with the exception of the naïve bns average tss of 0 4 for glmms anns and gps 0 2 for naive bns and 0 3 for tan bns when tested on the validation sets auc results show the glmms and gps scored the highest when models were trained on standard sampled data cstand however all models scored above 0 7 the anns showed the largest increase in auc when trained using stratified data indicating that these models benefited from having the extra positive examples to learn from 3 2 series 2 generalisation to nearby region tss and auc scores for models trained on data from neighbouring quintana roo datasets qstand and qstrat are given in fig 6 tss results indicated the anns and glms implemented here struggled to generalise to the new region unless trained on the stratified datasets with the higher proportion of deforested sample points results for sensitivity and specificity are available in appendix g 3 3 series 3 location of deforestation when models tested on the validation dataset n 1000 were mapped according to the predicted probability of deforestation all models were able to correctly identify the high risk region in the south western corner of the study area as well as the low risk area covered by the protected area fig 7 maps for the remaining models appendix h showed similar patterns to the glmm the naïve bn was a notable exception in that it predicted substantially more points at high risk results of the spearman s rank coefficient showed that predictions of all models trained on either datasets from sample cstand or cstrat were strongly correlated estimates min 0 68 max 0 99 mean 0 83 and significant for every combination p 001 this indicates that when prioritising which sample points would be deforested there was no significant difference between the maps generated by each model 3 4 series 4 factors affecting deforestation analysis of data sampled from campeche cstand showed that for the glmm metrics describing the characteristics of the surrounding forest and terrain were the most important predictors of deforestation table 3 and appendix i distance to the nearest pa boundary was also found to be correlated with increased chance of deforestation the bns also showed characteristics of the surrounding forest to be important although in contrast to the glmms the median elevation was less influential than other variables population pressure and distance to roads were not influential predictors in either technique it should be acknowledged that the factors considered as important by each model in this study are a reflection of the data and the algorithm used to analyse it thus the accuracy of these predictions on the ground has not been established results of the scenario analysis from the bn on the three most influential predictors are given in table 4 the highest risk areas based on these scenarios were those where more than 6 4 of the surrounding areas are already deforested areas that were more than 1 km from the forest edge were the least likely to be deforested the distance to deforestation becomes less relevant after 4 km 4 discussion the models developed for this study demonstrate practical implementations of several common machine learning techniques for studying deforestation in many instances the more complicated models out performed the simpler models when predicting the location of deforestation or provided more detailed insights into the drivers of forest loss however these benefits are offset by complicated design requirements or extended run time model selection should therefore be based not only on the predictive performance of the model but also considering constraints arising from the study objectives user capacity and the study context table 5 the vcs approved methodology for avoided unplanned deforestation vcs 2012 provides a real world case study to demonstrate how the constraints listed in table 5 will affect model suitability for implementing a quantitative deforestation model the implications of the study objective for example are evident in step 3 of the vcs analysing the causes of deforestation glmms provide statistical significance of predictor variables which are commonly reported in deforestation studies bns facilitate scenario analysis and more detailed information such as looking at whether distance to the nearest deforestation becomes less relevant past some distance threshold in some cases an easy to interpret correlation could be considered an advantage of glmms however in a complex decision making scenarios such as forest protection policies the easy scenario analysis facilitated by bns may provide valuable insights while there are numerous sensitivity analysis sa methods that can be used to interpret the importance of variables for black box ml techniques such as anns cortez and embrechts 2013 ye et al 2019 these require additional analysis beyond the initial model development considered here in step 4 of the vcs methodology projection of future deforestation when preparing risk maps of expected deforestation for this case study of deforestation in southern mexico each technique made similar predictions in terms of which areas were at a high risk of deforestation the exception was the naïve bns which predicted more areas at higher risk than the other techniques this highlights a disadvantage of bns in that they are sensitive to model structure and require a certain level of expertise to make sure models are correctly implemented and interpreted in cases where time or modelling expertise is limited a glmm may be the most suitable option if the required modelling skills are available and time permits predictive performance may be improved using anns although this is not always the case tayyebi et al 2014 mayfield et al 2017 glmms and gps have fewer more straight forward design decisions than anns and bns making the former options more suited in cases where modelling expertise is limited however although we elected to use glmms based on previous studies mayfield et al 2017 other regression variants such as stepwise selection or including interactions between the predictors may be more suitable in some instances and the time required to decide which of these is most suitable often via multiple trials should also considered if not parameterised correctly anns are prone to overfitting where the network learns the specific patterns in dataset rather than general trends that can be transferred to predict for other datasets geman et al 1992 deciding on parameters such as model structure and when to cease model training often by trial and error maeda et al 2009 müller and mburu 2009 gómez ossa and botero fernández 2017 is especially important if the model is intended to be used to predict deforestation in nearby areas or future time steps unlike anns the structure of a bn will affect the interpretability and should be considered carefully in instances were specific scenarios are desired celio et al 2014 for example areas at risk in fragmented compared to intact forests for complex problems such as deforestation that have social demographic and environmental factors at play designing models to fit the local context such as ensuring all key drivers are included is crucial each technique has varying scope for local knowledge of deforestation subject expertise to play a role in the modelling process firstly all four methods require an initial selection of predictors likely a combination of environmental and demographic factors past this point anns and gps are traditionally black box models and have little scope for subject expert intervention in the design phase as they learn the interactions between drivers directly from the data provided this introduces the risk that key interactions or thresholds can be missed if not strongly represented in the data land use change models that allow for incorporation of expert knowledge can potentially be more accurate for these reasons pérez vega et al 2012 expert knowledge is also critical for interpreting the model outputs to ensure the results are logical particularly for anns which are prone to overfitting geman et al 1992 and bns which can produce unreliable predictions if cpt tables are incomplete while the predictive performance of the expert bns in this example was lower than the naïve and tan structures they proved to be a useful tool for incorporating expert knowledge about the relationships between deforestation predictors into quantitative models this has also been demonstrated in other areas of environmental management rumpff et al 2011 chen and pollino 2012 celio et al 2014 expert elicitation can be also be used for bns to design the network structure and fill in the cpts for a bn estimating the drivers of land use change celio et al 2014 there at least two ways in which data availability could affect the choice of modelling technique missing spatial data on existing land cover i e fewer cases to learn from for example caused by cloud cover in satellite images can make it difficult to learn patterns of deforestation regardless of the algorithms applied some regions may also have missing or incomplete data for demographic predictors such as population density if training data for the study area is not available bns and gps in this study gave better predictions than glms and anns when trained on samples from a neighbouring state particularly when one data class in this case deforestation is scarce it should be noted however that this approach is only suitable when the regions have similar patterns and driving factors of deforestation if sufficient data exists for a stratified sampling this can be used to improve performance in predicting the location of deforestation particularly for the anns and glmms gps have also been previously found to have good predictive performance when trained on only the geographic coordinates of the existing deforestation mayfield et al 2017 the vcs is designed to facilitate the introduction of redd programs meaning that any analysis will be expected within a set timeframe to allow approved projects to proceed gps have a high processing time however processing requirements are becoming less of a constraint as advancements in both hardware and algorithms evolve all models are capable of being scaled up to include more predictor variables or increasing the size of the geographic area being modelled gps will be more constrained than the other methods by computing constraints as the size of the datasets increase bns and anns will also require more time to design and develop due to the number and potential impact of the design decisions it is acknowledged that the values given for time constraints particularly the implementation time are in part a reflection on the software selected from a possible range of programs these values have nevertheless been provided as a guide the vcs methodology states that modelling techniques for each step must have been published in an international peer reviewed journal vcs 2012 glmms are commonly used green et al 2013 and evidence exists for the use of anns mas et al 2004 ye et al 2019 and bns dlamini 2016 gps have fewer empirical studies in deforestation but have been previously used in environmental management wang et al 2009 while the advice presented is given in relation to deforestation many of the restrictions such as time constraints or modelling expertise are not unique to deforestation and many of the findings presented here will be relevant to other fields of environmental modelling in particular there are strong parallels with species distribution modelling which also requires both an understanding of the factors affecting habitat suitability of a species and predicting where the species is likely to occur araújo and guisan 2006 the link between the two is becoming more evident in the literature combines both issues de souza and de marco 2014 the models implemented here demonstrate the design decisions of each technique as well as an example implementation there are however numerous other software packages for each method appendix a either with graphical point and click interfaces or scripting options that allow for repeatable trials functionality to implement a glmm comes as a standard in many statistical packages many of which also have functionality for anns while the gps were implemented here in matlab kriging an equivalent geostatistics method can be implemented in arcgis esri 2013 several of the model limitations discussed here are already being overcome by recent advances in ml for example bn software such as genie druzdzel 1999 now contains functionality to work with distributions rather than having to discretise states and there are new methodologies looking to remove the restriction on feedback loops using dynamic bns maldonado et al 2019 marcot and penman 2019 anns in particular are benefiting from the continued improvements in computer processing capabilities with the onset of deep learning ye et al 2019 combining different techniques to generate ensemble methods is also becoming more common lima et al 2015 marcot and penman 2019 5 conclusion ml has demonstrated advantages over simpler statistical techniques for many types of deforestation and environmental modelling studies the added complexity of the more advanced models can provide improved explanatory capacity and incorporation of expert knowledge in the case of bns or improved predictions in the case of anns these advantages come at a cost though and there are instances where a simple statistical model is preferable while model performance should always be considered when interpreting predictions suitability of a technique can depend as much on the constraints of the study context such a time constraints user capacity such as limited modelling expertise and study requirements as it does on predictive ability we encourage practitioners and researchers with sufficient resources to consider branching out from statistical methods but urge them to consider carefully when selecting a technique declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we would like thank the four deforestation experts who donated their time to participate in this study and gratefully acknowledge funding provided to hm by an australian postgraduate award we would also like to thank dr lauren coad for her assistance with conceptualising the evaluation framework and the anonymous reviewers for their helpful and constructive feedback appendix i supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 multimedia component 3 multimedia component 3 multimedia component 4 multimedia component 4 multimedia component 5 multimedia component 5 multimedia component 6 multimedia component 6 multimedia component 7 multimedia component 7 multimedia component 8 multimedia component 8 multimedia component 9 multimedia component 9 appendix i supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104741 
25994,there are a many reasons for creating quantitative models of deforestation supported by a variety of modelling techniques for doing so we examine the suitability of four different modelling techniques for predicting deforestation bayesian networks bns artificial neural networks anns gaussian processes gps and generalised linear mixed models glmms the analysis is provided in the context of the verified carbon standard approved methodology for avoided unplanned deforestation to illustrate specific examples of scenarios where each technique would be suitable anns potentially improve predictions over glmms while gps were better able to generalise to other geographic areas anns were sensitive to design decisions which may limit their applications where results are required within short time frames bns easily facilitate scenario analysis but are also sensitive to design decisions with machine learning becoming easier each day we encourage researchers to consider carefully which technique is suitable for their situation prior to beginning implementation keywords machine learning deforestation generalized linear mixed models bayesian networks artificial neural networks 1 introduction assessing deforestation risk requires an understanding of complex phenomena involving many different and interrelated variables while statistical techniques have traditionally been the tool of choice machine learning ml techniques are now gaining popularity this shift is supported by a wide range of techniques jakeman et al 2006 kelly et al 2013 and modelling tools soares filho et al 2002 hallgren et al 2016 however not every technique will necessarily meet the requirements of every analysis which will vary depending on the overall purpose of the study be it protected area pa management planning robinson and lokina 2011 implementing a scheme for reduced emissions from avoided deforestation and degradation redd di lallo et al 2017 or analysing biodiversity offset schemes virah sawmy et al 2014 for example protected area management studies may be looking to either evaluate the effectiveness of a pa by comparing against some counterfactual scenario mas 2005 or predict which areas are most at risk allnutt et al 2013 a data model for a redd scheme simonet et al 2019 on the other hand may be designed primarily as a decision support tool to establish a deforestation baseline for calculating payments regardless of the purpose deforestation studies generally share one or some combination of three specific objectives determining the factors that affect deforestation freitas et al 2010 wyman and stein 2010 predicting where deforestation will occur müller and mburu 2009 gómez ossa and botero fernández 2017 and predicting how much forest will be lost singh et al 2017 factors affecting deforestation can be examined on several levels in some instances an assessment of the factors that predispose an area to deforestation such as road density freitas et al 2010 soil type or rainfall bax and francesconi 2018 may be appropriate whereas other situations may require measures of direct causes such as the potential of the area for fuel wood extraction geist and lambin 2001 predicting the amount of forest loss either for a business as usual bau or for some counterfactual scenario is often more difficult than predicting the location of deforestation because deforestation rates are often influenced by economic and political factors such as international treaties or local policies veldkamp and lambin 2001 these factors are often outside of the model scope and are liable to change within the study period each modelling technique has different strengths and weaknesses when addressing these three objectives in addition organisations such as governments environmental groups non governmental organisations ngos and private enterprise will each have differing motives expertise resources and regulations which will restrict which techniques can or should be implemented constraints can also potentially arise from the design and specifications of the study including the content and structure of the data and the size of the datasets in particular data imbalance haibo and garcia 2009 where one data class is much rarer than another may disadvantage techniques that require a large number of examples to learn from this is especially relevant in areas with low rates of deforestation this paper compares the suitability of four data analysis techniques for two common objectives of deforestation studies predicting the location of areas at risk of deforestation and analysing the variables that drive this risk three ml techniques are considered bayesian networks bns artificial neural networks anns and gaussian processes gps as well as a common statistical method generalised linear mixed models glmms results evaluating the usefulness of the datasets and the accuracy of the modelling technique have been published previously mayfield et al 2017 here we extend this work by placing each technique in the context of a real world decision making situation using the verified carbon standard vcs as a case study vcs 2012 we consider the impact of both user capacity and the study constraints on the suitability of each method fig 1 by also illustrating each technique with a detailed example this analysis is designed to assist practitioners with little or no experience in ml to decide which if any of the listed techniques are appropriate to their situation 1 1 verified carbon standard case study the example of the verified carbon standard vcs approved methodology for avoided unplanned deforestation serves as a good illustration of how different techniques suit different situations the vcs details the accepted procedure for conducting an analysis prior to the introduction of a redd program vcs 2012 the methodology outlines several distinct stages in the process step 1 definition of spatial and temporal boundaries carbon pools and sources of emissions step 2 analysis of historical land cover change base rate step 3 analysis of agents drivers and underlying causes of deforestation and their likely future development step 4 projection of future deforestation allows for either a function approach extrapolating historical trends using regression techniques or a modelling approach modelling deforestation as a function of the driver variables includes four steps o preparation of factor maps o preparation of risk maps for deforestation o selection of the most accurate deforestation risk map o mapping locations of future deforestation step 5 definition of the land use and land cover change component of the baseline data analysis is an essential tool for steps 2 through to 5 but the requirement for the analysis will differ at each step for example in step 3 a method is needed that allows for a comparison of the hierarchy of importance for the drivers and underlying causes of the deforestation mapping future deforestation in step 4 would not necessarily require this interpretability rather focusing on the accuracy of predicting at risk locations 1 2 machine learning in deforestation statistical regression models are commonly used for analysing deforestation and have been previously implemented for examining the drivers of deforestation freitas et al 2010 wyman and stein 2010 predicting the magnitude of deforestation rosa et al 2013 and examining the effectiveness of protected areas in avoiding deforestation green et al 2013 more advanced ml models are also becoming widely applied anns have been successfully used to forecast deforestation hotspots mas et al 2004 müller and mburu 2009 gómez ossa and botero fernández 2017 singh et al 2017 predict the location of forest fire maeda et al 2009 and more generally to model land use change tayyebi et al 2014 samardzic petrovic et al 2017 advances in deep learning have also shown particular promise for predicting high risk areas ye et al 2019 other ml classifiers that have been used to model land use change include support vector machines samardzic petrovic et al 2017 regression trees tayyebi et al 2014 random forests bax and francesconi 2018 and bns silva et al 2020 bns designed by experts rather than learnt solely from quantitative data have also been used to integrate stakeholder knowledge and preferences for modelling drivers of land use change celio et al 2014 specialised methods from other fields such as presence absence models used in species distribution are also being re purposed to model deforestation distribution de souza and de marco 2018 comparisons of multiple methods often report only model performance and with little context of how each might be applicable in different decision making scenarios pérez vega et al 2012 tayyebi et al 2014 samardzic petrovic et al 2017 exceptions to this include studies that also report on comprehensibility and method intricacy kampichler et al 2010 and model calibration time rodrigues and de la riva 2014 this study helps to address this gap by discussing the models in terms of both performance and the practical aspects of their implementation the three ml techniques chosen represent different model families as identified in jakeman et al 2006 anns are used as an example of empirical data based models bns are used as an example of conceptual models and gps are used as an example of spatial based models glmms are included as a representative statistical technique based on results reported in mayfield et al 2017 generalised linear models glms are regression models that estimate a single coefficient for each predictor variable and combine these using a covariance function to predict the value of the dependant variable hastie et al 2009 interactions between the predictor variables are not taken into account unless they are explicitly specified in the model glmms build on glms by allowing certain variables to be modelled as random rather than fixed effects meaning that its coefficient is defined by a probability distribution defining a variable as a random effect accounts for levels within a dataset and is often included to account for spatial autocorrelation within the model green et al 2013 stepwise procedures that systematically remove variables to find more parsimonious models are available in standard r packages such as mass venables and ripley 2002 and can be used to address issues with collinearity between predictor variables options also exist to run glms that will attempt to find significant interactions between predictors calcagno 2013 however these are both computationally expensive restricted to linear relationships between a small subset of predictors and do not always improve model performance mayfield et al 2017 anns are a non linear extension of glms which learn relationships in the data by repeatedly passing values through a network of nodes and links haykin 2009 ann structure is defined by the number of layers in the network and the number of nodes in each layer fig 2 the most common network structure is a fully connected network meaning that each hidden node those not directly representing inputs or outputs is linked to every other node in the preceding and following layers haykin 2009 weights roughly equivalent to the covariant in a glm on each link are adjusted during each pass epoch of the training cycle in the direction that brings the network s predicted values closer to the observed values hastie et al 2009 as well as the network structure the rate at which the network learns and the conditions under which learning is terminated to avoid overfitting to the training data also need to be specified in each case often through trial and error the increased complexity of an ann over a glm allows it to learn interactions between the predictor variables rather having to make them explicit a priori bns fenton and neil 2013 marcot and penman 2019 are directed acyclic graphs that can represent the cause and effect relationships among variables fig 3 bns represent variables as nodes which can be continuous numerical or discrete categorical numeric variables are often discretised although there is recent work looking to remove this requirement zhu and collette 2015 conditional probabilities are used to quantify the relationships between nodes both the network structure and conditional probabilities can be either user defined or learnt from data using bn software packages appendix a a key feature of bns is that they are often represented with a graphical user interface gui that allow users to directly interact with the network to explore relationships and scenarios the simplest bn structure is a naïve network fig 3 in which the response variable is modelled as a parent node predictors are modelled as child nodes and there are no causal links between the predictors i e predictors are considered independent a tree augmented network tan structure expands on a naïve network by allowing each node to have at most one other parent in addition to the target node fenton and neil 2013 the final model described gps are a spatial model which rather than learning a single function representing the relationship between the response and predictor variables instead learn a probability distribution over a range of functions rasmussen and williams 2006 from this the model is able to calculate the value of the response variable for a previously unseen combination of predictor values based on the average values at known surrounding points formulas for gps are given in appendix b 2 methods 2 1 study area two study areas were selected in the yucatan peninsula of southern mexico fig 4 the yucatan peninsula was selected because of its importance as a conservation area and its historically high deforestation rates perez verdin et al 2009 the amount of data available mayfield et al 2017 also allows for a feasible comparison of the selected modelling techniques the main study area 100 km 200 km is located in the state of campeche and overlaps the calakmul biosphere reserve the second study area 130 km 90 km is located in the state of quintana roo 2 2 datasets and variable design to implement the example models we obtained several freely available or low cost datasets described fully in mayfield et al 2017 the datasets used were the conservation international land use change data vaca et al 2012 the world database on protected areas wdpa 2010 landscan population pressure ut battelle 2013 landsat slope and elevation data nasa 2015 mapability road locations mapability 2012 and the natural earth city locations ne 2013 predictor variables were extracted from these datasets using esri arcmap esri 2013 and python python software foundation 2012 predictor variables describing the fragmentation of forest cover were calculated using the fragstats software program mcgarigal et al 2012 datasets were generated by selecting random points spaced a minimum of 250 m apart from within each of the study areas the response variable was whether or not any deforestation had occurred between the years 2000 and 2005 within a 500 m 500 m target area surrounding each point predictors included the geographic coordinates of the sample point surrounding deforestation between 1990 and 2000 fragmentation of surrounding forest proximity to forest edge surrounding pas elevation slope ruggedness proximity to road population pressure and proximity to the nearest city to avoid strongly correlated predictors we generated a correlation matrix and removed one variable of any pair with a pearson s correlation coefficient above 0 8 full variable definitions are available in appendix c we generated four datasets using either random sampling or stratified random sampling table 1 the stratified sample involved selecting an equal number of points with and without observed deforestation any sample points with no forest in the target area at the start of the prediction time period 2000 were replaced until each sample contained 8000 points we set aside 1000 points from each sample to form a validation set for independent model testing i e none of these points were used in the training of the models we then used repeated random subsampling to split the remaining 7000 points into 20 training and testing data set pairs with replacement of sample points between each subsample for each trial 50 of points were allocated to each of the testing and training sets sampling the areas with and without deforestation separately and hence maintaining the original deforestation prevalence rate in each sample 2 3 model design and implementation glmms were implemented using the mass package in r venables and ripley 2002 using the binomial link function design considerations for the models included the transformation of predictor variables and the choice of random effects we elected not to transform the variables to avoid variation in the datasets used between models for the random effects we used the geographic coordinates of the sample point rounded to create a 10 11 grid on the study area cell size approximately 10 km 20 km for main study area for models tested on the study area other than the one they were trained on the x and y coordinates were excluded from the list of predictor variables resulting in glms rather than glmms for the statistical models anns were implemented in matlab mathworks 2014 using a resilient backpropagation algorithm preliminary trials were used to guide the design decisions and select the number of hidden layers and nodes initial values for the weights and suitable stopping clauses to make sure models had sufficient runs to learn without overfitting to the training set the same network structure was used for models trained on either the standard random sampling or the stratified random sampling bns were implemented in netica norsys software corp 2013 the design decisions were network structure i e the cause and effect relationships between nodes and the discretisation method for the continuous variables three network structures were tested naïve tan and expert structured bns the structure for the tan bns was learnt from the training dataset using netica software norsys software corp 2013 conditional probability tables cpts for all three bn designs were learnt from the data using the entropy maximisation algorithm available in the netica for the expert defined models five deforestation experts were drafted to individually suggest a structure each structure was implemented as a bn trained on the data to learn the cpts and then tested against unseen data the expert structure with the highest true skill statistic tss score allouche et al 2006 was compared to the naïve and tan bns full details on the creation of the expert structured bns are given in appendix d for discretisation of continuous nodes we used a custom algorithm mayfield et al 2017 that ensured every node state contained a minimum number of sample points gps were implemented using the gp toolbox in matlab rasmussen and nickisch 2015 while several settings can be altered within the package rasmussen and williams 2006 defaults are suitable for general model implementation to reduce the high runtime of the gp models we tested the effect of learning the hyperparameters kernel parameters which are estimated prior to the main training with sub samples of various sizes for the models implemented in this study we translate the outputs to be the probability that deforestation will occur for the glmms anns and gps this equates to converting the log odds of presence for the bns this value is given as a direct output of the model 2 4 evaluation we carried out four series of tests to evaluate each modelling technique s performance in different circumstances table 2 models in series 1 evaluating the overall performance of the models under either balanced or imbalanced data and series 2 evaluating how well each model generalises to a nearby region were evaluated using sensitivity the proportion of observed presences correctly predicted specificity the proportion of observed absences correctly predicted and tss tss provides a single value combining sensitivity and specificity allouche et al 2006 and is commonly used in testing presence absence models for example lu et al 2012 and aben et al 2014 we used the most probable outcome as the cut off for evaluation i e if the model predicted a probability of 50 for deforestation then deforestation was predicted to be present if not the prediction was assigned as forest persistence model sensitivity specificity and tss were then calculated for each trial to evaluate the models independently of the cut off required for the other metrics we also used the area under the receiver operator curve auc fielding 1997 in series 1 we also examined the effect of correcting for deforestation rates by adjusting the model predictions to reflect the observed amount of deforestation this was done by selecting the points with the highest chance of deforestation until the number of predicted points matched the observed number of points deforested predictions from series 3 designed to evaluate how well each technique predicted the location of high risk areas were mapped and compared to the known outcomes for these 1000 points this allowed the predicted spatial distribution of deforestation for each model to be compared against the observed spatial distribution we also conducted a spearman s rank correlation test on these results to examine whether there were significant differences in predictions between models for series 4 bns and glmms were assessed for how well they described relative importance of deforestation predictors the explanatory ability of glmms was evaluated by looking at the coefficient standard error and p value for each predictor variable for the bns a sensitivity to findings analysis was carried out this analysis indicates the relative influence of a predictor variable by reporting the percent variance reduction in the response variable a measure of how much the variance in the response variable in this case deforestation presence is reduced by knowing the state of the predictor variable while the two analyses are not strictly equivalent they serve to demonstrate the potential interpretability of each model the bn was also used to conduct a scenario analysis by setting the node states of the three most influential predictors to high low and medium values and examining the effect on the target node i e the probability of deforestation 3 results as a result of the preliminary trials the anns were structured with a single hidden layer with 60 nodes and set to a maximum run of 3000 epochs per trial with at most 300 epochs without improvement before training was terminated for the bns both the naïve bns and tan bns have been included for comparison because they performed differently when the different sampling methods were used however both outperformed the expert structured models the structure of the tan bn is given in appendix e gps were run using 1500 points for learning the hyperparameters 3 1 series 1 overall performance tss and auc results are presented here as a measure of overall technique performance fig 5 sensitivity and specificity results are provided in appendix f the naïve bns were the most stable over the sample methods with high scores on the standard sampling as a result of high sensitivity scores see appendix f while the tss scores were generally low they reflect in part the models tendency to under predict the amount of deforestation when defined according to a 50 probability cut off when predictions were adjusted to reflect the observed amount of deforestation tss results for models trained on standard data generally improved with the exception of the naïve bns average tss of 0 4 for glmms anns and gps 0 2 for naive bns and 0 3 for tan bns when tested on the validation sets auc results show the glmms and gps scored the highest when models were trained on standard sampled data cstand however all models scored above 0 7 the anns showed the largest increase in auc when trained using stratified data indicating that these models benefited from having the extra positive examples to learn from 3 2 series 2 generalisation to nearby region tss and auc scores for models trained on data from neighbouring quintana roo datasets qstand and qstrat are given in fig 6 tss results indicated the anns and glms implemented here struggled to generalise to the new region unless trained on the stratified datasets with the higher proportion of deforested sample points results for sensitivity and specificity are available in appendix g 3 3 series 3 location of deforestation when models tested on the validation dataset n 1000 were mapped according to the predicted probability of deforestation all models were able to correctly identify the high risk region in the south western corner of the study area as well as the low risk area covered by the protected area fig 7 maps for the remaining models appendix h showed similar patterns to the glmm the naïve bn was a notable exception in that it predicted substantially more points at high risk results of the spearman s rank coefficient showed that predictions of all models trained on either datasets from sample cstand or cstrat were strongly correlated estimates min 0 68 max 0 99 mean 0 83 and significant for every combination p 001 this indicates that when prioritising which sample points would be deforested there was no significant difference between the maps generated by each model 3 4 series 4 factors affecting deforestation analysis of data sampled from campeche cstand showed that for the glmm metrics describing the characteristics of the surrounding forest and terrain were the most important predictors of deforestation table 3 and appendix i distance to the nearest pa boundary was also found to be correlated with increased chance of deforestation the bns also showed characteristics of the surrounding forest to be important although in contrast to the glmms the median elevation was less influential than other variables population pressure and distance to roads were not influential predictors in either technique it should be acknowledged that the factors considered as important by each model in this study are a reflection of the data and the algorithm used to analyse it thus the accuracy of these predictions on the ground has not been established results of the scenario analysis from the bn on the three most influential predictors are given in table 4 the highest risk areas based on these scenarios were those where more than 6 4 of the surrounding areas are already deforested areas that were more than 1 km from the forest edge were the least likely to be deforested the distance to deforestation becomes less relevant after 4 km 4 discussion the models developed for this study demonstrate practical implementations of several common machine learning techniques for studying deforestation in many instances the more complicated models out performed the simpler models when predicting the location of deforestation or provided more detailed insights into the drivers of forest loss however these benefits are offset by complicated design requirements or extended run time model selection should therefore be based not only on the predictive performance of the model but also considering constraints arising from the study objectives user capacity and the study context table 5 the vcs approved methodology for avoided unplanned deforestation vcs 2012 provides a real world case study to demonstrate how the constraints listed in table 5 will affect model suitability for implementing a quantitative deforestation model the implications of the study objective for example are evident in step 3 of the vcs analysing the causes of deforestation glmms provide statistical significance of predictor variables which are commonly reported in deforestation studies bns facilitate scenario analysis and more detailed information such as looking at whether distance to the nearest deforestation becomes less relevant past some distance threshold in some cases an easy to interpret correlation could be considered an advantage of glmms however in a complex decision making scenarios such as forest protection policies the easy scenario analysis facilitated by bns may provide valuable insights while there are numerous sensitivity analysis sa methods that can be used to interpret the importance of variables for black box ml techniques such as anns cortez and embrechts 2013 ye et al 2019 these require additional analysis beyond the initial model development considered here in step 4 of the vcs methodology projection of future deforestation when preparing risk maps of expected deforestation for this case study of deforestation in southern mexico each technique made similar predictions in terms of which areas were at a high risk of deforestation the exception was the naïve bns which predicted more areas at higher risk than the other techniques this highlights a disadvantage of bns in that they are sensitive to model structure and require a certain level of expertise to make sure models are correctly implemented and interpreted in cases where time or modelling expertise is limited a glmm may be the most suitable option if the required modelling skills are available and time permits predictive performance may be improved using anns although this is not always the case tayyebi et al 2014 mayfield et al 2017 glmms and gps have fewer more straight forward design decisions than anns and bns making the former options more suited in cases where modelling expertise is limited however although we elected to use glmms based on previous studies mayfield et al 2017 other regression variants such as stepwise selection or including interactions between the predictors may be more suitable in some instances and the time required to decide which of these is most suitable often via multiple trials should also considered if not parameterised correctly anns are prone to overfitting where the network learns the specific patterns in dataset rather than general trends that can be transferred to predict for other datasets geman et al 1992 deciding on parameters such as model structure and when to cease model training often by trial and error maeda et al 2009 müller and mburu 2009 gómez ossa and botero fernández 2017 is especially important if the model is intended to be used to predict deforestation in nearby areas or future time steps unlike anns the structure of a bn will affect the interpretability and should be considered carefully in instances were specific scenarios are desired celio et al 2014 for example areas at risk in fragmented compared to intact forests for complex problems such as deforestation that have social demographic and environmental factors at play designing models to fit the local context such as ensuring all key drivers are included is crucial each technique has varying scope for local knowledge of deforestation subject expertise to play a role in the modelling process firstly all four methods require an initial selection of predictors likely a combination of environmental and demographic factors past this point anns and gps are traditionally black box models and have little scope for subject expert intervention in the design phase as they learn the interactions between drivers directly from the data provided this introduces the risk that key interactions or thresholds can be missed if not strongly represented in the data land use change models that allow for incorporation of expert knowledge can potentially be more accurate for these reasons pérez vega et al 2012 expert knowledge is also critical for interpreting the model outputs to ensure the results are logical particularly for anns which are prone to overfitting geman et al 1992 and bns which can produce unreliable predictions if cpt tables are incomplete while the predictive performance of the expert bns in this example was lower than the naïve and tan structures they proved to be a useful tool for incorporating expert knowledge about the relationships between deforestation predictors into quantitative models this has also been demonstrated in other areas of environmental management rumpff et al 2011 chen and pollino 2012 celio et al 2014 expert elicitation can be also be used for bns to design the network structure and fill in the cpts for a bn estimating the drivers of land use change celio et al 2014 there at least two ways in which data availability could affect the choice of modelling technique missing spatial data on existing land cover i e fewer cases to learn from for example caused by cloud cover in satellite images can make it difficult to learn patterns of deforestation regardless of the algorithms applied some regions may also have missing or incomplete data for demographic predictors such as population density if training data for the study area is not available bns and gps in this study gave better predictions than glms and anns when trained on samples from a neighbouring state particularly when one data class in this case deforestation is scarce it should be noted however that this approach is only suitable when the regions have similar patterns and driving factors of deforestation if sufficient data exists for a stratified sampling this can be used to improve performance in predicting the location of deforestation particularly for the anns and glmms gps have also been previously found to have good predictive performance when trained on only the geographic coordinates of the existing deforestation mayfield et al 2017 the vcs is designed to facilitate the introduction of redd programs meaning that any analysis will be expected within a set timeframe to allow approved projects to proceed gps have a high processing time however processing requirements are becoming less of a constraint as advancements in both hardware and algorithms evolve all models are capable of being scaled up to include more predictor variables or increasing the size of the geographic area being modelled gps will be more constrained than the other methods by computing constraints as the size of the datasets increase bns and anns will also require more time to design and develop due to the number and potential impact of the design decisions it is acknowledged that the values given for time constraints particularly the implementation time are in part a reflection on the software selected from a possible range of programs these values have nevertheless been provided as a guide the vcs methodology states that modelling techniques for each step must have been published in an international peer reviewed journal vcs 2012 glmms are commonly used green et al 2013 and evidence exists for the use of anns mas et al 2004 ye et al 2019 and bns dlamini 2016 gps have fewer empirical studies in deforestation but have been previously used in environmental management wang et al 2009 while the advice presented is given in relation to deforestation many of the restrictions such as time constraints or modelling expertise are not unique to deforestation and many of the findings presented here will be relevant to other fields of environmental modelling in particular there are strong parallels with species distribution modelling which also requires both an understanding of the factors affecting habitat suitability of a species and predicting where the species is likely to occur araújo and guisan 2006 the link between the two is becoming more evident in the literature combines both issues de souza and de marco 2014 the models implemented here demonstrate the design decisions of each technique as well as an example implementation there are however numerous other software packages for each method appendix a either with graphical point and click interfaces or scripting options that allow for repeatable trials functionality to implement a glmm comes as a standard in many statistical packages many of which also have functionality for anns while the gps were implemented here in matlab kriging an equivalent geostatistics method can be implemented in arcgis esri 2013 several of the model limitations discussed here are already being overcome by recent advances in ml for example bn software such as genie druzdzel 1999 now contains functionality to work with distributions rather than having to discretise states and there are new methodologies looking to remove the restriction on feedback loops using dynamic bns maldonado et al 2019 marcot and penman 2019 anns in particular are benefiting from the continued improvements in computer processing capabilities with the onset of deep learning ye et al 2019 combining different techniques to generate ensemble methods is also becoming more common lima et al 2015 marcot and penman 2019 5 conclusion ml has demonstrated advantages over simpler statistical techniques for many types of deforestation and environmental modelling studies the added complexity of the more advanced models can provide improved explanatory capacity and incorporation of expert knowledge in the case of bns or improved predictions in the case of anns these advantages come at a cost though and there are instances where a simple statistical model is preferable while model performance should always be considered when interpreting predictions suitability of a technique can depend as much on the constraints of the study context such a time constraints user capacity such as limited modelling expertise and study requirements as it does on predictive ability we encourage practitioners and researchers with sufficient resources to consider branching out from statistical methods but urge them to consider carefully when selecting a technique declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we would like thank the four deforestation experts who donated their time to participate in this study and gratefully acknowledge funding provided to hm by an australian postgraduate award we would also like to thank dr lauren coad for her assistance with conceptualising the evaluation framework and the anonymous reviewers for their helpful and constructive feedback appendix i supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 multimedia component 3 multimedia component 3 multimedia component 4 multimedia component 4 multimedia component 5 multimedia component 5 multimedia component 6 multimedia component 6 multimedia component 7 multimedia component 7 multimedia component 8 multimedia component 8 multimedia component 9 multimedia component 9 appendix i supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104741 
