index,text
25520,sand ripples are geomorphic features on the seafloor that affect bottom boundary layer dynamics including wave attenuation and sediment transport we present a new equilibrium ripple predictor using a machine learning approach that outputs a probability distribution of wave generated equilibrium wavelengths and statistics including an estimate of ripple height the most probable ripple wavelength and sediment and flow parameterizations the bayesian optimal model system boms is an ensemble machine learning system that combines two machine learning algorithms and two deterministic empirical ripple predictors with a bayesian meta learner to produce probabilistic wave generated equilibrium ripple wavelength estimates in sandy locations a ten fold cross validation of boms resulted in an adjusted r squared value of 0 93 and an average root mean square error rmse of 8 0 cm during both cross validation and testing on three unique field datasets boms provided more accurate wavelength predictions than each individual base model and other common ripple predictors keywords sand ripples bedforms seafloor machine learning probabilistic prediction 1 introduction sand ripples form on the seafloor when bottom velocities generated by waves and currents mobilize sediment equilibrium ripples occur when the hydrodynamic forcing exceeds a critical threshold and persists long enough to generate ripples correlated to the amplitude and frequency of bottom fluid velocities the presence of ripples influences bottom boundary layer hydrodynamics as well as sediment transport and resuspension equilibrium ripple dimensions are an important parameter in time dependent wave bottom boundary layer wbbl models therefore the relation between hydrodynamic forcing and ripple geometry is fundamental for accurate predictions of the formation and evolution of seafloor ripples in shallow water changing wave conditions cause the continual reworking of seafloor sediments in sandy shallow areas ripples are constantly being diffused generated and evolved and planed out by low mid and high bottom shear respectively produced by waves and currents one could argue that ripples in nature are very rarely in an equilibrium state however time dependent wbbl models depend on equilibrium ripple geometry information to drive ripples from their present state to a state correlated with the instantaneous wave forcing time dependent bottom boundary models can provide valuable information on the seabed state and the potential for the mobilization of seafloor sediments traykovski 2007 developed a time dependent spectral ripple model by estimating the change in the seafloor spectrum with time as a function of the bottom velocity amplitude and frequency an adjustment timescale factor and the equilibrium ripple height and length associated with the instantaneous wave forcing that model was expanded to two dimensions nelson and voulgaris 2015 to include current generated ripples soulsby et al 2012 and applied to field data to explain a measured acoustic response penko et al 2017 these time dependent models rely on an accurate estimation of equilibrium ripple geometry in order to estimate the time dependent response of ripples to changing wave forcing decades of research on equilibrium ripples in sandy regions has resulted in many ripple geometry prediction equations using least squares fit to equilibrium ripple observations e g pedocchi and garcia 2009 soulsby and whitehouse 2005 grasmeijer and kleinhans 2004 faraci and foti 2002 styles and glenn 2002 wiberg and harris 1994 mogridge et al 1994 van rijn et al 1993 grant and madsen 1982 nielsen 1981 and others see nelson et al 2013 for an extensive review of existing equilibrium ripple equations most equations predict ripple length and ripple height or steepness as functions of hydrodynamic forcing parameters e g mobility number orbital excursion grain size shields parameter etc despite many of the equations being developed from the same set of observations significant uncertainty remains in estimates of ripple dimensions equation skill has increased as new data were collected however the method of fitting a deterministic equation to observations of ripple geometry has remained the same machine learning ml methods are an effective approach to build prediction models for data rich problems with an ever increasing volume of available data and increases in computational power and technology ml is a promising method to use in the marine and coastal science fields machine learning for analytical model building is a sub field of artificial intelligence in which mathematical algorithms learn and improve from experience to produce predictions without being explicitly programmed the many ml algorithms range from simple e g linear regression to complex e g neural networks each with its own benefits and disadvantages determining which algorithm s to use for a specific problem may require extensive trial and error testing it is common to test a variety of algorithms and choose the one or combination that performs best for a particular problem the review by goldstein et al 2019 examines coastal morphodynamic and sediment transport studies that used machine learning the review presents rough guidelines for the most effective algorithm s to address marine and coastal science problems several researchers have applied ml to the prediction of sand ripple geometry the first used an artificial neural network ann built with laboratory and field observations yan et al 2008 while the model showed promising validation results when compared to existing empirical ripple predictors it was not tested on data outside the training data which was very limited 347 data points goldstein et al 2013 built a genetic programming gp system to predict wave generated ripple height wavelength and steepness using sediment grain size and orbital excursion data genetic programming optimizes a population of predictor algorithms via a process that simulates darwinian evolution they used a very similar dataset to that presented here and the product represented the current state of the art in ml applied to ripple geometry prediction until now this paper presents a very different methodology using an ensemble machine learning technique and model stacking as the foundation for the equilibrium ripple wavelength prediction system and tests the model s accuracy with timeseries of field observed ripple wavelengths 2 methods a machine learning approach was used to create a new model to predict equilibrium wave orbital ripple geometry generated by wave forcing while ripples can also be generated by current and combined wave current flows here we are only addressing wave generated ripples due to the amount of observations available the approach results in a probability distribution of equilibrium ripple wavelengths that provides prediction uncertainty as well as a deterministic equilibrium ripple height machine learning methods typically use a dataset to train a model using correlations between the independent and dependent variables the choice of the models and the spread of the training data influence the overall skill of the resulting predictions the complex nature of ripples demands techniques that can represent both linear and nonlinear processes such as ensemble machine learning techniques the implementation of ensemble machine learning has been applied in coastal zone studies e g beuzen et al 2019 and simmons and splinter 2022 here we focused on the ensemble machine learning method of model stacking to aggregate a variety of models into one system model stacking wolpert 1992 or stacked generalization uses the predictions from one or more base algorithms chosen by the user as the input to a final algorithm this final algorithm sometimes called the meta learner and also chosen by the user combines the predictions from the base algorithms by deducing their biases to produce a final optimal prediction the bayesian optimal model system boms presented here was created as a specialized machine learning stacked generalizer that combines multiple base model algorithms with a bayesian meta learner to produce probabilistic equilibrium ripple wavelength predictions fig 1 boms was coded in the python 3 9 5 programming language and utilizes machine learning algorithms from the scikit learn library and the bayesian model building interface bambi capretto et al 2020 the system was trained with the 50 year dataset compiled by nelson et al 2013 see p 3208 table 3 for an exhaustive list and range of conditions the compilation contains observations from both field and laboratory studies including controlled laboratory simulations diver observations underwater camera observations and field observations from stationary sector scanning sonar and velocimeters parameters included in the compilation are ripple height ripple wavelength median grain size wave orbital velocity semi orbital excursion water depth wave period and water density nelson et al 2013 used these data to develop the most recent empirical deterministic equilibrium ripple predictor formulation that typically outperforms all previously developed predictor formulations and was the present state of the art equilibrium geometry model until now 2 1 data preprocessing preprocessing refers to the preparation and transformation of data to make it suitable input to boms preprocessing includes filtering handling null values and scaling since the objective was to only model wave generated equilibrium ripples and the full compiled dataset included field timeseries data it was necessary to filter the dataset to include only times when ripples were in equilibrium with the instantaneous wave forcing specifically for the field datasets the times when the ripples were in transition or when the bottom shear stress was below the critical threshold for sediment motion needed to be excluded from the training data to exclude the non equilibrium ripples from the dataset the equilibrium ripple criteria established in nelson et al 2013 were used first the shields parameter was used to parameterize the bottom shear stress imposed on the seafloor by the waves 1 θ 0 5 f w u b 2 s 1 g d 50 where f w is the friction factor swart 1974 u b is the wave orbital velocity at the bed s is the specific gravity of the sediment and d 50 is the mean grain size diameter to ensure that the forcing was producing sediment mobilization shields parameter values less than a critical threshold were removed from the dataset for laboratory data the equilibrium criterion in nelson et al 2013 states that the shields parameter be greater than the critical shields parameter θ c r 0 05 for sediment motion for non timeseries field observations i e single points in time the equilibrium criterion states that the shields parameter be greater than two times the critical shields parameter θ 2 θ c r to ensure the shear stress is great enough for timely ripple transitions for timeseries field observations a critical shields parameter and an adjustment timescale parameter t k traykovski 2007 was used to approximate the minimum time interval of persistent wave forcing in order to fully adjust the instantaneous ripples to a new profile the adjustment timescale parameter is the ratio of the cross sectional area of a given ripple profile to the bed load sediment transport rate at a given time t and is defined as 2 t k t α 1 ϕ 0 16 λ t 2 2 q t where λ is the ripple wavelength α is an empirical time constant ϕ is the sediment porosity 0 4 and q is the volumetric sediment transport rate as defined by meyer peter and müller 1948 the adjustment timescale parameter represents the relative amount of time to transport the sediment in a unit volume ripple with wavelength λ under instantaneous wave forcing producing a sediment transport rate q at time t it provides a factor that allows for larger ripples under low stresses to require a longer time interval of persistent wave forcing to adjust than small ripples under high stresses timeseries field data are considered in equilibrium with forcing when 3 0 d θ d t 0 1 θ t t k t as well as θ 1 5 θ c r to exclude low energy conditions the d θ d t term represents the persistence of the wave forcing in time and the 0 1 θ t k term allows for up to a 10 change in the instantaneous wave forcing over the time interval required for the adjustment the initial nelson et al 2013 dataset consisted of 12 215 data points also similar to the methodology in nelson et al 2013 we specifically filter the dataset to only include wave generated orbital ripples with wavelengths less than a specific threshold a maximum ripple wavelength threshold of 1 m for laboratory data and 1 5 m for field data was applied to filter out potential megaripples that have longer wavelengths due to being forced by other processes tides infragravity waves etc the final equilibrium ripple filtered dataset consisted of 3 622 data points the scikit learn models used in boms required the removal or replacement of null values within the training and testing datasets several of the training datasets did not include ripple height measurements with the ripple and hydrodynamic observations historical ripple height observations are much sparser than ripple wavelength observations due to the typical acoustic instrumentation used in the field acoustic backscatter which does not provide direct ripple height information for those data scikit learn s knnimputer class was used to impute missing ripple height values using datasets that did include ripple height measurements fig 2 imputation refers to substituting missing or null data with estimated values imputing missing values via k nearest neighbors knnimputer is among the more robust methods of imputation troyanskaya et al 2001 compared to other methods of handling missing values such as substituting with the mean or the mode of the observed values the nearest neighbors to the missing value s are determined using euclidean distance the number of nearest neighbors is defined by the user here the number of nearest neighbors used is ten the user can also define the weight of the nearest neighbor values when calculating an estimate for the missing value s here the weights are based on the distance to each neighbor in order to determine the skill of the knnimputer s prediction of ripple height and since it is a ml algorithm a ten fold cross validation was performed on the imputation of ripple height values in the equilibrium ripple filtered training dataset fig 3 the ten fold cross validation resulted in an adjusted r squared r a d j 2 0 88 and root mean square error rmse 0 014 m the knnimputer also functions as a deterministic ripple height predictor since the algorithm essentially estimates a ripple height given hydrodynamic input imputed ripple height values are included with the statistical output of boms which also includes the most probable ripple wavelength and the sediment and flow parameterizations the knn algorithm determines the nearest neighbors based on euclidean distance and as a result the data must be scaled appropriately scaling was performed to standardize the independent features of a dataset so distributions will have a mean value of 0 and a standard deviation of 1 the mean of each feature was subtracted from each value in that feature column and divided by the standard deviation of the feature column standardization scaling was performed using scikit learn s standardscaler class inverse transformation was applied to the data after imputation so the values resembled the original units of measurement this process can also be applied to any new observation data in the development of boms predictor variables or features had to be determined for the system an initial list of features was built from parameters important for bottom boundary layer flow and sediment processes that have been previously used in deterministic empirical ripple predictors these parameters included bottom wave orbital velocity wave semi orbital excursion median grain size diameter sediment density fluid density and gravity however bottom wave orbital velocity and wave semi orbital excursion are not easily observed unless bottom mounted instrumentation is deployed instead linear wave theory and the dispersion relation is commonly applied to estimate the bottom velocity and orbital excursion from observed water depths wave heights and periods which are easily observed from wave buoys therefore the tested predictor variables for boms included the parameters that were most often included in the nelson et al 2013 training dataset and then feature selection was used to determine the most beneficial variables or features to include as predictors here we used spearman s correlation to determine the monotonic relationship between variables in order to select the best features for boms spearman s rank correlation coefficients were calculated between the all the observed variables in the training dataset ripple wavelength ripple height orbital velocity semi orbital excursion and wave period to determine the strengths of any potential linear and or nonlinear monotonic relationships fig 4 coefficients were calculated within each laboratory field experiment subset in the full training dataset therefore variables that were constant within each experiment e g sediment specific gravity median grain size and water depth were not included in the coefficient calculations i e since their value was the same for every other observation in that experiment their correlation coefficients were zero however both sediment grain size and water depth were included as features in the final training dataset the calculations were performed after filtering the datasets for equilibrium ripple data points and imputing any missing ripple height observations coefficient values range from 1 to 1 where a value of 1 indicates a perfect negative relationship between two features and a value of 1 indicates a perfect positive relationship between two features a coefficient value of 0 indicates no existing relationship between two features this work focused on the spearman correlations between the independent features and ripple wavelength analysis of the spearman correlations shows that ripple wavelength has a strong positive monotonic relationship with ripple height and moderate positive monotonic relationships with wave period and semi orbital excursion orbital velocity was the only feature in the correlation matrix to have a weak relationship with ripple wavelength therefore it was excluded as a predictor variable after all the data preprocessing the training dataset consisted of 3 499 samples and six features median grain size wave period water depth semi orbital excursion ripple height and ripple wavelength 2 2 base model layer four base model algorithms make up the first layer of boms a diverse set of algorithms is necessary to handle the complex relations between the predictor variables and ripple wavelength several different types of models were chosen to diversify predictive performance two machine learning algorithms and two deterministic empirical equilibrium ripple wavelength formulations choosing the number and type of base models for stacked generalization is subjective due to the lack of a specific process or technique and the individuality of the application eight machine learning base models linear regression svr linear kernel svr rbf kernel k neighbors regression random forest regression adaboost regression gradient boost regression and xgboost regression and four deterministic empirical ripple models traykovski et al 1999 nelson et al 2013 mogridge et al 1994 and soulsby and whitehouse 2005 were evaluated to determine which combination produced the highest overall prediction skill the testing resulted in a configuration with two machine algorithms gradient boosting regressor friedman 2002 friedman 2000 and xgboost regressor chen and guestrin 2016 and two deterministic empirical ripple formulations traykovski et al 1999 and nelson et al 2013 the inclusion of deterministic empirical ripple formulations as base models expanded the diversity through the implementation of non machine learning equations i e the parameters of these equations do not change during the training of the system these equations represent the present state of the art of deterministic ripple geometry predictions based on over 50 years of research and observations the nelson et al 2013 and soulsby and whitehouse 2005 equations are very similar equations with different coefficients while both traykovski et al 1999 and nelson et al 2013 are functions of wave orbital excursion traykovski et al 1999 includes an additional dependence on sediment settling velocity and wave period the mogridge et al 1994 set of equations dependent on the period parameter was also tested as a candidate addition to boms but was rejected due to poor predictive performance and no positive effect on the predictive skill of the boms system the two chosen formulations nelson et al 2013 and traykovski et al 1999 incorporate a variety of independent variables from which the empirical functions were derived from and resulted in the highest skill traykovski et al 1999 is given by 4 λ 0 75 d o u b w s 4 2 6 3 w s ω u b w s 4 2 where λ is the predicted ripple wavelength d o is wave semi orbital excursion u b is orbital velocity w s is particle settling velocity calculated using gibbs et al 1971 and ω is wave radian frequency 2 π t nelson et al 2013 is defined as 5 λ a b 0 72 2 0 1 0 3 a b d 50 1 exp 1 57 1 0 4 a b d 50 1 15 1 where a b is the wave orbital amplitude 2 d o section 2 1 discusses the need for data standardization in machine learning models however to avoid data leakage it is essential to maintain care when performing standardization scaling data leakage occurs when test data are accidentally included in the training data kaufman et al 2012 one of the most common causes of data leakage is improper fitting and transforming during standardization scaling fitting refers to calculating the parameters for a model equation using the training data i e training the model and transforming refers to applying the calculations performed during fitting to the data scaling can be especially problematic when using a ten fold cross validation because fitting a model with an already preprocessed dataset will result in a bias scikit learn s pipeline class mitigates this problem by only fitting the preprocessing methods to the training set for each fold to mitigate data leakage in boms each machine learning base model is contained within a separate pipeline the machine learning algorithms in the base model layer of boms are functions of adjustable parameters and hyperparameters that heavily influence how the model performs a model parameter refers to a configuration variable internal to a model and is determined while fitting the model to training data a hyperparameter refers to a configuration variable external to a model and is set by the user hyperparameters are typically tuned using grid search functions to determine the optimal values here a grid search constructed using scikit learn s gridsearchcv function efficiently determined the optimal hyperparameter values for the gradient boosting regressor machine learning base model while hyperparameter tuning is crucial for increasing model performance it has an inherent danger of overfitting to mitigate overfitting only the hyperparameters in the gradient boosting regressor base model were tuned the hyperparameters in the xgboost regressor base model were set to default values table 1 details the hyperparameter values applied in the gradient boosting regressor base model determined by the grid search another concern when utilizing machine learning models is a bias variance tradeoff a high bias results in under fitting the model to the training data an under fit model is too generalized and therefore cannot make precise predictions a high variance results in overfitting the model to the training data an overfit model is too sensitive to small fluctuations in the training data and indicates the model is not generalized enough reducing bias will increase variance and reducing variance will increase bias ample testing of the base model optimization is necessary to determine the ideal balance for the base models used in boms the optimized gradient boosting regression has lower bias and higher variance while the non optimized xgboost regression has higher bias and lower variance including one optimized model and one non optimized model decreases the likelihood of overfitting while not overly increasing the bias ten fold cross validation was also applied to mitigate overfitting in the machine learning models fig 5 ten fold cross validation uses subsets i e folds of the entire training dataset to train test and evaluate a model each fold possesses a unique test set to evaluate predictions of the model using a training set that excludes the test data ten fold cross validation is also utilized in the grid search process for tuning hyperparameters ensuring that each iteration of possible hyperparameter values is equally tested on the entirety of the training dataset model performance was evaluated with r a d j 2 rmse and bias table 2 shows the values of each of these performance metrics from the ten fold cross validations of each of the base models and the boms system as a whole i e the stacked system of the base models and bayesian regression r 2 is the percentage of the dependent feature variation that is explained by the model generally an r 2 value closer to 1 indicates the model explains most of the variability of the dependent feature around its mean since boms includes more than one predictor feature an adjusted r 2 which takes into account the number of independent variables was also used r a d j 2 increases decreases when significant insignificant predictors are added rmse is the square root of the average of the squared errors or a measure of the spread of the residuals generally a relatively small rmse value may indicate overfitting in conjunction with rmse bias was calculated as the average of the residuals bias can be viewed as an accuracy measurement as it is the difference between the predicted value s and the corresponding observed value s both rmse and bias are dimensional here in predicting ripple wavelength their units are meters m the base models demonstrated a range in skill in the ten fold cross validation table 2 the gradient boosting regression and the xgboost regression machine learning algorithms have high r a d j 2 scores rmse values of 8 2 and 8 7 cm respectively and have near zero biases these values indicate that while the machine learning algorithms are unbiased they may overfit the training data the two deterministic empirical equations are more biased have lower r a d j 2 scores and higher rmse values at 19 9 and 21 5 cm respectively while the deterministic empirical equations have higher errors they may have lower variances when predicting future data compared to the machine learning models that may be subject to slight overfitting all of the base models were negatively biased meaning they tended to under predict the training dataset after training each base model outputs its predictions of the test set observations for every cross validation fold the predictions were compiled in a new data frame that included the output from each of the base models as well as the observation values the new dataset was used as the input data to the meta learner section 2 3 2 3 meta learner bayesian regression the meta learner for boms a bayesian linear regression blr run via the bambi model building interface which utilizes the pymc3 salvatier et al 2016 package for python produced the final probabilistic predictions from the posterior distribution generated using markov chain monte carlo sampling in the blr approach bayesian inference determines the posterior distribution of the model features from a prior probability since the independent features input to the meta learner are the predictions from the base models the priors are unknown per standard practice a normal distribution was assigned as the prior probability for each parameter the blr was subjected to the same ten fold cross validation process as applied to the individual base models fig 6 the predictions from the ten fold cross validations of the base models were fed into the blr as predictor variables which serves as the training set for the blr the ten fold cross validation of the blr therefore represents the performance of the overall stacked system the results table 2 show that the stacked system boms performed better than any of the base models individually with an r a d j 2 value of 0 93 rmse of 8 0 cm and a near zero positive bias determination of the posterior probability relies on markov chain monte carlo mcmc sampling to produce a distribution of possible model parameters trace plots the particular mcmc algorithm used in boms is the no u turn sampler nuts hoffman and gelman 2014 to create the trace nuts was set to two chains at 2 000 draws and 2 000 iterations to tune the mean and standard deviation of the predicted value were extracted from the posterior a custom function was defined in python to evaluate the performance of the blr by comparing the predictions with the true observations the function draws 1 000 random samples from a gaussian distribution whose center and scale is the mean and standard deviation of the predicted value respectively too few random samples may result in an unrepresentative density curve the random sample values were used to create a histogram with a set number of bins m two formulas were used in tandem to determine the number of bins to use in the histogram to produce the boms equilibrium ripple wavelength estimate distribution plots sturges rule and the rice rule sturges rule which is widely used in many statistical packages for constructing histograms is given by 6 m s t u r g e s 1 l o g 2 n where m is the number of bins and n is the number of observations caution should be taken when using sturges rule as it has faced criticism for over smoothing hyndman 1995 therefore the rice rule was also used in determining the number of bins 7 m r i c e 2 n 3 the median of m s t u r g e s and m r i c e 15 bins was used as the overall number of bins these final histograms were used to produce the posterior probability plots to compare to observed ripple wavelengths here most probable value was compared to the observed value and the posterior distribution width indicates the model prediction uncertainty 3 results this section describes applicability and performance of boms at field sites with data not included in the optimization and training of the model system three separate field experiments in the gulf of mexico in 2013 and off the virginia and maryland coasts in 2014 and 2015 respectively provided observations of wave heights wave periods bottom velocities sediment grain size and ripple wavelength these datasets were preprocessed using the same methods that were applied to the nelson et al 2013 training dataset detailed in section 2 1 like the training dataset the field datasets used for validation were filtered to only include equilibrium ripples using the nelson et al 2013 equilibrium ripple criteria filter imputed to replace missing ripple height values and scaled in the pipeline ripple data with wavelengths exceeding 1 5 m were also filtered out to ensure only wave generated orbital ripples were included in the analysis the following subsections describe the field experiments 3 1 target and reverberation experiment trex13 the target and reverberation experiment trex13 off the coast of panama city florida in spring of 2015 included moored instruments to collect in situ observations of hydrodynamics and the seabed two instrumented quadpods at approximately 7 5 m and 20 m water depths collected data for 34 days 20 april 23 may 2013 an upward looking nortek awac ast recorded wave height period and direction at 2 hz for 1 024 s every 30 min bottom wave orbital excursions were calculated using linear wave theory and observations of wave height and period over the month long deployment the conditions ranged from calm significant wave heights of 0 24 m and peak periods of 5 s to fairly energetic significant wave heights of 2 m and peak periods of 8 s bedforms were observed with high frequency 2 25 mhz sector scanning sonar in about a 15 m 2 area of the seabed every 12 min penko et al 2017 ripple wavelengths were extracted from the acoustic backscatter data the sediment at the site was observed to have a median grain size of 0 23 mm the trex13 dataset was split into two subsets based on water depth one subset for data collected at the 7 5 m water depth and the other for data collected at 20 m water depth fig 7 is a plot of the ripple length observations in the two depths during the trex13 experiment with the blue lines representing the times when the ripples were in equilibrium with the instantaneous wave conditions as described in section 2 1 note that the 20 m depth observations have considerable scatter due to the acoustic backscatter extraction method and the only equilibrium ripples that occurred were during a large storm that passed over the instruments about midway through the experiment at the 7 5 m depth equilibrium ripples occur frequently before and after the storm 3 2 winter quarter shoals wqs14 the u s geological survey usgs and university of delaware deployed a bottom moored instrument frame in summer and fall of 2014 at winter quarter shoals wqs14 located 11 km off the coast of assateague island virginia see pendleton et al 2016 the depth at wqs ranges from 4 to 12 m both deployments were located on the shallow ne slope of the shoal the sediment at the site was observed to have a median grain size of 0 66 mm the frame was equipped with an upward looking teledyne rd instruments workhorse sentinel 600 khz acoustic doppler current profiler adcp to measure water column currents and surface waves as well as an imagenex 881 tilt head fanbeam rotary sonar for time lapse acoustic imagery of the seabed both instruments sampled every 60 min again bottom wave orbital excursions were calculated using linear wave theory and observations of wave height and period the first deployment occurred from june 19 aug 21 2014 the second deployment occurred from sep 2 oct 10 2014 nearly constant ripple formation and evolution of the seabed was observed over the course of both deployments except for several times of low wave energy when relict ripples existed fig 8 is a plot of the ripple wavelength observations during the wqs14 experiment with the blue lines representing the times when the ripples were in equilibrium with the instantaneous wave forcing as described in section 2 1 3 3 assateague island asis15 experiment an additional experiment performed by the usgs and university of delaware off the coast of assateague island in 2015 asis15 provided a rich dataset of bedform evolution see duval et al 2021 and trembanis et al 2019 the team deployed an upward looking teledyne rd instruments workhorse sentinel 600 khz adcp at 14 m water depth to measure currents and waves and an imagenex 881 tilt head fanbeam rotary sonar to capture acoustic imagery of the seabed every hour the sediment samples taken at the site indicated poorly sorted gravelly sand with a median grain size of 1 03 mm hurricane joaquin passed through the experiment site in early october generating near bed velocities over 1 m s and generating large ripples with wavelengths o 1 m fig 9 is a plot of the ripple length observations during the asis15 experiment with the blue lines representing the times when the ripples were in equilibrium with the instantaneous wave forcing as described in section 2 1 3 4 model observation comparisons boms trained with the nelson et al 2013 dataset was used to produce a timeseries of posterior distributions of equilibrium ripple wavelengths observed during the three field experiments fig 10 is a plot of the most probable value of the posterior distributions provided by boms plotted versus the observed value for each of the field sites perfect predictions would be plotted directly on the 1 1 line boms performance metrics for each site are in table 3 the rmse and bias are fairly consistent between the different site locations and range from 5 to 11 cm and 4 and 5 cm respectively in general boms was more likely to over predict especially in the low mid wavelengths between 0 3 m and 0 6 m timeseries of the most probable ripple wavelengths from boms at the equilibrium times figs 7 9 and all the observed ripple wavelengths are plotted in fig 11 despite the apparent bias in the scatter plots boms agrees well with the observed wavelengths at the equilibrium times during trex13 the predictions made before and after the high energy event which occurred between around 2013 05 03 and 2013 05 08 agreed extremely well with the observed wavelengths during the high energy event at the 7 5 m location the wave forcing is above the critical threshold and the ripples are washed out to a planar bed during the high energy event at the 20 m location the ripples during the first half of trex13 were generated by a previous wave condition i e they are relict ripples however as the storm passes over the site on 2013 05 05 the ripples are formed by the extreme forcing and boms only slightly over predicts the ripple wavelengths which then persist as relict ripples for the remainder of the deployment the wqs14 site is characterized by frequent ripple formation and evolution during this time period boms agreed well with the observed ripple wavelengths especially when greater than 0 5 m but tended to slightly over predict in the low mid wavelengths 0 2 0 5 m however overall boms captures the frequent changes in the ripple wavelengths throughout the experiment examination of the asis15 predictions reveals recurring themes seen in the other field sites boms tended to over predict the low mid wavelengths while still capturing the trends of ripple wavelength change during a high energy event in early october hurricane joaquin the over predictions turned to slight under predictions as the wavelengths increased to above 1 0 m as mentioned in section 2 1 boms also outputs an estimate of the ripple height along with the ripple wavelength and flow statistics however it is not plotted here as there were no observations of ripple height measured for which to compare unlike previous deterministic models that use a least squares fit to derive an equation boms provides a distribution of predicted values the mean predicted values and standard deviation determined by the mcmc trace in the bayesian regression were used to construct a density histogram for each sample the density plots can be converted into probability plots by extracting the bin information and normalizing each density value by the number of estimates boms made per sample the probability histograms from a randomly sampled time during each of the field experiments are plotted in fig 12 for visualization purposes the most probable ripple wavelength versus the mean wavelength is denoted with a blue dashed line in each plot ideally the distributions would be narrow indicating a small standard deviation with mean values close to the observed values denoted by a black solid line as seen in fig 12 for these randomly sampled times the observed value typically falls within one standard deviation or less of the most probable predicted value the inherent advantage of using machine learning system like boms is ability to have not only a prediction of ripple wavelength but a probability distribution that also provides the model uncertainty unlike the previous state of the art empirical deterministic predictions 4 discussion 4 1 comparisons to other models it is useful to know the utility of boms and how it compares to other models commonly used to predict equilibrium wavelength boms was compared to the pre existing empirical equilibrium ripple wavelength predictors included in the base model layer the traykovski et al 1999 predictor and the nelson et al 2013 predictor table 4 using the pre processed nelson et al 2013 training dataset described in section 2 2 a ten fold cross validation was applied to each predictor a normalized rmse eq 8 was used to compare models outside of this study where y is the mean of the observations 8 nrmse rmse y boms with a nrmse of 0 19 has less residual variance than the traykovski et al 1999 and nelson et al 2013 predictors with nrmse values of 0 47 and 0 51 respectively goldstein et al 2013 and yan et al 2008 are the two other published machine learning models to predict ripple geometry in the literature however we cannot directly compare to these two models due to the different outputs the gp predictor of goldstein et al 2013 reportedly had an nrmse of 0 74 also normalized with the mean of the observations while most of the data in the nelson et al 2013 dataset is utilized in goldstein et al 2013 the data did not undergo the same pre processing techniques as were performed here most notably they assumed all field measurements of ripples were at or near equilibrium this assumption has the potential to produce higher errors in the model output resulting in a higher nrmse yan et al 2008 did not report on the nrmse of their ann and instead used the statistical parameters of scatter index correlation coefficient and the mean geometric deviation therefore the ann also cannot be directly compared to the boms results however boms overall outperformed the other equilibrium ripple predictors and also includes the model uncertainty a quantity not output by the other models 4 2 model limitations and applicability machine learning model skill is inherently limited by the range of conditions that exist in a model s training dataset the filtering of the training data described in section 2 1 deliberately removes any observations that may not be wave generated orbital ripples in equilibrium with the instantaneous wave forcing due to the different physical processes and time length scales associated with other types of bedforms e g anorbital ripples relict non equilibrium ripples current combined wave current generated bedforms megaripples etc we also focused on sandy locations due to the difference in ripple formation in heterogeneous and or muddy sediments the filtering of the data affects the prediction of any ripples outside the range of conditions in the training data therefore boms will have better skill in sandy wave dominated areas such as the coastal nearshore than it will in muddy current dominated areas such as a tidal inlet the advantage of boms is that it will still output a prediction for any given input outside the range of its trained conditions however the model will recognize that it was not trained with any similar observations and will provide a wide distribution indicating a large prediction uncertainty the user can then take into account the uncertainty when analyzing model skill we can examine the prediction range and limitations of the model given by the range of conditions included in the training data one hypothesis explaining the differences in model skill between the field sites was that the range of tested field data conditions may not have been completely included within the range of the training data histogram plots of the variable ranges fig 13 from the training blue shaded regions and testing colored shaded regions data show the comparison of the conditions for each dataset the variable ranges in the field datasets predominately fell within those of the training dataset the most profound deviation is the median grain size distribution plot fig 13c each field dataset is composed of only one constant median grain size for a total of 3 grain sizes in the testing datasets the trex13 and asis15 median grain size values are well represented in the training data however the number of samples in the training dataset that have a median grain size value equal to that of the wqs14 dataset is several orders of magnitude smaller this difference may or may not account for the tendency of boms to over predict when tested on the wqs14 data but it is a factor that should be considered when using boms as a prediction tool another difference between the training and testing datasets is that the training dataset includes both laboratory and field observations however research has shown that ripples behave differently in the laboratory compared to the field o donoghue and clubb 2001 calantoni et al 2013 and the relations between the variables observed in the laboratory experiments may not adequately represent the relations between the variables as they occur in nature i e field experiments future work could include compiling additional field data and separating the data into laboratory and field training datasets for laboratory and field predictions respectively not every possible base model and predictor variable combination was tested during the initial construction of boms however the system was built to be an evolving model that can be continually changed and re optimized further experimentation of combinations of different base models could yield an overall more skillful predictor the addition of more training data particularly those with observations that exceed the ranges currently in the training dataset would likely make boms more robust as the model system is exposed to new environmental conditions and the relations between those conditions boms is a versatile model system that is not restricted to predicting only equilibrium ripple wavelength the framework is conducive to be applied to predict many ripple and sediment transport parameters anorbital ripple wavelength suspended sediment concentration boundary layer thickness ripple orientation megaripples etc given a significant set of observations and empirical prediction formulations here we chose an important parameter needed for time dependent wbbl models and for which there existed many observations for the prediction of non equilibrium ripples the duration of the forcing the equilibrium ripples the seafloor is being driven to and the time at which the ripples stop changing are all extremely important parameters that need to be accounted for in a non equilibrium ripple model boms was developed to produce distributions of wave generated equilibrium ripple wavelengths needed to drive a probabilistic time dependent ripple model however the boms framework can be trained with any number of features and base models future iterations of boms will include predictions of current and combined wave current generated ripples and suspended sediment profiles boms is presently run using a graphical user interface gui that allows users to make predictions as well as add their own training datasets the probabilistic output allows for the analysis of prediction uncertainties this feature is especially useful when applied as a forecasting tool there has been an increasing demand for probabilistic predictions of hydrodynamic and sediment transport conditions boms is a step in the direction of providing distributions of seafloor sediment parameters for use in ensemble modeling systems 5 conclusions the bayesian optimal model system boms predicts probabilistic distributions of wave generated equilibrium sand ripple wavelengths and estimations of a deterministic ripple height using machine learning techniques in combination with pre existing deterministic empirical equilibrium ripple predictors the model system was trained and validated using the 50 year ripple dataset compiled by nelson et al 2013 which contains both field and laboratory observations model testing was performed with three separate field datasets not used in model training each from a different location and with unique environmental conditions boms has the ability to model uncertainties as well as the potential to become more robust with the addition of more training data overall boms provides more accurate predictions of ripple wavelength during both cross validation and testing compared to the performance of each independent base model and other common equilibrium ripple predictors the structure of boms built with the machine learning technique of stacked generalization was shown to be a viable method for predicting wave generated equilibrium ripple wavelength practical applications of boms include probabilistic ripple geometry forecasting and the coupling with time dependent ripple and sediment transport models credit authorship contribution statement r e phillip designed the research performed the research wrote all model code analyzed the data wrote the paper a m penko designed the research analyzed the data wrote the paper provided supervision project administration funding acquisition m l palmsten research conception edited the paper c b duval provided field datasets analyzed the data edited the paper declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the acquisition of the wqs14 and asis15 field data was supported by the u s national park service united states award p14ac00380 and the u s geological survey award g14ac00088 data collected by the asis15 experiment are available for download on the zenodo repository at https doi org 10 5281 zenodo 4519547 data collected by the wqs14 experiment are available via a usgs data release https doi org 10 5066 f7mw2f60 trex13 was jointly funded by the office of naval research onr the u s naval research laboratory nrl and the strategic environmental research and development program serdp munitions response program mr 2320 data are available upon request of the author a m p and r e p were supported by u s naval research laboratory base funding c b d was jointly funded by a national research council postdoctoral fellowship and u s naval research laboratory base funding m l p was funded by u s naval research laboratory base funding at the time of research conception and by the u s geological survey thereafter the authors would like to thank art trembanis at the university of delaware for sharing the wqs14 and asis15 data any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government 
25520,sand ripples are geomorphic features on the seafloor that affect bottom boundary layer dynamics including wave attenuation and sediment transport we present a new equilibrium ripple predictor using a machine learning approach that outputs a probability distribution of wave generated equilibrium wavelengths and statistics including an estimate of ripple height the most probable ripple wavelength and sediment and flow parameterizations the bayesian optimal model system boms is an ensemble machine learning system that combines two machine learning algorithms and two deterministic empirical ripple predictors with a bayesian meta learner to produce probabilistic wave generated equilibrium ripple wavelength estimates in sandy locations a ten fold cross validation of boms resulted in an adjusted r squared value of 0 93 and an average root mean square error rmse of 8 0 cm during both cross validation and testing on three unique field datasets boms provided more accurate wavelength predictions than each individual base model and other common ripple predictors keywords sand ripples bedforms seafloor machine learning probabilistic prediction 1 introduction sand ripples form on the seafloor when bottom velocities generated by waves and currents mobilize sediment equilibrium ripples occur when the hydrodynamic forcing exceeds a critical threshold and persists long enough to generate ripples correlated to the amplitude and frequency of bottom fluid velocities the presence of ripples influences bottom boundary layer hydrodynamics as well as sediment transport and resuspension equilibrium ripple dimensions are an important parameter in time dependent wave bottom boundary layer wbbl models therefore the relation between hydrodynamic forcing and ripple geometry is fundamental for accurate predictions of the formation and evolution of seafloor ripples in shallow water changing wave conditions cause the continual reworking of seafloor sediments in sandy shallow areas ripples are constantly being diffused generated and evolved and planed out by low mid and high bottom shear respectively produced by waves and currents one could argue that ripples in nature are very rarely in an equilibrium state however time dependent wbbl models depend on equilibrium ripple geometry information to drive ripples from their present state to a state correlated with the instantaneous wave forcing time dependent bottom boundary models can provide valuable information on the seabed state and the potential for the mobilization of seafloor sediments traykovski 2007 developed a time dependent spectral ripple model by estimating the change in the seafloor spectrum with time as a function of the bottom velocity amplitude and frequency an adjustment timescale factor and the equilibrium ripple height and length associated with the instantaneous wave forcing that model was expanded to two dimensions nelson and voulgaris 2015 to include current generated ripples soulsby et al 2012 and applied to field data to explain a measured acoustic response penko et al 2017 these time dependent models rely on an accurate estimation of equilibrium ripple geometry in order to estimate the time dependent response of ripples to changing wave forcing decades of research on equilibrium ripples in sandy regions has resulted in many ripple geometry prediction equations using least squares fit to equilibrium ripple observations e g pedocchi and garcia 2009 soulsby and whitehouse 2005 grasmeijer and kleinhans 2004 faraci and foti 2002 styles and glenn 2002 wiberg and harris 1994 mogridge et al 1994 van rijn et al 1993 grant and madsen 1982 nielsen 1981 and others see nelson et al 2013 for an extensive review of existing equilibrium ripple equations most equations predict ripple length and ripple height or steepness as functions of hydrodynamic forcing parameters e g mobility number orbital excursion grain size shields parameter etc despite many of the equations being developed from the same set of observations significant uncertainty remains in estimates of ripple dimensions equation skill has increased as new data were collected however the method of fitting a deterministic equation to observations of ripple geometry has remained the same machine learning ml methods are an effective approach to build prediction models for data rich problems with an ever increasing volume of available data and increases in computational power and technology ml is a promising method to use in the marine and coastal science fields machine learning for analytical model building is a sub field of artificial intelligence in which mathematical algorithms learn and improve from experience to produce predictions without being explicitly programmed the many ml algorithms range from simple e g linear regression to complex e g neural networks each with its own benefits and disadvantages determining which algorithm s to use for a specific problem may require extensive trial and error testing it is common to test a variety of algorithms and choose the one or combination that performs best for a particular problem the review by goldstein et al 2019 examines coastal morphodynamic and sediment transport studies that used machine learning the review presents rough guidelines for the most effective algorithm s to address marine and coastal science problems several researchers have applied ml to the prediction of sand ripple geometry the first used an artificial neural network ann built with laboratory and field observations yan et al 2008 while the model showed promising validation results when compared to existing empirical ripple predictors it was not tested on data outside the training data which was very limited 347 data points goldstein et al 2013 built a genetic programming gp system to predict wave generated ripple height wavelength and steepness using sediment grain size and orbital excursion data genetic programming optimizes a population of predictor algorithms via a process that simulates darwinian evolution they used a very similar dataset to that presented here and the product represented the current state of the art in ml applied to ripple geometry prediction until now this paper presents a very different methodology using an ensemble machine learning technique and model stacking as the foundation for the equilibrium ripple wavelength prediction system and tests the model s accuracy with timeseries of field observed ripple wavelengths 2 methods a machine learning approach was used to create a new model to predict equilibrium wave orbital ripple geometry generated by wave forcing while ripples can also be generated by current and combined wave current flows here we are only addressing wave generated ripples due to the amount of observations available the approach results in a probability distribution of equilibrium ripple wavelengths that provides prediction uncertainty as well as a deterministic equilibrium ripple height machine learning methods typically use a dataset to train a model using correlations between the independent and dependent variables the choice of the models and the spread of the training data influence the overall skill of the resulting predictions the complex nature of ripples demands techniques that can represent both linear and nonlinear processes such as ensemble machine learning techniques the implementation of ensemble machine learning has been applied in coastal zone studies e g beuzen et al 2019 and simmons and splinter 2022 here we focused on the ensemble machine learning method of model stacking to aggregate a variety of models into one system model stacking wolpert 1992 or stacked generalization uses the predictions from one or more base algorithms chosen by the user as the input to a final algorithm this final algorithm sometimes called the meta learner and also chosen by the user combines the predictions from the base algorithms by deducing their biases to produce a final optimal prediction the bayesian optimal model system boms presented here was created as a specialized machine learning stacked generalizer that combines multiple base model algorithms with a bayesian meta learner to produce probabilistic equilibrium ripple wavelength predictions fig 1 boms was coded in the python 3 9 5 programming language and utilizes machine learning algorithms from the scikit learn library and the bayesian model building interface bambi capretto et al 2020 the system was trained with the 50 year dataset compiled by nelson et al 2013 see p 3208 table 3 for an exhaustive list and range of conditions the compilation contains observations from both field and laboratory studies including controlled laboratory simulations diver observations underwater camera observations and field observations from stationary sector scanning sonar and velocimeters parameters included in the compilation are ripple height ripple wavelength median grain size wave orbital velocity semi orbital excursion water depth wave period and water density nelson et al 2013 used these data to develop the most recent empirical deterministic equilibrium ripple predictor formulation that typically outperforms all previously developed predictor formulations and was the present state of the art equilibrium geometry model until now 2 1 data preprocessing preprocessing refers to the preparation and transformation of data to make it suitable input to boms preprocessing includes filtering handling null values and scaling since the objective was to only model wave generated equilibrium ripples and the full compiled dataset included field timeseries data it was necessary to filter the dataset to include only times when ripples were in equilibrium with the instantaneous wave forcing specifically for the field datasets the times when the ripples were in transition or when the bottom shear stress was below the critical threshold for sediment motion needed to be excluded from the training data to exclude the non equilibrium ripples from the dataset the equilibrium ripple criteria established in nelson et al 2013 were used first the shields parameter was used to parameterize the bottom shear stress imposed on the seafloor by the waves 1 θ 0 5 f w u b 2 s 1 g d 50 where f w is the friction factor swart 1974 u b is the wave orbital velocity at the bed s is the specific gravity of the sediment and d 50 is the mean grain size diameter to ensure that the forcing was producing sediment mobilization shields parameter values less than a critical threshold were removed from the dataset for laboratory data the equilibrium criterion in nelson et al 2013 states that the shields parameter be greater than the critical shields parameter θ c r 0 05 for sediment motion for non timeseries field observations i e single points in time the equilibrium criterion states that the shields parameter be greater than two times the critical shields parameter θ 2 θ c r to ensure the shear stress is great enough for timely ripple transitions for timeseries field observations a critical shields parameter and an adjustment timescale parameter t k traykovski 2007 was used to approximate the minimum time interval of persistent wave forcing in order to fully adjust the instantaneous ripples to a new profile the adjustment timescale parameter is the ratio of the cross sectional area of a given ripple profile to the bed load sediment transport rate at a given time t and is defined as 2 t k t α 1 ϕ 0 16 λ t 2 2 q t where λ is the ripple wavelength α is an empirical time constant ϕ is the sediment porosity 0 4 and q is the volumetric sediment transport rate as defined by meyer peter and müller 1948 the adjustment timescale parameter represents the relative amount of time to transport the sediment in a unit volume ripple with wavelength λ under instantaneous wave forcing producing a sediment transport rate q at time t it provides a factor that allows for larger ripples under low stresses to require a longer time interval of persistent wave forcing to adjust than small ripples under high stresses timeseries field data are considered in equilibrium with forcing when 3 0 d θ d t 0 1 θ t t k t as well as θ 1 5 θ c r to exclude low energy conditions the d θ d t term represents the persistence of the wave forcing in time and the 0 1 θ t k term allows for up to a 10 change in the instantaneous wave forcing over the time interval required for the adjustment the initial nelson et al 2013 dataset consisted of 12 215 data points also similar to the methodology in nelson et al 2013 we specifically filter the dataset to only include wave generated orbital ripples with wavelengths less than a specific threshold a maximum ripple wavelength threshold of 1 m for laboratory data and 1 5 m for field data was applied to filter out potential megaripples that have longer wavelengths due to being forced by other processes tides infragravity waves etc the final equilibrium ripple filtered dataset consisted of 3 622 data points the scikit learn models used in boms required the removal or replacement of null values within the training and testing datasets several of the training datasets did not include ripple height measurements with the ripple and hydrodynamic observations historical ripple height observations are much sparser than ripple wavelength observations due to the typical acoustic instrumentation used in the field acoustic backscatter which does not provide direct ripple height information for those data scikit learn s knnimputer class was used to impute missing ripple height values using datasets that did include ripple height measurements fig 2 imputation refers to substituting missing or null data with estimated values imputing missing values via k nearest neighbors knnimputer is among the more robust methods of imputation troyanskaya et al 2001 compared to other methods of handling missing values such as substituting with the mean or the mode of the observed values the nearest neighbors to the missing value s are determined using euclidean distance the number of nearest neighbors is defined by the user here the number of nearest neighbors used is ten the user can also define the weight of the nearest neighbor values when calculating an estimate for the missing value s here the weights are based on the distance to each neighbor in order to determine the skill of the knnimputer s prediction of ripple height and since it is a ml algorithm a ten fold cross validation was performed on the imputation of ripple height values in the equilibrium ripple filtered training dataset fig 3 the ten fold cross validation resulted in an adjusted r squared r a d j 2 0 88 and root mean square error rmse 0 014 m the knnimputer also functions as a deterministic ripple height predictor since the algorithm essentially estimates a ripple height given hydrodynamic input imputed ripple height values are included with the statistical output of boms which also includes the most probable ripple wavelength and the sediment and flow parameterizations the knn algorithm determines the nearest neighbors based on euclidean distance and as a result the data must be scaled appropriately scaling was performed to standardize the independent features of a dataset so distributions will have a mean value of 0 and a standard deviation of 1 the mean of each feature was subtracted from each value in that feature column and divided by the standard deviation of the feature column standardization scaling was performed using scikit learn s standardscaler class inverse transformation was applied to the data after imputation so the values resembled the original units of measurement this process can also be applied to any new observation data in the development of boms predictor variables or features had to be determined for the system an initial list of features was built from parameters important for bottom boundary layer flow and sediment processes that have been previously used in deterministic empirical ripple predictors these parameters included bottom wave orbital velocity wave semi orbital excursion median grain size diameter sediment density fluid density and gravity however bottom wave orbital velocity and wave semi orbital excursion are not easily observed unless bottom mounted instrumentation is deployed instead linear wave theory and the dispersion relation is commonly applied to estimate the bottom velocity and orbital excursion from observed water depths wave heights and periods which are easily observed from wave buoys therefore the tested predictor variables for boms included the parameters that were most often included in the nelson et al 2013 training dataset and then feature selection was used to determine the most beneficial variables or features to include as predictors here we used spearman s correlation to determine the monotonic relationship between variables in order to select the best features for boms spearman s rank correlation coefficients were calculated between the all the observed variables in the training dataset ripple wavelength ripple height orbital velocity semi orbital excursion and wave period to determine the strengths of any potential linear and or nonlinear monotonic relationships fig 4 coefficients were calculated within each laboratory field experiment subset in the full training dataset therefore variables that were constant within each experiment e g sediment specific gravity median grain size and water depth were not included in the coefficient calculations i e since their value was the same for every other observation in that experiment their correlation coefficients were zero however both sediment grain size and water depth were included as features in the final training dataset the calculations were performed after filtering the datasets for equilibrium ripple data points and imputing any missing ripple height observations coefficient values range from 1 to 1 where a value of 1 indicates a perfect negative relationship between two features and a value of 1 indicates a perfect positive relationship between two features a coefficient value of 0 indicates no existing relationship between two features this work focused on the spearman correlations between the independent features and ripple wavelength analysis of the spearman correlations shows that ripple wavelength has a strong positive monotonic relationship with ripple height and moderate positive monotonic relationships with wave period and semi orbital excursion orbital velocity was the only feature in the correlation matrix to have a weak relationship with ripple wavelength therefore it was excluded as a predictor variable after all the data preprocessing the training dataset consisted of 3 499 samples and six features median grain size wave period water depth semi orbital excursion ripple height and ripple wavelength 2 2 base model layer four base model algorithms make up the first layer of boms a diverse set of algorithms is necessary to handle the complex relations between the predictor variables and ripple wavelength several different types of models were chosen to diversify predictive performance two machine learning algorithms and two deterministic empirical equilibrium ripple wavelength formulations choosing the number and type of base models for stacked generalization is subjective due to the lack of a specific process or technique and the individuality of the application eight machine learning base models linear regression svr linear kernel svr rbf kernel k neighbors regression random forest regression adaboost regression gradient boost regression and xgboost regression and four deterministic empirical ripple models traykovski et al 1999 nelson et al 2013 mogridge et al 1994 and soulsby and whitehouse 2005 were evaluated to determine which combination produced the highest overall prediction skill the testing resulted in a configuration with two machine algorithms gradient boosting regressor friedman 2002 friedman 2000 and xgboost regressor chen and guestrin 2016 and two deterministic empirical ripple formulations traykovski et al 1999 and nelson et al 2013 the inclusion of deterministic empirical ripple formulations as base models expanded the diversity through the implementation of non machine learning equations i e the parameters of these equations do not change during the training of the system these equations represent the present state of the art of deterministic ripple geometry predictions based on over 50 years of research and observations the nelson et al 2013 and soulsby and whitehouse 2005 equations are very similar equations with different coefficients while both traykovski et al 1999 and nelson et al 2013 are functions of wave orbital excursion traykovski et al 1999 includes an additional dependence on sediment settling velocity and wave period the mogridge et al 1994 set of equations dependent on the period parameter was also tested as a candidate addition to boms but was rejected due to poor predictive performance and no positive effect on the predictive skill of the boms system the two chosen formulations nelson et al 2013 and traykovski et al 1999 incorporate a variety of independent variables from which the empirical functions were derived from and resulted in the highest skill traykovski et al 1999 is given by 4 λ 0 75 d o u b w s 4 2 6 3 w s ω u b w s 4 2 where λ is the predicted ripple wavelength d o is wave semi orbital excursion u b is orbital velocity w s is particle settling velocity calculated using gibbs et al 1971 and ω is wave radian frequency 2 π t nelson et al 2013 is defined as 5 λ a b 0 72 2 0 1 0 3 a b d 50 1 exp 1 57 1 0 4 a b d 50 1 15 1 where a b is the wave orbital amplitude 2 d o section 2 1 discusses the need for data standardization in machine learning models however to avoid data leakage it is essential to maintain care when performing standardization scaling data leakage occurs when test data are accidentally included in the training data kaufman et al 2012 one of the most common causes of data leakage is improper fitting and transforming during standardization scaling fitting refers to calculating the parameters for a model equation using the training data i e training the model and transforming refers to applying the calculations performed during fitting to the data scaling can be especially problematic when using a ten fold cross validation because fitting a model with an already preprocessed dataset will result in a bias scikit learn s pipeline class mitigates this problem by only fitting the preprocessing methods to the training set for each fold to mitigate data leakage in boms each machine learning base model is contained within a separate pipeline the machine learning algorithms in the base model layer of boms are functions of adjustable parameters and hyperparameters that heavily influence how the model performs a model parameter refers to a configuration variable internal to a model and is determined while fitting the model to training data a hyperparameter refers to a configuration variable external to a model and is set by the user hyperparameters are typically tuned using grid search functions to determine the optimal values here a grid search constructed using scikit learn s gridsearchcv function efficiently determined the optimal hyperparameter values for the gradient boosting regressor machine learning base model while hyperparameter tuning is crucial for increasing model performance it has an inherent danger of overfitting to mitigate overfitting only the hyperparameters in the gradient boosting regressor base model were tuned the hyperparameters in the xgboost regressor base model were set to default values table 1 details the hyperparameter values applied in the gradient boosting regressor base model determined by the grid search another concern when utilizing machine learning models is a bias variance tradeoff a high bias results in under fitting the model to the training data an under fit model is too generalized and therefore cannot make precise predictions a high variance results in overfitting the model to the training data an overfit model is too sensitive to small fluctuations in the training data and indicates the model is not generalized enough reducing bias will increase variance and reducing variance will increase bias ample testing of the base model optimization is necessary to determine the ideal balance for the base models used in boms the optimized gradient boosting regression has lower bias and higher variance while the non optimized xgboost regression has higher bias and lower variance including one optimized model and one non optimized model decreases the likelihood of overfitting while not overly increasing the bias ten fold cross validation was also applied to mitigate overfitting in the machine learning models fig 5 ten fold cross validation uses subsets i e folds of the entire training dataset to train test and evaluate a model each fold possesses a unique test set to evaluate predictions of the model using a training set that excludes the test data ten fold cross validation is also utilized in the grid search process for tuning hyperparameters ensuring that each iteration of possible hyperparameter values is equally tested on the entirety of the training dataset model performance was evaluated with r a d j 2 rmse and bias table 2 shows the values of each of these performance metrics from the ten fold cross validations of each of the base models and the boms system as a whole i e the stacked system of the base models and bayesian regression r 2 is the percentage of the dependent feature variation that is explained by the model generally an r 2 value closer to 1 indicates the model explains most of the variability of the dependent feature around its mean since boms includes more than one predictor feature an adjusted r 2 which takes into account the number of independent variables was also used r a d j 2 increases decreases when significant insignificant predictors are added rmse is the square root of the average of the squared errors or a measure of the spread of the residuals generally a relatively small rmse value may indicate overfitting in conjunction with rmse bias was calculated as the average of the residuals bias can be viewed as an accuracy measurement as it is the difference between the predicted value s and the corresponding observed value s both rmse and bias are dimensional here in predicting ripple wavelength their units are meters m the base models demonstrated a range in skill in the ten fold cross validation table 2 the gradient boosting regression and the xgboost regression machine learning algorithms have high r a d j 2 scores rmse values of 8 2 and 8 7 cm respectively and have near zero biases these values indicate that while the machine learning algorithms are unbiased they may overfit the training data the two deterministic empirical equations are more biased have lower r a d j 2 scores and higher rmse values at 19 9 and 21 5 cm respectively while the deterministic empirical equations have higher errors they may have lower variances when predicting future data compared to the machine learning models that may be subject to slight overfitting all of the base models were negatively biased meaning they tended to under predict the training dataset after training each base model outputs its predictions of the test set observations for every cross validation fold the predictions were compiled in a new data frame that included the output from each of the base models as well as the observation values the new dataset was used as the input data to the meta learner section 2 3 2 3 meta learner bayesian regression the meta learner for boms a bayesian linear regression blr run via the bambi model building interface which utilizes the pymc3 salvatier et al 2016 package for python produced the final probabilistic predictions from the posterior distribution generated using markov chain monte carlo sampling in the blr approach bayesian inference determines the posterior distribution of the model features from a prior probability since the independent features input to the meta learner are the predictions from the base models the priors are unknown per standard practice a normal distribution was assigned as the prior probability for each parameter the blr was subjected to the same ten fold cross validation process as applied to the individual base models fig 6 the predictions from the ten fold cross validations of the base models were fed into the blr as predictor variables which serves as the training set for the blr the ten fold cross validation of the blr therefore represents the performance of the overall stacked system the results table 2 show that the stacked system boms performed better than any of the base models individually with an r a d j 2 value of 0 93 rmse of 8 0 cm and a near zero positive bias determination of the posterior probability relies on markov chain monte carlo mcmc sampling to produce a distribution of possible model parameters trace plots the particular mcmc algorithm used in boms is the no u turn sampler nuts hoffman and gelman 2014 to create the trace nuts was set to two chains at 2 000 draws and 2 000 iterations to tune the mean and standard deviation of the predicted value were extracted from the posterior a custom function was defined in python to evaluate the performance of the blr by comparing the predictions with the true observations the function draws 1 000 random samples from a gaussian distribution whose center and scale is the mean and standard deviation of the predicted value respectively too few random samples may result in an unrepresentative density curve the random sample values were used to create a histogram with a set number of bins m two formulas were used in tandem to determine the number of bins to use in the histogram to produce the boms equilibrium ripple wavelength estimate distribution plots sturges rule and the rice rule sturges rule which is widely used in many statistical packages for constructing histograms is given by 6 m s t u r g e s 1 l o g 2 n where m is the number of bins and n is the number of observations caution should be taken when using sturges rule as it has faced criticism for over smoothing hyndman 1995 therefore the rice rule was also used in determining the number of bins 7 m r i c e 2 n 3 the median of m s t u r g e s and m r i c e 15 bins was used as the overall number of bins these final histograms were used to produce the posterior probability plots to compare to observed ripple wavelengths here most probable value was compared to the observed value and the posterior distribution width indicates the model prediction uncertainty 3 results this section describes applicability and performance of boms at field sites with data not included in the optimization and training of the model system three separate field experiments in the gulf of mexico in 2013 and off the virginia and maryland coasts in 2014 and 2015 respectively provided observations of wave heights wave periods bottom velocities sediment grain size and ripple wavelength these datasets were preprocessed using the same methods that were applied to the nelson et al 2013 training dataset detailed in section 2 1 like the training dataset the field datasets used for validation were filtered to only include equilibrium ripples using the nelson et al 2013 equilibrium ripple criteria filter imputed to replace missing ripple height values and scaled in the pipeline ripple data with wavelengths exceeding 1 5 m were also filtered out to ensure only wave generated orbital ripples were included in the analysis the following subsections describe the field experiments 3 1 target and reverberation experiment trex13 the target and reverberation experiment trex13 off the coast of panama city florida in spring of 2015 included moored instruments to collect in situ observations of hydrodynamics and the seabed two instrumented quadpods at approximately 7 5 m and 20 m water depths collected data for 34 days 20 april 23 may 2013 an upward looking nortek awac ast recorded wave height period and direction at 2 hz for 1 024 s every 30 min bottom wave orbital excursions were calculated using linear wave theory and observations of wave height and period over the month long deployment the conditions ranged from calm significant wave heights of 0 24 m and peak periods of 5 s to fairly energetic significant wave heights of 2 m and peak periods of 8 s bedforms were observed with high frequency 2 25 mhz sector scanning sonar in about a 15 m 2 area of the seabed every 12 min penko et al 2017 ripple wavelengths were extracted from the acoustic backscatter data the sediment at the site was observed to have a median grain size of 0 23 mm the trex13 dataset was split into two subsets based on water depth one subset for data collected at the 7 5 m water depth and the other for data collected at 20 m water depth fig 7 is a plot of the ripple length observations in the two depths during the trex13 experiment with the blue lines representing the times when the ripples were in equilibrium with the instantaneous wave conditions as described in section 2 1 note that the 20 m depth observations have considerable scatter due to the acoustic backscatter extraction method and the only equilibrium ripples that occurred were during a large storm that passed over the instruments about midway through the experiment at the 7 5 m depth equilibrium ripples occur frequently before and after the storm 3 2 winter quarter shoals wqs14 the u s geological survey usgs and university of delaware deployed a bottom moored instrument frame in summer and fall of 2014 at winter quarter shoals wqs14 located 11 km off the coast of assateague island virginia see pendleton et al 2016 the depth at wqs ranges from 4 to 12 m both deployments were located on the shallow ne slope of the shoal the sediment at the site was observed to have a median grain size of 0 66 mm the frame was equipped with an upward looking teledyne rd instruments workhorse sentinel 600 khz acoustic doppler current profiler adcp to measure water column currents and surface waves as well as an imagenex 881 tilt head fanbeam rotary sonar for time lapse acoustic imagery of the seabed both instruments sampled every 60 min again bottom wave orbital excursions were calculated using linear wave theory and observations of wave height and period the first deployment occurred from june 19 aug 21 2014 the second deployment occurred from sep 2 oct 10 2014 nearly constant ripple formation and evolution of the seabed was observed over the course of both deployments except for several times of low wave energy when relict ripples existed fig 8 is a plot of the ripple wavelength observations during the wqs14 experiment with the blue lines representing the times when the ripples were in equilibrium with the instantaneous wave forcing as described in section 2 1 3 3 assateague island asis15 experiment an additional experiment performed by the usgs and university of delaware off the coast of assateague island in 2015 asis15 provided a rich dataset of bedform evolution see duval et al 2021 and trembanis et al 2019 the team deployed an upward looking teledyne rd instruments workhorse sentinel 600 khz adcp at 14 m water depth to measure currents and waves and an imagenex 881 tilt head fanbeam rotary sonar to capture acoustic imagery of the seabed every hour the sediment samples taken at the site indicated poorly sorted gravelly sand with a median grain size of 1 03 mm hurricane joaquin passed through the experiment site in early october generating near bed velocities over 1 m s and generating large ripples with wavelengths o 1 m fig 9 is a plot of the ripple length observations during the asis15 experiment with the blue lines representing the times when the ripples were in equilibrium with the instantaneous wave forcing as described in section 2 1 3 4 model observation comparisons boms trained with the nelson et al 2013 dataset was used to produce a timeseries of posterior distributions of equilibrium ripple wavelengths observed during the three field experiments fig 10 is a plot of the most probable value of the posterior distributions provided by boms plotted versus the observed value for each of the field sites perfect predictions would be plotted directly on the 1 1 line boms performance metrics for each site are in table 3 the rmse and bias are fairly consistent between the different site locations and range from 5 to 11 cm and 4 and 5 cm respectively in general boms was more likely to over predict especially in the low mid wavelengths between 0 3 m and 0 6 m timeseries of the most probable ripple wavelengths from boms at the equilibrium times figs 7 9 and all the observed ripple wavelengths are plotted in fig 11 despite the apparent bias in the scatter plots boms agrees well with the observed wavelengths at the equilibrium times during trex13 the predictions made before and after the high energy event which occurred between around 2013 05 03 and 2013 05 08 agreed extremely well with the observed wavelengths during the high energy event at the 7 5 m location the wave forcing is above the critical threshold and the ripples are washed out to a planar bed during the high energy event at the 20 m location the ripples during the first half of trex13 were generated by a previous wave condition i e they are relict ripples however as the storm passes over the site on 2013 05 05 the ripples are formed by the extreme forcing and boms only slightly over predicts the ripple wavelengths which then persist as relict ripples for the remainder of the deployment the wqs14 site is characterized by frequent ripple formation and evolution during this time period boms agreed well with the observed ripple wavelengths especially when greater than 0 5 m but tended to slightly over predict in the low mid wavelengths 0 2 0 5 m however overall boms captures the frequent changes in the ripple wavelengths throughout the experiment examination of the asis15 predictions reveals recurring themes seen in the other field sites boms tended to over predict the low mid wavelengths while still capturing the trends of ripple wavelength change during a high energy event in early october hurricane joaquin the over predictions turned to slight under predictions as the wavelengths increased to above 1 0 m as mentioned in section 2 1 boms also outputs an estimate of the ripple height along with the ripple wavelength and flow statistics however it is not plotted here as there were no observations of ripple height measured for which to compare unlike previous deterministic models that use a least squares fit to derive an equation boms provides a distribution of predicted values the mean predicted values and standard deviation determined by the mcmc trace in the bayesian regression were used to construct a density histogram for each sample the density plots can be converted into probability plots by extracting the bin information and normalizing each density value by the number of estimates boms made per sample the probability histograms from a randomly sampled time during each of the field experiments are plotted in fig 12 for visualization purposes the most probable ripple wavelength versus the mean wavelength is denoted with a blue dashed line in each plot ideally the distributions would be narrow indicating a small standard deviation with mean values close to the observed values denoted by a black solid line as seen in fig 12 for these randomly sampled times the observed value typically falls within one standard deviation or less of the most probable predicted value the inherent advantage of using machine learning system like boms is ability to have not only a prediction of ripple wavelength but a probability distribution that also provides the model uncertainty unlike the previous state of the art empirical deterministic predictions 4 discussion 4 1 comparisons to other models it is useful to know the utility of boms and how it compares to other models commonly used to predict equilibrium wavelength boms was compared to the pre existing empirical equilibrium ripple wavelength predictors included in the base model layer the traykovski et al 1999 predictor and the nelson et al 2013 predictor table 4 using the pre processed nelson et al 2013 training dataset described in section 2 2 a ten fold cross validation was applied to each predictor a normalized rmse eq 8 was used to compare models outside of this study where y is the mean of the observations 8 nrmse rmse y boms with a nrmse of 0 19 has less residual variance than the traykovski et al 1999 and nelson et al 2013 predictors with nrmse values of 0 47 and 0 51 respectively goldstein et al 2013 and yan et al 2008 are the two other published machine learning models to predict ripple geometry in the literature however we cannot directly compare to these two models due to the different outputs the gp predictor of goldstein et al 2013 reportedly had an nrmse of 0 74 also normalized with the mean of the observations while most of the data in the nelson et al 2013 dataset is utilized in goldstein et al 2013 the data did not undergo the same pre processing techniques as were performed here most notably they assumed all field measurements of ripples were at or near equilibrium this assumption has the potential to produce higher errors in the model output resulting in a higher nrmse yan et al 2008 did not report on the nrmse of their ann and instead used the statistical parameters of scatter index correlation coefficient and the mean geometric deviation therefore the ann also cannot be directly compared to the boms results however boms overall outperformed the other equilibrium ripple predictors and also includes the model uncertainty a quantity not output by the other models 4 2 model limitations and applicability machine learning model skill is inherently limited by the range of conditions that exist in a model s training dataset the filtering of the training data described in section 2 1 deliberately removes any observations that may not be wave generated orbital ripples in equilibrium with the instantaneous wave forcing due to the different physical processes and time length scales associated with other types of bedforms e g anorbital ripples relict non equilibrium ripples current combined wave current generated bedforms megaripples etc we also focused on sandy locations due to the difference in ripple formation in heterogeneous and or muddy sediments the filtering of the data affects the prediction of any ripples outside the range of conditions in the training data therefore boms will have better skill in sandy wave dominated areas such as the coastal nearshore than it will in muddy current dominated areas such as a tidal inlet the advantage of boms is that it will still output a prediction for any given input outside the range of its trained conditions however the model will recognize that it was not trained with any similar observations and will provide a wide distribution indicating a large prediction uncertainty the user can then take into account the uncertainty when analyzing model skill we can examine the prediction range and limitations of the model given by the range of conditions included in the training data one hypothesis explaining the differences in model skill between the field sites was that the range of tested field data conditions may not have been completely included within the range of the training data histogram plots of the variable ranges fig 13 from the training blue shaded regions and testing colored shaded regions data show the comparison of the conditions for each dataset the variable ranges in the field datasets predominately fell within those of the training dataset the most profound deviation is the median grain size distribution plot fig 13c each field dataset is composed of only one constant median grain size for a total of 3 grain sizes in the testing datasets the trex13 and asis15 median grain size values are well represented in the training data however the number of samples in the training dataset that have a median grain size value equal to that of the wqs14 dataset is several orders of magnitude smaller this difference may or may not account for the tendency of boms to over predict when tested on the wqs14 data but it is a factor that should be considered when using boms as a prediction tool another difference between the training and testing datasets is that the training dataset includes both laboratory and field observations however research has shown that ripples behave differently in the laboratory compared to the field o donoghue and clubb 2001 calantoni et al 2013 and the relations between the variables observed in the laboratory experiments may not adequately represent the relations between the variables as they occur in nature i e field experiments future work could include compiling additional field data and separating the data into laboratory and field training datasets for laboratory and field predictions respectively not every possible base model and predictor variable combination was tested during the initial construction of boms however the system was built to be an evolving model that can be continually changed and re optimized further experimentation of combinations of different base models could yield an overall more skillful predictor the addition of more training data particularly those with observations that exceed the ranges currently in the training dataset would likely make boms more robust as the model system is exposed to new environmental conditions and the relations between those conditions boms is a versatile model system that is not restricted to predicting only equilibrium ripple wavelength the framework is conducive to be applied to predict many ripple and sediment transport parameters anorbital ripple wavelength suspended sediment concentration boundary layer thickness ripple orientation megaripples etc given a significant set of observations and empirical prediction formulations here we chose an important parameter needed for time dependent wbbl models and for which there existed many observations for the prediction of non equilibrium ripples the duration of the forcing the equilibrium ripples the seafloor is being driven to and the time at which the ripples stop changing are all extremely important parameters that need to be accounted for in a non equilibrium ripple model boms was developed to produce distributions of wave generated equilibrium ripple wavelengths needed to drive a probabilistic time dependent ripple model however the boms framework can be trained with any number of features and base models future iterations of boms will include predictions of current and combined wave current generated ripples and suspended sediment profiles boms is presently run using a graphical user interface gui that allows users to make predictions as well as add their own training datasets the probabilistic output allows for the analysis of prediction uncertainties this feature is especially useful when applied as a forecasting tool there has been an increasing demand for probabilistic predictions of hydrodynamic and sediment transport conditions boms is a step in the direction of providing distributions of seafloor sediment parameters for use in ensemble modeling systems 5 conclusions the bayesian optimal model system boms predicts probabilistic distributions of wave generated equilibrium sand ripple wavelengths and estimations of a deterministic ripple height using machine learning techniques in combination with pre existing deterministic empirical equilibrium ripple predictors the model system was trained and validated using the 50 year ripple dataset compiled by nelson et al 2013 which contains both field and laboratory observations model testing was performed with three separate field datasets not used in model training each from a different location and with unique environmental conditions boms has the ability to model uncertainties as well as the potential to become more robust with the addition of more training data overall boms provides more accurate predictions of ripple wavelength during both cross validation and testing compared to the performance of each independent base model and other common equilibrium ripple predictors the structure of boms built with the machine learning technique of stacked generalization was shown to be a viable method for predicting wave generated equilibrium ripple wavelength practical applications of boms include probabilistic ripple geometry forecasting and the coupling with time dependent ripple and sediment transport models credit authorship contribution statement r e phillip designed the research performed the research wrote all model code analyzed the data wrote the paper a m penko designed the research analyzed the data wrote the paper provided supervision project administration funding acquisition m l palmsten research conception edited the paper c b duval provided field datasets analyzed the data edited the paper declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the acquisition of the wqs14 and asis15 field data was supported by the u s national park service united states award p14ac00380 and the u s geological survey award g14ac00088 data collected by the asis15 experiment are available for download on the zenodo repository at https doi org 10 5281 zenodo 4519547 data collected by the wqs14 experiment are available via a usgs data release https doi org 10 5066 f7mw2f60 trex13 was jointly funded by the office of naval research onr the u s naval research laboratory nrl and the strategic environmental research and development program serdp munitions response program mr 2320 data are available upon request of the author a m p and r e p were supported by u s naval research laboratory base funding c b d was jointly funded by a national research council postdoctoral fellowship and u s naval research laboratory base funding m l p was funded by u s naval research laboratory base funding at the time of research conception and by the u s geological survey thereafter the authors would like to thank art trembanis at the university of delaware for sharing the wqs14 and asis15 data any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government 
25521,a multivariate polynomial regression modeling mpr framework is developed to estimate total dissolved solids tds in stormwater runoffs from rangelands in the colorado river basin in the southwestern united states an accurate tds estimation model is needed to simulate terrestrial and aquatic salt transport processes on rangelands identify critical source areas and manage these sources effectively however modeling stormwater tds runoff on rangeland sodic soils is challenging due to its complex correlation with variables in many aspects such as topography climate soil and vegetation we propose a two stage mpr framework based on field data collected from multiple rainfall simulator experiments 1 variable selection with factor analysis and 2 tds modeling via mpr considering the nonlinear relationships between variables tabu search ts is used to optimize the tds model in mpr the proposed framework achieved a high prediction accuracy of 74 7 in estimating the tds runoff transport keywords total dissolved solids machine learning artificial intelligence polynomial regression factor analysis colorado river basin data availability data will be made available on request 1 introduction the colorado river basin covers over 629 100 km2 across seven states from wyoming to colorado utah arizona new mexico nevada and california u s geological survey 2018 the colorado river cr supplies water to more than 30 million residents in the southwest irrigates approximately 16 200 km2 of agricultural lands and generates hydroelectric power from multiple dams as water demand grows sharply with population growth and increasing economic activities in the southwestern united states the cr faces many threats such as reduction in flow and deterioration of water quality consequently the cr has been designated as america s most endangered river colorado river basin salinity control forum 2017 since the salinity in the cr originates mainly from naturally existing geologic salts located in the upper colorado river basin ucrb as tributaries discharge saline water to the main reach along the river water users in the lower basin experience elevated salinity miller 1986 timothy 1988 hayes 1995 tuttle and grauch 2009 the level of salinity in the cr highly depends on several factors streamflow reservoir storage and climatic conditions u s bureau of reclamation 2017 natural climatic variations in rainfall wind temperature and snowmelt runoff continuously cause changes in flow and salinity in the river from year to year weltz et al 2014 u s bureau of reclamation 2017 over the past decades vegetation cover in the ucrb has reduced due to high temperature and low rainfall u s natural resources conservation service 2006 sankey et al 2015 under such conditions a flash storm can lead to high levels of surface runoff salts are transported in runoff or through weathering processes such as wind or water erosion ponce et al 1975 laronne and shen 1982 cadaret et al 2016b according to ator et al 2022 the tds concentration in the ucrb was expected to exceed the standards for both drinking water 500 mg l and agricultural use 1200 mg l salt transport in rangelands is a natural process resulting from interactions among soil vegetation topographic position land use and management and climate weltz et al 2014 therefore understanding salt dynamics under various biophysical conditions and salinity management practices is essential for controlling salt transport in the landscape deemer et al 2020 understanding salt mobility is critical to forecasting and minimizing salinity transport in the ucrb process based models specially designed for simulating soil erosion water quality are valuable tools to evaluate the efficiency of management practices on salt transport to surface waters these models can represent the dynamic interaction of management practices and salt transport processes thus predicting the effect of management practices on the basin s net salt balances a sophisticated and reliable prediction model is needed to estimate salt transport across the landscapes on rangelands salts in the terrestrial uplands of the ucrb are carried by surface runoff in response to storm events snowmelt or irrigation water return flow the excess runoff can cause soil erosion and sediment yield and increase the mobility of salt ions by dissolving soil chemical materials in the water the soil sediment concentration or load is influenced by the rate and magnitude of erosion soil properties e g texture infiltration rate and organic matter land slope and vegetation cover olson 2018 recent studies found that the tds concentration in rangeland runoff is influenced by several soil properties and topographic characteristics weltz et al 2014 this study s main objective is to develop a general framework for parameterizing hydro environmental processes on rangelands using a multivariate polynomial regression technique with accelerated optimization specifically a parameter estimation model for tds in the stormwater runoff on rangelands was developed based on experimental data collected at multiple locations in the ucrb cadaret et al 2016a 2016b rainfall simulator experiments were conducted at multiple locations in the ucrb the experiments were designed to quantify soil erosion and solute transport processes on rangelands in a saline environment cadaret et al 2016a 2016b the experimental data were used to develop a parameter estimation equation for tds using the multivariate polynomial regression mpr technique with flexible modeling capability for multiple variables kim et al 2017 unlike existing graphical machine learning algorithms e g a neural network with a random structure the mpr can clearly explain the impact of parent variables i e predictors on dependent variables i e response variables because the model incorporates coefficients of all potential variables based on a multivariate normal distribution cui et al 2021 showed that the gaussian process gp enables the statistical identification of optimal parameters to predict groundwater salinity in australia in this study the tabu search method is incorporated as the optimization technique into the mpr modeling framework to overcome the computational demands of the mpr when solving high degree of freedom problems multiple studies have addressed and validated the significance of simulation models in rangelands gross et al 2006 ibáñez et al 2014 lakshmi and sudheer 2021 the landscape salt transport model will be instrumental in assessing salt transport processes from the landscape to the river if incorporated into a watershed model the paper is organized as follows section 2 introduces current regression modeling techniques while section 3 proposes a two stage tds modeling framework for mpr section 4 demonstrates the proposed framework s performance regarding model accuracy and computational demands based on field data collected in the ucrb section 5 concludes the paper and suggests future work 2 multivariate polynomial regression as mentioned in section 1 tds is widely used as a parameter to measure water quality rusydi 2018 traditionally the linear tds model which is a function of electrical conductivity ec has been widely used allison and richards 1954 1 tds mg l k ec μs cm in eq 1 k is a coefficient addressing the relationship between ec and tds theoretically tds is defined as all inorganic and organic substances in water meaning that tds includes ions ionic compounds and organic ions e g pollutants herbicides hydrocarbons and organic matter according to taylor et al 2018 the value of k varies in the range 0 55 0 85 so it can be calibrated based on collected data for better estimation of tds unlike the linear tds model there are several studies to estimate tds via multivariate linear regression mlr which consists of multiple predictors and a response variable montgomery and runger 2013 equation 2 shows the basic structure of the mlr model 2 y β 0 β 1 x 1 β 2 x 2 β m x m ε where y is a response variable and the variable x i is the ith predictor the parameter set β β 0 β 1 β 2 β m t represents the impact of predictors i e x 1 x 1 x 2 x m t on dependent variable y note that the predictors are independent of each other and error ε follows a normal distribution with a mean of 0 and a standard deviation of σ 2 i e ε n o r m a l 0 σ 2 to minimize estimation error parameters or coefficients are estimated using least squares meng et al 2013 3 β ˆ x t x 1 x y where y y 1 y 2 y n t and n is the number of samples or observations due to its flexibility and simplicity in application regression models are widely used in water quality modeling and land management studies khan et al 2020 wong et al 2021 in particular aduojo et al 2020 considered parameters of the frequency domain electromagnetic em data to estimate the tds of groundwater around waste dump sites equation 4 represents the example model consisting of three em parameters aduojo et al 2020 4 t d s m g l 60 42 9 58 h d 40 3 53 v d 20 12 70 v d 40 the three em parameters are vd20 vertical dipole coil orientation data measured at 20 m interval vd40 vertical dipole coil orientation data measured at 40 m interval and hd40 horizontal dipole coil orientation data measured at 40 m interval the primary limitation of the mlr model is that it can only explain a linear relationship between predictors and a response variable thus if there is a combined or coupling effect of multiple independent predictors on predicting the response variable y an interaction term between these variables has to be incorporated into the regression model as follows 5 y β 0 β 1 x 1 β 2 x 2 β 3 x 1 x 2 ε in eq 5 x 1 x 2 is the interaction term with two predictors this can be modified as eq 6 which is the tds model estimated from red green blue rgb images of surface water in coastal bangladesh ferdous et al 2019 6 t d s m g l 3311 4364 41 3237 r 0 0779 g r 0 00006353 r g b the major challenge of the mlr model is its complexity if there are more than two independent variables the number of interaction terms exponentially increases equation 7 refers to a case with three variables 7 y β 0 β 1 x 1 β 2 x 2 β 3 x 3 β 4 x 1 x 2 β 5 x 1 x 3 β 6 x 2 x 3 β 6 x 1 x 2 x 3 ε in brief the number of interaction terms increases as follows 8 y k 1 m m k m k this implies that if there is a large scale data set the multivariate regression model with interaction terms cannot be practically used to define the number of predictors for example if there are 20 predictors the number of interaction terms is 1 048 575 moreover eqs 5 and 7 still cannot efficiently capture the nonlinear relationship particularly when a response variable is estimated via a specific predictor s polynomial function ali et al 2012 found that the tds concentration does not always follow a linear relationship with ec for example in some highly concentrated solutions e g wastewater ionic interaction can alter the linear relationship between concentration and conductivity this implies that the mlr model should be enhanced to explain the nonlinear relationship between predictors and a response variable to overcome the computational demand issue of the mlr kim et al 2017 devised the multivariate polynomial regression mpr 9 y f 1 x 1 f 2 x 2 f m x m ε where f j x j β j 0 β j 1 x j β j 2 x j 2 β j l x j l similar to the multivariate regression model ε follows a normal distribution with a mean of 0 and a standard deviation of j 1 m σ j 2 according to the maximum likelihood approach meng et al 2013 the estimated response can be represented as follows 10 y ˆ n o r m a l j 0 m x j β ˆ j j 1 m σ ˆ j 2 where x j x j 0 x j 1 x j l β j β ˆ j 0 β ˆ j 1 β ˆ j l t a n d σ ˆ j 2 y x j β ˆ j 2 m the main advantage of the mpr model is that it considers a nonlinear relationship between predictors and a response variable lima et al 2015 showed the importance of nonlinear modeling in environmental sciences via a performance comparison between nine different models in addition once the parameter set β j is estimated a user can easily apply the model to estimate a value of y under given x j due to the above reasons kim et al 2017 proposed a nonlinear energy consumption model with the mpr model under the given machining parameters i e spindle speed depth of cut and feed rate the model accurately estimated the energy consumption of an individual cnc machine at a manufacturing facility although other ai approaches such as multi layer neural network mnn or deep learning dl model can also be utilized for the estimation modeling of tds banadkooki et al 2020 the graphical machine learning algorithms i e mnn or dl are black box approaches so they cannot clearly explain the impact of predictors on response variables morala et al 2021 more specifically mnn or dl mainly focuses on selecting the best model or model structure with the highest prediction accuracy under the given data thus their modeling outcome i e a model or a network varies according to the dataset hence identifying relationships between predictors and response variables is inappropriate the major challenge of building an mpr model is associated with the structural learning of the model to be more specific the polynomial function f j x j has the l th power of x j so the total number of possible terms in the mpr model becomes l 1 multiplied by m suppose that there are four predictors and one response variable with 1000 samples if each predictor has the third power polynomial function the number of required parameters will be 15 in addition to 5 constants this implies that 256 combinations the 4th power of 4 of possible polynomial functions have to be checked for the four predictors if there are 20 predictors with a 3rd power polynomial function the number of parameter combinations becomes 160 000 i e the 4th power of 20 this study proposes a metaheuristic optimization algorithm that is devised to efficiently find reliable solutions to complex combinatorial problems taha 2017 which is computationally efficient via incorporating an mpr model to the optimization algorithm 3 two stage tds modeling framework with multivariate polynomial regression the proposed framework includes two major stages 1 variable selection with factor analysis and 2 structural learning of mpr with a metaheuristic optimization algorithm in general a large scale data set includes multiple variables this study proposes a framework for developing a better predictive model to forecast tds concentration in water tds is the total of inorganic salt ions e g calcium magnesium potassium sodium bicarbonates chloride and sulfate and dissolved organic matter and can be related to electrical conductivity ec and ph values the tds concentration is the sum of anions and cations including cl so4 2 no3 ca2 mg2 na k and nh4 in water site characteristics including topography e g slope and vegetation cover and soil properties e g sediment concentration texture and ec influence the amount of tds therefore the constituents associated with water topological and soil properties should be considered potential predictors to build a general tds model it is less than practical to consider all possible predictors in a regression model because some of them are correlated to other predictors to cause a multi collinearity problem in the first stage of the study only relevant predictors were chosen via a factor analysis with a variable integration technique to prevent this issue the second stage involves the development of an mpr model using the tabu search algorithm based on the selected predictors 3 1 variable integration factor analysis fa is a popular method to reduce variables for regression modeling rencher 2003 this method eliminates redundant observed variables by identifying a relationship between observed and unobserved variables i e factors for example suppose that there are two variables i e x 1 and x 2 and both variables are linearly correlated with an unobserved factor f 1 following standard normal distribution i e n 0 1 then x 1 and x 2 can be rewritten as follows 11 x 1 β 10 β 11 f 1 ε 1 12 x 2 β 20 β 21 f 1 ε 2 13 e x 1 e β 10 β 11 f 1 ε 1 β 10 14 e x 2 e β 20 β 21 f 1 ε 2 β 20 15 v a r x 1 β 11 2 v a r f 1 v a r ε 1 β 11 2 σ 1 2 16 v a r x 2 β 21 2 v a r f 2 v a r ε 2 β 21 2 σ 2 2 in eqs 11 and 12 β 11 and β 21 are known as the communality of both variables and can be explained by the factor f 1 the σ 1 2 and σ 2 2 are variances only associated with x 1 and x 2 this implies that f 1 can be used to represent two variables when σ 1 2 and σ 2 2 are relatively smaller than the values of communality of both variables tryfos 1998 in this study fa with varimax rotation costello and osborne 2005 is adapted to reduce the number of predictors and prevent the multi collinearity problem for mpr modeling in particular if the variable x i associated with factor f j has an absolute loading greater than or equal to 0 6 i e β i j 0 6 it is considered a significant variable that explains the factor f j once the factor f j has a list of relevant variables its factor score is computed as follows 17 f j 1 m j i 1 m j x i β 0 j ε β i j 1 m j i 1 m j x i β 0 j β i j note that m j represents the number of variables selected to explain the factor f j also each variable can describe only one factor so each one will be independent of the others during the variable selection process there exists a loss of information with f j as follows 18 f j 1 m i 1 m x i β i 0 β i j 1 m j i 1 m j x i β i 0 β i j where m is the number of variables 3 2 multivariate polynomial regression with tabu search once independent factors are selected in the first stage mpr modeling is conducted via the tabu search glover and laguna 1998 fig 3 shows that the tabu search is a metaheuristic algorithm devised to achieve a global optimum solution using a tabu list the major challenge in modeling is to efficiently and effectively determine the structure of an mpr with minimum estimation error equation 19 represents the objective function to find the optimum mpr model with the minimum mean error of k fold cross validation cv which is a well known validation technique in the field of regression modeling meng et al 2013 the k fold cv evenly divides the data set into k sub datasets and k 1 subsets are used as a training set while one subset is used as a validation set rogers and girolami 2016 equation 20 has the mpr model with parameters given by the method of least squares see eqs 3 and 7 19 argmin l l c v l k 1 k i 1 n y k i y ˆ k i l 2 k n 20 y ˆ k i l j 1 m f k i j x k i j l j l j 0 j j where l j is an integer decision variable representing the order of a polynomial function of f k i j used in ar the i is an index of a data point j is an index of a factor and k is an index of a fold as l j increases the number of curves in a polynomial function increases so that the mpr with higher l j tends to have a higher r 2 value figs 1 and 2 show this trend via the tds model with sediment concentration sedconc developed from the rainfall simulator experiment dataset sample size 995 in fig 1 the black dotted line represents tds estimates using a linear regression lr model the gray dots are the original observed data showing the relationship between tds and sediment concentration the lr model was proposed by nouwakpo et al 2018 to estimate tds based on sediment concentration x as presented by eq 21 21 y 2 36x 0 99 equation 21 only explains 55 of the tds data set variability shown in fig 1 r 2 0 5514 similar to fig 1 the black dotted line in fig 2 represents the estimated tds by mpr using the fifth power polynomial function notably to minimize its estimation error the mpr model has multiple curves the major challenge is to find the most appropriate polynomial functions when the mpr model includes numerous factors suppose that twenty factors that are influential to tds exist and that each factor potentially has the 5th power polynomial function in such case approximately 10 trillion i e 5 20 models must be evaluated to find the best fitting model to reduce such computational demands the tabu search algorithm glover and laguna 1998 is adopted in this study as presented in fig 3 1 initialize a current set of l 2 create a candidate list of neighbors to current solution 3 find the best solution set lʹ 4 if lʹ is in the tabu list delete lʹ from the candidate list otherwise let l lʹ and update the tabu list 5 if stopping condition is satisfied stop the search otherwise go to line 2 the tabu search algorithm terminates its search process when the given stopping condition is satisfied although it cannot guarantee the optimality of the final solution it allows users to find a local optimum solution efficiently to be more specific the algorithm recalls previous search moves and disregards them from new candidate solutions to reduce the total search time to find the best answer taha 2017 4 case study 4 1 description of the rainfall simulation experiment rainfall simulator experiments were conducted in the ucrb cadaret et al 2016a 2016b runoff volume sediment yield and the concentration of salt ions were recorded under various rainfall intensities at six desert rangeland sites in the ucrb table 1 presents the soil characteristics of each experimental site the data was collected between 2014 and 2016 a rainfall simulator was installed at each site on a hillslope experimental plot of an area measuring 6 m 2 m at five of the six sites the rainfall simulator was used to generate rainfall events of four intensities of 2 year 50 8 mm h 1 10 year 88 9 mm h 1 25 year 114 3 mm h 1 and 50 year storm return intervals 139 7 mm h 1 three replications of the same rainfall intensity were applied on three plots to test the effect of vegetation cover on runoff the 25 year return period storm intensity was used in four replicated three vegetation cover types at the sixth site runoff sediment samples were collected in a 1 l bottle every 3 min in each rainfall simulation the runoff water was conveyed in a supercritical flume where a flowmeter teledyne 4230 isco inc lincoln neb was installed to record the runoff stage the instantaneous runoff discharge q and sediment discharge rate qs at the timed interval were obtained by weighing the amount of sediment in the sample using values of q and qs the cumulative runoff sr and total soil loss sl were determined the runoff water quality samples were collected in a 50 ml vial every 1 5 min then the concentration of various salt ions i e cl so4 2 no3 ca2 mg2 na k and nh4 and ec and ph were analyzed cumulative tds was estimated as the sum of all ion concentrations and dissolved organic matter to determine specific soil physio chemical properties surface soil samples were collected before and after rainfall simulation at each site surface soil samples were collected at two microsites including vegetation and bare soil interspace sites the average soil property values obtained from vegetation and interspace sites were used for the regression analysis soluble phase cations and anions ec ph and cation exchange capacity cec for each soil surface layer were quantified by the immiscible displacement method mubarak and olsen 1976 the sodium adsorption ratio sar of the soil surface layer after rainfall simulation was calculated using eq 22 22 sar na sqrt 0 5 ca 2 mg 2 the concentrations of na ca2 and mg2 are in mmol l 1 detailed descriptions of sites rainfall simulation experimental setup and procedures of water and soil chemical analyses are addressed in cadaret et al 2016b 4 2 selection of variables initially a multivariate linear regression model involving twenty predictors was considered to develop a tds prediction model table 2 shows the result of the regression modeling the multivariate linear regression model yielded a significant correlation with r 2 0 76 explaining 76 variability of the original data however variance inflation factor vif values of several variables such as silt and clay are higher than 10 implying that the regression model involves a significant multi collinearity issue between variables note that vif measures the impact of collinearity between predictors in a regression model see eq 23 23 vif i 1 1 r i 2 in other words each variable s regression coefficient coef alone cannot represent the significance of the variable influencing the tds concentration therefore factor analysis fa was used to select appropriate variables without causing multi collinearity issues between predictors table 3 presents the result of fa with minitab 16 0 statistic software a predictor has a high correlation with a selected factor when its absolute loading on a selected factor is greater than 0 6 for example factor 1 f1 includes sand clay silt rock bare ec soil and sedconc while factor 2 f2 includes cec soil sar soil ph soil and slope also any unselected predictors are considered independent factors factor 3 f3 is canopy factor 4 f4 is cryptogam factor 5 f5 is sar water factor 6 f6 is basal factor 7 f7 is ec water factor 8 f8 is runoff factor 9 f9 is ph water factor 10 f10 is ammonium and factor 11 f11 is litter note that the litter includes leaves and other natural materials that are left on the soil surface as a result 11 selected factors can explain 87 2 variability of the original data with 20 predictors factors involving multiple predictors i e factor 1 and factor 2 are generated by eq 17 presented in section 3 moreover in table 4 there is no strong correlation i e correlation coefficient 0 7 between the selected factors thus a tds model can be developed by relating the 11 selected factors 4 3 multivariate polynomial regression modeling in this section the mpr model is developed via the proposed approach and its performance is compared with the existing tds modeling approaches addressed in section 2 before we conduct the mpr modeling the mlr model is developed to understand the impact of the 11 selected factors on tds discussed in section 4 2 table 5 presents the multivariate linear regression mlr model involving the 11 selected factors all vif values are less than ten so that the model does not have multi collinearity issues thus the absolute coefficients of the chosen factors can explain their impact on the tds regarding the coefficient values in table 5 f7 ec water is the most critical factor associated with tds similarly f9 ph water f1 sand clay silt rock bare ec soil and sediment concentration and f6 basal are two other important factors in estimating the value of tds although f2 and f10 are not statistically significant at α 0 05 we decided to keep both factors regarding the model accuracy accounting for about 70 96 of the tds data set r2 0 7096 fig 4 compares the observed tds and estimated tds by the mlr model the tds values of 995 samples from the rainfall simulator experiment dataset see figs 1 and 2 are considered the comparison result describes that the estimation accuracy can be improved by considering multiple factors regarding the r 2 of the lr model only involving sediment concentration being 55 14 the impacts of other variables i e sand clay silt rock bare ec soil cec soil sar soil ph soil slope canopy cryptogam sar water basal ec water runoff ph water ammonium and litter on tds exist similarly the traditional linear tds model which is a function of electrical conductivity allison and richards 1954 is not sufficient to estimate tds however as mentioned in section 2 mlr cannot capture nonlinear relationships between predictors and response variables so the proposed mpr should be used to better estimate tds table 6 describes coefficients of the developed mpr model similar to the mlr model involving the 11 factors the mpr model prevents the multi collinearity issue because all vif values are less than 10 of these f7 ec water has the highest loading implying that it is the most significant factor associated with tds regarding loadings of the other ten factors f9 ph water and f1 sand clay silt rock bare ec soil and sediment concentration are two other important factors to estimate tds loads in stormwater runoff moreover unlike the result in table 5 f6 basal exhibits a robust nonlinear relationship with tds only f6 squared f62 instead of f6 is statistically significant at α 0 05 so there is a convex relationship between tds and f6 this implies that the traditional multivariate linear regression model is not appropriate for estimating a tds value although the mpr model has several insignificant factors i e f4 f6 f10 and f11 it can still account for about 74 74 of the tds data r2 0 7474 this model accuracy i e r2 is higher than the multivariate linear regression model shown in table 5 fig 5 compares the observed tds and estimated tds by the mpr model the mpr model shown in table 6 is developed via the tabu search algorithm as stated in section 3 2 finding the optimal structure of mpr without using the tabu search algorithm should consider 520 cases when all twenty factors potentially have the 5th power polynomial function i e the base of 520 the proposed tabu search algorithm enables the optimal mpr model to be found that is identical to the mrp model illustrated in table 6 the performance of the proposed algorithm is evaluated in terms of computational complexity or modeling complexity and prediction accuracy in addition its performance is compared with existing tds modeling approaches such as linear tds see eq 1 lr see eq 21 and multi layer neural network mnn with a learning rate of 0 01 the evaluation is conducted under the computing environment of intel core i5 8250u cpu 1 60 ghz table 7 summarizes the performance comparison results in table 7 the computational complexity or modeling complexity is measured by the modeling time since linear tds and lr are existing simple equations see eqs 1 and 21 they do not require modeling time similarly the parameters of mlr are simply computed by eq 3 so that its computational time is only 0 4 s however ai algorithms such as mpr with tabu search and mnn require structural learning processes to identify their optimal models so that they have higher modeling time than the former three approaches among the two ai algorithms the proposed approach performs better than mnn this is because the tabu search algorithm is a heuristic optimization algorithm only considering promising potential candidates i e models instead of reviewing all possible cases in the structure learning process of mrp as regards the prediction accuracy in terms of r 2 the proposed mpr model has a 0 81 higher value than that of mnn and 5 33 higher than mlr in fact mrp and mnn have better prediction accuracy than mlr it is difficult to argue that mrp is better than mnn given that the performance of both approaches is dependent on their structural learning processes however unlike the mnn the proposed mrp enables the impact of eleven factors on a response variable i e tds to be shown this analytical capability is critical to engineers and scientists who must identify significant factors to develop an effective tds management plan 5 conclusions and future research this study introduces a new tds modeling framework based on mpr and tabu search for an accurate simulation of salt transport across the landscapes on rangelands the proposed two stage tds modeling framework includes 1 selecting variables with fa and 2 tds model development based on mpr and tabu search in the first step fa is used to eliminate redundant predictors by identifying correlations between the observed and unobserved predictors i e factors from the original data with 20 predictors fa selects eleven factors namely factor 1 sand clay silt rock bare ec soil and sedconc factor 2 cec soil sar soil ph soil and slope factor 3 canopy factor 4 cryptogam factor 5 sar water factor 6 basal factor 7 ec water factor 8 runoff factor 9 ph water factor 10 ammonium and factor 11 litter the 87 2 of variability involved in the original data with 20 predictors is captured by the selected factors so that those selected can represent the impact of the original predictors on tds also the proposed framework can prevent the multi collinearity problem between predictors which results in incorrect calculation of the statistical significance of each independent predictor the second step conducts tds model development via mpr with the tabu search algorithm unlike the traditional multivariate regression modeling approach mpr clearly explains the nonlinear impact of parent variables i e predictors on dependent variables i e response variables to develop the best fitting polynomial tds model the prediction accuracy of the developed mpr model measured by r 2 is approximately 74 74 in addition to overcome the computation demand of mpr association with its structural learning process the tabu search algorithm is used to efficiently find the optimal structure see table 6 of mpr by considering effective candidate solutions from all possible alternatives the results show that tabu search can quickly build an mpr model for tds with high accuracy although both mrp and mnn provide high prediction accuracy than that of existing models i e linear tds lr and mlr the proposed mrp has the analytical capability thus the proposed framework will be helpful to engineers and scientists who have to identify significant factors to develop an effective tds management plan the tds model has been incorporated into the agricultural policy environmental extender apex model for watershed scale assessment of salt transport in the crb bailey et al 2022 the apex model williams and izaurralde 2010 is a watershed scale hydrologic and water quality model that has emerged as a valuable tool for simulating hydrodynamic and pollutant transport processes and evaluating the effects of land management practices as tds was used in apex as the indicator of salinity level in the water goehring 2017 the proposed tds modeling framework eventually enhanced the model s prediction accuracy on salt balances and transport in the price river watershed in future research the mpr framework will be extended to consider the interaction effects of parent variables on a dependent variable this will relax one assumption i e independence between parent variables of the proposed mpr model so that the modeling accuracy and flexibility of mpr can be significantly enhanced declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this project was supported by the bureau of land management award l17ac00125 we would like to thank dr mark weltz at usda agricultural research service at reno nevada for providing field experiment data 
25521,a multivariate polynomial regression modeling mpr framework is developed to estimate total dissolved solids tds in stormwater runoffs from rangelands in the colorado river basin in the southwestern united states an accurate tds estimation model is needed to simulate terrestrial and aquatic salt transport processes on rangelands identify critical source areas and manage these sources effectively however modeling stormwater tds runoff on rangeland sodic soils is challenging due to its complex correlation with variables in many aspects such as topography climate soil and vegetation we propose a two stage mpr framework based on field data collected from multiple rainfall simulator experiments 1 variable selection with factor analysis and 2 tds modeling via mpr considering the nonlinear relationships between variables tabu search ts is used to optimize the tds model in mpr the proposed framework achieved a high prediction accuracy of 74 7 in estimating the tds runoff transport keywords total dissolved solids machine learning artificial intelligence polynomial regression factor analysis colorado river basin data availability data will be made available on request 1 introduction the colorado river basin covers over 629 100 km2 across seven states from wyoming to colorado utah arizona new mexico nevada and california u s geological survey 2018 the colorado river cr supplies water to more than 30 million residents in the southwest irrigates approximately 16 200 km2 of agricultural lands and generates hydroelectric power from multiple dams as water demand grows sharply with population growth and increasing economic activities in the southwestern united states the cr faces many threats such as reduction in flow and deterioration of water quality consequently the cr has been designated as america s most endangered river colorado river basin salinity control forum 2017 since the salinity in the cr originates mainly from naturally existing geologic salts located in the upper colorado river basin ucrb as tributaries discharge saline water to the main reach along the river water users in the lower basin experience elevated salinity miller 1986 timothy 1988 hayes 1995 tuttle and grauch 2009 the level of salinity in the cr highly depends on several factors streamflow reservoir storage and climatic conditions u s bureau of reclamation 2017 natural climatic variations in rainfall wind temperature and snowmelt runoff continuously cause changes in flow and salinity in the river from year to year weltz et al 2014 u s bureau of reclamation 2017 over the past decades vegetation cover in the ucrb has reduced due to high temperature and low rainfall u s natural resources conservation service 2006 sankey et al 2015 under such conditions a flash storm can lead to high levels of surface runoff salts are transported in runoff or through weathering processes such as wind or water erosion ponce et al 1975 laronne and shen 1982 cadaret et al 2016b according to ator et al 2022 the tds concentration in the ucrb was expected to exceed the standards for both drinking water 500 mg l and agricultural use 1200 mg l salt transport in rangelands is a natural process resulting from interactions among soil vegetation topographic position land use and management and climate weltz et al 2014 therefore understanding salt dynamics under various biophysical conditions and salinity management practices is essential for controlling salt transport in the landscape deemer et al 2020 understanding salt mobility is critical to forecasting and minimizing salinity transport in the ucrb process based models specially designed for simulating soil erosion water quality are valuable tools to evaluate the efficiency of management practices on salt transport to surface waters these models can represent the dynamic interaction of management practices and salt transport processes thus predicting the effect of management practices on the basin s net salt balances a sophisticated and reliable prediction model is needed to estimate salt transport across the landscapes on rangelands salts in the terrestrial uplands of the ucrb are carried by surface runoff in response to storm events snowmelt or irrigation water return flow the excess runoff can cause soil erosion and sediment yield and increase the mobility of salt ions by dissolving soil chemical materials in the water the soil sediment concentration or load is influenced by the rate and magnitude of erosion soil properties e g texture infiltration rate and organic matter land slope and vegetation cover olson 2018 recent studies found that the tds concentration in rangeland runoff is influenced by several soil properties and topographic characteristics weltz et al 2014 this study s main objective is to develop a general framework for parameterizing hydro environmental processes on rangelands using a multivariate polynomial regression technique with accelerated optimization specifically a parameter estimation model for tds in the stormwater runoff on rangelands was developed based on experimental data collected at multiple locations in the ucrb cadaret et al 2016a 2016b rainfall simulator experiments were conducted at multiple locations in the ucrb the experiments were designed to quantify soil erosion and solute transport processes on rangelands in a saline environment cadaret et al 2016a 2016b the experimental data were used to develop a parameter estimation equation for tds using the multivariate polynomial regression mpr technique with flexible modeling capability for multiple variables kim et al 2017 unlike existing graphical machine learning algorithms e g a neural network with a random structure the mpr can clearly explain the impact of parent variables i e predictors on dependent variables i e response variables because the model incorporates coefficients of all potential variables based on a multivariate normal distribution cui et al 2021 showed that the gaussian process gp enables the statistical identification of optimal parameters to predict groundwater salinity in australia in this study the tabu search method is incorporated as the optimization technique into the mpr modeling framework to overcome the computational demands of the mpr when solving high degree of freedom problems multiple studies have addressed and validated the significance of simulation models in rangelands gross et al 2006 ibáñez et al 2014 lakshmi and sudheer 2021 the landscape salt transport model will be instrumental in assessing salt transport processes from the landscape to the river if incorporated into a watershed model the paper is organized as follows section 2 introduces current regression modeling techniques while section 3 proposes a two stage tds modeling framework for mpr section 4 demonstrates the proposed framework s performance regarding model accuracy and computational demands based on field data collected in the ucrb section 5 concludes the paper and suggests future work 2 multivariate polynomial regression as mentioned in section 1 tds is widely used as a parameter to measure water quality rusydi 2018 traditionally the linear tds model which is a function of electrical conductivity ec has been widely used allison and richards 1954 1 tds mg l k ec μs cm in eq 1 k is a coefficient addressing the relationship between ec and tds theoretically tds is defined as all inorganic and organic substances in water meaning that tds includes ions ionic compounds and organic ions e g pollutants herbicides hydrocarbons and organic matter according to taylor et al 2018 the value of k varies in the range 0 55 0 85 so it can be calibrated based on collected data for better estimation of tds unlike the linear tds model there are several studies to estimate tds via multivariate linear regression mlr which consists of multiple predictors and a response variable montgomery and runger 2013 equation 2 shows the basic structure of the mlr model 2 y β 0 β 1 x 1 β 2 x 2 β m x m ε where y is a response variable and the variable x i is the ith predictor the parameter set β β 0 β 1 β 2 β m t represents the impact of predictors i e x 1 x 1 x 2 x m t on dependent variable y note that the predictors are independent of each other and error ε follows a normal distribution with a mean of 0 and a standard deviation of σ 2 i e ε n o r m a l 0 σ 2 to minimize estimation error parameters or coefficients are estimated using least squares meng et al 2013 3 β ˆ x t x 1 x y where y y 1 y 2 y n t and n is the number of samples or observations due to its flexibility and simplicity in application regression models are widely used in water quality modeling and land management studies khan et al 2020 wong et al 2021 in particular aduojo et al 2020 considered parameters of the frequency domain electromagnetic em data to estimate the tds of groundwater around waste dump sites equation 4 represents the example model consisting of three em parameters aduojo et al 2020 4 t d s m g l 60 42 9 58 h d 40 3 53 v d 20 12 70 v d 40 the three em parameters are vd20 vertical dipole coil orientation data measured at 20 m interval vd40 vertical dipole coil orientation data measured at 40 m interval and hd40 horizontal dipole coil orientation data measured at 40 m interval the primary limitation of the mlr model is that it can only explain a linear relationship between predictors and a response variable thus if there is a combined or coupling effect of multiple independent predictors on predicting the response variable y an interaction term between these variables has to be incorporated into the regression model as follows 5 y β 0 β 1 x 1 β 2 x 2 β 3 x 1 x 2 ε in eq 5 x 1 x 2 is the interaction term with two predictors this can be modified as eq 6 which is the tds model estimated from red green blue rgb images of surface water in coastal bangladesh ferdous et al 2019 6 t d s m g l 3311 4364 41 3237 r 0 0779 g r 0 00006353 r g b the major challenge of the mlr model is its complexity if there are more than two independent variables the number of interaction terms exponentially increases equation 7 refers to a case with three variables 7 y β 0 β 1 x 1 β 2 x 2 β 3 x 3 β 4 x 1 x 2 β 5 x 1 x 3 β 6 x 2 x 3 β 6 x 1 x 2 x 3 ε in brief the number of interaction terms increases as follows 8 y k 1 m m k m k this implies that if there is a large scale data set the multivariate regression model with interaction terms cannot be practically used to define the number of predictors for example if there are 20 predictors the number of interaction terms is 1 048 575 moreover eqs 5 and 7 still cannot efficiently capture the nonlinear relationship particularly when a response variable is estimated via a specific predictor s polynomial function ali et al 2012 found that the tds concentration does not always follow a linear relationship with ec for example in some highly concentrated solutions e g wastewater ionic interaction can alter the linear relationship between concentration and conductivity this implies that the mlr model should be enhanced to explain the nonlinear relationship between predictors and a response variable to overcome the computational demand issue of the mlr kim et al 2017 devised the multivariate polynomial regression mpr 9 y f 1 x 1 f 2 x 2 f m x m ε where f j x j β j 0 β j 1 x j β j 2 x j 2 β j l x j l similar to the multivariate regression model ε follows a normal distribution with a mean of 0 and a standard deviation of j 1 m σ j 2 according to the maximum likelihood approach meng et al 2013 the estimated response can be represented as follows 10 y ˆ n o r m a l j 0 m x j β ˆ j j 1 m σ ˆ j 2 where x j x j 0 x j 1 x j l β j β ˆ j 0 β ˆ j 1 β ˆ j l t a n d σ ˆ j 2 y x j β ˆ j 2 m the main advantage of the mpr model is that it considers a nonlinear relationship between predictors and a response variable lima et al 2015 showed the importance of nonlinear modeling in environmental sciences via a performance comparison between nine different models in addition once the parameter set β j is estimated a user can easily apply the model to estimate a value of y under given x j due to the above reasons kim et al 2017 proposed a nonlinear energy consumption model with the mpr model under the given machining parameters i e spindle speed depth of cut and feed rate the model accurately estimated the energy consumption of an individual cnc machine at a manufacturing facility although other ai approaches such as multi layer neural network mnn or deep learning dl model can also be utilized for the estimation modeling of tds banadkooki et al 2020 the graphical machine learning algorithms i e mnn or dl are black box approaches so they cannot clearly explain the impact of predictors on response variables morala et al 2021 more specifically mnn or dl mainly focuses on selecting the best model or model structure with the highest prediction accuracy under the given data thus their modeling outcome i e a model or a network varies according to the dataset hence identifying relationships between predictors and response variables is inappropriate the major challenge of building an mpr model is associated with the structural learning of the model to be more specific the polynomial function f j x j has the l th power of x j so the total number of possible terms in the mpr model becomes l 1 multiplied by m suppose that there are four predictors and one response variable with 1000 samples if each predictor has the third power polynomial function the number of required parameters will be 15 in addition to 5 constants this implies that 256 combinations the 4th power of 4 of possible polynomial functions have to be checked for the four predictors if there are 20 predictors with a 3rd power polynomial function the number of parameter combinations becomes 160 000 i e the 4th power of 20 this study proposes a metaheuristic optimization algorithm that is devised to efficiently find reliable solutions to complex combinatorial problems taha 2017 which is computationally efficient via incorporating an mpr model to the optimization algorithm 3 two stage tds modeling framework with multivariate polynomial regression the proposed framework includes two major stages 1 variable selection with factor analysis and 2 structural learning of mpr with a metaheuristic optimization algorithm in general a large scale data set includes multiple variables this study proposes a framework for developing a better predictive model to forecast tds concentration in water tds is the total of inorganic salt ions e g calcium magnesium potassium sodium bicarbonates chloride and sulfate and dissolved organic matter and can be related to electrical conductivity ec and ph values the tds concentration is the sum of anions and cations including cl so4 2 no3 ca2 mg2 na k and nh4 in water site characteristics including topography e g slope and vegetation cover and soil properties e g sediment concentration texture and ec influence the amount of tds therefore the constituents associated with water topological and soil properties should be considered potential predictors to build a general tds model it is less than practical to consider all possible predictors in a regression model because some of them are correlated to other predictors to cause a multi collinearity problem in the first stage of the study only relevant predictors were chosen via a factor analysis with a variable integration technique to prevent this issue the second stage involves the development of an mpr model using the tabu search algorithm based on the selected predictors 3 1 variable integration factor analysis fa is a popular method to reduce variables for regression modeling rencher 2003 this method eliminates redundant observed variables by identifying a relationship between observed and unobserved variables i e factors for example suppose that there are two variables i e x 1 and x 2 and both variables are linearly correlated with an unobserved factor f 1 following standard normal distribution i e n 0 1 then x 1 and x 2 can be rewritten as follows 11 x 1 β 10 β 11 f 1 ε 1 12 x 2 β 20 β 21 f 1 ε 2 13 e x 1 e β 10 β 11 f 1 ε 1 β 10 14 e x 2 e β 20 β 21 f 1 ε 2 β 20 15 v a r x 1 β 11 2 v a r f 1 v a r ε 1 β 11 2 σ 1 2 16 v a r x 2 β 21 2 v a r f 2 v a r ε 2 β 21 2 σ 2 2 in eqs 11 and 12 β 11 and β 21 are known as the communality of both variables and can be explained by the factor f 1 the σ 1 2 and σ 2 2 are variances only associated with x 1 and x 2 this implies that f 1 can be used to represent two variables when σ 1 2 and σ 2 2 are relatively smaller than the values of communality of both variables tryfos 1998 in this study fa with varimax rotation costello and osborne 2005 is adapted to reduce the number of predictors and prevent the multi collinearity problem for mpr modeling in particular if the variable x i associated with factor f j has an absolute loading greater than or equal to 0 6 i e β i j 0 6 it is considered a significant variable that explains the factor f j once the factor f j has a list of relevant variables its factor score is computed as follows 17 f j 1 m j i 1 m j x i β 0 j ε β i j 1 m j i 1 m j x i β 0 j β i j note that m j represents the number of variables selected to explain the factor f j also each variable can describe only one factor so each one will be independent of the others during the variable selection process there exists a loss of information with f j as follows 18 f j 1 m i 1 m x i β i 0 β i j 1 m j i 1 m j x i β i 0 β i j where m is the number of variables 3 2 multivariate polynomial regression with tabu search once independent factors are selected in the first stage mpr modeling is conducted via the tabu search glover and laguna 1998 fig 3 shows that the tabu search is a metaheuristic algorithm devised to achieve a global optimum solution using a tabu list the major challenge in modeling is to efficiently and effectively determine the structure of an mpr with minimum estimation error equation 19 represents the objective function to find the optimum mpr model with the minimum mean error of k fold cross validation cv which is a well known validation technique in the field of regression modeling meng et al 2013 the k fold cv evenly divides the data set into k sub datasets and k 1 subsets are used as a training set while one subset is used as a validation set rogers and girolami 2016 equation 20 has the mpr model with parameters given by the method of least squares see eqs 3 and 7 19 argmin l l c v l k 1 k i 1 n y k i y ˆ k i l 2 k n 20 y ˆ k i l j 1 m f k i j x k i j l j l j 0 j j where l j is an integer decision variable representing the order of a polynomial function of f k i j used in ar the i is an index of a data point j is an index of a factor and k is an index of a fold as l j increases the number of curves in a polynomial function increases so that the mpr with higher l j tends to have a higher r 2 value figs 1 and 2 show this trend via the tds model with sediment concentration sedconc developed from the rainfall simulator experiment dataset sample size 995 in fig 1 the black dotted line represents tds estimates using a linear regression lr model the gray dots are the original observed data showing the relationship between tds and sediment concentration the lr model was proposed by nouwakpo et al 2018 to estimate tds based on sediment concentration x as presented by eq 21 21 y 2 36x 0 99 equation 21 only explains 55 of the tds data set variability shown in fig 1 r 2 0 5514 similar to fig 1 the black dotted line in fig 2 represents the estimated tds by mpr using the fifth power polynomial function notably to minimize its estimation error the mpr model has multiple curves the major challenge is to find the most appropriate polynomial functions when the mpr model includes numerous factors suppose that twenty factors that are influential to tds exist and that each factor potentially has the 5th power polynomial function in such case approximately 10 trillion i e 5 20 models must be evaluated to find the best fitting model to reduce such computational demands the tabu search algorithm glover and laguna 1998 is adopted in this study as presented in fig 3 1 initialize a current set of l 2 create a candidate list of neighbors to current solution 3 find the best solution set lʹ 4 if lʹ is in the tabu list delete lʹ from the candidate list otherwise let l lʹ and update the tabu list 5 if stopping condition is satisfied stop the search otherwise go to line 2 the tabu search algorithm terminates its search process when the given stopping condition is satisfied although it cannot guarantee the optimality of the final solution it allows users to find a local optimum solution efficiently to be more specific the algorithm recalls previous search moves and disregards them from new candidate solutions to reduce the total search time to find the best answer taha 2017 4 case study 4 1 description of the rainfall simulation experiment rainfall simulator experiments were conducted in the ucrb cadaret et al 2016a 2016b runoff volume sediment yield and the concentration of salt ions were recorded under various rainfall intensities at six desert rangeland sites in the ucrb table 1 presents the soil characteristics of each experimental site the data was collected between 2014 and 2016 a rainfall simulator was installed at each site on a hillslope experimental plot of an area measuring 6 m 2 m at five of the six sites the rainfall simulator was used to generate rainfall events of four intensities of 2 year 50 8 mm h 1 10 year 88 9 mm h 1 25 year 114 3 mm h 1 and 50 year storm return intervals 139 7 mm h 1 three replications of the same rainfall intensity were applied on three plots to test the effect of vegetation cover on runoff the 25 year return period storm intensity was used in four replicated three vegetation cover types at the sixth site runoff sediment samples were collected in a 1 l bottle every 3 min in each rainfall simulation the runoff water was conveyed in a supercritical flume where a flowmeter teledyne 4230 isco inc lincoln neb was installed to record the runoff stage the instantaneous runoff discharge q and sediment discharge rate qs at the timed interval were obtained by weighing the amount of sediment in the sample using values of q and qs the cumulative runoff sr and total soil loss sl were determined the runoff water quality samples were collected in a 50 ml vial every 1 5 min then the concentration of various salt ions i e cl so4 2 no3 ca2 mg2 na k and nh4 and ec and ph were analyzed cumulative tds was estimated as the sum of all ion concentrations and dissolved organic matter to determine specific soil physio chemical properties surface soil samples were collected before and after rainfall simulation at each site surface soil samples were collected at two microsites including vegetation and bare soil interspace sites the average soil property values obtained from vegetation and interspace sites were used for the regression analysis soluble phase cations and anions ec ph and cation exchange capacity cec for each soil surface layer were quantified by the immiscible displacement method mubarak and olsen 1976 the sodium adsorption ratio sar of the soil surface layer after rainfall simulation was calculated using eq 22 22 sar na sqrt 0 5 ca 2 mg 2 the concentrations of na ca2 and mg2 are in mmol l 1 detailed descriptions of sites rainfall simulation experimental setup and procedures of water and soil chemical analyses are addressed in cadaret et al 2016b 4 2 selection of variables initially a multivariate linear regression model involving twenty predictors was considered to develop a tds prediction model table 2 shows the result of the regression modeling the multivariate linear regression model yielded a significant correlation with r 2 0 76 explaining 76 variability of the original data however variance inflation factor vif values of several variables such as silt and clay are higher than 10 implying that the regression model involves a significant multi collinearity issue between variables note that vif measures the impact of collinearity between predictors in a regression model see eq 23 23 vif i 1 1 r i 2 in other words each variable s regression coefficient coef alone cannot represent the significance of the variable influencing the tds concentration therefore factor analysis fa was used to select appropriate variables without causing multi collinearity issues between predictors table 3 presents the result of fa with minitab 16 0 statistic software a predictor has a high correlation with a selected factor when its absolute loading on a selected factor is greater than 0 6 for example factor 1 f1 includes sand clay silt rock bare ec soil and sedconc while factor 2 f2 includes cec soil sar soil ph soil and slope also any unselected predictors are considered independent factors factor 3 f3 is canopy factor 4 f4 is cryptogam factor 5 f5 is sar water factor 6 f6 is basal factor 7 f7 is ec water factor 8 f8 is runoff factor 9 f9 is ph water factor 10 f10 is ammonium and factor 11 f11 is litter note that the litter includes leaves and other natural materials that are left on the soil surface as a result 11 selected factors can explain 87 2 variability of the original data with 20 predictors factors involving multiple predictors i e factor 1 and factor 2 are generated by eq 17 presented in section 3 moreover in table 4 there is no strong correlation i e correlation coefficient 0 7 between the selected factors thus a tds model can be developed by relating the 11 selected factors 4 3 multivariate polynomial regression modeling in this section the mpr model is developed via the proposed approach and its performance is compared with the existing tds modeling approaches addressed in section 2 before we conduct the mpr modeling the mlr model is developed to understand the impact of the 11 selected factors on tds discussed in section 4 2 table 5 presents the multivariate linear regression mlr model involving the 11 selected factors all vif values are less than ten so that the model does not have multi collinearity issues thus the absolute coefficients of the chosen factors can explain their impact on the tds regarding the coefficient values in table 5 f7 ec water is the most critical factor associated with tds similarly f9 ph water f1 sand clay silt rock bare ec soil and sediment concentration and f6 basal are two other important factors in estimating the value of tds although f2 and f10 are not statistically significant at α 0 05 we decided to keep both factors regarding the model accuracy accounting for about 70 96 of the tds data set r2 0 7096 fig 4 compares the observed tds and estimated tds by the mlr model the tds values of 995 samples from the rainfall simulator experiment dataset see figs 1 and 2 are considered the comparison result describes that the estimation accuracy can be improved by considering multiple factors regarding the r 2 of the lr model only involving sediment concentration being 55 14 the impacts of other variables i e sand clay silt rock bare ec soil cec soil sar soil ph soil slope canopy cryptogam sar water basal ec water runoff ph water ammonium and litter on tds exist similarly the traditional linear tds model which is a function of electrical conductivity allison and richards 1954 is not sufficient to estimate tds however as mentioned in section 2 mlr cannot capture nonlinear relationships between predictors and response variables so the proposed mpr should be used to better estimate tds table 6 describes coefficients of the developed mpr model similar to the mlr model involving the 11 factors the mpr model prevents the multi collinearity issue because all vif values are less than 10 of these f7 ec water has the highest loading implying that it is the most significant factor associated with tds regarding loadings of the other ten factors f9 ph water and f1 sand clay silt rock bare ec soil and sediment concentration are two other important factors to estimate tds loads in stormwater runoff moreover unlike the result in table 5 f6 basal exhibits a robust nonlinear relationship with tds only f6 squared f62 instead of f6 is statistically significant at α 0 05 so there is a convex relationship between tds and f6 this implies that the traditional multivariate linear regression model is not appropriate for estimating a tds value although the mpr model has several insignificant factors i e f4 f6 f10 and f11 it can still account for about 74 74 of the tds data r2 0 7474 this model accuracy i e r2 is higher than the multivariate linear regression model shown in table 5 fig 5 compares the observed tds and estimated tds by the mpr model the mpr model shown in table 6 is developed via the tabu search algorithm as stated in section 3 2 finding the optimal structure of mpr without using the tabu search algorithm should consider 520 cases when all twenty factors potentially have the 5th power polynomial function i e the base of 520 the proposed tabu search algorithm enables the optimal mpr model to be found that is identical to the mrp model illustrated in table 6 the performance of the proposed algorithm is evaluated in terms of computational complexity or modeling complexity and prediction accuracy in addition its performance is compared with existing tds modeling approaches such as linear tds see eq 1 lr see eq 21 and multi layer neural network mnn with a learning rate of 0 01 the evaluation is conducted under the computing environment of intel core i5 8250u cpu 1 60 ghz table 7 summarizes the performance comparison results in table 7 the computational complexity or modeling complexity is measured by the modeling time since linear tds and lr are existing simple equations see eqs 1 and 21 they do not require modeling time similarly the parameters of mlr are simply computed by eq 3 so that its computational time is only 0 4 s however ai algorithms such as mpr with tabu search and mnn require structural learning processes to identify their optimal models so that they have higher modeling time than the former three approaches among the two ai algorithms the proposed approach performs better than mnn this is because the tabu search algorithm is a heuristic optimization algorithm only considering promising potential candidates i e models instead of reviewing all possible cases in the structure learning process of mrp as regards the prediction accuracy in terms of r 2 the proposed mpr model has a 0 81 higher value than that of mnn and 5 33 higher than mlr in fact mrp and mnn have better prediction accuracy than mlr it is difficult to argue that mrp is better than mnn given that the performance of both approaches is dependent on their structural learning processes however unlike the mnn the proposed mrp enables the impact of eleven factors on a response variable i e tds to be shown this analytical capability is critical to engineers and scientists who must identify significant factors to develop an effective tds management plan 5 conclusions and future research this study introduces a new tds modeling framework based on mpr and tabu search for an accurate simulation of salt transport across the landscapes on rangelands the proposed two stage tds modeling framework includes 1 selecting variables with fa and 2 tds model development based on mpr and tabu search in the first step fa is used to eliminate redundant predictors by identifying correlations between the observed and unobserved predictors i e factors from the original data with 20 predictors fa selects eleven factors namely factor 1 sand clay silt rock bare ec soil and sedconc factor 2 cec soil sar soil ph soil and slope factor 3 canopy factor 4 cryptogam factor 5 sar water factor 6 basal factor 7 ec water factor 8 runoff factor 9 ph water factor 10 ammonium and factor 11 litter the 87 2 of variability involved in the original data with 20 predictors is captured by the selected factors so that those selected can represent the impact of the original predictors on tds also the proposed framework can prevent the multi collinearity problem between predictors which results in incorrect calculation of the statistical significance of each independent predictor the second step conducts tds model development via mpr with the tabu search algorithm unlike the traditional multivariate regression modeling approach mpr clearly explains the nonlinear impact of parent variables i e predictors on dependent variables i e response variables to develop the best fitting polynomial tds model the prediction accuracy of the developed mpr model measured by r 2 is approximately 74 74 in addition to overcome the computation demand of mpr association with its structural learning process the tabu search algorithm is used to efficiently find the optimal structure see table 6 of mpr by considering effective candidate solutions from all possible alternatives the results show that tabu search can quickly build an mpr model for tds with high accuracy although both mrp and mnn provide high prediction accuracy than that of existing models i e linear tds lr and mlr the proposed mrp has the analytical capability thus the proposed framework will be helpful to engineers and scientists who have to identify significant factors to develop an effective tds management plan the tds model has been incorporated into the agricultural policy environmental extender apex model for watershed scale assessment of salt transport in the crb bailey et al 2022 the apex model williams and izaurralde 2010 is a watershed scale hydrologic and water quality model that has emerged as a valuable tool for simulating hydrodynamic and pollutant transport processes and evaluating the effects of land management practices as tds was used in apex as the indicator of salinity level in the water goehring 2017 the proposed tds modeling framework eventually enhanced the model s prediction accuracy on salt balances and transport in the price river watershed in future research the mpr framework will be extended to consider the interaction effects of parent variables on a dependent variable this will relax one assumption i e independence between parent variables of the proposed mpr model so that the modeling accuracy and flexibility of mpr can be significantly enhanced declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this project was supported by the bureau of land management award l17ac00125 we would like to thank dr mark weltz at usda agricultural research service at reno nevada for providing field experiment data 
25522,this study evaluates an emerging capability to monitor high spatial pixel size 3 7 m and temporal daily to sub daily imagery resolution coastal change using planetscope cubesats a new toolkit coastsat planetscope is presented that enables users to map shorelines from planetscope imagery using a workflow of image co registration segmentation thresholding shoreline detection and elevation correction the toolkit is subsequently tested at narrabeen collaroy beach se australia evaluating combinations of shoreline detection indices thresholding approaches and elevation correction an optimal shoreline accuracy of 3 5 m rmse is found for this coastline using the difference between the near infrared and blue bands and a weighted peaks thresholding approach a generic elevation correction model that considers tidal variability and wave setup at the shoreline is then proposed with a growing archive of high resolution imagery planetscope presents enormous potential for enhanced coastline mapping of the coast and complements existing approaches using landsat sentinel 2 imagery keywords coastal monitoring remote sensing extreme storms narrabeen coastsat data availability all in situ shoreline data used is publicly available software availability software name coastsat planetscope developer yarran doherty year first official release 2021 hardware requirements pc system requirements windows linux mac program language python program size 18 mb availability https github com ydoherty coastsat planetscope license gpl 3 0 documentation readme in github repository and guided example in jupyter notebook run times the data presented here was run a 2013 mackbook pro 8 gb ram with a 2 5 ghz dual core intel i5 processor for the 1000 images downloaded each image between 2 and 5 mb the total processing time was approximately 5 h and as follows 20 min for pre processing toa conversion and masking 2 5 h for co registration 35 min for scene merging 1 h for image classification and 50 min for shoreline extraction 1 introduction the two dimensional shoreline position is a fundamental metric in coastal engineering and coastal management boak and turner 2005 luijendijk et al 2018 vousdoukas et al 2020 among other things the shoreline position is a good proxy for the amount of beach sediment stored on the subaerial beach farris and list 2007 the width of the dry beach for both recreational purposes e g silberman and klock 1988 and coastal habitats e g fish et al 2008 and the amount of sand buffer needed for protection against extreme storm events e g harley et al 2009 plant and stockdon 2012 while time series of shoreline position can be obtained using in situ instrumentation emery 1961 moore 2000 mapping the shoreline position is increasingly undertaken using remote sensing splinter et al 2018 this has historically comprised aerial photographs or photogrammetry romine and fletcher 2013 and as technology has progressed extended to other platforms such as ground based video holman et al 2003 pianca et al 2015 uav pucino et al 2021 turner et al 2016a smartphones harley et al 2019 jaud et al 2019 and satellites almeida et al 2021 pardo pascual et al 2012 vos et al 2019b bishop taylor et al 2021 providing global coverage satellites are particularly appealing to monitoring coastline change and satellite derived shoreline or sds mapping has been the focus of recent research e g liu et al 2017 pardo pascual et al 2012 applications range from opportunistic site specific investigations e g ford 2013 to global scale studies of multi decadal shoreline trends e g luijendijk et al 2018 mentaschi et al 2018 the vast majority of studies involving satellite derived shorelines to date have been based on the publicly available landsat and or sentinel 2 data e g almonacid caballer et al 2016 hagenaars et al 2018 pardo pascual et al 2012 2018 mao et al 2021 this has been in part driven by cloud computing resources such as google earth engine gorelick et al 2017 that streamline data access to the vast landsat and sentinel 2 image archive open source tools such as the desktop based python toolkit coastsat vos et al 2019b and web based cassie almeida et al 2021 have subsequently been developed to automate the task of shoreline extraction for a selected coastal location using information from the images stored in the short wave infrared swir and red green blue rgb colour bands validation of these sds tools indicate shoreline accuracy in the range of 10 15 m which through subpixel image processing techniques is typically smaller than the pixel resolution of landsat sentinel 2 imagery 10 30 m because of their multi decadal length and approximately bi weekly sampling frequency these tools are well suited to monitoring intra annual to decadal shoreline variability hagenaars et al 2018 vos et al 2019a in comparison to publicly available satellite data applications using commercial satellites have to date been relatively limited this has been primary due to the inhibitive data cost of commercial satellite imagery but also a result of their patchy data coverage since data collection is primarily tasked to monitor certain locations and their much shorter archive length compared to the multi decadal landsat and sentinel 2 archive belward and skøien 2015 nevertheless examples of commercial satellite imagery applied to the coast include aster dewi 2019 quickbird almonacid caballer et al 2016 spot garcía rubio et al 2015 rapideye duarte et al 2018 ikonos worldview 2 and geoeye 1 ford 2013 one emerging field of coastal monitoring using satellites is that of constellations comprising hundreds of small 10x10x30cm cubesat micro satellites equipped with rgb and infrared sensors of this range of micro satellites the constellation known as planetscope has been an industry leader first launched in march 2016 the planetscope constellation comprises 130 satellites known as doves arranged in a sun synchronous orbit around the earth with a swath size of 24 32 5 km planet 2020 due to their small size cubesat satellites are cheap to manufacture and launch relative to traditional custom built satellites consequently the planetscope fleet is regularly updated via opportunistic third party rocket launches with improved satellite components and sensor upgrades this also results in a temporal variability of available images at specific study sites of interest as more satellites come online and others are discontinued the images used in this study contain three generations of sensors ps2 ps2 sd and psb sd the reader is referred to the planet documentations for full details and additional sensor upgrades planet 2020 a summary of planetscope history and imaging capabilities relative to landsat and sentinel 2 missions is presented in fig 1 compared to the monthly to bi weekly temporal resolution of landsat and sentinel 2 images the planetscope dove constellation is capable of sampling at daily to sub daily resolution fig 1b additionally the pixel size of planetscope is 3 7m at nadir resampled to a 3 m fixed resolution which is significantly greater than the 10 30 m resolution of landsat and sentinel 2 imagery fig 1c however the current generation of planetscope images do not contain the swir band that is commonly used in mapping satellite derived shorelines as such the tools developed for landsat and sentinel images cannot be directly applied to this new and high resolution data source if other image bands such as rgb and near infrared can be shown to accurately capture the shoreline position this opens up the opportunity for much higher resolution shoreline analysis kelly and gontz 2020 with a growing image archive the high spatial resolution and frequent revisit period of planetscope imagery has the potential to provide both long term and discrete storm event monitoring at a consistent temporal resolution that has previously been unattainable outside of fixed remote sensing shoreline monitoring stations such as video camera networks bracs et al 2016 harley et al 2019 holman and stanley 2007 notably this technology has the potential to do so for any coastline on earth every day however more information is needed into the best methodology to extract shoreline features from these images this paper presents a new open source toolkit coastsat planetscope that builds on the popular coastsat package of vos et al 2019b and is described in section 2 the coastsat package was originally developed and optimized for landsat and sentinel 2 imagery using the google earth engine api and has since been applied at many locations worldwide including europe castelle et al 2021 australia cuttler et al 2020 africa lawson et al 2021 and asia adebisi et al 2021 however the application of satellite shoreline detection to planetscope imagery requires adaptation of the original coastsat algorithm to address some new data challenges as well as potential improvements associated with the increased accuracy and frequency of the planetscope products first the reduced number of spectral bands provided by planetscope cubesats specifically the lack of a short wave infrared band means that the modified normalized difference water index mndwi refer xu 2006 that underpins coastsat cannot be utilized for shoreline detection as such this work focuses on optimizing a new shoreline extraction algorithm based on the available planetscope bands of red green blue and near infrared nir second as the spatial resolution of the planetscope imagery is significantly higher than that of landsat and sentinel 2 the potential improvement in shoreline accuracy when correcting for both tides and waves as opposed to just tides in the original coastsat algorithm is also explored application of this new coastsat planetscope toolbox is validated at narrabeen collaroy beach in south east australia with a high quality multidecadal shoreline monitoring program well established at the site turner et al 2016b the narrabeen collaroy dataset includes monthly surveys along five cross shore transects with additional surveys performed before and after major erosive events a storm erosion case study investigates the ability of planetscope derived shorelines to monitor alongshore variable erosion during discrete storm events lastly a blind validation of the toolkit is performed at duck north carolina usa using a generalized water level correction method 2 toolkit overview and testing the overall workflow for the coastsat planetscope toolkit is outlined in fig 2 and comprises several key steps from raw image data to shoreline products 1 planetscope image pre processing 2 image band extraction and thresholding and 3 shoreline extraction and elevation correction briefly once the user has defined an area of interest in step 1 raw digital number planetscope images rbg nir are pre processed to top of atmosphere toa cropped and merged to the area of interest co registered and classified with a pre trained supervised machine learning neural network algorithm in step 2 the 4 bands rgb nir are used to calculate a water index e g ndwi red minus blue from which the shoreline is extracted with the marching squares contouring technique based on an image specific threshold finally the 2d shorelines are intersected with shore normal transects and the water level correction is applied along each transect a more detailed description of the workflow is provided in sections 2 2 and 2 3 below algorithm testing and optimization were undertaken using in situ shoreline measurements from the narrabeen collaroy coastal monitoring site hereafter narrabeen a dynamic sandy beach located in south east australia where continuous beach measurements have been undertaken since 1976 turner et al 2016b a total of 24 combinations of indices thresholding methods and elevation models are tested to obtain an optimal combination that is subsequently adopted in the toolkit the application of the toolkit at narrabeen provides a useful demonstration for the user and a step by step workthrough is provided in an accompanying jupyter notebook 2 1 narrabeen testing site narrabeen is located on the northern beaches of sydney australia fig 3 and is one of the longest continuously monitored beaches worldwide turner et al 2016b its open access dataset is used as a testbed for a range of coastal remote sensing applications including shorelines derived from landsat sentinel 2 liu et al 2017 luijendijk et al 2018 vos et al 2019a coastal imaging harley et al 2011 and smartphones harley et al 2019 harley and kinsela 2022 at approximately 3 6 km in length the narrabeen embayment is bounded to the north by narrabeen head and to the south by long reef point narrabeen exhibits a microtidal tide regime with a mean spring range of 1 3 m the deep water wave climate is primarily composed of sse swell waves with a mean significant wave height hs of 1 6 m a mean peak wave period tp of 10 s and a storm wave height defined by hs 3m corresponding to the 5 exceedance of hs nearshore wave conditions vary significantly alongshore due to sheltering from long reef point of the predominantly southerly waves with the northern end characteristically more exposed to the persistent southerly waves and southern end relatively sheltered turner et al 2016b the beach is primarily composed of fine to medium grained quartz sand with a d50 of approximately 0 3 mm the intertidal beach slope varies significantly through time with typical variability between 0 07 and 0 13 and an average intertidal slope of 0 09 vos et al 2020 the shoreline varies at weekly to monthly timescales due to the frequent passing of storms and subsequent beach recovery phillips et al 2017 in addition annual and interannual variability on the order of 50m is also visible over the 40 years of monitoring turner et al 2016b for the purpose of algorithm testing in situ survey data at narrabeen were utilized for the time period july 2016 coinciding when planetscope imagery first became available at the site to august 2020 these measurements comprise 90 individual in situ survey dates at an average interval of 16 days in situ surveys were conducted using rtk gnss vertical accuracy 0 05 m horizontal accuracy 0 03 m at the five historical cross shore transects that have been measured continuously since 1976 profiles pf1 pf2 pf4 pf6 and pf8 fig 1 following previous studies e g harley et al 2011 cross shore shoreline positions were defined at these transects using the intersection of the surveyed profile with mean high water springs mhws which corresponds at this site to an elevation of 0 7 m above mean sea level amsl the mhws elevation has been shown to be a suitable reference datum for mapping shoreline variability at this site as it minimizes the influence of ephemeral low tide features such as low tide terraces and swash bars on shoreline variability phillips et al 2017 in addition to the routine surveying high resolution 50 100 m transect spacing survey data was also collected using rtk gnss for a significant storm event that occurred between 8 and 10 february 2020 this storm was characterised by waves over 3 m hs from an ese direction for a duration of 39 h reaching a peak hs of 6 5 m on 9 february high resolution rapid response survey data were collected both two days prior to the storm 6 february 2020 and four days after storm conditions subsided 14 february 2020 these additional high resolution data enabled to evaluate the capacity of planetscope derived shorelines to capture alongshore variability in erosion following a large storm 2 2 planetscope image pre processing imagery supplied by planet from the planetscope satellite constellation is distributed into three data products all based on raw sensor data captured at an average ground sample distance of 3 7 m at nadir planet 2020 the four band red green blue and near infrared analytic ortho scene product is specifically used for shoreline mapping and tested in this study this product orthorectifies and resamples image pixel data from a 3 7 m pixel resolution at nadir to a consistent 3 m pixel resolution with data provided as digital number dn values that correspond to at sensor detected radiance to reduce scene to scene variability and enable direct comparison between images chander et al 2009 dn values were converted in this study to top of atmosphere toa reflectance via planetscope provided conversion factors an accompanying unusable data mask udm file is also provided for each image to aid in the detection and filtering of sensor errors data loss and regions of cloud cover the orthorectification accuracy of raw planetscope analytic ortho scenes is stated at less than 10 m planet 2020 further independent studies have verified this accuracy with observed horizontal rmse values between 3 4 m and 5 2 m dobrinić et al 2018 lemajic et al 2017 to reduce geolocation error prior to shoreline mapping all individual downloaded images are co registered an image co registration step is performed in the toolkit using the open source arosics library scheffler et al 2017 briefly this co registration software uses phase correlation in the frequency domain to automatically detect and correct sub pixel misalignments between a reference image and a set of target images rmse of 0 3 pixels the reference image is chosen by the user from the available images where the image is focussed cloud free and a shoreline can be seen clearly following this co registration step overlapping scenes from the same sensor on the same day are snapshots from a flyover and are merged and treated as one scene to reduce unnecessary data and processing times an area of interest aoi is subsequently defined by the user to crop the images after merging and cropping images image pixels in the ortho scene then undergo an image classification where one of four separate classes are identified sand water whitewater and other where other refers to various features such as buildings roads vegetation etc this image classification is undertaken using a neural network multilayer perceptron in the open source python toolkit scikit learn pedregosa et al 2011 in line with the methodology presented in coastsat vos et al 2019b a total of 572 planetscope analytic ortho scenes at narrabeen spanning 457 unique dates were downloaded for the july 2016 to august 2020 testing period these 572 images were selected based on a cloud cover threshold over the entire scene of 50 or less which from preliminary testing was found to remove the majority of unusable cloud impacted scenes at the site as the planetscope constellation has a sun synchronous orbit the vast majority of images occurred within an hour of the median local solar image timestamp at narrabeen 9 30 a m due to the small fleet size in 2016 it was noted that image recurrence intervals were considerably more sporadic at the start of the study period with a maximum time interval of 111 days recorded as shown in fig 1a image recurrence increased significantly to near daily from mid 2017 onwards with the launch of additional dove satellite fleets training data for the image classification neural network was based on a subset of 25 images these were manually digitized to include approximately 25 000 pixels for each of the image classes except the whitewater class which only had 500 pixels the classification accuracy was assessed by means of a 10 fold cross validation which determined a pixel classification accuracy of 99 5 2 2 1 shoreline detection algorithm with each scene corrected to the top of atmosphere masked co registered and classified a generalized shoreline detection algorithm is then used to identify the two dimensional shoreline position this shoreline detection algorithm first creates a greyscale image based on a certain index tested below with a particular focus on the water and sand classes identified from the image classification step above next a thresholding technique is applied to find the pixel value that delineates between water and sand pixels finally a sub pixel marching squares contouring algorithm cipolletti et al 2012 is applied based on the identified threshold to identify the two dimensional shoreline a total of six different spectral indices were tested at narrabeen using combinations of the four bands available in the planetscope analytic ortho scene these indices were based on both absolute and normalized values of common water indices used in remote sensing applications e g the normalized difference water index mcfeeters 1996 as well as indices found suitable specifically for shoreline mapping based on colour bands only e g red minus blue harley et al 2019 these six indices are defined below 1 r m b r e d b l u e 2 r m b n o r m r e d b l u e r e d b l u e 3 n m g n i r g r e e n 4 n d w i n i r g r e e n n i r g r e e n 5 n m b n i r b l u e 6 n m b n o r m n i r b l u e n i r b l u e where red blue green and nir correspond to the toa values of each spectral band for each of these indices two localized thresholding algorithms were investigated to determine the optimum threshold index opt for shoreline detection between sand and water pixels as classified by the neural network described above the two algorithms tested were the commonly used otsu thresholding technique otsu 1979 as well as an alternate weighted peaks wp thresholding approach the latter approach was developed by harley et al 2019 and calculated based on the location of the peaks in the probability density function pdf of the sand and water pixel index values in contrast to the common otsu thresholding which maximises interclass variance the wp thresholding algorithm weights the optimized index threshold towards the sand peak which has been previously found to be robust for shoreline mapping purposes at a range of coastal locations worldwide plant et al 2007 harley et al 2019 the optimum index based on wp thresholding is calculated as follows 7 i n d e x o p t w p x ˆ w a t e r 0 7 x ˆ s a n d x ˆ w a t e r where x ˆ w a t e r and x ˆ s a n d are the x location of the peaks in the pdf of the water and sand pixels respectively following identification of the optimum threshold the shoreline is identified along pixels of this optimum index value by means of a marching squares contouring algorithm cipolletti et al 2012 that determines a 2d contour line via linear interpolation of adjacent cells values an example of the entire shoreline detection process from image classification indexing and thresholding for the specific case of the nmb index with wp thresholding is shown in fig 4 2 3 elevation correction model the following section outlines more advanced techniques that can be applied to further correct the extracted satellite derived shoreline time series described above if the user desires these techniques require various additional data sources that may include a time series of ocean water level elevations intertidal beach slopes and wave characteristics astronomical tides or observed ocean water levels may be extracted from various sources at the user s discretion the intertidal beach slope may be derived from in situ data e g beach profiles or from remote sensing algorithms such as coastsat slope vos et al 2020 local wave time series wave height period may be provided from local in situ wave buoys or from wave hindcasts reanalysis since satellite imagery represent an instantaneous snapshot of the coast repeat shoreline measurements inherently include high frequency i e sub daily oscillations associated with rising falling ocean water levels tides non tidal anomalies swash motions wave setup and runup as well as other factors changing lighting conditions beach morphology granulometry etc previous approaches to reduce this inherent noise have been to either map satellite derived shorelines on composite images averaged over large i e annual time windows almeida et al 2021 bishop taylor et al 2021 liu et al 2017 luijendijk et al 2018 or apply an elevation correction model to correct for reduce these high frequency effects castelle et al 2021 pardo pascual et al 2018 vos et al 2019a based on lower resolution 10 m landsat and sentinel 2 images a simple tidal correction δ x t i d e has been previously applied to project the cross shore shoreline position detected by the satellite xsat to a consistent elevation based on a specific elevation datum xdatum 8 x d a t u m t x s a t t δ x c o r r t 9 δ x c o r r t z t i d e t z d a t u m m simple tidal correction model where ztide t is the ocean water level elevation at the time of image capture taken from a nearby tide gauge zdatum is the reference elevation datum to define the shoreline and m is the intertidal beach slope recognizing the potential for improved shoreline accuracy from high resolution planetscope imagery a more advanced elevation correction model was explored in this study where tide or ocean water levels intertidal beach slope and local wave conditions were known as shown in fig 5 the horizontal difference δxcorr between the detected shoreline position x sat measured in reference to the landward benchmark and the datum based shoreline can be expressed as the linear sum of tidal effects δxtide wave effects δxwave and systematic biases due to the satellite detection method as well as any other remaining factors δxdet 10 δ x c o r r t δ x t i d e t δ x w a v e t δ x d e t t following eq 3 in harley et al 2011 this is expressed by the empirical elevation correction model 11 δ x c o r r t α z t i d e t z d a t u m m β h 0 t l 0 t c where α β and c are empirically derived parameters obtained from multiple linear regression see below h 0 is the deep water significant wave height and l 0 the deep water wavelength derived from linear wave theory note that where the coefficient of the simple tidal correction model eq 9 used in previous studies is assumed to be equal to unity i e α 1 representing the simple case of a planar beach face and no tidal modulations of other processes here this coefficient is allowed to vary to potentially take into account other tidal effects time varying wave runup setup processes are incorporated in this elevation model via the second term in eq 11 which is derived from numerous lab and field observations holman et al 2003 hunt 1958 stockdon et al 2006 that show that these processes scale well by h 0 l 0 similar to α here β is allowed to freely vary and does not take on any predefined value e g 0 35 for stockdon et al 2006 the third term c represents any remaining systematic offsets positive c corresponding to a landward bias which is shown later can also include time averaged wave runup setup processes that are not accounted for in the second term the three parameters in eq 11 were solved at each of the five transects at narrabeen using a least squares multiple linear regression based on the horizontal difference between the detected shoreline position x sat and the measured shoreline position at the reference datum in this case the 0 7m elevation corresponding to mhws obtained from rtk gnss surveys deep water wave data were obtained from the sydney waverider buoy located 11 km offshore in 80 m water depth and ocean water level data obtained from a nearby tide gauge hmas penguin intertidal beach slopes m were calculated based on average values for each transect derived from the rtk gnss surveys over the 2016 2020 study period these equated to m 0 11 pf1 pf2 pf4 m 0 13 pf6 and m 0 12 pf8 while a time invariant beach slope was chosen in this example as in most cases worldwide where beach slope is not well resolved it is acknowledged that a user may also wish to use a time varying slope term if the data is available finally to test whether incorporating time varying wave data improves the accuracy of planetscope shorelines at the validation site an elevation model that removed the second term in eq 11 was also tested 12 δ x c o r r t α z t i d e t z d a t u m m c the two parameters of this model α and c were calculated using the same multiple linear regression process above 3 testing results 3 1 optimal shoreline extraction methodology to determine the optimal shoreline extraction methodology for the narrabeen validation site planetscope derived shoreline positions were calculated at each of the five transects and compared with in situ data for all proposed extraction methodologies described above taking into account the six different indices tested refer eqs 1 6 two thresholding algorithms otsu and weighted peaks thresholding and two elevation detection models refer eqs 11 and 12 this amounted to 24 unique combinations shoreline results at all five transects were then combined to provide an overall summary of the residual error based on the cross shore rmse between planetscope derived and measured shoreline positions at the reference datum 0 7 m amsl table 1 summarizes results for all 24 survey extraction methodologies planetscope derived shoreline accuracies were found to range from an rmse of 3 5 m 5 1 m as discussed later in section 4 3 this represents a substantial improvement in shoreline accuracy compared to equivalent efforts using landsat and sentinel 2 imagery at this site based on the coastsat toolbox rmse 8 2 m vos et al 2019a four of these methodologies were found to have equally optimal accuracy with each indicating a cross shore rmse between planetscope derived and measured shoreline positions of 3 5 m these four optimal methodologies were using the near infrared minus green nmg eq 3 or near infrared minus blue nmb eq 6 indices in combination with the wp thresholding and either of the two elevation models see shoreline extraction method 6 8 14 and 16 in table 1 the lowest performing methodologies in terms of rmse meanwhile were the normalized red minus blue rmb norm index eq 2 in combination with otsu thresholding and either of the two elevation correction models see shoreline extraction method 21 and 23 table 1 comparing the results for each of the shoreline extraction methods the analyses indicate a moderate sensitivity to the thresholding approach and index type but a negligible sensitivity between the two elevation models tested eq 11 vs eq 12 in terms of the thresholding approach a 5 45 improvement in shoreline accuracy was found for each index when using the weighted peaks thresholding over the otsu thresholding approach the index meanwhile was particularly sensitive to whether normalizing was applied with non normalized indices i e nmb rmb and nmg providing noticeably improved shoreline performance average rmse 3 7 m to those with normalizing applied i e ndwi nmb norm and rmb norm average rmse 4 4 m regarding the elevation correction model the addition of a time varying wave runup and setup term to the shoreline elevation model eq 11 only resulted in a 1 improvement in shoreline accuracy compared to the model with only ocean water level variability and a constant eq 12 this suggests that at the steep beach morphology tested here time varying wave information such as that sourced from a nearby wave gauge might not necessarily be needed to improve shoreline accuracy discussed in more detail in section 4 1 since four combinations were found to have equally optimal performance only the nmb wp tide only model methodology method 8 was subsequently selected for further analysis below fig 6 presents the corresponding time series of shoreline change at each of the five transects in comparison to in situ measurements using method 8 overall there is a strong agreement between the in situ and planescope derived shorelines with the planetscope derived shorelines describing between 86 and 91 of shoreline variation over the four year time period this includes seasonal cycles of erosion and accretion high frequency i e monthly shoreline variability as well as episodic storm erosion and recovery such as that observed in the february 2020 storm at the end of the time series see section 3 3 below note that profile pf6 is backed by a seawall cross shore location shown in fig 6 by red dash line and shoreline points were subsequently truncated landward of this mark on a transect by transect basis the rmse using this optimal methodology was found to vary from a minimum rmse of 2 9 m pf8 to a maximum rmse of 4 2 m pf1 these minimum and maximum errors coincide with the most sheltered and most exposed parts of the embayment in terms of wave energy which suggests that error is enhanced by increased individual swash motions at the more exposed location pf1 and reduced in regions of lower wave activity pf8 3 2 elevation correction model parameters the results above were based on optimized parameters of the two elevation correction models tested using multiple linear regression of the shoreline data combined from all five beach transects these parameters were the tidal correction model coefficient α the time varying wave runup setup coefficient β and the constant c accounting for any remaining systematic offsets as summarized in table 1 across all 24 survey extraction methodologies the tidal correction model coefficient was found to be close to unity α 0 9 to 1 07 suggesting that the simple planar profile is a suitable approximation for tidal corrections at this site for the 12 methodologies that also included the additional time varying wave runup setup term the coefficient β was found to vary from 0 01 methods 1 5 6 9 and 14 to 0 07 method 2 the constant c meanwhile resulted in a wide range of optimized values from negative values that represent a systematically more seawards horizontal offset c 3 8 m method 1 to positive values characterising a systematically more landwards offset c 5 3 m method 12 generally there was little variability in the constant c between the tide and wave model and the tide only model for each index and thresholding average difference between the two optimized c values 0 3 m for the selected method 8 a tidal correction model coefficient of α 0 95 and a constant horizontal offset c 5 2 m was found 3 3 performance of planetscope derived shorelines on individual extreme storm response in addition to evaluating the accuracy of planetscope imagery in obtaining time series of shoreline change e g fig 6 a key capability explored in this study was the ability of planetscope derived shorelines to map alongshore variability in shoreline change following an extreme episodic storm event fig 6 presents comparisons between pre and post storm shoreline positions derived from both planetscope imagery using the optimal shoreline extraction methodology identified above and rtk gnss survey data for the large storm event that struck the coastline at narrabeen between 9 and 10 february 2020 these comparisons were undertaken for both the individual pre and post storm shoreline positions fig 7 a as well as for the alongshore variability in shoreline change resulting from the storm fig 7b planetscope imagery for shoreline mapping was selected five days prior to storm and just two days after the storm based on the nearest available data spanning the storm event overall a strong agreement was found between individual pre and post storm shorelines r2 0 98 and 0 93 rmse 2 4 m and 3 5 m respectively derived from planetscope and in situ rtk gnss shoreline measurements at the reference datum examining the difference between pre and post storm shorelines fig 7b presents the change in shoreline due to the storm as determined by planetscope shorelines and in situ measurements respectively this figure indicates significant alongshore variability in storm erosion with some hotspots local maxima of shoreline erosion observed at 600 m alongshore measured shoreline change 34 m and 1800 m alongshore measured shoreline change 30 m a general trend of enhanced shoreline retreat at the northern end 2000 m alongshore compared to the southern end 2000 m alongshore is also evident in the rtk gnss measurements the corresponding planetscope derived shoreline change measurements also show good agreement with these general trends and local maxima such that the overall rmse is 3 66 m and r2 0 86 the alongshore averaged shoreline change for the entire 3 6 km beach was also estimated accurately by the planetscope data with 18 m of shoreline change for the planetscope derived shorelines and 19 m for the rtk gnss survey data 4 discussion 4 1 optimized elevation correction model parameters optimization of the model parameters used to correct for elevation differences between the instantaneous planetscope derived shorelines and the reference datum provide crucial insights into the shoreline detection performance regarding the tidal correction term the model coefficient α was found to be roughly equal to the value of unity typically assigned for tidal correction based on the assumption of a planar beach face and no tidal modulation of elevation differences castelle et al 2021 vos et al 2019a this suggests that these assumptions are valid for the narrabeen collaroy site with small differences likely due to subtle beach slope m changes over time that are not accounted for in the satellite derived shoreline elevation correction model tidal correction based on a time varying beach slope was also trialled although this resulted in a reduction in shoreline accuracy relative to time averaged values for the wave runup setup term comparisons between optimized models both including and excluding time varying wave runup and setup found negligible differences in terms of planetscope derived shoreline accuracy this result suggests that time varying wave setup and runup is not an important consideration in planetscope shoreline mapping at the narrabeen site the practical implication of this finding is that it suggests there might be little advantage in terms of shoreline accuracy in including time series of wave variability which is often difficult to obtain without ready access to in situ wave measurements from a nearby wave buoy it should be noted that this result is likely to be site specific and related to the steep and microtidal nature of the narrabeen site for example in southwest france castelle et al 2021 found using lower resolution landsat imagery that shoreline accuracy was improved by a factor of two when a time varying wave runup equation was included in the elevation correction model this was attributed to the low beach face gradient m 0 05 and energetic and highly seasonal wave conditions at this site average hs 1 1 m 2 4 m in summer winter respectively such conditions can result in much larger horizontal variations in wave runup relative to steeper beaches with less seasonally varying wave climates like narrabeen further validation of this new algorithm across a range of sandy beach environments is needed to explore the robustness of this result the influences of wave runup and setup processes on the planetscope derived shoreline position at narrabeen instead appear to be captured within the constant offset term c fig 8 illustrates the alongshore variability in both α and c for each transect along the beach using the method 8 combination comprising the nmb index the weighted peaks thresholding and the tide only elevation correction model a distinct gradient in the offset term fig 8b is indicated that appears similar in shape to the variability of wave exposure along the beach with c 4 0 m at the most sheltered profile pf8 and c 5 5 m 6 2 m for the three northern profiles exposed to the predominantly sse waves of the region pf1 pf2 and pf4 to further explore alongshore variability in wave runup and setup effects the optimized c term at each transect was subsequently compared to that estimated from a standard wave setup equation at the shoreline stockdon et al 2006 eq 10 this is expressed horizontally as 13 η m 0 35 h 0 l 0 where η m is the estimated horizontal offset by wave setup at the shoreline using equation 13 the average time invariant horizontal offset by wave setup was estimated at each transect by first transforming average offshore wave conditions at narrabeen h0 1 6 m tp 10 s θ 135 to the 10 m depth contour using a nearshore wave transformation tool based on 1000s of swan model runs refer to turner et al 2016b since equation 13 is based on deep water conditions this wave data at the 10 m depth contour was then reverse shoaled to give the equivalent deep water wave conditions i e h0 and l0 at each transect from which the wave setup was calculated estimates of η are shown as crosses in fig 8b and reveal a reasonable agreement r2 0 60 between this offset term and that estimated using the wave setup formulation deviations between the optimized c value and the estimated setup also only range from 0 1 m pf6 to 1 5 m pf2 suggesting that this systematic offset can be estimated using this commonly used wave setup formulation 4 2 blind application of generalized elevation model at duck usa the above results are based on optimized model coefficients derived from extensive in situ beach measurements which are rare and only available at a small subset of sites worldwide turner et al 2016a to test the ability of coastsat planetscope to map shoreline variability at an unseen coastal location the toolkit was applied blindly to planetscope imagery covering the u s army corps of engineers field research facility at duck north carolina usa zhang and larson 2021 the duck field research facility is an internationally recognized coastal observatory located on the northern outer banks a low lying barrier island system the beach slope at duck is similar to that of narrabeen m 0 10 and has a similar microtidal and semidiurnal tide range mean spring tidal range 1 1 m wave conditions are typically less energetic relative to narrabeen mean hs 1 0 m tp 9 s and interrupted by storm events defined by a threshold hs 2m from hurricanes and atlantic nor easters a 1 2 km stretch of coastline centred on a research pier has been monitored extensively at duck since 1977 using a combination of survey techniques o dea et al 2019 pianca et al 2015 here the blind application is focused on four cross shore transects defined by the local alongshore reference system oriented roughly from sse to nnw pf 91 m pf 320 m pf 731 m and pf 1097 m refer zhang and larson 2021 a total of 166 planetscope images were sourced at the duck site over the period november 2017 to august 2020 equivalent to an average interval of one image every 6 days following the results above shoreline detection was undertaken using the method 8 combination as above two alternate steps were subsequently taken in order to make this application at duck truly blind and not rely on any in situ beach measurements to estimate the beach slope m needed for the elevation correction model eq 12 the method of vos et al 2020 was applied to the planetscope derived shorelines this algorithm estimates the beach slope by iteratively searching the optimum m value that minimizes short term shoreline variability in the frequency band of tidal constituents identified in the shoreline time series validation of this technique including at both the duck and narrabeen sites has found that the beach slope can be estimated to within 0 01 the application of this step identified beach slopes of 0 08 0 11 for the four duck profiles the second step comprised the use of a generalized elevation correction model rather than relying on site specific optimized model coefficients as undertaken at narrabeen based on the findings above that the time varying wave runup and setup term may have little practical benefit at some sites and that the constant offset c can be approximated using the stockdon et al 2006 formulation the following generalized model was adopted 14 δ x c o r r t z t i d e t z d a t u m m 0 35 h 0 l 0 tide conditions at the time of image capture were taken from the nearby tide gauge at the field research facility fig 9 presents time series of planetscope derived shorelines relative to in situ survey measurements at the four cross shore profiles at duck using the same reference datum used at narrabeen shoreline accuracy using this blind application is shown to vary from a minimum rmse of 5 1 m pf 320 m to 6 4 m pf 1097 this represents a slight decrease in shoreline accuracy relative to narrabeen using site specific optimized model coefficients but is more accurate than using lower resolution landsat or sentinel 2 imagery at duck with the coastsat algorithm rmse 9 0 vos et al 2019a the increase in accuracy obtained by planetscope imagery is subsequently sufficient to capture smaller scale variability at the sub annual or seasonal time scales particularly considering the generally smaller magnitude of shoreline variability at duck relative to narrabeen this was identified as a key limitation of applying coastsat and landsat sentinel 2 at less dynamic sites vos et al 2019a also evident in this time series is a significant artificial beach nourishment works that was undertaken at the northern end of the site refer profile pf 1097 fig 7 in may june 2017 the high frequency imagery of the planetscope constellation meant that the shoreline both immediately before and immediately after the nourishment works could be mapped enabling accurate quantification of the shoreline response to this engineering intervention a corresponding 60 m increase in shoreline position is observed in this northern time series which is followed by a gradual reduction as sediment is moved alongshore further south at profile pf 731 the shoreline is observed to increase approximately 30 m at a lag of approximately three months following the nourishment 4 3 benefits and challenges of planetscope imagery for shoreline mapping the narrabeen beach monitoring site has been used as a testbed for a range of advanced coastal remote sensing applications splinter et al 2018 thereby making it possible to compare the overall performance of planetscope derived shoreline detection to other optical remote sensing applications table 2 summarizes results of previous shoreline accuracy tests applied at narrabeen including satellite derived methods luijendijk et al 2018 vos et al 2019a this study land based video imaging using the argus coastal imaging system harley et al 2011 and a citizen science based approach using consumer smartphones known as coastsnap harley et al 2019 harley and kinsela 2022 these different approaches span a range of image pixel resolutions depending on the approach and sensor used from 30 m for older landsat 5 satellite missions and no panchromatic sharpening to 0 1 m pixels for land based argus video imaging in the image nearfield note that whereas satellite imagery is typically close to nadir and hence have negligible resolution variability over the spatial scale of individual beaches kilometers land based approaches have significant variability in pixel resolution alongshore due to their low angle of obliquity holman and stanley 2007 as expected a general trend of increased shoreline accuracy with pixel resolution is observed in table 2 with the planetscope derived shorelines significantly more accurate than landsat and sentinel 2 derived approaches horizontal accuracy 7 4 m 13 7 m but less accurate than land based methods using stationary video cameras or smartphones minimum horizontal accuracy 1 2 m for argus video imaging this suggests that further improvements could be made in satellite derived shoreline accuracy with the addition of other very high resolution vhr satellite missions such as the maxar worldview 3 satellites launched in 2014 pixel resolution 0 31 m turner et al 2021 however as shown in this study the shoreline accuracy reported here 3 5 m is commensurate to the pixel resolution of the satellite image 3 7 m which contrasts with past work using coarser satellite imagery and sub pixel techniques this may suggest a limit in the pixel resolution substantially reducing the overall shoreline detection error with the remaining error attributed to unresolved processes such as individual swash motions additionally an important consideration with regards to accessing this vhr image data including planetscope is that it is presently limited to either restricted research licenses organisations countries with data sharing agreements or cost prohibitive pay per image funding models for instance the planetscope imagery obtained in this study was made available through planet s education and research program that provides a monthly download quota of 5000 km2 to university affiliated students and researchers for the 4 years of data used in this study the study area required up to 2 images per scene to cover the extents of narrabeen collaroy and resulted in roughly 1000 raw images being downloaded and was covered within a monthly download limit for longer timescales and larger study areas as well as more frequent passes of the planet constellation longer download times may be needed to meet the quota restrictions this provides a major barrier for uptake of planetscope and other vhr imagery for routine shoreline monitoring and makes publicly available imagery like landsat and sentinel 2 or land based approaches still appealing another challenge of satellite derived shorelines approaches such as planetscope is their present reliance on instantaneous snapshots of the coast from single satellite passes the instantaneous sampling of what is typically a highly dynamic and turbulent land water interface particularly on wave dominated coastlines like narrabeen introduces error into the shoreline detection as high frequency swash motions are difficult to resolve using simple elevation correction models such as that proposed in eqs 11 and 12 these errors are typically reduced in land based approaches using time averaging where video frames over a certain time period nominally 10 min are averaged to smooth out these high frequency motions for example harley et al 2019 compared shorelines mapped by both instantaneous images and time averaged images averaged over 10 min and observed an 18 improvement in shoreline accuracy although not explored in this study the growing constellation of dove cubesats can result in multiple satellite passes over a single day fig 1 this sub daily shoreline data could potentially be exploited as a rudimentary means of smoothing shoreline oscillations and reduce shoreline noise along with routine shoreline mapping over many years the findings of this study highlight the benefit of planetscope imagery for rapid quantification of shoreline response due to extreme storm events with an approximate median image recurrence of 2 days fig 1 planetscope imagery is able to capture pending cloud coverage constraints the shoreline position both immediately prior to and following an extreme storm event striking the coast as shown in fig 7 for a storm event that occurred in february 2020 planetscope derived shorelines were able to quantify not only the average shoreline change caused by the event but significant detail regarding the alongshore variability in erosion as well as localized erosion hotpots at the spatial scale of approximately 100 m the capability of planetscope imagery for rapid shoreline assessment was also investigated by kelly and gontz 2020 who applied planetscope derived shorelines to assess the impact of a severe tropical cyclone cyclone oma that struck eastern australia in february 2019 horizontal deviations between planetscope and measured shorelines in their study obtained from a lidar survey were found to range between 0 m and 22 m with a mean offset equivalent to the parameter c in the present study of 9 m while the horizontal rmse was not reported in kelly and gontz 2020 the seemingly lower accuracy range compared to the present study is likely due to their study being based on a single validation survey and planetscope shorelines were manually digitized rather than using the more robust method detailed here as highlighted in table 1 the results show that significant improvements in shoreline accuracy can be obtained using an optimized combination of shoreline index thresholding approach and elevation correction model and that these factors should be considered for accurate shoreline mapping using planetscope imagery as is the case with other satellite remote sensing toolkits e g coastsat site specific calibration of the classifier and additional techniques to improve mapping on more diverse coastlines including macrotidal coasts will likely lead to improved site specific accuracy we now welcome the broader coastal remote sensing community to continue to build upon this open source software package presented here 5 conclusions this study has presented a new open source tool known as coastsat planetscope that enables users to map shorelines from the growing catalogue of high resolution planetscope cubesat image products that have been operational since 2016 building on the popular coastsat toolbox developed for landsat and sentinel 2 imagery vos et al 2019b the study explored both the optimal shoreline extraction technique needed for these new high resolution image products as well as the potential improvement in shoreline accuracy that could be obtained when including a more advanced elevation correction model robust testing and optimization of this new toolkit was first undertaken at narrabeen collaroy beach in south east australia followed by a blind application at duck north carolina to test its more generic applicability the conclusions of this study can be summarized as follows horizontal shoreline accuracy of planetscope shoreline detection using this toolkit was found to vary depending on detection methodology from a minimum optimal shoreline accuracy of 3 5 m rmse to a lowest accuracy of 5 1 m the optimal shoreline methodologies identified were using either the near infrared minus green band nmg or near infrared minus blue band nmb indexes for distinguishing the land water interface in combination with the weighted peaks thresholding technique the inclusion of time varying wave information in the elevation correction model resulted in negligible difference in shoreline accuracy compared to the elevation correction model comprising only tide variability and a horizontal offset term hence planetscope shoreline detection appears insensitive to time varying wave conditions at narrabeen a relatively steep microtidal beach application of planetscope shoreline detection for an extreme storm event that struck the study site in february 2020 found that this new technique can capture alongshore variability in shoreline change r2 0 86 including the identification of localized erosion hotspots along a coastal stretch this represents a marked improvement compared to lower resolution landsat and sentinel 2 derived shoreline mapping of coastal storm response vos et al 2019b using a generic shoreline elevation model based on a standard wave setup formula the blind application of planetscope shoreline mapping to a contrasting site in the usa duck north carolina found a comparable level of shoreline accuracy rmse 5 1 m while this offers encouragement for the application of this technique at a range of sandy environments globally care should be taken especially for more dissipative environments where wave runup effects in particular are more pronounced e g castelle et al 2021 providing over four years of imagery and a near daily imaging frequency planetscope imagery opens up the possibility to extract shoreline data at sampling frequencies and accuracies previously unattainable from existing satellite derived shoreline capabilities using landsat and sentinel 2 as an expanding fleet of satellites with continual data collection the value of planetscope imagery is only expected to increase into the future as further data is collected it is envisaged that this open source toolkit will help facilitate use of this new dataset and provide a valuable resource to the coastal research community declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to thank planet for enabling free access to the archive of planetscope imagery via a standard research license the narrabeen collaroy coastal monitoring program has been supported by the australian research council discovery and linkage northern beaches council sims foundation and nsw department of planningand environment nsw dpe high resolution topographic data was collected by nashwan matheen chris leaman raimundo ibaceta and tom beuzen wave and tide data at narrabeen collaroy were kindly provided by the manly hydraulics laboratory on behalf of nsw dpe the authors would like to thank the united states army corps of engineers field research facility at duck north carolina for making their data publicly available via the chl thredds data server https chlthredds erdc dren mil thredds catalog frf catalog html 
25522,this study evaluates an emerging capability to monitor high spatial pixel size 3 7 m and temporal daily to sub daily imagery resolution coastal change using planetscope cubesats a new toolkit coastsat planetscope is presented that enables users to map shorelines from planetscope imagery using a workflow of image co registration segmentation thresholding shoreline detection and elevation correction the toolkit is subsequently tested at narrabeen collaroy beach se australia evaluating combinations of shoreline detection indices thresholding approaches and elevation correction an optimal shoreline accuracy of 3 5 m rmse is found for this coastline using the difference between the near infrared and blue bands and a weighted peaks thresholding approach a generic elevation correction model that considers tidal variability and wave setup at the shoreline is then proposed with a growing archive of high resolution imagery planetscope presents enormous potential for enhanced coastline mapping of the coast and complements existing approaches using landsat sentinel 2 imagery keywords coastal monitoring remote sensing extreme storms narrabeen coastsat data availability all in situ shoreline data used is publicly available software availability software name coastsat planetscope developer yarran doherty year first official release 2021 hardware requirements pc system requirements windows linux mac program language python program size 18 mb availability https github com ydoherty coastsat planetscope license gpl 3 0 documentation readme in github repository and guided example in jupyter notebook run times the data presented here was run a 2013 mackbook pro 8 gb ram with a 2 5 ghz dual core intel i5 processor for the 1000 images downloaded each image between 2 and 5 mb the total processing time was approximately 5 h and as follows 20 min for pre processing toa conversion and masking 2 5 h for co registration 35 min for scene merging 1 h for image classification and 50 min for shoreline extraction 1 introduction the two dimensional shoreline position is a fundamental metric in coastal engineering and coastal management boak and turner 2005 luijendijk et al 2018 vousdoukas et al 2020 among other things the shoreline position is a good proxy for the amount of beach sediment stored on the subaerial beach farris and list 2007 the width of the dry beach for both recreational purposes e g silberman and klock 1988 and coastal habitats e g fish et al 2008 and the amount of sand buffer needed for protection against extreme storm events e g harley et al 2009 plant and stockdon 2012 while time series of shoreline position can be obtained using in situ instrumentation emery 1961 moore 2000 mapping the shoreline position is increasingly undertaken using remote sensing splinter et al 2018 this has historically comprised aerial photographs or photogrammetry romine and fletcher 2013 and as technology has progressed extended to other platforms such as ground based video holman et al 2003 pianca et al 2015 uav pucino et al 2021 turner et al 2016a smartphones harley et al 2019 jaud et al 2019 and satellites almeida et al 2021 pardo pascual et al 2012 vos et al 2019b bishop taylor et al 2021 providing global coverage satellites are particularly appealing to monitoring coastline change and satellite derived shoreline or sds mapping has been the focus of recent research e g liu et al 2017 pardo pascual et al 2012 applications range from opportunistic site specific investigations e g ford 2013 to global scale studies of multi decadal shoreline trends e g luijendijk et al 2018 mentaschi et al 2018 the vast majority of studies involving satellite derived shorelines to date have been based on the publicly available landsat and or sentinel 2 data e g almonacid caballer et al 2016 hagenaars et al 2018 pardo pascual et al 2012 2018 mao et al 2021 this has been in part driven by cloud computing resources such as google earth engine gorelick et al 2017 that streamline data access to the vast landsat and sentinel 2 image archive open source tools such as the desktop based python toolkit coastsat vos et al 2019b and web based cassie almeida et al 2021 have subsequently been developed to automate the task of shoreline extraction for a selected coastal location using information from the images stored in the short wave infrared swir and red green blue rgb colour bands validation of these sds tools indicate shoreline accuracy in the range of 10 15 m which through subpixel image processing techniques is typically smaller than the pixel resolution of landsat sentinel 2 imagery 10 30 m because of their multi decadal length and approximately bi weekly sampling frequency these tools are well suited to monitoring intra annual to decadal shoreline variability hagenaars et al 2018 vos et al 2019a in comparison to publicly available satellite data applications using commercial satellites have to date been relatively limited this has been primary due to the inhibitive data cost of commercial satellite imagery but also a result of their patchy data coverage since data collection is primarily tasked to monitor certain locations and their much shorter archive length compared to the multi decadal landsat and sentinel 2 archive belward and skøien 2015 nevertheless examples of commercial satellite imagery applied to the coast include aster dewi 2019 quickbird almonacid caballer et al 2016 spot garcía rubio et al 2015 rapideye duarte et al 2018 ikonos worldview 2 and geoeye 1 ford 2013 one emerging field of coastal monitoring using satellites is that of constellations comprising hundreds of small 10x10x30cm cubesat micro satellites equipped with rgb and infrared sensors of this range of micro satellites the constellation known as planetscope has been an industry leader first launched in march 2016 the planetscope constellation comprises 130 satellites known as doves arranged in a sun synchronous orbit around the earth with a swath size of 24 32 5 km planet 2020 due to their small size cubesat satellites are cheap to manufacture and launch relative to traditional custom built satellites consequently the planetscope fleet is regularly updated via opportunistic third party rocket launches with improved satellite components and sensor upgrades this also results in a temporal variability of available images at specific study sites of interest as more satellites come online and others are discontinued the images used in this study contain three generations of sensors ps2 ps2 sd and psb sd the reader is referred to the planet documentations for full details and additional sensor upgrades planet 2020 a summary of planetscope history and imaging capabilities relative to landsat and sentinel 2 missions is presented in fig 1 compared to the monthly to bi weekly temporal resolution of landsat and sentinel 2 images the planetscope dove constellation is capable of sampling at daily to sub daily resolution fig 1b additionally the pixel size of planetscope is 3 7m at nadir resampled to a 3 m fixed resolution which is significantly greater than the 10 30 m resolution of landsat and sentinel 2 imagery fig 1c however the current generation of planetscope images do not contain the swir band that is commonly used in mapping satellite derived shorelines as such the tools developed for landsat and sentinel images cannot be directly applied to this new and high resolution data source if other image bands such as rgb and near infrared can be shown to accurately capture the shoreline position this opens up the opportunity for much higher resolution shoreline analysis kelly and gontz 2020 with a growing image archive the high spatial resolution and frequent revisit period of planetscope imagery has the potential to provide both long term and discrete storm event monitoring at a consistent temporal resolution that has previously been unattainable outside of fixed remote sensing shoreline monitoring stations such as video camera networks bracs et al 2016 harley et al 2019 holman and stanley 2007 notably this technology has the potential to do so for any coastline on earth every day however more information is needed into the best methodology to extract shoreline features from these images this paper presents a new open source toolkit coastsat planetscope that builds on the popular coastsat package of vos et al 2019b and is described in section 2 the coastsat package was originally developed and optimized for landsat and sentinel 2 imagery using the google earth engine api and has since been applied at many locations worldwide including europe castelle et al 2021 australia cuttler et al 2020 africa lawson et al 2021 and asia adebisi et al 2021 however the application of satellite shoreline detection to planetscope imagery requires adaptation of the original coastsat algorithm to address some new data challenges as well as potential improvements associated with the increased accuracy and frequency of the planetscope products first the reduced number of spectral bands provided by planetscope cubesats specifically the lack of a short wave infrared band means that the modified normalized difference water index mndwi refer xu 2006 that underpins coastsat cannot be utilized for shoreline detection as such this work focuses on optimizing a new shoreline extraction algorithm based on the available planetscope bands of red green blue and near infrared nir second as the spatial resolution of the planetscope imagery is significantly higher than that of landsat and sentinel 2 the potential improvement in shoreline accuracy when correcting for both tides and waves as opposed to just tides in the original coastsat algorithm is also explored application of this new coastsat planetscope toolbox is validated at narrabeen collaroy beach in south east australia with a high quality multidecadal shoreline monitoring program well established at the site turner et al 2016b the narrabeen collaroy dataset includes monthly surveys along five cross shore transects with additional surveys performed before and after major erosive events a storm erosion case study investigates the ability of planetscope derived shorelines to monitor alongshore variable erosion during discrete storm events lastly a blind validation of the toolkit is performed at duck north carolina usa using a generalized water level correction method 2 toolkit overview and testing the overall workflow for the coastsat planetscope toolkit is outlined in fig 2 and comprises several key steps from raw image data to shoreline products 1 planetscope image pre processing 2 image band extraction and thresholding and 3 shoreline extraction and elevation correction briefly once the user has defined an area of interest in step 1 raw digital number planetscope images rbg nir are pre processed to top of atmosphere toa cropped and merged to the area of interest co registered and classified with a pre trained supervised machine learning neural network algorithm in step 2 the 4 bands rgb nir are used to calculate a water index e g ndwi red minus blue from which the shoreline is extracted with the marching squares contouring technique based on an image specific threshold finally the 2d shorelines are intersected with shore normal transects and the water level correction is applied along each transect a more detailed description of the workflow is provided in sections 2 2 and 2 3 below algorithm testing and optimization were undertaken using in situ shoreline measurements from the narrabeen collaroy coastal monitoring site hereafter narrabeen a dynamic sandy beach located in south east australia where continuous beach measurements have been undertaken since 1976 turner et al 2016b a total of 24 combinations of indices thresholding methods and elevation models are tested to obtain an optimal combination that is subsequently adopted in the toolkit the application of the toolkit at narrabeen provides a useful demonstration for the user and a step by step workthrough is provided in an accompanying jupyter notebook 2 1 narrabeen testing site narrabeen is located on the northern beaches of sydney australia fig 3 and is one of the longest continuously monitored beaches worldwide turner et al 2016b its open access dataset is used as a testbed for a range of coastal remote sensing applications including shorelines derived from landsat sentinel 2 liu et al 2017 luijendijk et al 2018 vos et al 2019a coastal imaging harley et al 2011 and smartphones harley et al 2019 harley and kinsela 2022 at approximately 3 6 km in length the narrabeen embayment is bounded to the north by narrabeen head and to the south by long reef point narrabeen exhibits a microtidal tide regime with a mean spring range of 1 3 m the deep water wave climate is primarily composed of sse swell waves with a mean significant wave height hs of 1 6 m a mean peak wave period tp of 10 s and a storm wave height defined by hs 3m corresponding to the 5 exceedance of hs nearshore wave conditions vary significantly alongshore due to sheltering from long reef point of the predominantly southerly waves with the northern end characteristically more exposed to the persistent southerly waves and southern end relatively sheltered turner et al 2016b the beach is primarily composed of fine to medium grained quartz sand with a d50 of approximately 0 3 mm the intertidal beach slope varies significantly through time with typical variability between 0 07 and 0 13 and an average intertidal slope of 0 09 vos et al 2020 the shoreline varies at weekly to monthly timescales due to the frequent passing of storms and subsequent beach recovery phillips et al 2017 in addition annual and interannual variability on the order of 50m is also visible over the 40 years of monitoring turner et al 2016b for the purpose of algorithm testing in situ survey data at narrabeen were utilized for the time period july 2016 coinciding when planetscope imagery first became available at the site to august 2020 these measurements comprise 90 individual in situ survey dates at an average interval of 16 days in situ surveys were conducted using rtk gnss vertical accuracy 0 05 m horizontal accuracy 0 03 m at the five historical cross shore transects that have been measured continuously since 1976 profiles pf1 pf2 pf4 pf6 and pf8 fig 1 following previous studies e g harley et al 2011 cross shore shoreline positions were defined at these transects using the intersection of the surveyed profile with mean high water springs mhws which corresponds at this site to an elevation of 0 7 m above mean sea level amsl the mhws elevation has been shown to be a suitable reference datum for mapping shoreline variability at this site as it minimizes the influence of ephemeral low tide features such as low tide terraces and swash bars on shoreline variability phillips et al 2017 in addition to the routine surveying high resolution 50 100 m transect spacing survey data was also collected using rtk gnss for a significant storm event that occurred between 8 and 10 february 2020 this storm was characterised by waves over 3 m hs from an ese direction for a duration of 39 h reaching a peak hs of 6 5 m on 9 february high resolution rapid response survey data were collected both two days prior to the storm 6 february 2020 and four days after storm conditions subsided 14 february 2020 these additional high resolution data enabled to evaluate the capacity of planetscope derived shorelines to capture alongshore variability in erosion following a large storm 2 2 planetscope image pre processing imagery supplied by planet from the planetscope satellite constellation is distributed into three data products all based on raw sensor data captured at an average ground sample distance of 3 7 m at nadir planet 2020 the four band red green blue and near infrared analytic ortho scene product is specifically used for shoreline mapping and tested in this study this product orthorectifies and resamples image pixel data from a 3 7 m pixel resolution at nadir to a consistent 3 m pixel resolution with data provided as digital number dn values that correspond to at sensor detected radiance to reduce scene to scene variability and enable direct comparison between images chander et al 2009 dn values were converted in this study to top of atmosphere toa reflectance via planetscope provided conversion factors an accompanying unusable data mask udm file is also provided for each image to aid in the detection and filtering of sensor errors data loss and regions of cloud cover the orthorectification accuracy of raw planetscope analytic ortho scenes is stated at less than 10 m planet 2020 further independent studies have verified this accuracy with observed horizontal rmse values between 3 4 m and 5 2 m dobrinić et al 2018 lemajic et al 2017 to reduce geolocation error prior to shoreline mapping all individual downloaded images are co registered an image co registration step is performed in the toolkit using the open source arosics library scheffler et al 2017 briefly this co registration software uses phase correlation in the frequency domain to automatically detect and correct sub pixel misalignments between a reference image and a set of target images rmse of 0 3 pixels the reference image is chosen by the user from the available images where the image is focussed cloud free and a shoreline can be seen clearly following this co registration step overlapping scenes from the same sensor on the same day are snapshots from a flyover and are merged and treated as one scene to reduce unnecessary data and processing times an area of interest aoi is subsequently defined by the user to crop the images after merging and cropping images image pixels in the ortho scene then undergo an image classification where one of four separate classes are identified sand water whitewater and other where other refers to various features such as buildings roads vegetation etc this image classification is undertaken using a neural network multilayer perceptron in the open source python toolkit scikit learn pedregosa et al 2011 in line with the methodology presented in coastsat vos et al 2019b a total of 572 planetscope analytic ortho scenes at narrabeen spanning 457 unique dates were downloaded for the july 2016 to august 2020 testing period these 572 images were selected based on a cloud cover threshold over the entire scene of 50 or less which from preliminary testing was found to remove the majority of unusable cloud impacted scenes at the site as the planetscope constellation has a sun synchronous orbit the vast majority of images occurred within an hour of the median local solar image timestamp at narrabeen 9 30 a m due to the small fleet size in 2016 it was noted that image recurrence intervals were considerably more sporadic at the start of the study period with a maximum time interval of 111 days recorded as shown in fig 1a image recurrence increased significantly to near daily from mid 2017 onwards with the launch of additional dove satellite fleets training data for the image classification neural network was based on a subset of 25 images these were manually digitized to include approximately 25 000 pixels for each of the image classes except the whitewater class which only had 500 pixels the classification accuracy was assessed by means of a 10 fold cross validation which determined a pixel classification accuracy of 99 5 2 2 1 shoreline detection algorithm with each scene corrected to the top of atmosphere masked co registered and classified a generalized shoreline detection algorithm is then used to identify the two dimensional shoreline position this shoreline detection algorithm first creates a greyscale image based on a certain index tested below with a particular focus on the water and sand classes identified from the image classification step above next a thresholding technique is applied to find the pixel value that delineates between water and sand pixels finally a sub pixel marching squares contouring algorithm cipolletti et al 2012 is applied based on the identified threshold to identify the two dimensional shoreline a total of six different spectral indices were tested at narrabeen using combinations of the four bands available in the planetscope analytic ortho scene these indices were based on both absolute and normalized values of common water indices used in remote sensing applications e g the normalized difference water index mcfeeters 1996 as well as indices found suitable specifically for shoreline mapping based on colour bands only e g red minus blue harley et al 2019 these six indices are defined below 1 r m b r e d b l u e 2 r m b n o r m r e d b l u e r e d b l u e 3 n m g n i r g r e e n 4 n d w i n i r g r e e n n i r g r e e n 5 n m b n i r b l u e 6 n m b n o r m n i r b l u e n i r b l u e where red blue green and nir correspond to the toa values of each spectral band for each of these indices two localized thresholding algorithms were investigated to determine the optimum threshold index opt for shoreline detection between sand and water pixels as classified by the neural network described above the two algorithms tested were the commonly used otsu thresholding technique otsu 1979 as well as an alternate weighted peaks wp thresholding approach the latter approach was developed by harley et al 2019 and calculated based on the location of the peaks in the probability density function pdf of the sand and water pixel index values in contrast to the common otsu thresholding which maximises interclass variance the wp thresholding algorithm weights the optimized index threshold towards the sand peak which has been previously found to be robust for shoreline mapping purposes at a range of coastal locations worldwide plant et al 2007 harley et al 2019 the optimum index based on wp thresholding is calculated as follows 7 i n d e x o p t w p x ˆ w a t e r 0 7 x ˆ s a n d x ˆ w a t e r where x ˆ w a t e r and x ˆ s a n d are the x location of the peaks in the pdf of the water and sand pixels respectively following identification of the optimum threshold the shoreline is identified along pixels of this optimum index value by means of a marching squares contouring algorithm cipolletti et al 2012 that determines a 2d contour line via linear interpolation of adjacent cells values an example of the entire shoreline detection process from image classification indexing and thresholding for the specific case of the nmb index with wp thresholding is shown in fig 4 2 3 elevation correction model the following section outlines more advanced techniques that can be applied to further correct the extracted satellite derived shoreline time series described above if the user desires these techniques require various additional data sources that may include a time series of ocean water level elevations intertidal beach slopes and wave characteristics astronomical tides or observed ocean water levels may be extracted from various sources at the user s discretion the intertidal beach slope may be derived from in situ data e g beach profiles or from remote sensing algorithms such as coastsat slope vos et al 2020 local wave time series wave height period may be provided from local in situ wave buoys or from wave hindcasts reanalysis since satellite imagery represent an instantaneous snapshot of the coast repeat shoreline measurements inherently include high frequency i e sub daily oscillations associated with rising falling ocean water levels tides non tidal anomalies swash motions wave setup and runup as well as other factors changing lighting conditions beach morphology granulometry etc previous approaches to reduce this inherent noise have been to either map satellite derived shorelines on composite images averaged over large i e annual time windows almeida et al 2021 bishop taylor et al 2021 liu et al 2017 luijendijk et al 2018 or apply an elevation correction model to correct for reduce these high frequency effects castelle et al 2021 pardo pascual et al 2018 vos et al 2019a based on lower resolution 10 m landsat and sentinel 2 images a simple tidal correction δ x t i d e has been previously applied to project the cross shore shoreline position detected by the satellite xsat to a consistent elevation based on a specific elevation datum xdatum 8 x d a t u m t x s a t t δ x c o r r t 9 δ x c o r r t z t i d e t z d a t u m m simple tidal correction model where ztide t is the ocean water level elevation at the time of image capture taken from a nearby tide gauge zdatum is the reference elevation datum to define the shoreline and m is the intertidal beach slope recognizing the potential for improved shoreline accuracy from high resolution planetscope imagery a more advanced elevation correction model was explored in this study where tide or ocean water levels intertidal beach slope and local wave conditions were known as shown in fig 5 the horizontal difference δxcorr between the detected shoreline position x sat measured in reference to the landward benchmark and the datum based shoreline can be expressed as the linear sum of tidal effects δxtide wave effects δxwave and systematic biases due to the satellite detection method as well as any other remaining factors δxdet 10 δ x c o r r t δ x t i d e t δ x w a v e t δ x d e t t following eq 3 in harley et al 2011 this is expressed by the empirical elevation correction model 11 δ x c o r r t α z t i d e t z d a t u m m β h 0 t l 0 t c where α β and c are empirically derived parameters obtained from multiple linear regression see below h 0 is the deep water significant wave height and l 0 the deep water wavelength derived from linear wave theory note that where the coefficient of the simple tidal correction model eq 9 used in previous studies is assumed to be equal to unity i e α 1 representing the simple case of a planar beach face and no tidal modulations of other processes here this coefficient is allowed to vary to potentially take into account other tidal effects time varying wave runup setup processes are incorporated in this elevation model via the second term in eq 11 which is derived from numerous lab and field observations holman et al 2003 hunt 1958 stockdon et al 2006 that show that these processes scale well by h 0 l 0 similar to α here β is allowed to freely vary and does not take on any predefined value e g 0 35 for stockdon et al 2006 the third term c represents any remaining systematic offsets positive c corresponding to a landward bias which is shown later can also include time averaged wave runup setup processes that are not accounted for in the second term the three parameters in eq 11 were solved at each of the five transects at narrabeen using a least squares multiple linear regression based on the horizontal difference between the detected shoreline position x sat and the measured shoreline position at the reference datum in this case the 0 7m elevation corresponding to mhws obtained from rtk gnss surveys deep water wave data were obtained from the sydney waverider buoy located 11 km offshore in 80 m water depth and ocean water level data obtained from a nearby tide gauge hmas penguin intertidal beach slopes m were calculated based on average values for each transect derived from the rtk gnss surveys over the 2016 2020 study period these equated to m 0 11 pf1 pf2 pf4 m 0 13 pf6 and m 0 12 pf8 while a time invariant beach slope was chosen in this example as in most cases worldwide where beach slope is not well resolved it is acknowledged that a user may also wish to use a time varying slope term if the data is available finally to test whether incorporating time varying wave data improves the accuracy of planetscope shorelines at the validation site an elevation model that removed the second term in eq 11 was also tested 12 δ x c o r r t α z t i d e t z d a t u m m c the two parameters of this model α and c were calculated using the same multiple linear regression process above 3 testing results 3 1 optimal shoreline extraction methodology to determine the optimal shoreline extraction methodology for the narrabeen validation site planetscope derived shoreline positions were calculated at each of the five transects and compared with in situ data for all proposed extraction methodologies described above taking into account the six different indices tested refer eqs 1 6 two thresholding algorithms otsu and weighted peaks thresholding and two elevation detection models refer eqs 11 and 12 this amounted to 24 unique combinations shoreline results at all five transects were then combined to provide an overall summary of the residual error based on the cross shore rmse between planetscope derived and measured shoreline positions at the reference datum 0 7 m amsl table 1 summarizes results for all 24 survey extraction methodologies planetscope derived shoreline accuracies were found to range from an rmse of 3 5 m 5 1 m as discussed later in section 4 3 this represents a substantial improvement in shoreline accuracy compared to equivalent efforts using landsat and sentinel 2 imagery at this site based on the coastsat toolbox rmse 8 2 m vos et al 2019a four of these methodologies were found to have equally optimal accuracy with each indicating a cross shore rmse between planetscope derived and measured shoreline positions of 3 5 m these four optimal methodologies were using the near infrared minus green nmg eq 3 or near infrared minus blue nmb eq 6 indices in combination with the wp thresholding and either of the two elevation models see shoreline extraction method 6 8 14 and 16 in table 1 the lowest performing methodologies in terms of rmse meanwhile were the normalized red minus blue rmb norm index eq 2 in combination with otsu thresholding and either of the two elevation correction models see shoreline extraction method 21 and 23 table 1 comparing the results for each of the shoreline extraction methods the analyses indicate a moderate sensitivity to the thresholding approach and index type but a negligible sensitivity between the two elevation models tested eq 11 vs eq 12 in terms of the thresholding approach a 5 45 improvement in shoreline accuracy was found for each index when using the weighted peaks thresholding over the otsu thresholding approach the index meanwhile was particularly sensitive to whether normalizing was applied with non normalized indices i e nmb rmb and nmg providing noticeably improved shoreline performance average rmse 3 7 m to those with normalizing applied i e ndwi nmb norm and rmb norm average rmse 4 4 m regarding the elevation correction model the addition of a time varying wave runup and setup term to the shoreline elevation model eq 11 only resulted in a 1 improvement in shoreline accuracy compared to the model with only ocean water level variability and a constant eq 12 this suggests that at the steep beach morphology tested here time varying wave information such as that sourced from a nearby wave gauge might not necessarily be needed to improve shoreline accuracy discussed in more detail in section 4 1 since four combinations were found to have equally optimal performance only the nmb wp tide only model methodology method 8 was subsequently selected for further analysis below fig 6 presents the corresponding time series of shoreline change at each of the five transects in comparison to in situ measurements using method 8 overall there is a strong agreement between the in situ and planescope derived shorelines with the planetscope derived shorelines describing between 86 and 91 of shoreline variation over the four year time period this includes seasonal cycles of erosion and accretion high frequency i e monthly shoreline variability as well as episodic storm erosion and recovery such as that observed in the february 2020 storm at the end of the time series see section 3 3 below note that profile pf6 is backed by a seawall cross shore location shown in fig 6 by red dash line and shoreline points were subsequently truncated landward of this mark on a transect by transect basis the rmse using this optimal methodology was found to vary from a minimum rmse of 2 9 m pf8 to a maximum rmse of 4 2 m pf1 these minimum and maximum errors coincide with the most sheltered and most exposed parts of the embayment in terms of wave energy which suggests that error is enhanced by increased individual swash motions at the more exposed location pf1 and reduced in regions of lower wave activity pf8 3 2 elevation correction model parameters the results above were based on optimized parameters of the two elevation correction models tested using multiple linear regression of the shoreline data combined from all five beach transects these parameters were the tidal correction model coefficient α the time varying wave runup setup coefficient β and the constant c accounting for any remaining systematic offsets as summarized in table 1 across all 24 survey extraction methodologies the tidal correction model coefficient was found to be close to unity α 0 9 to 1 07 suggesting that the simple planar profile is a suitable approximation for tidal corrections at this site for the 12 methodologies that also included the additional time varying wave runup setup term the coefficient β was found to vary from 0 01 methods 1 5 6 9 and 14 to 0 07 method 2 the constant c meanwhile resulted in a wide range of optimized values from negative values that represent a systematically more seawards horizontal offset c 3 8 m method 1 to positive values characterising a systematically more landwards offset c 5 3 m method 12 generally there was little variability in the constant c between the tide and wave model and the tide only model for each index and thresholding average difference between the two optimized c values 0 3 m for the selected method 8 a tidal correction model coefficient of α 0 95 and a constant horizontal offset c 5 2 m was found 3 3 performance of planetscope derived shorelines on individual extreme storm response in addition to evaluating the accuracy of planetscope imagery in obtaining time series of shoreline change e g fig 6 a key capability explored in this study was the ability of planetscope derived shorelines to map alongshore variability in shoreline change following an extreme episodic storm event fig 6 presents comparisons between pre and post storm shoreline positions derived from both planetscope imagery using the optimal shoreline extraction methodology identified above and rtk gnss survey data for the large storm event that struck the coastline at narrabeen between 9 and 10 february 2020 these comparisons were undertaken for both the individual pre and post storm shoreline positions fig 7 a as well as for the alongshore variability in shoreline change resulting from the storm fig 7b planetscope imagery for shoreline mapping was selected five days prior to storm and just two days after the storm based on the nearest available data spanning the storm event overall a strong agreement was found between individual pre and post storm shorelines r2 0 98 and 0 93 rmse 2 4 m and 3 5 m respectively derived from planetscope and in situ rtk gnss shoreline measurements at the reference datum examining the difference between pre and post storm shorelines fig 7b presents the change in shoreline due to the storm as determined by planetscope shorelines and in situ measurements respectively this figure indicates significant alongshore variability in storm erosion with some hotspots local maxima of shoreline erosion observed at 600 m alongshore measured shoreline change 34 m and 1800 m alongshore measured shoreline change 30 m a general trend of enhanced shoreline retreat at the northern end 2000 m alongshore compared to the southern end 2000 m alongshore is also evident in the rtk gnss measurements the corresponding planetscope derived shoreline change measurements also show good agreement with these general trends and local maxima such that the overall rmse is 3 66 m and r2 0 86 the alongshore averaged shoreline change for the entire 3 6 km beach was also estimated accurately by the planetscope data with 18 m of shoreline change for the planetscope derived shorelines and 19 m for the rtk gnss survey data 4 discussion 4 1 optimized elevation correction model parameters optimization of the model parameters used to correct for elevation differences between the instantaneous planetscope derived shorelines and the reference datum provide crucial insights into the shoreline detection performance regarding the tidal correction term the model coefficient α was found to be roughly equal to the value of unity typically assigned for tidal correction based on the assumption of a planar beach face and no tidal modulation of elevation differences castelle et al 2021 vos et al 2019a this suggests that these assumptions are valid for the narrabeen collaroy site with small differences likely due to subtle beach slope m changes over time that are not accounted for in the satellite derived shoreline elevation correction model tidal correction based on a time varying beach slope was also trialled although this resulted in a reduction in shoreline accuracy relative to time averaged values for the wave runup setup term comparisons between optimized models both including and excluding time varying wave runup and setup found negligible differences in terms of planetscope derived shoreline accuracy this result suggests that time varying wave setup and runup is not an important consideration in planetscope shoreline mapping at the narrabeen site the practical implication of this finding is that it suggests there might be little advantage in terms of shoreline accuracy in including time series of wave variability which is often difficult to obtain without ready access to in situ wave measurements from a nearby wave buoy it should be noted that this result is likely to be site specific and related to the steep and microtidal nature of the narrabeen site for example in southwest france castelle et al 2021 found using lower resolution landsat imagery that shoreline accuracy was improved by a factor of two when a time varying wave runup equation was included in the elevation correction model this was attributed to the low beach face gradient m 0 05 and energetic and highly seasonal wave conditions at this site average hs 1 1 m 2 4 m in summer winter respectively such conditions can result in much larger horizontal variations in wave runup relative to steeper beaches with less seasonally varying wave climates like narrabeen further validation of this new algorithm across a range of sandy beach environments is needed to explore the robustness of this result the influences of wave runup and setup processes on the planetscope derived shoreline position at narrabeen instead appear to be captured within the constant offset term c fig 8 illustrates the alongshore variability in both α and c for each transect along the beach using the method 8 combination comprising the nmb index the weighted peaks thresholding and the tide only elevation correction model a distinct gradient in the offset term fig 8b is indicated that appears similar in shape to the variability of wave exposure along the beach with c 4 0 m at the most sheltered profile pf8 and c 5 5 m 6 2 m for the three northern profiles exposed to the predominantly sse waves of the region pf1 pf2 and pf4 to further explore alongshore variability in wave runup and setup effects the optimized c term at each transect was subsequently compared to that estimated from a standard wave setup equation at the shoreline stockdon et al 2006 eq 10 this is expressed horizontally as 13 η m 0 35 h 0 l 0 where η m is the estimated horizontal offset by wave setup at the shoreline using equation 13 the average time invariant horizontal offset by wave setup was estimated at each transect by first transforming average offshore wave conditions at narrabeen h0 1 6 m tp 10 s θ 135 to the 10 m depth contour using a nearshore wave transformation tool based on 1000s of swan model runs refer to turner et al 2016b since equation 13 is based on deep water conditions this wave data at the 10 m depth contour was then reverse shoaled to give the equivalent deep water wave conditions i e h0 and l0 at each transect from which the wave setup was calculated estimates of η are shown as crosses in fig 8b and reveal a reasonable agreement r2 0 60 between this offset term and that estimated using the wave setup formulation deviations between the optimized c value and the estimated setup also only range from 0 1 m pf6 to 1 5 m pf2 suggesting that this systematic offset can be estimated using this commonly used wave setup formulation 4 2 blind application of generalized elevation model at duck usa the above results are based on optimized model coefficients derived from extensive in situ beach measurements which are rare and only available at a small subset of sites worldwide turner et al 2016a to test the ability of coastsat planetscope to map shoreline variability at an unseen coastal location the toolkit was applied blindly to planetscope imagery covering the u s army corps of engineers field research facility at duck north carolina usa zhang and larson 2021 the duck field research facility is an internationally recognized coastal observatory located on the northern outer banks a low lying barrier island system the beach slope at duck is similar to that of narrabeen m 0 10 and has a similar microtidal and semidiurnal tide range mean spring tidal range 1 1 m wave conditions are typically less energetic relative to narrabeen mean hs 1 0 m tp 9 s and interrupted by storm events defined by a threshold hs 2m from hurricanes and atlantic nor easters a 1 2 km stretch of coastline centred on a research pier has been monitored extensively at duck since 1977 using a combination of survey techniques o dea et al 2019 pianca et al 2015 here the blind application is focused on four cross shore transects defined by the local alongshore reference system oriented roughly from sse to nnw pf 91 m pf 320 m pf 731 m and pf 1097 m refer zhang and larson 2021 a total of 166 planetscope images were sourced at the duck site over the period november 2017 to august 2020 equivalent to an average interval of one image every 6 days following the results above shoreline detection was undertaken using the method 8 combination as above two alternate steps were subsequently taken in order to make this application at duck truly blind and not rely on any in situ beach measurements to estimate the beach slope m needed for the elevation correction model eq 12 the method of vos et al 2020 was applied to the planetscope derived shorelines this algorithm estimates the beach slope by iteratively searching the optimum m value that minimizes short term shoreline variability in the frequency band of tidal constituents identified in the shoreline time series validation of this technique including at both the duck and narrabeen sites has found that the beach slope can be estimated to within 0 01 the application of this step identified beach slopes of 0 08 0 11 for the four duck profiles the second step comprised the use of a generalized elevation correction model rather than relying on site specific optimized model coefficients as undertaken at narrabeen based on the findings above that the time varying wave runup and setup term may have little practical benefit at some sites and that the constant offset c can be approximated using the stockdon et al 2006 formulation the following generalized model was adopted 14 δ x c o r r t z t i d e t z d a t u m m 0 35 h 0 l 0 tide conditions at the time of image capture were taken from the nearby tide gauge at the field research facility fig 9 presents time series of planetscope derived shorelines relative to in situ survey measurements at the four cross shore profiles at duck using the same reference datum used at narrabeen shoreline accuracy using this blind application is shown to vary from a minimum rmse of 5 1 m pf 320 m to 6 4 m pf 1097 this represents a slight decrease in shoreline accuracy relative to narrabeen using site specific optimized model coefficients but is more accurate than using lower resolution landsat or sentinel 2 imagery at duck with the coastsat algorithm rmse 9 0 vos et al 2019a the increase in accuracy obtained by planetscope imagery is subsequently sufficient to capture smaller scale variability at the sub annual or seasonal time scales particularly considering the generally smaller magnitude of shoreline variability at duck relative to narrabeen this was identified as a key limitation of applying coastsat and landsat sentinel 2 at less dynamic sites vos et al 2019a also evident in this time series is a significant artificial beach nourishment works that was undertaken at the northern end of the site refer profile pf 1097 fig 7 in may june 2017 the high frequency imagery of the planetscope constellation meant that the shoreline both immediately before and immediately after the nourishment works could be mapped enabling accurate quantification of the shoreline response to this engineering intervention a corresponding 60 m increase in shoreline position is observed in this northern time series which is followed by a gradual reduction as sediment is moved alongshore further south at profile pf 731 the shoreline is observed to increase approximately 30 m at a lag of approximately three months following the nourishment 4 3 benefits and challenges of planetscope imagery for shoreline mapping the narrabeen beach monitoring site has been used as a testbed for a range of advanced coastal remote sensing applications splinter et al 2018 thereby making it possible to compare the overall performance of planetscope derived shoreline detection to other optical remote sensing applications table 2 summarizes results of previous shoreline accuracy tests applied at narrabeen including satellite derived methods luijendijk et al 2018 vos et al 2019a this study land based video imaging using the argus coastal imaging system harley et al 2011 and a citizen science based approach using consumer smartphones known as coastsnap harley et al 2019 harley and kinsela 2022 these different approaches span a range of image pixel resolutions depending on the approach and sensor used from 30 m for older landsat 5 satellite missions and no panchromatic sharpening to 0 1 m pixels for land based argus video imaging in the image nearfield note that whereas satellite imagery is typically close to nadir and hence have negligible resolution variability over the spatial scale of individual beaches kilometers land based approaches have significant variability in pixel resolution alongshore due to their low angle of obliquity holman and stanley 2007 as expected a general trend of increased shoreline accuracy with pixel resolution is observed in table 2 with the planetscope derived shorelines significantly more accurate than landsat and sentinel 2 derived approaches horizontal accuracy 7 4 m 13 7 m but less accurate than land based methods using stationary video cameras or smartphones minimum horizontal accuracy 1 2 m for argus video imaging this suggests that further improvements could be made in satellite derived shoreline accuracy with the addition of other very high resolution vhr satellite missions such as the maxar worldview 3 satellites launched in 2014 pixel resolution 0 31 m turner et al 2021 however as shown in this study the shoreline accuracy reported here 3 5 m is commensurate to the pixel resolution of the satellite image 3 7 m which contrasts with past work using coarser satellite imagery and sub pixel techniques this may suggest a limit in the pixel resolution substantially reducing the overall shoreline detection error with the remaining error attributed to unresolved processes such as individual swash motions additionally an important consideration with regards to accessing this vhr image data including planetscope is that it is presently limited to either restricted research licenses organisations countries with data sharing agreements or cost prohibitive pay per image funding models for instance the planetscope imagery obtained in this study was made available through planet s education and research program that provides a monthly download quota of 5000 km2 to university affiliated students and researchers for the 4 years of data used in this study the study area required up to 2 images per scene to cover the extents of narrabeen collaroy and resulted in roughly 1000 raw images being downloaded and was covered within a monthly download limit for longer timescales and larger study areas as well as more frequent passes of the planet constellation longer download times may be needed to meet the quota restrictions this provides a major barrier for uptake of planetscope and other vhr imagery for routine shoreline monitoring and makes publicly available imagery like landsat and sentinel 2 or land based approaches still appealing another challenge of satellite derived shorelines approaches such as planetscope is their present reliance on instantaneous snapshots of the coast from single satellite passes the instantaneous sampling of what is typically a highly dynamic and turbulent land water interface particularly on wave dominated coastlines like narrabeen introduces error into the shoreline detection as high frequency swash motions are difficult to resolve using simple elevation correction models such as that proposed in eqs 11 and 12 these errors are typically reduced in land based approaches using time averaging where video frames over a certain time period nominally 10 min are averaged to smooth out these high frequency motions for example harley et al 2019 compared shorelines mapped by both instantaneous images and time averaged images averaged over 10 min and observed an 18 improvement in shoreline accuracy although not explored in this study the growing constellation of dove cubesats can result in multiple satellite passes over a single day fig 1 this sub daily shoreline data could potentially be exploited as a rudimentary means of smoothing shoreline oscillations and reduce shoreline noise along with routine shoreline mapping over many years the findings of this study highlight the benefit of planetscope imagery for rapid quantification of shoreline response due to extreme storm events with an approximate median image recurrence of 2 days fig 1 planetscope imagery is able to capture pending cloud coverage constraints the shoreline position both immediately prior to and following an extreme storm event striking the coast as shown in fig 7 for a storm event that occurred in february 2020 planetscope derived shorelines were able to quantify not only the average shoreline change caused by the event but significant detail regarding the alongshore variability in erosion as well as localized erosion hotpots at the spatial scale of approximately 100 m the capability of planetscope imagery for rapid shoreline assessment was also investigated by kelly and gontz 2020 who applied planetscope derived shorelines to assess the impact of a severe tropical cyclone cyclone oma that struck eastern australia in february 2019 horizontal deviations between planetscope and measured shorelines in their study obtained from a lidar survey were found to range between 0 m and 22 m with a mean offset equivalent to the parameter c in the present study of 9 m while the horizontal rmse was not reported in kelly and gontz 2020 the seemingly lower accuracy range compared to the present study is likely due to their study being based on a single validation survey and planetscope shorelines were manually digitized rather than using the more robust method detailed here as highlighted in table 1 the results show that significant improvements in shoreline accuracy can be obtained using an optimized combination of shoreline index thresholding approach and elevation correction model and that these factors should be considered for accurate shoreline mapping using planetscope imagery as is the case with other satellite remote sensing toolkits e g coastsat site specific calibration of the classifier and additional techniques to improve mapping on more diverse coastlines including macrotidal coasts will likely lead to improved site specific accuracy we now welcome the broader coastal remote sensing community to continue to build upon this open source software package presented here 5 conclusions this study has presented a new open source tool known as coastsat planetscope that enables users to map shorelines from the growing catalogue of high resolution planetscope cubesat image products that have been operational since 2016 building on the popular coastsat toolbox developed for landsat and sentinel 2 imagery vos et al 2019b the study explored both the optimal shoreline extraction technique needed for these new high resolution image products as well as the potential improvement in shoreline accuracy that could be obtained when including a more advanced elevation correction model robust testing and optimization of this new toolkit was first undertaken at narrabeen collaroy beach in south east australia followed by a blind application at duck north carolina to test its more generic applicability the conclusions of this study can be summarized as follows horizontal shoreline accuracy of planetscope shoreline detection using this toolkit was found to vary depending on detection methodology from a minimum optimal shoreline accuracy of 3 5 m rmse to a lowest accuracy of 5 1 m the optimal shoreline methodologies identified were using either the near infrared minus green band nmg or near infrared minus blue band nmb indexes for distinguishing the land water interface in combination with the weighted peaks thresholding technique the inclusion of time varying wave information in the elevation correction model resulted in negligible difference in shoreline accuracy compared to the elevation correction model comprising only tide variability and a horizontal offset term hence planetscope shoreline detection appears insensitive to time varying wave conditions at narrabeen a relatively steep microtidal beach application of planetscope shoreline detection for an extreme storm event that struck the study site in february 2020 found that this new technique can capture alongshore variability in shoreline change r2 0 86 including the identification of localized erosion hotspots along a coastal stretch this represents a marked improvement compared to lower resolution landsat and sentinel 2 derived shoreline mapping of coastal storm response vos et al 2019b using a generic shoreline elevation model based on a standard wave setup formula the blind application of planetscope shoreline mapping to a contrasting site in the usa duck north carolina found a comparable level of shoreline accuracy rmse 5 1 m while this offers encouragement for the application of this technique at a range of sandy environments globally care should be taken especially for more dissipative environments where wave runup effects in particular are more pronounced e g castelle et al 2021 providing over four years of imagery and a near daily imaging frequency planetscope imagery opens up the possibility to extract shoreline data at sampling frequencies and accuracies previously unattainable from existing satellite derived shoreline capabilities using landsat and sentinel 2 as an expanding fleet of satellites with continual data collection the value of planetscope imagery is only expected to increase into the future as further data is collected it is envisaged that this open source toolkit will help facilitate use of this new dataset and provide a valuable resource to the coastal research community declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to thank planet for enabling free access to the archive of planetscope imagery via a standard research license the narrabeen collaroy coastal monitoring program has been supported by the australian research council discovery and linkage northern beaches council sims foundation and nsw department of planningand environment nsw dpe high resolution topographic data was collected by nashwan matheen chris leaman raimundo ibaceta and tom beuzen wave and tide data at narrabeen collaroy were kindly provided by the manly hydraulics laboratory on behalf of nsw dpe the authors would like to thank the united states army corps of engineers field research facility at duck north carolina for making their data publicly available via the chl thredds data server https chlthredds erdc dren mil thredds catalog frf catalog html 
25523,in this study we evaluated the use of synthetic aperture radar sar and multispectral data to detect aquaculture waterbodies in southern bangladesh to quantify fish production on a national scale for this purpose we developed an object based framework comprised of three sequential stages 1 water detection 2 feature segmentation and 3 feature classification techniques such as edge otsu for binary thresholding edge detection with convolution filters and various supervised and unsupervised machine learning methods were used as part of a workflow we found that ensemble products combining individual subproducts resulted in higher overall accuracy for water detection overall detection rate around 60 and waterbodies classification overall accuracies up to 79 moreover we showed that sar data and shape indices played important roles in better discriminating waterbodies however limitations in edge detection outcomes affected the identification of small and isolated aquaculture waterbodies especially those integrated into rice fields or in areas with trees keywords water detection google earth engine supervised classification feature classification feature segmentation data availability data will be made available on request 1 introduction aquaculture is a rapidly growing food production activity that provides for millions of people around the world little et al 2016 global aquaculture accounts for 48 of all fish production projected to reach 53 by 2030 with aquaculture production in asia vastly surpassing that of any other continent fao 2020 for bangladesh the fifth largest aquaculture producer in the world fao 2020 aquaculture output increased 75 between 2002 and 2020 from 1 47 million metric tons to 2 58 million metric tons dof 2020 2002 while aquaculture has provided income and nutritious food for vast numbers of people in bangladesh belton et al 2012 its rapid growth has created competition with crop production in coastal and arid lands paul and rashid 2017 yu et al 2020 moreover national statistics are thought not to have kept pace with aquaculture growth belton and azad 2012 this confluence of land use competition food security concerns and patchy data creates an urgent need for new aquaculture identification methods currently there are two viable methods for identifying aquaculture ponds the first option is to conduct extensive ground surveys surveys are expensive and time consuming especially when considering large regions rhodes et al 2015 but they provide very accurate and specific information the second option is utilizing remote sensing for aquaculture waterbody identification this is a newer method with many benefits e g low cost and some shortcomings e g low accuracy many remote sensing products are free and can be used to analyze large areas quickly in recent years there has been a significant increase in the use of remote sensing products to support aquaculture management ottinger et al 2016 applications include site selection and mapping anand et al 2020 ottinger et al 2022 long term monitoring duan et al 2020b gusmawati et al 2018 luo et al 2022 stiller et al 2019 aquaculture inventory prasad et al 2019 travaglia et al 2004 virdis 2014 and assessment of large scale environmental and economic impacts of aquaculture practices al sayah et al 2020 ottinger et al 2018 in general three types of products are used for aquaculture infrastructure detection synthetic aperture radar sar medium resolution multispectral imagery from optical sensors and high and very high resolution multispectral and panchromatic imagery ottinger et al 2016 selecting a particular product mostly depends on trade offs between cost spatial resolution temporal availability and surface area coverage high and very high resolution imagery from either sar or optical sensors are useful for detecting small water bodies i e surface areas 800 m2 and are typically the most expensive products limiting surface coverage and temporal availability jayanthi et al 2018 however certain sar products such as sentinel 1 imagery can be obtained for free but have a very limited temporal availability because of their recent operational starting date luo et al 2022 it is worth noting that in contrast to optical sensors radar can capture water surfaces regardless of daylight and cloud cover cloud interference is a major difficulty for aquaculture waterbody detection in tropical regions and coastal areas using multispectral imagery limiting temporal availability and continuity ottinger et al 2016 still medium resolution multispectral products are generally free cover long periods and can detect aquaculture waterbodies with surface areas above 800 1000 m2 which is mainly used for long term analysis and large scale applications duan et al 2020a 2020b luo et al 2022 typically automatic and semi automatic approaches for aquaculture waterbody detection using multispectral imagery incorporate water surface extraction using water indexes and cloud free imagery al sayah et al 2020 anand et al 2020 ren et al 2019 zeng et al 2019 recent studies have shown that combining different product types i e sar and multispectral makes it possible to obtain cost effective aquaculture waterbody detection results even for long periods of time and with good spatial resolutions ottinger et al 2022 stiller et al 2019 in this context the aim of this study is to determine the presence of aquaculture waterbodies using sar and multispectral imagery in southern bangladesh the detection of aquaculture waterbodies in this zone is particularly challenging in this region due to their small size diversity geometric complexity surrounding conditions and seasonality in comparison to larger and more standardized aquaculture infrastructure found in many other coastal areas sun et al 2020 here we propose a framework built on top of a recently developed workflow using multispectral imagery ferriby et al 2021 to address these challenges specific objectives of this study are to 1 evaluate the use of sar and multiple water indices for surface water detection 2 compare waterbodies segmentation based on sar and multispectral imagery 3 determine the predictive performance of different unsupervised and supervised classification methods and 4 identify the main factors affecting the overall performance of the proposed framework 2 materials and methods the proposed framework presented in fig 1 is comprised of three sequential stages 1 water detection 2 feature segmentation and 3 feature classification we applied a binary thresholding method for water detection for both sar and multispectral imagery to classify image pixels into land or water then we implemented a convolution operation for edge detection under the feature segmentation stage the output was used to obtain polygons that intersected with the resulting water mask from the previous stage to identify water polygons next and during the feature classification stage the water polygons were grouped as aquaculture or non aquaculture waterbodies for this purpose we implemented various statistical and machine learning methods using shape indices and spectral backscatter information as predictors the proposed framework is further described in sections 2 3 2 4 and 2 5 finally we evaluated the classification results using various performance metrics to identify the best aquaculture waterbody detection approach for the study area which are presented in more detail in section 2 6 2 1 study area the proposed framework is implemented in seven districts within southwest and south central bangladesh comprising an area of 17 385 km2 fig 2 these districts have a high concentration of aquaculture farms and are targeted as strategic areas for food security and long term development programs usaid 2021 overall the dominant land use is agriculture 62 followed by built up areas 23 waterbodies 13 and wetlands 2 china ministry of natural resources 2021 the sundarban reserve forest the world s largest contiguous mangrove forest located in the southwest was excluded from the study region given the generalized low concentration of a built environment in that area the study area has four seasons winter december to february pre monsoon march to may monsoon june to september and post monsoon october and november annual precipitation ranges from 1200 to 2800 mm across the seven districts with the southeastern area having a longer and heavier rainy season from may to september a variety of types of aquaculture waterbodies are found in the region these include homestead ponds commercial ponds and ghers with or without concurrent rice cultivation a gher is a modified rice field comprising an excavated trench around part of the periphery of the rice field and elevated dikes constructed to maintain a water depth of at least 1 m in the trench these four waterbody types differ in shape size water depth and period containing water for this study we focused exclusively on these four types of enclosed waterbody and excluded other facilities used to grow fish such as stocked natural waterbodies pens and cages for simplicity we refer to all enclosed waterbodies used for aquaculture as aquaculture waterbodies for the remainder of the paper 2 2 data we collected sar and multispectral imagery with 10 m spatial resolution from sentinel 1 and 2 missions using google earth engine gee for this study image collections were obtained from october 26 2020 to november 15 2020 post monsoon season during this period all types of aquaculture waterbodies are usually filled with water and multispectral images have less than 10 cloud coverage in the study area for edge detection purposes see section 2 4 we obtained sar images from april 1 2019 to march 31 2020 which were averaged to obtain a single sar scene for the study area sentinel 1 imagery available at gee corresponds to c band ground range detected grd scenes processed using the european spatial agency s esa sentinel 1 toolbox s1tbx pre processing steps include thermal noise removal radiometric calibration and terrain correction the pre processing output is reported as backscatter values in decibels db this study used the single co polarization vertical transmit vertical receive vv band for water detection and feature segmentation purposes for classification we employed both the vv and vertical transmit horizontal receive vh bands regarding multispectral imagery we used level 1c top of atmosphere toa multispectral reflectance data from sentinel 2 available in gee each scene contains 13 spectral bands used in the first stage to compute different water indices and implement a recently proposed data transformation method for water detection li et al 2021 2 3 proposed aquaculture waterbodies detection framework 2 3 1 stage 1 water detection when using sentinel 1 imagery we directly used vv backscatter data to detect water pixels for sentinel 2 imagery we either computed multiple water indices or performed a data transformation routine before implementing a binary thresholding method for water detection in total we generated ten individual water masks i e one from sar data eight from different water indices and one from sentinel 2 transformed data the final water mask a k a ensemble mask was obtained by majority vote among these individually generated water masks the last water mask was also vectorized the water indices used in this study were the normalized difference water index ndwi the modified ndwi mnwi the automated water extraction index shadow aewish and no shadow aewinsh the general water index gwi the multiband water index mbwi the water ratio index wri and the new water index nwi equations to compute each index are presented in table 1 except for ndwi all the water indices were computed using the indices module in hydrologic remote sensing analysis for floods hydrofloods python package servir 2019 ndwi was computed using the normalized difference function in the gee python api with the green and near infrared nir bands as inputs as an alternative to using a water index based approach we applied the tasseled cap transformation a k a k t transformation to the sentinel 2 data to perform water extraction li et al 2021 the k t transformation which is a special principal component analysis case reprojects the multispectral data into three major components brightness greenness and wetness these components are generally associated with albedo vegetation and soil moisture respectively li et al 2021 equations for each component are derived using gram schmidt orthogonalization or the procrustes analysis pcp method in this study we used the wetness component coefficients derived from the pcp method reported by shi and xu 2019 as recommended by li et al 2021 the automatic thresholding method we used to classify sar vv water indices and k t transformed data into land and water pixels was the edge otsu method as presented by markert et al 2020 otsu s method for image thresholding uses a histogram to identify two classes with maximum inter class variance in the edge otsu method an initial user provided threshold classifies the input image into water and land classes next from the resulting binary image features edges are detected using a canny edge filter only edge features over 200 m in length are valid to prevent misclassifications due to the initial user provided threshold markert et al 2020 it is important to note that this threshold was originally used for detecting large waterbodies then a buffer is built around the filtered edges pixel values from the original image i e sar vv water index transformed data are subsequently sampled within the buffer to build the histogram to be used as input in otsu s method the resulting threshold is then applied to the entire study region to obtain the water mask the edge otsu method was applied using hydrafloods the initial threshold was 16 db for sar vv 0 for most water indices except for wri which was 0 7 and 300 for the k t transformed data the buffer widths were defined as 300 m 150 m on either side of the edges 2 3 2 stage 2 feature segmentation in this stage we applied a convolution operation for edge detection using a 3 3 laplacian 8 kernel followed by a gaussian smoothing operation using a 5 5 kernel these operations were applied to sar vv and ndwi scenes covering the entire study area using the gee python api we used ndwi because all its input bands have a spatial resolution of 10 m other water indices use at least one band with coarser resolution for the sar vv scene we obtained an average composite for the 2019 2020 water year to attenuate the noise found in individual sar scenes and obtain a better definition of polygon borders the ndwi scene that we used was obtained from stage 1 pixels from each edge detection output i e sar vv and ndwi outputs were classified into two classes i e edges and no edges no edges were vectorized with their contour simplified using an existing river mask that was manually obtained from stage 1 we removed polygons with their centroids falling within the major rivers in the study area then any gaps within the remaining polygons were automatically filled finally the filled polygons were intersected with the final water mask obtained from stage 1 the intersecting polygons were then extracted to generate the water polygons that are classified into aquaculture waterbodies and other waterbodies in stage 3 it is worth noting that we obtained two collections of water polygons one for sar vv and another for ndwi 2 3 3 stage 3 feature classification in this stage we compared the performance of four different supervised classification methods logistic regression lr support vector machine svm classification and regression trees cart and random forests rf these methods were used to separate the water polygons from stage 2 into aquaculture waterbodies and other waterbodies using the caret library in r kuhn 2008 we randomly partitioned the ground truthing data gtd into two groups one for training 80 and another for validation 20 within each partition we kept similar proportions of aquaculture waterbody and other waterbody objects using a stratified sampling approach then the machine learning techniques were trained using 5 fold cross validation we computed multiple shape indices and feature wide spectral and backscatter values these values were used as predictors and were computed for the gtd and for the extracted water polygons from the previous stage we computed ten shape indices see table 2 including eccentricity solidity convexity rectangularity iso perimetric quotient ipq shape roughness patch fractal dimensions pfd square pixel metric sqp perimeter to area ratio para and ratio of the area and the smallest circumscribing circle circle some of these metrics have been used in previous studies for object based classification yu et al 2020 regarding the spectral information we computed the average reflectance and backscatter values for the blue green red nir swir1 swir2 vv and vh bands for each gtd and water polygon in addition the polygon area was used as a predictor variable finally using the shape indices and the feature wide spectral and backscatter values for the water polygons derived from the vv and ndwi scenes we performed a clustering analysis using the k medoids method in arcgis pro 2 7 jain 2010 this unsupervised classification approach was used as an alternative to the supervised classification methods to avoid any influence of the gtd on the overall aquaculture waterbody detection process we determined the optimal number of clusters by maximizing the calinski harabasz pseudo f statistic which represents the ratio of between cluster and within cluster variance calinski and harabasz 1974 2 4 performance evaluation using the gtd we evaluated the performance of different stages of the proposed aquaculture waterbody detection workflow for stage 1 we determined the proportion of aquaculture waterbody centroids that fell within the multiple water masks from sar vv water indices k t transformed spectral values and the final i e ensemble mask also we obtained a summary of major land cover classes associated with those aquaculture waterbodies that did not fall within each water mask for this purpose we used the worldcover layer esa 2021 this layer is a global land cover product at a 10 m resolution based on sentinel 1 and 2 data for 2020 meanwhile for stage 3 we obtained multiple metrics representing the overall and per class performance such as accuracy kappa coefficient sensitivity specificity precision recall f1 score and balanced accuracy for each tested method using the validation samples since the validation samples were slightly unbalanced with 57 1 of the samples being aquaculture waterbodies we reported additional metrics such as the positive and negative predicted values detection rate and detection prevalence the first two measures provide unconditional evaluations of the classification outcomes by accounting for prevalence in the data while the latter two allowed us to identify under or over prediction in aquaculture waterbody detection lastly for the final classification results we reported the proportions in terms of the number and surface area of gtd aquaculture waterbodies with their centroids falling within water polygons classified as aquaculture waterbodies for the performance analysis of the final prediction results we only considered those polygons with surface areas below 40 ha this threshold corresponds to the maximum size of ghers without rice which is a type of aquaculture waterbodies in the study area 3 results 3 1 performance of sar and multispectral imagery in detecting water in table 3 we present the proportion of gtd aquaculture waterbody centroids falling within each water mask that was generated during stage 1 the ensemble mask which was obtained using a majority vote provided the highest proportions overall 60 2 and per district from 19 to 87 in general the sar derived water mask underperformed 35 5 overall from 3 7 to 64 6 per district compared to the other water masks that were derived from multispectral imagery overall 48 underperformed the best performing water index for water detection was aewish 56 7 followed by mbwi 55 8 and gwi 54 1 overall other widely used water indices such as ndwi and mndwi fell behind 47 7 and 41 0 respectively the worst performing index was wri 29 9 which failed to detect any aquaculture waterbody in barisal bhola and gopalganj districts these districts consistently resulted in low performances among all the water masks used for water detection on the other hand the districts with the highest proportions among the different water masks were jashore bagerhat and khulna the ensemble obtained the highest proportion of detected aquaculture waterbodies 86 7 in the jashore district regarding the percentage of surface area covered by water according to the final mask the results were as follows barishal 17 6 bhola 15 5 gopalganj 27 3 khulna 43 5 satkhira 53 7 bagerhat 43 4 and jashore 18 5 3 2 land cover classes for non detected aquaculture waterbodies we identified the major land cover classes for those aquaculture waterbodies with their centroids falling outside the different water masks here we present the results for the sar vv and ensemble masks the latter was used as a representative outcome for multispectral imagery since the majority vote was mostly determined by the reflectance based water masks in general the major land cover categories found for non detected aquaculture waterbodies using these masks were trees 43 5 and 55 for sar vv and ensemble respectively and cropland 45 7 and 40 2 for sar vv and ensemble respectively when disaggregating these results by district see table 4 we identified that non detected aquaculture waterbodies within the worst performing districts i e barisal bhola gopalganj fell mostly under the trees category for both water masks 80 for sar vv 90 for the ensemble it is worth noting that for these districts the sar vv mask resulted in more non detected aquaculture waterbodies under the cropland category i e 10 17 compared to the ensemble mask i e 9 for the remaining districts other waterbodies are mostly fell under the cropland category 31 70 for sar vv and 5 1 69 for the ensemble 3 3 water polygons classification results using different machine learning techniques rf resulted in the best overall performance among the evaluated classifiers for water polygons classification into aquaculture waterbodies and other waterbodies followed very closely by cart table 5 in addition confidence intervals for the overall accuracy indicated that cart rf and svm performed significantly better than lr in fact the latter did not significantly improve the no information rate of 57 1 i e the highest frequency in the gtd among the two prediction classes resulting in an overall poor predictive performance in terms of the kappa statistic cart and rf outperformed svm the former showing an acceptable agreement with the gtd kappa 0 5 when evaluating the per class performance for each classifier the highest true positive rate represented by the sensitivity measure was obtained by rf meanwhile the highest false positive rate represented by low specificity values was obtained by svm this outcome also indicated that the svm classifier underperformed in other waterbody detection explaining its limited overall reliability again cart and rf resulted in the most balanced accuracy i e average of sensitivity and specificity among the four tested classifiers in general aquaculture waterbody detection performance by cart rf and svm was good with precision recall and f1 metrics around 80 being rf and cart the best performing classifiers results for positive predicted values indicated that cart provided the highest probability of incorrectly detecting aquaculture waterbodies likewise results for negative predicted values revealed that rf provided the highest probability incorrectly detecting other waterbodies cart rf and svm presented very similar aquaculture waterbody detection rates above 45 however detection prevalence 60 for rf and svm were higher than the prevalence of 57 1 indicating a slight overperformance regarding the unsupervised classification results two clusters were identified using the optimal pseudo f statistic i e aquaculture waterbody and other waterbody for both vv and ndwi based water polygons 3 4 relative importance of predictors used for water polygons classification the scaled relative importance of predictors used by the best performing classifiers is presented in fig 3 a from the 19 predictors considered in this study only seven presented scaled relative importance greater than zero when using cart sorted in decreasing importance these predictors were solidity convexity para area swir1 sqp and ipq it is worth noting that the final classification tree obtained by cross validation only employed solidity and para to classify water polygons into aquaculture waterbodies and other waterbodies see fig 3b however these two variables were not as relevant to rf and svm which consistently gave more importance to eccentricity and vh the only variable that showed a consistent relevance among the three classifiers was the waterbody area shape indices that consistently showed low relative importance were shape roughness pfd circle and rectangularity likewise vv blue and swir2 information consistently showed low influence on the classification results overall the geometric attributes were stronger predictors compared to multispectral predictors only svm showed relatively higher importance scores for spectral information i e green red nir and swir1 reflectance in summary cart relied mostly on shape indices rf used both shape and backscatter variables and svm used shape reflectance and backscatter information for water polygons classification the unsupervised classification results also revealed that the shape indices were better in discriminating water polygons by computing the proportion of the total variance of each predictor variable explained by the two clusters i e r 2 the top five indices were solidity sqp ipq pfd and convexity for both sets of water polygons excepting pfd these predictors all have r 2 0 5 are included in the set of predictors showing relative importance greater than zero when using cart 3 5 predictive performance of the proposed framework for aquaculture waterbody detection compared to water detection using only the final water mask from stage 1 please see ensemble row in table 3 the proportion of aquaculture waterbodies detected by incorporating the classifiers was reduced by about 20 table 6 however this reduction in aquaculture waterbody detection varied across the different districts with lower reductions within the worst performing districts i e barisal bhola and gopalganj which contain lots of small homestead ponds with tree cover as presented in table 6 cart and svm showed the highest detection performance among the different classifiers however they mostly classified all the water polygons as aquaculture waterbodies including highly irregular polygons that are closely identified as natural waterbodies or crops by visually inspecting high resolution multispectral imagery figs 4 and 5 on the other hand rf and k medoids generated a more balanced classification at the expense of lower true positive detection rates since cart svm and rf resulted in similar training performance and unsupervised classification was similar to rf we used these four classifiers to obtain an ensemble this ensemble classified a water polygon as an aquaculture waterbody if two or more individual classifiers detected the same polygon as an aquaculture waterbody we did not consider lr in the ensemble given its poor performance during stage 3 as a result the ensemble showed a better performance than rf and k medoids when comparing the predictions with the gtd while maintaining a consistent behavior based on visual inspection figs 4f and 5f we also compared the classification results when using water polygons obtained from different types of imagery i e sar and multispectral see table 7 it is worth noting that the sar image used for segmentation corresponded to an annual average whereas the ndwi image was a two week composite during the post monsoon season therefore the sar image accounted for temporal variation in backscatter values that facilitated the identification of dry borders still note that visual results for industrial style aquaculture waterbodies are very similar for both sar and ndwi images considered in this study see the bottom left region in figs 4 and 5 whereas the differences are more evident within agricultural land in general water polygons derived from the ndwi scene resulted in higher detection rates compared to the sar scene table 6 likewise proportions of gtd total area were higher when using the ndwi scene this occurred because the ndwi based water polygons were generally bigger and less segmented than the sar based water polygons figs 4 and 6 therefore using ndwi based water polygons made it more difficult to identify smaller aquaculture waterbodies located within large agricultural areas only the satkhira district showed better results when using sar based water polygons compared to the ndwi scene this district is characterized by large continuous areas classified as water during stage 1 with more than half of its surface covered by water see section 3 1 4 discussion 4 1 using both sar and multispectral imagery for aquaculture waterbodies detection previous studies on aquaculture waterbody detection have mainly used multispectral and sar imagery separately based mostly on unsupervised thresholding methods sar imagery has been preferred over multispectral imagery in several cases given its increased temporal availability and higher spatial resolution compared to some publicly available multispectral datasets notwithstanding when used together multispectral imagery has been typically employed for generating static water masks while sar imagery has been used to quantify the temporal variability of detected water bodies regardless of the imagery type geometry attributes have been commonly used as explanatory variables when using object based classification approaches luo et al 2022 sun et al 2020 yu et al 2020 in this study we found that existing approaches for aquaculture waterbody detection can be improved when 1 generating ensembles for water detection accounting for individual results from both multispectral and sar data 2 using sar and multispectral imagery for feature segmentation purposes and 3 incorporating backscatter data to train machine learning classifiers and inform unsupervised methods regarding water detection we showed an improvement in true positive rates using the gtd of aquaculture waterbodies as a reference in effect we observed at least an overall 4 increase in true positive rates when using the ensemble water mask compared to the individually generated masks table 3 moreover the ensemble mask indicated that different individual masks contributed differently across districts to attain water detection rates ranging from 18 to 87 individually each mask had detection rates ranging from 0 to 86 following the main findings by ferriby et al 2021 we opted for using scenes collected in a short time window i e two week during the post monsoon season as a compromise between cloud coverage effects and the wide ranging presence of operating aquaculture waterbodies in the study area since we were interested in contrasting sar and multispectral imagery performance when detecting water at stage 1 we did not use a multi temporal approach which is considered a limitation of our proposed framework therefore we recommend exploring the incorporation of multi temporal water detection using ensemble schemes in future work with respect to feature segmentation we found that both sar and multispectral imagery can be employed for this purpose using edge detection based on convolution filters when using sar data we found that an average composite calculated over a water year resulted in more segments with smaller feature areas compared to those obtained using an ndwi average scene computed over a much shorter period i e two weeks average sar imagery accounted for temporal variability of waterbodies and reduced speckle effects typically found in individual sar scenes prasad et al 2019 however in both cases we found important limitations when segmenting water within agricultural and forested environments where we obtained either very irregular and interconnected patches or very large water polygons these limitations are attributed mainly to the spatial resolution of the datasets i e 10 m and vegetation interference we also acknowledge that more sophisticated edge detection methods are needed to improve the segmentation results from stage 2 note that the application and comparison of different edge detection approaches were beyond the scope of this study finally sar data also played a critical role in model training and prediction during stage 3 for instance while vv polarization values were used for both water detection and feature segmentation vh polarization values showed a high relative importance in two machine learning classifiers i e svm and rf when using rf vh values showed a very high relative importance compared to other geometric and spectral attributes except for eccentricity which discriminates between elongated and squared shapes it is worth noting that most of the top explanatory factors in terms of relative importance across the different tested methods corresponded to shape indices however in some cases the prediction results failed in better discriminating between aquaculture waterbodies and other waterbodies as revealed by visual inspection across the study area this occurred when using classifiers i e cart svm that did not provide significant importance to vh values in relation to other shape indices or spectral data therefore we recommend future studies explicitly consider water indices as explanatory variables instead of reflectance values as well as other metrics such as the vv vh ratio in addition it is important to evaluate the relevancy of nonlinear interactions for waterbodies classification texture metrics derived from both multispectral and sar imagery might also help to improve classification outcomes 4 2 factors affecting the performance in aquaculture waterbodies detection results for stage 1 showed that the detected water extension was generally lower when using sar data compared to water index based extraction this difference meant an under detection of water bodies using c band sar imagery and highlighted the importance of considering near infrared reflectance i e nir and swir bands related to soil moisture poor soil moisture detection by c band sar imagery is indicative of vegetation interference preventing the microwave signal from reaching the soil el hajj et al 2019 in fact vegetation interference partially explains limitations in water detection in cropland and trees land cover classes as presented in table 4 it is worth noting that we only tested the vv polarization in c band sar imagery for water mask extraction therefore testing other polarizations and band combinations is recommended for future studies likewise limitations in feature segmentation during stage 2 also impacted the overall aquaculture waterbody detection highly large irregular polygons resulting from the 10 m spatial resolution and edge detection approach prevented the identification of small waterbodies also when removing very large water polygons 40 ha and those intersecting the river mask obtained from stage 1 some potential aquaculture waterbodies that are adjacent to major rivers and within agricultural forested and urbanized areas were affected this was especially the case when using sar imagery and was more evident in districts largely covered by surface water such as satkhira khulna bagerhat and gopalganj see fig 6b e g f meanwhile water polygons resulting from sar imagery tended to have smaller surface areas i e thicker borders than those obtained from the ndwi scene e g fig 6a however ndwi derived borders were generally poorly defined resulting in potentially higher false negative rates e g fig 6f additionally smaller aquaculture waterbodies i e surface areas 0 04 ha were hardly or not identified due to the dataset spatial resolution and vegetation interference especially in forested areas see fig 6b c d in general the edge detection approach used in this study yielded multiple irregular shapes that affected the overall aquaculture waterbody detection this problem is not exclusive to edge detection results but is also found when using thresholding methods directly for feature segmentation in those cases some authors have applied morphological operations e g dilation erosion to smooth irregularities at the expense of reducing the identification of smaller waterbodies duan et al 2020b other authors have proposed using either polygon border simplification or convex rectangular hulls ottinger et al 2022 polygon irregularity resulting from feature segmentation is problematic because the gtd polygons we used during model training were in contrast very regular therefore the transferability of the training outcomes to prediction was affected by the apparent disconnection between the data we used during stage 3 and the features we obtained from stage 2 for instance we passed from accuracy of 79 for rf during validation to 35 and 24 when using the ndwi and sar based water polygons in prediction mode respectively consequently we found no warranty that good performances during model training and validation imply a good performance during model prediction for this reason we recommend that gtd collection be as consistent as possible with feature segmentation outcomes to maximize information content and improve the reliability of the overall performance evaluation process 5 conclusions in this study an object based framework integrating sentinel 1 and 2 imagery was developed to improve the detection of aquaculture waterbodies in southwest bangladesh we employed both sar and reflectance data for water detection feature segmentation and waterbodies classification we combined water masks obtained from microwave backscatter values water indices and transformed reflectance data into a majority vote ensemble as a result we improved water detection rates concerning individual water masks results with overall rates of 60 and up to 87 in individual districts likewise by obtaining an ensemble of multiple supervised and unsupervised classifiers we obtained a more reliable detection of aquaculture waterbodies across the study area moreover we found that c band sar vh information and shape indices such as eccentricity played important roles in better differentiating waterbodies according to variable importance results on the other hand limitations in water detection were mainly dictated by the dataset s spatial resolution and vegetation interference particularly we obtained poor results when targeting small aquaculture waterbodies i e surface areas 0 04 ha and isolated waterbodies in agricultural and forested areas this may result from the fact that 200 m length was used as the initial user provided threshold meanwhile shortcomings in feature segmentation which mostly resulted from poorly defined borders or highly irregular water polygons affected the transferability of well performing supervised classification results into final predictions i e overall accuracies up to 79 for validation to 24 35 for prediction therefore for future work we recommend performing ground truthing data collection in accordance with preliminary feature segmentation outcomes likewise further efforts should be addressed to develop better performing feature segmentation techniques incorporate multi temporal information test additional explanatory factors e g texture metrics and take advantage of higher resolution datasets declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the feed the future innovation lab for fish is managed by mississippi state university through an award from the united states agency for international development usaid award no 7200aa18ca00030 m lawrence pi and provides support to this project grant no 193900 312455 12 b belton pi haque pi this work was also supported by the usda national institute of food and agriculture hatch project 1019654 
25523,in this study we evaluated the use of synthetic aperture radar sar and multispectral data to detect aquaculture waterbodies in southern bangladesh to quantify fish production on a national scale for this purpose we developed an object based framework comprised of three sequential stages 1 water detection 2 feature segmentation and 3 feature classification techniques such as edge otsu for binary thresholding edge detection with convolution filters and various supervised and unsupervised machine learning methods were used as part of a workflow we found that ensemble products combining individual subproducts resulted in higher overall accuracy for water detection overall detection rate around 60 and waterbodies classification overall accuracies up to 79 moreover we showed that sar data and shape indices played important roles in better discriminating waterbodies however limitations in edge detection outcomes affected the identification of small and isolated aquaculture waterbodies especially those integrated into rice fields or in areas with trees keywords water detection google earth engine supervised classification feature classification feature segmentation data availability data will be made available on request 1 introduction aquaculture is a rapidly growing food production activity that provides for millions of people around the world little et al 2016 global aquaculture accounts for 48 of all fish production projected to reach 53 by 2030 with aquaculture production in asia vastly surpassing that of any other continent fao 2020 for bangladesh the fifth largest aquaculture producer in the world fao 2020 aquaculture output increased 75 between 2002 and 2020 from 1 47 million metric tons to 2 58 million metric tons dof 2020 2002 while aquaculture has provided income and nutritious food for vast numbers of people in bangladesh belton et al 2012 its rapid growth has created competition with crop production in coastal and arid lands paul and rashid 2017 yu et al 2020 moreover national statistics are thought not to have kept pace with aquaculture growth belton and azad 2012 this confluence of land use competition food security concerns and patchy data creates an urgent need for new aquaculture identification methods currently there are two viable methods for identifying aquaculture ponds the first option is to conduct extensive ground surveys surveys are expensive and time consuming especially when considering large regions rhodes et al 2015 but they provide very accurate and specific information the second option is utilizing remote sensing for aquaculture waterbody identification this is a newer method with many benefits e g low cost and some shortcomings e g low accuracy many remote sensing products are free and can be used to analyze large areas quickly in recent years there has been a significant increase in the use of remote sensing products to support aquaculture management ottinger et al 2016 applications include site selection and mapping anand et al 2020 ottinger et al 2022 long term monitoring duan et al 2020b gusmawati et al 2018 luo et al 2022 stiller et al 2019 aquaculture inventory prasad et al 2019 travaglia et al 2004 virdis 2014 and assessment of large scale environmental and economic impacts of aquaculture practices al sayah et al 2020 ottinger et al 2018 in general three types of products are used for aquaculture infrastructure detection synthetic aperture radar sar medium resolution multispectral imagery from optical sensors and high and very high resolution multispectral and panchromatic imagery ottinger et al 2016 selecting a particular product mostly depends on trade offs between cost spatial resolution temporal availability and surface area coverage high and very high resolution imagery from either sar or optical sensors are useful for detecting small water bodies i e surface areas 800 m2 and are typically the most expensive products limiting surface coverage and temporal availability jayanthi et al 2018 however certain sar products such as sentinel 1 imagery can be obtained for free but have a very limited temporal availability because of their recent operational starting date luo et al 2022 it is worth noting that in contrast to optical sensors radar can capture water surfaces regardless of daylight and cloud cover cloud interference is a major difficulty for aquaculture waterbody detection in tropical regions and coastal areas using multispectral imagery limiting temporal availability and continuity ottinger et al 2016 still medium resolution multispectral products are generally free cover long periods and can detect aquaculture waterbodies with surface areas above 800 1000 m2 which is mainly used for long term analysis and large scale applications duan et al 2020a 2020b luo et al 2022 typically automatic and semi automatic approaches for aquaculture waterbody detection using multispectral imagery incorporate water surface extraction using water indexes and cloud free imagery al sayah et al 2020 anand et al 2020 ren et al 2019 zeng et al 2019 recent studies have shown that combining different product types i e sar and multispectral makes it possible to obtain cost effective aquaculture waterbody detection results even for long periods of time and with good spatial resolutions ottinger et al 2022 stiller et al 2019 in this context the aim of this study is to determine the presence of aquaculture waterbodies using sar and multispectral imagery in southern bangladesh the detection of aquaculture waterbodies in this zone is particularly challenging in this region due to their small size diversity geometric complexity surrounding conditions and seasonality in comparison to larger and more standardized aquaculture infrastructure found in many other coastal areas sun et al 2020 here we propose a framework built on top of a recently developed workflow using multispectral imagery ferriby et al 2021 to address these challenges specific objectives of this study are to 1 evaluate the use of sar and multiple water indices for surface water detection 2 compare waterbodies segmentation based on sar and multispectral imagery 3 determine the predictive performance of different unsupervised and supervised classification methods and 4 identify the main factors affecting the overall performance of the proposed framework 2 materials and methods the proposed framework presented in fig 1 is comprised of three sequential stages 1 water detection 2 feature segmentation and 3 feature classification we applied a binary thresholding method for water detection for both sar and multispectral imagery to classify image pixels into land or water then we implemented a convolution operation for edge detection under the feature segmentation stage the output was used to obtain polygons that intersected with the resulting water mask from the previous stage to identify water polygons next and during the feature classification stage the water polygons were grouped as aquaculture or non aquaculture waterbodies for this purpose we implemented various statistical and machine learning methods using shape indices and spectral backscatter information as predictors the proposed framework is further described in sections 2 3 2 4 and 2 5 finally we evaluated the classification results using various performance metrics to identify the best aquaculture waterbody detection approach for the study area which are presented in more detail in section 2 6 2 1 study area the proposed framework is implemented in seven districts within southwest and south central bangladesh comprising an area of 17 385 km2 fig 2 these districts have a high concentration of aquaculture farms and are targeted as strategic areas for food security and long term development programs usaid 2021 overall the dominant land use is agriculture 62 followed by built up areas 23 waterbodies 13 and wetlands 2 china ministry of natural resources 2021 the sundarban reserve forest the world s largest contiguous mangrove forest located in the southwest was excluded from the study region given the generalized low concentration of a built environment in that area the study area has four seasons winter december to february pre monsoon march to may monsoon june to september and post monsoon october and november annual precipitation ranges from 1200 to 2800 mm across the seven districts with the southeastern area having a longer and heavier rainy season from may to september a variety of types of aquaculture waterbodies are found in the region these include homestead ponds commercial ponds and ghers with or without concurrent rice cultivation a gher is a modified rice field comprising an excavated trench around part of the periphery of the rice field and elevated dikes constructed to maintain a water depth of at least 1 m in the trench these four waterbody types differ in shape size water depth and period containing water for this study we focused exclusively on these four types of enclosed waterbody and excluded other facilities used to grow fish such as stocked natural waterbodies pens and cages for simplicity we refer to all enclosed waterbodies used for aquaculture as aquaculture waterbodies for the remainder of the paper 2 2 data we collected sar and multispectral imagery with 10 m spatial resolution from sentinel 1 and 2 missions using google earth engine gee for this study image collections were obtained from october 26 2020 to november 15 2020 post monsoon season during this period all types of aquaculture waterbodies are usually filled with water and multispectral images have less than 10 cloud coverage in the study area for edge detection purposes see section 2 4 we obtained sar images from april 1 2019 to march 31 2020 which were averaged to obtain a single sar scene for the study area sentinel 1 imagery available at gee corresponds to c band ground range detected grd scenes processed using the european spatial agency s esa sentinel 1 toolbox s1tbx pre processing steps include thermal noise removal radiometric calibration and terrain correction the pre processing output is reported as backscatter values in decibels db this study used the single co polarization vertical transmit vertical receive vv band for water detection and feature segmentation purposes for classification we employed both the vv and vertical transmit horizontal receive vh bands regarding multispectral imagery we used level 1c top of atmosphere toa multispectral reflectance data from sentinel 2 available in gee each scene contains 13 spectral bands used in the first stage to compute different water indices and implement a recently proposed data transformation method for water detection li et al 2021 2 3 proposed aquaculture waterbodies detection framework 2 3 1 stage 1 water detection when using sentinel 1 imagery we directly used vv backscatter data to detect water pixels for sentinel 2 imagery we either computed multiple water indices or performed a data transformation routine before implementing a binary thresholding method for water detection in total we generated ten individual water masks i e one from sar data eight from different water indices and one from sentinel 2 transformed data the final water mask a k a ensemble mask was obtained by majority vote among these individually generated water masks the last water mask was also vectorized the water indices used in this study were the normalized difference water index ndwi the modified ndwi mnwi the automated water extraction index shadow aewish and no shadow aewinsh the general water index gwi the multiband water index mbwi the water ratio index wri and the new water index nwi equations to compute each index are presented in table 1 except for ndwi all the water indices were computed using the indices module in hydrologic remote sensing analysis for floods hydrofloods python package servir 2019 ndwi was computed using the normalized difference function in the gee python api with the green and near infrared nir bands as inputs as an alternative to using a water index based approach we applied the tasseled cap transformation a k a k t transformation to the sentinel 2 data to perform water extraction li et al 2021 the k t transformation which is a special principal component analysis case reprojects the multispectral data into three major components brightness greenness and wetness these components are generally associated with albedo vegetation and soil moisture respectively li et al 2021 equations for each component are derived using gram schmidt orthogonalization or the procrustes analysis pcp method in this study we used the wetness component coefficients derived from the pcp method reported by shi and xu 2019 as recommended by li et al 2021 the automatic thresholding method we used to classify sar vv water indices and k t transformed data into land and water pixels was the edge otsu method as presented by markert et al 2020 otsu s method for image thresholding uses a histogram to identify two classes with maximum inter class variance in the edge otsu method an initial user provided threshold classifies the input image into water and land classes next from the resulting binary image features edges are detected using a canny edge filter only edge features over 200 m in length are valid to prevent misclassifications due to the initial user provided threshold markert et al 2020 it is important to note that this threshold was originally used for detecting large waterbodies then a buffer is built around the filtered edges pixel values from the original image i e sar vv water index transformed data are subsequently sampled within the buffer to build the histogram to be used as input in otsu s method the resulting threshold is then applied to the entire study region to obtain the water mask the edge otsu method was applied using hydrafloods the initial threshold was 16 db for sar vv 0 for most water indices except for wri which was 0 7 and 300 for the k t transformed data the buffer widths were defined as 300 m 150 m on either side of the edges 2 3 2 stage 2 feature segmentation in this stage we applied a convolution operation for edge detection using a 3 3 laplacian 8 kernel followed by a gaussian smoothing operation using a 5 5 kernel these operations were applied to sar vv and ndwi scenes covering the entire study area using the gee python api we used ndwi because all its input bands have a spatial resolution of 10 m other water indices use at least one band with coarser resolution for the sar vv scene we obtained an average composite for the 2019 2020 water year to attenuate the noise found in individual sar scenes and obtain a better definition of polygon borders the ndwi scene that we used was obtained from stage 1 pixels from each edge detection output i e sar vv and ndwi outputs were classified into two classes i e edges and no edges no edges were vectorized with their contour simplified using an existing river mask that was manually obtained from stage 1 we removed polygons with their centroids falling within the major rivers in the study area then any gaps within the remaining polygons were automatically filled finally the filled polygons were intersected with the final water mask obtained from stage 1 the intersecting polygons were then extracted to generate the water polygons that are classified into aquaculture waterbodies and other waterbodies in stage 3 it is worth noting that we obtained two collections of water polygons one for sar vv and another for ndwi 2 3 3 stage 3 feature classification in this stage we compared the performance of four different supervised classification methods logistic regression lr support vector machine svm classification and regression trees cart and random forests rf these methods were used to separate the water polygons from stage 2 into aquaculture waterbodies and other waterbodies using the caret library in r kuhn 2008 we randomly partitioned the ground truthing data gtd into two groups one for training 80 and another for validation 20 within each partition we kept similar proportions of aquaculture waterbody and other waterbody objects using a stratified sampling approach then the machine learning techniques were trained using 5 fold cross validation we computed multiple shape indices and feature wide spectral and backscatter values these values were used as predictors and were computed for the gtd and for the extracted water polygons from the previous stage we computed ten shape indices see table 2 including eccentricity solidity convexity rectangularity iso perimetric quotient ipq shape roughness patch fractal dimensions pfd square pixel metric sqp perimeter to area ratio para and ratio of the area and the smallest circumscribing circle circle some of these metrics have been used in previous studies for object based classification yu et al 2020 regarding the spectral information we computed the average reflectance and backscatter values for the blue green red nir swir1 swir2 vv and vh bands for each gtd and water polygon in addition the polygon area was used as a predictor variable finally using the shape indices and the feature wide spectral and backscatter values for the water polygons derived from the vv and ndwi scenes we performed a clustering analysis using the k medoids method in arcgis pro 2 7 jain 2010 this unsupervised classification approach was used as an alternative to the supervised classification methods to avoid any influence of the gtd on the overall aquaculture waterbody detection process we determined the optimal number of clusters by maximizing the calinski harabasz pseudo f statistic which represents the ratio of between cluster and within cluster variance calinski and harabasz 1974 2 4 performance evaluation using the gtd we evaluated the performance of different stages of the proposed aquaculture waterbody detection workflow for stage 1 we determined the proportion of aquaculture waterbody centroids that fell within the multiple water masks from sar vv water indices k t transformed spectral values and the final i e ensemble mask also we obtained a summary of major land cover classes associated with those aquaculture waterbodies that did not fall within each water mask for this purpose we used the worldcover layer esa 2021 this layer is a global land cover product at a 10 m resolution based on sentinel 1 and 2 data for 2020 meanwhile for stage 3 we obtained multiple metrics representing the overall and per class performance such as accuracy kappa coefficient sensitivity specificity precision recall f1 score and balanced accuracy for each tested method using the validation samples since the validation samples were slightly unbalanced with 57 1 of the samples being aquaculture waterbodies we reported additional metrics such as the positive and negative predicted values detection rate and detection prevalence the first two measures provide unconditional evaluations of the classification outcomes by accounting for prevalence in the data while the latter two allowed us to identify under or over prediction in aquaculture waterbody detection lastly for the final classification results we reported the proportions in terms of the number and surface area of gtd aquaculture waterbodies with their centroids falling within water polygons classified as aquaculture waterbodies for the performance analysis of the final prediction results we only considered those polygons with surface areas below 40 ha this threshold corresponds to the maximum size of ghers without rice which is a type of aquaculture waterbodies in the study area 3 results 3 1 performance of sar and multispectral imagery in detecting water in table 3 we present the proportion of gtd aquaculture waterbody centroids falling within each water mask that was generated during stage 1 the ensemble mask which was obtained using a majority vote provided the highest proportions overall 60 2 and per district from 19 to 87 in general the sar derived water mask underperformed 35 5 overall from 3 7 to 64 6 per district compared to the other water masks that were derived from multispectral imagery overall 48 underperformed the best performing water index for water detection was aewish 56 7 followed by mbwi 55 8 and gwi 54 1 overall other widely used water indices such as ndwi and mndwi fell behind 47 7 and 41 0 respectively the worst performing index was wri 29 9 which failed to detect any aquaculture waterbody in barisal bhola and gopalganj districts these districts consistently resulted in low performances among all the water masks used for water detection on the other hand the districts with the highest proportions among the different water masks were jashore bagerhat and khulna the ensemble obtained the highest proportion of detected aquaculture waterbodies 86 7 in the jashore district regarding the percentage of surface area covered by water according to the final mask the results were as follows barishal 17 6 bhola 15 5 gopalganj 27 3 khulna 43 5 satkhira 53 7 bagerhat 43 4 and jashore 18 5 3 2 land cover classes for non detected aquaculture waterbodies we identified the major land cover classes for those aquaculture waterbodies with their centroids falling outside the different water masks here we present the results for the sar vv and ensemble masks the latter was used as a representative outcome for multispectral imagery since the majority vote was mostly determined by the reflectance based water masks in general the major land cover categories found for non detected aquaculture waterbodies using these masks were trees 43 5 and 55 for sar vv and ensemble respectively and cropland 45 7 and 40 2 for sar vv and ensemble respectively when disaggregating these results by district see table 4 we identified that non detected aquaculture waterbodies within the worst performing districts i e barisal bhola gopalganj fell mostly under the trees category for both water masks 80 for sar vv 90 for the ensemble it is worth noting that for these districts the sar vv mask resulted in more non detected aquaculture waterbodies under the cropland category i e 10 17 compared to the ensemble mask i e 9 for the remaining districts other waterbodies are mostly fell under the cropland category 31 70 for sar vv and 5 1 69 for the ensemble 3 3 water polygons classification results using different machine learning techniques rf resulted in the best overall performance among the evaluated classifiers for water polygons classification into aquaculture waterbodies and other waterbodies followed very closely by cart table 5 in addition confidence intervals for the overall accuracy indicated that cart rf and svm performed significantly better than lr in fact the latter did not significantly improve the no information rate of 57 1 i e the highest frequency in the gtd among the two prediction classes resulting in an overall poor predictive performance in terms of the kappa statistic cart and rf outperformed svm the former showing an acceptable agreement with the gtd kappa 0 5 when evaluating the per class performance for each classifier the highest true positive rate represented by the sensitivity measure was obtained by rf meanwhile the highest false positive rate represented by low specificity values was obtained by svm this outcome also indicated that the svm classifier underperformed in other waterbody detection explaining its limited overall reliability again cart and rf resulted in the most balanced accuracy i e average of sensitivity and specificity among the four tested classifiers in general aquaculture waterbody detection performance by cart rf and svm was good with precision recall and f1 metrics around 80 being rf and cart the best performing classifiers results for positive predicted values indicated that cart provided the highest probability of incorrectly detecting aquaculture waterbodies likewise results for negative predicted values revealed that rf provided the highest probability incorrectly detecting other waterbodies cart rf and svm presented very similar aquaculture waterbody detection rates above 45 however detection prevalence 60 for rf and svm were higher than the prevalence of 57 1 indicating a slight overperformance regarding the unsupervised classification results two clusters were identified using the optimal pseudo f statistic i e aquaculture waterbody and other waterbody for both vv and ndwi based water polygons 3 4 relative importance of predictors used for water polygons classification the scaled relative importance of predictors used by the best performing classifiers is presented in fig 3 a from the 19 predictors considered in this study only seven presented scaled relative importance greater than zero when using cart sorted in decreasing importance these predictors were solidity convexity para area swir1 sqp and ipq it is worth noting that the final classification tree obtained by cross validation only employed solidity and para to classify water polygons into aquaculture waterbodies and other waterbodies see fig 3b however these two variables were not as relevant to rf and svm which consistently gave more importance to eccentricity and vh the only variable that showed a consistent relevance among the three classifiers was the waterbody area shape indices that consistently showed low relative importance were shape roughness pfd circle and rectangularity likewise vv blue and swir2 information consistently showed low influence on the classification results overall the geometric attributes were stronger predictors compared to multispectral predictors only svm showed relatively higher importance scores for spectral information i e green red nir and swir1 reflectance in summary cart relied mostly on shape indices rf used both shape and backscatter variables and svm used shape reflectance and backscatter information for water polygons classification the unsupervised classification results also revealed that the shape indices were better in discriminating water polygons by computing the proportion of the total variance of each predictor variable explained by the two clusters i e r 2 the top five indices were solidity sqp ipq pfd and convexity for both sets of water polygons excepting pfd these predictors all have r 2 0 5 are included in the set of predictors showing relative importance greater than zero when using cart 3 5 predictive performance of the proposed framework for aquaculture waterbody detection compared to water detection using only the final water mask from stage 1 please see ensemble row in table 3 the proportion of aquaculture waterbodies detected by incorporating the classifiers was reduced by about 20 table 6 however this reduction in aquaculture waterbody detection varied across the different districts with lower reductions within the worst performing districts i e barisal bhola and gopalganj which contain lots of small homestead ponds with tree cover as presented in table 6 cart and svm showed the highest detection performance among the different classifiers however they mostly classified all the water polygons as aquaculture waterbodies including highly irregular polygons that are closely identified as natural waterbodies or crops by visually inspecting high resolution multispectral imagery figs 4 and 5 on the other hand rf and k medoids generated a more balanced classification at the expense of lower true positive detection rates since cart svm and rf resulted in similar training performance and unsupervised classification was similar to rf we used these four classifiers to obtain an ensemble this ensemble classified a water polygon as an aquaculture waterbody if two or more individual classifiers detected the same polygon as an aquaculture waterbody we did not consider lr in the ensemble given its poor performance during stage 3 as a result the ensemble showed a better performance than rf and k medoids when comparing the predictions with the gtd while maintaining a consistent behavior based on visual inspection figs 4f and 5f we also compared the classification results when using water polygons obtained from different types of imagery i e sar and multispectral see table 7 it is worth noting that the sar image used for segmentation corresponded to an annual average whereas the ndwi image was a two week composite during the post monsoon season therefore the sar image accounted for temporal variation in backscatter values that facilitated the identification of dry borders still note that visual results for industrial style aquaculture waterbodies are very similar for both sar and ndwi images considered in this study see the bottom left region in figs 4 and 5 whereas the differences are more evident within agricultural land in general water polygons derived from the ndwi scene resulted in higher detection rates compared to the sar scene table 6 likewise proportions of gtd total area were higher when using the ndwi scene this occurred because the ndwi based water polygons were generally bigger and less segmented than the sar based water polygons figs 4 and 6 therefore using ndwi based water polygons made it more difficult to identify smaller aquaculture waterbodies located within large agricultural areas only the satkhira district showed better results when using sar based water polygons compared to the ndwi scene this district is characterized by large continuous areas classified as water during stage 1 with more than half of its surface covered by water see section 3 1 4 discussion 4 1 using both sar and multispectral imagery for aquaculture waterbodies detection previous studies on aquaculture waterbody detection have mainly used multispectral and sar imagery separately based mostly on unsupervised thresholding methods sar imagery has been preferred over multispectral imagery in several cases given its increased temporal availability and higher spatial resolution compared to some publicly available multispectral datasets notwithstanding when used together multispectral imagery has been typically employed for generating static water masks while sar imagery has been used to quantify the temporal variability of detected water bodies regardless of the imagery type geometry attributes have been commonly used as explanatory variables when using object based classification approaches luo et al 2022 sun et al 2020 yu et al 2020 in this study we found that existing approaches for aquaculture waterbody detection can be improved when 1 generating ensembles for water detection accounting for individual results from both multispectral and sar data 2 using sar and multispectral imagery for feature segmentation purposes and 3 incorporating backscatter data to train machine learning classifiers and inform unsupervised methods regarding water detection we showed an improvement in true positive rates using the gtd of aquaculture waterbodies as a reference in effect we observed at least an overall 4 increase in true positive rates when using the ensemble water mask compared to the individually generated masks table 3 moreover the ensemble mask indicated that different individual masks contributed differently across districts to attain water detection rates ranging from 18 to 87 individually each mask had detection rates ranging from 0 to 86 following the main findings by ferriby et al 2021 we opted for using scenes collected in a short time window i e two week during the post monsoon season as a compromise between cloud coverage effects and the wide ranging presence of operating aquaculture waterbodies in the study area since we were interested in contrasting sar and multispectral imagery performance when detecting water at stage 1 we did not use a multi temporal approach which is considered a limitation of our proposed framework therefore we recommend exploring the incorporation of multi temporal water detection using ensemble schemes in future work with respect to feature segmentation we found that both sar and multispectral imagery can be employed for this purpose using edge detection based on convolution filters when using sar data we found that an average composite calculated over a water year resulted in more segments with smaller feature areas compared to those obtained using an ndwi average scene computed over a much shorter period i e two weeks average sar imagery accounted for temporal variability of waterbodies and reduced speckle effects typically found in individual sar scenes prasad et al 2019 however in both cases we found important limitations when segmenting water within agricultural and forested environments where we obtained either very irregular and interconnected patches or very large water polygons these limitations are attributed mainly to the spatial resolution of the datasets i e 10 m and vegetation interference we also acknowledge that more sophisticated edge detection methods are needed to improve the segmentation results from stage 2 note that the application and comparison of different edge detection approaches were beyond the scope of this study finally sar data also played a critical role in model training and prediction during stage 3 for instance while vv polarization values were used for both water detection and feature segmentation vh polarization values showed a high relative importance in two machine learning classifiers i e svm and rf when using rf vh values showed a very high relative importance compared to other geometric and spectral attributes except for eccentricity which discriminates between elongated and squared shapes it is worth noting that most of the top explanatory factors in terms of relative importance across the different tested methods corresponded to shape indices however in some cases the prediction results failed in better discriminating between aquaculture waterbodies and other waterbodies as revealed by visual inspection across the study area this occurred when using classifiers i e cart svm that did not provide significant importance to vh values in relation to other shape indices or spectral data therefore we recommend future studies explicitly consider water indices as explanatory variables instead of reflectance values as well as other metrics such as the vv vh ratio in addition it is important to evaluate the relevancy of nonlinear interactions for waterbodies classification texture metrics derived from both multispectral and sar imagery might also help to improve classification outcomes 4 2 factors affecting the performance in aquaculture waterbodies detection results for stage 1 showed that the detected water extension was generally lower when using sar data compared to water index based extraction this difference meant an under detection of water bodies using c band sar imagery and highlighted the importance of considering near infrared reflectance i e nir and swir bands related to soil moisture poor soil moisture detection by c band sar imagery is indicative of vegetation interference preventing the microwave signal from reaching the soil el hajj et al 2019 in fact vegetation interference partially explains limitations in water detection in cropland and trees land cover classes as presented in table 4 it is worth noting that we only tested the vv polarization in c band sar imagery for water mask extraction therefore testing other polarizations and band combinations is recommended for future studies likewise limitations in feature segmentation during stage 2 also impacted the overall aquaculture waterbody detection highly large irregular polygons resulting from the 10 m spatial resolution and edge detection approach prevented the identification of small waterbodies also when removing very large water polygons 40 ha and those intersecting the river mask obtained from stage 1 some potential aquaculture waterbodies that are adjacent to major rivers and within agricultural forested and urbanized areas were affected this was especially the case when using sar imagery and was more evident in districts largely covered by surface water such as satkhira khulna bagerhat and gopalganj see fig 6b e g f meanwhile water polygons resulting from sar imagery tended to have smaller surface areas i e thicker borders than those obtained from the ndwi scene e g fig 6a however ndwi derived borders were generally poorly defined resulting in potentially higher false negative rates e g fig 6f additionally smaller aquaculture waterbodies i e surface areas 0 04 ha were hardly or not identified due to the dataset spatial resolution and vegetation interference especially in forested areas see fig 6b c d in general the edge detection approach used in this study yielded multiple irregular shapes that affected the overall aquaculture waterbody detection this problem is not exclusive to edge detection results but is also found when using thresholding methods directly for feature segmentation in those cases some authors have applied morphological operations e g dilation erosion to smooth irregularities at the expense of reducing the identification of smaller waterbodies duan et al 2020b other authors have proposed using either polygon border simplification or convex rectangular hulls ottinger et al 2022 polygon irregularity resulting from feature segmentation is problematic because the gtd polygons we used during model training were in contrast very regular therefore the transferability of the training outcomes to prediction was affected by the apparent disconnection between the data we used during stage 3 and the features we obtained from stage 2 for instance we passed from accuracy of 79 for rf during validation to 35 and 24 when using the ndwi and sar based water polygons in prediction mode respectively consequently we found no warranty that good performances during model training and validation imply a good performance during model prediction for this reason we recommend that gtd collection be as consistent as possible with feature segmentation outcomes to maximize information content and improve the reliability of the overall performance evaluation process 5 conclusions in this study an object based framework integrating sentinel 1 and 2 imagery was developed to improve the detection of aquaculture waterbodies in southwest bangladesh we employed both sar and reflectance data for water detection feature segmentation and waterbodies classification we combined water masks obtained from microwave backscatter values water indices and transformed reflectance data into a majority vote ensemble as a result we improved water detection rates concerning individual water masks results with overall rates of 60 and up to 87 in individual districts likewise by obtaining an ensemble of multiple supervised and unsupervised classifiers we obtained a more reliable detection of aquaculture waterbodies across the study area moreover we found that c band sar vh information and shape indices such as eccentricity played important roles in better differentiating waterbodies according to variable importance results on the other hand limitations in water detection were mainly dictated by the dataset s spatial resolution and vegetation interference particularly we obtained poor results when targeting small aquaculture waterbodies i e surface areas 0 04 ha and isolated waterbodies in agricultural and forested areas this may result from the fact that 200 m length was used as the initial user provided threshold meanwhile shortcomings in feature segmentation which mostly resulted from poorly defined borders or highly irregular water polygons affected the transferability of well performing supervised classification results into final predictions i e overall accuracies up to 79 for validation to 24 35 for prediction therefore for future work we recommend performing ground truthing data collection in accordance with preliminary feature segmentation outcomes likewise further efforts should be addressed to develop better performing feature segmentation techniques incorporate multi temporal information test additional explanatory factors e g texture metrics and take advantage of higher resolution datasets declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the feed the future innovation lab for fish is managed by mississippi state university through an award from the united states agency for international development usaid award no 7200aa18ca00030 m lawrence pi and provides support to this project grant no 193900 312455 12 b belton pi haque pi this work was also supported by the usda national institute of food and agriculture hatch project 1019654 
25524,complex physical models are the most advanced tools available for producing realistic simulations of the climate system however such levels of realism imply high computational cost and restrictions on their use for policymaking and risk assessment two central characteristics of climate change are uncertainty and that it is a dynamic problem in which international actions can significantly alter climate projections and information needs including partial and full compliance of global climate goals here we present aircc clim a simple climate model emulator that produces regional probabilistic climate change projections of monthly and annual temperature and precipitation as well as risk measures based both on standard and user defined emissions scenarios for six greenhouse gases aircc clim emulates 37 atm ocean coupled general circulation models with low computational and technical requirements for the user this standalone user friendly software is designed for a variety of applications including impact assessments climate policy evaluation and integrated assessment modelling keywords climate change scenarios climate model emulator impact vulnerability and adaptation assessment stochastic simulation data availability data will be made available on request 1 software availability aircc clim can be downloaded at no cost from https sites google com view aircc lab airccclim aircc clim 2 introduction climate change projections are one of the main inputs for assessing the potential consequences of different socioeconomic development pathways and international climate policy on natural and human systems due to the complexity of the systems involved and their interactions climate projections are inherently uncertain gay and estrada 2010 curry and webster 2011 knutti and sedláček 2012 moreover computational and technical costs of state of the art physical models allow exploration of only a small fraction of the range of possible climate futures and hinder assessing risk through probabilistic scenarios knutti et al 2010 sanderson et al 2015 for most decision makers and researchers these costs make it infeasible to explore how current and hypothetical changes in international mitigation policy can influence future warming and its consequences for society in a time of proactive international mitigation policy the dynamic nature of projecting future climate becomes even more evident and decision making requirements can go beyond fixed illustrative emissions scenarios such as the rcps estrada and botzen 2021 for example nationally determined contributions ndcs that represent greenhouse gas emission reductions that countries promise as their contributions to the paris agreement are currently a key focus of international climate policy moreover due to the nonlinearity of most climate impacts even small deviations from a high warming trajectory can produce large changes in the associated impacts estrada and botzen 2021 ignjacevic et al 2021 international efforts such as the coupled model intercomparison project cmip which build and host large databases of climate models simulations publicly available have significantly contributed to improving the accessibility and utilization of climate scenarios for the wider research community and decision makers knutti and sedláček 2012 stocker et al 2013 taylor et al 2012 however many users still face the problem of processing large datasets for adapting them to their particular needs e g temporal frequency and spatial domains as such studies and decisions are commonly based on a few illustrative greenhouse gases emissions trajectories and a handful of climate models simulations which are often selected due to their availability and ease of use such as worldclim fick and hijmans 2017 such a selection of climate model runs can hardly provide a good representation of uncertainty and indicate if a model s projections for a given region may represent extreme realizations in comparison to the majority of other models weigel et al 2010 sanderson et al 2015 even in cases when climate models performance is assessed the resulting selection of models does not guaranty that those projections of future climate are reliable altamirano del carmen et al 2021 climate model selection remains an unresolved problem as commonly used metrics can be non informative about the models ability to reproduce the observed climate change signal and indicate much less about their ability for projecting future climate knutti et al 2010 uncertainty is a key characteristic of climate change and how it is understood and included in climate impact assessments can have profound effects on the estimates of the consequences of this phenomenon and on the design of policies to address these consequences ipcc tgica 2007 gay and estrada 2010 the development of tools and methods for better uncertainty management and for improving the usefulness of the large thousands of terabytes databases that are currently available are increasingly relevant research topics more efficient simple and flexible approaches for taking advantage of the available databases can transform data into useful information and knowledge for better assessments of impacts risks and climate policy options reduced complexity models and emulators of more advanced models allow exploring at low computational and technical costs for the user a wide range of possible futures and emissions trajectories parameterizations as well as probabilistic assessment of risks for natural and human systems meinshausen et al 2011a blanc 2017 estrada et al 2020 integrated assessment modelling benefits from such models and emulators to provide tools for supporting decision making and providing estimates for cases in which complex climate impacts model runs are not available a notable example of the usefulness of such models is the magicc software which has made significant contributions to climate change research particularly in impact vulnerability and adaptation iva assessments and integrated assessment modelling wigley 1995 meinshausen et al 2011c a the magicc software has been regularly used in the ipcc reports when simulations produced by general circulation climate models are not available and also in the national climate change assessments of several countries conde et al 2011 ipcc 2018 2021 magicc6 is able to produce probabilistic global temperature change projections and to capture uncertainties in radiative forcing as well as in the climate and carbon cycle responses nicholls et al 2021 the magicc7 version is now on line available https magicc org here we present aircc clim assessment of impacts and risks of climate change probabilistic climate model emulator a simple and flexible climate model emulator for producing probabilistic regional projections of monthly and annual temperature and precipitation as well as risk measures aircc clim has five main innovations compared to other reduced complexity models first it emulates the results from 37 atm ocean coupled general circulation models from cmip phase 5 cmip5 included in the fifth assessment report of the ipcc stocker et al 2013 with low computational and technical requirements on the user second it produces spatially explicit 2 5 2 5 climate projections which allows to explore climate change developments at the regional level which can be visualized on maps third it generates global and spatially explicit probabilistic projections of climate change which allows to represent uncertainty about future climate conditions at different spatial scales fourth the model allows constructing a variety of risk measures tailored to the needs of the user such as the time for exceeding predefined thresholds of changes in climate variables and probabilities of exceedance fifth aircc clim includes a graphical emissions editor to facilitate introducing new emissions trajectories defined by the user additionally this editor can read external emissions scenarios from excel files and it also allows the user to directly modify the emissions scenarios in a table included within the interface another advantage that enables easy integration with other software applications is the capability of exporting results in netcdf and geotiff formats in addition to high quality png files it is a standalone software for windows and linux operating systems that requires no programming or advanced technical skills from the user and runs on computers with standard memory and processing resources aircc clim is designed for a variety of applications including iva assessments integrated assessment modelling and the quick evaluation of the consequences on global and regional climate of user defined experiments of international mitigation the remainder of this paper is structured as follows section 2 describes the aircc clim model structure and describes in detail each of its modules in terms of data sources modelling approaches and methods and their output this section also describes the file options for exporting climate projections and their characteristics an application of the model is shown in section 3 in which the benefits of stringent international mitigation efforts are illustrated in terms of avoided warming changes in precipitation and risk reduction section 4 concludes and discusses model extensions and integration with iva models 3 model structure data and methods aircc clim is composed of four main modules greenhouse gas emission scenario editor global climate models a regional scenario generator and a climate risk index generator fig 1 in the following paragraphs each module is discussed including detailed descriptions of the modelling approaches methods and data sources 3 1 emissions scenario editor aircc clim offers a graphic interface for constructing user defined global emissions scenarios for six climate changing substances fossil and industrial carbon dioxide co2 in mtc yr methane ch4 in mtch4 yr nitrous oxide n2o mtn2o n yr chlorofluorocarbons cfc11 cfc12 in kt yr and sulfur hexafluoride sf6 in kt yr four rcp emissions trajectories are included by default and the user can select one of them as starting point for editing as shown in fig 2 aircc clim has three input options for global emissions 1 by means of editing the values displayed on a table 2 by selecting and plotting the substance of interest and modifying the emissions trajectories directly in a graph 3 loading an excel file xlsx with user defined emissions trajectories for each gas in a predefined format see the aircc clim user guide included in the si the modified emissions scenario is saved in an excel xlsx file and used for generating the corresponding climate projections in aircc clim 3 2 global climate projections global temperature projections in aircc clim can be generated by running a simple climate model as well as by using precalculated projections from two reduced complexity climate models as described in the following paragraphs in all cases probabilistic projections are constructed by means of stochastic simulation to represent uncertainty in the climate sensitivity cs parameter 3 2 1 modified schneider thompson model the schneider thompson st simple climate model schneider and thompson 1981 includes three components that allows to calculate the atmospheric concentrations of greenhouse gases the corresponding change in radiative forcing and the resulting increase in global temperature due to its flexibility and low computational cost this climate model and modified versions of it is used in some of the most popular integrated assessment models such as fund and rice nordhaus and boyer 2003 anthoff and tol 2014a the version of the st model in aircc clim builds upon that used in the fund integrated assessment model tol and fankhauser 1998 which is available at http www fund model org in this section the st model and the modifications that are introduced in aircc clim as described below several aircc clim s parameters are calibrated using magicc6 output meinshausen et al 2011b as well as the best estimates and likely ranges included in the ipcc s fifth assessment report ar5 the magicc6 model was calibrated with climate projections from different general circulation models included in cmip5 meinshausen et al 2011c a calculation of atmospheric concentrations and radiative forcing a five box model maier reimer and hasselmann 1987 tol 2019a is used to convert annual emissions of co2 mtc into atmospheric concentrations ppm the initial value for co2 concentrations is 278 ppm which represent preindustrial times circa 1750 the carbon cycle model is represented by the following equation 1 c i t c o 2 1 α i c i t 1 c o 2 γ i β e t c o 2 2 c t c o 2 i 1 5 c i t c o 2 where c i t c o 2 represents the atmospheric concentrations of co2 in box i 1 5 at time t e t are the co2 emissions at time t α i determines how long co2 remain in box i while γ i is the proportion of emissions that enter box i and β is a scale parameter β 0 00045 the α i and γ i parameters are taken from the literature maier reimer and hasselmann 1987 hammitt et al 1992 tol 2019b while the β is a calibration parameter obtained by minimizing the sum of squares of the differences between the magicc6 concentrations reported in meinshausen et al 2011b and those obtained from equations 1 and 2 under the rcp8 5 scenario the co2 emissions in aircc clim correspond to those of fossil and industrial emissions these boxes are characterized by different decay times that resemble the slow and fast components of the carbon cycle however these boxes do not represent physical processes and are just simple mathematical devices that allow to approximate the observed concentrations schneider and thompson 1981 maier reimer and hasselmann 1987 tol 2019a c t c o 2 is the total concentration of co2 in the atmosphere at time t parameter values are reproduced in table s1 we use a common parameterization for co2 radiative forcing meinshausen et al 2011a 3 f t c o 2 5 35 l n c i t c o 2 c p r e c o 2 where c p r e c o 2 278 represents the preindustrial atmospheric concentrations of co2 ch4 concentrations are calculated using the following equation 4 c t c h 4 1 a 1 c t 1 c h 4 a 1 c p r e c h 4 b 1 e t c h 4 where c t c h 4 are the atmospheric concentrations of ch4 at time t c p r e c h 4 721 89 is the preindustrial concentrations of ch4 e t c h 4 are the emissions of ch4 at time t a 1 1 12 with 12 years representing the permanence of ch4 in the atmosphere stocker et al 2013 anthoff and tol 2014b and b 1 0 2954 is a scaling factor and was obtained by minimizing the sum of squared differences between the magicc6 concentrations and those obtained from equation 4 under the rcp8 5 scenario the atmospheric concentrations of n2o are calculated using an equation similar to that of ch4 5 c t n 2 o 1 a 2 c t 1 n 2 o a 2 c p r e n 2 o b 2 e t n 2 o where the emissions and atmospheric concentrations of n2o are denoted by e t n 2 o and c t n 2 o respectively while c p r e n 2 o 272 96 a 2 1 120 with 120 years representing the persistence of the gas in the atmosphere stocker et al 2013 tol 2019b and b 2 0 1550 obtained by minimizing the sum of squared differences between the magicc6 concentrations and those obtained from equation 4 under the rcp8 5 scenario calculating the radiative forcing of ch4 and n2o involves an interaction term between these gases to account for their overlap in the absorption bands as represented in the following equations 6 i n t t c h 4 f m n 0 f m 0 n 0 7 i n t t n 2 o f m 0 n f m 0 n 0 where these interaction functions are given by 8 f m n 0 p 1 l n 1 p 2 c t c h 4 c t 0 n 2 o 0 75 p 3 c t c h 4 c t c h 4 c t 0 n 2 o 1 52 9 f m 0 n p 1 l n 1 p 2 c t 0 c h 4 c t n 2 o 0 75 p 3 c t c h 4 c t 0 c h 4 c t n 2 o 1 52 10 f m 0 n 0 p 1 l n 1 p 2 c p r e c h 4 c t 0 n 2 o 0 75 p 3 c p r e c h 4 c p r e c h 4 c t 0 n 2 o 1 52 11 f m 0 n 0 p 1 l n 1 p 2 c t 0 c h 4 c p r e n 2 o 0 75 p 3 c p r e n 2 o c t 0 c h 4 c p r e n 2 o 1 52 with p 1 0 47 p 2 2 01 10 5 p 3 5 31 10 15 ramaswamy et al 2001 anthoff and tol 2014b tol 2019b the radiative forcing of ch4 and n20 is calculated as 12 f t c h 4 0 036 c t c h 4 0 5 c p r e c h 4 0 5 i n t t c h 4 13 f t n 2 o 0 12 c t n 2 o 0 5 c p r e n 2 o 0 5 i n t t n 2 o cfc11 and cfc12 concentrations are calculated as follows 14 c t c f c 11 1 a 3 c t 1 c f c 11 b 3 e t c f c 11 15 c t c f c 12 1 a 4 c t 1 c f c 12 b 4 e t c f c 12 in which a 3 and a 4 are equal to 1 45 and 1 100 with 45 and 100 years representing the permanence of cfc11 and cfc12 in the atmosphere stocker et al 2013 tol 2019b while b 3 0 0423 and b 4 0 0481 were obtained by minimizing the sum of squared differences between the magicc concentrations reported in meinshausen et al 2011b and those obtained from equations 14 and 15 under the rcp8 5 scenario respectively the radiative forcing from cfcs is calculated by multiplying it by a scaling factor equal to 0 25 1000 in the case of cfc11 and 0 32 1000 for cfc12 ramaswamy et al 2001 anthoff and tol 2014b tol 2019b finally the concentrations of s f 6 are obtained using the following equation 16 c t s f 6 1 a 5 c t 1 s f 6 a 5 c p r e s f 6 b 5 e t s f 6 where a 5 1 3200 with 3200 years as the permanence of s f 6 in the atmosphere forster et al 2007 and b 5 0 0393 was obtained by minimizing the sum of squared differences between the magicc concentrations and those obtained from equation 16 under the rcp8 5 scenario the radiative forcing of s f 6 is calculated as f t s f 6 0 00052 c t s f 6 ramaswamy et al 2001 tol 2019b as shown in fig s1 the equations and parameterizations used for calculating the concentrations and radiative forcings from the greenhouse gases included in aircc clim can closely approximate those produced with magicc6 in all cases the goodness of fit from running a regression between the output from aircc clim and magicc6 under the rcp8 5 scenario produces r 2 values larger than 0 99 fig s1 fig s2 shows the aircc clim and magicc6 concentrations and radiative forcing for the rcp8 5 rcp6 rcp4 5 and rcp2 6 for individual greenhouse gases and for the sum of their radiative forcing the average differences in the sum of radiative forcing from the aircc clim and magicc6 output are 2 7 6 7 for the rcp8 5 rcp6 rcp4 5 and rcp2 6 respectively calculation of global temperature change the st model uses the following two interlinked equations to produce annual mean global air and ocean surface temperatures 17 t t a t t 1 a λ 1 λ 2 f t t t 1 a λ 3 t t 1 o t t 1 a 18 t t o t t 1 o λ 4 t t 1 a t t 1 o following tol 2019a we use the following parameter values as the initial calibration λ 1 0 0256 λ 2 1 14891 λ 3 0 00738 and λ 4 0 00568 the cs of this model is calculated as λ 2 5 35 ln 2 and it is the main parameter for calibrating the model to reproduce observed or projected annual mean global air surface temperature a limitation with this approach is that the accuracy of its projections will depend on which temperature series is used for calibration to illustrate this we use the projections obtained from the magicc6 model for the rcp8 5 rcp6 rcp4 5 and rcp2 6 for the period 2005 2100 these simulations are obtained using the total radiative forcing factors not only the 6 gases described above for parameter calibration the temperature change projections of the st model are calculated using the same total radiative forcing used to produce those of magicc6 the calibration procedure used was to minimize the sum of the squared residuals between the st and the magicc6 projections using ordinary least squares the optimal cs values that calculated for each scenario are 3 27 rcp8 5 2 70 rcp6 2 64 rcp4 5 and 2 23 rcp2 6 choosing any single value of this parameter to project all the rcp scenarios would lead to over or underestimating future temperatures particularly in the case of extreme scenarios rcp2 6 and rcp8 5 as such this approach could have an impact on for example the evaluation of the benefits of mitigation policies the level of warming is closely related to the total cumulative emissions of co2 tcre fig s3 the relationship between global temperature change and tcre is relatively constant over time and independent from the emissions scenario used but it is dependent on the global climate model cs and transient climate response collins et al 2013 fig s3 shows the differences in the temperature change projections of magicc6 and the st as a function of total cumulative emissions of co2 from different rcp scenarios in this figure the st projections are shown for when the cs is fixed and calibrated for a particular rcp and for when the cs is allowed to vary across rcp scenarios when the cs is calibrated using one of the rcp scenarios and it is used for projecting temperature change for the other rcp scenarios the differences between the st and the magicc6 projections can be as large as 1 55 c these differences are much smaller when the cs value is allowed to change for different rcps the mean and maximum differences are 0 02 c and 0 17 c respectively which suggest that the emulating capacity of the st model could be greatly improved if the cs value is allowed to vary across cumulative emissions simple climate models such as st are based on extremely stylized representations of the climate system and necessarily ignore several relevant aspects of for example climate feedbacks and other processes that determine the responses to changes in external forcing and thus the cs while the cs in the climate system and in complex climate models is an emergent property in reduced complexity climate models is a parameter to be calibrated the st ignores many climate feedbacks and other radiatively active compounds which also contributes to the differences in temperature projections obtained with magicc6 a sensible calibration of the cs can help compensate the omissions in simple models and increase their ability to reproduce the output of more complex models we stress that the cs specification in aircc clim is as a useful device for approximating more closely temperature change projections for the range of rcps projections but that the parameterization of the cs should not be interpreted as a physical statement about how the climate system operates fig 3 shows a linear relationship between cumulative co2 emissions and the cs value the st model requires to better approximate the temperature change projections from magicc6 given this relationship we propose a dynamic cs parameter for the st model that is calculated based on the cumulative co2 emissions which are dominant in any rcp sres or other realistic emissions scenario the proposed parameterization for the cs has the practical purpose of increasing the emulation capacity of the st model and to fully exploit the ability of simple models to approximate the results from much more complex ones with only a small fraction of computing effort while the proposed dynamic cs is only a practical device to increase the emulation capacity of the model there is increasing evidence that the climate system s feedbacks are dependent on forcing magnitudes and timescales and thus the cs may be not a constant value both in reality as in complex climate models knutti and rugenstein 2015 in the proposed dynamic cs this parameter is calculated prior to projecting global temperatures with the st model the regression equation that relates the cs and the cumulative co2 emissions is c s 2 05 6 18 10 7 c o 2 r 2 0 96 fig 3 to get a better approximation for the lowest and highest cumulative scenarios we also fitted a line based on the cs values for rcp2 6 and the rcp8 5 the results were compared to the projections in the ipcc s ar5 and were optimized to reproduce the best estimates stocker et al 2013 the final extreme cs values are 2 1 c and 3 3 c for the rcp2 6 and rcp8 5 scenarios respectively the final equation relating cs and cumulative co2 emissions is 19 c s 1 85 7 51 10 7 c o 2 the cs values are restricted to the interval 2 1 3 3 table 1 shows the projections of the modified st model and the best estimates and likely ranges in the ar5 collins et al 2013 for all four rcp emissions scenarios and both short and long term horizons the average difference is about 0 08 c and the maximum 0 23 c this illustrates that the modified st model very closely resembles the ipcc ar5 temperature projections the differences in projections between magicc6 and aircc clim are in part due to the omission of other radiative active compounds as the latter only considers 6 greenhouse gases the differences in the 2046 2065 horizon are slightly larger and may be related to the omission of aerosols and land use co2 emissions which have a more significant influence in the shorter term to extend the st model to account for the uncertainty in cs values and to produce probabilistic projections we represent the model s cs with a triangular distribution the model output of global temperature change projections is affected by the distribution chosen for the cs parameter our choice to represent the cs with a triangular distribution is common in the integrated assessment literature such as in the page and climrisk models moore et al 2018 estrada and botzen 2021 note that other probability distributions and parameterizations could be used to explore different assumptions about the cs gay and estrada 2010 in this distribution the most likely value is given by equation 19 above and the lower and upper limits are the calculated cs value plus minus some constants that are to be calibrated the equations for the upper c s h i g h and lower limits c s l o w of the triangular distribution for cs are 20 c s l o w c s h 1 21 c s h i g h c s h 2 the h 1 and h 2 constants are calibrated to match the likely ranges reported in the ipcc s ar5 simulations of 10 000 realizations were used and the 5th and 95th percentiles were chosen to construct the st likely ranges the parameter values that provided good fit for all rcps are h 1 1 1 and h 2 1 5 table 1 shows that the likely ranges of the modified st model closely reproduce those reported in the ipcc for both short and long term horizons showing almost exact overlap with differences in upper and lower limits typically smaller than 0 2 c 3 2 2 generating probabilistic global temperature projections using precalculated runs from magicc6 and the thermodynamic climate model the magicc6 and the thermodynamic climate model tcm are reduced complexity climate models that were designed for different objectives magicc is composed of a set of coupled models that include gas cycles and climate and ice melt models designed to explore the effects of anthropogenic emissions of greenhouse gas concentrations radiative forcing and changes in global mean annual temperature and sea level wigley 1995 meinshausen et al 2011c a the tcm was originally conceived as a weather forecast model for the northern hemisphere and then extended to the global scale for studying the climate of earth and of other planets in the solar system adem 1991 in the case of magicc6 precalculated projections of global temperature are included in aircc clim for the emissions scenarios rcp2 6 rcp4 5 rcp6 and rcp8 5 while in the case of the tcm the precalculated that were available are the rcp4 5 rcp6 and rcp8 5 to account for the uncertainty in cs values and to produce probabilistic projections we propose a simple method based on linear regression and statistical simulation due to the availability of climate models output we base our calculations on magicc6 projections for each rcp scenario were obtained using of magicc6 for three different cs values that represent medium cs 3 0 c low cs 1 5 c and high cs 4 5 c to emulate magicc s results for high and low climate sensitivities we propose the use of some scaling weights w that would approximate them when multiplied by the global temperature obtained using a medium cs value such weights can be obtained by means of the following regression 22 t t s e n s ω t t m e d i u m ε t where t t s e n s is the global temperature projection obtained with magicc6 using either low or high cs while t t m e d i u m corresponds to the global temperature projection obtained with medium cs ω is the slope parameter and ε t are the regression residuals in all cases the r 2 is higher than 0 99 note that the objective of these models is not to make inference about parameter values but just to produce a close fit of projections using different values of cs the values of parameter ω for the different combinations of cs and for each rcp emissions scenario are provided in table 2 the estimated parameter values are very similar for different rcps with an average value of 1 34 and 0 57 for high and low cs values respectively as shown in fig 4 these values allow to closely approximate the simulations of magicc6 produced using high low cs values by scaling a simulation of the same model calculated with medium sensitivity the approximation is less precise when used on scenarios that lead to stabilization fig 4d but the error is still very small 0 17 c for the high cs projection using the rcp2 6 to produce probabilistic scenarios using the st model we use a triangular distribution to scale the magicc6 runs obtained with a medium cs the parameters of the triangular distribution are 1 for the mode or most likely value and 0 5679 and 1 3414 for the lower and upper limits respectively table 2 these parameter values allow to emulate the projections that would be obtained with magicc6 randomly choosing cs values contained in the interval 1 5 c to 4 5 c assigning a larger probability of occurrence to values that are closer to the medium sensitivity of 3 c cs is high uncertainty rogelj et al 2014 lewis and curry 2015 freeman et al 2015 tan et al 2016 friedrich et al 2016 knutti et al 2017 cox et al 2018 however the consensus is that the cs is probably within the 1 5 c to 4 5 c interval with a central estimate of 3 c bony et al 2013 callendar 1938 freeman et al 2015 santer et al 2019 stocker et al 2013 this cs range encompasses the range produced by the state of the art models included in the cmip5 jonko et al 2018 stocker et al 2013 given that in the tcm the cs is not an explicit parameter as in magicc6 but an emerging property of the model there are no high low cs projections that can be used to find the corresponding parameters in table 2 as such we apply the same scaling parameters we found for magicc6 to represent uncertainty in cs values and to extend the tcm projections to be probabilistic 3 3 regional climate projections pattern scaling is a technique for producing regional scenarios based on the robust finding documented in the literature of stationarity in geographical patterns of change in some climate variables in particular temperature and precipitation santer et al 1990 collins et al 2013 tebaldi and arblaster 2014 these patterns can be scaled by global temperature change to produce regional climate change scenarios in a simple and computationally efficient manner that provide a useful approximation to those produced by complex global climate models the performance of pattern scaling techniques has been evaluated in the literature in several occasions mitchell 2003 cabré et al 2010 collins et al 2013 tebaldi and arblaster 2014 herger et al 2015 kravitz et al 2017a zelazowski et al 2018 tebaldi and knutti 2018 osborn et al 2018 these techniques produce adequate approximations for variables such as annual seasonal temperature and precipitation and other variables excluding extreme events and time scales in which natural variability is dominant these patterns are also adequate for most of the emissions scenarios including low warming policy trajectories and most regions except where local forcing is strong and time varying collins et al 2013 estrada and botzen 2021 an example of the latter are scenarios that involve very high aerosols emissions the reason is that the modelling approach does not capture the effects of regional cooling by aerosols or land surface feedbacks due to conditions that not reflected by the library of scaling patterns the pattern scaling technique can be described as follows collins et al 2013 tebaldi and arblaster 2014 estrada and botzen 2021 23 p i j t e y s t t e p i j y s ξ i j t s where i j denote longitude and latitude respectively and t is time e represents the emissions scenario y is the climate variable of interest s is the time of the year for which the scenarios is constructed annual month season and t t is the global annual mean temperature change at time t under the emission scenario e p is the projected field of change for variable y obtained using a complex climate model while p is the time emissions scenario invariant spatial pattern of change per 1 c change in annual global mean temperature for the climate variable y ξ i j t s represents is an error term due to both natural variability and the limitations of the pattern scaling methodology collins et al 2013 estrada and botzen 2021 the patterns p i j y s were calculated for monthly and annual air surface temperature and precipitation using simulations from a battery of coupled ocean atmosphere general circulation models table s2 under the rcp8 5 emissions scenario and that are available at the cmip5 data portal https esgf node llnl gov search cmip5 all simulations were bilinearly interpolated into a common grid with a spatial resolution of 2 5 2 5 the hodrick prescott filter hodrick and prescott 1997 was applied to the time series from each grid point from the climate models simulations to remove the effects of high frequency variability then temperature precipitation time series from each grid cell were regressed on the global mean temperature and the slope coefficients were stored as maps that represent the scaling patterns kravitz et al 2017b lynch et al 2017 for producing the regional climate change scenarios global mean temperature projections from section 2 2 are used to scale the patterns produced in this section the projections are expressed in c for changes in temperatures and in percentage of change for precipitation with respect to preindustrial conditions c 1750 aircc clim allows the user to select the scaling patters for any given climate model as well as to generate probabilistic scenarios combining all of them this stochastic version of aircc clim uses a uniform distribution which assigns the same probability to each of the climate models a variety of approaches for assigning probabilities to climate models output have been proposed in the literature xu et al 2010 knutti et al 2010 mendlik and gobiet 2016 but there is no agreement on which would be the best way of doing it knutti 2010 stephenson et al 2012 deser et al 2014 notz 2015 the uniform distribution follows the principle of insufficient reason which is the maximum entropy distribution in absence of any additional information jaynes 1957 jaynes et al 2003 gay and estrada 2010 other probability distributions based on performance evaluation or model dependence could be implemented however these distributions may be as arbitrary as assigning equal probabilities to each model and may lead to unjustified dismissal of uncertainty potter and colman 2003 gay and estrada 2010 altamirano del altamirano del carmen et al 2021 climate change scenarios for temperature and precipitation can be exported as geotiff and netcdf files for three time horizons 2030 2021 2040 2050 2041 2060 and 2070 2061 2080 maps of changes in temperature and precipitation for any year between 2005 and 2100 can be exported as high quality png and as netcdf files 3 4 climate risk index generator aircc clim uses the probabilistic nature of its projections to produce user defined risk measures based on thresholds the current version of the model includes two types of risk metrics probabilities of exceedance and the dates when the selected threshold is exceeded the threshold values are selected by the user to reflect his perceptions of risk and information needs note that due to the exporting capabilities of this software aircc clim s output can be combined with information external from the model such as population projections to indicate hotspots of severe climate change in populated areas the calculation of the risks indices is done following the same procedure as climrisk estrada and botzen 2021 first the indicator function is used to identify in which simulations and grid cells the threshold is exceeded in the case of changes in temperature t we have 24 i r t i j t s i m i t i j t s i m t where i r t i j t s i m is a four dimensional matrix in which i j are the longitude and the latitude that define the location of the gird cell t is time s i m is the number that identifies each of the realizations of the simulation experiment and t is the user defined threshold in c for cases in which the threshold t is exceeded the indicator function returns a value of 1 and zero otherwise in the case of the change in precipitation p the threshold of interest p can be positive or negative and thus the indicator function is applied as follows 25 i r p i j t s i m i p i j t s i m p i f p 0 i p i j t s i m p o t h e r w i s e i r p i j t s i m is a four dimensional matrix defines as above in which the indicator function returns a value of 1 if the threshold p is exceeded and zero otherwise estimates of the probabilities of exceeding the thresholds t and p can be obtained by summing over the s i m dimension 26 p i r t i j t p t i j t t s i m 1 n i r t i j t s i m n 27a p i r p i j t p p i j t p s i m 1 n i r p i j t s i m n i f p 0 27b p i r p i j t p p i j t p s i m 1 n i r p i j t s i m n i f p 0 where p i r t i j t and p i r p i j t are three dimensional matrices containing probability estimates the estimated probability maps can be exported as png and netcdf files for any year in the period 2005 2100 these probability estimates of exceeding critical thresholds are used to estimate the expected dates when such thresholds would be attained these dates can be computed as follows 28 i d t i j t i p i r t i j t γ 29 i d p i j t i p i r p i j t γ where the parameter γ represents the percentage of simulations that is required by the user to declare the threshold has been exceeded i d t i j t and i d p i j t are matrices in which the entries take the value 1 if the confidence level γ is attained or exceeded and zero otherwise in aircc clim and climrisk this value is called confidence level and can vary with the risk tolerance of different users the default value in aircc clim is 50 the estimated dates for exceeding the risk threshold are obtained by mapping into a year index the first occurrence of the value 1 in the time dimension of all gird cells in i d t i j t and i d p i j t the maps of the dates of exceedance can be exported as png and netcdf files table s3 provides a list of the climate risk indices and their definition 4 estimating the risks of delaying the implementation of deep mitigation efforts in this section we provide an example application of aircc clim in which the effects on climate from a high emissions trajectory rcp6 0 are compared to those of a deep mitigation scenario rcp2 6 consistent with the goals of the paris agreement and to those in which such mitigation effort is delayed ten years figure s1 shows the trajectories of c o 2 c h 4 and n 2 o for the rcp6 0 rcp2 6 and the modified version of the rcp2 6 scenarios in which mitigation is delayed for 10 years starting in 2020 the modified version of the rcp2 6 was edited in excel and directly loaded to aircc clim using the user defined option for emissions scenarios the rcp6 0 and the rcp2 6 scenarios produce contrasting results in terms of their effects on climate while the first produces a mean increase in global temperature of about 3 c with respect to preindustrial conditions at the end of this century and up to 4 c when the 95th percentile is considered the rcp2 6 limits warming below 2 c for the ensemble mean about 1 7 c although this limit is exceeded for the 95th percentile fig 5 the magicc model produces similar results with a mean increase in global temperature of 3 1 c and 1 6 c for the end of the century under the rcp6 0 and rcp2 6 scenarios respectively fig s2 the tcm produces a considerably larger mean increase for the rcp6 0 3 9 c but that still lies within the likely range of the cmip5 experiment table 1 the st simulations for the modified rcp2 6 in which the mitigation effort is delayed for 10 years show that the mean increase in global temperature reaches 2 c in 2100 and up to 3 c in the 95th percentile figs 6 and 7 show the changes in annual mean temperature and total annual precipitation at the grid cell level for the three emissions scenarios and two time horizons 2050 and 2100 temperature increase is much larger in high latitudes of the northern hemisphere due to the arctic amplification phenomenon pithan and mauritsen 2014 reaching about 6 c for the mid century and more than 8 c in 2100 under the rcp6 0 scenario see figs s3 and s4 for results using the precalculated magicc6 and tcm runs most of the continents would experience temperature increases in 2050 of about 2 c to 3 c and of about 4 c to 5 c in 2100 apart from the arctic region midlatitudes in north america and in eurasia would have the largest increases 5 c 6 c in temperature by the end of the present century followed by southern asia and the middle east north and south africa parts of the west coast of north america mexico and the amazonian region and the northern part of brazil large changes are also expected in precipitation under the rcp6 0 scenario with large increases in high latitudes the equatorial pacific ocean and some parts of the middle east and large decreases in the mediterranean the caribbean mexico and the southern part of the us as well as in southern parts of africa and america figs s3 and s4 the implementation of a deep mitigation effort consistent with the goals of the paris agreement would significantly limit these changes in climate under the rcp2 6 most of the arctic would not exceed a warming of 5 c during this century most continents would not exceed 3 5 c and precipitation change would be notably smaller aircc clim portraits the risks of climate change and the benefits of mitigation in a clearer way due to its probabilistic nature and its capacity to produce maps of probabilities and of dates of exceedance figs 8 and 9 show the probabilities of exceeding a 2 5 c increase in annual temperatures and a decrease of 15 in annual precipitation in 2050 and 2100 under the rcp6 0 by 2050 the probabilities of exceeding 2 5 c increase in annual temperatures are higher than 60 for most of the continents while the arctic region and the high latitudes of the northern hemisphere are virtually certain to exceed this threshold by mid century fig s5 these simulations also show that a 2 5 c warming would be exceeded in all continents by 2100 and that there is a high probability above 60 that this threshold would be crossed in all oceans except for parts of the southern hemisphere and the atlantic the probabilities of exceeding a decrease of 15 in total annual precipitation in 2050 are higher than 50 for several areas of the world such as the mediterranean parts of northern western and southern africa the caribbean and mexico fig s5 for 2100 these probabilities increase to more than 60 in those regions and extend to parts of south america and west australia if the emissions trajectory described in by the rcp2 6 is achieved by 2050 the probability of exceeding 2 5 c would be lower than 40 over the continents except for the arctic and parts of the midlatitudes in the northern hemisphere where the probabilities range from 60 to 100 fig 8 likewise the probabilities of exceeding decreases in precipitation of at least 15 are considerably smaller in comparison to the rcp6 0 however by the end of this century the probabilities of exceeding this threshold would be close to 50 for the southern region of spain parts of north and west africa morocco mauritania mali senegal sierra leone and guinea and about 40 for parts of the caribbean central and south america colombia and venezuela fig 9 fig 10 shows the dates when the 2 5 c threshold would be exceeded the default confidence level γ 50 was used for the estimates presented in this section the results show that under the rcp6 0 most of the planet except for part of the southern oceans and part of the north atlantic would exceed the 2 5 c threshold during this century and that some parts of the world already exceeded it fig s5 the date of exceedance for the arctic occurred during the 2000s while for much of the high latitudes in the northern hemisphere the middle east parts of south asia and west and south africa the exceedance is expected to occur in the 2020s 2030s the remainder of the continents and the antarctic region would go over this threshold during the period 2040 2060 and most of the oceans above the 20 s would exceed the 2 5 c threshold during this century the regions in which reductions of at least 15 in annual precipitation is exceeded are fewer and form well defined geographical patterns that cover the mediterranean parts of north west and south africa central america and the caribbean mexico and colombia and venezuela in south america as well as the west part of australia and part of the southern pacific ocean fig 10 and s5 the dates for exceedance on these regions are typically reached in the 2050 2060 decades although regions of spain and west africa could exceed this threshold as early as the 2040s achieving rcp2 6 would prevent exceeding these thresholds for most of the world during this century fig 10 however as mentioned above some regions such as the arctic and the high latitudes of the northern hemisphere already have exceeded the 2 5 c temperature threshold or will do so during the next decade regardless of the emission scenario that is selected for parts of the midlatitudes the rcp2 6 represents delaying reaching the 2 5 c threshold for about 20 years which buys time for adapting to the projected changes and to reduce risks and damages the occurrence of this threshold would also be delayed until 2060 in some parts of the sahara south africa the middle east and india exceeding decreases in precipitation of more than 15 would not occur during this century with the exception of a few grid cells in africa spain and central america aircc clim runs illustrate that delaying the deep mitigation efforts of the rcp2 6 by ten years would significantly increase the risks of climate change during this century the bottom rows of figs 8 and 9 show that under delayed international action the probabilities of exceeding 2 5 c in annual temperature and a decrease of at least 15 annual precipitation in 2100 are similar to those obtained in 2050 for the rcp6 0 and much higher than those of the original rcp2 6 the consequences of delaying for ten years the mitigation efforts described in the rcp2 6 are clearly illustrated by fig 10 north and south africa south america the middle east and central asia would exceed a 2 5 c increase in temperatures around 2060 when this threshold was not reached during this century under the original rcp2 6 trajectory parts of australia would exceed this threshold at the end of the present century if mitigation efforts were postponed in terms of precipitation reductions the mediterranean region would be most affected as the dates of exceedance of decreases of at least 15 in precipitation would occur as soon as 2040 2050 for the south of spain and north africa and at the end of this century for greece western africa would go over this threshold in 2040 and parts of central america and south africa would see decreases of more than 15 in the 2050 2060 period however it is important to note that this delayed action scenario still provides important benefits in comparison with the rcp6 0 which is commonly used to represent current international mitigation commitments some of the most affected regions due to warming would buy time about 2 decades for adapting to a 2 5 c under the delayed version of the rcp2 6 this is not so clear with regard to exceeding 15 decrease in annual precipitation for the most affected regions as in comparison with the rcp6 0 the delayed version of the rcp2 6 would buy them only 5 10 years for implementing adaptation actions monthly estimates of changes in precipitation and temperature are commonly needed for assessing the impacts of climate change in natural and human systems aircc clim also generates estimates of monthly temperature and precipitation change as well as estimates of probabilities and dates of exceedance for user defined thresholds fig 11 illustrates this feature for the rcp6 emissions scenario and for the central month of winter and summer i e february and july during the coldest months in the northern hemisphere s winter the threshold of 2 5 c was exceeded at the beginning of this century in the arctic while for parts of the midlatitudes it will be exceeded during this decade of in the 2030s fig 11a in most of the remaining parts of the northern hemisphere the 2 5 c threshold in temperatures during february would be exceeded in the 2030 2050 decades for parts of north and central america the driest months occur in winter and precipitation in february in those areas would decrease at least 15 as soon as 2030 in the case of regions in southern hemisphere such as australia central and south africa as well as most of south america the temperature threshold during one of the hottest months february would occur before 2060 and in some parts of these region this threshold could be exceeded 20 years earlier the hottest months in the northern hemisphere occur during the boreal summer as shown in fig 11c exceeding the threshold a 2 5 c increase in july would happen in this decade for regions in the mediterranean such as spain france italy greece and parts of north africa in these regions exceeding this threshold in temperatures during july would be accompanied by decreases of at least 15 in precipitation before 2050 in the same month which is one of the driest in the mediterranean moreover due to its capabilities for exporting output aircc clim can be easily combined with other products to address the user s specific information needs fig 12 combines external population projections from the ssp3 scenario that were obtained from the climrisk model estrada and botzen 2021 with two risk measures produced with aircc clim to provide a first approximation of risk and exposure climate and population projections show that by 2050 some regions of the world will have high exposure and high probabilities of experiencing large changes in climate in the bivariate map shown in fig 12a dark magenta color indicate regions for which large population and high probabilities of exceeding 15 decrease in precipitation are projected these high risk high exposure regions include large fractions of the mediterranean central america and parts of the middle east and south asia light yellow areas indicate regions characterized by large population but low probabilities of decreases in precipitation of at least 15 these include high latitude regions in the northern hemisphere for which most climate models projections suggest an increase in precipitation this combination of population and precipitation change would be found in parts of india china parts of central northern and eastern europe northern us and canada light blue regions such as australia large parts of noth africa and south america are where decreases in precipitation of at least 15 are highly likely but where population counts are low fig 12b shows a bivariate map of population counts and the probability of exceeding 2 5 c in annual temperature change by year 2050 regions such as the eastern part of the us central america most of europe india china the middle east and parts of africa are shown in dark magenta color these regions are characterized by high probabilities of exceeding 2 5 c and large population counts regions in light blue hue represent places where the probability of exceeding a warming of at least 2 5 c in 2050 are high but population in those areas is not large this is the case of high latitudes in the northern hemisphere australia the amazon rainforest the sahara namib and the arabian deserts moreover fig 12 helps to identify risk hotspots in which population counts will be high in the future and significantly dryer and hotter conditions will likely occur such combination of factors has been associated with higher risks of human conflict and migration barrios et al 2006 hsiang et al 2013 hodler and raschky 2014 world bank 2016 puente et al 2016 as well as impacts on biomass production and more frequent wildfires de dato et al 2008 stevens rumann et al 2018 5 conclusions here we present aircc clim an emulator of complex climate models included in the ipcc s fifth assessment report that allows generating probabilistic climate change projections and risk measures for rcp emissions scenarios as well as for user defined emissions scenarios global temperature projections are produced using a modified version of the st model and precalculated runs of the magicc and tcm models aircc clim has a spatial resolution of 2 5 2 5 and produces monthly and annual temperature and precipitation scenarios this is a user friendly stand alone software aimed for students decision makers and researchers that allows for quick estimates of changes in climate as well as of the probabilities and dates of exceedance of user defined thresholds the aircc clim model attempts to fill users needs for models that have low technical and computing requirements but that are able to emulate complex climate models output and produce spatially explicit probabilistic projections and risk measures aircc clim extends the st climate model to include a dynamic climate sensitivity that takes advantage of the well established approximately linear relationship between cumulative co2 emissions and global temperature increase this extension of the st model combined with stochastic simulation allows to closely approximate the best estimate and likely range included in the fifth assessment report of the ipcc by means of a simple stochastic simulation procedure we account for the uncertainty in the climate sensitivity parameter and produce probabilistic scenarios based on magicc and tcm precalculated runs extensions and future development of this model include the integration with iva and integrated assessment models such as simple agricultural emulators and climate economy models estrada et al 2020 ignjacevic et al 2020 2021 estrada and botzen 2021 the inclusion of additional climate variables e g minimum and maximum temperatures sea level rise and bioclimatic indices as well as complementary uni and multivariate risk measures declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements francisco estrada acknowledges financial support from dgapa unam through the projects papiit in110718 and in111221 and from pincc unam appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105528 
25524,complex physical models are the most advanced tools available for producing realistic simulations of the climate system however such levels of realism imply high computational cost and restrictions on their use for policymaking and risk assessment two central characteristics of climate change are uncertainty and that it is a dynamic problem in which international actions can significantly alter climate projections and information needs including partial and full compliance of global climate goals here we present aircc clim a simple climate model emulator that produces regional probabilistic climate change projections of monthly and annual temperature and precipitation as well as risk measures based both on standard and user defined emissions scenarios for six greenhouse gases aircc clim emulates 37 atm ocean coupled general circulation models with low computational and technical requirements for the user this standalone user friendly software is designed for a variety of applications including impact assessments climate policy evaluation and integrated assessment modelling keywords climate change scenarios climate model emulator impact vulnerability and adaptation assessment stochastic simulation data availability data will be made available on request 1 software availability aircc clim can be downloaded at no cost from https sites google com view aircc lab airccclim aircc clim 2 introduction climate change projections are one of the main inputs for assessing the potential consequences of different socioeconomic development pathways and international climate policy on natural and human systems due to the complexity of the systems involved and their interactions climate projections are inherently uncertain gay and estrada 2010 curry and webster 2011 knutti and sedláček 2012 moreover computational and technical costs of state of the art physical models allow exploration of only a small fraction of the range of possible climate futures and hinder assessing risk through probabilistic scenarios knutti et al 2010 sanderson et al 2015 for most decision makers and researchers these costs make it infeasible to explore how current and hypothetical changes in international mitigation policy can influence future warming and its consequences for society in a time of proactive international mitigation policy the dynamic nature of projecting future climate becomes even more evident and decision making requirements can go beyond fixed illustrative emissions scenarios such as the rcps estrada and botzen 2021 for example nationally determined contributions ndcs that represent greenhouse gas emission reductions that countries promise as their contributions to the paris agreement are currently a key focus of international climate policy moreover due to the nonlinearity of most climate impacts even small deviations from a high warming trajectory can produce large changes in the associated impacts estrada and botzen 2021 ignjacevic et al 2021 international efforts such as the coupled model intercomparison project cmip which build and host large databases of climate models simulations publicly available have significantly contributed to improving the accessibility and utilization of climate scenarios for the wider research community and decision makers knutti and sedláček 2012 stocker et al 2013 taylor et al 2012 however many users still face the problem of processing large datasets for adapting them to their particular needs e g temporal frequency and spatial domains as such studies and decisions are commonly based on a few illustrative greenhouse gases emissions trajectories and a handful of climate models simulations which are often selected due to their availability and ease of use such as worldclim fick and hijmans 2017 such a selection of climate model runs can hardly provide a good representation of uncertainty and indicate if a model s projections for a given region may represent extreme realizations in comparison to the majority of other models weigel et al 2010 sanderson et al 2015 even in cases when climate models performance is assessed the resulting selection of models does not guaranty that those projections of future climate are reliable altamirano del carmen et al 2021 climate model selection remains an unresolved problem as commonly used metrics can be non informative about the models ability to reproduce the observed climate change signal and indicate much less about their ability for projecting future climate knutti et al 2010 uncertainty is a key characteristic of climate change and how it is understood and included in climate impact assessments can have profound effects on the estimates of the consequences of this phenomenon and on the design of policies to address these consequences ipcc tgica 2007 gay and estrada 2010 the development of tools and methods for better uncertainty management and for improving the usefulness of the large thousands of terabytes databases that are currently available are increasingly relevant research topics more efficient simple and flexible approaches for taking advantage of the available databases can transform data into useful information and knowledge for better assessments of impacts risks and climate policy options reduced complexity models and emulators of more advanced models allow exploring at low computational and technical costs for the user a wide range of possible futures and emissions trajectories parameterizations as well as probabilistic assessment of risks for natural and human systems meinshausen et al 2011a blanc 2017 estrada et al 2020 integrated assessment modelling benefits from such models and emulators to provide tools for supporting decision making and providing estimates for cases in which complex climate impacts model runs are not available a notable example of the usefulness of such models is the magicc software which has made significant contributions to climate change research particularly in impact vulnerability and adaptation iva assessments and integrated assessment modelling wigley 1995 meinshausen et al 2011c a the magicc software has been regularly used in the ipcc reports when simulations produced by general circulation climate models are not available and also in the national climate change assessments of several countries conde et al 2011 ipcc 2018 2021 magicc6 is able to produce probabilistic global temperature change projections and to capture uncertainties in radiative forcing as well as in the climate and carbon cycle responses nicholls et al 2021 the magicc7 version is now on line available https magicc org here we present aircc clim assessment of impacts and risks of climate change probabilistic climate model emulator a simple and flexible climate model emulator for producing probabilistic regional projections of monthly and annual temperature and precipitation as well as risk measures aircc clim has five main innovations compared to other reduced complexity models first it emulates the results from 37 atm ocean coupled general circulation models from cmip phase 5 cmip5 included in the fifth assessment report of the ipcc stocker et al 2013 with low computational and technical requirements on the user second it produces spatially explicit 2 5 2 5 climate projections which allows to explore climate change developments at the regional level which can be visualized on maps third it generates global and spatially explicit probabilistic projections of climate change which allows to represent uncertainty about future climate conditions at different spatial scales fourth the model allows constructing a variety of risk measures tailored to the needs of the user such as the time for exceeding predefined thresholds of changes in climate variables and probabilities of exceedance fifth aircc clim includes a graphical emissions editor to facilitate introducing new emissions trajectories defined by the user additionally this editor can read external emissions scenarios from excel files and it also allows the user to directly modify the emissions scenarios in a table included within the interface another advantage that enables easy integration with other software applications is the capability of exporting results in netcdf and geotiff formats in addition to high quality png files it is a standalone software for windows and linux operating systems that requires no programming or advanced technical skills from the user and runs on computers with standard memory and processing resources aircc clim is designed for a variety of applications including iva assessments integrated assessment modelling and the quick evaluation of the consequences on global and regional climate of user defined experiments of international mitigation the remainder of this paper is structured as follows section 2 describes the aircc clim model structure and describes in detail each of its modules in terms of data sources modelling approaches and methods and their output this section also describes the file options for exporting climate projections and their characteristics an application of the model is shown in section 3 in which the benefits of stringent international mitigation efforts are illustrated in terms of avoided warming changes in precipitation and risk reduction section 4 concludes and discusses model extensions and integration with iva models 3 model structure data and methods aircc clim is composed of four main modules greenhouse gas emission scenario editor global climate models a regional scenario generator and a climate risk index generator fig 1 in the following paragraphs each module is discussed including detailed descriptions of the modelling approaches methods and data sources 3 1 emissions scenario editor aircc clim offers a graphic interface for constructing user defined global emissions scenarios for six climate changing substances fossil and industrial carbon dioxide co2 in mtc yr methane ch4 in mtch4 yr nitrous oxide n2o mtn2o n yr chlorofluorocarbons cfc11 cfc12 in kt yr and sulfur hexafluoride sf6 in kt yr four rcp emissions trajectories are included by default and the user can select one of them as starting point for editing as shown in fig 2 aircc clim has three input options for global emissions 1 by means of editing the values displayed on a table 2 by selecting and plotting the substance of interest and modifying the emissions trajectories directly in a graph 3 loading an excel file xlsx with user defined emissions trajectories for each gas in a predefined format see the aircc clim user guide included in the si the modified emissions scenario is saved in an excel xlsx file and used for generating the corresponding climate projections in aircc clim 3 2 global climate projections global temperature projections in aircc clim can be generated by running a simple climate model as well as by using precalculated projections from two reduced complexity climate models as described in the following paragraphs in all cases probabilistic projections are constructed by means of stochastic simulation to represent uncertainty in the climate sensitivity cs parameter 3 2 1 modified schneider thompson model the schneider thompson st simple climate model schneider and thompson 1981 includes three components that allows to calculate the atmospheric concentrations of greenhouse gases the corresponding change in radiative forcing and the resulting increase in global temperature due to its flexibility and low computational cost this climate model and modified versions of it is used in some of the most popular integrated assessment models such as fund and rice nordhaus and boyer 2003 anthoff and tol 2014a the version of the st model in aircc clim builds upon that used in the fund integrated assessment model tol and fankhauser 1998 which is available at http www fund model org in this section the st model and the modifications that are introduced in aircc clim as described below several aircc clim s parameters are calibrated using magicc6 output meinshausen et al 2011b as well as the best estimates and likely ranges included in the ipcc s fifth assessment report ar5 the magicc6 model was calibrated with climate projections from different general circulation models included in cmip5 meinshausen et al 2011c a calculation of atmospheric concentrations and radiative forcing a five box model maier reimer and hasselmann 1987 tol 2019a is used to convert annual emissions of co2 mtc into atmospheric concentrations ppm the initial value for co2 concentrations is 278 ppm which represent preindustrial times circa 1750 the carbon cycle model is represented by the following equation 1 c i t c o 2 1 α i c i t 1 c o 2 γ i β e t c o 2 2 c t c o 2 i 1 5 c i t c o 2 where c i t c o 2 represents the atmospheric concentrations of co2 in box i 1 5 at time t e t are the co2 emissions at time t α i determines how long co2 remain in box i while γ i is the proportion of emissions that enter box i and β is a scale parameter β 0 00045 the α i and γ i parameters are taken from the literature maier reimer and hasselmann 1987 hammitt et al 1992 tol 2019b while the β is a calibration parameter obtained by minimizing the sum of squares of the differences between the magicc6 concentrations reported in meinshausen et al 2011b and those obtained from equations 1 and 2 under the rcp8 5 scenario the co2 emissions in aircc clim correspond to those of fossil and industrial emissions these boxes are characterized by different decay times that resemble the slow and fast components of the carbon cycle however these boxes do not represent physical processes and are just simple mathematical devices that allow to approximate the observed concentrations schneider and thompson 1981 maier reimer and hasselmann 1987 tol 2019a c t c o 2 is the total concentration of co2 in the atmosphere at time t parameter values are reproduced in table s1 we use a common parameterization for co2 radiative forcing meinshausen et al 2011a 3 f t c o 2 5 35 l n c i t c o 2 c p r e c o 2 where c p r e c o 2 278 represents the preindustrial atmospheric concentrations of co2 ch4 concentrations are calculated using the following equation 4 c t c h 4 1 a 1 c t 1 c h 4 a 1 c p r e c h 4 b 1 e t c h 4 where c t c h 4 are the atmospheric concentrations of ch4 at time t c p r e c h 4 721 89 is the preindustrial concentrations of ch4 e t c h 4 are the emissions of ch4 at time t a 1 1 12 with 12 years representing the permanence of ch4 in the atmosphere stocker et al 2013 anthoff and tol 2014b and b 1 0 2954 is a scaling factor and was obtained by minimizing the sum of squared differences between the magicc6 concentrations and those obtained from equation 4 under the rcp8 5 scenario the atmospheric concentrations of n2o are calculated using an equation similar to that of ch4 5 c t n 2 o 1 a 2 c t 1 n 2 o a 2 c p r e n 2 o b 2 e t n 2 o where the emissions and atmospheric concentrations of n2o are denoted by e t n 2 o and c t n 2 o respectively while c p r e n 2 o 272 96 a 2 1 120 with 120 years representing the persistence of the gas in the atmosphere stocker et al 2013 tol 2019b and b 2 0 1550 obtained by minimizing the sum of squared differences between the magicc6 concentrations and those obtained from equation 4 under the rcp8 5 scenario calculating the radiative forcing of ch4 and n2o involves an interaction term between these gases to account for their overlap in the absorption bands as represented in the following equations 6 i n t t c h 4 f m n 0 f m 0 n 0 7 i n t t n 2 o f m 0 n f m 0 n 0 where these interaction functions are given by 8 f m n 0 p 1 l n 1 p 2 c t c h 4 c t 0 n 2 o 0 75 p 3 c t c h 4 c t c h 4 c t 0 n 2 o 1 52 9 f m 0 n p 1 l n 1 p 2 c t 0 c h 4 c t n 2 o 0 75 p 3 c t c h 4 c t 0 c h 4 c t n 2 o 1 52 10 f m 0 n 0 p 1 l n 1 p 2 c p r e c h 4 c t 0 n 2 o 0 75 p 3 c p r e c h 4 c p r e c h 4 c t 0 n 2 o 1 52 11 f m 0 n 0 p 1 l n 1 p 2 c t 0 c h 4 c p r e n 2 o 0 75 p 3 c p r e n 2 o c t 0 c h 4 c p r e n 2 o 1 52 with p 1 0 47 p 2 2 01 10 5 p 3 5 31 10 15 ramaswamy et al 2001 anthoff and tol 2014b tol 2019b the radiative forcing of ch4 and n20 is calculated as 12 f t c h 4 0 036 c t c h 4 0 5 c p r e c h 4 0 5 i n t t c h 4 13 f t n 2 o 0 12 c t n 2 o 0 5 c p r e n 2 o 0 5 i n t t n 2 o cfc11 and cfc12 concentrations are calculated as follows 14 c t c f c 11 1 a 3 c t 1 c f c 11 b 3 e t c f c 11 15 c t c f c 12 1 a 4 c t 1 c f c 12 b 4 e t c f c 12 in which a 3 and a 4 are equal to 1 45 and 1 100 with 45 and 100 years representing the permanence of cfc11 and cfc12 in the atmosphere stocker et al 2013 tol 2019b while b 3 0 0423 and b 4 0 0481 were obtained by minimizing the sum of squared differences between the magicc concentrations reported in meinshausen et al 2011b and those obtained from equations 14 and 15 under the rcp8 5 scenario respectively the radiative forcing from cfcs is calculated by multiplying it by a scaling factor equal to 0 25 1000 in the case of cfc11 and 0 32 1000 for cfc12 ramaswamy et al 2001 anthoff and tol 2014b tol 2019b finally the concentrations of s f 6 are obtained using the following equation 16 c t s f 6 1 a 5 c t 1 s f 6 a 5 c p r e s f 6 b 5 e t s f 6 where a 5 1 3200 with 3200 years as the permanence of s f 6 in the atmosphere forster et al 2007 and b 5 0 0393 was obtained by minimizing the sum of squared differences between the magicc concentrations and those obtained from equation 16 under the rcp8 5 scenario the radiative forcing of s f 6 is calculated as f t s f 6 0 00052 c t s f 6 ramaswamy et al 2001 tol 2019b as shown in fig s1 the equations and parameterizations used for calculating the concentrations and radiative forcings from the greenhouse gases included in aircc clim can closely approximate those produced with magicc6 in all cases the goodness of fit from running a regression between the output from aircc clim and magicc6 under the rcp8 5 scenario produces r 2 values larger than 0 99 fig s1 fig s2 shows the aircc clim and magicc6 concentrations and radiative forcing for the rcp8 5 rcp6 rcp4 5 and rcp2 6 for individual greenhouse gases and for the sum of their radiative forcing the average differences in the sum of radiative forcing from the aircc clim and magicc6 output are 2 7 6 7 for the rcp8 5 rcp6 rcp4 5 and rcp2 6 respectively calculation of global temperature change the st model uses the following two interlinked equations to produce annual mean global air and ocean surface temperatures 17 t t a t t 1 a λ 1 λ 2 f t t t 1 a λ 3 t t 1 o t t 1 a 18 t t o t t 1 o λ 4 t t 1 a t t 1 o following tol 2019a we use the following parameter values as the initial calibration λ 1 0 0256 λ 2 1 14891 λ 3 0 00738 and λ 4 0 00568 the cs of this model is calculated as λ 2 5 35 ln 2 and it is the main parameter for calibrating the model to reproduce observed or projected annual mean global air surface temperature a limitation with this approach is that the accuracy of its projections will depend on which temperature series is used for calibration to illustrate this we use the projections obtained from the magicc6 model for the rcp8 5 rcp6 rcp4 5 and rcp2 6 for the period 2005 2100 these simulations are obtained using the total radiative forcing factors not only the 6 gases described above for parameter calibration the temperature change projections of the st model are calculated using the same total radiative forcing used to produce those of magicc6 the calibration procedure used was to minimize the sum of the squared residuals between the st and the magicc6 projections using ordinary least squares the optimal cs values that calculated for each scenario are 3 27 rcp8 5 2 70 rcp6 2 64 rcp4 5 and 2 23 rcp2 6 choosing any single value of this parameter to project all the rcp scenarios would lead to over or underestimating future temperatures particularly in the case of extreme scenarios rcp2 6 and rcp8 5 as such this approach could have an impact on for example the evaluation of the benefits of mitigation policies the level of warming is closely related to the total cumulative emissions of co2 tcre fig s3 the relationship between global temperature change and tcre is relatively constant over time and independent from the emissions scenario used but it is dependent on the global climate model cs and transient climate response collins et al 2013 fig s3 shows the differences in the temperature change projections of magicc6 and the st as a function of total cumulative emissions of co2 from different rcp scenarios in this figure the st projections are shown for when the cs is fixed and calibrated for a particular rcp and for when the cs is allowed to vary across rcp scenarios when the cs is calibrated using one of the rcp scenarios and it is used for projecting temperature change for the other rcp scenarios the differences between the st and the magicc6 projections can be as large as 1 55 c these differences are much smaller when the cs value is allowed to change for different rcps the mean and maximum differences are 0 02 c and 0 17 c respectively which suggest that the emulating capacity of the st model could be greatly improved if the cs value is allowed to vary across cumulative emissions simple climate models such as st are based on extremely stylized representations of the climate system and necessarily ignore several relevant aspects of for example climate feedbacks and other processes that determine the responses to changes in external forcing and thus the cs while the cs in the climate system and in complex climate models is an emergent property in reduced complexity climate models is a parameter to be calibrated the st ignores many climate feedbacks and other radiatively active compounds which also contributes to the differences in temperature projections obtained with magicc6 a sensible calibration of the cs can help compensate the omissions in simple models and increase their ability to reproduce the output of more complex models we stress that the cs specification in aircc clim is as a useful device for approximating more closely temperature change projections for the range of rcps projections but that the parameterization of the cs should not be interpreted as a physical statement about how the climate system operates fig 3 shows a linear relationship between cumulative co2 emissions and the cs value the st model requires to better approximate the temperature change projections from magicc6 given this relationship we propose a dynamic cs parameter for the st model that is calculated based on the cumulative co2 emissions which are dominant in any rcp sres or other realistic emissions scenario the proposed parameterization for the cs has the practical purpose of increasing the emulation capacity of the st model and to fully exploit the ability of simple models to approximate the results from much more complex ones with only a small fraction of computing effort while the proposed dynamic cs is only a practical device to increase the emulation capacity of the model there is increasing evidence that the climate system s feedbacks are dependent on forcing magnitudes and timescales and thus the cs may be not a constant value both in reality as in complex climate models knutti and rugenstein 2015 in the proposed dynamic cs this parameter is calculated prior to projecting global temperatures with the st model the regression equation that relates the cs and the cumulative co2 emissions is c s 2 05 6 18 10 7 c o 2 r 2 0 96 fig 3 to get a better approximation for the lowest and highest cumulative scenarios we also fitted a line based on the cs values for rcp2 6 and the rcp8 5 the results were compared to the projections in the ipcc s ar5 and were optimized to reproduce the best estimates stocker et al 2013 the final extreme cs values are 2 1 c and 3 3 c for the rcp2 6 and rcp8 5 scenarios respectively the final equation relating cs and cumulative co2 emissions is 19 c s 1 85 7 51 10 7 c o 2 the cs values are restricted to the interval 2 1 3 3 table 1 shows the projections of the modified st model and the best estimates and likely ranges in the ar5 collins et al 2013 for all four rcp emissions scenarios and both short and long term horizons the average difference is about 0 08 c and the maximum 0 23 c this illustrates that the modified st model very closely resembles the ipcc ar5 temperature projections the differences in projections between magicc6 and aircc clim are in part due to the omission of other radiative active compounds as the latter only considers 6 greenhouse gases the differences in the 2046 2065 horizon are slightly larger and may be related to the omission of aerosols and land use co2 emissions which have a more significant influence in the shorter term to extend the st model to account for the uncertainty in cs values and to produce probabilistic projections we represent the model s cs with a triangular distribution the model output of global temperature change projections is affected by the distribution chosen for the cs parameter our choice to represent the cs with a triangular distribution is common in the integrated assessment literature such as in the page and climrisk models moore et al 2018 estrada and botzen 2021 note that other probability distributions and parameterizations could be used to explore different assumptions about the cs gay and estrada 2010 in this distribution the most likely value is given by equation 19 above and the lower and upper limits are the calculated cs value plus minus some constants that are to be calibrated the equations for the upper c s h i g h and lower limits c s l o w of the triangular distribution for cs are 20 c s l o w c s h 1 21 c s h i g h c s h 2 the h 1 and h 2 constants are calibrated to match the likely ranges reported in the ipcc s ar5 simulations of 10 000 realizations were used and the 5th and 95th percentiles were chosen to construct the st likely ranges the parameter values that provided good fit for all rcps are h 1 1 1 and h 2 1 5 table 1 shows that the likely ranges of the modified st model closely reproduce those reported in the ipcc for both short and long term horizons showing almost exact overlap with differences in upper and lower limits typically smaller than 0 2 c 3 2 2 generating probabilistic global temperature projections using precalculated runs from magicc6 and the thermodynamic climate model the magicc6 and the thermodynamic climate model tcm are reduced complexity climate models that were designed for different objectives magicc is composed of a set of coupled models that include gas cycles and climate and ice melt models designed to explore the effects of anthropogenic emissions of greenhouse gas concentrations radiative forcing and changes in global mean annual temperature and sea level wigley 1995 meinshausen et al 2011c a the tcm was originally conceived as a weather forecast model for the northern hemisphere and then extended to the global scale for studying the climate of earth and of other planets in the solar system adem 1991 in the case of magicc6 precalculated projections of global temperature are included in aircc clim for the emissions scenarios rcp2 6 rcp4 5 rcp6 and rcp8 5 while in the case of the tcm the precalculated that were available are the rcp4 5 rcp6 and rcp8 5 to account for the uncertainty in cs values and to produce probabilistic projections we propose a simple method based on linear regression and statistical simulation due to the availability of climate models output we base our calculations on magicc6 projections for each rcp scenario were obtained using of magicc6 for three different cs values that represent medium cs 3 0 c low cs 1 5 c and high cs 4 5 c to emulate magicc s results for high and low climate sensitivities we propose the use of some scaling weights w that would approximate them when multiplied by the global temperature obtained using a medium cs value such weights can be obtained by means of the following regression 22 t t s e n s ω t t m e d i u m ε t where t t s e n s is the global temperature projection obtained with magicc6 using either low or high cs while t t m e d i u m corresponds to the global temperature projection obtained with medium cs ω is the slope parameter and ε t are the regression residuals in all cases the r 2 is higher than 0 99 note that the objective of these models is not to make inference about parameter values but just to produce a close fit of projections using different values of cs the values of parameter ω for the different combinations of cs and for each rcp emissions scenario are provided in table 2 the estimated parameter values are very similar for different rcps with an average value of 1 34 and 0 57 for high and low cs values respectively as shown in fig 4 these values allow to closely approximate the simulations of magicc6 produced using high low cs values by scaling a simulation of the same model calculated with medium sensitivity the approximation is less precise when used on scenarios that lead to stabilization fig 4d but the error is still very small 0 17 c for the high cs projection using the rcp2 6 to produce probabilistic scenarios using the st model we use a triangular distribution to scale the magicc6 runs obtained with a medium cs the parameters of the triangular distribution are 1 for the mode or most likely value and 0 5679 and 1 3414 for the lower and upper limits respectively table 2 these parameter values allow to emulate the projections that would be obtained with magicc6 randomly choosing cs values contained in the interval 1 5 c to 4 5 c assigning a larger probability of occurrence to values that are closer to the medium sensitivity of 3 c cs is high uncertainty rogelj et al 2014 lewis and curry 2015 freeman et al 2015 tan et al 2016 friedrich et al 2016 knutti et al 2017 cox et al 2018 however the consensus is that the cs is probably within the 1 5 c to 4 5 c interval with a central estimate of 3 c bony et al 2013 callendar 1938 freeman et al 2015 santer et al 2019 stocker et al 2013 this cs range encompasses the range produced by the state of the art models included in the cmip5 jonko et al 2018 stocker et al 2013 given that in the tcm the cs is not an explicit parameter as in magicc6 but an emerging property of the model there are no high low cs projections that can be used to find the corresponding parameters in table 2 as such we apply the same scaling parameters we found for magicc6 to represent uncertainty in cs values and to extend the tcm projections to be probabilistic 3 3 regional climate projections pattern scaling is a technique for producing regional scenarios based on the robust finding documented in the literature of stationarity in geographical patterns of change in some climate variables in particular temperature and precipitation santer et al 1990 collins et al 2013 tebaldi and arblaster 2014 these patterns can be scaled by global temperature change to produce regional climate change scenarios in a simple and computationally efficient manner that provide a useful approximation to those produced by complex global climate models the performance of pattern scaling techniques has been evaluated in the literature in several occasions mitchell 2003 cabré et al 2010 collins et al 2013 tebaldi and arblaster 2014 herger et al 2015 kravitz et al 2017a zelazowski et al 2018 tebaldi and knutti 2018 osborn et al 2018 these techniques produce adequate approximations for variables such as annual seasonal temperature and precipitation and other variables excluding extreme events and time scales in which natural variability is dominant these patterns are also adequate for most of the emissions scenarios including low warming policy trajectories and most regions except where local forcing is strong and time varying collins et al 2013 estrada and botzen 2021 an example of the latter are scenarios that involve very high aerosols emissions the reason is that the modelling approach does not capture the effects of regional cooling by aerosols or land surface feedbacks due to conditions that not reflected by the library of scaling patterns the pattern scaling technique can be described as follows collins et al 2013 tebaldi and arblaster 2014 estrada and botzen 2021 23 p i j t e y s t t e p i j y s ξ i j t s where i j denote longitude and latitude respectively and t is time e represents the emissions scenario y is the climate variable of interest s is the time of the year for which the scenarios is constructed annual month season and t t is the global annual mean temperature change at time t under the emission scenario e p is the projected field of change for variable y obtained using a complex climate model while p is the time emissions scenario invariant spatial pattern of change per 1 c change in annual global mean temperature for the climate variable y ξ i j t s represents is an error term due to both natural variability and the limitations of the pattern scaling methodology collins et al 2013 estrada and botzen 2021 the patterns p i j y s were calculated for monthly and annual air surface temperature and precipitation using simulations from a battery of coupled ocean atmosphere general circulation models table s2 under the rcp8 5 emissions scenario and that are available at the cmip5 data portal https esgf node llnl gov search cmip5 all simulations were bilinearly interpolated into a common grid with a spatial resolution of 2 5 2 5 the hodrick prescott filter hodrick and prescott 1997 was applied to the time series from each grid point from the climate models simulations to remove the effects of high frequency variability then temperature precipitation time series from each grid cell were regressed on the global mean temperature and the slope coefficients were stored as maps that represent the scaling patterns kravitz et al 2017b lynch et al 2017 for producing the regional climate change scenarios global mean temperature projections from section 2 2 are used to scale the patterns produced in this section the projections are expressed in c for changes in temperatures and in percentage of change for precipitation with respect to preindustrial conditions c 1750 aircc clim allows the user to select the scaling patters for any given climate model as well as to generate probabilistic scenarios combining all of them this stochastic version of aircc clim uses a uniform distribution which assigns the same probability to each of the climate models a variety of approaches for assigning probabilities to climate models output have been proposed in the literature xu et al 2010 knutti et al 2010 mendlik and gobiet 2016 but there is no agreement on which would be the best way of doing it knutti 2010 stephenson et al 2012 deser et al 2014 notz 2015 the uniform distribution follows the principle of insufficient reason which is the maximum entropy distribution in absence of any additional information jaynes 1957 jaynes et al 2003 gay and estrada 2010 other probability distributions based on performance evaluation or model dependence could be implemented however these distributions may be as arbitrary as assigning equal probabilities to each model and may lead to unjustified dismissal of uncertainty potter and colman 2003 gay and estrada 2010 altamirano del altamirano del carmen et al 2021 climate change scenarios for temperature and precipitation can be exported as geotiff and netcdf files for three time horizons 2030 2021 2040 2050 2041 2060 and 2070 2061 2080 maps of changes in temperature and precipitation for any year between 2005 and 2100 can be exported as high quality png and as netcdf files 3 4 climate risk index generator aircc clim uses the probabilistic nature of its projections to produce user defined risk measures based on thresholds the current version of the model includes two types of risk metrics probabilities of exceedance and the dates when the selected threshold is exceeded the threshold values are selected by the user to reflect his perceptions of risk and information needs note that due to the exporting capabilities of this software aircc clim s output can be combined with information external from the model such as population projections to indicate hotspots of severe climate change in populated areas the calculation of the risks indices is done following the same procedure as climrisk estrada and botzen 2021 first the indicator function is used to identify in which simulations and grid cells the threshold is exceeded in the case of changes in temperature t we have 24 i r t i j t s i m i t i j t s i m t where i r t i j t s i m is a four dimensional matrix in which i j are the longitude and the latitude that define the location of the gird cell t is time s i m is the number that identifies each of the realizations of the simulation experiment and t is the user defined threshold in c for cases in which the threshold t is exceeded the indicator function returns a value of 1 and zero otherwise in the case of the change in precipitation p the threshold of interest p can be positive or negative and thus the indicator function is applied as follows 25 i r p i j t s i m i p i j t s i m p i f p 0 i p i j t s i m p o t h e r w i s e i r p i j t s i m is a four dimensional matrix defines as above in which the indicator function returns a value of 1 if the threshold p is exceeded and zero otherwise estimates of the probabilities of exceeding the thresholds t and p can be obtained by summing over the s i m dimension 26 p i r t i j t p t i j t t s i m 1 n i r t i j t s i m n 27a p i r p i j t p p i j t p s i m 1 n i r p i j t s i m n i f p 0 27b p i r p i j t p p i j t p s i m 1 n i r p i j t s i m n i f p 0 where p i r t i j t and p i r p i j t are three dimensional matrices containing probability estimates the estimated probability maps can be exported as png and netcdf files for any year in the period 2005 2100 these probability estimates of exceeding critical thresholds are used to estimate the expected dates when such thresholds would be attained these dates can be computed as follows 28 i d t i j t i p i r t i j t γ 29 i d p i j t i p i r p i j t γ where the parameter γ represents the percentage of simulations that is required by the user to declare the threshold has been exceeded i d t i j t and i d p i j t are matrices in which the entries take the value 1 if the confidence level γ is attained or exceeded and zero otherwise in aircc clim and climrisk this value is called confidence level and can vary with the risk tolerance of different users the default value in aircc clim is 50 the estimated dates for exceeding the risk threshold are obtained by mapping into a year index the first occurrence of the value 1 in the time dimension of all gird cells in i d t i j t and i d p i j t the maps of the dates of exceedance can be exported as png and netcdf files table s3 provides a list of the climate risk indices and their definition 4 estimating the risks of delaying the implementation of deep mitigation efforts in this section we provide an example application of aircc clim in which the effects on climate from a high emissions trajectory rcp6 0 are compared to those of a deep mitigation scenario rcp2 6 consistent with the goals of the paris agreement and to those in which such mitigation effort is delayed ten years figure s1 shows the trajectories of c o 2 c h 4 and n 2 o for the rcp6 0 rcp2 6 and the modified version of the rcp2 6 scenarios in which mitigation is delayed for 10 years starting in 2020 the modified version of the rcp2 6 was edited in excel and directly loaded to aircc clim using the user defined option for emissions scenarios the rcp6 0 and the rcp2 6 scenarios produce contrasting results in terms of their effects on climate while the first produces a mean increase in global temperature of about 3 c with respect to preindustrial conditions at the end of this century and up to 4 c when the 95th percentile is considered the rcp2 6 limits warming below 2 c for the ensemble mean about 1 7 c although this limit is exceeded for the 95th percentile fig 5 the magicc model produces similar results with a mean increase in global temperature of 3 1 c and 1 6 c for the end of the century under the rcp6 0 and rcp2 6 scenarios respectively fig s2 the tcm produces a considerably larger mean increase for the rcp6 0 3 9 c but that still lies within the likely range of the cmip5 experiment table 1 the st simulations for the modified rcp2 6 in which the mitigation effort is delayed for 10 years show that the mean increase in global temperature reaches 2 c in 2100 and up to 3 c in the 95th percentile figs 6 and 7 show the changes in annual mean temperature and total annual precipitation at the grid cell level for the three emissions scenarios and two time horizons 2050 and 2100 temperature increase is much larger in high latitudes of the northern hemisphere due to the arctic amplification phenomenon pithan and mauritsen 2014 reaching about 6 c for the mid century and more than 8 c in 2100 under the rcp6 0 scenario see figs s3 and s4 for results using the precalculated magicc6 and tcm runs most of the continents would experience temperature increases in 2050 of about 2 c to 3 c and of about 4 c to 5 c in 2100 apart from the arctic region midlatitudes in north america and in eurasia would have the largest increases 5 c 6 c in temperature by the end of the present century followed by southern asia and the middle east north and south africa parts of the west coast of north america mexico and the amazonian region and the northern part of brazil large changes are also expected in precipitation under the rcp6 0 scenario with large increases in high latitudes the equatorial pacific ocean and some parts of the middle east and large decreases in the mediterranean the caribbean mexico and the southern part of the us as well as in southern parts of africa and america figs s3 and s4 the implementation of a deep mitigation effort consistent with the goals of the paris agreement would significantly limit these changes in climate under the rcp2 6 most of the arctic would not exceed a warming of 5 c during this century most continents would not exceed 3 5 c and precipitation change would be notably smaller aircc clim portraits the risks of climate change and the benefits of mitigation in a clearer way due to its probabilistic nature and its capacity to produce maps of probabilities and of dates of exceedance figs 8 and 9 show the probabilities of exceeding a 2 5 c increase in annual temperatures and a decrease of 15 in annual precipitation in 2050 and 2100 under the rcp6 0 by 2050 the probabilities of exceeding 2 5 c increase in annual temperatures are higher than 60 for most of the continents while the arctic region and the high latitudes of the northern hemisphere are virtually certain to exceed this threshold by mid century fig s5 these simulations also show that a 2 5 c warming would be exceeded in all continents by 2100 and that there is a high probability above 60 that this threshold would be crossed in all oceans except for parts of the southern hemisphere and the atlantic the probabilities of exceeding a decrease of 15 in total annual precipitation in 2050 are higher than 50 for several areas of the world such as the mediterranean parts of northern western and southern africa the caribbean and mexico fig s5 for 2100 these probabilities increase to more than 60 in those regions and extend to parts of south america and west australia if the emissions trajectory described in by the rcp2 6 is achieved by 2050 the probability of exceeding 2 5 c would be lower than 40 over the continents except for the arctic and parts of the midlatitudes in the northern hemisphere where the probabilities range from 60 to 100 fig 8 likewise the probabilities of exceeding decreases in precipitation of at least 15 are considerably smaller in comparison to the rcp6 0 however by the end of this century the probabilities of exceeding this threshold would be close to 50 for the southern region of spain parts of north and west africa morocco mauritania mali senegal sierra leone and guinea and about 40 for parts of the caribbean central and south america colombia and venezuela fig 9 fig 10 shows the dates when the 2 5 c threshold would be exceeded the default confidence level γ 50 was used for the estimates presented in this section the results show that under the rcp6 0 most of the planet except for part of the southern oceans and part of the north atlantic would exceed the 2 5 c threshold during this century and that some parts of the world already exceeded it fig s5 the date of exceedance for the arctic occurred during the 2000s while for much of the high latitudes in the northern hemisphere the middle east parts of south asia and west and south africa the exceedance is expected to occur in the 2020s 2030s the remainder of the continents and the antarctic region would go over this threshold during the period 2040 2060 and most of the oceans above the 20 s would exceed the 2 5 c threshold during this century the regions in which reductions of at least 15 in annual precipitation is exceeded are fewer and form well defined geographical patterns that cover the mediterranean parts of north west and south africa central america and the caribbean mexico and colombia and venezuela in south america as well as the west part of australia and part of the southern pacific ocean fig 10 and s5 the dates for exceedance on these regions are typically reached in the 2050 2060 decades although regions of spain and west africa could exceed this threshold as early as the 2040s achieving rcp2 6 would prevent exceeding these thresholds for most of the world during this century fig 10 however as mentioned above some regions such as the arctic and the high latitudes of the northern hemisphere already have exceeded the 2 5 c temperature threshold or will do so during the next decade regardless of the emission scenario that is selected for parts of the midlatitudes the rcp2 6 represents delaying reaching the 2 5 c threshold for about 20 years which buys time for adapting to the projected changes and to reduce risks and damages the occurrence of this threshold would also be delayed until 2060 in some parts of the sahara south africa the middle east and india exceeding decreases in precipitation of more than 15 would not occur during this century with the exception of a few grid cells in africa spain and central america aircc clim runs illustrate that delaying the deep mitigation efforts of the rcp2 6 by ten years would significantly increase the risks of climate change during this century the bottom rows of figs 8 and 9 show that under delayed international action the probabilities of exceeding 2 5 c in annual temperature and a decrease of at least 15 annual precipitation in 2100 are similar to those obtained in 2050 for the rcp6 0 and much higher than those of the original rcp2 6 the consequences of delaying for ten years the mitigation efforts described in the rcp2 6 are clearly illustrated by fig 10 north and south africa south america the middle east and central asia would exceed a 2 5 c increase in temperatures around 2060 when this threshold was not reached during this century under the original rcp2 6 trajectory parts of australia would exceed this threshold at the end of the present century if mitigation efforts were postponed in terms of precipitation reductions the mediterranean region would be most affected as the dates of exceedance of decreases of at least 15 in precipitation would occur as soon as 2040 2050 for the south of spain and north africa and at the end of this century for greece western africa would go over this threshold in 2040 and parts of central america and south africa would see decreases of more than 15 in the 2050 2060 period however it is important to note that this delayed action scenario still provides important benefits in comparison with the rcp6 0 which is commonly used to represent current international mitigation commitments some of the most affected regions due to warming would buy time about 2 decades for adapting to a 2 5 c under the delayed version of the rcp2 6 this is not so clear with regard to exceeding 15 decrease in annual precipitation for the most affected regions as in comparison with the rcp6 0 the delayed version of the rcp2 6 would buy them only 5 10 years for implementing adaptation actions monthly estimates of changes in precipitation and temperature are commonly needed for assessing the impacts of climate change in natural and human systems aircc clim also generates estimates of monthly temperature and precipitation change as well as estimates of probabilities and dates of exceedance for user defined thresholds fig 11 illustrates this feature for the rcp6 emissions scenario and for the central month of winter and summer i e february and july during the coldest months in the northern hemisphere s winter the threshold of 2 5 c was exceeded at the beginning of this century in the arctic while for parts of the midlatitudes it will be exceeded during this decade of in the 2030s fig 11a in most of the remaining parts of the northern hemisphere the 2 5 c threshold in temperatures during february would be exceeded in the 2030 2050 decades for parts of north and central america the driest months occur in winter and precipitation in february in those areas would decrease at least 15 as soon as 2030 in the case of regions in southern hemisphere such as australia central and south africa as well as most of south america the temperature threshold during one of the hottest months february would occur before 2060 and in some parts of these region this threshold could be exceeded 20 years earlier the hottest months in the northern hemisphere occur during the boreal summer as shown in fig 11c exceeding the threshold a 2 5 c increase in july would happen in this decade for regions in the mediterranean such as spain france italy greece and parts of north africa in these regions exceeding this threshold in temperatures during july would be accompanied by decreases of at least 15 in precipitation before 2050 in the same month which is one of the driest in the mediterranean moreover due to its capabilities for exporting output aircc clim can be easily combined with other products to address the user s specific information needs fig 12 combines external population projections from the ssp3 scenario that were obtained from the climrisk model estrada and botzen 2021 with two risk measures produced with aircc clim to provide a first approximation of risk and exposure climate and population projections show that by 2050 some regions of the world will have high exposure and high probabilities of experiencing large changes in climate in the bivariate map shown in fig 12a dark magenta color indicate regions for which large population and high probabilities of exceeding 15 decrease in precipitation are projected these high risk high exposure regions include large fractions of the mediterranean central america and parts of the middle east and south asia light yellow areas indicate regions characterized by large population but low probabilities of decreases in precipitation of at least 15 these include high latitude regions in the northern hemisphere for which most climate models projections suggest an increase in precipitation this combination of population and precipitation change would be found in parts of india china parts of central northern and eastern europe northern us and canada light blue regions such as australia large parts of noth africa and south america are where decreases in precipitation of at least 15 are highly likely but where population counts are low fig 12b shows a bivariate map of population counts and the probability of exceeding 2 5 c in annual temperature change by year 2050 regions such as the eastern part of the us central america most of europe india china the middle east and parts of africa are shown in dark magenta color these regions are characterized by high probabilities of exceeding 2 5 c and large population counts regions in light blue hue represent places where the probability of exceeding a warming of at least 2 5 c in 2050 are high but population in those areas is not large this is the case of high latitudes in the northern hemisphere australia the amazon rainforest the sahara namib and the arabian deserts moreover fig 12 helps to identify risk hotspots in which population counts will be high in the future and significantly dryer and hotter conditions will likely occur such combination of factors has been associated with higher risks of human conflict and migration barrios et al 2006 hsiang et al 2013 hodler and raschky 2014 world bank 2016 puente et al 2016 as well as impacts on biomass production and more frequent wildfires de dato et al 2008 stevens rumann et al 2018 5 conclusions here we present aircc clim an emulator of complex climate models included in the ipcc s fifth assessment report that allows generating probabilistic climate change projections and risk measures for rcp emissions scenarios as well as for user defined emissions scenarios global temperature projections are produced using a modified version of the st model and precalculated runs of the magicc and tcm models aircc clim has a spatial resolution of 2 5 2 5 and produces monthly and annual temperature and precipitation scenarios this is a user friendly stand alone software aimed for students decision makers and researchers that allows for quick estimates of changes in climate as well as of the probabilities and dates of exceedance of user defined thresholds the aircc clim model attempts to fill users needs for models that have low technical and computing requirements but that are able to emulate complex climate models output and produce spatially explicit probabilistic projections and risk measures aircc clim extends the st climate model to include a dynamic climate sensitivity that takes advantage of the well established approximately linear relationship between cumulative co2 emissions and global temperature increase this extension of the st model combined with stochastic simulation allows to closely approximate the best estimate and likely range included in the fifth assessment report of the ipcc by means of a simple stochastic simulation procedure we account for the uncertainty in the climate sensitivity parameter and produce probabilistic scenarios based on magicc and tcm precalculated runs extensions and future development of this model include the integration with iva and integrated assessment models such as simple agricultural emulators and climate economy models estrada et al 2020 ignjacevic et al 2020 2021 estrada and botzen 2021 the inclusion of additional climate variables e g minimum and maximum temperatures sea level rise and bioclimatic indices as well as complementary uni and multivariate risk measures declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements francisco estrada acknowledges financial support from dgapa unam through the projects papiit in110718 and in111221 and from pincc unam appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105528 
