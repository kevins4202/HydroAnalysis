index,text
26250,ice phenology defined as the timing of freeze up and ice cover breakup plays a key role in streamflow regimes in cold region river catchments river freeze up and ice cover breakup events are controlled by meteorological and hydrological variables in this study we present a modelling framework consisting of a physically based semi distributed hydrological model and the integration of a 1d stream temperature model that can predict the ice duration in cold region rivers the hydrological model provides streamflow and hydraulic parameters for the stream temperature model to obtain instream water temperature the model was successfully applied in the athabasca river basin in western canada calibration was carried out using the water temperature recorded in the stations at the towns of hinton athabasca and fort mcmurray model results show consistent correspondence between simulated freeze up and breakup dates and the hydrometric station data in the main tributaries of the basin freeze up timing spans from the last week of september to the second week of november and ice cover breakup occurs from the second week of march to the last week of may the model presents an application of water temperature and ice phenology simulation which can be incorporated in ice jam flood forecasting and future climate change studies keywords stream temperature simulation freeze up breakup athabasca river basin 1 introduction in mid to high latitude lakes and rivers the formation and breakup of ice covers are important seasonal events river freeze up and ice cover breakup events are influenced by stream water and air temperature and are sensitive to the characteristics of individual waterbodies beltaos and prowse 2001 in cold region rivers the ice phenology plays a key role in the effects of ice on river bank and bed erosion sedimentation and regulation of the stream flow regime in winter e g beltaos and prowse 2009 terry et al 2017 climate influences the timing of freeze onset ice duration ice melt and thermal dynamics that feed back to the climate system initiating further changes the ice formation process in rivers is initiated once the air temperature has been below 0 c long enough to super cool the water and form frazil ice e g osterkamp 1978 beltaos 2000 dynamic physical and mechanical factors are involved in the formation of ice covers turcotte and morse 2013 on lentic waters such as lakes and along river shorelines with low flow velocity a solid continuous ice sheet forms with a horizontal uniform structure this ice sheet originates from either the waterbody shores or pre formed plate ice that extends outward to the center of the waterbody in rivers with more turbulent flow the formation is more dynamic and mainly results after the accumulation of frazil slush pancake ice and ice floes e g beltaos 2000 prowse and beltaos 2002 accumulation on rivers is either an ice jam or a hummocked ice cover pressure ice mounds haphazardly arranged when it refreezes the breakup of river ice covers is triggered by mild weather conditions and encompasses a variety of processes associated with thermal deterioration initial fracture movement fragmentation transport jamming and final clearance of ice during ice cover breakup multiple stages such as pre breakup onset drive and wash occur concurrently within a specific reach beltaos 2008 during the pre breakup phase the ice cover becomes more susceptible to fracture and movement via thermal and solar radiation induced reductions in its thickness and strength at the same time spring and early summer weather conditions bring about snowmelt and rainfall events that cause an increase in flow discharges and the rise of stream water levels that fracture drive and wash out the ice phenology records obtained over long time periods and at regular time intervals are thus required to better understand inconsistencies and changes in meteorological conditions and how such changes impact river ice processes latifovic and pouliot 2007 due to the lack of observed records stream water temperature models constitute important tools to simulate ice phenology data especially in ungauged catchments several model approaches such as linear and multivariable regression methods neural network models and thermal energy budget methods have been developed and implemented to simulate stream water temperature e g benyahya et al 2007 dugdale et al 2017 most of these data driven models have been applied to specific stream reaches and calibrated using observed data in contrast mechanistic models have been applied for regional and catchment scale problems to understand the spatial and temporal behaviour of the energy transfer mechanisms controlling water temperature stream temperature models have been utilized for short term e g daily and long term from weeks to years water temperature forecasting dugdale et al 2017 similarly water temperature hindcasting has been performed to reconstruct historical records benyahya et al 2007 water temperature models are also employed to investigate the potential impact of anthropogenic effects on the thermal regime of rivers sun et al 2015 and the implications for aquatic life and ecosystems e g gu et al 1999 despite the variety of applications physically based distributed stream water temperature models have not been used to simulate the timing of freeze up and breakup events in large river catchments in this paper the river basin model rbm a semi lagrangian water temperature model for advection dominated river systems yearsley 2012 was coupled to mesh a semi distributed hydrological model pietroniro et al 2007 to simulate stream water temperature in the athabasca river basin arb although the model has been successfully coupled to the variable infiltration capacity vic model e g van vliet et al 2012 widely implemented in large scale hydrological modelling the mesh rbm system is more suited to cold region catchments due to its ability to simulate snow fall snow redistribution and snow melt and their effect on stream water temperature due to the lack of stream water temperature data in the upland areas of the arb temperature data in these areas was extracted from thermal infrared imagery via remote sensing techniques to impose model boundary conditions at the headwater streams simulated stream water temperature time series were used to estimate the timing of river freeze up and ice cover breakup events in the arb the model may be useful for improving ice jam flood event preparedness and mitigation and to study future ice phenology 2 methods 2 1 rbm model description rbm is a mechanistic one dimensional stream temperature model that solves the 1d head advection equation 1 d t w a x d t h a i r w a t e r w x ρ w c p q t r b δ t t r b δ x q e f f δ t e f f δ x where ρ w is water density kg m 3 c p is the specific heat of water j kg 1 o c 1 t w is water temperature o c a x is the cross section area at distance x m 2 w x is stream width at distance x m q t r b is the advected flow from tributaries or the subsurface m 3 s 1 δ t t r b is the difference between advected temperature from tributaries or the subsurface respectively t t r b and t w o c q e f f is the advected flow from head dumps e g thermoelectric power plants m 3 s 1 δ t e f f is the difference between advected temperature from head dumps t e f f and t w o c δ x is the longitudinal distance along the river axis m and t is time s the net heat flux at the air water interface h a i r w a t e r is calculated in j m 2 s 1 as 2 h a i r w a t e r h s w h r s h a h a r h e v a p h c o n d h b a c k where h s w is short wave solar radiation j m 2 s 1 h r s is reflected short wave solar radiation j m 2 s 1 h a is long wave atmospheric radiation j m 2 s 1 h a r is reflected atmospheric radiation j m 2 s 1 h e v a p is the evaporative heat flux j m 2 s 1 h c o n d is conductive heat flux j m 2 s 1 and h b a c k is black body radiation from the water surface j m 2 s 1 although some authors have considered hyporheic flow riparian vegetation and stream bed head exchange important factors affecting h a i r w a t e r equation 2 these are not relevant due to the scale of our study case and the purpose of this research rbm uses an explicit semi lagrangian numerical scheme yearsley 2009 to resolve equation 1 the scheme tracks individual water packets along the river network in the flow direction constituted by interconnected segments and nodes of homogeneous characteristics and saves the simulated water temperature at every segment of the network 2 2 boundary conditions in rbm rbm requires water temperature boundary conditions at the basin headwaters different approaches such as equalizing water temperature with simulated daily soil temperature from land surface numerical models and estimating water temperature as a function of air temperature have been tested before van vliet et al 2012 the non linear water temperature regression model described by mohseni et al 1998 has yielded acceptable results this model relates weekly water temperatures as a non linear function s curve of weekly air temperatures as 3 t w h e a d μ α μ 1 e γ β t a s m o o t h where μ is the lower bound of water temperature o c α is the upper bound of water temperature o c γ 4 tan θ α μ is the measure of the slope at the inflection point of the s shaped relation o c 1 where tan θ is the slope at the inflection point β is the air temperature at the inflection point o c and t w h e a d is the headwater temperature o c as water temperature variation lags behind air temperature fluctuations at short time scales e g daily basis t a s m o o t h is the smoothed air temperature o c represented as 4 t a s m o o t h 1 λ t a t 1 λ t a t where t a t 1 is air temperature at time t 1 o c and t a t is air temperature at time t o c two different parameter sets for first order headwater streams located either in the lowlands or in the highlands were established for the lowland headwaters μ 0 5 α 20 5 γ 0 2 β 6 5 and λ 0 1 were chosen from the literature considering the geographical similarities between the multiple basins analysed in bawden et al 2014 in the rocky mountains glacier melt dominates the athabasca river flow regime and constitutes an important source of cold water into the system due to the lack of water temperature records at these locations remote sensing tools were used to retrieve water surface temperatures from 13 cloud free and snow free locations landsat 5 and 8 thermal images were retrieved for the time frame between september 2003 and september 2016 those images have a 30 m resolution and are terrain corrected level 1 products downloaded from usgs 2018 for landsat 8 thermal images only thermal band 10 10 6 11 9 μm was used because thermal band 11 11 50 12 51 μm is significantly affected by stray light and thus not recommended for quantitative analysis montanaro et al 2014 the mono window method developed by qin et al 2001 was used to derive water surface temperatures at jasper lake the derived water temperature and the observed hourly air temperature at 10 am mountain time zone at the jasper warden weather station e g fig 1 a were used to fit the parameters in equation 3 using the least squared error method see fig 1b and c the fitted model performed relatively well nash sutcliffe efficiency nse 0 54 r2 0 55 with low error rmse 0 72 c and bias pbias 0 8 after comparing simulated against retrieved surface water temperatures at jasper lake fig 1c the resulting parameters μ 0 5 α 16 28171 γ 0 103247 β 2 440703 and λ 0 1 were in agreement with the ranges established by van vliet et al 2012 the calibrated parameters were used to set up the boundary conditions at all the headwater streams located upstream of the hinton station 2 3 integration of rbm and mesh rbm was designed to be used as stand alone software or in conjunction with hydrological models such as vic e g van vliet et al 2012 and dhsvm sun et al 2015 to simulate nominal daily water temperatures in river basins in this research we coupled rbm with the mec surface hydrology system mesh model pietroniro et al 2007 carrera et al 2010 to first simulate hydrological processes and then stream temperature along the river reaches to extract the timing of freeze up and breakup events in the arb catchment fig 2 mesh is a semi distributed physically based land surface hydrological modelling system developed by environment climate change canada for hydrological applications pietroniro et al 2007 it uses the canadian land surface scheme verseghy et al 1993 for vertical exchanges and generation of lateral fluxes of energy and water for vegetation soil and snow watrof is incorporated in the model for lateral movement of soil and surface water to the drainage system and the watroute routes streamflow through the river channels using the kinematic wave approach kouwen 1988 it also uses the group response unit gru approach i e combining areas of similar hydrological behaviour to reduce complexity and heterogeneity in the drainage basin for computational efficiency kouwen et al 1993 this is a more suitable approach for large scale drainage basins due to its operational simplicity while retaining the basic physics and behaviour of a distributed model pietroniro and soulis 2003 mesh has been successfully implemented in multiple large scale cold region catchments that face river ice events every year e g davison et al 2006 haghnegahdar et al 2014 mekonnen et al 2014 davison et al 2016 yassin et al 2017 mesh requires seven meteorological input files precipitation air temperature wind speed barometric pressure specific humidity incoming longwave radiation and incoming shortwave radiation precipitation data was obtained from the canadian precipitation analysis database which is operationally run by the canadian meteorological centre cmc the cmc produces accumulated precipitation for 6 h and 12 h time intervals over all of north america on a 10 km 10 km grid fortin et al 2015 precipitation is interpolated using a geostatistical approach by combining cmc s short range precipitation forecasts as the background field a land surface network of rain gauge measurements satellite observations and radar derived rain rates mahfouf et al 2007 the rest of the gridded meteorological forcing data were obtained from the global environmental multiscale model gem which is an integrated forecasting and data assimilation system developed by environment canada côté et al 1998 available at spatial and temporal resolutions of 15 km and 1 h respectively our model coupling approach requires running both mesh and rbm models independently with data from mesh being transferred to rbm mesh and rbm were both set up on the same orthogonal grid with a spatial resolution of 0 1250 resulting in 1326 grid cells and channels that comprehend the athabasca river and tributaries firstly mesh was run at a 3 h time step for the period 01 jan 2002 to 31 dec 2012 to simulate streamflow water depth width and averaged water velocity in every channel these hydrological and hydraulic variables in a second step were aggregated and introduced into rbm at daily time steps similarly to mesh rbm requires meteorological variables at daily time steps however because meteorological observations are rarely discretized at the resolution of each segment node meteorological data were interpolated onto the river network additionally river network topology data necessary for the computations were also introduced into rbm thirdly rbm was run at daily time steps to compute water depth averaged temperatures at every channel of the stream network finally simulated water temperature time series were used to extract the freeze up and breakup dates using a gaussian linear filter algorithm to detect the inflection point of high water temperature gradients when the temperature drops to 0 c in the fall and rises from the same temperature in the spring the gaussian filter uses a variable rectangular window that moves along the time series to smooth the noise associated to the rapid water temperature fluctuations aiding to detect the inflection points 2 4 study case the athabasca river basin is alberta s largest unregulated river system with a total drainage area of more than 150 000 km2 e g cheng et al 2017 see fig 3 the basin extends across the canadian prairie provinces of alberta and saskatchewan starting at the columbia icefield in jasper national park headwaters and travelling 1538 km northeast across alberta to drain into lake athabasca e g bawden et al 2014 the arb has a mean annual temperature of 2 c and average yearly precipitation of 460 mm roughly 70 of which occurs as rain e g bawden et al 2014 canada 2016 a snow melt driven flow characterizes the hydrological regime of the basin beginning in late april or may peak flows generally occur in june or july and are driven by snowpack runoff from the rocky mountains the remaining months experience gradually declining flows until winter when low flow conditions resume in december burn et al 2004 every year the athabasca river experiences multiple ice jam formations and occasional flood events e g andres and doyle 1984 peters et al 2006 ice cover formation in the athabasca river usually happens from october to november while its breakup usually occurs at the end of march or beginning of april pietroniro et al 1998 hutchison and hicks 2007 she et al 2009 every year during the breakup time the risk of ice jam formation and flooding is high in the athabasca river severe floods in 1881 1885 1925 1928 1936 1962 1963 1977 and 1997 have proven the importance of river ice modelling 3 results 3 1 calibration of the hydrology model the calibration of mesh was performed using ostrich which is an open source auto calibration and multi algorithm parameter optimization software matott 2005 within ostrich a parallel version of the dynamically dimensioned search pdds tolson and shoemaker 2008 algorithm was used to calibrate vegetation soil drainage and routing parameters from three dominant group response units grus though mesh was run at a 3 h time step output streamflows were generated at a daily scale for comparison and analysis the model was first calibrated for the 2002 2008 period the nash sutcliffe efficiency nse was estimated to evaluate the model performance since nse leads to an overestimation of the model performance during peak flows and an underestimation during low flow condition the logarithm of the nse log ns was estimated in order to evaluate the model integrally the year 2002 was considered a spin up period and the remaining 6 years as the calibration period then the model was validated for 6 years from 2009 to 2014 the percent bias pbias which measures the average tendency of the simulated data to be larger positive overestimation or smaller negative underestimation than their observed counterparts was also estimated fig 4 shows the results of the mesh calibration and validation of the flows recorded at the gauging station athabasca river below mcmurray 07da001 see fig 3 for station location showing good agreement between simulated and observed flows the ns value of 0 68 log ns value of 0 65 and pbias of 6 8 were achieved for the calibration years whereas for the validation years a ns value of 0 66 log ns value of 0 68 and pbias of 6 9 were obtained slight discrepancies between observed and simulated flows during the falling limb and the low flow period of the hydrographs are related to insufficient complexity of the baseflow algorithms in mesh 3 2 performance of stream temperature simulations rbm was set up using the hydrological variables estimated in mesh and the meteorological forcing data mentioned previously the model was run for the 2002 2012 period and 2002 was chosen as the spin up period daily water temperature simulations were compared to instantaneous water temperature observations unevenly obtained from alberta environment and parks http aep alberta ca water reports data surface water quality data at three different stations located on the athabasca river see fig 3 for station locations these water temperature observations are instantaneous measurements taken approximately between 0 m and 1 m below the water surface around mid day the comparison between observed and simulated water temperature fig 5 shows good agreement with nses 0 8 and determination coefficients r2 0 8 in all the stations revealing the model s ability to resemble the timing and magnitude of observed water temperatures although the model reproduces the daily water temperatures quite well it tends to overestimate water temperature at the upstream and middle catchment sectors pbias 2 2 and underestimate it pbias 1 8 at the outlet of the catchment water temperature overestimation in the highland tributaries can be attributed to the model s inability to fully capture the headwater cooling from snowmelt and glacier melt inflows a characteristic of the simulated water temperature time series is the high frequency oscillation of the temperature along its rising limb during spring and at the peaks during summer such high frequency fluctuations represent model responses to rapid variations in the hydrological and meteorological forcing data 3 3 multi annual stream temperature simulations stream water temperatures change spatially and temporally across the catchment warmer water temperatures are expected during the spring and summer months when air temperatures rise over 25 c in some tributary basins located at the mid catchment area see fig 6 in contrast water temperatures decrease to zero degrees during late autumn when the water surfaces freeze and remain frozen during winter and into early spring when the river ice breaks up in april and may water temperatures of tributaries in the rocky mountains are much lower and under 5 c due to continuous inflows of snow and glacier melt waters however during summer water temperatures of the rocky mountains tributaries rise continuously reaching temperatures higher than 12 c until the end of september after september water cools rapidly in most of the tributaries except in the athabasca river s main stem where water temperatures are still around 10 c due to large water discharges water temperatures decline to values close to 0 o c in november and remain so during the winter at tributaries located in the mid catchment water temperatures are the highest especially at small tributaries where streamflows are relatively low when compared to air temperature fields we observed that air temperature spatial patterns coincide with the simulated water temperatures indicating the high correlation with meteorological forcing data moreover this area is mainly covered by crops and grasslands that reduce stream shading and thus facilitate water heating in such small tributaries the energy balance at the water surface strongly controls the water temperature regime indicating a larger dependency on meteorological forcing conditions than on streamflow yearsley 2012 this leads us to conclude that boundary condition effects on water temperature rapidly decrease downstream and become very weak at the main river network tributaries where thermal regimes are more influenced by the water mixing of inflowing tributaries inter annual maps of water temperature show some temporal changes across the catchments that reflect climate variability see fig 7 during the 2002 2003 period water temperatures appear to be the highest particularly in the mid catchment area where annual averages were around 12 c and coincided with intense droughts across the canadian northern prairies bonsal and regier 2007 yirdaw et al 2008 during the subsequent years water temperatures declined and were characterized by a relatively uniform pattern between 6 and 9 c in the lowland tributaries and between 2 and 4 c in tributaries in the rocky mountains similar patterns of uniform water temperatures were observed during 2007 2008 and 2011 which had been wetter years 3 4 validation of freeze up and breakup timing freeze up and breakup dates were extracted from the simulated stream water temperature time series across the catchment comparing the simulated freeze up and breakup dates against observed b flags where b indicates backwater effect due to the presence of ice in the stream recorded by water survey of canada at the three analysed stations we observed that the model was able to simulate these events with reasonable accuracy and an acceptable determination coefficient r2 range between 0 45 and 0 91 see fig 8 as fig 8b and 8c shows the model is able to simulate the breakup dates slightly better than the freeze up dates with rmse 2 days and rmse 4 days respectively in general the model performs better for the town of athabasca and fort mcmurray stations where nse are around 0 5 and 0 7 for the freeze up and breakup events respectively than at hinton station nse 0 41 at the hinton station we observed that the model error for freeze up and breakup dates was reflected in a rmse of 5 and 10 days respectively uncertainty is quite high in the simulated breakup dates which is shown by a wider uncertainty band in the scatter plot difficulties in predicting breakup events in the athabasca river were reported by martin et al 2013 who implemented a 2d hydrodynamic model and predicted early river ice melt for some years as compared to the other two downstream stations earlier river freeze up and ice cover breakup occur at hinton around the first weeks of november and april respectively temporal variability of the timing of freeze up and breakup events is controlled by the interaction of climatic and hydrological factors increases in air temperature and relatively constant streamflow during 2006 and 2009 were responsible for delaying river water surface freeze up at the towns of athabasca and fort mcmurray by approximately 10 days during the same period at hinton freeze up timing did not change considerably due to slight increases in streamflow that compensated for the air temperature rise in contrast air temperature reductions accompanied by streamflow reductions during 2005 and 2006 accelerated the freeze up of the water surface within 8 days at the three stations despite air temperature rises of around 2 c later freeze up occurred during 2003 and 2004 particularly at the towns of athabasca and fort mcmurray due to significant increases in streamflow of more than 200 during these two years the timing of freeze up was delayed by significant increases in streamflow and reductions in air temperature the timing of ice cover breakup correlates ρ 0 78 with the air temperature changes it is observed that when air temperature increases early breakup occurs at all stations while air temperature decreases delay these events an acceleration of ice cover breakup occurs from 2002 to 2004 at hinton from 2002 to 2005 at the town of athabasca and from 2002 to 2006 at fort mcmurray a considerable spring air temperature rise of more than 3 c was observed during these years at these stations in contrast streamflow fluctuations are slightly reduced at the town of athabasca and fort mcmurray however this is not the case at hinton where the rise in air temperatures clearly enhances the acceleration of breakup by around 15 days in 2002 even though there is a decline in streamflow from 80 to 40 m3 s 1 similarly zachrisson 1989 reported the advancement of breakup events by around 15 days after 3 c air temperature increases in the tornealven river in finland during the 1870 1950 period in contrast decreases in air temperature of more than 2 c during 2005 and 2009 at the town of athabasca and fort mcmurray stations delayed breakup occurrence by more than 10 days in 2005 3 5 spatio temporal variation of freeze up and breakup events the timing of freeze up and breakup events varies spatially across the catchment although freeze up date maps fig 9 do not show consistent spatial patterns we can observe that freeze up will always occur later at the main tributaries than in small streams influenced mainly by air temperature variability main tributaries froze over during early november and small tributaries in early october however in tributaries located in the mid catchment freeze up occurs later due to higher air and water temperatures at tributaries located in the rocky mountains in the north western areas of the catchment early freeze up is expected due to lower air and water temperatures for example analysing the timing of freeze up events along the athabasca river see fig 11 we observe in general that freeze up advances in time with increases in air and water temperature and streamflow at the headwater tributaries in the rocky mountains freeze up occurs during the second week of october freeze up occurs later downstream and reaches a maximum freeze up timing during the third week of november in reaches located in the foothills freeze up timing occurs earlier until the end of october near hinton but occurs progressively later in the flow direction quite smoothly and linearly until it reaches downstream of the town of athabasca where freeze up occurs consistently by the second week of november consistent freeze up timing occurs until near the outlet where decreasing air and water temperatures and increases in streamflow accelerate freeze up during the first week of november the timing of breakup and freeze up events vary not only spatially but also temporally in the catchment comparing simulated and observed boxplots of freeze up dates see the inset in fig 9 we observed the capability of the model in capturing the temporal variability of freeze up events throughout the analysed years as mentioned before variability in freeze up timing is mostly associated with air temperature changes therefore during 2002 and 2005 the freeze up occurrence shifted from the third week of october to the first week of november after 2005 freeze up occurs earlier with the earliest occurring around the second week of october in 2009 there is then a drastic delay in 2010 when freeze up occurred in the first week of november freeze up accelerates in the later years occurring in the third week of october in 2012 due to a reduction in autumn air and water temperatures the spatial behaviour of breakup dates exhibits relatively clear patterns see fig 10 at tributaries located in the rocky mountains breakup events occur in late may however whereas downstream of hinton particularly in large tributaries breakup occurs earlier in late march and early april mid may breakups are observed in tributaries located in north eastern areas of the catchments and along main reaches of the athabasca river such tributaries where breakup occurs mostly in may correspond to catchment areas with low air and water temperatures very early breakups are observed during mid march particularly in 2005 2007 and 2009 at tributaries in the mid catchment area between hinton and the town of athabasca due to high air and water temperatures analysing breakup timing along these reaches of the athabasca river we observed in general that breakup timing occurs earlier and is consistent with increases in air and water temperature and streamflow along the athabasca river see fig 11 b in the river breakup begins along the foothill reaches near the rocky mountains in the first week of march after 20 days during the first week of april breakup occurs nearly at the same time in reaches located between hinton and the town of athabasca breakup advances progressively until fort mcmurray where breakup generally occurs during the second week of april late breakup occurs in mid june at the headwater reaches in the rocky mountains and just upstream of hinton where air and water temperatures are quite low and streamflows are close to 5 m3 s 1 comparing box plots of observed and simulated breakup dates for the simulated years we observed some discrepancies but similar variance in both data sets as mentioned before the model tends to underestimate the timing of breakup events particularly in the lowlands despite the discrepancies a periodic breakup timing behaviour controlled by air temperature variability is observed in both data sets breakup occurs progressively earlier from 2002 to 2006 later breakup events occurred until 2008 earlier breakup occurred again from 2008 to 2010 remaining constant until 2012 4 discussion and conclusions 4 1 estimation of the timing of river ice freeze up and breakup events the estimation of the timing of river ice freeze up and breakup events involves many challenges computational models such as hydraulic models equipped with energy balance algorithms have been successfully utilized e g lindenschmidt et al 2012a b to estimate stream water level velocity and discharge necessary to trigger ice formation during autumn and ice breakup during spring at local scales the validation of these models has always been challenging because the observed data sets are taken infrequently and rely on subjective opinions also due to the scale of the model application lack of data impedes the correct setup of model boundary conditions consisting of a streamflow hydrograph and a stream water temperature time series our hydrological and water temperature modelling approach represents a potential tool for estimating the timing of river ice freeze up and breakup events on a large scale basis in contrast to hydraulic river ice models this modelling approach shows the spatio temporal variability on a large scale of how freeze up and breakup events change the model not only allows for the evaluation of the effects of climate and hydrological variability temporally but also spatially in order to understand those patterns that characterized the occurrence of these events this approach presents a novel modelling system that could provide useful information to watershed managers regarding the timing of river freeze up and ice cover breakup within the catchment in order to establish ice jam flood mitigation plans in the past different authors have analysed river ice records and spring and autumn climate data to understand the temporal and spatial variability of freeze up and breakup events e g magnuson et al 2000 prowse and bonsal 2004 commonly the spatial and temporal variability of the air temperature 0 o c isotherm has been studied regarding changes in freeze up and breakup events and has been linked to the retreat or the advancement of the isotherm for instance bonsal and prowse 2003 found significantly earlier spring dates of the timing of the 0 o c isotherm in western canada whereas the trend is weaker in eastern canada prowse et al 2007 reported a higher correlation between the spring 0 o c isotherm and ice off dates for rivers located in the interior cold regions of canada but lower correlation for warmer rivers located in maritime climate zones although these methodologies help us to understand the links between climatic drivers and river freeze up and ice cover breakup events they do not consider the hydraulic and thermodynamic processes involved in these two events gray and prowse 1993 indicated that south to north temperature gradients from the headwaters to the lowlands are sometimes dominated by ice and glacier melt and lowland streams in cold region catchments can impact breakup events substantially our modelling approach combines meteorological hydrological and hydrothermal drivers to estimate the timing of river freeze up and ice breakup events retrieved from stream water 0 c isotherm simulated after coupling a physically based hydrological model and a water temperature model in general our model shows that in the main tributaries of the arb freeze up timing spans from the last week of september to the second week of november and becomes progressively later non linearly from the headwater reaches to the catchment outlet the breakup timing spans from the second week of march to the last week of may and occurs first at headwater reaches and lastly at the intermediate and outlet reaches the model reports high correlation between freeze up and breakup dates with meteorological and hydrological variables also showing that such events are spatially controlled by non linear increases in air and water temperatures and streamflows along the streams although it is known that the formation of river ice is driven by atmospheric and hydrological conditions prowse and beltaos 2002 prowse 2006 we do not know in which proportion these two factors affect river freeze up and ice cover breakup our model shows that for instance spatial negative positive gradients of air temperature can trigger later earlier freeze up events in contiguous catchment tributaries particularly in the athabasca river slightly negative gradients of air temperature near the catchment outlet downstream of fort mcmurray can cause earlier freeze up despite relatively substantial streamflow increases according to the model results it seems that freeze up timing correlates better and is more sensitive to changes in air temperature than to changes in streamflow in the catchment results reported by ginzburg 1992 and soldatova 1992 indicate strong correlations between autumn and spring air temperature and freeze up and breakup timing respectively although there is a high correspondence between observed and simulated breakup timing at the stations in the towns of athabasca and fort mcmurray breakup events show larger uncertainty in other regions within the catchment smith 2000 reported a lack of significant trends in breakup events that might be related to inter decadal variability and inconsistent data length prowse and bonsal 2004 other sources of error may be related to the fact that different criteria have been applied to recorded breakup events for instance some agencies report the dates of breakup initiation whereas others record the date of ice clearance at the end of breakup since differences between these two dates can sometimes be as much as 4 weeks that may represent a large source of discrepancies 4 2 spatial variability of river ice freeze up and breakup events theoretically at freeze up an ice cover will initiate at a bridging for example at the basin outlet and extend towards the headwaters by juxtapositioning when frazil ice that is generated in the upstream open water stretches accumulates at the ice cover front to extend it in the upstream direction however it is quite common that bridgings occur at several points along the river often at river width constrictions sharp meanders bridge piers or islands to form many ice covers simultaneously during the river freezing period this was observed along the dauphin river in manitoba when four ice covers were formed along a 40 km stretch in just a few days during a cold bout lindenschmidt et al 2013 hence it should not be a surprise to observe freeze up dates often occurring earlier at hinton than at the town of athabasca see fig 12 top left panel with most of the freeze up date points lying below the 1 1 line indicating an earlier freeze over of the athabasca river at the hinton reach than the reach at the town of athabasca many of the freeze up date points for the town of athabasca and fort mcmurray lie on the 1 1 line fig 12 top right panel indicating that the freeze over of the river stretch between these two towns can be quite rapid which is also reflected in the profiles of fig 11 however freeze over can be later at the town of athabasca than at fort mcmurray which is captured in the uncertainty band of the profile in fig 11 better accuracy of the freeze up dates may be obtained when a river ice hydraulics model for example rivice lindenschmidt 2017 is coupled to the mesh rbm system a topic of future work for initial steps of this research please see das et al 2017 rokaya et al 2017 and rokaya et al accepted for rivers flowing in a northerly direction breakup theoretically begins at the headwaters and works its way downstream towards the basin outlet breakup consistently occurred earlier at the town of athabasca than at fort mcmurray see fig 12 bottom right panel on average approximately 3 4 days earlier this is reflected in the breakup profile in fig 12 however the breakup dates between hinton and the town of athabasca are quite variable sometimes hinton experiences an ice cover breakup earlier sometimes later than at the town of athabasca the variability in the breakup times between the two towns can be quite substantial up to several weeks difference which is also captured in the uncertainty band of the breakup dates profile in fig 11 4 3 model limitations and future work some model limitations need to be addressed for future work although the proposed framework provides a flexible approach to feeding mesh output results into the rbm stream water temperature model there are some key sources of uncertainty that will need to be kept in perspective while applying the methodology the four primary sources of uncertainty in the current study are 1 water temperature boundary conditions at headwater reaches usually dominated by snow and glacier melt waters 2 lack of consideration of anthropogenic impacts along the main catchment streams 3 propagation of errors from mesh to rbm and 4 meteorological forcing data uncertainties particularly those associated with precipitation products we believe that these uncertainties introduce errors in the estimation of freeze up and breakup events which need to be considered in future work to address the first point we implemented remote sensing techniques to extract water temperatures in mountainous areas to impose boundary conditions at the headwaters although acceptable results were obtained more effort needs to be made in order to obtain longer records and apply them in areas which lack water temperature records in relation to anthropogenic impacts in catchments mesh has been substantially improved in order to consider streamflow regulation and water abstraction factors that modify hydrological functioning in downstream waters affecting river ice dynamics errors in streamflow and hydraulic variables introduced in rbm and estimated in mesh need to be reduced by a better discretization of both the catchment domain and meteorological forcing data future modelling work using the approach described here will investigate climate change scenarios to predict ice phenology estimation of rbm boundary conditions using the class energy balance computations is worthy to be tested in future works in order to facilitate the imposition of boundary conditions in headwater catchments this modelling approach is potentially useful for decision support for example for water quality planning hydropower management and resource management on a large scale risk analyses of ice jam floods and to study climate change impacts of large river basins around the world acknowledgments this study was funded by the university of saskatchewan s global institute for water security through the canada excellence research chair cerc in water security and the global water future program 
26250,ice phenology defined as the timing of freeze up and ice cover breakup plays a key role in streamflow regimes in cold region river catchments river freeze up and ice cover breakup events are controlled by meteorological and hydrological variables in this study we present a modelling framework consisting of a physically based semi distributed hydrological model and the integration of a 1d stream temperature model that can predict the ice duration in cold region rivers the hydrological model provides streamflow and hydraulic parameters for the stream temperature model to obtain instream water temperature the model was successfully applied in the athabasca river basin in western canada calibration was carried out using the water temperature recorded in the stations at the towns of hinton athabasca and fort mcmurray model results show consistent correspondence between simulated freeze up and breakup dates and the hydrometric station data in the main tributaries of the basin freeze up timing spans from the last week of september to the second week of november and ice cover breakup occurs from the second week of march to the last week of may the model presents an application of water temperature and ice phenology simulation which can be incorporated in ice jam flood forecasting and future climate change studies keywords stream temperature simulation freeze up breakup athabasca river basin 1 introduction in mid to high latitude lakes and rivers the formation and breakup of ice covers are important seasonal events river freeze up and ice cover breakup events are influenced by stream water and air temperature and are sensitive to the characteristics of individual waterbodies beltaos and prowse 2001 in cold region rivers the ice phenology plays a key role in the effects of ice on river bank and bed erosion sedimentation and regulation of the stream flow regime in winter e g beltaos and prowse 2009 terry et al 2017 climate influences the timing of freeze onset ice duration ice melt and thermal dynamics that feed back to the climate system initiating further changes the ice formation process in rivers is initiated once the air temperature has been below 0 c long enough to super cool the water and form frazil ice e g osterkamp 1978 beltaos 2000 dynamic physical and mechanical factors are involved in the formation of ice covers turcotte and morse 2013 on lentic waters such as lakes and along river shorelines with low flow velocity a solid continuous ice sheet forms with a horizontal uniform structure this ice sheet originates from either the waterbody shores or pre formed plate ice that extends outward to the center of the waterbody in rivers with more turbulent flow the formation is more dynamic and mainly results after the accumulation of frazil slush pancake ice and ice floes e g beltaos 2000 prowse and beltaos 2002 accumulation on rivers is either an ice jam or a hummocked ice cover pressure ice mounds haphazardly arranged when it refreezes the breakup of river ice covers is triggered by mild weather conditions and encompasses a variety of processes associated with thermal deterioration initial fracture movement fragmentation transport jamming and final clearance of ice during ice cover breakup multiple stages such as pre breakup onset drive and wash occur concurrently within a specific reach beltaos 2008 during the pre breakup phase the ice cover becomes more susceptible to fracture and movement via thermal and solar radiation induced reductions in its thickness and strength at the same time spring and early summer weather conditions bring about snowmelt and rainfall events that cause an increase in flow discharges and the rise of stream water levels that fracture drive and wash out the ice phenology records obtained over long time periods and at regular time intervals are thus required to better understand inconsistencies and changes in meteorological conditions and how such changes impact river ice processes latifovic and pouliot 2007 due to the lack of observed records stream water temperature models constitute important tools to simulate ice phenology data especially in ungauged catchments several model approaches such as linear and multivariable regression methods neural network models and thermal energy budget methods have been developed and implemented to simulate stream water temperature e g benyahya et al 2007 dugdale et al 2017 most of these data driven models have been applied to specific stream reaches and calibrated using observed data in contrast mechanistic models have been applied for regional and catchment scale problems to understand the spatial and temporal behaviour of the energy transfer mechanisms controlling water temperature stream temperature models have been utilized for short term e g daily and long term from weeks to years water temperature forecasting dugdale et al 2017 similarly water temperature hindcasting has been performed to reconstruct historical records benyahya et al 2007 water temperature models are also employed to investigate the potential impact of anthropogenic effects on the thermal regime of rivers sun et al 2015 and the implications for aquatic life and ecosystems e g gu et al 1999 despite the variety of applications physically based distributed stream water temperature models have not been used to simulate the timing of freeze up and breakup events in large river catchments in this paper the river basin model rbm a semi lagrangian water temperature model for advection dominated river systems yearsley 2012 was coupled to mesh a semi distributed hydrological model pietroniro et al 2007 to simulate stream water temperature in the athabasca river basin arb although the model has been successfully coupled to the variable infiltration capacity vic model e g van vliet et al 2012 widely implemented in large scale hydrological modelling the mesh rbm system is more suited to cold region catchments due to its ability to simulate snow fall snow redistribution and snow melt and their effect on stream water temperature due to the lack of stream water temperature data in the upland areas of the arb temperature data in these areas was extracted from thermal infrared imagery via remote sensing techniques to impose model boundary conditions at the headwater streams simulated stream water temperature time series were used to estimate the timing of river freeze up and ice cover breakup events in the arb the model may be useful for improving ice jam flood event preparedness and mitigation and to study future ice phenology 2 methods 2 1 rbm model description rbm is a mechanistic one dimensional stream temperature model that solves the 1d head advection equation 1 d t w a x d t h a i r w a t e r w x ρ w c p q t r b δ t t r b δ x q e f f δ t e f f δ x where ρ w is water density kg m 3 c p is the specific heat of water j kg 1 o c 1 t w is water temperature o c a x is the cross section area at distance x m 2 w x is stream width at distance x m q t r b is the advected flow from tributaries or the subsurface m 3 s 1 δ t t r b is the difference between advected temperature from tributaries or the subsurface respectively t t r b and t w o c q e f f is the advected flow from head dumps e g thermoelectric power plants m 3 s 1 δ t e f f is the difference between advected temperature from head dumps t e f f and t w o c δ x is the longitudinal distance along the river axis m and t is time s the net heat flux at the air water interface h a i r w a t e r is calculated in j m 2 s 1 as 2 h a i r w a t e r h s w h r s h a h a r h e v a p h c o n d h b a c k where h s w is short wave solar radiation j m 2 s 1 h r s is reflected short wave solar radiation j m 2 s 1 h a is long wave atmospheric radiation j m 2 s 1 h a r is reflected atmospheric radiation j m 2 s 1 h e v a p is the evaporative heat flux j m 2 s 1 h c o n d is conductive heat flux j m 2 s 1 and h b a c k is black body radiation from the water surface j m 2 s 1 although some authors have considered hyporheic flow riparian vegetation and stream bed head exchange important factors affecting h a i r w a t e r equation 2 these are not relevant due to the scale of our study case and the purpose of this research rbm uses an explicit semi lagrangian numerical scheme yearsley 2009 to resolve equation 1 the scheme tracks individual water packets along the river network in the flow direction constituted by interconnected segments and nodes of homogeneous characteristics and saves the simulated water temperature at every segment of the network 2 2 boundary conditions in rbm rbm requires water temperature boundary conditions at the basin headwaters different approaches such as equalizing water temperature with simulated daily soil temperature from land surface numerical models and estimating water temperature as a function of air temperature have been tested before van vliet et al 2012 the non linear water temperature regression model described by mohseni et al 1998 has yielded acceptable results this model relates weekly water temperatures as a non linear function s curve of weekly air temperatures as 3 t w h e a d μ α μ 1 e γ β t a s m o o t h where μ is the lower bound of water temperature o c α is the upper bound of water temperature o c γ 4 tan θ α μ is the measure of the slope at the inflection point of the s shaped relation o c 1 where tan θ is the slope at the inflection point β is the air temperature at the inflection point o c and t w h e a d is the headwater temperature o c as water temperature variation lags behind air temperature fluctuations at short time scales e g daily basis t a s m o o t h is the smoothed air temperature o c represented as 4 t a s m o o t h 1 λ t a t 1 λ t a t where t a t 1 is air temperature at time t 1 o c and t a t is air temperature at time t o c two different parameter sets for first order headwater streams located either in the lowlands or in the highlands were established for the lowland headwaters μ 0 5 α 20 5 γ 0 2 β 6 5 and λ 0 1 were chosen from the literature considering the geographical similarities between the multiple basins analysed in bawden et al 2014 in the rocky mountains glacier melt dominates the athabasca river flow regime and constitutes an important source of cold water into the system due to the lack of water temperature records at these locations remote sensing tools were used to retrieve water surface temperatures from 13 cloud free and snow free locations landsat 5 and 8 thermal images were retrieved for the time frame between september 2003 and september 2016 those images have a 30 m resolution and are terrain corrected level 1 products downloaded from usgs 2018 for landsat 8 thermal images only thermal band 10 10 6 11 9 μm was used because thermal band 11 11 50 12 51 μm is significantly affected by stray light and thus not recommended for quantitative analysis montanaro et al 2014 the mono window method developed by qin et al 2001 was used to derive water surface temperatures at jasper lake the derived water temperature and the observed hourly air temperature at 10 am mountain time zone at the jasper warden weather station e g fig 1 a were used to fit the parameters in equation 3 using the least squared error method see fig 1b and c the fitted model performed relatively well nash sutcliffe efficiency nse 0 54 r2 0 55 with low error rmse 0 72 c and bias pbias 0 8 after comparing simulated against retrieved surface water temperatures at jasper lake fig 1c the resulting parameters μ 0 5 α 16 28171 γ 0 103247 β 2 440703 and λ 0 1 were in agreement with the ranges established by van vliet et al 2012 the calibrated parameters were used to set up the boundary conditions at all the headwater streams located upstream of the hinton station 2 3 integration of rbm and mesh rbm was designed to be used as stand alone software or in conjunction with hydrological models such as vic e g van vliet et al 2012 and dhsvm sun et al 2015 to simulate nominal daily water temperatures in river basins in this research we coupled rbm with the mec surface hydrology system mesh model pietroniro et al 2007 carrera et al 2010 to first simulate hydrological processes and then stream temperature along the river reaches to extract the timing of freeze up and breakup events in the arb catchment fig 2 mesh is a semi distributed physically based land surface hydrological modelling system developed by environment climate change canada for hydrological applications pietroniro et al 2007 it uses the canadian land surface scheme verseghy et al 1993 for vertical exchanges and generation of lateral fluxes of energy and water for vegetation soil and snow watrof is incorporated in the model for lateral movement of soil and surface water to the drainage system and the watroute routes streamflow through the river channels using the kinematic wave approach kouwen 1988 it also uses the group response unit gru approach i e combining areas of similar hydrological behaviour to reduce complexity and heterogeneity in the drainage basin for computational efficiency kouwen et al 1993 this is a more suitable approach for large scale drainage basins due to its operational simplicity while retaining the basic physics and behaviour of a distributed model pietroniro and soulis 2003 mesh has been successfully implemented in multiple large scale cold region catchments that face river ice events every year e g davison et al 2006 haghnegahdar et al 2014 mekonnen et al 2014 davison et al 2016 yassin et al 2017 mesh requires seven meteorological input files precipitation air temperature wind speed barometric pressure specific humidity incoming longwave radiation and incoming shortwave radiation precipitation data was obtained from the canadian precipitation analysis database which is operationally run by the canadian meteorological centre cmc the cmc produces accumulated precipitation for 6 h and 12 h time intervals over all of north america on a 10 km 10 km grid fortin et al 2015 precipitation is interpolated using a geostatistical approach by combining cmc s short range precipitation forecasts as the background field a land surface network of rain gauge measurements satellite observations and radar derived rain rates mahfouf et al 2007 the rest of the gridded meteorological forcing data were obtained from the global environmental multiscale model gem which is an integrated forecasting and data assimilation system developed by environment canada côté et al 1998 available at spatial and temporal resolutions of 15 km and 1 h respectively our model coupling approach requires running both mesh and rbm models independently with data from mesh being transferred to rbm mesh and rbm were both set up on the same orthogonal grid with a spatial resolution of 0 1250 resulting in 1326 grid cells and channels that comprehend the athabasca river and tributaries firstly mesh was run at a 3 h time step for the period 01 jan 2002 to 31 dec 2012 to simulate streamflow water depth width and averaged water velocity in every channel these hydrological and hydraulic variables in a second step were aggregated and introduced into rbm at daily time steps similarly to mesh rbm requires meteorological variables at daily time steps however because meteorological observations are rarely discretized at the resolution of each segment node meteorological data were interpolated onto the river network additionally river network topology data necessary for the computations were also introduced into rbm thirdly rbm was run at daily time steps to compute water depth averaged temperatures at every channel of the stream network finally simulated water temperature time series were used to extract the freeze up and breakup dates using a gaussian linear filter algorithm to detect the inflection point of high water temperature gradients when the temperature drops to 0 c in the fall and rises from the same temperature in the spring the gaussian filter uses a variable rectangular window that moves along the time series to smooth the noise associated to the rapid water temperature fluctuations aiding to detect the inflection points 2 4 study case the athabasca river basin is alberta s largest unregulated river system with a total drainage area of more than 150 000 km2 e g cheng et al 2017 see fig 3 the basin extends across the canadian prairie provinces of alberta and saskatchewan starting at the columbia icefield in jasper national park headwaters and travelling 1538 km northeast across alberta to drain into lake athabasca e g bawden et al 2014 the arb has a mean annual temperature of 2 c and average yearly precipitation of 460 mm roughly 70 of which occurs as rain e g bawden et al 2014 canada 2016 a snow melt driven flow characterizes the hydrological regime of the basin beginning in late april or may peak flows generally occur in june or july and are driven by snowpack runoff from the rocky mountains the remaining months experience gradually declining flows until winter when low flow conditions resume in december burn et al 2004 every year the athabasca river experiences multiple ice jam formations and occasional flood events e g andres and doyle 1984 peters et al 2006 ice cover formation in the athabasca river usually happens from october to november while its breakup usually occurs at the end of march or beginning of april pietroniro et al 1998 hutchison and hicks 2007 she et al 2009 every year during the breakup time the risk of ice jam formation and flooding is high in the athabasca river severe floods in 1881 1885 1925 1928 1936 1962 1963 1977 and 1997 have proven the importance of river ice modelling 3 results 3 1 calibration of the hydrology model the calibration of mesh was performed using ostrich which is an open source auto calibration and multi algorithm parameter optimization software matott 2005 within ostrich a parallel version of the dynamically dimensioned search pdds tolson and shoemaker 2008 algorithm was used to calibrate vegetation soil drainage and routing parameters from three dominant group response units grus though mesh was run at a 3 h time step output streamflows were generated at a daily scale for comparison and analysis the model was first calibrated for the 2002 2008 period the nash sutcliffe efficiency nse was estimated to evaluate the model performance since nse leads to an overestimation of the model performance during peak flows and an underestimation during low flow condition the logarithm of the nse log ns was estimated in order to evaluate the model integrally the year 2002 was considered a spin up period and the remaining 6 years as the calibration period then the model was validated for 6 years from 2009 to 2014 the percent bias pbias which measures the average tendency of the simulated data to be larger positive overestimation or smaller negative underestimation than their observed counterparts was also estimated fig 4 shows the results of the mesh calibration and validation of the flows recorded at the gauging station athabasca river below mcmurray 07da001 see fig 3 for station location showing good agreement between simulated and observed flows the ns value of 0 68 log ns value of 0 65 and pbias of 6 8 were achieved for the calibration years whereas for the validation years a ns value of 0 66 log ns value of 0 68 and pbias of 6 9 were obtained slight discrepancies between observed and simulated flows during the falling limb and the low flow period of the hydrographs are related to insufficient complexity of the baseflow algorithms in mesh 3 2 performance of stream temperature simulations rbm was set up using the hydrological variables estimated in mesh and the meteorological forcing data mentioned previously the model was run for the 2002 2012 period and 2002 was chosen as the spin up period daily water temperature simulations were compared to instantaneous water temperature observations unevenly obtained from alberta environment and parks http aep alberta ca water reports data surface water quality data at three different stations located on the athabasca river see fig 3 for station locations these water temperature observations are instantaneous measurements taken approximately between 0 m and 1 m below the water surface around mid day the comparison between observed and simulated water temperature fig 5 shows good agreement with nses 0 8 and determination coefficients r2 0 8 in all the stations revealing the model s ability to resemble the timing and magnitude of observed water temperatures although the model reproduces the daily water temperatures quite well it tends to overestimate water temperature at the upstream and middle catchment sectors pbias 2 2 and underestimate it pbias 1 8 at the outlet of the catchment water temperature overestimation in the highland tributaries can be attributed to the model s inability to fully capture the headwater cooling from snowmelt and glacier melt inflows a characteristic of the simulated water temperature time series is the high frequency oscillation of the temperature along its rising limb during spring and at the peaks during summer such high frequency fluctuations represent model responses to rapid variations in the hydrological and meteorological forcing data 3 3 multi annual stream temperature simulations stream water temperatures change spatially and temporally across the catchment warmer water temperatures are expected during the spring and summer months when air temperatures rise over 25 c in some tributary basins located at the mid catchment area see fig 6 in contrast water temperatures decrease to zero degrees during late autumn when the water surfaces freeze and remain frozen during winter and into early spring when the river ice breaks up in april and may water temperatures of tributaries in the rocky mountains are much lower and under 5 c due to continuous inflows of snow and glacier melt waters however during summer water temperatures of the rocky mountains tributaries rise continuously reaching temperatures higher than 12 c until the end of september after september water cools rapidly in most of the tributaries except in the athabasca river s main stem where water temperatures are still around 10 c due to large water discharges water temperatures decline to values close to 0 o c in november and remain so during the winter at tributaries located in the mid catchment water temperatures are the highest especially at small tributaries where streamflows are relatively low when compared to air temperature fields we observed that air temperature spatial patterns coincide with the simulated water temperatures indicating the high correlation with meteorological forcing data moreover this area is mainly covered by crops and grasslands that reduce stream shading and thus facilitate water heating in such small tributaries the energy balance at the water surface strongly controls the water temperature regime indicating a larger dependency on meteorological forcing conditions than on streamflow yearsley 2012 this leads us to conclude that boundary condition effects on water temperature rapidly decrease downstream and become very weak at the main river network tributaries where thermal regimes are more influenced by the water mixing of inflowing tributaries inter annual maps of water temperature show some temporal changes across the catchments that reflect climate variability see fig 7 during the 2002 2003 period water temperatures appear to be the highest particularly in the mid catchment area where annual averages were around 12 c and coincided with intense droughts across the canadian northern prairies bonsal and regier 2007 yirdaw et al 2008 during the subsequent years water temperatures declined and were characterized by a relatively uniform pattern between 6 and 9 c in the lowland tributaries and between 2 and 4 c in tributaries in the rocky mountains similar patterns of uniform water temperatures were observed during 2007 2008 and 2011 which had been wetter years 3 4 validation of freeze up and breakup timing freeze up and breakup dates were extracted from the simulated stream water temperature time series across the catchment comparing the simulated freeze up and breakup dates against observed b flags where b indicates backwater effect due to the presence of ice in the stream recorded by water survey of canada at the three analysed stations we observed that the model was able to simulate these events with reasonable accuracy and an acceptable determination coefficient r2 range between 0 45 and 0 91 see fig 8 as fig 8b and 8c shows the model is able to simulate the breakup dates slightly better than the freeze up dates with rmse 2 days and rmse 4 days respectively in general the model performs better for the town of athabasca and fort mcmurray stations where nse are around 0 5 and 0 7 for the freeze up and breakup events respectively than at hinton station nse 0 41 at the hinton station we observed that the model error for freeze up and breakup dates was reflected in a rmse of 5 and 10 days respectively uncertainty is quite high in the simulated breakup dates which is shown by a wider uncertainty band in the scatter plot difficulties in predicting breakup events in the athabasca river were reported by martin et al 2013 who implemented a 2d hydrodynamic model and predicted early river ice melt for some years as compared to the other two downstream stations earlier river freeze up and ice cover breakup occur at hinton around the first weeks of november and april respectively temporal variability of the timing of freeze up and breakup events is controlled by the interaction of climatic and hydrological factors increases in air temperature and relatively constant streamflow during 2006 and 2009 were responsible for delaying river water surface freeze up at the towns of athabasca and fort mcmurray by approximately 10 days during the same period at hinton freeze up timing did not change considerably due to slight increases in streamflow that compensated for the air temperature rise in contrast air temperature reductions accompanied by streamflow reductions during 2005 and 2006 accelerated the freeze up of the water surface within 8 days at the three stations despite air temperature rises of around 2 c later freeze up occurred during 2003 and 2004 particularly at the towns of athabasca and fort mcmurray due to significant increases in streamflow of more than 200 during these two years the timing of freeze up was delayed by significant increases in streamflow and reductions in air temperature the timing of ice cover breakup correlates ρ 0 78 with the air temperature changes it is observed that when air temperature increases early breakup occurs at all stations while air temperature decreases delay these events an acceleration of ice cover breakup occurs from 2002 to 2004 at hinton from 2002 to 2005 at the town of athabasca and from 2002 to 2006 at fort mcmurray a considerable spring air temperature rise of more than 3 c was observed during these years at these stations in contrast streamflow fluctuations are slightly reduced at the town of athabasca and fort mcmurray however this is not the case at hinton where the rise in air temperatures clearly enhances the acceleration of breakup by around 15 days in 2002 even though there is a decline in streamflow from 80 to 40 m3 s 1 similarly zachrisson 1989 reported the advancement of breakup events by around 15 days after 3 c air temperature increases in the tornealven river in finland during the 1870 1950 period in contrast decreases in air temperature of more than 2 c during 2005 and 2009 at the town of athabasca and fort mcmurray stations delayed breakup occurrence by more than 10 days in 2005 3 5 spatio temporal variation of freeze up and breakup events the timing of freeze up and breakup events varies spatially across the catchment although freeze up date maps fig 9 do not show consistent spatial patterns we can observe that freeze up will always occur later at the main tributaries than in small streams influenced mainly by air temperature variability main tributaries froze over during early november and small tributaries in early october however in tributaries located in the mid catchment freeze up occurs later due to higher air and water temperatures at tributaries located in the rocky mountains in the north western areas of the catchment early freeze up is expected due to lower air and water temperatures for example analysing the timing of freeze up events along the athabasca river see fig 11 we observe in general that freeze up advances in time with increases in air and water temperature and streamflow at the headwater tributaries in the rocky mountains freeze up occurs during the second week of october freeze up occurs later downstream and reaches a maximum freeze up timing during the third week of november in reaches located in the foothills freeze up timing occurs earlier until the end of october near hinton but occurs progressively later in the flow direction quite smoothly and linearly until it reaches downstream of the town of athabasca where freeze up occurs consistently by the second week of november consistent freeze up timing occurs until near the outlet where decreasing air and water temperatures and increases in streamflow accelerate freeze up during the first week of november the timing of breakup and freeze up events vary not only spatially but also temporally in the catchment comparing simulated and observed boxplots of freeze up dates see the inset in fig 9 we observed the capability of the model in capturing the temporal variability of freeze up events throughout the analysed years as mentioned before variability in freeze up timing is mostly associated with air temperature changes therefore during 2002 and 2005 the freeze up occurrence shifted from the third week of october to the first week of november after 2005 freeze up occurs earlier with the earliest occurring around the second week of october in 2009 there is then a drastic delay in 2010 when freeze up occurred in the first week of november freeze up accelerates in the later years occurring in the third week of october in 2012 due to a reduction in autumn air and water temperatures the spatial behaviour of breakup dates exhibits relatively clear patterns see fig 10 at tributaries located in the rocky mountains breakup events occur in late may however whereas downstream of hinton particularly in large tributaries breakup occurs earlier in late march and early april mid may breakups are observed in tributaries located in north eastern areas of the catchments and along main reaches of the athabasca river such tributaries where breakup occurs mostly in may correspond to catchment areas with low air and water temperatures very early breakups are observed during mid march particularly in 2005 2007 and 2009 at tributaries in the mid catchment area between hinton and the town of athabasca due to high air and water temperatures analysing breakup timing along these reaches of the athabasca river we observed in general that breakup timing occurs earlier and is consistent with increases in air and water temperature and streamflow along the athabasca river see fig 11 b in the river breakup begins along the foothill reaches near the rocky mountains in the first week of march after 20 days during the first week of april breakup occurs nearly at the same time in reaches located between hinton and the town of athabasca breakup advances progressively until fort mcmurray where breakup generally occurs during the second week of april late breakup occurs in mid june at the headwater reaches in the rocky mountains and just upstream of hinton where air and water temperatures are quite low and streamflows are close to 5 m3 s 1 comparing box plots of observed and simulated breakup dates for the simulated years we observed some discrepancies but similar variance in both data sets as mentioned before the model tends to underestimate the timing of breakup events particularly in the lowlands despite the discrepancies a periodic breakup timing behaviour controlled by air temperature variability is observed in both data sets breakup occurs progressively earlier from 2002 to 2006 later breakup events occurred until 2008 earlier breakup occurred again from 2008 to 2010 remaining constant until 2012 4 discussion and conclusions 4 1 estimation of the timing of river ice freeze up and breakup events the estimation of the timing of river ice freeze up and breakup events involves many challenges computational models such as hydraulic models equipped with energy balance algorithms have been successfully utilized e g lindenschmidt et al 2012a b to estimate stream water level velocity and discharge necessary to trigger ice formation during autumn and ice breakup during spring at local scales the validation of these models has always been challenging because the observed data sets are taken infrequently and rely on subjective opinions also due to the scale of the model application lack of data impedes the correct setup of model boundary conditions consisting of a streamflow hydrograph and a stream water temperature time series our hydrological and water temperature modelling approach represents a potential tool for estimating the timing of river ice freeze up and breakup events on a large scale basis in contrast to hydraulic river ice models this modelling approach shows the spatio temporal variability on a large scale of how freeze up and breakup events change the model not only allows for the evaluation of the effects of climate and hydrological variability temporally but also spatially in order to understand those patterns that characterized the occurrence of these events this approach presents a novel modelling system that could provide useful information to watershed managers regarding the timing of river freeze up and ice cover breakup within the catchment in order to establish ice jam flood mitigation plans in the past different authors have analysed river ice records and spring and autumn climate data to understand the temporal and spatial variability of freeze up and breakup events e g magnuson et al 2000 prowse and bonsal 2004 commonly the spatial and temporal variability of the air temperature 0 o c isotherm has been studied regarding changes in freeze up and breakup events and has been linked to the retreat or the advancement of the isotherm for instance bonsal and prowse 2003 found significantly earlier spring dates of the timing of the 0 o c isotherm in western canada whereas the trend is weaker in eastern canada prowse et al 2007 reported a higher correlation between the spring 0 o c isotherm and ice off dates for rivers located in the interior cold regions of canada but lower correlation for warmer rivers located in maritime climate zones although these methodologies help us to understand the links between climatic drivers and river freeze up and ice cover breakup events they do not consider the hydraulic and thermodynamic processes involved in these two events gray and prowse 1993 indicated that south to north temperature gradients from the headwaters to the lowlands are sometimes dominated by ice and glacier melt and lowland streams in cold region catchments can impact breakup events substantially our modelling approach combines meteorological hydrological and hydrothermal drivers to estimate the timing of river freeze up and ice breakup events retrieved from stream water 0 c isotherm simulated after coupling a physically based hydrological model and a water temperature model in general our model shows that in the main tributaries of the arb freeze up timing spans from the last week of september to the second week of november and becomes progressively later non linearly from the headwater reaches to the catchment outlet the breakup timing spans from the second week of march to the last week of may and occurs first at headwater reaches and lastly at the intermediate and outlet reaches the model reports high correlation between freeze up and breakup dates with meteorological and hydrological variables also showing that such events are spatially controlled by non linear increases in air and water temperatures and streamflows along the streams although it is known that the formation of river ice is driven by atmospheric and hydrological conditions prowse and beltaos 2002 prowse 2006 we do not know in which proportion these two factors affect river freeze up and ice cover breakup our model shows that for instance spatial negative positive gradients of air temperature can trigger later earlier freeze up events in contiguous catchment tributaries particularly in the athabasca river slightly negative gradients of air temperature near the catchment outlet downstream of fort mcmurray can cause earlier freeze up despite relatively substantial streamflow increases according to the model results it seems that freeze up timing correlates better and is more sensitive to changes in air temperature than to changes in streamflow in the catchment results reported by ginzburg 1992 and soldatova 1992 indicate strong correlations between autumn and spring air temperature and freeze up and breakup timing respectively although there is a high correspondence between observed and simulated breakup timing at the stations in the towns of athabasca and fort mcmurray breakup events show larger uncertainty in other regions within the catchment smith 2000 reported a lack of significant trends in breakup events that might be related to inter decadal variability and inconsistent data length prowse and bonsal 2004 other sources of error may be related to the fact that different criteria have been applied to recorded breakup events for instance some agencies report the dates of breakup initiation whereas others record the date of ice clearance at the end of breakup since differences between these two dates can sometimes be as much as 4 weeks that may represent a large source of discrepancies 4 2 spatial variability of river ice freeze up and breakup events theoretically at freeze up an ice cover will initiate at a bridging for example at the basin outlet and extend towards the headwaters by juxtapositioning when frazil ice that is generated in the upstream open water stretches accumulates at the ice cover front to extend it in the upstream direction however it is quite common that bridgings occur at several points along the river often at river width constrictions sharp meanders bridge piers or islands to form many ice covers simultaneously during the river freezing period this was observed along the dauphin river in manitoba when four ice covers were formed along a 40 km stretch in just a few days during a cold bout lindenschmidt et al 2013 hence it should not be a surprise to observe freeze up dates often occurring earlier at hinton than at the town of athabasca see fig 12 top left panel with most of the freeze up date points lying below the 1 1 line indicating an earlier freeze over of the athabasca river at the hinton reach than the reach at the town of athabasca many of the freeze up date points for the town of athabasca and fort mcmurray lie on the 1 1 line fig 12 top right panel indicating that the freeze over of the river stretch between these two towns can be quite rapid which is also reflected in the profiles of fig 11 however freeze over can be later at the town of athabasca than at fort mcmurray which is captured in the uncertainty band of the profile in fig 11 better accuracy of the freeze up dates may be obtained when a river ice hydraulics model for example rivice lindenschmidt 2017 is coupled to the mesh rbm system a topic of future work for initial steps of this research please see das et al 2017 rokaya et al 2017 and rokaya et al accepted for rivers flowing in a northerly direction breakup theoretically begins at the headwaters and works its way downstream towards the basin outlet breakup consistently occurred earlier at the town of athabasca than at fort mcmurray see fig 12 bottom right panel on average approximately 3 4 days earlier this is reflected in the breakup profile in fig 12 however the breakup dates between hinton and the town of athabasca are quite variable sometimes hinton experiences an ice cover breakup earlier sometimes later than at the town of athabasca the variability in the breakup times between the two towns can be quite substantial up to several weeks difference which is also captured in the uncertainty band of the breakup dates profile in fig 11 4 3 model limitations and future work some model limitations need to be addressed for future work although the proposed framework provides a flexible approach to feeding mesh output results into the rbm stream water temperature model there are some key sources of uncertainty that will need to be kept in perspective while applying the methodology the four primary sources of uncertainty in the current study are 1 water temperature boundary conditions at headwater reaches usually dominated by snow and glacier melt waters 2 lack of consideration of anthropogenic impacts along the main catchment streams 3 propagation of errors from mesh to rbm and 4 meteorological forcing data uncertainties particularly those associated with precipitation products we believe that these uncertainties introduce errors in the estimation of freeze up and breakup events which need to be considered in future work to address the first point we implemented remote sensing techniques to extract water temperatures in mountainous areas to impose boundary conditions at the headwaters although acceptable results were obtained more effort needs to be made in order to obtain longer records and apply them in areas which lack water temperature records in relation to anthropogenic impacts in catchments mesh has been substantially improved in order to consider streamflow regulation and water abstraction factors that modify hydrological functioning in downstream waters affecting river ice dynamics errors in streamflow and hydraulic variables introduced in rbm and estimated in mesh need to be reduced by a better discretization of both the catchment domain and meteorological forcing data future modelling work using the approach described here will investigate climate change scenarios to predict ice phenology estimation of rbm boundary conditions using the class energy balance computations is worthy to be tested in future works in order to facilitate the imposition of boundary conditions in headwater catchments this modelling approach is potentially useful for decision support for example for water quality planning hydropower management and resource management on a large scale risk analyses of ice jam floods and to study climate change impacts of large river basins around the world acknowledgments this study was funded by the university of saskatchewan s global institute for water security through the canada excellence research chair cerc in water security and the global water future program 
26251,an effective parallelization algorithm for dem generalization based on cuda qianjiao wu a yumin chen a john p wilson b xuejun liu c huifang li a a school of resource and environment science wuhan university 129 luoyu road wuhan 430079 china school of resource and environment science wuhan university 129 luoyu road wuhan 430079 china b spatial sciences institute university of southern california los angeles ca 90089 0374 usa spatial sciences institute university of southern california los angeles ca 90089 0374 usa c key laboratory of virtual geographic environment ministry of education nanjing normal university 1 wenyuan road nanjing 210023 china key laboratory of virtual geographic environment ministry of education nanjing normal university 1 wenyuan road nanjing 210023 china corresponding author an effective parallelization algorithm based on the compute unified device architecture cuda is developed for dem generalization that is critical to multi scale terrain analysis it aims to efficiently retrieve the critical points for generating coarser resolution dems which maximally maintain the significant terrain features cuda is embedded into a multi point algorithm to provide a parallel multi point algorithm for enhancing its computing efficiency the outcomes are compared with the anudem compound and maximum z tolerance methods and the results demonstrate the proposed algorithm reduces response time by up to 96 compared to other methods as to rmse it performs better than anudem and needs half the number of points to keep the same rmse the mean slope and surface roughness are reduced by less than 1 in the tested cases the parallel algorithm provides better streamline matching given its high computing efficiency the proposed algorithm can retrieve more critical points to meet the demands of higher precision keywords dem generalization parallelization algorithm cuda software availability name of software criticalpointextractioncuda developers qianjiao wu and yumin chen contact address school of resource and environment science wuhan university 129 luoyu road wuhan 430079 china email ymchen whu edu cn year first available 2016 hardware requirements nvidia graphics card with compute capability 2 1 or above software requirements cuda toolkit 8 0 programming language c cuda program size 123 mb availability https github com parallelproject criticalpointextractioncuda cost free of charge 1 introduction dem generalization is the key to multi scale terrain analysis once a coarser resolution dem is demanded it should efficiently generalize the original finer resolution dem to the demanded resolution and maintain the significant terrain critical points and structural lines li et al 2005 liu et al 2015 wilson 2018 there is much research on dem generalization methods for grid and tin based dems e g li 2008 ai and li 2010 chen and zhou 2013 the grid based dem generalization methods include the resampling wavelet transform morphology based and drainage constrained methods the resampling method obtains the coarser resolution dems by computing the mean elevation of neighboring cells around each cell on the grid dem kienzle 2004 the wavelet transform generates the multi scale dem using the wavelet decomposition and reconstruction algorithm yang et al 2005b since the resampling algorithm has the effect of peak clipping and valley filling and the wavelet transform method provides large redundancy and difficulties in selecting a suitable wavelet function morphology based methods have been proposed for multi scale terrain expression these methods introduce critical points triangle leveling points etc and structure lines based on the gridded dem and assign the elevation of the critical points and lines or the regional mean elevation to the integrated dem grid cells for maintaining the basic elevation characteristics li and li 2015 these methods have proved helpful for mapping applications however further verification is needed to support their use in terrain analysis applications the drainage networks are the basic skeleton in a watershed and are of great importance in the drainage constrained approach researchers have proposed numerous drainage constrained methods based on either stream burning or surface fitting algorithms the stream burning algorithm can utilize a grid representation of the drainage lines to extract the known hydrological characteristics from the dem at the pre set depth saunders 1999 chen et al 2012 it is simple efficient and can limit the numbers of cells whose elevation is altered however this approach may change the elevation value of the drainage cells and thereby modify the dem derivatives callow et al 2007 anudem provides a typical surface fitting approach and uses the elevation points contour lines drainage networks and cliff lines to generate dems based on an iterative finite difference interpolation hutchinson 1989 it can more accurately reflect the important hydrological features and offers high calculation efficiency however the choice of input dataset may influence the results obtained with this method yang et al 2005a the tin based methods retrieve significant terrain critical points and or lines to construct tins for generating coarser resolution dems based on the basic theory for dem generalization the existing methods are categorized into point additive point subtractive 3d line critical point and mixed methods the point additive methods mainly consist of the hierarchical subdivision schmitt and gholizadeh 1985 scarlatos and pavlidis 1992 and refinement methods heller 1990 schmitt and chen 1991 for example de floriani et al 1984 proposed the hierarchical triangulation algorithm inserting the break point with the maximum error in each triangle to split the triangle and build a tree hierarchy this process iterates until the error of all points are lower than the pre set precision chang 2007 developed the maximum z tolerance algorithm in which the point with the maximum error in elevation that exceeds the desired error threshold is treated as the critical point that will be inserted into the tin model the process iterates until the point s maximum difference in elevation is smaller than the pre set threshold similar to the above algorithms the point additive method provides an iterative method for adding critical points the point subtractive method begins with all dem discrete points to construct a triangulation and then deducts discrete points over the triangulation until the pre set precision is attained for example the well known drop heuristic algorithm starts with the entire elevation point set and gradually discards the vertices with the minimum error in elevation until the desired level is lower than the minimum error lee 1989 scarlatos 1993 suggested a vertex decimation method that begins with an initial triangulation and deletes points with the increasing significance computed by a relationship function until no point can be discarded garland and heckbert 1997 proposed an algorithm to simplify the terrain surface based on iterative pair contractions during the simplification quadric error metrics are used to estimate and store the error with the final vertices even though these two kinds of methods can embody the overall morphological features well they pay no attention to the shape and topological relation of the drainage networks especially in flatter regions zhou and chen 2011 and some algorithms such as the maximum z tolerance algorithm may incur large calculation costs the 3d line method was developed by expanding the douglas peucker algorithm from two to three dimensions fei and he 2009 it regards the points with the maximum distance from the point to the corresponding facet as the critical points although it offers high efficiency the method depends on the 3d feature lines which are difficult to consistently retrieve from the dem especially in large and complex areas chen and li 2013 the critical point method retrieves the terrain critical points such as peaks pits ridges and valleys to construct tins fowler and little 1979 retrieving critical points constitutes the key step in each of these methods for instance the very important point algorithm assesses the importance of each discrete point by the elevation error between the actual value and the simulated value the latter is calculated using the elevation values of the points surrounding the center point in each convolution window chen and guevara 1987 although it performs well and offers high efficiency it cannot ensure the overall level of the critical points in order to solve the problem chen and li 2013 presented the mlos method for dem generalization it uses the mq algorithm to construct the surface and the ols algorithm to select the significant points it has a better ability to retain terrain features however because of the complex internal calculations it has a large computing cost chen et al 2016 developed a w8d algorithm by extracting the critical points based on a watershed and tree structure for generalizing dems it can retain the ridges and valleys well but has a problem with the computing efficiency given the large number of points on the watershed border the mixed methods rely on combinations of these different methods for instance vivoni et al 2004 considered the topographic hydrographic and hydrologic features in a watershed in the construction of tin models for hydrologic applications the compound method cpe proposed by zhou and chen 2011 utilizes the maximum z tolerance to retrieve the critical points and the d8 algorithm o callaghan and mark 1984 for obtaining the streamlines for building the drainage constrained tin these methods offer better precision on the whole and retain the terrain structural lines very well but are time consuming because they incorporate complicated computations ma et al 2016 suggested the 4 dp algorithm for dem generalization which constructs the tins by combining the vip algorithm with the simplification of the profile line this method avoids the geometric distortion in local neighborhoods but its precision offers no enhancement considering both the accuracy and computing efficiency of the existing dem generalization methods this article proposes a method for dem generalization that can retrieve the critical points with high calculation efficiency to generate coarser resolution dems which maximally maintain the significant terrain features by building drainage constrained tins a multi point algorithm is proposed to rapidly and accurately retrieve the critical points from the gridded dem depending on the point s significance in identifying the geomorphological characteristics then in order to further enhance its computing efficiency the parallel computation is embedded into the multi point algorithm for a parallel multi point algorithm to acquire terrain critical points recently numerous parallel computation methods have been proposed such as mpi openmp mpi cloud computing and gpu parallel computing compared to multi core cpu parallel computing gpu parallel computing is able to conduct simple big data parallel computing in a pc environment making single high performance processing and improvement of the computational efficiency possible the compute unified device architecture cuda performs the parallel computations on the gpu in general purpose fashion schenk et al 2008 mielikainen et al 2013 cuda is able to solve many complex computing tasks more efficiently than cpus using nvidia corporation s 2010a 2010b gpu parallel computing engine it is an extended c programming language and allows programmers to develop the kernel function to support high density arithmetic calculations cheng 2012 since cuda was launched in 2007 it has been widely applied in many fields such as environment modelling abouali et al 2013 le et al 2015 xia and liang 2016 vacondio et al 2017 hydrological simulation ji et al 2014 juez et al 2016 vitor et al 2017 carlotto et al 2018 remote sensing goodman et al 2011 pascal et al 2014 chang et al 2016 and numerous other fields e g xia et al 2010 amore et al 2015 sayadi et al 2018 it has also been applied in the field of terrain analysis ortega and rueda 2010 for example adopted cuda to parallelize the d8 watershed network extraction algorithm and showed that the speed up ratio can be up to 8x qin and zhan 2012 parallelized the d8 and mfd algorithms which can be used to predict flow routing based on cuda the results showed that the speedup rates of the former and latter algorithms can reach 22 3x and 10 9x respectively rueda et al 2016 made use of cuda to parallelize the d8 algorithm for extracting river networks and compared the performance with the parallel openacc algorithm they found that the river network can be extracted with the d8 parallel algorithm based on cuda with good performance and that the computational efficiency was greatly improved generally speaking cuda is suitable for processing problems with large amounts of data high parallelism low coupling high data density calculations and immobilization process problems these are the characteristics of the multi point algorithm and thus cuda is utilized for parallelizing the parallel multi point algorithm this study therefore proposes an effective parallelization algorithm for dem generalization based on cuda the proposed algorithm can quickly retrieve critical points for generating coarser resolution dems that maximally keep the significant terrain features by building the drainage constrained tin critical points are retrieved from the gridded dem by the parallel multi point algorithm the drainage constrained tin is then constructed by inserting the drainage networks extracted by the d8 algorithm into the critical points the outcome is then compared with the anudem and compound methods to analyze the accuracy and computational efficiency the remainder of this article is organized as follows the methodology of the parallelization algorithm is described in section 2 section 3 describes the experiments and results section 4 discusses the accuracy and computing performance of the proposed method the conclusions and some suggestions for future work are provided in section 5 2 methodology the methodology adopted for this study incorporated four major components 1 proposing a multi point algorithm 2 parallelizing the multi point algorithm based on cuda 3 generating coarser resolution dems by building drainage constrained tins and 4 evaluating the accuracy and computing efficiency of the proposed method the specific steps in the proposed algorithm are illustrated in fig 1 2 1 proposing a multi point algorithm a multi point algorithm is presented to rapidly and accurately retrieve the critical points from the gridded dem depending on the point s significance in identifying the geomorphological characteristics inherent in the dem the multi point algorithm is an iterative process first it selects the four corners of the dem to construct the initial tin as shown in fig 2 a second it obtains the point with the maximum difference in elevation for every triangle facet the acquisition method is as follows 1 computing the plane equation of the triangle facet assumption is z ax by c 2 finding the discrete points within the triangle facet assumption of the discrete points is x 0 y 0 z 0 x n y n z n 3 calculating the differences in the elevation of these discrete points they are respectively a x 0 b y 0 c z 0 a x n b y n c z n 4 comparing these differences in elevation to obtain the point with the maximum difference in elevation 5 traversing the triangle facets over the tin to acquire all of the points with the maximum differences in elevation as indicated by the blue points in fig 2 b third the differences in the elevations of the blue points are compared with the pre set threshold if a point exceeds the pre set threshold we will regard it as a critical point as indicated by the red points in fig 2 c fourth the method rebuilds the tin by inserting the new critical points and finally it repeats the above step until all of the calculated maximum differences in elevation are less than the pre set threshold for the final critical points shown in fig 2 d 2 2 parallelizing the multi point algorithm based on cuda for the purpose of further optimizing the computing efficiency cuda is embedded into the multi point algorithm to provide a parallel multi point algorithm we analyzed the multi point algorithm using the microsoft visual studio performance analysis tool the large dem used for the experiments in section 3 was first used with several threshold values to evaluate the performance of this new algorithm the performance results representing the average values of several runs are summarized in table 1 from table 1 we can know that the iterative calculation of the maximum difference in elevation of each triangle is the most time consuming section because it took 81 5 of the total time therefore this section was adapted to be the kernel function of cuda to improve the computational efficiency the kernel function of parallelizing the calculation of critical points by cuda is composed of two parallel styles 1 the parallelization of discrete points in the dem fig 3 a and 2 the parallelization of triangles over the tin fig 3 b the first parallelization style is traversing every triangle and parallelizing the discrete points over the dem the second parallelization style is traversing the discrete points in the dem and parallelizing the triangles over the tin when the number of the triangles over the tin is few the discrete points in the dem covered by the triangle is large and thus the first style is suitable for this case with the reconstruction of the tin the number of triangles increases and the number of discrete points within each triangle decreases therefore the second style is used in place of the first style to retrieve the critical points part way through the parallelization process the whole parallelization based on cuda includes three parts 1 the data transmission process 2 the model used by cuda to divide the threads and 3 the implementation of the kernel function 2 2 1 data transmission process the complex process of data transmission is shown in fig 4 the dem data the triangle data in the tin and the pre set threshold are delivered to the gpu from the cpu then the kernel function is used to obtain the difference in elevation of every discrete point during the first parallelization style such as diff i i 1 2 3 m and column and row numbers of critical points during the second parallelization style such as ii and ji i 1 2 3 m as shown in fig 4 next they are delivered to the cpu and we find the maximum value of the diff array if it exceeds the threshold the discrete point corresponding to the index is the critical point where column and row numbers can be calculated by its position in the array this last task is not embedded into the kernel function because it can be handled more efficiently by the cpu finally the process depends on whether the list of critical points is empty if it is not empty it indicates that there are new critical points these new critical points will be inserted into the original critical points to reconstruct the tin it redirects the triangle data in the new tin for the next initiation until it cannot obtain any more new critical points once the program is finished with this task all of the critical points will be retrieved 2 2 2 model used by cuda to divide the threads to define the kernel the programmer needs to use the specifier global or device to state that the function is running on the gpu and utilizing the x y x bc bd and y tc td syntax to specify the number of threads invoked by cuda as shown in fig 5 in it x defines the division model of thread blocks in one thread grid bc and bd respectively indicate the number of thread blocks in one thread grid in the x and y directions y defines the division model of threads in one thread block and tc and td respectively indicate the number of threads in one thread block in the x and y directions the compute capability of the gpu influences the division model of threads by cuda for example the compute capability of the gpu is 6 1 in this article which indicates that the product of tc and td can be up to 1024 threads in one thread block for the first parallelization style each thread handles a discrete point in the dem thus the division model is as follows y tc td x recwidth tc 1 tc recheight td 1 td the recwidth and recheight are the width and height of the traversed triangle s boundary rectangle respectively for the second parallelization style each thread deals with a triangle over the tin thus the division model is as follows y tc td x trinum tc td 1 tc td 1 where the trinum indicates the number of triangles in the tin 2 2 3 implementation of the kernel function each thread of the kernel function has a unique thread id that can be accessed and queried by the built in variables threadidx and blockidx the pseudo codes for the implementation of the first parallelization style is provided in listing 1 first the thread index of discrete points in the dem is defined as the tid by the built in variables second the row column and elevation of the discrete point will be calculated by the triangle coordinates and tid third it computes the triangle plane equation fourth the area of the triangle and three sub triangles composed by the discrete point and the triangles points are calculated by the helen formula respectively fifth it calculates the difference in elevation when the discrete point is within the triangle bounding rectangle finally all of the differences in the elevations of the discrete points covered by the triangle will be recorded in an array and output to the cpu image 1076 similarly the pseudo code for the implementation of the second parallelization style can be found in listing 2 below first the thread index of triangles over the tin is defined as the tid by the built in variables second the triangle s plane equation will be calculated using the tid and coordinate data of the triangle and the triangle s area will be computed third it will iteratively compute and obtain the point with the maximum difference in elevation within the triangle fourth it determines the magnitude of the maximum difference in elevation and compared it with the pre set tolerance if the former is larger than the latter the point will be regarded as a critical point finally the columns and rows of the critical points within all of the triangles will be output to the cpu image 102 2 3 generating coarser resolution dems by building drainage constrained tins zhou and chen 2011 have investigated the importance of inserting the stream networks into the critical points to build the drainage constrained tins for generating coarser resolution dems thus in this study we employ the proposed parallel multi point algorithm to retrieve the critical points and the d8 algorithm is utilized for tracking the flow from each pixel to one steepest pixel among the eight neighboring pixels to extract the stream networks the steam networks are regarded as the constrained edges to refine the triangulation over the tin constructed by the critical points similar to the compound method elaborated in zhou and chen 2011 2 4 evaluating the accuracy and computing efficiency of the proposed method in this study the root mean standard error rmse was used with all of the grid cells to assess the error between the generalized dems and the original dem in addition to rmse the mean slope ms mean surface roughness mr streamline matching rate smr and streamline matching error sme were also employed to estimate the accuracy for all of the grid cells the computing time was recorded to evaluate the computing efficiency the rmse ms mr smr and sme can be expressed as follows 1 r m s e i 1 n z i z i n 2 m s i 1 n s i a i i 1 n a i 3 m r a a i 1 n a i s e c s i i 1 n a i 4 s m r l l 100 5 s m e δ a l where z i is the original elevation z i is the estimated elevation i is the ith cell n is the number of grid cells s is the slope s e c s i is the secant of the slope at the ith grid cell a is the projected area a is the surface area l is the length of streamlines within the stream buffer zones l is the total length of the streamlines and δ a is the area of sliver polygons between the streamlines extracted from the original and generalized dems 3 experiments and results for evaluating the accuracy and computational performance of the proposed method a watershed and a mountainous region were selected for this study the watershed located in northwest china provides a typical loess hilly and gully landscape with a complex terrain surface and cross cutting gullies the test dem for the region for short small dem 2134 1821 grid cells fig 6 a has a 5 m resolution it is derived from a 1 10 000 scale topographic map the mountainous region is topographically complex with high mountains steep slopes substantial local relief and a high density stream network the test dem for this region for short large dem 5958 3514 grid cells fig 6 b has a 30 m resolution it was obtained from the advanced spaceborne thermal emission and reflection aster global digital elevation model gdem dataset the drainage constrained tins were generated on the small and large dems using different thresholds by the parallelization algorithm to reduce data redundancy during dem generalization a certain number of critical points were retrieved by the parallel multi point algorithm from the original dem to construct a tin for generalizing a dem this approach retained the accuracy of all of the points in the original dem to the greatest degree possible these critical points were included in both the original and the generalized dems the accuracy assessment used all of the grid cells in the generalized and original dems for the small dem thresholds of 2 4 6 8 10 12 14 16 and 18 m respectively were used for the large dem threshold values of 20 22 24 26 28 30 32 34 and 36 m respectively were used the drainage network was extracted by the d8 algorithm during the extraction process this algorithm also needs a threshold value to extract the streamlines after the calculation of the flow accumulation when the flow accumulation value of a grid cell is larger than the threshold value the grid cell will be treated as a stream the choice of threshold value s depends on the specific situation we chose a series of thresholds 1 000 2 000 3 000 4 000 5000 and 6000 m2 to generate a series of possible values for the work at hand the douglas peucker algorithm was also deployed and used to simplify the drainage network using threshold values of 5 and 30 m respectively to match the grid spacing of the small and large dems in addition to the parallelization algorithm the anudem compound and maximum z tolerance methods were used to generalize the dems and these results were used to assess the relative accuracy and computing performance of the new method the proposed parallel multi point and maximum z tolerance algorithms respectively were used to retrieve the critical points for the anudem and compound methods both methods utilized the d8 algorithm to extract the streamlines the anudem method combines the critical points and streamlines to directly generate the dems based on an iterative finite difference interpolation the method is compared to highlight the advantages of incorporating the tin based structure in the parallelization algorithm the compound method inserts the streamlines into the critical points to construct the tins for generating the dems the comparison with this method is for highlighting the impact of using the parallel multi point algorithm in the proposed method finally the new method is compared to the maximum z tolerance algorithm to illustrate the advantages of combining the parallel multi point and streamlines experiments demonstrated that the rmse values from the anudem method increased along with the threshold values of the d8 algorithm on the small dem the rmse values from the compound and parallel methods decreased with increasing threshold values thus the threshold values for the anudem compound and parallel methods were set at 1000 6000 and 6000 respectively on the small dem in the case of the large dem the rmse values for all three methods declined with increasing threshold values and the preferred threshold was set at 6000 for all three methods the generalized dems were next compared with the original dem to derive the rmse values for the four methods on the small and large dems tables 2 3 the several threshold values and corresponding rmse values were used to select the appropriate threshold values for example we considered that an rmse of 5 m in elevation was acceptable to match the scale of the small dem state bureau of surveying and mapping 2001 the appropriate values reported in table 2 were next used to derive the best fitting formulae between the threshold value and rmse and to calculate the corresponding correlation coefficients r2 shown in table 4 the r2 values exceed 0 99 and show the formulae fit the experiment data very well the sound threshold values for the anudem compound maximum z tolerance and parallel methods are 12 04 m 15 78 m 15 98 m and 16 75 m respectively in addition we validated the rmse values of the generalized dems using these same values and they are very close to 5 m as well the experiments to assess the computing performance of the parallelization algorithm were implemented on a desktop computer with an i5 6500 cpu 8 gb ram 1060 nvidia geforce gtx and microsoft windows 7 using the 64 bit option in terms of computing performance the anudem method incorporates critical point retrieval drainage extraction and interpolation the compound method which is the same as the parallelization algorithm incorporates critical point retrieval drainage extraction and tin construction the maximum z tolerance incorporates critical point retrieval and tin construction the calculation times obtained by varying the threshold value for the selected algorithms are summarized in tables 5 6 4 discussion this study aimed to efficiently use finer resolution dems to generalize coarse resolution dems which maximally maintained the significant terrain features thus we not only use the time statistics to evaluate the computing efficiency but also employ rmse ms mr smr and sme to make comparisons in terms of accuracy between the proposed parallelization algorithm and three existing approaches 4 1 computing performance enhancement measures fig 7 shows the comparisons of computing time between the selected algorithms on the two test dems the computing time of the parallelization algorithm is sharply decreased and combined with the results reported in tables 5 and 6 these results demonstrate that the computing time of the parallelization algorithm is reduced to 4 of the anudem and compound methods in the best case with the increase of the number of dem grid cells the parallelization algorithm is much more efficient in terms of the calculations and uses 3 of the computing time used by the anudem and compound methods in the worst case fig 7 b in addition the computing efficiency of the proposed method is reduced to half that of the anudem algorithm notwithstanding the fact they use the same algorithm to extract critical points thus the proposed algorithm has extremely high computational efficiency compared to the three existing methods 4 2 accuracy measures in terms of terrain analysis the representation of the terrain features is a critical measure fig 8 demonstrates that the proposed algorithm performs much better than the anudem method and it can sustain the same rmse using half the number of points used by anudem however the new algorithm performs slightly worse than the other two algorithms it will achieve the same rmses when using 30 40 more points and given the high computing efficiency of the parallelization algorithm generalized dems with higher accuracy can be generated by retrieving larger numbers of the critical points figs 9 10 illustrate the comparison of ms and mr between the compound maximum z tolerance and parallel methods on the two dems respectively given the poor rmse values produced with anudem this method is not included in these comparisons the ms values exceed 27 0 and 24 0 respectively and the mr values exceed 1 14 and 1 11 respectively for the small and large dems using all three methods tested the line traces in figs 9 10 show that the three methods follow each other closely in terms of performance with the parallelization algorithm performing slightly better than the maximum z tolerance method and slightly worse than the compound method but the differences are 1 in every case such that their impact on the terrain features can be ignored figs 11 12 show the comparisons of smr and sme for the compound maximum z tolerance and parallel methods on the two dems respectively the streamline matching of the parallel method is almost double and the matching deviation is reduced by half compared to the maximum z tolerance algorithm the parallel algorithm shows a slightly better streamline matching rate and a slightly lower matching error compared to the compound method as well overall these results show that the parallel algorithm can better sustain the drainage features during the generalization process 5 conclusions in this study an effective parallelization algorithm for dem generalization based on cuda was proposed the goal was to use the minimum time to retrieve critical points from a finer resolution dem for generating multi scale dems with high accuracy the proposed method took the parallel multi point and d8 algorithms to generate the coarser resolution dems by constructing drainage constrained tins the parallel multi point algorithm was obtained by embedding cuda into a multi point algorithm which was presented to accurately retrieve the critical points depending on the point s significance in identifying inherent geomorphological characteristics the performance of the new method was compared to the aundem compound and maximum z tolerance methods in terms of efficiency and accuracy the results demonstrate that the parallelization algorithm can generalize dems with high efficiency and accuracy future studies will focus on the parallelization of the tin construction algorithm and incorporating it into the parallel multi point algorithm in addition ortega and rueda 2010 paralleled the d8 algorithm based on cuda the combination of the parallelization of extracting critical points and stream networks based on cuda is another area for further research research will also focus on the development of quick multi scale dems with high accuracy to support the simulation of complex and dynamic surface hydrological processes declarations of interest none acknowledgements this work was supported by national key r d program of china grant number 2018yfb0505302 and the national nature science foundation of china grant numbers 41671380 
26251,an effective parallelization algorithm for dem generalization based on cuda qianjiao wu a yumin chen a john p wilson b xuejun liu c huifang li a a school of resource and environment science wuhan university 129 luoyu road wuhan 430079 china school of resource and environment science wuhan university 129 luoyu road wuhan 430079 china b spatial sciences institute university of southern california los angeles ca 90089 0374 usa spatial sciences institute university of southern california los angeles ca 90089 0374 usa c key laboratory of virtual geographic environment ministry of education nanjing normal university 1 wenyuan road nanjing 210023 china key laboratory of virtual geographic environment ministry of education nanjing normal university 1 wenyuan road nanjing 210023 china corresponding author an effective parallelization algorithm based on the compute unified device architecture cuda is developed for dem generalization that is critical to multi scale terrain analysis it aims to efficiently retrieve the critical points for generating coarser resolution dems which maximally maintain the significant terrain features cuda is embedded into a multi point algorithm to provide a parallel multi point algorithm for enhancing its computing efficiency the outcomes are compared with the anudem compound and maximum z tolerance methods and the results demonstrate the proposed algorithm reduces response time by up to 96 compared to other methods as to rmse it performs better than anudem and needs half the number of points to keep the same rmse the mean slope and surface roughness are reduced by less than 1 in the tested cases the parallel algorithm provides better streamline matching given its high computing efficiency the proposed algorithm can retrieve more critical points to meet the demands of higher precision keywords dem generalization parallelization algorithm cuda software availability name of software criticalpointextractioncuda developers qianjiao wu and yumin chen contact address school of resource and environment science wuhan university 129 luoyu road wuhan 430079 china email ymchen whu edu cn year first available 2016 hardware requirements nvidia graphics card with compute capability 2 1 or above software requirements cuda toolkit 8 0 programming language c cuda program size 123 mb availability https github com parallelproject criticalpointextractioncuda cost free of charge 1 introduction dem generalization is the key to multi scale terrain analysis once a coarser resolution dem is demanded it should efficiently generalize the original finer resolution dem to the demanded resolution and maintain the significant terrain critical points and structural lines li et al 2005 liu et al 2015 wilson 2018 there is much research on dem generalization methods for grid and tin based dems e g li 2008 ai and li 2010 chen and zhou 2013 the grid based dem generalization methods include the resampling wavelet transform morphology based and drainage constrained methods the resampling method obtains the coarser resolution dems by computing the mean elevation of neighboring cells around each cell on the grid dem kienzle 2004 the wavelet transform generates the multi scale dem using the wavelet decomposition and reconstruction algorithm yang et al 2005b since the resampling algorithm has the effect of peak clipping and valley filling and the wavelet transform method provides large redundancy and difficulties in selecting a suitable wavelet function morphology based methods have been proposed for multi scale terrain expression these methods introduce critical points triangle leveling points etc and structure lines based on the gridded dem and assign the elevation of the critical points and lines or the regional mean elevation to the integrated dem grid cells for maintaining the basic elevation characteristics li and li 2015 these methods have proved helpful for mapping applications however further verification is needed to support their use in terrain analysis applications the drainage networks are the basic skeleton in a watershed and are of great importance in the drainage constrained approach researchers have proposed numerous drainage constrained methods based on either stream burning or surface fitting algorithms the stream burning algorithm can utilize a grid representation of the drainage lines to extract the known hydrological characteristics from the dem at the pre set depth saunders 1999 chen et al 2012 it is simple efficient and can limit the numbers of cells whose elevation is altered however this approach may change the elevation value of the drainage cells and thereby modify the dem derivatives callow et al 2007 anudem provides a typical surface fitting approach and uses the elevation points contour lines drainage networks and cliff lines to generate dems based on an iterative finite difference interpolation hutchinson 1989 it can more accurately reflect the important hydrological features and offers high calculation efficiency however the choice of input dataset may influence the results obtained with this method yang et al 2005a the tin based methods retrieve significant terrain critical points and or lines to construct tins for generating coarser resolution dems based on the basic theory for dem generalization the existing methods are categorized into point additive point subtractive 3d line critical point and mixed methods the point additive methods mainly consist of the hierarchical subdivision schmitt and gholizadeh 1985 scarlatos and pavlidis 1992 and refinement methods heller 1990 schmitt and chen 1991 for example de floriani et al 1984 proposed the hierarchical triangulation algorithm inserting the break point with the maximum error in each triangle to split the triangle and build a tree hierarchy this process iterates until the error of all points are lower than the pre set precision chang 2007 developed the maximum z tolerance algorithm in which the point with the maximum error in elevation that exceeds the desired error threshold is treated as the critical point that will be inserted into the tin model the process iterates until the point s maximum difference in elevation is smaller than the pre set threshold similar to the above algorithms the point additive method provides an iterative method for adding critical points the point subtractive method begins with all dem discrete points to construct a triangulation and then deducts discrete points over the triangulation until the pre set precision is attained for example the well known drop heuristic algorithm starts with the entire elevation point set and gradually discards the vertices with the minimum error in elevation until the desired level is lower than the minimum error lee 1989 scarlatos 1993 suggested a vertex decimation method that begins with an initial triangulation and deletes points with the increasing significance computed by a relationship function until no point can be discarded garland and heckbert 1997 proposed an algorithm to simplify the terrain surface based on iterative pair contractions during the simplification quadric error metrics are used to estimate and store the error with the final vertices even though these two kinds of methods can embody the overall morphological features well they pay no attention to the shape and topological relation of the drainage networks especially in flatter regions zhou and chen 2011 and some algorithms such as the maximum z tolerance algorithm may incur large calculation costs the 3d line method was developed by expanding the douglas peucker algorithm from two to three dimensions fei and he 2009 it regards the points with the maximum distance from the point to the corresponding facet as the critical points although it offers high efficiency the method depends on the 3d feature lines which are difficult to consistently retrieve from the dem especially in large and complex areas chen and li 2013 the critical point method retrieves the terrain critical points such as peaks pits ridges and valleys to construct tins fowler and little 1979 retrieving critical points constitutes the key step in each of these methods for instance the very important point algorithm assesses the importance of each discrete point by the elevation error between the actual value and the simulated value the latter is calculated using the elevation values of the points surrounding the center point in each convolution window chen and guevara 1987 although it performs well and offers high efficiency it cannot ensure the overall level of the critical points in order to solve the problem chen and li 2013 presented the mlos method for dem generalization it uses the mq algorithm to construct the surface and the ols algorithm to select the significant points it has a better ability to retain terrain features however because of the complex internal calculations it has a large computing cost chen et al 2016 developed a w8d algorithm by extracting the critical points based on a watershed and tree structure for generalizing dems it can retain the ridges and valleys well but has a problem with the computing efficiency given the large number of points on the watershed border the mixed methods rely on combinations of these different methods for instance vivoni et al 2004 considered the topographic hydrographic and hydrologic features in a watershed in the construction of tin models for hydrologic applications the compound method cpe proposed by zhou and chen 2011 utilizes the maximum z tolerance to retrieve the critical points and the d8 algorithm o callaghan and mark 1984 for obtaining the streamlines for building the drainage constrained tin these methods offer better precision on the whole and retain the terrain structural lines very well but are time consuming because they incorporate complicated computations ma et al 2016 suggested the 4 dp algorithm for dem generalization which constructs the tins by combining the vip algorithm with the simplification of the profile line this method avoids the geometric distortion in local neighborhoods but its precision offers no enhancement considering both the accuracy and computing efficiency of the existing dem generalization methods this article proposes a method for dem generalization that can retrieve the critical points with high calculation efficiency to generate coarser resolution dems which maximally maintain the significant terrain features by building drainage constrained tins a multi point algorithm is proposed to rapidly and accurately retrieve the critical points from the gridded dem depending on the point s significance in identifying the geomorphological characteristics then in order to further enhance its computing efficiency the parallel computation is embedded into the multi point algorithm for a parallel multi point algorithm to acquire terrain critical points recently numerous parallel computation methods have been proposed such as mpi openmp mpi cloud computing and gpu parallel computing compared to multi core cpu parallel computing gpu parallel computing is able to conduct simple big data parallel computing in a pc environment making single high performance processing and improvement of the computational efficiency possible the compute unified device architecture cuda performs the parallel computations on the gpu in general purpose fashion schenk et al 2008 mielikainen et al 2013 cuda is able to solve many complex computing tasks more efficiently than cpus using nvidia corporation s 2010a 2010b gpu parallel computing engine it is an extended c programming language and allows programmers to develop the kernel function to support high density arithmetic calculations cheng 2012 since cuda was launched in 2007 it has been widely applied in many fields such as environment modelling abouali et al 2013 le et al 2015 xia and liang 2016 vacondio et al 2017 hydrological simulation ji et al 2014 juez et al 2016 vitor et al 2017 carlotto et al 2018 remote sensing goodman et al 2011 pascal et al 2014 chang et al 2016 and numerous other fields e g xia et al 2010 amore et al 2015 sayadi et al 2018 it has also been applied in the field of terrain analysis ortega and rueda 2010 for example adopted cuda to parallelize the d8 watershed network extraction algorithm and showed that the speed up ratio can be up to 8x qin and zhan 2012 parallelized the d8 and mfd algorithms which can be used to predict flow routing based on cuda the results showed that the speedup rates of the former and latter algorithms can reach 22 3x and 10 9x respectively rueda et al 2016 made use of cuda to parallelize the d8 algorithm for extracting river networks and compared the performance with the parallel openacc algorithm they found that the river network can be extracted with the d8 parallel algorithm based on cuda with good performance and that the computational efficiency was greatly improved generally speaking cuda is suitable for processing problems with large amounts of data high parallelism low coupling high data density calculations and immobilization process problems these are the characteristics of the multi point algorithm and thus cuda is utilized for parallelizing the parallel multi point algorithm this study therefore proposes an effective parallelization algorithm for dem generalization based on cuda the proposed algorithm can quickly retrieve critical points for generating coarser resolution dems that maximally keep the significant terrain features by building the drainage constrained tin critical points are retrieved from the gridded dem by the parallel multi point algorithm the drainage constrained tin is then constructed by inserting the drainage networks extracted by the d8 algorithm into the critical points the outcome is then compared with the anudem and compound methods to analyze the accuracy and computational efficiency the remainder of this article is organized as follows the methodology of the parallelization algorithm is described in section 2 section 3 describes the experiments and results section 4 discusses the accuracy and computing performance of the proposed method the conclusions and some suggestions for future work are provided in section 5 2 methodology the methodology adopted for this study incorporated four major components 1 proposing a multi point algorithm 2 parallelizing the multi point algorithm based on cuda 3 generating coarser resolution dems by building drainage constrained tins and 4 evaluating the accuracy and computing efficiency of the proposed method the specific steps in the proposed algorithm are illustrated in fig 1 2 1 proposing a multi point algorithm a multi point algorithm is presented to rapidly and accurately retrieve the critical points from the gridded dem depending on the point s significance in identifying the geomorphological characteristics inherent in the dem the multi point algorithm is an iterative process first it selects the four corners of the dem to construct the initial tin as shown in fig 2 a second it obtains the point with the maximum difference in elevation for every triangle facet the acquisition method is as follows 1 computing the plane equation of the triangle facet assumption is z ax by c 2 finding the discrete points within the triangle facet assumption of the discrete points is x 0 y 0 z 0 x n y n z n 3 calculating the differences in the elevation of these discrete points they are respectively a x 0 b y 0 c z 0 a x n b y n c z n 4 comparing these differences in elevation to obtain the point with the maximum difference in elevation 5 traversing the triangle facets over the tin to acquire all of the points with the maximum differences in elevation as indicated by the blue points in fig 2 b third the differences in the elevations of the blue points are compared with the pre set threshold if a point exceeds the pre set threshold we will regard it as a critical point as indicated by the red points in fig 2 c fourth the method rebuilds the tin by inserting the new critical points and finally it repeats the above step until all of the calculated maximum differences in elevation are less than the pre set threshold for the final critical points shown in fig 2 d 2 2 parallelizing the multi point algorithm based on cuda for the purpose of further optimizing the computing efficiency cuda is embedded into the multi point algorithm to provide a parallel multi point algorithm we analyzed the multi point algorithm using the microsoft visual studio performance analysis tool the large dem used for the experiments in section 3 was first used with several threshold values to evaluate the performance of this new algorithm the performance results representing the average values of several runs are summarized in table 1 from table 1 we can know that the iterative calculation of the maximum difference in elevation of each triangle is the most time consuming section because it took 81 5 of the total time therefore this section was adapted to be the kernel function of cuda to improve the computational efficiency the kernel function of parallelizing the calculation of critical points by cuda is composed of two parallel styles 1 the parallelization of discrete points in the dem fig 3 a and 2 the parallelization of triangles over the tin fig 3 b the first parallelization style is traversing every triangle and parallelizing the discrete points over the dem the second parallelization style is traversing the discrete points in the dem and parallelizing the triangles over the tin when the number of the triangles over the tin is few the discrete points in the dem covered by the triangle is large and thus the first style is suitable for this case with the reconstruction of the tin the number of triangles increases and the number of discrete points within each triangle decreases therefore the second style is used in place of the first style to retrieve the critical points part way through the parallelization process the whole parallelization based on cuda includes three parts 1 the data transmission process 2 the model used by cuda to divide the threads and 3 the implementation of the kernel function 2 2 1 data transmission process the complex process of data transmission is shown in fig 4 the dem data the triangle data in the tin and the pre set threshold are delivered to the gpu from the cpu then the kernel function is used to obtain the difference in elevation of every discrete point during the first parallelization style such as diff i i 1 2 3 m and column and row numbers of critical points during the second parallelization style such as ii and ji i 1 2 3 m as shown in fig 4 next they are delivered to the cpu and we find the maximum value of the diff array if it exceeds the threshold the discrete point corresponding to the index is the critical point where column and row numbers can be calculated by its position in the array this last task is not embedded into the kernel function because it can be handled more efficiently by the cpu finally the process depends on whether the list of critical points is empty if it is not empty it indicates that there are new critical points these new critical points will be inserted into the original critical points to reconstruct the tin it redirects the triangle data in the new tin for the next initiation until it cannot obtain any more new critical points once the program is finished with this task all of the critical points will be retrieved 2 2 2 model used by cuda to divide the threads to define the kernel the programmer needs to use the specifier global or device to state that the function is running on the gpu and utilizing the x y x bc bd and y tc td syntax to specify the number of threads invoked by cuda as shown in fig 5 in it x defines the division model of thread blocks in one thread grid bc and bd respectively indicate the number of thread blocks in one thread grid in the x and y directions y defines the division model of threads in one thread block and tc and td respectively indicate the number of threads in one thread block in the x and y directions the compute capability of the gpu influences the division model of threads by cuda for example the compute capability of the gpu is 6 1 in this article which indicates that the product of tc and td can be up to 1024 threads in one thread block for the first parallelization style each thread handles a discrete point in the dem thus the division model is as follows y tc td x recwidth tc 1 tc recheight td 1 td the recwidth and recheight are the width and height of the traversed triangle s boundary rectangle respectively for the second parallelization style each thread deals with a triangle over the tin thus the division model is as follows y tc td x trinum tc td 1 tc td 1 where the trinum indicates the number of triangles in the tin 2 2 3 implementation of the kernel function each thread of the kernel function has a unique thread id that can be accessed and queried by the built in variables threadidx and blockidx the pseudo codes for the implementation of the first parallelization style is provided in listing 1 first the thread index of discrete points in the dem is defined as the tid by the built in variables second the row column and elevation of the discrete point will be calculated by the triangle coordinates and tid third it computes the triangle plane equation fourth the area of the triangle and three sub triangles composed by the discrete point and the triangles points are calculated by the helen formula respectively fifth it calculates the difference in elevation when the discrete point is within the triangle bounding rectangle finally all of the differences in the elevations of the discrete points covered by the triangle will be recorded in an array and output to the cpu image 1076 similarly the pseudo code for the implementation of the second parallelization style can be found in listing 2 below first the thread index of triangles over the tin is defined as the tid by the built in variables second the triangle s plane equation will be calculated using the tid and coordinate data of the triangle and the triangle s area will be computed third it will iteratively compute and obtain the point with the maximum difference in elevation within the triangle fourth it determines the magnitude of the maximum difference in elevation and compared it with the pre set tolerance if the former is larger than the latter the point will be regarded as a critical point finally the columns and rows of the critical points within all of the triangles will be output to the cpu image 102 2 3 generating coarser resolution dems by building drainage constrained tins zhou and chen 2011 have investigated the importance of inserting the stream networks into the critical points to build the drainage constrained tins for generating coarser resolution dems thus in this study we employ the proposed parallel multi point algorithm to retrieve the critical points and the d8 algorithm is utilized for tracking the flow from each pixel to one steepest pixel among the eight neighboring pixels to extract the stream networks the steam networks are regarded as the constrained edges to refine the triangulation over the tin constructed by the critical points similar to the compound method elaborated in zhou and chen 2011 2 4 evaluating the accuracy and computing efficiency of the proposed method in this study the root mean standard error rmse was used with all of the grid cells to assess the error between the generalized dems and the original dem in addition to rmse the mean slope ms mean surface roughness mr streamline matching rate smr and streamline matching error sme were also employed to estimate the accuracy for all of the grid cells the computing time was recorded to evaluate the computing efficiency the rmse ms mr smr and sme can be expressed as follows 1 r m s e i 1 n z i z i n 2 m s i 1 n s i a i i 1 n a i 3 m r a a i 1 n a i s e c s i i 1 n a i 4 s m r l l 100 5 s m e δ a l where z i is the original elevation z i is the estimated elevation i is the ith cell n is the number of grid cells s is the slope s e c s i is the secant of the slope at the ith grid cell a is the projected area a is the surface area l is the length of streamlines within the stream buffer zones l is the total length of the streamlines and δ a is the area of sliver polygons between the streamlines extracted from the original and generalized dems 3 experiments and results for evaluating the accuracy and computational performance of the proposed method a watershed and a mountainous region were selected for this study the watershed located in northwest china provides a typical loess hilly and gully landscape with a complex terrain surface and cross cutting gullies the test dem for the region for short small dem 2134 1821 grid cells fig 6 a has a 5 m resolution it is derived from a 1 10 000 scale topographic map the mountainous region is topographically complex with high mountains steep slopes substantial local relief and a high density stream network the test dem for this region for short large dem 5958 3514 grid cells fig 6 b has a 30 m resolution it was obtained from the advanced spaceborne thermal emission and reflection aster global digital elevation model gdem dataset the drainage constrained tins were generated on the small and large dems using different thresholds by the parallelization algorithm to reduce data redundancy during dem generalization a certain number of critical points were retrieved by the parallel multi point algorithm from the original dem to construct a tin for generalizing a dem this approach retained the accuracy of all of the points in the original dem to the greatest degree possible these critical points were included in both the original and the generalized dems the accuracy assessment used all of the grid cells in the generalized and original dems for the small dem thresholds of 2 4 6 8 10 12 14 16 and 18 m respectively were used for the large dem threshold values of 20 22 24 26 28 30 32 34 and 36 m respectively were used the drainage network was extracted by the d8 algorithm during the extraction process this algorithm also needs a threshold value to extract the streamlines after the calculation of the flow accumulation when the flow accumulation value of a grid cell is larger than the threshold value the grid cell will be treated as a stream the choice of threshold value s depends on the specific situation we chose a series of thresholds 1 000 2 000 3 000 4 000 5000 and 6000 m2 to generate a series of possible values for the work at hand the douglas peucker algorithm was also deployed and used to simplify the drainage network using threshold values of 5 and 30 m respectively to match the grid spacing of the small and large dems in addition to the parallelization algorithm the anudem compound and maximum z tolerance methods were used to generalize the dems and these results were used to assess the relative accuracy and computing performance of the new method the proposed parallel multi point and maximum z tolerance algorithms respectively were used to retrieve the critical points for the anudem and compound methods both methods utilized the d8 algorithm to extract the streamlines the anudem method combines the critical points and streamlines to directly generate the dems based on an iterative finite difference interpolation the method is compared to highlight the advantages of incorporating the tin based structure in the parallelization algorithm the compound method inserts the streamlines into the critical points to construct the tins for generating the dems the comparison with this method is for highlighting the impact of using the parallel multi point algorithm in the proposed method finally the new method is compared to the maximum z tolerance algorithm to illustrate the advantages of combining the parallel multi point and streamlines experiments demonstrated that the rmse values from the anudem method increased along with the threshold values of the d8 algorithm on the small dem the rmse values from the compound and parallel methods decreased with increasing threshold values thus the threshold values for the anudem compound and parallel methods were set at 1000 6000 and 6000 respectively on the small dem in the case of the large dem the rmse values for all three methods declined with increasing threshold values and the preferred threshold was set at 6000 for all three methods the generalized dems were next compared with the original dem to derive the rmse values for the four methods on the small and large dems tables 2 3 the several threshold values and corresponding rmse values were used to select the appropriate threshold values for example we considered that an rmse of 5 m in elevation was acceptable to match the scale of the small dem state bureau of surveying and mapping 2001 the appropriate values reported in table 2 were next used to derive the best fitting formulae between the threshold value and rmse and to calculate the corresponding correlation coefficients r2 shown in table 4 the r2 values exceed 0 99 and show the formulae fit the experiment data very well the sound threshold values for the anudem compound maximum z tolerance and parallel methods are 12 04 m 15 78 m 15 98 m and 16 75 m respectively in addition we validated the rmse values of the generalized dems using these same values and they are very close to 5 m as well the experiments to assess the computing performance of the parallelization algorithm were implemented on a desktop computer with an i5 6500 cpu 8 gb ram 1060 nvidia geforce gtx and microsoft windows 7 using the 64 bit option in terms of computing performance the anudem method incorporates critical point retrieval drainage extraction and interpolation the compound method which is the same as the parallelization algorithm incorporates critical point retrieval drainage extraction and tin construction the maximum z tolerance incorporates critical point retrieval and tin construction the calculation times obtained by varying the threshold value for the selected algorithms are summarized in tables 5 6 4 discussion this study aimed to efficiently use finer resolution dems to generalize coarse resolution dems which maximally maintained the significant terrain features thus we not only use the time statistics to evaluate the computing efficiency but also employ rmse ms mr smr and sme to make comparisons in terms of accuracy between the proposed parallelization algorithm and three existing approaches 4 1 computing performance enhancement measures fig 7 shows the comparisons of computing time between the selected algorithms on the two test dems the computing time of the parallelization algorithm is sharply decreased and combined with the results reported in tables 5 and 6 these results demonstrate that the computing time of the parallelization algorithm is reduced to 4 of the anudem and compound methods in the best case with the increase of the number of dem grid cells the parallelization algorithm is much more efficient in terms of the calculations and uses 3 of the computing time used by the anudem and compound methods in the worst case fig 7 b in addition the computing efficiency of the proposed method is reduced to half that of the anudem algorithm notwithstanding the fact they use the same algorithm to extract critical points thus the proposed algorithm has extremely high computational efficiency compared to the three existing methods 4 2 accuracy measures in terms of terrain analysis the representation of the terrain features is a critical measure fig 8 demonstrates that the proposed algorithm performs much better than the anudem method and it can sustain the same rmse using half the number of points used by anudem however the new algorithm performs slightly worse than the other two algorithms it will achieve the same rmses when using 30 40 more points and given the high computing efficiency of the parallelization algorithm generalized dems with higher accuracy can be generated by retrieving larger numbers of the critical points figs 9 10 illustrate the comparison of ms and mr between the compound maximum z tolerance and parallel methods on the two dems respectively given the poor rmse values produced with anudem this method is not included in these comparisons the ms values exceed 27 0 and 24 0 respectively and the mr values exceed 1 14 and 1 11 respectively for the small and large dems using all three methods tested the line traces in figs 9 10 show that the three methods follow each other closely in terms of performance with the parallelization algorithm performing slightly better than the maximum z tolerance method and slightly worse than the compound method but the differences are 1 in every case such that their impact on the terrain features can be ignored figs 11 12 show the comparisons of smr and sme for the compound maximum z tolerance and parallel methods on the two dems respectively the streamline matching of the parallel method is almost double and the matching deviation is reduced by half compared to the maximum z tolerance algorithm the parallel algorithm shows a slightly better streamline matching rate and a slightly lower matching error compared to the compound method as well overall these results show that the parallel algorithm can better sustain the drainage features during the generalization process 5 conclusions in this study an effective parallelization algorithm for dem generalization based on cuda was proposed the goal was to use the minimum time to retrieve critical points from a finer resolution dem for generating multi scale dems with high accuracy the proposed method took the parallel multi point and d8 algorithms to generate the coarser resolution dems by constructing drainage constrained tins the parallel multi point algorithm was obtained by embedding cuda into a multi point algorithm which was presented to accurately retrieve the critical points depending on the point s significance in identifying inherent geomorphological characteristics the performance of the new method was compared to the aundem compound and maximum z tolerance methods in terms of efficiency and accuracy the results demonstrate that the parallelization algorithm can generalize dems with high efficiency and accuracy future studies will focus on the parallelization of the tin construction algorithm and incorporating it into the parallel multi point algorithm in addition ortega and rueda 2010 paralleled the d8 algorithm based on cuda the combination of the parallelization of extracting critical points and stream networks based on cuda is another area for further research research will also focus on the development of quick multi scale dems with high accuracy to support the simulation of complex and dynamic surface hydrological processes declarations of interest none acknowledgements this work was supported by national key r d program of china grant number 2018yfb0505302 and the national nature science foundation of china grant numbers 41671380 
26252,the aim of this study is to propose a new system for ranking models performance the system is based on the standardized ranking performance index srpi which combines the results of any number and type of criteria from different simulation scenarios the srpi ranges between 0 and 1 1 for the best and 0 for the worst performing model based on the relative distance of models accuracy the srpi is also combined with an additional index called rci ranking criteria index that identifies which criteria are the most and the least representative for a specific dataset scenario the presentation of srpi methodology is performed comparing ten reference evapotranspiration models using a large list of statistical criteria for both daily and monthly steps the srpi methodology could significantly reduce the complexity to evaluate models performance based on multiple scenarios and can identify the most descriptive criteria reducing their overuse in modelling studies keywords statistical criteria relative distance of models criteria selection cluster analysis reference crop evapotranspiration models abbreviations a c j the average of y k j values of criteria belonging to the same c cluster for a specific model j a c max maximum observed a c j value among all models for the same c cluster of criteria a c min minimum observed a c j value among all models for the same c cluster of criteria aic akaike information criterion ame absolute maximum error ars percentage of absolute ranking similarity between two criteria β total number of criteria belonging to a cluster bic bayesian information criterion c total number of criteria clusters for a simulation scenario c d factor for defining the number of clusters in a dendrogram based on a maximum distance threshold d max 1 maximum distance in the initial dendrogram for c 1 d max c maximum distance in a dendrogram for a final total number of c clusters of criteria et o reference crop evapotranspiration irmse inertia root mean squared error j total number of models which are subjected to evaluation based on srpi k total number of statistical criteria which are used in srpi formula for a specific scenario kge kling gupta efficiency m total number of simulation scenarios for evaluating models mae mean absolute error mare mean absolute relative error mdape median absolute percentage error me mean error mre mean relative error msde mean squared derivative error msle mean squared logarithmic error msre mean squared relative error n number of data series nsc number of sign changes nse coefficient of efficiency nash sutcliffe efficiency o observed or benchmark data pdiff peak difference pep percent error in peak pi persistence index r 2 r squared statistic of the regression in the 1 1 plots r4ms4e fourth root of the mean quadrupled error rae relative absolute error rci k ranking criteria index of the k criterion rci k total total rci of the k criterion based on a total m number of simulation scenarios rmse root mean squared error rve relative volume error rpi j m ranking performance index of the j model based on a total k number of statistical criteria for the m simulation scenario rpi max m maximum observed rpi j m value among all models for the m simulation scenario rpi min m minimum observed rpi j m value among all models for the m simulation scenario rpi j total total ranking performance index of the model j based on a total m number of simulation scenarios rpi max total maximum observed rpi j total value among all models based on a total m number of simulation scenarios rpi min total minimum observed rpi j total value among all models based on a total m number of simulation scenarios rsqr r squared statistic coefficient of determination srpi j m standardized rpi of the j model for the scenario m s simulated data sse sum square error v k j original value of the statistical criterion k for the j model v k max maximum observed v k j value of the statistical criterion k among all models ve volumetric efficiency x k j primary transformation of the v k j in the range 0 1 where 1 denotes perfect fit x k max maximum observed x k j value of the statistical criterion k among all models x k min minimum observed x k j value of the statistical criterion k among all models y c j standardized value of a cluster of criteria for the j model y k j secondary transformation of the v k j in the full range 0 1 where the best performing model gets value 1 and the worst model gets value 0 b ordinal number of criteria belonging to a cluster c ordinal number of clusters d index of agreement j ordinal number of models which are subjected to evaluation based on srpi k ordinal number of statistical criteria which are used in rpi formula for a specific scenario m ordinal number of simulation scenarios q ordinal ranking score of models based on an individual criterion or srpi ρ spearman correlation 1 introduction the modelling of natural phenomena has become a crucial procedure for their scientific analysis and interpretation in many cases the investigation of a phenomenon can be performed by many models which are subjected to various evaluation procedures bellocchi et al 2010 the most popular procedure includes the use of various statistical criteria evaluation metrics that analyze in different ways the agreement between observed benchmark o and simulated s data large lists of such criteria and discussions about their advantages and disadvantages are provided in the works of legates and mccabe 1999 2013 krause et al 2005 moriasi et al 2007 criss and winston 2008 dawson et al 2007 2010 reusser et al 2009 usepa 2009 defra 2010 bellocchi et al 2010 hwang et al 2012 pushpalatha et al 2012 willmott et al 2012 2015 bennett et al 2013 bardsley 2013 hauduc et al 2015 zambrano bigiarini 2017 emery et al 2017 and others the need for many criteria lays on the fact that there is not a criterion that can be used as a universal measure of performance while the selected criteria should correspond to the particular modelling objectives of each individual application dawson et al 2007 jain and sudheer 2008 on the other hand the use of many statistical criteria may also lead to a dead end when the criteria provide different rankings of models not allowing an easy evaluation of their performance attempts to solve this problem were made towards combining some of them graphically for example the taylor diagram taylor 2001 combines the pearson correlation the root mean square error and the standard deviation the problem with the taylor diagram is that it requires graphical interpretation and it is based only on three specific criteria thus it may not be able to provide a visually clear ranking if the models provide slightly different simulations venkatram 2008 also provided an alternative graphical representation of taylor diagrams based on the geometric mean and the geometric standard deviation of the ratio between observed and simulated data but again the same problems may appear a solution to the aforementioned problems was given through the ideal point error index ipe index elshorbagy et al 2010a b domínguez et al 2011 dawson et al 2012 which combines various criteria domínguez et al 2011 and dawson et al 2012 suggested the use of correlations and principal component analysis pca for neglecting criteria of similar response in order to remove redundant information in the calculation of ipe the use of criteria correlations or pca may lead to significant loss of information about models rankings since two or more highly correlated criteria may provide rankings that are completely different this aspect will be shown in the case study of this paper in the case of statistical models e g regression models the problem of using combinations of many criteria for ranking their performance was partly solved through the use of individual criteria like the akaike information criterion aic akaike 1974 the bayesian information criterion bic schwarz 1978 or other similar ones like the corrected akaike information criterion aicc hurvich and tsai 1989 and kashyap information criterion kic kashyap 1982 ye et al 2008 these criteria consider the number of calibrated coefficients used in the formulas of statistical models the main problem of these criteria is that they cannot be used to compare deterministic models models based on groups of physically based equations it is also not common to use these criteria even for statistically based models when they are subjected to external validation i e comparison of pre calibrated models without re calibrating them using the same benchmark dataset the use of many criteria may become even more complicated if researchers want to assess the overall ranking of many models based on different time steps different periods of observation or different case studies i e different simulation scenarios for example the formulas of reference crop evapotranspiration et o or hydrological models that simulate runoff can be analyzed at hourly daily and monthly steps in this case the datasets of various time steps cannot be merged due to differences in the magnitude of values and due to the different number of observations of each dataset moreover such models may get different rankings at different time steps similar problems appear when the same models are used in different case studies thus an overall ranking based on many scenarios could be a great challenge if the models and the selected criteria are too many considering the aforementioned constraints a method that is able to provide an integrated ranking of many models using many statistical criteria for many simulation scenarios is missing additionally there is not an integrated method that can rank the representativeness of the criteria for a specific dataset scenario taking into account the aforementioned issues the aim of the study is to propose a new methodology based on a new index standardized ranking performance index srpi for ranking many models combining many criteria and many scenarios the proposed methodology also includes new procedures for reducing the redundant effects of criteria and a new procedure for ranking their representativeness through an additional index called rci ranking criteria index climatic data for evaluating 10 reference crop evapotranspiration formulas are used in this study in order to present the srpi methodology 2 methods and data 2 1 general methodology of standardized ranking performance index srpi various software packages have been developed for calculating a large number of statistical criteria some of the most popular ones are the hydrogof r package zambrano bigiarini 2017 the hydrotest software dawson et al 2007 2010 and others for the purposes of this study the hydrotest package was used that calculates a list of 26 criteria eqs s1 s26 in table s1 of the supplementary material 1 table s1 also provides an indicative description of their characteristics having already a large list of available criteria the next step is to select the most or to exclude the least appropriate ones depending on the modelling objectives and based on expert judgment the next step is to divide the selected criteria into two groups the type i and type ii groups the type i criteria provide v values that range between 0 and 1 or between and 1 where v equal to 1 denotes perfect fit of the model some of these criteria are the coefficient of determination rsqr the index of agreement d willmott 1981 the nash sutcliffe efficiency nse nash and sutcliffe 1970 the kling gupta efficiency kge gupta et al 2009 and others table s1 in the supplementary material 1 the criteria which provide values between and 1 get negative values when the model performance is so bad that the mean value of the observed data could be a better predictor than the model krause et al 2005 gupta et al 2009 if these type of criteria show negative values then they are also excluded from the analysis the type ii of criteria provide v values that range between and or between 0 and where v equal to 0 denotes perfect fit of the model some of these criteria are the mean error me the mean absolute error mae the root mean square error rmse and others see table s1 in the supplementary material in type ii group could also be included specific metrics which are based on a unique and countable feature of the data by using the abstraction between its observed and simulated value o s for example the slope of the flow duration curve carrillo et al 2011 is of great interest in hydrological studies and the abstraction between its observed and simulated value can be handled as a type ii metric the original v values of the selected criteria are transformed in two steps where x denotes the primary transformation and y denotes the secondary transformation of the v values for the criteria belonging to type i group the original value v k j of criterion k for the model j is transformed to x k j according to the following rule 1 x k j v k j when v k j 0 if v k j 0 then the k criterion is excluded for the criteria belonging to type ii group the original value v k j of criterion k for model j is transformed to x k j according to the following rule 2 x k j 1 v k j 1 v k max 1 where v k j is the absolute v k j value of criterion k for the model j and v k max is the maximum absolute v k j value of criterion k among all models the x k j values of eqs 1 and 2 range between 0 and 1 where 1 indicates perfect fit a basic difference between the type i and ii is that the x k j values of a criterion belonging to type i are independent from the v k j value of the worst performing model while they are not in the case of a criterion belonging to type ii this difference between type i and ii of criteria ceases to exist when the secondary transformation y is performed using the following equation 3 y k j x k j x k min x k max x k min where x k min and x k max are the minimum and maximum x k j values of the k criterion respectively among all models the x and finally y transformations succeed to standardize the initial v values of any criterion to the full range between 0 and 1 based on their relative distance where the best performing model gets always a y value equal to 1 y equal to 1 does not necessary indicate perfect fit and the worst performing model gets always a y value equal to 0 eq 3 makes all criteria regardless of their type to be dependent by the performance of the worst performing model if we assume that a total j number of models are evaluated based on a total k number of statistical criteria for a specific simulation scenario m then the non standardized ranking performance index rpi of each j model is estimated by the following formula 4 r p i j m 1 k k 1 k y k j m where rpi j m is the non standardized ranking performance index of the j model based on a final k number of statistical criteria for the simulation scenario m y k j m is the secondary transformation of the statistical criterion k of the model j for the simulation scenario m the rpi j m of each model is standardized according to the following formula 5 s r p i j m r p i j m r p i min m r p i max m r p i min m where srpi j m is the standardized ranking performance index of model j for the scenario m rpi j m is the value of rpi of model j for the scenario m while rpi max m and rpi min m are the respective maximum and minimum rpi j m values among all models for the scenario m eq 5 standardizes the values of rpi values of each model to the full range between 0 and 1 based on their relative distance where the best performing model gets always a srpi value equal to 1 and the worst performing one gets a srpi value equal to 0 the aforementioned procedures for a general application of srpi for a specific simulation scenario are summarized in the flow chart of fig 1 a if we assume that the same criteria have been used to evaluate the same models for other simulation scenarios then the overall ranking of models based on all scenarios can be performed by the following formula 6 r p i j total 1 m m 1 m s r p i j m where rpi j total is the total ranking performance index of the model j based on a total m number of simulation scenarios rpi j total is also standardized to srpi j total using the following formula 7 s r p i j total r p i j total r p i min total r p i max total r p i min total where srpi j total is the standardized ranking performance index of model j for all scenarios rpi j total is the value of rpi of model j for all simulation scenarios while rpi max total and rpi min total are the respective maximum and minimum rpi j total values among all models for all simulation scenarios the aforementioned procedure for a general application of srpi for multiple simulation scenarios is summarized in the flow chart of fig 1b the values of any individual criterion and the values of srpi either for one simulation scenario eq 5 or multiple scenarios eq 7 can be converted to ranking scores q using ordinal numbers i e q 1 2 3 j where q equal to 1 refers to the model with the best performance in this study the format of q ranking is based on a specific scheme which considers that when two models present the same value of an individual criterion or srpi then they get the same q ranking value while the next model gets a value equal to q 2 neglecting completely the ranking position q 1 such q rankings can easily be calculated with the rank function in excel see supplementary material 2 rank considering the aforementioned procedures the srpi methodology can combine any number of criteria and any number of simulation scenarios in order to provide an overall ranking of models performance on the other hand a large number of criteria may contain criteria that capture similar information of models behavior the redudant information increases non proportionally its effect in the final ranking of models and for this reason two cases for further selection and elaboration of criteria are proposed in the following sections in order to make srpi more robust 2 2 excluding redundant criteria two tools are proposed in this study that can be used in order to exclude criteria that capture redundant information the first one is the use of spearman correlations ρ among criteria when ρ correlations are performed between two criteria it is preferable to use the y values instead of v values in order to avoid sign confusion this option is also used in this study and thus all spearman ρ correlations among criteria are referred to their y values the second tool is the new term of percent of absolute ranking similarity ars which compares the q rankings of models provided by two criteria ars calculates the percentage ratio between the number of models that got the same q value in both criteria and the total number of models when two criteria c1 and c2 provide absolutely the same ranking of models then ars c1 c2 100 an example of ars calculation in excel is provided in the supplementary material 2 ars the tools of ρ and ars are used in this study according to the following rule case a of srpi analysis when the q values of two criteria show ars 100 and their y values show correlation ρ 0 95 then the one criterion that shows larger average ρ value with the rest criteria should be removed in this way even highly correlated criteria that do not provide absolutely the same ranking of models remain in the srpi analysis the procedure of case a is shown as a flow chart in fig 2 a the aforementioned rule aims to show a paradigm of ρ and ars combination with specific thresholds in other studies it is also valid to use solely either ρ or ars stricter thresholds for ρ or ars can also be used in order to significantly reduce the criteria used in srpi analysis another valid case is also to select from the beginning a small number of criteria that target to capture specific features and to use ρ and or ars in order to show that the selected criteria do not transfer redundant information in srpi 2 3 developing clusters of criteria apart from excluding criteria using the options given in section 2 2 e g case a there is an additional procedure for reducing the effect of redundant information based on clusters of criteria the criteria are subjected to cluster analysis with dendrograms using the q ranking values of each model i e the y values of criteria are converted in q values in order to create subgroups with criteria of high similarity the dendrogram starts with one cluster group in order to assess the maximum distance d max 1 then the clusters are increased step by step until the maximum observed distance in the c final total number of clusters to become d max c c d d max 1 where c d 1 the value of c d mainly depends on the number of models for example if we have to compare ten models using many criteria a c d 0 5 means that the criteria of each group cluster will carry maximum 50 of different information in this study the dendrograms were created using nearest neighbor with squared euclidean distance without standardizing the q values and the clusters were formed based on a c d 0 5 the maximum distances were derived from the agglomeration schedule tables cluster analysis was performed in statgraphics centurion xv statpoint inc if two or more criteria belong to the same cluster c then the average a c j of their y k j values for each j model is estimated the a c j values are not anymore in the full range 0 1 with 1 for the best and 0 for the worst performing model for this reason they should be readjusted again using the following formula 8a b y c j a c j a c min a c max a c min where a c j b 1 b y k j where a c j is the average of y k j values of criteria that belong to the same c cluster for the j model a c max and a c min are the maximum and minimum a c j values respectively among models for the cluster c of criteria after this procedure the rpi j m of each model for a specific simulation scenario m is calculated based on eq 4 considering the y c j values instead of the y k j values while the srpi j m is calculated based on eq 5 the inclusion of criteria clusters provides a balanced type of srpi that reduces even more the effect of redundant information the final y c j values of each cluster can also be used separately to highlight specific aspects of models ranking the clustering procedure for srpi analysis can be performed either directly on a large number of criteria or after following the proposed procedures of section 2 2 e g case a for criteria exclusion in this study the analysis of srpi is performed based on case a fig 2a but also using a second case case b where srpi is calculated based on case a followed by criteria clustering the procedure of case b is shown as a flow chart in fig 2b 2 4 evaluating statistical criteria based on srpi an alternative use of srpi could also be the identification of the most and the least representative criteria if we perform spearman ρ correlations between srpi and the y values of individual criteria the higher ρ may indicate that a specific criterion is the most representative among those used for building the srpi inversely the lower correlation can indicate the least representative criterion on the other hand a very high correlation does not necessarily mean that the specific criterion provides a high value of ars with srpi when their q values are compared thus the relationship between a criterion and the final srpi should be evaluated by both their correlation and ars considering this an additional index was formed ranking criteria index rci for ranking the criteria based on their relationship with srpi 9 r c i k a r s k s r p i ρ y k s r p i where rci k is the ranking index of the k criterion based on srpi ars k srpi is the absolute ranking similarity between the q values provided by the k criterion and the q values provided by srpi and ρ υ k srpi is the spearman ρ correlation between the y transformed values of the k criterion versus the respective values of srpi for the rci analysis the srpi values of case a should be used because it uses all the criteria equally while it also carries the information of the criteria that were removed during any procedure of criteria exclusion as these described in section 2 2 if more than one scenario is used to evaluate models performance then a respective overall evaluation of the criteria based on all scenarios can also be performed according to the following 10 r c i k t o t a l 1 m m 1 m r c i k m where m is the total number of scenarios the aforementioned procedure is extremely valuable since it indicates that srpi is not only able to rank the models combining many criteria but it is also able to rank the criteria based on their representativeness 2 5 1 1 plots many authors have suggested that the comparison of models performance should also be accompanied by visual inspection through graphical techniques such as the graphic comparison of observed o and simulated s time series the 1 1 scatter plots of o versus s values residual error plots frequency distributions etc biondi et al 2012 dawson et al 2012 the 1 1 scatter plots are usually selected since they are more easily interpretable for long data series providing an objective reference given by the 1 1 line of perfect fit biondi et al 2012 a linear regression based on the scatter points can also provide significant information about the distribution trends of the points along the 1 1 line of perfect fit the r 2 of the regression line is estimated by eq s27 when the regression is linear without forcing intercept to be 0 in this case the r 2 has the same value with the coefficient of determination rsqr eq s15 the calculation of r 2 for a linear regression with intercept forced to 0 is theoretically problematic and should be avoided eisenhauer 2003 another reason for not using linear regression with 0 intercept in 1 1 plots is that the regression line neglects information associated to the performance of a model in the low values for the datasets of this study all the comparisons between observed and simulated data were visualized through 1 1 plots using the y axis for the observed and the x axis for the simulated values as suggested by piñeiro et al 2008 a linear regression equation without forcing intercept to 0 on the scatter data of 1 1 plots was also added in order to show the distribution trends of the scattered points along the 1 1 line of perfect fit 2 6 data the dataset used for presenting the srpi methodology was developed based on climatic data from aristotle university farm 40 32 ν 22 59 ε in thessaloniki greece covering the 7 month warm growing period 1 april 31 october of the years 2008 2009 and 2010 data of seven months from three years these data were used by aschonitis et al 2012 2017a to estimate the reference crop evapotranspiration for short reference crop et o using the asce standardized formula asces eq a1 table a1 in the appendix this method is equivalent to fao 56 model for daily step calculations of short reference crop allen et al 1998 2005 this et o formula is considered the most precise and it is commonly used as a benchmark for comparing other simplified et o formulas walter et al 2001 allen et al 2005 the climatic data of november march were not used in this study due to the large number of days with extremely low et o values 0 3 mm d 1 for which asces validity may also be questionable the examination of srpi methodology for comparing et o methods is used in this case study for the following reasons evapotranspiration studies that propose new et o formulas usually perform comparisons with many others using many statistical criteria alexandris et al 2006 valiantzas 2013a heydari et al 2014 valipour 2015a b c 2017 a b aschonitis et al 2017a b bourletsikas et al 2017 there are many global pre calibrated et o formulas of reduced parameters e g original hargreaves samani and priestley taylor equations which are subjected to external validation based on local studies the selection and interpretation of some criteria e g rsqr aic bic etc for evaluating such models based on external validation presents differences in comparison to the case where the models are calibrated and evaluated using the same benchmark data et o can be analyzed based on various time steps e g hourly daily monthly thus the predictive accuracy of the same et o methods may be different when compared at different time steps because larger time steps e g monthly may counterbalance large daily et o variations in this case it would be useful for scientists to have the individual but also an overall ranking of et o methods considering all the possible time steps considering the above ten formulas models apart from asces table a1 eqs a2 a11 in the appendix were used to estimate et o at daily and monthly time steps based on the aforementioned climatic data for greek conditions 642 daily and 21 monthly et o values for each model their predictive accuracy was compared versus the respective daily and monthly benchmark values of asces method the specific ten et o models j 10 were selected because they have been already calibrated validated for global and applied for greek conditions in previous studies using different climate datasets thus the comparison of these models versus the benchmark data of asces constitutes a case study of external validation in this study the srpi analysis is performed using the daily and the monthly data denoted as daily and monthly scenario in order to investigate the performance of the models based on the two different time steps the srpi of the two time steps are further combined in order to rank the et o models based on both time steps of analysis m 2 in eq 6 the srpi analysis is performed twice based on case a and case b fig 2 respectively finally the procedure of 2 4 section is performed in order to assess the representativeness of the criteria based on rci analysis 3 results 3 1 selection of statistical criteria based on modelling objectives and 1 1 scatter plots the interpretation of statistical criteria when comparing et o models is generally less complex in comparison to other hydrological parameters e g runoff because et o transitions are smoother without showing extreme changes from the 26 criteria provided by hydrotest package eqs s1 s26 table s1 six of them pdiff nsc pep pi msde irmse were not calculated and not included in the srpi methodology for analyzing et o because they are more appropriate for hydrograph and flow analysis i e phenomena with steep transitions or analysis of sequential time series where random selection of data is not allowed the coefficient of determination rsqr and the aic and bic criteria were also not included in the srpi methodology because the et o models are not calibrated and they are directly subjected to external validation these criteria have a meaning only if the srpi analysis is performed during calibration of models using the same benchmark dataset the remaining 17 criteria ame mae me rmse r4ms4e rae mare mdape mre msre rve d nse msle ve kge and sse were all included in the next steps the modelling objectives were to find which et o model better fits the daily benchmark data of asces the monthly benchmark data of asces and both the objectives were kept more general in order to show the ability of the proposed methodology to handle ten et o models with seventeen criteria while the results of srpi were inversely used in order to extract information about the models and the aforementioned criteria for constructing more specific modelling objectives the 1 1 plots between the et o values of benchmark asces method eq a1 versus the respective values of the other ten et o methods eqs a2 a11 for the daily and the monthly time step are given in figs 3 and 4 respectively the rowwise statistics of the daily and monthly et o estimations of the eleven models are given in tables s2a and b respectively of the supplementary material 1 3 2 estimation transformation and relations between criteria the aforementioned seventeen criteria were divided in groups type i or ii and their original v values were calculated using the estimations of the ten et o models eqs a2 a11 versus the benchmark values of asces model eq a1 based on the daily and the monthly simulation scenario the v values for the daily and the monthly simulation scenario are given in tables s3a and b respectively of the supplementary material 1 the v values of criteria in tables s3a and b were transformed to x values using eqs 1 and 2 depending on the type and the results are given in tables s4a and b respectively tables s4a and b also include the x max and x min values of each criterion which are used in the secondary transformation to y values based on eq 3 the final transformation of the criteria to y values based on the daily and monthly scenario are given in tables s5a and b respectively the q ranking values of models for each individual criterion based on y values are given in tables s6a and b for the daily and monthly scenario respectively the y values of the criteria for each scenario tables s5a and b were subjected to multiple spearman ρ correlations and the results are given in tables s7a and b respectively the q ranking scores of individual criteria tables s6a and b were subjected to ars analysis and they are given in tables s8a and b for each simulation scenario taking into account the ρ and ars results of tables s7 and s8 respectively it was observed that there are many pairs of criteria that present high ρ values associated with very low ars for example in the case of daily et o data analysis the nse and mre criteria present ρ 0 83 p 0 01 table s7a and ars 20 table s8a this observation is extremely important since eight out of ten models are ranked differently by nse and mre despite their high ρ value the overall comparison of ρ and ars indicated that highly correlated criteria present significantly different rankings of models low ars values in both daily and monthly scenario this is verified by the non linear regressions made between ρ table s7 and ars table s8 values and the results are given in fig 5 a and b for each scenario fig 5 indicates that the use of ρ correlations or associated methods like pca for excluding completely some criteria method used in ipe method may lead to significant loss of information related to different aspects in the performance of the models fig 5 also shows that even for a rate of ars 50 between two criteria the ρ values do not fall below 0 9 in both daily and monthly scenario 3 3 calculation of srpi based on case a as indicated in section 2 2 the case a of analysis for excluding criteria considers the rule when the q values of two criteria show ars 100 and their y values show correlation ρ 0 95 then the one that shows larger average ρ value with the rest criteria should be removed such pairs of criteria are indicated with grey in table s7 following this rule five criteria from the daily and six criteria for the monthly scenario were excluded table 1 a the y values of the remaining criteria table 1a from the daily and monthly simulation scenarios were used to calculate the srpi values eq 5 of each et o model for each individual scenario the srpi values of the ten models for the daily and monthly scenario according to case a srpi daily a and srpi monthly a respectively are given in fig 6 the values of srpi daily a and srpi monthly a were further used to calculate the total srpi total a for both scenarios according to eq 7 fig 6 3 4 calculation of srpi based on case b taking into account the procedures of case b in section 2 3 the 12 and 11 remaining criteria of the daily and monthly simulation scenario table 1a respectively were subjected to cluster analysis with dendrograms based on their q values which are given in tables s6a and b of supplementary material 1 fig 7 a shows the initial dendrogram of criteria for the daily scenario where d max 1 56 the number of clusters was gradually increased in order to reach a c number of clusters until d max c 0 5 d max 1 at the end c 4 clusters were developed with d max 4 24 fig 7b clusters d1 d2 d3 and d4 indicated in table 1b similarly for the case of monthly scenario the initial dendrogram is given in fig 7c where d max 1 was found equal to 14 the number of clusters was again gradually increased until d max c 0 5 d max 1 at the end c 3 clusters were developed with d max 3 6 fig 7d clusters m1 m2 and m3 indicated in table 1b the y k j values of the criteria belonging to each cluster of the daily and monthly scenario were elaborated using eq 8a b in order to derive the respective y c j values table s9 in supplementary material 1 the y c j values were used to calculate the srpi values of the ten models for the daily and monthly scenario according to case b srpi daily b and srpi monthly b respectively fig 8 srpi daily b and srpi monthly b were further used to calculate the srpi total b fig 8 3 5 ranking of models the respective q rankings of the models taking into account the results of srpi daily a srpi monthly a and srpi total a for the case a fig 6 were derived and they are given in table 2 a similarly the respective q rankings of the models taking into account the results of srpi daily b srpi monthly b and srpi total b for the case b fig 8 were derived and they are given in table 2b the form of table 2 provides an overall aspect about the rankings of the models based on both cases a and b of srpi analysis the models that hold the best ranking positions q 1 in both case a and b of srpi analysis for the daily monthly and both simulation scenarios table 2a b are the hs a30s val1 copais and hs dral1 taking into account table 2 more specific modelling objectives can be satisfied as follows the val1 model showed the best performance for the monthly simulation scenario in both cases a and b q monthly a 1 and q monthly b 1 table 2a b thus if the modelling objective is to achieve higher accuracy only in monthly et o estimations then the optimum choice is the val1 model the hs a30s model showed the best performance for the daily simulation scenario of case a q dailyy a 1 in table 2a while the copais model showed the best performance for the daily simulation scenario of case b q dailyy b 1 in table 2b thus if the modelling objective is to achieve higher accuracy only in daily et o estimations then the first optimum choice is the copais models and the second one is the hs a30s since the ranking of case b is considered more robust compared to case a the hs a30s model showed the best combined performance for both daily and monthly simulation scenarios of case a q total a 1 in table 2a while the hs dral1 model showed the best combined performance for both daily and monthly simulation scenarios of case b q total b 1 in table 2b thus for a general application where high accuracy is required in both daily and monthly et o estimations then the first optimum choice is the hs dral1 model and the second one is the hs a30s since the rankings of case b are considered more robust compared to case a 3 6 calculation of rci and ranking of the criteria during the step of criteria exclusion of case a 17 criteria were initial considered while 12 and 11 were finally selected table 1a to participate in the srpi daily a and srpi monthly a respectively the basic information of the five and six criteria which were removed due to their high redundancy effects also exists in the final selected criteria of case a thus the final srpi daily a and srpi monthly a carry information from all the 17 criteria considering this element the ars and ρ between each criterion and srpi daily a for the daily scenario and between each criterion and srpi monthly a for the monthly scenario were estimated table s10 in supplementary material 1 based on the procedure of section 2 4 their values were finally used to estimate the rci eq 9 values of all 17 criteria for the daily rci daily and monthly rci monthly scenario table 3 rci daily and rci monthly were further combined according to eq 10 to derive rci total for both daily and monthly data analysis considering the results of table 3 the following observations were made in the case of daily scenario rci daily the three most representative criteria were the nse rmse and sse these criteria are based on s i o i 2 and focus on the overall error according to table s7a these three criteria are absolutely intercorrelated providing the same rankings of the models table s8a ars 100 thus selecting just one of them is enough for describing the performance of the models based on the overall error on the other hand the three least representative criteria were msle ame and kge msle uses the transformation of natural logarithm for data transformation giving more weight to low simulated values providing a suggestion about which model can perform better in the low values pushpalatha et al 2012 ame seeks for the larger s i o i which is very important when someone is looking for a model with low risk of high failure finally kge focuses on the overall error but it is based on an equal weighting of the three components of correlation bias and variability measures while nse rmse and sse do not gupta et al 2009 in the case of monthly scenario rci monthly the three most representative criteria were the ve mae and rae these criteria are based on s i o i and focus on the overall error according to table s7b these three criteria are absolutely intercorrelated providing the same rankings of the models table s8b ars 100 thus selecting just one of them is enough for describing the performance of the models based on the overall error on the other hand the three least representative criteria were r4ms4e ame and kge the role of ame and kge was described in the previous case r4ms4e gives more weight to high simulated values providing a suggestion about which model can perform better in the high values in the case of rci total the three most and the three least representative criteria are the same as in the case of rci monthly the aforementioned results are of high importance since they show that the most and the least representative criteria focus on very different aspects of models performance thus their combined use could provide a small number of criteria that are able to provide an integrated aspect about the performance of the models avoiding criteria overuse 4 discussion 4 1 other cases for reducing the redundant information the presentation of srpi methodology included two cases of analysis where in the case a all the criteria participated equally while in the case b the criteria were clustered in order to reduce further the redundant information another option could be to neglect completely the step of cluster analysis following a stricter rule for removing criteria directly in case a for example the use of the rule if ars 100 and ρ 0 95 between two criteria then the one is excluded can change by reducing the respective ars and ρ thresholds the ars vs ρ graph as in fig 5a and b can be used as a guide for choosing appropriate thresholds 4 2 the role of ars in this study special attention was given to the parameter ars as it was shown in fig 5 many pairs of criteria showed high ρ correlations with low similarity of models s rankings low ars values this observation was extremely important since in many studies the correlations among criteria or similar approaches e g orthogonality rules based on pca are the base for reducing their number domínguez et al 2011 dawson et al 2012 the importance of this parameter is proportional to the number of models i e the larger the number of models is the larger is the possibility to observe highly correlated criteria with low ars 4 3 effects of a poorly performing model on srpi another subject for discussion is the role of a poorly performing model in the srpi analysis as indicated by dawson et al 2012 the participation of a poorly performing model may reduce significantly the differences of the other models when its performance is used for standardizing the values of a criterion e g eqs 2 and 3 a poorly performing model can be easily identified since it will not have only srpi 0 but also y 0 for every individual criterion this model will take anyway the last position and can be removed from the comparison while srpi can be recalculated in order to provide more robust rankings for the rest models another option could also be the use of a naïve model dawson et al 2012 for standardizing the values of the criteria 4 4 the effects of high observed and simulated values in this case study the modelling objectives were set in a general context in order to present the srpi methodology without considering the basic element that the majority of statistical criteria is strongly affected by the higher observed and simulated values de vos and rientjes 2007 pushpalatha et al 2012 hydrological phenomena present high seasonality and the low values of observed hydrological parameters are also integral components of the hydrologic regime while the demand for good forecasts for the low intensity periods is rising due to the increased competition between water uses pushpalatha et al 2012 apart from the analysis and simulation of hydrological phenomena which usually focus on high intensity periods there are also other simulated environmental parameters e g crop yields simulation using crop models aschonitis et al 2013 where the smaller values of the investigated parameter e g yield are more important for the development of management plans and decision support systems proposed solutions for reducing the specific problem are a the separate analysis of low and high intensity periods and b data transformation types such as box cox square root logarithmic etc that increase the weight effect of low intensity values on the criteria chiew et al 1993 smakhtin et al 1998 oudin et al 2006 willems 2009 pushpalatha et al 2012 before the use of criteria in srpi analysis 4 5 are all criteria equally significant all the participating criteria in the analysis of srpi for the case a were considered equally significant similarly all the participating clusters of criteria in the analysis of srpi for the case b were also considered equally significant someone may argue about the use of equal significance if the modelling objective targets to specific elements if this is the case the srpi users should consider the ranking of models based on individual criteria that are a priori designed to capture such specific elements or to use a weight factor that will increase their effect in the rpi formula eq 4 4 6 the role of aic bic and other similar metrics the aic akaike 1974 and bic schwarz 1978 criteria that can be provided by hydrotest were not used in this study because the et o models were already calibrated in other studies based on different datasets these criteria are of high importance since their objective can be similar to srpi concept a means of statistical model selection based on the relative quality of models for a given set of data the two main differences between srpi and aic or bic criteria are a that srpi can be formed using different criteria depending on the modelling objectives and b aic and bic take into account the number of calibrated coefficients of a given model while srpi does not the second element is extremely important since statistical models with fewer coefficients are more stable with less risk of overfitting there are also other similar criteria to aic and bic such as the aicc hurvich and tsai 1989 and kic kashyap 1982 these type of criteria aic bic aicc kic present differences and limitations which should also be taken into account before their use burnham and anderson 2002 2004 ye et al 2008 in the case where the comparison is performed between statistical models calibrated with the same data the aic and bic criteria or other similar ones can be used in parallel with srpi providing a a different aspect of ranking considering the number of calibrated coefficients of the models and b a method for validating the srpi and vice versa when the compared models have the same number of calibrated coefficients the incorporation of aic bic or similar criteria in srpi could be an additional option but these criteria do not belong in group type i or in type ii since they provide a range of v values between and where the best model is the one with the smaller v for example if three models give the following aic values 20 0 10 then the best model is the one with aic 20 what matters in this type of criteria is the distance between their values burnham and anderson 2002 for this reason if negative values of these criteria exist then a constant should be added in order to make them all non negative with the best model having 0 value if negative values do not exist then the values remain the same the non negative values of these criteria are transformed to x and y values using eqs 2 and 3 before their use in srpi 4 7 sensitivity of statistical criteria another element that could be considered in srpi methodology is the sensitivity of the criteria in many cases the length of time series for comparing models may be a subject of criticism in such cases the analysis of sensitivity of statistical criteria may be an intermediate step in the selection procedure methods like bootstrapping random selection and replacement of data ritter and muñoz carpena 2013 generation of synthetic data series using bootstrap like sampling methods berthet et al 2010 in combination with error models yang et al 2007 can be used as means to evaluate the robustness of the criteria the exclusion of criteria with extremely large variations would increase the robustness of srpi special attention should be given to some criteria such as pdiff and pep that target on the analysis of specific and possibly rare elements of the time series extreme peaks or criteria such as pi irmse msde and nsc where random selection of data is not allowed 4 8 selecting criteria for individual use based on srpi rci the srpi index can be formed based on different criteria and it is proposed as a tool to rank models and not as a new statistical criterion since it is almost impossible to interpret its results without investigating the individual criteria that participate in its formula for this reason individual criteria are always needed in order to describe models specificities the problem in this case is how to select a small number of criteria that will be highly informative as indicated in section 3 6 srpi through rci provided a ranking of the representativeness of the criteria this can be an alternative use of srpi and in some cases more important than ranking the models especially when the raking of models cannot be interpreted easily this is a very crucial element since the srpi rci analysis can facilitate the users to justify the selection of a small number of statistical criteria and to interpret the models performance only with these neglecting completely the ranking of models that was produced by srpi considering the results of 3 6 section about the analysis of et o models it was shown that the combined use of the most and the least representative criteria could provide an integrated aspect about the performance of the models 5 conclusions a new system for ranking many models based on many criteria and multiple scenarios was proposed in this study the system is based on the new standardized ranking performance index srpi that describes the relative distance between the models in the full range 0 1 where the best performing model has srpi 1 and the worst performing model has srpi 0 the combination of srpi analysis based on both cases case a equally weighted criteria case b clusters of criteria for the 10 et o models and for two scenarios daily and monthly showed its ability to provide different optimum model choices depending on the modelling objectives on the other hand its alternative use through rci for comparing criteria was extremely valuable since it led to very important conclusions about how to choose few but very informative criteria for describing models performance future studies could focus on a the evaluation of the srpi methodology based on other types of models which simulate parameters with more steep behavior compared to et o e g runoff b the use of statistical models calibrated based on the same benchmark data in order to include in srpi other metrics such as aic and bic which were excluded in this study due to the application of external validation and c the use of naïve models instead of the worst model for standardizing the y and srpi formulas appendix a supplementary data the following are the supplementary data to this article the supplementary material contains a word file supplementary 1 doc with the tables s1 s2 s3 s4 s5 s6 s7 s8 s9 and s10 and an excel file supplementary 2 xls that shows an example for calculating the q ranking values and an example for calculating the ars between two criteria multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 01 005 appendix table a1 reference crop evapotranspiration models table a1 modelabbreviation citation formula climate data requirements indexing asces allen et al 2005 e t o 0 408 δ r n g 900 γ u 2 e s e a t mean 273 16 δ γ 1 0 34 u 2 t mean t max t min r s rh u 2 eq a1 hs a30s hargreaves and samani 1982 1985 aschonitis et al 2017b e t o c 1 0 408 τ m e a n 17 8 r a t d 0 5 where c 1 0 0023 t mean t max t min eq a2 hs a0 5d hargreaves and samani 1982 1985 aschonitis et al 2017b e t o c 2 0 408 τ m e a n 17 8 r a t d 0 5 where c 2 0 0021 t mean t max t min eq a3 hs dral1 droogers and allen 2002 e t o 0 00102 r a τ m e a n 16 8 t d 0 5 t mean t max t min eq a4 hs dral2 droogers and allen 2002 e t o 0 0005304 r a τ m e a n 17 0 t d 0 0123 p c p 0 76 t mean t max t min pcp eq a5 pt a30s priestley and taylor 1972 aschonitis et al 2017b e t o a 1 0 408 δ δ γ r n g where a 1 1 36 t mean t max t min r s eq a6 pt a0 5d priestley and taylor 1972 aschonitis et al 2017b e t o a 2 0 408 δ δ γ r n g where a 2 1 31 t mean t max t min r s eq a7 copais alexandris et al 2006 e t o 0 057 0 227 c 2 0 643 c 1 0 0124 c 1 c 2 where c 1 0 6416 0 00784 r h 0 372 r s 0 00264 r s r h c 2 0 0033 0 00812 t m e a n 0 101 r s 0 00584 r s t m e a n t mean r s rh eq a8 val1 valiantzas 2013b 2014 e t o 0 0393 r s t m e a n 9 5 0 19 r s 0 6 φ 0 15 0 0061 t m e a n 20 1 12 t m e a n t min 2 0 7 t mean t min r s eq a9 val2 valiantzas 2013b 2014 e t o 0 0393 r s t m e a n 9 5 0 19 r s 0 6 φ 0 15 0 078 t m e a n 20 1 r h 100 t mean r s rh eq a10 val3 valiantzas 2013c e t o 0 0393 r s t m e a n 9 5 2 4 r s r a 2 c u t m e a n 20 1 r h 100 where cu 0 054 for rh 65 and cu 0 083 for rh 65 t mean r s rh eq a11 t mean max min mean maximum and minimum temperature oc td difference between maximum and minimum temperature oc r s incident solar radiation mj m 2 d 1 r a extraterrestrial solar radiation mj m 2 d 1 rh relative humidity δ slope of the saturation vapor pressure temperature curve kpa oc 1 γ psychrometric constant kpa oc 1 g the soil heat flux density at the soil surface mj m 2 d 1 e s the saturation vapor pressure kpa e a the actual vapor pressure kpa φ absolute value of latitude rads pcp precipitation mm month 1 the values c 1 c 2 and a 1 a 2 are locally calibrated coefficients of the original hargreaves and samani 1985 and priestley and taylor 1972 formulas respectively for the position of the station based on global climatic data of the period 1950 2000 these values were obtained from raster data of the respective annual coefficients provided by aschonitis et al 2017b based on 30 arc sec resolution maps case of c 1 and a 1 and 0 5 resolution maps case of c 2 and a 2 for the case of eq a2 the c 1 0 0023 was found identical to the original coefficient of hargreaves and samani 1985 formula the complete global raster data are given in the following permanent data repositories https doi pangaea de 10 1594 pangaea 868808 http www esrn database org 
26252,the aim of this study is to propose a new system for ranking models performance the system is based on the standardized ranking performance index srpi which combines the results of any number and type of criteria from different simulation scenarios the srpi ranges between 0 and 1 1 for the best and 0 for the worst performing model based on the relative distance of models accuracy the srpi is also combined with an additional index called rci ranking criteria index that identifies which criteria are the most and the least representative for a specific dataset scenario the presentation of srpi methodology is performed comparing ten reference evapotranspiration models using a large list of statistical criteria for both daily and monthly steps the srpi methodology could significantly reduce the complexity to evaluate models performance based on multiple scenarios and can identify the most descriptive criteria reducing their overuse in modelling studies keywords statistical criteria relative distance of models criteria selection cluster analysis reference crop evapotranspiration models abbreviations a c j the average of y k j values of criteria belonging to the same c cluster for a specific model j a c max maximum observed a c j value among all models for the same c cluster of criteria a c min minimum observed a c j value among all models for the same c cluster of criteria aic akaike information criterion ame absolute maximum error ars percentage of absolute ranking similarity between two criteria β total number of criteria belonging to a cluster bic bayesian information criterion c total number of criteria clusters for a simulation scenario c d factor for defining the number of clusters in a dendrogram based on a maximum distance threshold d max 1 maximum distance in the initial dendrogram for c 1 d max c maximum distance in a dendrogram for a final total number of c clusters of criteria et o reference crop evapotranspiration irmse inertia root mean squared error j total number of models which are subjected to evaluation based on srpi k total number of statistical criteria which are used in srpi formula for a specific scenario kge kling gupta efficiency m total number of simulation scenarios for evaluating models mae mean absolute error mare mean absolute relative error mdape median absolute percentage error me mean error mre mean relative error msde mean squared derivative error msle mean squared logarithmic error msre mean squared relative error n number of data series nsc number of sign changes nse coefficient of efficiency nash sutcliffe efficiency o observed or benchmark data pdiff peak difference pep percent error in peak pi persistence index r 2 r squared statistic of the regression in the 1 1 plots r4ms4e fourth root of the mean quadrupled error rae relative absolute error rci k ranking criteria index of the k criterion rci k total total rci of the k criterion based on a total m number of simulation scenarios rmse root mean squared error rve relative volume error rpi j m ranking performance index of the j model based on a total k number of statistical criteria for the m simulation scenario rpi max m maximum observed rpi j m value among all models for the m simulation scenario rpi min m minimum observed rpi j m value among all models for the m simulation scenario rpi j total total ranking performance index of the model j based on a total m number of simulation scenarios rpi max total maximum observed rpi j total value among all models based on a total m number of simulation scenarios rpi min total minimum observed rpi j total value among all models based on a total m number of simulation scenarios rsqr r squared statistic coefficient of determination srpi j m standardized rpi of the j model for the scenario m s simulated data sse sum square error v k j original value of the statistical criterion k for the j model v k max maximum observed v k j value of the statistical criterion k among all models ve volumetric efficiency x k j primary transformation of the v k j in the range 0 1 where 1 denotes perfect fit x k max maximum observed x k j value of the statistical criterion k among all models x k min minimum observed x k j value of the statistical criterion k among all models y c j standardized value of a cluster of criteria for the j model y k j secondary transformation of the v k j in the full range 0 1 where the best performing model gets value 1 and the worst model gets value 0 b ordinal number of criteria belonging to a cluster c ordinal number of clusters d index of agreement j ordinal number of models which are subjected to evaluation based on srpi k ordinal number of statistical criteria which are used in rpi formula for a specific scenario m ordinal number of simulation scenarios q ordinal ranking score of models based on an individual criterion or srpi ρ spearman correlation 1 introduction the modelling of natural phenomena has become a crucial procedure for their scientific analysis and interpretation in many cases the investigation of a phenomenon can be performed by many models which are subjected to various evaluation procedures bellocchi et al 2010 the most popular procedure includes the use of various statistical criteria evaluation metrics that analyze in different ways the agreement between observed benchmark o and simulated s data large lists of such criteria and discussions about their advantages and disadvantages are provided in the works of legates and mccabe 1999 2013 krause et al 2005 moriasi et al 2007 criss and winston 2008 dawson et al 2007 2010 reusser et al 2009 usepa 2009 defra 2010 bellocchi et al 2010 hwang et al 2012 pushpalatha et al 2012 willmott et al 2012 2015 bennett et al 2013 bardsley 2013 hauduc et al 2015 zambrano bigiarini 2017 emery et al 2017 and others the need for many criteria lays on the fact that there is not a criterion that can be used as a universal measure of performance while the selected criteria should correspond to the particular modelling objectives of each individual application dawson et al 2007 jain and sudheer 2008 on the other hand the use of many statistical criteria may also lead to a dead end when the criteria provide different rankings of models not allowing an easy evaluation of their performance attempts to solve this problem were made towards combining some of them graphically for example the taylor diagram taylor 2001 combines the pearson correlation the root mean square error and the standard deviation the problem with the taylor diagram is that it requires graphical interpretation and it is based only on three specific criteria thus it may not be able to provide a visually clear ranking if the models provide slightly different simulations venkatram 2008 also provided an alternative graphical representation of taylor diagrams based on the geometric mean and the geometric standard deviation of the ratio between observed and simulated data but again the same problems may appear a solution to the aforementioned problems was given through the ideal point error index ipe index elshorbagy et al 2010a b domínguez et al 2011 dawson et al 2012 which combines various criteria domínguez et al 2011 and dawson et al 2012 suggested the use of correlations and principal component analysis pca for neglecting criteria of similar response in order to remove redundant information in the calculation of ipe the use of criteria correlations or pca may lead to significant loss of information about models rankings since two or more highly correlated criteria may provide rankings that are completely different this aspect will be shown in the case study of this paper in the case of statistical models e g regression models the problem of using combinations of many criteria for ranking their performance was partly solved through the use of individual criteria like the akaike information criterion aic akaike 1974 the bayesian information criterion bic schwarz 1978 or other similar ones like the corrected akaike information criterion aicc hurvich and tsai 1989 and kashyap information criterion kic kashyap 1982 ye et al 2008 these criteria consider the number of calibrated coefficients used in the formulas of statistical models the main problem of these criteria is that they cannot be used to compare deterministic models models based on groups of physically based equations it is also not common to use these criteria even for statistically based models when they are subjected to external validation i e comparison of pre calibrated models without re calibrating them using the same benchmark dataset the use of many criteria may become even more complicated if researchers want to assess the overall ranking of many models based on different time steps different periods of observation or different case studies i e different simulation scenarios for example the formulas of reference crop evapotranspiration et o or hydrological models that simulate runoff can be analyzed at hourly daily and monthly steps in this case the datasets of various time steps cannot be merged due to differences in the magnitude of values and due to the different number of observations of each dataset moreover such models may get different rankings at different time steps similar problems appear when the same models are used in different case studies thus an overall ranking based on many scenarios could be a great challenge if the models and the selected criteria are too many considering the aforementioned constraints a method that is able to provide an integrated ranking of many models using many statistical criteria for many simulation scenarios is missing additionally there is not an integrated method that can rank the representativeness of the criteria for a specific dataset scenario taking into account the aforementioned issues the aim of the study is to propose a new methodology based on a new index standardized ranking performance index srpi for ranking many models combining many criteria and many scenarios the proposed methodology also includes new procedures for reducing the redundant effects of criteria and a new procedure for ranking their representativeness through an additional index called rci ranking criteria index climatic data for evaluating 10 reference crop evapotranspiration formulas are used in this study in order to present the srpi methodology 2 methods and data 2 1 general methodology of standardized ranking performance index srpi various software packages have been developed for calculating a large number of statistical criteria some of the most popular ones are the hydrogof r package zambrano bigiarini 2017 the hydrotest software dawson et al 2007 2010 and others for the purposes of this study the hydrotest package was used that calculates a list of 26 criteria eqs s1 s26 in table s1 of the supplementary material 1 table s1 also provides an indicative description of their characteristics having already a large list of available criteria the next step is to select the most or to exclude the least appropriate ones depending on the modelling objectives and based on expert judgment the next step is to divide the selected criteria into two groups the type i and type ii groups the type i criteria provide v values that range between 0 and 1 or between and 1 where v equal to 1 denotes perfect fit of the model some of these criteria are the coefficient of determination rsqr the index of agreement d willmott 1981 the nash sutcliffe efficiency nse nash and sutcliffe 1970 the kling gupta efficiency kge gupta et al 2009 and others table s1 in the supplementary material 1 the criteria which provide values between and 1 get negative values when the model performance is so bad that the mean value of the observed data could be a better predictor than the model krause et al 2005 gupta et al 2009 if these type of criteria show negative values then they are also excluded from the analysis the type ii of criteria provide v values that range between and or between 0 and where v equal to 0 denotes perfect fit of the model some of these criteria are the mean error me the mean absolute error mae the root mean square error rmse and others see table s1 in the supplementary material in type ii group could also be included specific metrics which are based on a unique and countable feature of the data by using the abstraction between its observed and simulated value o s for example the slope of the flow duration curve carrillo et al 2011 is of great interest in hydrological studies and the abstraction between its observed and simulated value can be handled as a type ii metric the original v values of the selected criteria are transformed in two steps where x denotes the primary transformation and y denotes the secondary transformation of the v values for the criteria belonging to type i group the original value v k j of criterion k for the model j is transformed to x k j according to the following rule 1 x k j v k j when v k j 0 if v k j 0 then the k criterion is excluded for the criteria belonging to type ii group the original value v k j of criterion k for model j is transformed to x k j according to the following rule 2 x k j 1 v k j 1 v k max 1 where v k j is the absolute v k j value of criterion k for the model j and v k max is the maximum absolute v k j value of criterion k among all models the x k j values of eqs 1 and 2 range between 0 and 1 where 1 indicates perfect fit a basic difference between the type i and ii is that the x k j values of a criterion belonging to type i are independent from the v k j value of the worst performing model while they are not in the case of a criterion belonging to type ii this difference between type i and ii of criteria ceases to exist when the secondary transformation y is performed using the following equation 3 y k j x k j x k min x k max x k min where x k min and x k max are the minimum and maximum x k j values of the k criterion respectively among all models the x and finally y transformations succeed to standardize the initial v values of any criterion to the full range between 0 and 1 based on their relative distance where the best performing model gets always a y value equal to 1 y equal to 1 does not necessary indicate perfect fit and the worst performing model gets always a y value equal to 0 eq 3 makes all criteria regardless of their type to be dependent by the performance of the worst performing model if we assume that a total j number of models are evaluated based on a total k number of statistical criteria for a specific simulation scenario m then the non standardized ranking performance index rpi of each j model is estimated by the following formula 4 r p i j m 1 k k 1 k y k j m where rpi j m is the non standardized ranking performance index of the j model based on a final k number of statistical criteria for the simulation scenario m y k j m is the secondary transformation of the statistical criterion k of the model j for the simulation scenario m the rpi j m of each model is standardized according to the following formula 5 s r p i j m r p i j m r p i min m r p i max m r p i min m where srpi j m is the standardized ranking performance index of model j for the scenario m rpi j m is the value of rpi of model j for the scenario m while rpi max m and rpi min m are the respective maximum and minimum rpi j m values among all models for the scenario m eq 5 standardizes the values of rpi values of each model to the full range between 0 and 1 based on their relative distance where the best performing model gets always a srpi value equal to 1 and the worst performing one gets a srpi value equal to 0 the aforementioned procedures for a general application of srpi for a specific simulation scenario are summarized in the flow chart of fig 1 a if we assume that the same criteria have been used to evaluate the same models for other simulation scenarios then the overall ranking of models based on all scenarios can be performed by the following formula 6 r p i j total 1 m m 1 m s r p i j m where rpi j total is the total ranking performance index of the model j based on a total m number of simulation scenarios rpi j total is also standardized to srpi j total using the following formula 7 s r p i j total r p i j total r p i min total r p i max total r p i min total where srpi j total is the standardized ranking performance index of model j for all scenarios rpi j total is the value of rpi of model j for all simulation scenarios while rpi max total and rpi min total are the respective maximum and minimum rpi j total values among all models for all simulation scenarios the aforementioned procedure for a general application of srpi for multiple simulation scenarios is summarized in the flow chart of fig 1b the values of any individual criterion and the values of srpi either for one simulation scenario eq 5 or multiple scenarios eq 7 can be converted to ranking scores q using ordinal numbers i e q 1 2 3 j where q equal to 1 refers to the model with the best performance in this study the format of q ranking is based on a specific scheme which considers that when two models present the same value of an individual criterion or srpi then they get the same q ranking value while the next model gets a value equal to q 2 neglecting completely the ranking position q 1 such q rankings can easily be calculated with the rank function in excel see supplementary material 2 rank considering the aforementioned procedures the srpi methodology can combine any number of criteria and any number of simulation scenarios in order to provide an overall ranking of models performance on the other hand a large number of criteria may contain criteria that capture similar information of models behavior the redudant information increases non proportionally its effect in the final ranking of models and for this reason two cases for further selection and elaboration of criteria are proposed in the following sections in order to make srpi more robust 2 2 excluding redundant criteria two tools are proposed in this study that can be used in order to exclude criteria that capture redundant information the first one is the use of spearman correlations ρ among criteria when ρ correlations are performed between two criteria it is preferable to use the y values instead of v values in order to avoid sign confusion this option is also used in this study and thus all spearman ρ correlations among criteria are referred to their y values the second tool is the new term of percent of absolute ranking similarity ars which compares the q rankings of models provided by two criteria ars calculates the percentage ratio between the number of models that got the same q value in both criteria and the total number of models when two criteria c1 and c2 provide absolutely the same ranking of models then ars c1 c2 100 an example of ars calculation in excel is provided in the supplementary material 2 ars the tools of ρ and ars are used in this study according to the following rule case a of srpi analysis when the q values of two criteria show ars 100 and their y values show correlation ρ 0 95 then the one criterion that shows larger average ρ value with the rest criteria should be removed in this way even highly correlated criteria that do not provide absolutely the same ranking of models remain in the srpi analysis the procedure of case a is shown as a flow chart in fig 2 a the aforementioned rule aims to show a paradigm of ρ and ars combination with specific thresholds in other studies it is also valid to use solely either ρ or ars stricter thresholds for ρ or ars can also be used in order to significantly reduce the criteria used in srpi analysis another valid case is also to select from the beginning a small number of criteria that target to capture specific features and to use ρ and or ars in order to show that the selected criteria do not transfer redundant information in srpi 2 3 developing clusters of criteria apart from excluding criteria using the options given in section 2 2 e g case a there is an additional procedure for reducing the effect of redundant information based on clusters of criteria the criteria are subjected to cluster analysis with dendrograms using the q ranking values of each model i e the y values of criteria are converted in q values in order to create subgroups with criteria of high similarity the dendrogram starts with one cluster group in order to assess the maximum distance d max 1 then the clusters are increased step by step until the maximum observed distance in the c final total number of clusters to become d max c c d d max 1 where c d 1 the value of c d mainly depends on the number of models for example if we have to compare ten models using many criteria a c d 0 5 means that the criteria of each group cluster will carry maximum 50 of different information in this study the dendrograms were created using nearest neighbor with squared euclidean distance without standardizing the q values and the clusters were formed based on a c d 0 5 the maximum distances were derived from the agglomeration schedule tables cluster analysis was performed in statgraphics centurion xv statpoint inc if two or more criteria belong to the same cluster c then the average a c j of their y k j values for each j model is estimated the a c j values are not anymore in the full range 0 1 with 1 for the best and 0 for the worst performing model for this reason they should be readjusted again using the following formula 8a b y c j a c j a c min a c max a c min where a c j b 1 b y k j where a c j is the average of y k j values of criteria that belong to the same c cluster for the j model a c max and a c min are the maximum and minimum a c j values respectively among models for the cluster c of criteria after this procedure the rpi j m of each model for a specific simulation scenario m is calculated based on eq 4 considering the y c j values instead of the y k j values while the srpi j m is calculated based on eq 5 the inclusion of criteria clusters provides a balanced type of srpi that reduces even more the effect of redundant information the final y c j values of each cluster can also be used separately to highlight specific aspects of models ranking the clustering procedure for srpi analysis can be performed either directly on a large number of criteria or after following the proposed procedures of section 2 2 e g case a for criteria exclusion in this study the analysis of srpi is performed based on case a fig 2a but also using a second case case b where srpi is calculated based on case a followed by criteria clustering the procedure of case b is shown as a flow chart in fig 2b 2 4 evaluating statistical criteria based on srpi an alternative use of srpi could also be the identification of the most and the least representative criteria if we perform spearman ρ correlations between srpi and the y values of individual criteria the higher ρ may indicate that a specific criterion is the most representative among those used for building the srpi inversely the lower correlation can indicate the least representative criterion on the other hand a very high correlation does not necessarily mean that the specific criterion provides a high value of ars with srpi when their q values are compared thus the relationship between a criterion and the final srpi should be evaluated by both their correlation and ars considering this an additional index was formed ranking criteria index rci for ranking the criteria based on their relationship with srpi 9 r c i k a r s k s r p i ρ y k s r p i where rci k is the ranking index of the k criterion based on srpi ars k srpi is the absolute ranking similarity between the q values provided by the k criterion and the q values provided by srpi and ρ υ k srpi is the spearman ρ correlation between the y transformed values of the k criterion versus the respective values of srpi for the rci analysis the srpi values of case a should be used because it uses all the criteria equally while it also carries the information of the criteria that were removed during any procedure of criteria exclusion as these described in section 2 2 if more than one scenario is used to evaluate models performance then a respective overall evaluation of the criteria based on all scenarios can also be performed according to the following 10 r c i k t o t a l 1 m m 1 m r c i k m where m is the total number of scenarios the aforementioned procedure is extremely valuable since it indicates that srpi is not only able to rank the models combining many criteria but it is also able to rank the criteria based on their representativeness 2 5 1 1 plots many authors have suggested that the comparison of models performance should also be accompanied by visual inspection through graphical techniques such as the graphic comparison of observed o and simulated s time series the 1 1 scatter plots of o versus s values residual error plots frequency distributions etc biondi et al 2012 dawson et al 2012 the 1 1 scatter plots are usually selected since they are more easily interpretable for long data series providing an objective reference given by the 1 1 line of perfect fit biondi et al 2012 a linear regression based on the scatter points can also provide significant information about the distribution trends of the points along the 1 1 line of perfect fit the r 2 of the regression line is estimated by eq s27 when the regression is linear without forcing intercept to be 0 in this case the r 2 has the same value with the coefficient of determination rsqr eq s15 the calculation of r 2 for a linear regression with intercept forced to 0 is theoretically problematic and should be avoided eisenhauer 2003 another reason for not using linear regression with 0 intercept in 1 1 plots is that the regression line neglects information associated to the performance of a model in the low values for the datasets of this study all the comparisons between observed and simulated data were visualized through 1 1 plots using the y axis for the observed and the x axis for the simulated values as suggested by piñeiro et al 2008 a linear regression equation without forcing intercept to 0 on the scatter data of 1 1 plots was also added in order to show the distribution trends of the scattered points along the 1 1 line of perfect fit 2 6 data the dataset used for presenting the srpi methodology was developed based on climatic data from aristotle university farm 40 32 ν 22 59 ε in thessaloniki greece covering the 7 month warm growing period 1 april 31 october of the years 2008 2009 and 2010 data of seven months from three years these data were used by aschonitis et al 2012 2017a to estimate the reference crop evapotranspiration for short reference crop et o using the asce standardized formula asces eq a1 table a1 in the appendix this method is equivalent to fao 56 model for daily step calculations of short reference crop allen et al 1998 2005 this et o formula is considered the most precise and it is commonly used as a benchmark for comparing other simplified et o formulas walter et al 2001 allen et al 2005 the climatic data of november march were not used in this study due to the large number of days with extremely low et o values 0 3 mm d 1 for which asces validity may also be questionable the examination of srpi methodology for comparing et o methods is used in this case study for the following reasons evapotranspiration studies that propose new et o formulas usually perform comparisons with many others using many statistical criteria alexandris et al 2006 valiantzas 2013a heydari et al 2014 valipour 2015a b c 2017 a b aschonitis et al 2017a b bourletsikas et al 2017 there are many global pre calibrated et o formulas of reduced parameters e g original hargreaves samani and priestley taylor equations which are subjected to external validation based on local studies the selection and interpretation of some criteria e g rsqr aic bic etc for evaluating such models based on external validation presents differences in comparison to the case where the models are calibrated and evaluated using the same benchmark data et o can be analyzed based on various time steps e g hourly daily monthly thus the predictive accuracy of the same et o methods may be different when compared at different time steps because larger time steps e g monthly may counterbalance large daily et o variations in this case it would be useful for scientists to have the individual but also an overall ranking of et o methods considering all the possible time steps considering the above ten formulas models apart from asces table a1 eqs a2 a11 in the appendix were used to estimate et o at daily and monthly time steps based on the aforementioned climatic data for greek conditions 642 daily and 21 monthly et o values for each model their predictive accuracy was compared versus the respective daily and monthly benchmark values of asces method the specific ten et o models j 10 were selected because they have been already calibrated validated for global and applied for greek conditions in previous studies using different climate datasets thus the comparison of these models versus the benchmark data of asces constitutes a case study of external validation in this study the srpi analysis is performed using the daily and the monthly data denoted as daily and monthly scenario in order to investigate the performance of the models based on the two different time steps the srpi of the two time steps are further combined in order to rank the et o models based on both time steps of analysis m 2 in eq 6 the srpi analysis is performed twice based on case a and case b fig 2 respectively finally the procedure of 2 4 section is performed in order to assess the representativeness of the criteria based on rci analysis 3 results 3 1 selection of statistical criteria based on modelling objectives and 1 1 scatter plots the interpretation of statistical criteria when comparing et o models is generally less complex in comparison to other hydrological parameters e g runoff because et o transitions are smoother without showing extreme changes from the 26 criteria provided by hydrotest package eqs s1 s26 table s1 six of them pdiff nsc pep pi msde irmse were not calculated and not included in the srpi methodology for analyzing et o because they are more appropriate for hydrograph and flow analysis i e phenomena with steep transitions or analysis of sequential time series where random selection of data is not allowed the coefficient of determination rsqr and the aic and bic criteria were also not included in the srpi methodology because the et o models are not calibrated and they are directly subjected to external validation these criteria have a meaning only if the srpi analysis is performed during calibration of models using the same benchmark dataset the remaining 17 criteria ame mae me rmse r4ms4e rae mare mdape mre msre rve d nse msle ve kge and sse were all included in the next steps the modelling objectives were to find which et o model better fits the daily benchmark data of asces the monthly benchmark data of asces and both the objectives were kept more general in order to show the ability of the proposed methodology to handle ten et o models with seventeen criteria while the results of srpi were inversely used in order to extract information about the models and the aforementioned criteria for constructing more specific modelling objectives the 1 1 plots between the et o values of benchmark asces method eq a1 versus the respective values of the other ten et o methods eqs a2 a11 for the daily and the monthly time step are given in figs 3 and 4 respectively the rowwise statistics of the daily and monthly et o estimations of the eleven models are given in tables s2a and b respectively of the supplementary material 1 3 2 estimation transformation and relations between criteria the aforementioned seventeen criteria were divided in groups type i or ii and their original v values were calculated using the estimations of the ten et o models eqs a2 a11 versus the benchmark values of asces model eq a1 based on the daily and the monthly simulation scenario the v values for the daily and the monthly simulation scenario are given in tables s3a and b respectively of the supplementary material 1 the v values of criteria in tables s3a and b were transformed to x values using eqs 1 and 2 depending on the type and the results are given in tables s4a and b respectively tables s4a and b also include the x max and x min values of each criterion which are used in the secondary transformation to y values based on eq 3 the final transformation of the criteria to y values based on the daily and monthly scenario are given in tables s5a and b respectively the q ranking values of models for each individual criterion based on y values are given in tables s6a and b for the daily and monthly scenario respectively the y values of the criteria for each scenario tables s5a and b were subjected to multiple spearman ρ correlations and the results are given in tables s7a and b respectively the q ranking scores of individual criteria tables s6a and b were subjected to ars analysis and they are given in tables s8a and b for each simulation scenario taking into account the ρ and ars results of tables s7 and s8 respectively it was observed that there are many pairs of criteria that present high ρ values associated with very low ars for example in the case of daily et o data analysis the nse and mre criteria present ρ 0 83 p 0 01 table s7a and ars 20 table s8a this observation is extremely important since eight out of ten models are ranked differently by nse and mre despite their high ρ value the overall comparison of ρ and ars indicated that highly correlated criteria present significantly different rankings of models low ars values in both daily and monthly scenario this is verified by the non linear regressions made between ρ table s7 and ars table s8 values and the results are given in fig 5 a and b for each scenario fig 5 indicates that the use of ρ correlations or associated methods like pca for excluding completely some criteria method used in ipe method may lead to significant loss of information related to different aspects in the performance of the models fig 5 also shows that even for a rate of ars 50 between two criteria the ρ values do not fall below 0 9 in both daily and monthly scenario 3 3 calculation of srpi based on case a as indicated in section 2 2 the case a of analysis for excluding criteria considers the rule when the q values of two criteria show ars 100 and their y values show correlation ρ 0 95 then the one that shows larger average ρ value with the rest criteria should be removed such pairs of criteria are indicated with grey in table s7 following this rule five criteria from the daily and six criteria for the monthly scenario were excluded table 1 a the y values of the remaining criteria table 1a from the daily and monthly simulation scenarios were used to calculate the srpi values eq 5 of each et o model for each individual scenario the srpi values of the ten models for the daily and monthly scenario according to case a srpi daily a and srpi monthly a respectively are given in fig 6 the values of srpi daily a and srpi monthly a were further used to calculate the total srpi total a for both scenarios according to eq 7 fig 6 3 4 calculation of srpi based on case b taking into account the procedures of case b in section 2 3 the 12 and 11 remaining criteria of the daily and monthly simulation scenario table 1a respectively were subjected to cluster analysis with dendrograms based on their q values which are given in tables s6a and b of supplementary material 1 fig 7 a shows the initial dendrogram of criteria for the daily scenario where d max 1 56 the number of clusters was gradually increased in order to reach a c number of clusters until d max c 0 5 d max 1 at the end c 4 clusters were developed with d max 4 24 fig 7b clusters d1 d2 d3 and d4 indicated in table 1b similarly for the case of monthly scenario the initial dendrogram is given in fig 7c where d max 1 was found equal to 14 the number of clusters was again gradually increased until d max c 0 5 d max 1 at the end c 3 clusters were developed with d max 3 6 fig 7d clusters m1 m2 and m3 indicated in table 1b the y k j values of the criteria belonging to each cluster of the daily and monthly scenario were elaborated using eq 8a b in order to derive the respective y c j values table s9 in supplementary material 1 the y c j values were used to calculate the srpi values of the ten models for the daily and monthly scenario according to case b srpi daily b and srpi monthly b respectively fig 8 srpi daily b and srpi monthly b were further used to calculate the srpi total b fig 8 3 5 ranking of models the respective q rankings of the models taking into account the results of srpi daily a srpi monthly a and srpi total a for the case a fig 6 were derived and they are given in table 2 a similarly the respective q rankings of the models taking into account the results of srpi daily b srpi monthly b and srpi total b for the case b fig 8 were derived and they are given in table 2b the form of table 2 provides an overall aspect about the rankings of the models based on both cases a and b of srpi analysis the models that hold the best ranking positions q 1 in both case a and b of srpi analysis for the daily monthly and both simulation scenarios table 2a b are the hs a30s val1 copais and hs dral1 taking into account table 2 more specific modelling objectives can be satisfied as follows the val1 model showed the best performance for the monthly simulation scenario in both cases a and b q monthly a 1 and q monthly b 1 table 2a b thus if the modelling objective is to achieve higher accuracy only in monthly et o estimations then the optimum choice is the val1 model the hs a30s model showed the best performance for the daily simulation scenario of case a q dailyy a 1 in table 2a while the copais model showed the best performance for the daily simulation scenario of case b q dailyy b 1 in table 2b thus if the modelling objective is to achieve higher accuracy only in daily et o estimations then the first optimum choice is the copais models and the second one is the hs a30s since the ranking of case b is considered more robust compared to case a the hs a30s model showed the best combined performance for both daily and monthly simulation scenarios of case a q total a 1 in table 2a while the hs dral1 model showed the best combined performance for both daily and monthly simulation scenarios of case b q total b 1 in table 2b thus for a general application where high accuracy is required in both daily and monthly et o estimations then the first optimum choice is the hs dral1 model and the second one is the hs a30s since the rankings of case b are considered more robust compared to case a 3 6 calculation of rci and ranking of the criteria during the step of criteria exclusion of case a 17 criteria were initial considered while 12 and 11 were finally selected table 1a to participate in the srpi daily a and srpi monthly a respectively the basic information of the five and six criteria which were removed due to their high redundancy effects also exists in the final selected criteria of case a thus the final srpi daily a and srpi monthly a carry information from all the 17 criteria considering this element the ars and ρ between each criterion and srpi daily a for the daily scenario and between each criterion and srpi monthly a for the monthly scenario were estimated table s10 in supplementary material 1 based on the procedure of section 2 4 their values were finally used to estimate the rci eq 9 values of all 17 criteria for the daily rci daily and monthly rci monthly scenario table 3 rci daily and rci monthly were further combined according to eq 10 to derive rci total for both daily and monthly data analysis considering the results of table 3 the following observations were made in the case of daily scenario rci daily the three most representative criteria were the nse rmse and sse these criteria are based on s i o i 2 and focus on the overall error according to table s7a these three criteria are absolutely intercorrelated providing the same rankings of the models table s8a ars 100 thus selecting just one of them is enough for describing the performance of the models based on the overall error on the other hand the three least representative criteria were msle ame and kge msle uses the transformation of natural logarithm for data transformation giving more weight to low simulated values providing a suggestion about which model can perform better in the low values pushpalatha et al 2012 ame seeks for the larger s i o i which is very important when someone is looking for a model with low risk of high failure finally kge focuses on the overall error but it is based on an equal weighting of the three components of correlation bias and variability measures while nse rmse and sse do not gupta et al 2009 in the case of monthly scenario rci monthly the three most representative criteria were the ve mae and rae these criteria are based on s i o i and focus on the overall error according to table s7b these three criteria are absolutely intercorrelated providing the same rankings of the models table s8b ars 100 thus selecting just one of them is enough for describing the performance of the models based on the overall error on the other hand the three least representative criteria were r4ms4e ame and kge the role of ame and kge was described in the previous case r4ms4e gives more weight to high simulated values providing a suggestion about which model can perform better in the high values in the case of rci total the three most and the three least representative criteria are the same as in the case of rci monthly the aforementioned results are of high importance since they show that the most and the least representative criteria focus on very different aspects of models performance thus their combined use could provide a small number of criteria that are able to provide an integrated aspect about the performance of the models avoiding criteria overuse 4 discussion 4 1 other cases for reducing the redundant information the presentation of srpi methodology included two cases of analysis where in the case a all the criteria participated equally while in the case b the criteria were clustered in order to reduce further the redundant information another option could be to neglect completely the step of cluster analysis following a stricter rule for removing criteria directly in case a for example the use of the rule if ars 100 and ρ 0 95 between two criteria then the one is excluded can change by reducing the respective ars and ρ thresholds the ars vs ρ graph as in fig 5a and b can be used as a guide for choosing appropriate thresholds 4 2 the role of ars in this study special attention was given to the parameter ars as it was shown in fig 5 many pairs of criteria showed high ρ correlations with low similarity of models s rankings low ars values this observation was extremely important since in many studies the correlations among criteria or similar approaches e g orthogonality rules based on pca are the base for reducing their number domínguez et al 2011 dawson et al 2012 the importance of this parameter is proportional to the number of models i e the larger the number of models is the larger is the possibility to observe highly correlated criteria with low ars 4 3 effects of a poorly performing model on srpi another subject for discussion is the role of a poorly performing model in the srpi analysis as indicated by dawson et al 2012 the participation of a poorly performing model may reduce significantly the differences of the other models when its performance is used for standardizing the values of a criterion e g eqs 2 and 3 a poorly performing model can be easily identified since it will not have only srpi 0 but also y 0 for every individual criterion this model will take anyway the last position and can be removed from the comparison while srpi can be recalculated in order to provide more robust rankings for the rest models another option could also be the use of a naïve model dawson et al 2012 for standardizing the values of the criteria 4 4 the effects of high observed and simulated values in this case study the modelling objectives were set in a general context in order to present the srpi methodology without considering the basic element that the majority of statistical criteria is strongly affected by the higher observed and simulated values de vos and rientjes 2007 pushpalatha et al 2012 hydrological phenomena present high seasonality and the low values of observed hydrological parameters are also integral components of the hydrologic regime while the demand for good forecasts for the low intensity periods is rising due to the increased competition between water uses pushpalatha et al 2012 apart from the analysis and simulation of hydrological phenomena which usually focus on high intensity periods there are also other simulated environmental parameters e g crop yields simulation using crop models aschonitis et al 2013 where the smaller values of the investigated parameter e g yield are more important for the development of management plans and decision support systems proposed solutions for reducing the specific problem are a the separate analysis of low and high intensity periods and b data transformation types such as box cox square root logarithmic etc that increase the weight effect of low intensity values on the criteria chiew et al 1993 smakhtin et al 1998 oudin et al 2006 willems 2009 pushpalatha et al 2012 before the use of criteria in srpi analysis 4 5 are all criteria equally significant all the participating criteria in the analysis of srpi for the case a were considered equally significant similarly all the participating clusters of criteria in the analysis of srpi for the case b were also considered equally significant someone may argue about the use of equal significance if the modelling objective targets to specific elements if this is the case the srpi users should consider the ranking of models based on individual criteria that are a priori designed to capture such specific elements or to use a weight factor that will increase their effect in the rpi formula eq 4 4 6 the role of aic bic and other similar metrics the aic akaike 1974 and bic schwarz 1978 criteria that can be provided by hydrotest were not used in this study because the et o models were already calibrated in other studies based on different datasets these criteria are of high importance since their objective can be similar to srpi concept a means of statistical model selection based on the relative quality of models for a given set of data the two main differences between srpi and aic or bic criteria are a that srpi can be formed using different criteria depending on the modelling objectives and b aic and bic take into account the number of calibrated coefficients of a given model while srpi does not the second element is extremely important since statistical models with fewer coefficients are more stable with less risk of overfitting there are also other similar criteria to aic and bic such as the aicc hurvich and tsai 1989 and kic kashyap 1982 these type of criteria aic bic aicc kic present differences and limitations which should also be taken into account before their use burnham and anderson 2002 2004 ye et al 2008 in the case where the comparison is performed between statistical models calibrated with the same data the aic and bic criteria or other similar ones can be used in parallel with srpi providing a a different aspect of ranking considering the number of calibrated coefficients of the models and b a method for validating the srpi and vice versa when the compared models have the same number of calibrated coefficients the incorporation of aic bic or similar criteria in srpi could be an additional option but these criteria do not belong in group type i or in type ii since they provide a range of v values between and where the best model is the one with the smaller v for example if three models give the following aic values 20 0 10 then the best model is the one with aic 20 what matters in this type of criteria is the distance between their values burnham and anderson 2002 for this reason if negative values of these criteria exist then a constant should be added in order to make them all non negative with the best model having 0 value if negative values do not exist then the values remain the same the non negative values of these criteria are transformed to x and y values using eqs 2 and 3 before their use in srpi 4 7 sensitivity of statistical criteria another element that could be considered in srpi methodology is the sensitivity of the criteria in many cases the length of time series for comparing models may be a subject of criticism in such cases the analysis of sensitivity of statistical criteria may be an intermediate step in the selection procedure methods like bootstrapping random selection and replacement of data ritter and muñoz carpena 2013 generation of synthetic data series using bootstrap like sampling methods berthet et al 2010 in combination with error models yang et al 2007 can be used as means to evaluate the robustness of the criteria the exclusion of criteria with extremely large variations would increase the robustness of srpi special attention should be given to some criteria such as pdiff and pep that target on the analysis of specific and possibly rare elements of the time series extreme peaks or criteria such as pi irmse msde and nsc where random selection of data is not allowed 4 8 selecting criteria for individual use based on srpi rci the srpi index can be formed based on different criteria and it is proposed as a tool to rank models and not as a new statistical criterion since it is almost impossible to interpret its results without investigating the individual criteria that participate in its formula for this reason individual criteria are always needed in order to describe models specificities the problem in this case is how to select a small number of criteria that will be highly informative as indicated in section 3 6 srpi through rci provided a ranking of the representativeness of the criteria this can be an alternative use of srpi and in some cases more important than ranking the models especially when the raking of models cannot be interpreted easily this is a very crucial element since the srpi rci analysis can facilitate the users to justify the selection of a small number of statistical criteria and to interpret the models performance only with these neglecting completely the ranking of models that was produced by srpi considering the results of 3 6 section about the analysis of et o models it was shown that the combined use of the most and the least representative criteria could provide an integrated aspect about the performance of the models 5 conclusions a new system for ranking many models based on many criteria and multiple scenarios was proposed in this study the system is based on the new standardized ranking performance index srpi that describes the relative distance between the models in the full range 0 1 where the best performing model has srpi 1 and the worst performing model has srpi 0 the combination of srpi analysis based on both cases case a equally weighted criteria case b clusters of criteria for the 10 et o models and for two scenarios daily and monthly showed its ability to provide different optimum model choices depending on the modelling objectives on the other hand its alternative use through rci for comparing criteria was extremely valuable since it led to very important conclusions about how to choose few but very informative criteria for describing models performance future studies could focus on a the evaluation of the srpi methodology based on other types of models which simulate parameters with more steep behavior compared to et o e g runoff b the use of statistical models calibrated based on the same benchmark data in order to include in srpi other metrics such as aic and bic which were excluded in this study due to the application of external validation and c the use of naïve models instead of the worst model for standardizing the y and srpi formulas appendix a supplementary data the following are the supplementary data to this article the supplementary material contains a word file supplementary 1 doc with the tables s1 s2 s3 s4 s5 s6 s7 s8 s9 and s10 and an excel file supplementary 2 xls that shows an example for calculating the q ranking values and an example for calculating the ars between two criteria multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 01 005 appendix table a1 reference crop evapotranspiration models table a1 modelabbreviation citation formula climate data requirements indexing asces allen et al 2005 e t o 0 408 δ r n g 900 γ u 2 e s e a t mean 273 16 δ γ 1 0 34 u 2 t mean t max t min r s rh u 2 eq a1 hs a30s hargreaves and samani 1982 1985 aschonitis et al 2017b e t o c 1 0 408 τ m e a n 17 8 r a t d 0 5 where c 1 0 0023 t mean t max t min eq a2 hs a0 5d hargreaves and samani 1982 1985 aschonitis et al 2017b e t o c 2 0 408 τ m e a n 17 8 r a t d 0 5 where c 2 0 0021 t mean t max t min eq a3 hs dral1 droogers and allen 2002 e t o 0 00102 r a τ m e a n 16 8 t d 0 5 t mean t max t min eq a4 hs dral2 droogers and allen 2002 e t o 0 0005304 r a τ m e a n 17 0 t d 0 0123 p c p 0 76 t mean t max t min pcp eq a5 pt a30s priestley and taylor 1972 aschonitis et al 2017b e t o a 1 0 408 δ δ γ r n g where a 1 1 36 t mean t max t min r s eq a6 pt a0 5d priestley and taylor 1972 aschonitis et al 2017b e t o a 2 0 408 δ δ γ r n g where a 2 1 31 t mean t max t min r s eq a7 copais alexandris et al 2006 e t o 0 057 0 227 c 2 0 643 c 1 0 0124 c 1 c 2 where c 1 0 6416 0 00784 r h 0 372 r s 0 00264 r s r h c 2 0 0033 0 00812 t m e a n 0 101 r s 0 00584 r s t m e a n t mean r s rh eq a8 val1 valiantzas 2013b 2014 e t o 0 0393 r s t m e a n 9 5 0 19 r s 0 6 φ 0 15 0 0061 t m e a n 20 1 12 t m e a n t min 2 0 7 t mean t min r s eq a9 val2 valiantzas 2013b 2014 e t o 0 0393 r s t m e a n 9 5 0 19 r s 0 6 φ 0 15 0 078 t m e a n 20 1 r h 100 t mean r s rh eq a10 val3 valiantzas 2013c e t o 0 0393 r s t m e a n 9 5 2 4 r s r a 2 c u t m e a n 20 1 r h 100 where cu 0 054 for rh 65 and cu 0 083 for rh 65 t mean r s rh eq a11 t mean max min mean maximum and minimum temperature oc td difference between maximum and minimum temperature oc r s incident solar radiation mj m 2 d 1 r a extraterrestrial solar radiation mj m 2 d 1 rh relative humidity δ slope of the saturation vapor pressure temperature curve kpa oc 1 γ psychrometric constant kpa oc 1 g the soil heat flux density at the soil surface mj m 2 d 1 e s the saturation vapor pressure kpa e a the actual vapor pressure kpa φ absolute value of latitude rads pcp precipitation mm month 1 the values c 1 c 2 and a 1 a 2 are locally calibrated coefficients of the original hargreaves and samani 1985 and priestley and taylor 1972 formulas respectively for the position of the station based on global climatic data of the period 1950 2000 these values were obtained from raster data of the respective annual coefficients provided by aschonitis et al 2017b based on 30 arc sec resolution maps case of c 1 and a 1 and 0 5 resolution maps case of c 2 and a 2 for the case of eq a2 the c 1 0 0023 was found identical to the original coefficient of hargreaves and samani 1985 formula the complete global raster data are given in the following permanent data repositories https doi pangaea de 10 1594 pangaea 868808 http www esrn database org 
26253,floods introduced by rainfall events are responsible for tremendous economic and property losses it is important to delineate flood prone areas to minimize flood damages and make mitigation plans recently there has been recognition of the need to understand the risk from groundwater flooding caused by the emergence of groundwater at the ground surface mapping groundwater flood risk is more challenging compared to river flooding especially in karst systems where subsurface preferential flowpaths can affect groundwater flow we developed a coupled surface subsurface modelling framework resolving spatial information and hydrologic data to assess the groundwater flood risk at a karstic watershed koiliaris river basin greece the simulated groundwater table was used to delineate groundwater flooding modelled results revealed the role of faults in groundwater flooding generation we anticipate the coupled surface subsurface approach to be a starting point for more sophisticated flooding risk assessment including magnitude and temporal duration of groundwater flooding keywords groundwater flooding coupled surface subsurface model pihm risk assessment critical zone observatory koiliaris river basin 1 introduction flooding is one of the most frequently occurring natural disasters that cause significant economic damages and affect people s life around the world jonkman et al 2008 unisdr 2015 kundzewicz et al 2014 huntingford et al 2014 further more it has now been widely noticed that the risk of life and economic losses caused by flood may increase due to social development land subsidence and climate change arnell and gosling 2016 hirabayashi et al 2013 jongman et al 2015 ward et al 2017 archfield et al 2016 it is urgent to delineate flood prone areas under changing environment and to reduce or prevent the detrimental effects of flood waters to battle the problem of delineating flood prone areas most of the research focus on the study of parameters of surface river flooding fig 1 yamazaki et al 2011 li and duffy 2011 in our area of interest vozinaki et al 2015 and kourgialas and karatzas 2013 estimated crop loss due to overland flooding by considering several parameters like flood depth flow velocity and vegetation types costabile and macchione 2015 showed that implementing high resolution topographic data and man made structures could significantly improve overbank flows these studies yield important insights into adequate design and development of flood management strategies for river management however the previous approach neglect a significant factor groundwater flood which can be critical in flood management especially in karstic watersheds finch et al 2004 macdonald et al 2008 groundwater flooding fig 1 happens when groundwater tables rise above the land surface away from perennial river channels or in the floodplain before the river goes out of bank finch et al 2004 macdonald et al 2008 groundwater flooding receives less attention compared to river flooding because it happens less often with less severity e g ascott et al 2017 morris et al 2018 however the management practices to prevent groundwater flooding are limited compared to river flooding which can be reduced by building engineering structures embankments walls dams etc once it happens groundwater flooding will take time to dissipate because groundwater moves much more slowly than surface water pinault et al 2005 such long lasting groundwater flood can cause not only significant damage to infrastructure abboud et al 2017 but also more severe river flooding huntingford et al 2014 mapping groundwater flood prone area is more challenging compared to river flooding due to many controlling factors and spatial uncertainties throughout the whole watershed including topography land use geology etc kourgialas and karatzas 2014 integrated 6 factors flow accumulation slope land use rainfall intensity geology and elevation to map flood hazard across the koiliaris river basin krb greece the static gis information of vegetation topography and soil can provide valuable information for locations with potential flooding risk for example the vegetation cover is considerably affecting the response of the water infiltrating depth in the soil rossi et al 2018 furthermore soil type like e g silt loam can be water repellent and delay water infiltration li et al 2018 thus temporal dynamics of groundwater flooding is still unclear which requires continuous hydrologic modelling of groundwater table elevation groundwater flooding is rarely modelled spatially throughout a watershed especially in areas with complex geological karst setting like this in krb groundwater flooding is prone to be developed due to the combination of low storage channelized preferential flow and rapid lateral inflow from steep uphill area naughton et al 2012 upton jackson 2011 butscher and huggenberger 2007 used a conceptual karst model to approximate subsurface flow for hydrologic simulation and found that 32 of the springs are discharging from an uphill area with slopes greater than 20 nikolaidis et al 2013 coupled a distributed hydrologic model and a conceptual karst model for integrated water management at krb explicit representation of karsts in a hydrologic model will be helpful to examine hydrologic links between karst and the rest part of the watershed e g shoemaker et al 2008 reimann et al 2011 rugel et al 2016 improve understanding of groundwater flooding and achieve sophisticated water management in this study we examine the spatial and temporal response of groundwater conditions of a karst watershed to assess the risks of groundwater flooding our first objective is to test the ability of a coupled surface subsurface hydrologic model to capture groundwater flooding in the model the spatial heterogeneity of the watersheds is explicitly resolved in the watershed responses which includes vegetation geology and faults the second objective is to understand how faults development would affect the groundwater flooding processes the methodological steps of the study are 1 application of a coupled surface subsurface spatially distributed hydrologic model to delineate spatial distribution and temporal dynamics of groundwater flooding 2 examination of the sensitivity of fault conductivity on flood risk and 3 calculate agricultural loses based on groundwater flooding area and land use the novelty of this work is to understand groundwater flooding through spatial and temporal dynamics of groundwater table simulated by coupled surface subsurface models we anticipate this work to be a starting point for more sophisticated flooding risk assessment the incorporation of geological climate and soil data into an affordable computation load could explain spatial vulnerability assessment of magnitude and temporal duration of groundwater flooding 2 data and methods 2 1 koiliaris river basin the koiliaris river basin krb is a critical zone observatory menon et al 2014 that represents severely degraded soils due to heavy agricultural impact over many centuries the 130 km2 watershed is located 15 km east of chania greece fig 2 a chania has a population of 53 910 and is the second largest city of crete the elevations of krb range from 0 to 2041 m above mean sea level and the slopes range from 0 to 62 climate in the watershed is mediterranean and characterized by a separation of seasons the average rainfall is 652 mm of which 20 in the dry season with an average temperature of 24 celsius while in the wet season 80 of the average rainfall occurs with an average temperature of 13 celsius stamati et al 2013 the land crops include citrus trees and olive trees which are cultivated throughout the year while vegetables and wheat are cultivated continuously in alterations appropriate for each period moraetis et al 2015 we used hourly climate data including precipitation temperature relative humidity wind velocity solar radiation and vapour pressure from three meteorological stations i e psychro pigadi samonas and kalyves fig 2b four soil types fig 2c covers the krb with the majority is eutric leptosols 56 and calcaric regosols 24 leptosol and regosol soils are expecting to show some differentiation in hydrologic properties such as those mentioned by kirchen et al 2017 with leptosols water holding capacity to be the lower the watershed consists of 13 land cover types fig 2d based on the corine land cover types bossard et al 2000 and 28 of the watershed is covered by natural grassland pink colour in fig 2d while 23 of the total area is covered by sclerophyllous vegetation yellow colour in fig 2d 2 2 model description a coupled surface subsurface hydrologic model penn state integrated hydrologic model pihm was selected to perform hydrologic simulations of streamflow and groundwater dynamics pihm kumar 2009 qu and duffy 2007 simulates the hydrological cycle including interception throughfall infiltration recharge evapotranspiration overland flow unsaturated soil water groundwater flow and channel routing in a fully coupled scheme evapotranspiration et is calculated using the penman monteith approach adapted from noah lsm chen and dudhia 2001 overland flow is described in 2 d estimated of st venant equations pihm uses diffusive wave approximation for overland flow and streamflow the model conceptualizes each subsurface column into two parts unsaturated and saturated layers water balance equation based on hydraulic head which is capable of representing the state in both unsaturated and saturated zones while only considering vertical moisture movement in the unsaturated zone the model simulates 2 d lateral groundwater flow in unconfined aquifer based on the dupuit approximation without addressing the karstic conduits network and the diffuse flow system in deep e g butscher and huggenberger 2007 interception and snowmelt are obtained using a bucket model and a temperature index model respectively horizontally the modelling domain is decomposed into delaunay triangles the governing equation for each process is discretized on the triangles from canopy to bedrock forming partial differential equations pdes pihm uses a semi discrete finite volume formulation for solving the set of the coupled pdes resulting in a system of ordinary differential equations odes representing all processes of each triangle the ode system is solved using the cvode implicit solver cohen and hindmarsh 1996 detailed descriptions of the modelling theory and mathematical formulation can be found at the pihm website http www pihm psu edu and associated publication kumar 2009 qu 2005 qu and duffy 2007 in this study we applied the coupled surface subsurface hydrologic modelling method to explicitly simulate groundwater table elevation throughout krb for groundwater flood assessment there are several advantages for pihm modelling for this study 1 the easy setup for the physical properties of the surface and subsurface layers including faults by pihmgis bhatt et al 2014 2 the model captures fully coupled dynamics of surface water and groundwater interactions e g yu et al 2015a zhang et al 2018 zhang et al 2019 hence allowing spatial and temporal delineation of floods and 3 geologic faults with substantial impacts on groundwater flow can be explicitly represented in the model as large scale preferential flow through high permeable regions 2 3 model parameterization calibration pihmgis bhatt et al 2014 was used to parameterize the model domain using the aforementioned datasets i e dem meteorological soil and land cover data the krb and the extended karstic area were discretized into 914 triangles and the river was discretized to 92 linear segments fig s1 the extended karstic area has been assigned by others as contributing area in the water budget of our study area kourgialas et al 2010 we estimated subsurface hydrologic parameters using texture data and pedotransfer functions wösten et al 1999 the pedotransfer functions used are generally applied for the different soil textures within europe wösten et al 1999 and they are not site specific therefore it is suitable for a regional study such as the present work the distribution of major faults was implemented according to the description in tataris and christodoulou 1969 an analysis of the tectonic phases in crete has revealed different phases of deformation which have definitely influence the faults hydraulic properties for example the mega and meso folds with fold axis w e and ns which were created during the beginning of the orogenesis in crete perpendicular to these folds dip slip w e and ns striking faults were created during the extensional phase in crete fassoulas et al 2004 the previous faults considered the oldest generation of faults which led to intense karstification with vast and deep sinkholes and shafts like gourgoutakas and liontari caves moraetis et al 2010 adamopoulos 2013 these w e and ns are of high hydraulic connectivity compare to the other fault zone of nw se striking direction which are much younger with less karstification and extensive preservation of faults slickenlines fassoulas 2001 caputo et al 2010 both these generations of faults exist in koiliaris hydraulic conductivity is simulated as being 10 times in the oldest faults w e and ns and 2 times in the youngest faults nw se fig 2c the hydraulic conductivity is selected higher in the older faults since the intensely karstified areas are along the w e faults kartsic features especially sinkholes have been observed also in other areas along extensional faults like in our case prasad et al 2017 these karstified areas are largely affecting the water flow and the hydraulic conductivities along the faults the hydraulic conductivity can change from 10 7 10 6 m s in massive limestones kovács 2003 to 10 5 in the epikarst hartmann et al 2012 and to 10 3 m s in karstified faults polomčić et al 2013 finally the past speleological expeditions in the deep sinkholes in our study area shows chaotic vertical shafts from masl meters above sea level of 1500 m down to 300 masl the karstic shafts correspond to what moraetis et al 2010 and nikolaidis et al 2013 presented as fast responding karstic system or upper reservoir the land cover parameters were obtained from monthly gldas vegetation parameters rodell et al 2004 and mapped according to table s1 the depth of groundwater flow was estimated based on the depth of the groundwater in the low land areas approximately 20 m below the land surface the depth is consistent with the depth of the epikarst around the world jones 2013 the value was used uniformly throughout the watershed our simulations were performed for 3 years july 2007 july 2010 when flooding was severe fig s2 using pihm v2 2 2015 initial condition was achieved using spinup of two previous years july 2005 june 2007 initialized groundwater tables were below the land surface and above bedrock 20 m below the land surface observed daily streamflow data at stylos spring fig 2b and modis remotely sensed et dataset mod16 running et al 2017 were used for model calibration according to the hydrologic characteristics of parameter we calibrated two groups separately event and seasonal scale groups table 1 yu et al 2013 the event scale parameters were optimized through an evolutionary algorithm yu et al 2013 we selected streamflow observation data from feb 25 2009 to march 4 2009 in which highest streamflow happened for event scale parameter calibration the seasonal scale parameters were tuned based on evapotranspiration ornl daac 2017 using observations from july 1 2008 to june 30 2009 the rest of the simulation was used for validation we used the relative error e pearson product moment correlation coefficient r and nash sutcliffe coefficient of efficiency nse nash and sutcliffe 1970 to evaluate the model performances table 2 streamflow calibration results are shown in fig 3 a the dynamics of streamflow variation between observed and modelled results were in reasonable agreement and are considered acceptable similar modelled and observed runoff ratios of 0 60 and 0 62 respectively also indicated reliable partitioning of the water budget furthermore 8 day average et ornl daac 2017 of this watershed was in good agreement with modis data fig 3b 2 4 delineation of groundwater flooding map since the focus of this paper is on the dynamics of groundwater response further confidence in the model result in this context was built by evaluating the ability of the model to simulate the historic flooding locations we extracted daily groundwater table depths of each finite volume i e triangle of the modelling mesh from the calibrated pihm simulation results groundwater flooding locations and areas were delineated by analysing the temporal dynamics of pihm simulated groundwater tables according to eq 2 1 g r o u n d w a t e r f l o o d v u l n e r a b i l i t y h i g h m a x i m u m g w 0 m o d e r a t e s k e w n e s s r g w 0 l o w s k e w n e s s r g w 0 2 r g w g w m i n i m u m g w m a x i m u m g w m i n i m u m g w groundwater flooding may happen under two conditions 1 groundwater table rises above land surface during the simulation period and 2 groundwater table is shallow during the wet season and the area can be potentially flooded during rainfall events we used maximum groundwater table elevation maximum gw to quantify the first situation where groundwater table height was above surface these places are of high groundwater flooding risk the second situation was examined by the frequency of groundwater getting close to the surface we rescaled the groundwater table elevation to 0 to 1 as relative groundwater table elevation rgw eq 2 and then analysed the temporal dynamics using the skewness of the rescaled groundwater table elevations skewness rgw fig 4 if the skewness is less than 0 it means the groundwater table is often close to the land surface which can potentially result in groundwater flooding i e moderate groundwater flooding risk if the skewness is positive the groundwater is often far below the land surface hence less likely to cause groundwater flooding i e low groundwater flooding risk time lag between river flooding peak and groundwater table height peak was used to identify the temporal dynamics of groundwater flooding we selected 8 events during the 3 years in each event the time of peak streamflow and peak groundwater table height was analysed we varied the fault hydraulic conductivity for sensitivity analysis to assess the impacts of faults on groundwater flooding since the fault hydraulic conductivity can vary significantly based on field experiences the hydraulic conductivity of faults was changed at different magnitudes to simulate karst development scenarios we analysed both spatial locations of groundwater flooding and the time delays of groundwater flooding 2 5 cost estimation of agricultural losses due to groundwater flooding the estimation of groundwater flooding cost was based on the calculations demonstrated by vozinaki et al 2015 the designated area of high risk was ascribed with the land uses data from corine database european environment agency 2004 the intersection of land cover and high risk flooding areas produced the percentage of different land uses in the high risk flooding areas the agricultural flood damage was calculated by the total area of each crop type under high flooding risk the weight yield per area and the cost per unit weight of yield we retrieved the weight yield per area and the cost per unit weight of yield from vozinaki et al 2015 in the previous study the depth and flow velocity of the flood were mostly 0 1 2 m and 0 0 3 m s respectively thus our estimations are based on these calculations and we adopted the flood damage cost standard deviation given therein 3 results 3 1 spatial distribution of groundwater flooding groundwater flooding map was created as three types of risk high moderate and low with areas of 100 21 and 8 7 km2 respectively fig 5 the spatial distribution of groundwater flooding was located around low elevation regions and faults this was probably because groundwater recharge from sinkholes in mountainous area could quickly discharge through highly fractured faults eleven of the 30 historic flooded sites were captured by high flood risk and 14 historic flooded sites were captured by moderate flood risk the rest 5 historic sites not captured by high or moderate risks were located next to the riverbanks previous studies vozinaki et al 2015 kourgialas and karatzas 2014 used high resolution modelling of riverine areas to capture flooding locations since our model grid is an unstructured mesh it will be easy to increase spatial resolution around the river 3 2 temporal dynamics of groundwater flooding groundwater flooding was generally less abrupt than river flooding in all of the 9 selected rainfall runoff events fig 6 table 3 there was a significant delay of the peak between groundwater flooding and river flooding indicated by model simulation the average time lag fig 6b varied from 4 4 to 14 2 h with a mean of 8 9 h time lags between groundwater flooding and river flooding i e peak flow in the river showed spatial heterogeneity the time lags were generally small around the faults and large in the flood plain i e the northeast part of krb this was due to the high hydraulic conductivity of the faults when the precipitation event lasted more than one day the peaks of groundwater flooding delayed longer and showed diverse spatial patterns events 2 3 4 and 6 fig 7 apart from the time lags in the emergence of the event there were also spatio temporal changes in the decay of the phenomena figs 6 and 7 such information is critical to flood mitigation 3 3 sensitivity to hydraulic conductivity of faults increase fault hydraulic conductivity created wider areas of groundwater flooding fig 8 the delineated groundwater flooding areas were 20 3 29 4 and 32 0 km2 for low medium and high k of faults the high flooding risk areas were 8 0 8 7 and 9 2 km2 for low medium and high k of faults the moderate flooding risk areas were 12 0 20 7 and 22 8 km2 for low medium and high k of faults increase fault hydraulic conductivity reduced responding time of groundwater flooding the average time lags between groundwater flooding and river flooding were 10 4 8 9 and 6 7 h for low medium and high k of faults fig 9 3 4 damage cost calculation in our study the total area of high flooding risk is 7 of the total watershed and 23 of the cultivated area the land uses distribution in the high risk areas is given in table 4 the olive grooves have the highest coverage in the high risk areas 39 while agricultural land has the second highest coverage 24 table 4 the highest cost of damage derives from olive groves and the second comes from fruit trees table 4 the total damage cost is 2 1 106 the calculation of the cost followed the methodology of vozinaki et al 2015 where the synthetic damage curves were used even the number of data was not large in the previous study it was considered representative for our study area since a questionnaire survey had be conducted specifically for koiliaris watershed 4 discussion 4 1 data interpretation of coupled surface subsurface modelling the advantage of coupled surface subsurface modelling is to explicitly simulate the head above ground our simulation is distributed throughout the watershed and processes of infiltration and discharge are dynamically considered through coupled surface surface equations therefore identification of groundwater flooding according to modelling results is relatively easy groundwater models e g modflow are often used to identify location of groundwater spring e g reed et al 2016 gallegos et al 2013 similarly groundwater flooding can be simulated through groundwater flow modelling to avoid realistic interactions between surface water and groundwater however such strategy may have limitation around riverbank we note that drainage pack harbaugh 2005 of modflow can be a potential solution for groundwater flooding it will helpful to compare our results with groundwater modelling results the purpose of coupled surface subsurface modelling is to improve understanding on hydrologic processes rather than runoff prediction efficiency previous watershed modelling studies obtained higher nse of runoff e g 0 91 and 0 87 in kourgialas et al 2010 0 62 in nikolaidis et al 2013 ideally coupled surface subsurface modelling has more parameters and gives better fit practically the parameters are bounded by physical meanings and the calibration of coupled surface subsurface models is usually difficult yu et al 2013 in addition calibration to different types of data may result poorer fit as the parameters try to serve different dynamics that dominate different data sets both uncertainties in spatial parameters and temporal forcing can consistently create model discrepancy at the stream gauge it is acceptable that our simulated runoff was degraded comparing to previous modelling studies notably the application of coupled surface subsurface hydrologic models is not only on streamflow dynamics chen et al 2015 seo et al 2016 zhang et al 2017 but also on spatial and temporal dynamics of soil moisture shi et al 2015 groundwater table elevation yu et al 2016 snow accumulation kumar et al 2013 yu et al 2015b which can create continuous spatial products of hydrologic and ecologic variables yu et al 2015a condon et al 2015 shi et al 2018 interpretation of coupled surface subsurface modelling results requires combination of localized information and targeted problem coupled surface subsurface models usually simulate many hydrologic fluxes and state variables across spatial and temporal scales these simulated results are usually validated only by streamflow or groundwater table elevation at limited locations which leaves large uncertainties in spatial distribution of groundwater table yu et al 2016 and soil moisture shi et al 2015 without calibration on local groundwater table we showed that relative dynamics of groundwater table could be reliable in terms of spatial information when absolute groundwater table height showed deviated pattern this is probably due our model confidence in et performance further calibration on groundwater table and assessment of uncertainties are needed to refine the spatial information on flood risk 4 2 implications for flood management in this basin identification of faults is critical for prediction of groundwater flooding macropore has been found to contribute groundwater flooding due to accumulated wetness over several years pinault et al 2005 our results showed that distribution of faults could potentially increase groundwater flood risk fault development was not incorporated in vast majority of hydrology models at krb kourgialas et al 2008 and nikolaidis et al 2013 have introduced fault data however these faults were incorporated to support the delineation of extended watershed area for the karstic aquifer and forcing the routing of water from the extended karst to the springs inside the watershed our study suggested that including faults in a physically based 3d hydrologic model automatically the extended karst is delineated and thus the model provides additional information for flood management faults of high hydraulic conductivity are vulnerable to abrupt groundwater flooding when the rain intensity exceeds the infiltration rate of the karst gutiérrez et al 2014 the groundwater table can rise quickly due to high connectivity to surface water the previous explains probably also the observed time lag of groundwater flooding and river flooding the higher lag in the rain events where precipitation is distributed in more days could be possibly related to more effective infiltration from the karst on the other hand more intense events lower the infiltration capacity and increase the abrupt fault response not to neglect also the effect of the spatial distribution of the rain event close or not in the major faults area which can affect the lag between the groundwater flooding and the river flooding faults of low hydraulic conductivity are vulnerable to slowly developed groundwater flooding groundwater storage can be accumulated during the whole wet seasons years before groundwater discharge happens pinault et al 2005 this type of groundwater flooding may last long time and difficult to mitigate flooding risks associated with faults behaviours in the groundwater flooding events may impose significant costs in the agriculture sector for example the flood risk designation for krb from vozinaki et al 2015 revealed areas of high flooding risk around the river with an estimation of 0 2 106 damages for an inundated area of 0 4 km2 one order of magnitude smaller compared to 2 1 106 in this study for the whole river basin the percentage of the study area in the previous work is restricted to river over bank flooding and correspond only in the 0 003 of the total watershed and 0 02 of the agricultural land which is very low compare to our estimations of the affected areas 7 and 24 respectively the reduction of the affected area is due to the fact that the previous study considered the flooding caused by koiliaris river a 5 km stretch from the springs to the sea located in the alluvial plain without considering groundwater overland flow the cost of the flood damage in our case is expanding in million of euros compare to the hundred thousand obtained by vozinaki et al 2015 despite we have not considered the depth and the velocity of the flood in our cost estimation we believe that the cost will be maintained in the same magnitude other critical factors which are lately gaining large attention is the flood frequency and the flood duration ward et al 2017 it has been stressed that el niño southern oscillation enso is affecting the flood duration more than flood frequency ward et al 2017 however the recent studies in crete showed that climate becomes drier with higher frequency of extreme events tsanis et al 2011 the same work shows an increase of 35 of events with high precipitation which a clear indication that a minimum 35 of damages is likely to occur the flood frequency definitely will increase the damages both in economy and mortality hu et al 2018 the groundwater modelling of the present study is significant in the understanding of the soil degradation in karstic mountainous areas it has been presented from other studies that the karstic groundwater is conveying dissolved pollutants during flood events like e g nitrates from mountainous areas toward lowland areas at koiliaris watershed nikolaidis et al 2013 moraetis et al 2010 the previous impact is related to livestock grazing in the higher land where grasses are growing during summer the identification of groundwater flood areas will designate the areas accepting probably low quality water on the other hand the quality analysis of the groundwater flood water will offer the opportunity to understand the soil degradation reflection in the karst water 4 3 sources of uncertainty model uncertainty was created by the simplified vertical representation of geology the geology was represented by homogeneous texture from ground surface to bedrock in each triangle such simplification may likely results in high spikes in simulated streamflow consequently the groundwater flooding damage may be over predicted also we used 20 m as the thickness from ground surface to bedrock depth though we have noticed that the geologic layer can reach as high as several kilometres in the karst area and few meters in the alluvial since spatial thickness of geology is currently not available our simplification could be valid in terms of relative groundwater elevation and frequency analysis in addition uniform depth of geologic layer can significantly reduce the computational cost subsurface heterogeneity in 3d is another overlooked aspect in karstic regions groundwater flowpaths can be long and one can imagine that groundwater flooding in one place could results from hydrodynamics some distance away butscher and huggenberger 2007 gutiérrez et al 2014 hartmann et al 2014 rugel et al 2016 our model pihm can only represent very local and shallow near surface dynamics of groundwater tables new generations of models should address multilayer subsurface dynamics at timescales ranging from floods to seasonal long distance cavernous drainage though dynamic modelling of karstic system is computationally expensive the greatest limitation to the development at present is a scarcity of field observations precipitation can greatly influence groundwater flooding we only simulated 3 years annual runoff was 4 93 m3 s hydrologic processes due to intensive computational cost of coupled surface subsurface modelling the execution time was 4120 min on intel core i5 8500 3 00 ghz we selected relatively wet years since 1972 annual runoff was 2 96 m3 s from 1972 to 2010 fig s2 therefore we probably overestimated the cost of flood damage long term hydrologic modelling is necessary to reveal the impacts of regional climate mechanisms koutroulis et al 2010 on groundwater flooding when computation power is improved improvement of temporal and spatial et prediction is important to modelling flooding processes abiodun et al 2018 our simulated et was less confident during winter and spring season this was due to uncertainties in spatial parameters of land cover and soil properties spatially the et was primarily determined by vegetation type special attention should be paid to the dominant vegetation types to improve spatial accuracy of model simulation in addition the simulated et deviation in winter and spring may cause flood prediction error triggered by snow melting thus observation of snow accumulation is required to improve flood prediction finally the flood damage cost which has been referred in the present study is only an indicative estimation to show the magnitude of the problem of groundwater flood critical factors like flood depth and velocity were followed from other published work as already mentioned therefore these factors need to be elaborated for more accurate estimation of flood damage cost accordingly the coupled surface subsurface model can be adopted in regard to grid resolution process integration result post processing 5 summary we developed a coupled surface subsurface model for groundwater flood risk assessment the model can be used to delineate flood hazard areas and risk levels and predict flood time which are fundamental components of flood management the methodology presented in this paper could become a useful tool for the prediction of groundwater flooding areas and for the better understanding on the mechanisms of groundwater flooding this paper demonstrated a solution for improving environmental assessment dealing with groundwater dynamics in particular coupled surface and subsurface modelling can offer sophisticated solutions to the users regarding the coupling of hydrological information to not only flooding but also other groundwater related problems e g ecological conservation agricultural management in this sense the techniques proposed here about the distributed groundwater dynamics may be easily implemented in diverse environmental management since pihm is open source software acknowledgement the authors owe thanks to the four anonymous reviewers for their constructive comments which greatly improved the manuscript financial support for this study was provided by the european commission 7th framework programme as a large integrating project soiltrec grant agreement no 244118 the national natural science foundation of china grant no 51879289 and no 91547108 and the national key research and development program of china 2017yfc0405900 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 01 008 
26253,floods introduced by rainfall events are responsible for tremendous economic and property losses it is important to delineate flood prone areas to minimize flood damages and make mitigation plans recently there has been recognition of the need to understand the risk from groundwater flooding caused by the emergence of groundwater at the ground surface mapping groundwater flood risk is more challenging compared to river flooding especially in karst systems where subsurface preferential flowpaths can affect groundwater flow we developed a coupled surface subsurface modelling framework resolving spatial information and hydrologic data to assess the groundwater flood risk at a karstic watershed koiliaris river basin greece the simulated groundwater table was used to delineate groundwater flooding modelled results revealed the role of faults in groundwater flooding generation we anticipate the coupled surface subsurface approach to be a starting point for more sophisticated flooding risk assessment including magnitude and temporal duration of groundwater flooding keywords groundwater flooding coupled surface subsurface model pihm risk assessment critical zone observatory koiliaris river basin 1 introduction flooding is one of the most frequently occurring natural disasters that cause significant economic damages and affect people s life around the world jonkman et al 2008 unisdr 2015 kundzewicz et al 2014 huntingford et al 2014 further more it has now been widely noticed that the risk of life and economic losses caused by flood may increase due to social development land subsidence and climate change arnell and gosling 2016 hirabayashi et al 2013 jongman et al 2015 ward et al 2017 archfield et al 2016 it is urgent to delineate flood prone areas under changing environment and to reduce or prevent the detrimental effects of flood waters to battle the problem of delineating flood prone areas most of the research focus on the study of parameters of surface river flooding fig 1 yamazaki et al 2011 li and duffy 2011 in our area of interest vozinaki et al 2015 and kourgialas and karatzas 2013 estimated crop loss due to overland flooding by considering several parameters like flood depth flow velocity and vegetation types costabile and macchione 2015 showed that implementing high resolution topographic data and man made structures could significantly improve overbank flows these studies yield important insights into adequate design and development of flood management strategies for river management however the previous approach neglect a significant factor groundwater flood which can be critical in flood management especially in karstic watersheds finch et al 2004 macdonald et al 2008 groundwater flooding fig 1 happens when groundwater tables rise above the land surface away from perennial river channels or in the floodplain before the river goes out of bank finch et al 2004 macdonald et al 2008 groundwater flooding receives less attention compared to river flooding because it happens less often with less severity e g ascott et al 2017 morris et al 2018 however the management practices to prevent groundwater flooding are limited compared to river flooding which can be reduced by building engineering structures embankments walls dams etc once it happens groundwater flooding will take time to dissipate because groundwater moves much more slowly than surface water pinault et al 2005 such long lasting groundwater flood can cause not only significant damage to infrastructure abboud et al 2017 but also more severe river flooding huntingford et al 2014 mapping groundwater flood prone area is more challenging compared to river flooding due to many controlling factors and spatial uncertainties throughout the whole watershed including topography land use geology etc kourgialas and karatzas 2014 integrated 6 factors flow accumulation slope land use rainfall intensity geology and elevation to map flood hazard across the koiliaris river basin krb greece the static gis information of vegetation topography and soil can provide valuable information for locations with potential flooding risk for example the vegetation cover is considerably affecting the response of the water infiltrating depth in the soil rossi et al 2018 furthermore soil type like e g silt loam can be water repellent and delay water infiltration li et al 2018 thus temporal dynamics of groundwater flooding is still unclear which requires continuous hydrologic modelling of groundwater table elevation groundwater flooding is rarely modelled spatially throughout a watershed especially in areas with complex geological karst setting like this in krb groundwater flooding is prone to be developed due to the combination of low storage channelized preferential flow and rapid lateral inflow from steep uphill area naughton et al 2012 upton jackson 2011 butscher and huggenberger 2007 used a conceptual karst model to approximate subsurface flow for hydrologic simulation and found that 32 of the springs are discharging from an uphill area with slopes greater than 20 nikolaidis et al 2013 coupled a distributed hydrologic model and a conceptual karst model for integrated water management at krb explicit representation of karsts in a hydrologic model will be helpful to examine hydrologic links between karst and the rest part of the watershed e g shoemaker et al 2008 reimann et al 2011 rugel et al 2016 improve understanding of groundwater flooding and achieve sophisticated water management in this study we examine the spatial and temporal response of groundwater conditions of a karst watershed to assess the risks of groundwater flooding our first objective is to test the ability of a coupled surface subsurface hydrologic model to capture groundwater flooding in the model the spatial heterogeneity of the watersheds is explicitly resolved in the watershed responses which includes vegetation geology and faults the second objective is to understand how faults development would affect the groundwater flooding processes the methodological steps of the study are 1 application of a coupled surface subsurface spatially distributed hydrologic model to delineate spatial distribution and temporal dynamics of groundwater flooding 2 examination of the sensitivity of fault conductivity on flood risk and 3 calculate agricultural loses based on groundwater flooding area and land use the novelty of this work is to understand groundwater flooding through spatial and temporal dynamics of groundwater table simulated by coupled surface subsurface models we anticipate this work to be a starting point for more sophisticated flooding risk assessment the incorporation of geological climate and soil data into an affordable computation load could explain spatial vulnerability assessment of magnitude and temporal duration of groundwater flooding 2 data and methods 2 1 koiliaris river basin the koiliaris river basin krb is a critical zone observatory menon et al 2014 that represents severely degraded soils due to heavy agricultural impact over many centuries the 130 km2 watershed is located 15 km east of chania greece fig 2 a chania has a population of 53 910 and is the second largest city of crete the elevations of krb range from 0 to 2041 m above mean sea level and the slopes range from 0 to 62 climate in the watershed is mediterranean and characterized by a separation of seasons the average rainfall is 652 mm of which 20 in the dry season with an average temperature of 24 celsius while in the wet season 80 of the average rainfall occurs with an average temperature of 13 celsius stamati et al 2013 the land crops include citrus trees and olive trees which are cultivated throughout the year while vegetables and wheat are cultivated continuously in alterations appropriate for each period moraetis et al 2015 we used hourly climate data including precipitation temperature relative humidity wind velocity solar radiation and vapour pressure from three meteorological stations i e psychro pigadi samonas and kalyves fig 2b four soil types fig 2c covers the krb with the majority is eutric leptosols 56 and calcaric regosols 24 leptosol and regosol soils are expecting to show some differentiation in hydrologic properties such as those mentioned by kirchen et al 2017 with leptosols water holding capacity to be the lower the watershed consists of 13 land cover types fig 2d based on the corine land cover types bossard et al 2000 and 28 of the watershed is covered by natural grassland pink colour in fig 2d while 23 of the total area is covered by sclerophyllous vegetation yellow colour in fig 2d 2 2 model description a coupled surface subsurface hydrologic model penn state integrated hydrologic model pihm was selected to perform hydrologic simulations of streamflow and groundwater dynamics pihm kumar 2009 qu and duffy 2007 simulates the hydrological cycle including interception throughfall infiltration recharge evapotranspiration overland flow unsaturated soil water groundwater flow and channel routing in a fully coupled scheme evapotranspiration et is calculated using the penman monteith approach adapted from noah lsm chen and dudhia 2001 overland flow is described in 2 d estimated of st venant equations pihm uses diffusive wave approximation for overland flow and streamflow the model conceptualizes each subsurface column into two parts unsaturated and saturated layers water balance equation based on hydraulic head which is capable of representing the state in both unsaturated and saturated zones while only considering vertical moisture movement in the unsaturated zone the model simulates 2 d lateral groundwater flow in unconfined aquifer based on the dupuit approximation without addressing the karstic conduits network and the diffuse flow system in deep e g butscher and huggenberger 2007 interception and snowmelt are obtained using a bucket model and a temperature index model respectively horizontally the modelling domain is decomposed into delaunay triangles the governing equation for each process is discretized on the triangles from canopy to bedrock forming partial differential equations pdes pihm uses a semi discrete finite volume formulation for solving the set of the coupled pdes resulting in a system of ordinary differential equations odes representing all processes of each triangle the ode system is solved using the cvode implicit solver cohen and hindmarsh 1996 detailed descriptions of the modelling theory and mathematical formulation can be found at the pihm website http www pihm psu edu and associated publication kumar 2009 qu 2005 qu and duffy 2007 in this study we applied the coupled surface subsurface hydrologic modelling method to explicitly simulate groundwater table elevation throughout krb for groundwater flood assessment there are several advantages for pihm modelling for this study 1 the easy setup for the physical properties of the surface and subsurface layers including faults by pihmgis bhatt et al 2014 2 the model captures fully coupled dynamics of surface water and groundwater interactions e g yu et al 2015a zhang et al 2018 zhang et al 2019 hence allowing spatial and temporal delineation of floods and 3 geologic faults with substantial impacts on groundwater flow can be explicitly represented in the model as large scale preferential flow through high permeable regions 2 3 model parameterization calibration pihmgis bhatt et al 2014 was used to parameterize the model domain using the aforementioned datasets i e dem meteorological soil and land cover data the krb and the extended karstic area were discretized into 914 triangles and the river was discretized to 92 linear segments fig s1 the extended karstic area has been assigned by others as contributing area in the water budget of our study area kourgialas et al 2010 we estimated subsurface hydrologic parameters using texture data and pedotransfer functions wösten et al 1999 the pedotransfer functions used are generally applied for the different soil textures within europe wösten et al 1999 and they are not site specific therefore it is suitable for a regional study such as the present work the distribution of major faults was implemented according to the description in tataris and christodoulou 1969 an analysis of the tectonic phases in crete has revealed different phases of deformation which have definitely influence the faults hydraulic properties for example the mega and meso folds with fold axis w e and ns which were created during the beginning of the orogenesis in crete perpendicular to these folds dip slip w e and ns striking faults were created during the extensional phase in crete fassoulas et al 2004 the previous faults considered the oldest generation of faults which led to intense karstification with vast and deep sinkholes and shafts like gourgoutakas and liontari caves moraetis et al 2010 adamopoulos 2013 these w e and ns are of high hydraulic connectivity compare to the other fault zone of nw se striking direction which are much younger with less karstification and extensive preservation of faults slickenlines fassoulas 2001 caputo et al 2010 both these generations of faults exist in koiliaris hydraulic conductivity is simulated as being 10 times in the oldest faults w e and ns and 2 times in the youngest faults nw se fig 2c the hydraulic conductivity is selected higher in the older faults since the intensely karstified areas are along the w e faults kartsic features especially sinkholes have been observed also in other areas along extensional faults like in our case prasad et al 2017 these karstified areas are largely affecting the water flow and the hydraulic conductivities along the faults the hydraulic conductivity can change from 10 7 10 6 m s in massive limestones kovács 2003 to 10 5 in the epikarst hartmann et al 2012 and to 10 3 m s in karstified faults polomčić et al 2013 finally the past speleological expeditions in the deep sinkholes in our study area shows chaotic vertical shafts from masl meters above sea level of 1500 m down to 300 masl the karstic shafts correspond to what moraetis et al 2010 and nikolaidis et al 2013 presented as fast responding karstic system or upper reservoir the land cover parameters were obtained from monthly gldas vegetation parameters rodell et al 2004 and mapped according to table s1 the depth of groundwater flow was estimated based on the depth of the groundwater in the low land areas approximately 20 m below the land surface the depth is consistent with the depth of the epikarst around the world jones 2013 the value was used uniformly throughout the watershed our simulations were performed for 3 years july 2007 july 2010 when flooding was severe fig s2 using pihm v2 2 2015 initial condition was achieved using spinup of two previous years july 2005 june 2007 initialized groundwater tables were below the land surface and above bedrock 20 m below the land surface observed daily streamflow data at stylos spring fig 2b and modis remotely sensed et dataset mod16 running et al 2017 were used for model calibration according to the hydrologic characteristics of parameter we calibrated two groups separately event and seasonal scale groups table 1 yu et al 2013 the event scale parameters were optimized through an evolutionary algorithm yu et al 2013 we selected streamflow observation data from feb 25 2009 to march 4 2009 in which highest streamflow happened for event scale parameter calibration the seasonal scale parameters were tuned based on evapotranspiration ornl daac 2017 using observations from july 1 2008 to june 30 2009 the rest of the simulation was used for validation we used the relative error e pearson product moment correlation coefficient r and nash sutcliffe coefficient of efficiency nse nash and sutcliffe 1970 to evaluate the model performances table 2 streamflow calibration results are shown in fig 3 a the dynamics of streamflow variation between observed and modelled results were in reasonable agreement and are considered acceptable similar modelled and observed runoff ratios of 0 60 and 0 62 respectively also indicated reliable partitioning of the water budget furthermore 8 day average et ornl daac 2017 of this watershed was in good agreement with modis data fig 3b 2 4 delineation of groundwater flooding map since the focus of this paper is on the dynamics of groundwater response further confidence in the model result in this context was built by evaluating the ability of the model to simulate the historic flooding locations we extracted daily groundwater table depths of each finite volume i e triangle of the modelling mesh from the calibrated pihm simulation results groundwater flooding locations and areas were delineated by analysing the temporal dynamics of pihm simulated groundwater tables according to eq 2 1 g r o u n d w a t e r f l o o d v u l n e r a b i l i t y h i g h m a x i m u m g w 0 m o d e r a t e s k e w n e s s r g w 0 l o w s k e w n e s s r g w 0 2 r g w g w m i n i m u m g w m a x i m u m g w m i n i m u m g w groundwater flooding may happen under two conditions 1 groundwater table rises above land surface during the simulation period and 2 groundwater table is shallow during the wet season and the area can be potentially flooded during rainfall events we used maximum groundwater table elevation maximum gw to quantify the first situation where groundwater table height was above surface these places are of high groundwater flooding risk the second situation was examined by the frequency of groundwater getting close to the surface we rescaled the groundwater table elevation to 0 to 1 as relative groundwater table elevation rgw eq 2 and then analysed the temporal dynamics using the skewness of the rescaled groundwater table elevations skewness rgw fig 4 if the skewness is less than 0 it means the groundwater table is often close to the land surface which can potentially result in groundwater flooding i e moderate groundwater flooding risk if the skewness is positive the groundwater is often far below the land surface hence less likely to cause groundwater flooding i e low groundwater flooding risk time lag between river flooding peak and groundwater table height peak was used to identify the temporal dynamics of groundwater flooding we selected 8 events during the 3 years in each event the time of peak streamflow and peak groundwater table height was analysed we varied the fault hydraulic conductivity for sensitivity analysis to assess the impacts of faults on groundwater flooding since the fault hydraulic conductivity can vary significantly based on field experiences the hydraulic conductivity of faults was changed at different magnitudes to simulate karst development scenarios we analysed both spatial locations of groundwater flooding and the time delays of groundwater flooding 2 5 cost estimation of agricultural losses due to groundwater flooding the estimation of groundwater flooding cost was based on the calculations demonstrated by vozinaki et al 2015 the designated area of high risk was ascribed with the land uses data from corine database european environment agency 2004 the intersection of land cover and high risk flooding areas produced the percentage of different land uses in the high risk flooding areas the agricultural flood damage was calculated by the total area of each crop type under high flooding risk the weight yield per area and the cost per unit weight of yield we retrieved the weight yield per area and the cost per unit weight of yield from vozinaki et al 2015 in the previous study the depth and flow velocity of the flood were mostly 0 1 2 m and 0 0 3 m s respectively thus our estimations are based on these calculations and we adopted the flood damage cost standard deviation given therein 3 results 3 1 spatial distribution of groundwater flooding groundwater flooding map was created as three types of risk high moderate and low with areas of 100 21 and 8 7 km2 respectively fig 5 the spatial distribution of groundwater flooding was located around low elevation regions and faults this was probably because groundwater recharge from sinkholes in mountainous area could quickly discharge through highly fractured faults eleven of the 30 historic flooded sites were captured by high flood risk and 14 historic flooded sites were captured by moderate flood risk the rest 5 historic sites not captured by high or moderate risks were located next to the riverbanks previous studies vozinaki et al 2015 kourgialas and karatzas 2014 used high resolution modelling of riverine areas to capture flooding locations since our model grid is an unstructured mesh it will be easy to increase spatial resolution around the river 3 2 temporal dynamics of groundwater flooding groundwater flooding was generally less abrupt than river flooding in all of the 9 selected rainfall runoff events fig 6 table 3 there was a significant delay of the peak between groundwater flooding and river flooding indicated by model simulation the average time lag fig 6b varied from 4 4 to 14 2 h with a mean of 8 9 h time lags between groundwater flooding and river flooding i e peak flow in the river showed spatial heterogeneity the time lags were generally small around the faults and large in the flood plain i e the northeast part of krb this was due to the high hydraulic conductivity of the faults when the precipitation event lasted more than one day the peaks of groundwater flooding delayed longer and showed diverse spatial patterns events 2 3 4 and 6 fig 7 apart from the time lags in the emergence of the event there were also spatio temporal changes in the decay of the phenomena figs 6 and 7 such information is critical to flood mitigation 3 3 sensitivity to hydraulic conductivity of faults increase fault hydraulic conductivity created wider areas of groundwater flooding fig 8 the delineated groundwater flooding areas were 20 3 29 4 and 32 0 km2 for low medium and high k of faults the high flooding risk areas were 8 0 8 7 and 9 2 km2 for low medium and high k of faults the moderate flooding risk areas were 12 0 20 7 and 22 8 km2 for low medium and high k of faults increase fault hydraulic conductivity reduced responding time of groundwater flooding the average time lags between groundwater flooding and river flooding were 10 4 8 9 and 6 7 h for low medium and high k of faults fig 9 3 4 damage cost calculation in our study the total area of high flooding risk is 7 of the total watershed and 23 of the cultivated area the land uses distribution in the high risk areas is given in table 4 the olive grooves have the highest coverage in the high risk areas 39 while agricultural land has the second highest coverage 24 table 4 the highest cost of damage derives from olive groves and the second comes from fruit trees table 4 the total damage cost is 2 1 106 the calculation of the cost followed the methodology of vozinaki et al 2015 where the synthetic damage curves were used even the number of data was not large in the previous study it was considered representative for our study area since a questionnaire survey had be conducted specifically for koiliaris watershed 4 discussion 4 1 data interpretation of coupled surface subsurface modelling the advantage of coupled surface subsurface modelling is to explicitly simulate the head above ground our simulation is distributed throughout the watershed and processes of infiltration and discharge are dynamically considered through coupled surface surface equations therefore identification of groundwater flooding according to modelling results is relatively easy groundwater models e g modflow are often used to identify location of groundwater spring e g reed et al 2016 gallegos et al 2013 similarly groundwater flooding can be simulated through groundwater flow modelling to avoid realistic interactions between surface water and groundwater however such strategy may have limitation around riverbank we note that drainage pack harbaugh 2005 of modflow can be a potential solution for groundwater flooding it will helpful to compare our results with groundwater modelling results the purpose of coupled surface subsurface modelling is to improve understanding on hydrologic processes rather than runoff prediction efficiency previous watershed modelling studies obtained higher nse of runoff e g 0 91 and 0 87 in kourgialas et al 2010 0 62 in nikolaidis et al 2013 ideally coupled surface subsurface modelling has more parameters and gives better fit practically the parameters are bounded by physical meanings and the calibration of coupled surface subsurface models is usually difficult yu et al 2013 in addition calibration to different types of data may result poorer fit as the parameters try to serve different dynamics that dominate different data sets both uncertainties in spatial parameters and temporal forcing can consistently create model discrepancy at the stream gauge it is acceptable that our simulated runoff was degraded comparing to previous modelling studies notably the application of coupled surface subsurface hydrologic models is not only on streamflow dynamics chen et al 2015 seo et al 2016 zhang et al 2017 but also on spatial and temporal dynamics of soil moisture shi et al 2015 groundwater table elevation yu et al 2016 snow accumulation kumar et al 2013 yu et al 2015b which can create continuous spatial products of hydrologic and ecologic variables yu et al 2015a condon et al 2015 shi et al 2018 interpretation of coupled surface subsurface modelling results requires combination of localized information and targeted problem coupled surface subsurface models usually simulate many hydrologic fluxes and state variables across spatial and temporal scales these simulated results are usually validated only by streamflow or groundwater table elevation at limited locations which leaves large uncertainties in spatial distribution of groundwater table yu et al 2016 and soil moisture shi et al 2015 without calibration on local groundwater table we showed that relative dynamics of groundwater table could be reliable in terms of spatial information when absolute groundwater table height showed deviated pattern this is probably due our model confidence in et performance further calibration on groundwater table and assessment of uncertainties are needed to refine the spatial information on flood risk 4 2 implications for flood management in this basin identification of faults is critical for prediction of groundwater flooding macropore has been found to contribute groundwater flooding due to accumulated wetness over several years pinault et al 2005 our results showed that distribution of faults could potentially increase groundwater flood risk fault development was not incorporated in vast majority of hydrology models at krb kourgialas et al 2008 and nikolaidis et al 2013 have introduced fault data however these faults were incorporated to support the delineation of extended watershed area for the karstic aquifer and forcing the routing of water from the extended karst to the springs inside the watershed our study suggested that including faults in a physically based 3d hydrologic model automatically the extended karst is delineated and thus the model provides additional information for flood management faults of high hydraulic conductivity are vulnerable to abrupt groundwater flooding when the rain intensity exceeds the infiltration rate of the karst gutiérrez et al 2014 the groundwater table can rise quickly due to high connectivity to surface water the previous explains probably also the observed time lag of groundwater flooding and river flooding the higher lag in the rain events where precipitation is distributed in more days could be possibly related to more effective infiltration from the karst on the other hand more intense events lower the infiltration capacity and increase the abrupt fault response not to neglect also the effect of the spatial distribution of the rain event close or not in the major faults area which can affect the lag between the groundwater flooding and the river flooding faults of low hydraulic conductivity are vulnerable to slowly developed groundwater flooding groundwater storage can be accumulated during the whole wet seasons years before groundwater discharge happens pinault et al 2005 this type of groundwater flooding may last long time and difficult to mitigate flooding risks associated with faults behaviours in the groundwater flooding events may impose significant costs in the agriculture sector for example the flood risk designation for krb from vozinaki et al 2015 revealed areas of high flooding risk around the river with an estimation of 0 2 106 damages for an inundated area of 0 4 km2 one order of magnitude smaller compared to 2 1 106 in this study for the whole river basin the percentage of the study area in the previous work is restricted to river over bank flooding and correspond only in the 0 003 of the total watershed and 0 02 of the agricultural land which is very low compare to our estimations of the affected areas 7 and 24 respectively the reduction of the affected area is due to the fact that the previous study considered the flooding caused by koiliaris river a 5 km stretch from the springs to the sea located in the alluvial plain without considering groundwater overland flow the cost of the flood damage in our case is expanding in million of euros compare to the hundred thousand obtained by vozinaki et al 2015 despite we have not considered the depth and the velocity of the flood in our cost estimation we believe that the cost will be maintained in the same magnitude other critical factors which are lately gaining large attention is the flood frequency and the flood duration ward et al 2017 it has been stressed that el niño southern oscillation enso is affecting the flood duration more than flood frequency ward et al 2017 however the recent studies in crete showed that climate becomes drier with higher frequency of extreme events tsanis et al 2011 the same work shows an increase of 35 of events with high precipitation which a clear indication that a minimum 35 of damages is likely to occur the flood frequency definitely will increase the damages both in economy and mortality hu et al 2018 the groundwater modelling of the present study is significant in the understanding of the soil degradation in karstic mountainous areas it has been presented from other studies that the karstic groundwater is conveying dissolved pollutants during flood events like e g nitrates from mountainous areas toward lowland areas at koiliaris watershed nikolaidis et al 2013 moraetis et al 2010 the previous impact is related to livestock grazing in the higher land where grasses are growing during summer the identification of groundwater flood areas will designate the areas accepting probably low quality water on the other hand the quality analysis of the groundwater flood water will offer the opportunity to understand the soil degradation reflection in the karst water 4 3 sources of uncertainty model uncertainty was created by the simplified vertical representation of geology the geology was represented by homogeneous texture from ground surface to bedrock in each triangle such simplification may likely results in high spikes in simulated streamflow consequently the groundwater flooding damage may be over predicted also we used 20 m as the thickness from ground surface to bedrock depth though we have noticed that the geologic layer can reach as high as several kilometres in the karst area and few meters in the alluvial since spatial thickness of geology is currently not available our simplification could be valid in terms of relative groundwater elevation and frequency analysis in addition uniform depth of geologic layer can significantly reduce the computational cost subsurface heterogeneity in 3d is another overlooked aspect in karstic regions groundwater flowpaths can be long and one can imagine that groundwater flooding in one place could results from hydrodynamics some distance away butscher and huggenberger 2007 gutiérrez et al 2014 hartmann et al 2014 rugel et al 2016 our model pihm can only represent very local and shallow near surface dynamics of groundwater tables new generations of models should address multilayer subsurface dynamics at timescales ranging from floods to seasonal long distance cavernous drainage though dynamic modelling of karstic system is computationally expensive the greatest limitation to the development at present is a scarcity of field observations precipitation can greatly influence groundwater flooding we only simulated 3 years annual runoff was 4 93 m3 s hydrologic processes due to intensive computational cost of coupled surface subsurface modelling the execution time was 4120 min on intel core i5 8500 3 00 ghz we selected relatively wet years since 1972 annual runoff was 2 96 m3 s from 1972 to 2010 fig s2 therefore we probably overestimated the cost of flood damage long term hydrologic modelling is necessary to reveal the impacts of regional climate mechanisms koutroulis et al 2010 on groundwater flooding when computation power is improved improvement of temporal and spatial et prediction is important to modelling flooding processes abiodun et al 2018 our simulated et was less confident during winter and spring season this was due to uncertainties in spatial parameters of land cover and soil properties spatially the et was primarily determined by vegetation type special attention should be paid to the dominant vegetation types to improve spatial accuracy of model simulation in addition the simulated et deviation in winter and spring may cause flood prediction error triggered by snow melting thus observation of snow accumulation is required to improve flood prediction finally the flood damage cost which has been referred in the present study is only an indicative estimation to show the magnitude of the problem of groundwater flood critical factors like flood depth and velocity were followed from other published work as already mentioned therefore these factors need to be elaborated for more accurate estimation of flood damage cost accordingly the coupled surface subsurface model can be adopted in regard to grid resolution process integration result post processing 5 summary we developed a coupled surface subsurface model for groundwater flood risk assessment the model can be used to delineate flood hazard areas and risk levels and predict flood time which are fundamental components of flood management the methodology presented in this paper could become a useful tool for the prediction of groundwater flooding areas and for the better understanding on the mechanisms of groundwater flooding this paper demonstrated a solution for improving environmental assessment dealing with groundwater dynamics in particular coupled surface and subsurface modelling can offer sophisticated solutions to the users regarding the coupling of hydrological information to not only flooding but also other groundwater related problems e g ecological conservation agricultural management in this sense the techniques proposed here about the distributed groundwater dynamics may be easily implemented in diverse environmental management since pihm is open source software acknowledgement the authors owe thanks to the four anonymous reviewers for their constructive comments which greatly improved the manuscript financial support for this study was provided by the european commission 7th framework programme as a large integrating project soiltrec grant agreement no 244118 the national natural science foundation of china grant no 51879289 and no 91547108 and the national key research and development program of china 2017yfc0405900 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 01 008 
26254,computer modelling and design tools can assist in environmental management in particular post mining landscapes with large volumes of materials require shaping for optimal erosional stability and ecological and visual integration into the surrounding undisturbed landscape this paper evaluates the complementary capabilities of landscape evolution modelling siberia and geomorphic design software natural regrade with geofluv an existing 11 5 ha waste rock dump hunter valley new south wales australia served as the study site the siberia modelling demonstrated that geomorphic design reduced erosion by half that of conventional designs while being able to store an extra 7 of mine waste volume additionally the spatial pattern of gullying was able to be predicted by modelling which allowed management in subsequent geomorphic design and successively more stable patterns in conclusion the joint use of the natural regrade with geofluv geomorphic design software with the siberia landscape evolution model showed complementary capabilities for enhancing mine rehabilitation keywords siberia landscape evolution model natural regrade with geofluv software landscape modelling geomorphic landform design mine rehabilitation 1 introduction there is a growing recognition that an understanding of geomorphology can greatly improve environmental outcomes particularly for sites that have been subject to large scale earth movement this branch of knowledge and practise including principles design software development modelling construction and monitoring is aimed at developing alternative approaches to traditional engineered graded linear landforms in land rehabilitation such as contour banks or terraces and downdrains geomorphic solutions are an area of high interest in mining as 1 there is a litany of reported failures of post mining landscapes and associated structures mostly due to erosion see haigh 1979 1980 1985 1992 goodman and haigh 1981 hahn et al 1985 sawatsky and bestead 1996 sawatsky et al 2000 hancock 2004 martín duque et al 2015 among many others and 2 the hydrological ecological and visual integration and connectivity of traditional stand alone post mining engineered landforms with the surrounding terrain is largely neglected and a growing demand by the public and regulators is requiring that rehabilitation should blend and integrate much better into the surrounding landscape the demand for introducing geomorphic principles in mine rehabilitation or reclamation as both terms are used with the same purpose developed in the us and australia in the late 1970s and early 1980s initially a requirement that is still used today is the approximate original contour aoc concept included in the united states surface mining control and reclamation act smcra 1977 this concept made it compulsory that any us mine reclaimed area closely resembles the general surface configuration of the land prior to mining and blends into and complements the drainage pattern of the surrounding terrain the demand for complementing the drainage pattern of the surrounding terrain introduced a catchment approach in mine rehabilitation using the drainage basin as the fundamental unit for planning mine rehabilitation and guaranteeing hydrological connectivity these early writings asked for instance for the integration of the reclaimed surface and drainage network into the surrounding landscape stiller et al 1980 p 277 or for designing natural channels of progressively higher orders and therefore of greater capacity and cross sectional area hannan 1984 p 25 later a specific handbook on this issue landform design for rehabilitation environment australia 1998 based on hannan s book stated that referring to the drainage lines gradients should be progressively increased as the watercourse is constructed further back into the backfilled area it also mirrors stable natural landforms where watercourses become progressively steeper as one moves upstream environment australia 1998 p 20 essentially the goal was to replicate the patterns and complexity that landforms have in natural catchments a topic extensively studied by geomorphology for around 150 years mostly hillslope and fluvial geomorphology however the capabilities for designing such complex 3d landforms and drainage networks mimicking natural ones i e increasing channel cross sections downstream or concave channel longitudinal profiles with dividing ridge lands composed by s shaped convex concave slopes have not been possible only until very recently with the development of geomorphic software in addition the counterpart difficulty for building such complex landforms and landscapes is now possible with automatic gps guidance machine control bugosh and eckels 2006 to the best of our knowledge there are few software packages that could be termed as truly geomorphic design tools for land rehabilitation based on geomorphic principles the talus royal method has been successfully applied at rock roadcuts in france génie géologique 2017 and is starting to be applied in rock highwalls at quarries this method attempts to compress time by designing and building the rock cliffs or slopes that would tend to form and evolve with time through falls and slides that occur preferentially on weathered or fractured rocks equivalent natural cliffs or rock slopes are used as analogues the rosgen 1994 1996 morphological classification of rivers based on slope width to depth ratio bed material entrenchment ratio or sinuosity is in itself a geomorphic restoration method this approach has been widely employed for perennial stream restoration in the united states including mined sites rivermorph 2017 is a design software based on the principles established by rosgen 1994 1996 rehabilitation of the canadian oil sands led to the development of a site specific geomorphic approach for designing sustainable drainage systems sawatsky and beckstead 1996 the updates and learnings since the implementation of the aoc concept were the breeding ground for the reclamation method geofluv from geomorphic and fluvial bugosh 2000 2003 geofluv is a geomorphic method for land rehabilitation that is able to reproduce the complexity of natural landforms and drainage networks within catchments which become the basic rehabilitation design units specifically it designs mature and stable stages of catchments those which would naturally form by erosional processes for the materials climate and physiographic conditions at the site this technique began to be applied at large coal mines of new mexico united states in 1999 its implementation sought to achieve long term erosional stability reduced maintenance and increased biodiversity as compared to traditional mine rehabilitation landforms e g terrace berm downdrains natural regrade is the commercial software carlson software 2017 launched in 2005 that helps users to efficiently make geofluv designs in a cad format geofluv through natural regrade has been successfully used in the united states bugosh et al 2016 and spain martin duque and bugosh 2014 zapico et al 2018 from where it is extending to the european union as a recognised best available technique bat for the management of wastes of the extractives industries jrc 2018 the method has also been employed in australia waygood 2014 kelder and waygood 2016 and over south america bugosh et al 2016 if geomorphic rehabilitation methods and software are scarce truly geomorphic landscape landform modelling tools are also rare we are not referring here to soil erosion models which are more frequent widely and usefully used in mine rehabilitation such as the universal soil loss equation usle and its derivative revised universal soil loss equation rusle or the water erosion prediction program wepp see flanagan and livingston 1995 while very useful these models do not have a fully geomorphic catchment scope they neither evolve the landforms nor directly consider gullying by fluvial erosion the latter a common erosion process in post mining rehabilitation hancock et al 2000 landscape evolution models lem represent the current best practise of geomorphic modelling technology they offer all the functionality of soil erosion models but operate on a digital elevation model dem grid they calculate both erosion and deposition at each dem grid cell and adjust elevation accordingly in this way the landscape can evolve through time tucker and hancock 2010 this allows not just erosion rates to be determined but also to map and represent where erosion and deposition occurs the models can also visually show what the form of erosion is i e sheetwash rilling or gullying hancock et al 2013 there are a number of models that can be used to assess soil erosion and landscape evolution tucker and hancock 2010 originally developed in the 1970s ahnert 1976 these models all use a dem or mesh of grid cells to represent a catchment willgoose et al 1991a b coulthard et al 2012 these numerical models employ both fluvial and diffusive erosion processes together with climate expressed in rainfall amount and intensity a summary of key attributes is described in tucker and hancock 2010 and willgoose 2018 these models are particularly useful for assessing post mine landscape designs as they can be input into the lem and allowed to evolve models such as siberia http www telluricresearch com willgoose 1989 hancock and willgoose 2017 are ideal for assessing landscapes at annual time steps and can be run up to thousands of years hancock et al 2016 hancock and willgoose 2017 report the authors use of siberia in projects in australia argentina canada namibia papua new guinea tanzania and the united states they also outlined its application by consultants globally and that it has the strongest scientific base for landform design in mine rehabilitation evans and riley 1994 willgoose and riley 1998 hancock et al 2003 other lems such as caesar lisflood coulthard et al 2012 can run at hourly time steps and can assess the effects of storm events on erosional stability a new generation of soilscape models are also now available which incorporate both spatially variable hydrology as well as soil material properties cohen et al 2009 welivitiya et al 2016 as siberia is the most widely used tool at present we focus on this model however others such as caesar lisflood could also be used here we describe the combined use of siberia and geofluv for obtaining optimal mine rehabilitation outcomes evaluated in terms of erosional stability both tools share key common characteristics 1 a catchment scale approach 2 the combined use of hillslope and fluvial geomorphic principles and algorithms 3 and the concept of maturity in landforms and landscapes again at a catchment scale despite this the joint use of siberia and geofluv is only starting to be adopted as best practice landloch 2010 waygood 2014 kelder and waygood 2016 in this framework we evaluate the complementary capabilities of two separate computer programs 1 the landscape evolution and soil erosion modelling siberia and 2 the geomorphic design software natural regrade with geofluv in order to obtain optimum stable landforms through an iterative design modelling process specifically an 11 5 ha waste dump at the drayton mine new south wales australia served as the study site this landscape built in 2013 was the first geofluv rehabilitation example in australia here four different landscapes were assessed the first goal of this research was to evaluate the long term 100 yr erosional stability using the siberia model of the geofluv landscape as built at drayton mine from this information we sought to identify the stability factors for improvement and then develop an improved geofluv design which was assessed using siberia finally the two geofluv alternatives as built and improved were compared with equivalent simulations of two landform designs with the same footprint area and approximate drainage density and waste volume contour banks the most common rehabilitation landform in the hunter valley of australia and natural contouring a landscaping approach that is becoming widely used this comparison between four landscapes dealt exclusively with erosion as the landscape footprint together with soil and vegetation factors remain constant with this procedure we seek to contribute for best practise in mine rehabilitation 1 1 study site drayton is an open cut coal mine located in the hunter valley of new south wales australia fig 1 operated by angloamerican during the period of this study the mine commenced operation in 1982 with production starting in 1983 using draglines together with truck and shovel extraction methods the mine produced approximately 8 million tonnes of coal per year mostly for export markets the mine ceased operation in 2016 in 2013 an 11 5 ha waste dump comprised of mine waste rock of sedimentary origin see fig 2 a located on a high point of the mine fig 2b was re shaped using geofluv natural regrade fig 2c this site was used as a training and demonstration project for this methodology the design had four first order a type zig zag channels rosgen 1994 1996 draining the main slope of the waste dump and three b and c type meandering channels rosgen 1994 1996 one draining the upper platform and two located at the footslopes of the waste dump the whole design had a stable base level outlet located at a sandstone ledge area see fig 2c table 1 shows the inputs used for the design bugosh com pers the construction took place between november and december of 2013 using d9 and d6 bulldozers and an excavator gps machine control systems were used to guide the earth movements a 10 cm cover of topsoil clay soil was spread in mid december 2013 the area was seeded with a mixture of grasses and trees at the end of december 2013 several rainfall events occurred during construction causing some incision at the bottom of the built first order valleys this was addressed with topsoil spreading on february 2014 an intense storm occurred and photographs march 2014 show incision at the thalwegs and rilling at the slopes fig 3 the incised areas were repaired and reseeded between may and june of 2014 photos of the area taken in september 2014 show an incipient and homogeneous herbaceous vegetation cover fig 3a the site was revisited again three years after in september 2017 showing a dense vegetation cover fig 3b which has largely stabilized the site although a series of 20 cm on average discontinuous gullies exist at the bottom of several of the built valleys fig 3c 2 methodology 2 1 landform design scenarios the landform design alternatives to be modelled and compared are two geofluv designs as built gb and improved ig and two other ones using techniques currently being applied or considered for the coal mine rehabilitation at the hunter valley in australia contour banks cb and natural contouring nc for comparison the main condition was that any design alternative would have the same footprint 11 5 ha and similar drainage density and mine waste storage capacity a maximum 10 variation was established as a threshold for this parameter the design and assessment is an iterative process with the original geofluv design modelled using siberia and any flaws then amended also with geofluv the methodology is outlined in fig 4 2 1 1 geofluv natural regrade designs the geofluv method is a landform design approach currently used for coal mine rehabilitation in the hunter valley waygood 2014 kelder and waygood 2016 geofluv is the trademark name for a specific patented landform design method that uses algorithms based on slope and fluvial geomorphic principles natural regrade is the software that helps users construct geofluv geomorphic designs the complexity of natural stable landforms and drainage networks can be reproduced in a cad format s shape convex concave slopes concave longitudinal channel profiles sinuosity indexes and patterns or progressive variation of channel cross sections with increasing flow downstream among many others the method integrates key aspects of the rosgen 1994 1996 morphological drainage channels classification mostly the aa a b and c stream types defining them based on slope width to depth ratio or sinuosity here we focus on uplands and low order drainage basins geofluv natural regrade integrates also mathematical relationships for the geometry of natural meandering channels leopold and wolman 1960 williams 1986 geofluv natural regrade aims to design mature stable catchments the state to which a catchment naturally would evolve by erosional processes to steady state stability under the climatic and physiographic conditions at the site for that a suitable and stable mature reference area has to be identified in the field to provide initial input values for the rehabilitation design bugosh 2000 2003 examples of those inputs are drainage density a channel reach length maximum distance from ridgeline to channel s heads width to depth ratios and sinuosity for different types of channels among others in addition natural regrade uses the 2 yr 1 hr average recurrence interval ari event to design the bankfull channel dimensions and the 50 yr 6 hr ari event to design the flood prone dimensions since they have been found to be mathematically related with the stream channel pattern williams 1986 the natural regrade software has the capacity to use other intensity frequency duration ifd values according to specific needs or different geographical climatic characteristics for the rehabilitation landform the input values must be site specific measured at an appropriate reference area for each project using appropriate analogue areas surface properties and local meteorological records the input values are therefore not universal ones that can arbitrarily be used at any site this is a critical point as some published papers have used words like recommended sears et al 2014 and we emphasize here the importance of obtaining appropriate site specific project area input values from local reference areas and meteorological records once the inputs are established obtaining a coordinate model of the site is the first step fig 4 the procedure of design is described in detail at the users manual of carlson software http www carlsonsw com support manuals the constructed landform geofluv as built gb was designed by drayton staff the topography of the resultant landscape was supplied as an ungridded aerial survey using light detecting and ranging lidar that was undertaken for the site this data was gridded using ordinary kriging to a regular 0 2 m by 0 2 m grid this grid size and gridding method was used for all later landforms geomorphic designs are expected to have long term stability as they immediately re establish steady state or equilibrium landforms with the local environment data from monitored geofluv based mine rehabilitations is starting to be available bugosh and epp 2015 zapico et al 2018 however in most regions there will be no such previous field validation therefore computer models i e siberia are the only method to evaluate the performance of theoretical designs fig 4 shows the feedback process to produce stable geofluv designs by their iterative simulation with siberia here for an objective comparison we normalized the gb design for volume as there was a difference in volumes between the landscapes the normalization process maintained the spatial layout of contours but equalised volume the characteristics of the gb design have been described in the study area section finally as referred the ig design was created amending instabilities predicted by the siberia modelling 2 1 2 contour banks cb linear hillslopes with contour or level banks are the default mine rehabilitation design for many areas including the hunter valley the contour banks consist of linear slopes reaching gradients around 10 interrupted by channels or drains with a bank on the downhill side constructed accurately on the contour this erosion control earthwork has been transferred from agricultural practises to mining for retarding runoff and promoting infiltration hannan 1984 these banks interrupt the slopes length at approximately each 10 m of elevation the contour bank landform was designed with the carlson software tools of polyline drawing and offsetting following the design principles by hannan 1984 2 1 3 natural contouring nc of surrounding natural terrain as described in the introduction there is a global trend in mine rehabilitation of moving from current linear graded engineered landforms to more natural and complex ones this is promoting all types of imitations of the shape of natural landscapes with little consideration for geomorphic principles common approaches are to try to arbitrarily resemble the pre mine topography or to imitate the surrounding terrain of the mined lands therefore we considered this pre mine topography as a viable alternative the natural contouring that we used was faithful to the definition since literally we fitted the topography of a natural landform from within the mine lease to the waste dump area matching its configuration volume and slope gradient and approximate slope length the procedure was 1 identifying areas surrounding the mine with the same approximate slope and shape of the rehabilitated area subject of study 2 extracting the original natural contours by the intersection of a polyline that defines the rehabilitation area 3 pasting and fitting the contours at the rehabilitation site with base height corrected 2 2 landscape evolution assessment here we focus on the siberia model which has been employed extensively for agricultural and post mining landforms evans and loch 1996 willgoose and riley 1998 hancock et al 2000 2008 2010 martinez et al 2009 siberia mathematically simulates the geomorphic evolution of landforms subjected to fluvial and diffusive erosion willgoose et al 1991a b c d willgoose 2018 the model employs well accepted hydrology and erosion models at annual and longer time scales the sediment transport equation of siberia is 1 qs qsf qsd where qs m3 s m width is the sediment transport rate per unit width qsf is the fluvial sediment transport term and qsd is the diffusive transport term both m3 s m width the fluvial sediment transport expression qsf is based on the einstein brown equation and models incision of the land surface and can be expressed as 2 qsf β1 qm1 sn1 where q is the discharge per unit width m3 s m width s metre metre the slope in the steepest downslope direction and β1 m1 and n1 are calibrated parameters diffusive erosion qsd is 3 qsd ds where d m3 s m width is diffusivity and s is slope the diffusive term models smoothing of the land surface and combines the effects of creep and rain splash siberia does not directly model runoff but relates discharge to area a draining through a point as 4 q β 3 a m 3 where β3 is the runoff rate constant and m3 is the exponent of area m3 s m width the advantage of siberia over more traditional erosion models i e rusle wischmeier and smith 1978 lies in its use of digital elevation models for the determination of drainage areas and representation of catchment topography as well as its ability to adjust the landform elevations with time in response to the erosion and deposition that occurs a more detailed description of the principles underpinning the current version of siberia can be found in willgoose et al 1991a b c willgoose 2005 and willgoose 2018 2 2 1 calibration of siberia all soil erosion and landscape evolution models require input parameters specific for the site siberia is no different the most important parameters relate to the sediment transport equation equation 2 for calibration the most rigorous method is to use field plots where all rainfall runoff and total sediment load is collected for a significant number of storm events these plots should also be maintained for a number of years as it has been found that sediment loads on post mining landscapes can rapidly reduce through time hancock et al 2016 however the installation and maintenance of field plots is costly and time consuming and rare at mine sites an alternative method is to use high resolution surveying of representative hillslopes with each method having advantages and disadvantages laser scanning digital photogrammetry both these can be impaired by vegetation and in recent years airborne lidar are becoming increasingly available repeated scans can be used to assess type i e rilling gullying sheetwash and differencing one scan from another allows volumetric assessments to be made and erosion rates calculated however at this site and typical of most mine sites there are no field plots or survey data available that could be used for calibration here we calibrate by using historical data sediment transport theory and current measurements for the site it is well known that the values of m1 and n1 equation 2 vary widely but for most fluvial systems they both range between 1 and 3 kirkby 1971 however n1 has been measured to be as low as 0 5 in mining applications due to surface armouring willgoose and riley 1998 willgoose and sharmeen 2006 with everything else being equal steeper slopes developing coarser less erodible surfaces than flatter slopes e g see the area slope diameter plots of cohen et al 2009 and welivitiya et al 2016 as there were no data available for the site we employed a set of parameters derived for similar materials at another local mine site rixs creek hancock et al 2008 to calibrate siberia a fitting process was conducted where β1 was held constant and m1 and n1 adjusted until the form and position of erosion matched the field observation at the site this process found that values of m1 2 5 and n1 2 provided the best match to the available field data these value of m1 1 5 2 5 and n1 1 2 for rixs creek are within the range of values for fluvial process dominated catchments suggested by kirkby 1971 assuming a spatially uniform sediment production rate therefore the parameters are what could be reasonably expected for the site and materials examined here soil erodibility β1 is recognised to be well described by the rusle k factor which can be determined from the material particle size distribution evans and loch 1996 sheridan et al 2000 hazleton and murphy 2007 here five individual soil cores 100 mm deep and 65 mm diameter were collected from representative positions footslope midslope and top of slope on the reconstructed mine landform a depth of 100 mm was used as this was the average depth of topsoil that was placed over the waste rock the particle size distribution psd of the five samples was determined by sieve and hydrometer methods with all five samples having similar particle size average sand 51 silt 9 clay 40 range sand 50 54 silt 7 11 clay 39 42 using the soil particle size classification and k factor soil erodibility wischmeier and smith 1978 table of hazelton and murphy 2007 the material can be classified as a clay to which we have assigned a k factor of 0 01 this k factor can be input into the siberia model assuming the surface has an absence of vegetation willgoose 2012 for many sites where there is bare earth or where the site has been degraded with little or no vegetation i e mine sites with a bare non vegetated surface or a surface with vegetation removed by fire discussed later this erodibility k value can be used directly in the model however many sites have a rock cover or armour in the case here a good grass cover exists after three years post rehabilitation similar to the rusle k factor the rusle c wischmeier and smith 1978 factor can be used to determine the expected erosion reduction due to vegetation there is quite a lot of data on the role of vegetation and a c factor can be directly determined from tables wischmeier and smith 1978 blanco and lal 2008 from a variety of sources here we use a c value of 0 02 which represents a stand of dense sod like grass blanco and lal 2008 the siberia β1 value is then determined by multiplying the k value by the c value 0 0002 bulk density was calculated from the volume and mass of the cores described above 1 56 t m3 it should be noted that these values are estimated values only at this site it is not possible to validate the parameters as there is no field plot or survey data available therefore the erosion rate here is indicative only however the parameters are very close to the value determined for the nearby rixs creek mine site hancock et al 2008 which had an absence of vegetation and similar rilling and gullying to that of the surface examined here therefore the similarity of the parameters at this site to that determined by independent means of rixs creek provides confidence in the methods and data 2 2 2 siberia simulations the siberia model was run for all four landscapes using the parameters described above as all post mining landforms have a bare surface devoid of vegetation siberia was run for an initial period of three years with an erodibility representing a bare surface β1 0 01 this three year period represents an initial high erosion rate and allows drainage lines to rapidly form at three years β1 was changed to represent a fully vegetated surface β1 0 0002 and the simulation continued for 100 years this 100 year period while not geomorphic time is within the human management time period and allows any landscape design strengths and weaknesses to be identified it also represents the period of most rapid development of a new landform to assess erosion rates the dem of difference dod approach was used where the reconstructed landscape at year 100 was subtracted from the initial landscape at year 0 this approach also allows maximum depth of erosion in this case gully depth as well as depth of deposition to be determined 3 results and discussion 3 1 landform design scenarios siberia calibration and simulations fig 5 displays the four landform designs a geofluv as built gb b improved geofluv ig c contour banks cb d natural contouring nc table 2 shows the footprint and volumetric characteristics of all the modelled designs fig 6 shows both the rilling at the interbasin areas of the geofluv as built landforms and a sub section of the 2014 landscape dem gridded to 0 2 m after one year of erosion using the siberia lem demonstrating that the pattern of modelled rilling matched that observed at the site finally fig 7 shows the 100 yr siberia modelling for the four landform design scenarios 3 2 long term landform stability erosion rates erosion values obtained for the four studied alternatives range between 25 6 and 13 9 t ha yr ranked from higher to lower erosion rates they are cb gb nc and ig table 3 if erosion rates would be the only factor to be judged for their selection as rehabilitation alternatives perhaps they all could be acceptable as the values are not high compared to other australian disturbed landscape systems i e tilled agricultural fields bui et al 2011 and certainly they are much lower than poor standard mine rehabilitation in other regions of the world i e spain where martín moreno et al 2018 found that erosion rates can be an order of magnitude higher also for comparison the australian queensland department of mines and energy uses a range of 12 40 t ha yr as a target erosion rate for rehabilitated mine sites welsh et al 1994 williams 2000 elliott and dight 1986 in kelder and waygood 2016 state that natural landforms in the hunter valley baseline are expected to erode between 0 4 and 11 8 t ha yr the ig design has the lowest erosion rate 13 9 t ha yr and the highest waste volume storage 2 465 522 m3 an additional analysis of this ig design showed that 77 7 of the eroded material is deposited within the first order subcatchments this means that the real sediment yield value sediment exiting the catchment is 3 1 t ha yr for a global comparison there are only two geofluv based rehabilitation mine sites worldwide that have been monitored in terms of sediment yield at the la plata coal mine in the semi arid environment of new mexico united states bugosh and epp 2015 measured 8 3 t ha yr of sediment yield for a geofluv natural regrade rehabilitation with topdressing and poorly established vegetation and 5 7 t ha yr for a geofluv natural regrade rehabilitation with topdressing and significant vegetation establishment compared with 9 5 t ha yr for a neighbour undisturbed native site zapico et al 2018 measured 4 0 t ha yr of sediment yield at a geofluv natural regrade rehabilitation of a kaolin mine el machorro located in a temperate continental mediterranean environment of central spain the cb design did not have very high erosion rates compared to other traditional mine rehabilitation solutions worldwide see martín moreno 2013 for a compilation of references on this issue the obtained values 25 6 t ha yr are within what could be considered acceptable in some parts of australia 12 40 t ha yr for mine rehabilitated sites as commented before this is also in agreement with what gyasi agyei and willgoose 1996 found demonstrating that contour banks were more stable than linear slopes without them 3 3 erosion process identification and geomorphology in general rehabilitated areas with contour banks in active coal mines of the hunter valley show a broad acceptable erosive performance however there are three key aspects to consider here i failures gullying occur randomly due to inevitable overtopping of the channel behind the banks when the storage capacity is exceeded this can be due to a rain event with a higher intensity than that used for the design or most commonly by progressive infilling of those channels ii when contour banks fail they tend to trigger fewer but bigger gullies than without contour banks due to runoff concentration in such drainage lines iii although it is not a direct issue related with this research contour banks do not fulfil the best possible hydrologic ecologic and visual integration with the undisturbed surrounding landscapes as far as the 100 yr modelling of the gb landforms is concerned they showed two long term erosion issues i runoff was not properly split in subcatchments at the upland areas of the former design which produce excess runoff towards the slope catchments increasing the potential of gullying due to runon this is a common issue for rehabilitated landscapes with siberia modelling mapping and quantifying the process ii long term failure of the constructed earth bank at the base of the structure that encloses the perimeter meander channels the forecast of these gullying processes allows them to be considered in future designs this was performed through an iterative process of geofluv natural regrade design and siberia modelling fig 4 it could be argued that the same iterative process could be used for contour banks but for this landform there is not literally much room for topographical improvement outside the spacing and dimension of the banks whereas the geofluv landforms can be largely changed in topography until a stable design is reached since overtopping is inevitable in the long term contour banks are a feasible solution while maintenance is guaranteed this may be acceptable for instance if there are economic use of the post mining land but they are an option that requires on going maintenance and associated costs for this study site the nc design had a moderate erosion rate 21 7 t ha yr and developed gullies this does not occur on the pre mine landscape that has developed over geological time fig 5d and has structure soil horizons of the soil and substrata this suggests the unsuitability of arbitrarily trying to imitate the pre mine topography or that of the surrounding terrain of a mine as a rehabilitation landform alternative this approach can be well intentioned but lacks geomorphic basis unfortunately the authors have seen an increasing use of nc approaches a geomorphic approach to mine rehabilitation should not be a matter of simply looking like a natural landform it must be functionally stable often looking natural at the beginning may lead to widespread erosion as found here therefore this approach is not the best option given the site and material constraints 3 4 spatial patterns in drainage and gullying iterative modelling of geofluv designs with siberia allowed identification of critical issues leading to instability and gullying the most critical one was recognising when the 0 order subcatchments and swales were not correctly designed in these situations siberia was able to predict gullying for runoff trajectories fig 8 this observation lead to another important observation that some gullying may occur in the 0 order subcatchments swales of the geofluv designs and that this would not be a problem if the average erosion values are not high the key issue here is that geofluv designs with runoff split into small subcatchments add spatial predictability to erosion lines whereas critical erosion problems arise when gullying development is not controlled or predicted drainage network development is a chaotic process but if an initial drainage pattern is imposed some predictability should be imposed on the eroding system willgoose and riley 1998 p 257 and this is what the geofluv designs produce predicting the gullying prone drainage lines therefore the use of a lem is key with siberia highlighting areas of high erosion particularly gullies the geomorphic design can subsequently reduce it we interpret the fact that the gb even with some deviations in the design and in the building process and nc even when not recommended experience less erosion than cb because both gb and nc include valleys in the landform those valleys add drainage predictability and represent more mature landforms than cb so that they experience less modification by earth surface processes this reflection has been well explained by toy and chuse 2005 p 30 as the adjustments necessary to establish a steady state decrease the prospect for reclamation success increases and the demand for post reclamation site maintenance decreases that we argue here about the favourable conditions in terms of erosion of the natural contouring alternative and the fact that we do not recommend it may seem to be a contradiction but it is not this natural contouring solution did not produce high instability at this study site but a similar approach in another context could be very unstable introducing high unpredictability science based design and modelling tools e g the quel model ibbitt et al 1999 willgoose 2001 should always be involved in landform design processes following the same reasoning one of the problems with the contour banks alternative is that the spatial prediction of gullying formation and evolution is low a visual interpretation of the pattern of gully formation for the contour bank solution see fig 7c shows a dendritic organization of the gully network with multiple captures between rilled and gullied microcatchments leading to a chaotic and unpredictable drainage network development process this is also reflected in higher depths of the gullies for contour banks table 3 this gullying unpredictability can become more critical under extreme rainfall the gullying resulting from contour banks failures is a symptom of the adjustments of the geomorphic system trying to redevelop a new drainage network which has been completely obliterated by a linear slope whereas the other alternatives have an initial drainage network and within those three alternatives with a drainage network the main improvement in the ig landform compared with geofluv gb which performs similarly to other landforms in terms of erosion rate has been the proper splitting of the runoff in concave 0 order subcatchments avoiding long term failures and unpredicted gullied catchments captures 3 5 additional considerations the geofluv geomorphic rehabilitation at this mine 11 5 ha was a demonstration site we maintained the exact footprint 11 5 ha and similar characteristics waste volume and drainage density for all the designs compared to the gb solution but it has to be stressed that for a solution to be integrated at a mine scale for a typical hunter valley coal mine there is no need to have such a high drainage density as the one that was built 158 m ha the reason is that the drainage density of the reference area is lower 90 20 m ha see table 1 high drainage densities imply short slopes a 60 m slope length is typical of geofluv designs for 158 m ha which add difficulty and extra expense in construction there are also difficulties in fitting the channel convexity 40 m of distance of ridge to head of channel in this case as identified in the reference areas meaning that most of the slopes would be convex slope lengths longer than 100 m which fit well within a drainage density of 90 m ha would be more reasonable and feasible it is not easy to predict how a more realistic drainage density for all four designs according to reference area 90 m ha would affect the outcome and the interpretation of the results which are presented here theoretically geofluv designs would be even more stable since a lower drainage density would mean more concave reaches at the base of the slopes whereas a lower drainage density in contour banks less length of banks per hectare would imply theoretically more instability but this reasoning is only applicable circa 90 m ha because lower values in geofluv designs would mean gullying until reaching that 90 m ha of equilibrium state the results also demonstrate that the geofluv designs and the most common landform rehabilitation solution of the hunter valley contour banks can store approximately the same volume of waste see table 2 specifically the gb stores 12 3 more of waste volume that contour banks and the improved ig solution is able to store 7 2 more but in this latter case having about half of erosion rate 4 conclusions here we demonstrate and assess the first systematic integration of the use of geomorphic design geofluv natural regrade and assessment using a landscape evolution model siberia unstable design issues were identified using siberia and removed using an iterative process until both erosion was reduced and landscape volume optimised the improved geomorphic rehabilitation shows high erosional stability but would be expected to perform better if more design and modelling iterations would be performed the siberia modelling was critical for identifying four main causes of instability that were turned into stability by guaranteeing in subsequent designs a appropriate concavity at any foothill transitioning towards the channels b that runoff is always directed towards the swales 0 order subcatchments c a correct design of catchments from the top of the landscape to streamline d long term stability of the main valley meandering channels by constructing entire subcatchments the consideration of these factors in the geofluv designs produced successively lower erosion rates for each landscape iteration as a main conclusion for the coalfields of the hunter valley the improved geofluv design using siberia modelling reduced erosion by half while being able to store 7 more mine waste volume than contour banks additionally the gullying pattern was predicted by the landscape evolution model and 77 of the eroded material was predicted to be deposited within the first order subcatchments further reducing significantly sediment yield additionally while not being the main purpose of the research a cad based procedure has been developed to extract the natural contouring nc of undisturbed lands adjacent to mines drayton in this case as a theoretical landform design alternative this procedure has been strictly developed for scientific purposes and authors do not recommend this landform approach be used as an alternative for mine rehabilitation without careful geomorphic assessment in conclusion the joint use of geomorphic design software with a landscape evolution model showed complementary capabilities for optimised landform design through an iterative design process and landscape evolution modelling optimised geomorphic designs can be reached supplementary to this research compared assessments between geomorphic and traditional landform designs using economic hydrologic ecologic and visual issues should provide both the mining industry and regulators with improved rationale for decision making this is important as the public is demanding much higher standards of mine rehabilitation acknowledgements this paper is the outcome of a joint research of the three authors made possible by means of a research stay of j f martín duque hosted by g r hancock and g r willgoose at the university of newcastle uon australia this stay has been funded within a program estancias de movilidad de profesores e investigadores en centros extranjeros de enseñanza superior of the spanish minister of education culture and sports reference prx16 00441 the authors acknowledge the kind collaboration of the tom farrell institute for the environment of the uon drayton angloamerican has kindly supported this research by providing the geofluv as built topography along with all the needed information which main aim has been getting knowledge in order to develop best mine rehabilitation practises the assistance of matt lord and team is highly appreciated this manuscript is also a contribution under the joint research of the ecological restoration network remedinal 3 of the madrid community s2013 mae 2719 nicholas bugosh also provided useful information and comments finally two anonymous reviewers and francis rengers made very constructive comments at the review process 
26254,computer modelling and design tools can assist in environmental management in particular post mining landscapes with large volumes of materials require shaping for optimal erosional stability and ecological and visual integration into the surrounding undisturbed landscape this paper evaluates the complementary capabilities of landscape evolution modelling siberia and geomorphic design software natural regrade with geofluv an existing 11 5 ha waste rock dump hunter valley new south wales australia served as the study site the siberia modelling demonstrated that geomorphic design reduced erosion by half that of conventional designs while being able to store an extra 7 of mine waste volume additionally the spatial pattern of gullying was able to be predicted by modelling which allowed management in subsequent geomorphic design and successively more stable patterns in conclusion the joint use of the natural regrade with geofluv geomorphic design software with the siberia landscape evolution model showed complementary capabilities for enhancing mine rehabilitation keywords siberia landscape evolution model natural regrade with geofluv software landscape modelling geomorphic landform design mine rehabilitation 1 introduction there is a growing recognition that an understanding of geomorphology can greatly improve environmental outcomes particularly for sites that have been subject to large scale earth movement this branch of knowledge and practise including principles design software development modelling construction and monitoring is aimed at developing alternative approaches to traditional engineered graded linear landforms in land rehabilitation such as contour banks or terraces and downdrains geomorphic solutions are an area of high interest in mining as 1 there is a litany of reported failures of post mining landscapes and associated structures mostly due to erosion see haigh 1979 1980 1985 1992 goodman and haigh 1981 hahn et al 1985 sawatsky and bestead 1996 sawatsky et al 2000 hancock 2004 martín duque et al 2015 among many others and 2 the hydrological ecological and visual integration and connectivity of traditional stand alone post mining engineered landforms with the surrounding terrain is largely neglected and a growing demand by the public and regulators is requiring that rehabilitation should blend and integrate much better into the surrounding landscape the demand for introducing geomorphic principles in mine rehabilitation or reclamation as both terms are used with the same purpose developed in the us and australia in the late 1970s and early 1980s initially a requirement that is still used today is the approximate original contour aoc concept included in the united states surface mining control and reclamation act smcra 1977 this concept made it compulsory that any us mine reclaimed area closely resembles the general surface configuration of the land prior to mining and blends into and complements the drainage pattern of the surrounding terrain the demand for complementing the drainage pattern of the surrounding terrain introduced a catchment approach in mine rehabilitation using the drainage basin as the fundamental unit for planning mine rehabilitation and guaranteeing hydrological connectivity these early writings asked for instance for the integration of the reclaimed surface and drainage network into the surrounding landscape stiller et al 1980 p 277 or for designing natural channels of progressively higher orders and therefore of greater capacity and cross sectional area hannan 1984 p 25 later a specific handbook on this issue landform design for rehabilitation environment australia 1998 based on hannan s book stated that referring to the drainage lines gradients should be progressively increased as the watercourse is constructed further back into the backfilled area it also mirrors stable natural landforms where watercourses become progressively steeper as one moves upstream environment australia 1998 p 20 essentially the goal was to replicate the patterns and complexity that landforms have in natural catchments a topic extensively studied by geomorphology for around 150 years mostly hillslope and fluvial geomorphology however the capabilities for designing such complex 3d landforms and drainage networks mimicking natural ones i e increasing channel cross sections downstream or concave channel longitudinal profiles with dividing ridge lands composed by s shaped convex concave slopes have not been possible only until very recently with the development of geomorphic software in addition the counterpart difficulty for building such complex landforms and landscapes is now possible with automatic gps guidance machine control bugosh and eckels 2006 to the best of our knowledge there are few software packages that could be termed as truly geomorphic design tools for land rehabilitation based on geomorphic principles the talus royal method has been successfully applied at rock roadcuts in france génie géologique 2017 and is starting to be applied in rock highwalls at quarries this method attempts to compress time by designing and building the rock cliffs or slopes that would tend to form and evolve with time through falls and slides that occur preferentially on weathered or fractured rocks equivalent natural cliffs or rock slopes are used as analogues the rosgen 1994 1996 morphological classification of rivers based on slope width to depth ratio bed material entrenchment ratio or sinuosity is in itself a geomorphic restoration method this approach has been widely employed for perennial stream restoration in the united states including mined sites rivermorph 2017 is a design software based on the principles established by rosgen 1994 1996 rehabilitation of the canadian oil sands led to the development of a site specific geomorphic approach for designing sustainable drainage systems sawatsky and beckstead 1996 the updates and learnings since the implementation of the aoc concept were the breeding ground for the reclamation method geofluv from geomorphic and fluvial bugosh 2000 2003 geofluv is a geomorphic method for land rehabilitation that is able to reproduce the complexity of natural landforms and drainage networks within catchments which become the basic rehabilitation design units specifically it designs mature and stable stages of catchments those which would naturally form by erosional processes for the materials climate and physiographic conditions at the site this technique began to be applied at large coal mines of new mexico united states in 1999 its implementation sought to achieve long term erosional stability reduced maintenance and increased biodiversity as compared to traditional mine rehabilitation landforms e g terrace berm downdrains natural regrade is the commercial software carlson software 2017 launched in 2005 that helps users to efficiently make geofluv designs in a cad format geofluv through natural regrade has been successfully used in the united states bugosh et al 2016 and spain martin duque and bugosh 2014 zapico et al 2018 from where it is extending to the european union as a recognised best available technique bat for the management of wastes of the extractives industries jrc 2018 the method has also been employed in australia waygood 2014 kelder and waygood 2016 and over south america bugosh et al 2016 if geomorphic rehabilitation methods and software are scarce truly geomorphic landscape landform modelling tools are also rare we are not referring here to soil erosion models which are more frequent widely and usefully used in mine rehabilitation such as the universal soil loss equation usle and its derivative revised universal soil loss equation rusle or the water erosion prediction program wepp see flanagan and livingston 1995 while very useful these models do not have a fully geomorphic catchment scope they neither evolve the landforms nor directly consider gullying by fluvial erosion the latter a common erosion process in post mining rehabilitation hancock et al 2000 landscape evolution models lem represent the current best practise of geomorphic modelling technology they offer all the functionality of soil erosion models but operate on a digital elevation model dem grid they calculate both erosion and deposition at each dem grid cell and adjust elevation accordingly in this way the landscape can evolve through time tucker and hancock 2010 this allows not just erosion rates to be determined but also to map and represent where erosion and deposition occurs the models can also visually show what the form of erosion is i e sheetwash rilling or gullying hancock et al 2013 there are a number of models that can be used to assess soil erosion and landscape evolution tucker and hancock 2010 originally developed in the 1970s ahnert 1976 these models all use a dem or mesh of grid cells to represent a catchment willgoose et al 1991a b coulthard et al 2012 these numerical models employ both fluvial and diffusive erosion processes together with climate expressed in rainfall amount and intensity a summary of key attributes is described in tucker and hancock 2010 and willgoose 2018 these models are particularly useful for assessing post mine landscape designs as they can be input into the lem and allowed to evolve models such as siberia http www telluricresearch com willgoose 1989 hancock and willgoose 2017 are ideal for assessing landscapes at annual time steps and can be run up to thousands of years hancock et al 2016 hancock and willgoose 2017 report the authors use of siberia in projects in australia argentina canada namibia papua new guinea tanzania and the united states they also outlined its application by consultants globally and that it has the strongest scientific base for landform design in mine rehabilitation evans and riley 1994 willgoose and riley 1998 hancock et al 2003 other lems such as caesar lisflood coulthard et al 2012 can run at hourly time steps and can assess the effects of storm events on erosional stability a new generation of soilscape models are also now available which incorporate both spatially variable hydrology as well as soil material properties cohen et al 2009 welivitiya et al 2016 as siberia is the most widely used tool at present we focus on this model however others such as caesar lisflood could also be used here we describe the combined use of siberia and geofluv for obtaining optimal mine rehabilitation outcomes evaluated in terms of erosional stability both tools share key common characteristics 1 a catchment scale approach 2 the combined use of hillslope and fluvial geomorphic principles and algorithms 3 and the concept of maturity in landforms and landscapes again at a catchment scale despite this the joint use of siberia and geofluv is only starting to be adopted as best practice landloch 2010 waygood 2014 kelder and waygood 2016 in this framework we evaluate the complementary capabilities of two separate computer programs 1 the landscape evolution and soil erosion modelling siberia and 2 the geomorphic design software natural regrade with geofluv in order to obtain optimum stable landforms through an iterative design modelling process specifically an 11 5 ha waste dump at the drayton mine new south wales australia served as the study site this landscape built in 2013 was the first geofluv rehabilitation example in australia here four different landscapes were assessed the first goal of this research was to evaluate the long term 100 yr erosional stability using the siberia model of the geofluv landscape as built at drayton mine from this information we sought to identify the stability factors for improvement and then develop an improved geofluv design which was assessed using siberia finally the two geofluv alternatives as built and improved were compared with equivalent simulations of two landform designs with the same footprint area and approximate drainage density and waste volume contour banks the most common rehabilitation landform in the hunter valley of australia and natural contouring a landscaping approach that is becoming widely used this comparison between four landscapes dealt exclusively with erosion as the landscape footprint together with soil and vegetation factors remain constant with this procedure we seek to contribute for best practise in mine rehabilitation 1 1 study site drayton is an open cut coal mine located in the hunter valley of new south wales australia fig 1 operated by angloamerican during the period of this study the mine commenced operation in 1982 with production starting in 1983 using draglines together with truck and shovel extraction methods the mine produced approximately 8 million tonnes of coal per year mostly for export markets the mine ceased operation in 2016 in 2013 an 11 5 ha waste dump comprised of mine waste rock of sedimentary origin see fig 2 a located on a high point of the mine fig 2b was re shaped using geofluv natural regrade fig 2c this site was used as a training and demonstration project for this methodology the design had four first order a type zig zag channels rosgen 1994 1996 draining the main slope of the waste dump and three b and c type meandering channels rosgen 1994 1996 one draining the upper platform and two located at the footslopes of the waste dump the whole design had a stable base level outlet located at a sandstone ledge area see fig 2c table 1 shows the inputs used for the design bugosh com pers the construction took place between november and december of 2013 using d9 and d6 bulldozers and an excavator gps machine control systems were used to guide the earth movements a 10 cm cover of topsoil clay soil was spread in mid december 2013 the area was seeded with a mixture of grasses and trees at the end of december 2013 several rainfall events occurred during construction causing some incision at the bottom of the built first order valleys this was addressed with topsoil spreading on february 2014 an intense storm occurred and photographs march 2014 show incision at the thalwegs and rilling at the slopes fig 3 the incised areas were repaired and reseeded between may and june of 2014 photos of the area taken in september 2014 show an incipient and homogeneous herbaceous vegetation cover fig 3a the site was revisited again three years after in september 2017 showing a dense vegetation cover fig 3b which has largely stabilized the site although a series of 20 cm on average discontinuous gullies exist at the bottom of several of the built valleys fig 3c 2 methodology 2 1 landform design scenarios the landform design alternatives to be modelled and compared are two geofluv designs as built gb and improved ig and two other ones using techniques currently being applied or considered for the coal mine rehabilitation at the hunter valley in australia contour banks cb and natural contouring nc for comparison the main condition was that any design alternative would have the same footprint 11 5 ha and similar drainage density and mine waste storage capacity a maximum 10 variation was established as a threshold for this parameter the design and assessment is an iterative process with the original geofluv design modelled using siberia and any flaws then amended also with geofluv the methodology is outlined in fig 4 2 1 1 geofluv natural regrade designs the geofluv method is a landform design approach currently used for coal mine rehabilitation in the hunter valley waygood 2014 kelder and waygood 2016 geofluv is the trademark name for a specific patented landform design method that uses algorithms based on slope and fluvial geomorphic principles natural regrade is the software that helps users construct geofluv geomorphic designs the complexity of natural stable landforms and drainage networks can be reproduced in a cad format s shape convex concave slopes concave longitudinal channel profiles sinuosity indexes and patterns or progressive variation of channel cross sections with increasing flow downstream among many others the method integrates key aspects of the rosgen 1994 1996 morphological drainage channels classification mostly the aa a b and c stream types defining them based on slope width to depth ratio or sinuosity here we focus on uplands and low order drainage basins geofluv natural regrade integrates also mathematical relationships for the geometry of natural meandering channels leopold and wolman 1960 williams 1986 geofluv natural regrade aims to design mature stable catchments the state to which a catchment naturally would evolve by erosional processes to steady state stability under the climatic and physiographic conditions at the site for that a suitable and stable mature reference area has to be identified in the field to provide initial input values for the rehabilitation design bugosh 2000 2003 examples of those inputs are drainage density a channel reach length maximum distance from ridgeline to channel s heads width to depth ratios and sinuosity for different types of channels among others in addition natural regrade uses the 2 yr 1 hr average recurrence interval ari event to design the bankfull channel dimensions and the 50 yr 6 hr ari event to design the flood prone dimensions since they have been found to be mathematically related with the stream channel pattern williams 1986 the natural regrade software has the capacity to use other intensity frequency duration ifd values according to specific needs or different geographical climatic characteristics for the rehabilitation landform the input values must be site specific measured at an appropriate reference area for each project using appropriate analogue areas surface properties and local meteorological records the input values are therefore not universal ones that can arbitrarily be used at any site this is a critical point as some published papers have used words like recommended sears et al 2014 and we emphasize here the importance of obtaining appropriate site specific project area input values from local reference areas and meteorological records once the inputs are established obtaining a coordinate model of the site is the first step fig 4 the procedure of design is described in detail at the users manual of carlson software http www carlsonsw com support manuals the constructed landform geofluv as built gb was designed by drayton staff the topography of the resultant landscape was supplied as an ungridded aerial survey using light detecting and ranging lidar that was undertaken for the site this data was gridded using ordinary kriging to a regular 0 2 m by 0 2 m grid this grid size and gridding method was used for all later landforms geomorphic designs are expected to have long term stability as they immediately re establish steady state or equilibrium landforms with the local environment data from monitored geofluv based mine rehabilitations is starting to be available bugosh and epp 2015 zapico et al 2018 however in most regions there will be no such previous field validation therefore computer models i e siberia are the only method to evaluate the performance of theoretical designs fig 4 shows the feedback process to produce stable geofluv designs by their iterative simulation with siberia here for an objective comparison we normalized the gb design for volume as there was a difference in volumes between the landscapes the normalization process maintained the spatial layout of contours but equalised volume the characteristics of the gb design have been described in the study area section finally as referred the ig design was created amending instabilities predicted by the siberia modelling 2 1 2 contour banks cb linear hillslopes with contour or level banks are the default mine rehabilitation design for many areas including the hunter valley the contour banks consist of linear slopes reaching gradients around 10 interrupted by channels or drains with a bank on the downhill side constructed accurately on the contour this erosion control earthwork has been transferred from agricultural practises to mining for retarding runoff and promoting infiltration hannan 1984 these banks interrupt the slopes length at approximately each 10 m of elevation the contour bank landform was designed with the carlson software tools of polyline drawing and offsetting following the design principles by hannan 1984 2 1 3 natural contouring nc of surrounding natural terrain as described in the introduction there is a global trend in mine rehabilitation of moving from current linear graded engineered landforms to more natural and complex ones this is promoting all types of imitations of the shape of natural landscapes with little consideration for geomorphic principles common approaches are to try to arbitrarily resemble the pre mine topography or to imitate the surrounding terrain of the mined lands therefore we considered this pre mine topography as a viable alternative the natural contouring that we used was faithful to the definition since literally we fitted the topography of a natural landform from within the mine lease to the waste dump area matching its configuration volume and slope gradient and approximate slope length the procedure was 1 identifying areas surrounding the mine with the same approximate slope and shape of the rehabilitated area subject of study 2 extracting the original natural contours by the intersection of a polyline that defines the rehabilitation area 3 pasting and fitting the contours at the rehabilitation site with base height corrected 2 2 landscape evolution assessment here we focus on the siberia model which has been employed extensively for agricultural and post mining landforms evans and loch 1996 willgoose and riley 1998 hancock et al 2000 2008 2010 martinez et al 2009 siberia mathematically simulates the geomorphic evolution of landforms subjected to fluvial and diffusive erosion willgoose et al 1991a b c d willgoose 2018 the model employs well accepted hydrology and erosion models at annual and longer time scales the sediment transport equation of siberia is 1 qs qsf qsd where qs m3 s m width is the sediment transport rate per unit width qsf is the fluvial sediment transport term and qsd is the diffusive transport term both m3 s m width the fluvial sediment transport expression qsf is based on the einstein brown equation and models incision of the land surface and can be expressed as 2 qsf β1 qm1 sn1 where q is the discharge per unit width m3 s m width s metre metre the slope in the steepest downslope direction and β1 m1 and n1 are calibrated parameters diffusive erosion qsd is 3 qsd ds where d m3 s m width is diffusivity and s is slope the diffusive term models smoothing of the land surface and combines the effects of creep and rain splash siberia does not directly model runoff but relates discharge to area a draining through a point as 4 q β 3 a m 3 where β3 is the runoff rate constant and m3 is the exponent of area m3 s m width the advantage of siberia over more traditional erosion models i e rusle wischmeier and smith 1978 lies in its use of digital elevation models for the determination of drainage areas and representation of catchment topography as well as its ability to adjust the landform elevations with time in response to the erosion and deposition that occurs a more detailed description of the principles underpinning the current version of siberia can be found in willgoose et al 1991a b c willgoose 2005 and willgoose 2018 2 2 1 calibration of siberia all soil erosion and landscape evolution models require input parameters specific for the site siberia is no different the most important parameters relate to the sediment transport equation equation 2 for calibration the most rigorous method is to use field plots where all rainfall runoff and total sediment load is collected for a significant number of storm events these plots should also be maintained for a number of years as it has been found that sediment loads on post mining landscapes can rapidly reduce through time hancock et al 2016 however the installation and maintenance of field plots is costly and time consuming and rare at mine sites an alternative method is to use high resolution surveying of representative hillslopes with each method having advantages and disadvantages laser scanning digital photogrammetry both these can be impaired by vegetation and in recent years airborne lidar are becoming increasingly available repeated scans can be used to assess type i e rilling gullying sheetwash and differencing one scan from another allows volumetric assessments to be made and erosion rates calculated however at this site and typical of most mine sites there are no field plots or survey data available that could be used for calibration here we calibrate by using historical data sediment transport theory and current measurements for the site it is well known that the values of m1 and n1 equation 2 vary widely but for most fluvial systems they both range between 1 and 3 kirkby 1971 however n1 has been measured to be as low as 0 5 in mining applications due to surface armouring willgoose and riley 1998 willgoose and sharmeen 2006 with everything else being equal steeper slopes developing coarser less erodible surfaces than flatter slopes e g see the area slope diameter plots of cohen et al 2009 and welivitiya et al 2016 as there were no data available for the site we employed a set of parameters derived for similar materials at another local mine site rixs creek hancock et al 2008 to calibrate siberia a fitting process was conducted where β1 was held constant and m1 and n1 adjusted until the form and position of erosion matched the field observation at the site this process found that values of m1 2 5 and n1 2 provided the best match to the available field data these value of m1 1 5 2 5 and n1 1 2 for rixs creek are within the range of values for fluvial process dominated catchments suggested by kirkby 1971 assuming a spatially uniform sediment production rate therefore the parameters are what could be reasonably expected for the site and materials examined here soil erodibility β1 is recognised to be well described by the rusle k factor which can be determined from the material particle size distribution evans and loch 1996 sheridan et al 2000 hazleton and murphy 2007 here five individual soil cores 100 mm deep and 65 mm diameter were collected from representative positions footslope midslope and top of slope on the reconstructed mine landform a depth of 100 mm was used as this was the average depth of topsoil that was placed over the waste rock the particle size distribution psd of the five samples was determined by sieve and hydrometer methods with all five samples having similar particle size average sand 51 silt 9 clay 40 range sand 50 54 silt 7 11 clay 39 42 using the soil particle size classification and k factor soil erodibility wischmeier and smith 1978 table of hazelton and murphy 2007 the material can be classified as a clay to which we have assigned a k factor of 0 01 this k factor can be input into the siberia model assuming the surface has an absence of vegetation willgoose 2012 for many sites where there is bare earth or where the site has been degraded with little or no vegetation i e mine sites with a bare non vegetated surface or a surface with vegetation removed by fire discussed later this erodibility k value can be used directly in the model however many sites have a rock cover or armour in the case here a good grass cover exists after three years post rehabilitation similar to the rusle k factor the rusle c wischmeier and smith 1978 factor can be used to determine the expected erosion reduction due to vegetation there is quite a lot of data on the role of vegetation and a c factor can be directly determined from tables wischmeier and smith 1978 blanco and lal 2008 from a variety of sources here we use a c value of 0 02 which represents a stand of dense sod like grass blanco and lal 2008 the siberia β1 value is then determined by multiplying the k value by the c value 0 0002 bulk density was calculated from the volume and mass of the cores described above 1 56 t m3 it should be noted that these values are estimated values only at this site it is not possible to validate the parameters as there is no field plot or survey data available therefore the erosion rate here is indicative only however the parameters are very close to the value determined for the nearby rixs creek mine site hancock et al 2008 which had an absence of vegetation and similar rilling and gullying to that of the surface examined here therefore the similarity of the parameters at this site to that determined by independent means of rixs creek provides confidence in the methods and data 2 2 2 siberia simulations the siberia model was run for all four landscapes using the parameters described above as all post mining landforms have a bare surface devoid of vegetation siberia was run for an initial period of three years with an erodibility representing a bare surface β1 0 01 this three year period represents an initial high erosion rate and allows drainage lines to rapidly form at three years β1 was changed to represent a fully vegetated surface β1 0 0002 and the simulation continued for 100 years this 100 year period while not geomorphic time is within the human management time period and allows any landscape design strengths and weaknesses to be identified it also represents the period of most rapid development of a new landform to assess erosion rates the dem of difference dod approach was used where the reconstructed landscape at year 100 was subtracted from the initial landscape at year 0 this approach also allows maximum depth of erosion in this case gully depth as well as depth of deposition to be determined 3 results and discussion 3 1 landform design scenarios siberia calibration and simulations fig 5 displays the four landform designs a geofluv as built gb b improved geofluv ig c contour banks cb d natural contouring nc table 2 shows the footprint and volumetric characteristics of all the modelled designs fig 6 shows both the rilling at the interbasin areas of the geofluv as built landforms and a sub section of the 2014 landscape dem gridded to 0 2 m after one year of erosion using the siberia lem demonstrating that the pattern of modelled rilling matched that observed at the site finally fig 7 shows the 100 yr siberia modelling for the four landform design scenarios 3 2 long term landform stability erosion rates erosion values obtained for the four studied alternatives range between 25 6 and 13 9 t ha yr ranked from higher to lower erosion rates they are cb gb nc and ig table 3 if erosion rates would be the only factor to be judged for their selection as rehabilitation alternatives perhaps they all could be acceptable as the values are not high compared to other australian disturbed landscape systems i e tilled agricultural fields bui et al 2011 and certainly they are much lower than poor standard mine rehabilitation in other regions of the world i e spain where martín moreno et al 2018 found that erosion rates can be an order of magnitude higher also for comparison the australian queensland department of mines and energy uses a range of 12 40 t ha yr as a target erosion rate for rehabilitated mine sites welsh et al 1994 williams 2000 elliott and dight 1986 in kelder and waygood 2016 state that natural landforms in the hunter valley baseline are expected to erode between 0 4 and 11 8 t ha yr the ig design has the lowest erosion rate 13 9 t ha yr and the highest waste volume storage 2 465 522 m3 an additional analysis of this ig design showed that 77 7 of the eroded material is deposited within the first order subcatchments this means that the real sediment yield value sediment exiting the catchment is 3 1 t ha yr for a global comparison there are only two geofluv based rehabilitation mine sites worldwide that have been monitored in terms of sediment yield at the la plata coal mine in the semi arid environment of new mexico united states bugosh and epp 2015 measured 8 3 t ha yr of sediment yield for a geofluv natural regrade rehabilitation with topdressing and poorly established vegetation and 5 7 t ha yr for a geofluv natural regrade rehabilitation with topdressing and significant vegetation establishment compared with 9 5 t ha yr for a neighbour undisturbed native site zapico et al 2018 measured 4 0 t ha yr of sediment yield at a geofluv natural regrade rehabilitation of a kaolin mine el machorro located in a temperate continental mediterranean environment of central spain the cb design did not have very high erosion rates compared to other traditional mine rehabilitation solutions worldwide see martín moreno 2013 for a compilation of references on this issue the obtained values 25 6 t ha yr are within what could be considered acceptable in some parts of australia 12 40 t ha yr for mine rehabilitated sites as commented before this is also in agreement with what gyasi agyei and willgoose 1996 found demonstrating that contour banks were more stable than linear slopes without them 3 3 erosion process identification and geomorphology in general rehabilitated areas with contour banks in active coal mines of the hunter valley show a broad acceptable erosive performance however there are three key aspects to consider here i failures gullying occur randomly due to inevitable overtopping of the channel behind the banks when the storage capacity is exceeded this can be due to a rain event with a higher intensity than that used for the design or most commonly by progressive infilling of those channels ii when contour banks fail they tend to trigger fewer but bigger gullies than without contour banks due to runoff concentration in such drainage lines iii although it is not a direct issue related with this research contour banks do not fulfil the best possible hydrologic ecologic and visual integration with the undisturbed surrounding landscapes as far as the 100 yr modelling of the gb landforms is concerned they showed two long term erosion issues i runoff was not properly split in subcatchments at the upland areas of the former design which produce excess runoff towards the slope catchments increasing the potential of gullying due to runon this is a common issue for rehabilitated landscapes with siberia modelling mapping and quantifying the process ii long term failure of the constructed earth bank at the base of the structure that encloses the perimeter meander channels the forecast of these gullying processes allows them to be considered in future designs this was performed through an iterative process of geofluv natural regrade design and siberia modelling fig 4 it could be argued that the same iterative process could be used for contour banks but for this landform there is not literally much room for topographical improvement outside the spacing and dimension of the banks whereas the geofluv landforms can be largely changed in topography until a stable design is reached since overtopping is inevitable in the long term contour banks are a feasible solution while maintenance is guaranteed this may be acceptable for instance if there are economic use of the post mining land but they are an option that requires on going maintenance and associated costs for this study site the nc design had a moderate erosion rate 21 7 t ha yr and developed gullies this does not occur on the pre mine landscape that has developed over geological time fig 5d and has structure soil horizons of the soil and substrata this suggests the unsuitability of arbitrarily trying to imitate the pre mine topography or that of the surrounding terrain of a mine as a rehabilitation landform alternative this approach can be well intentioned but lacks geomorphic basis unfortunately the authors have seen an increasing use of nc approaches a geomorphic approach to mine rehabilitation should not be a matter of simply looking like a natural landform it must be functionally stable often looking natural at the beginning may lead to widespread erosion as found here therefore this approach is not the best option given the site and material constraints 3 4 spatial patterns in drainage and gullying iterative modelling of geofluv designs with siberia allowed identification of critical issues leading to instability and gullying the most critical one was recognising when the 0 order subcatchments and swales were not correctly designed in these situations siberia was able to predict gullying for runoff trajectories fig 8 this observation lead to another important observation that some gullying may occur in the 0 order subcatchments swales of the geofluv designs and that this would not be a problem if the average erosion values are not high the key issue here is that geofluv designs with runoff split into small subcatchments add spatial predictability to erosion lines whereas critical erosion problems arise when gullying development is not controlled or predicted drainage network development is a chaotic process but if an initial drainage pattern is imposed some predictability should be imposed on the eroding system willgoose and riley 1998 p 257 and this is what the geofluv designs produce predicting the gullying prone drainage lines therefore the use of a lem is key with siberia highlighting areas of high erosion particularly gullies the geomorphic design can subsequently reduce it we interpret the fact that the gb even with some deviations in the design and in the building process and nc even when not recommended experience less erosion than cb because both gb and nc include valleys in the landform those valleys add drainage predictability and represent more mature landforms than cb so that they experience less modification by earth surface processes this reflection has been well explained by toy and chuse 2005 p 30 as the adjustments necessary to establish a steady state decrease the prospect for reclamation success increases and the demand for post reclamation site maintenance decreases that we argue here about the favourable conditions in terms of erosion of the natural contouring alternative and the fact that we do not recommend it may seem to be a contradiction but it is not this natural contouring solution did not produce high instability at this study site but a similar approach in another context could be very unstable introducing high unpredictability science based design and modelling tools e g the quel model ibbitt et al 1999 willgoose 2001 should always be involved in landform design processes following the same reasoning one of the problems with the contour banks alternative is that the spatial prediction of gullying formation and evolution is low a visual interpretation of the pattern of gully formation for the contour bank solution see fig 7c shows a dendritic organization of the gully network with multiple captures between rilled and gullied microcatchments leading to a chaotic and unpredictable drainage network development process this is also reflected in higher depths of the gullies for contour banks table 3 this gullying unpredictability can become more critical under extreme rainfall the gullying resulting from contour banks failures is a symptom of the adjustments of the geomorphic system trying to redevelop a new drainage network which has been completely obliterated by a linear slope whereas the other alternatives have an initial drainage network and within those three alternatives with a drainage network the main improvement in the ig landform compared with geofluv gb which performs similarly to other landforms in terms of erosion rate has been the proper splitting of the runoff in concave 0 order subcatchments avoiding long term failures and unpredicted gullied catchments captures 3 5 additional considerations the geofluv geomorphic rehabilitation at this mine 11 5 ha was a demonstration site we maintained the exact footprint 11 5 ha and similar characteristics waste volume and drainage density for all the designs compared to the gb solution but it has to be stressed that for a solution to be integrated at a mine scale for a typical hunter valley coal mine there is no need to have such a high drainage density as the one that was built 158 m ha the reason is that the drainage density of the reference area is lower 90 20 m ha see table 1 high drainage densities imply short slopes a 60 m slope length is typical of geofluv designs for 158 m ha which add difficulty and extra expense in construction there are also difficulties in fitting the channel convexity 40 m of distance of ridge to head of channel in this case as identified in the reference areas meaning that most of the slopes would be convex slope lengths longer than 100 m which fit well within a drainage density of 90 m ha would be more reasonable and feasible it is not easy to predict how a more realistic drainage density for all four designs according to reference area 90 m ha would affect the outcome and the interpretation of the results which are presented here theoretically geofluv designs would be even more stable since a lower drainage density would mean more concave reaches at the base of the slopes whereas a lower drainage density in contour banks less length of banks per hectare would imply theoretically more instability but this reasoning is only applicable circa 90 m ha because lower values in geofluv designs would mean gullying until reaching that 90 m ha of equilibrium state the results also demonstrate that the geofluv designs and the most common landform rehabilitation solution of the hunter valley contour banks can store approximately the same volume of waste see table 2 specifically the gb stores 12 3 more of waste volume that contour banks and the improved ig solution is able to store 7 2 more but in this latter case having about half of erosion rate 4 conclusions here we demonstrate and assess the first systematic integration of the use of geomorphic design geofluv natural regrade and assessment using a landscape evolution model siberia unstable design issues were identified using siberia and removed using an iterative process until both erosion was reduced and landscape volume optimised the improved geomorphic rehabilitation shows high erosional stability but would be expected to perform better if more design and modelling iterations would be performed the siberia modelling was critical for identifying four main causes of instability that were turned into stability by guaranteeing in subsequent designs a appropriate concavity at any foothill transitioning towards the channels b that runoff is always directed towards the swales 0 order subcatchments c a correct design of catchments from the top of the landscape to streamline d long term stability of the main valley meandering channels by constructing entire subcatchments the consideration of these factors in the geofluv designs produced successively lower erosion rates for each landscape iteration as a main conclusion for the coalfields of the hunter valley the improved geofluv design using siberia modelling reduced erosion by half while being able to store 7 more mine waste volume than contour banks additionally the gullying pattern was predicted by the landscape evolution model and 77 of the eroded material was predicted to be deposited within the first order subcatchments further reducing significantly sediment yield additionally while not being the main purpose of the research a cad based procedure has been developed to extract the natural contouring nc of undisturbed lands adjacent to mines drayton in this case as a theoretical landform design alternative this procedure has been strictly developed for scientific purposes and authors do not recommend this landform approach be used as an alternative for mine rehabilitation without careful geomorphic assessment in conclusion the joint use of geomorphic design software with a landscape evolution model showed complementary capabilities for optimised landform design through an iterative design process and landscape evolution modelling optimised geomorphic designs can be reached supplementary to this research compared assessments between geomorphic and traditional landform designs using economic hydrologic ecologic and visual issues should provide both the mining industry and regulators with improved rationale for decision making this is important as the public is demanding much higher standards of mine rehabilitation acknowledgements this paper is the outcome of a joint research of the three authors made possible by means of a research stay of j f martín duque hosted by g r hancock and g r willgoose at the university of newcastle uon australia this stay has been funded within a program estancias de movilidad de profesores e investigadores en centros extranjeros de enseñanza superior of the spanish minister of education culture and sports reference prx16 00441 the authors acknowledge the kind collaboration of the tom farrell institute for the environment of the uon drayton angloamerican has kindly supported this research by providing the geofluv as built topography along with all the needed information which main aim has been getting knowledge in order to develop best mine rehabilitation practises the assistance of matt lord and team is highly appreciated this manuscript is also a contribution under the joint research of the ecological restoration network remedinal 3 of the madrid community s2013 mae 2719 nicholas bugosh also provided useful information and comments finally two anonymous reviewers and francis rengers made very constructive comments at the review process 
