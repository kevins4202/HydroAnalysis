index,text
26110,it is necessary to develop a flexible and extensible watershed modeling framework with the support of parallel computing to conduct long term high resolution simulations over large areas with diverse watershed characteristics this paper introduced an open source modular and parallelized watershed modeling framework called seims short for spatially explicit integrated modeling system to meet this need first a flexible modular structure with standard interfaces was designed in which each module corresponds to one simulation algorithm for a watershed subprocess then a parallel computing middleware based on an improved two level parallel computing approach was constructed to speed up the computational efficiency with seims users can add their own algorithms in a nearly serial programming manner and construct parallelized watershed models seims also supports model level parallel computation for applications which need numerous model runs the effectiveness and efficiency of seims were illustrated through the simulation of streamflow in the youwuzhen watershed southeastern china keywords watershed modeling framework modular parallelization watershed process simulation seims software availability software seims spatially explicit integrated modeling system operating systems supported windows linux and macos language c and python availability seims is open sourced on github https github com lreis2415 seims 1 introduction watershed modeling is the process of building computer based simulation models to aid users in understanding watershed processes such as hydrology soil erosion and nutrient cycling and has been widely used in applications such as the flood prediction and the evaluation of watershed management practices a typical procedure for watershed modeling can be summarized as follows 1 select or customize watershed models according to the physical characteristics of the study area and the requirements of applications 2 collect and preprocess input data including both the basic data such as climate landuse soil and digital elevation model dem data and application specific data such as the environmental efficiency data for the best management practices bmps under consideration qin et al 2018 3 perform sensitivity analysis and calibration manually or automatically arnold et al 2012 rouholahnejad et al 2012 zhan et al 2013 zhang et al 2013 4 analyze the model outputs or apply the calibrated model to evaluation purposes such as decision making support based on bmp scenarios analysis maringanti et al 2009 qin et al 2018 zhu et al 2019 two major challenges need to be overcome in order to carry out the above procedures effectively and efficiently on the one hand a flexible and extensible modeling system is needed to meet varied modeling purposes since an ideal and all purpose watershed model does not exist kneis 2015 on the other hand parallel computing is required because a large amount of computation is needed by both the model itself and the evaluation applications that require numerous model runs clark et al 2017 david et al 2013 zhang et al 2013 especially given the emerging trends of increasing complexity of process based models finer spatial and temporal resolutions larger study areas and longer simulation periods freeze and harlan 1969 liu et al 2016 environmental modeling frameworks provide an effective way to address the above mentioned challenges david et al 2013 formetta et al 2014 kneis 2015 wagener et al 2001 general purpose environmental modeling frameworks define standard interfaces to couple the models for different processes which make them flexible and extensible this type of framework includes the earth system modeling framework esmf hill et al 2004 open modeling interface openmi moore and tindall 2005 community surface dynamics modeling system csdms peckham et al 2013 and so on these general purpose frameworks often provide parallel computing support for common operations such as data communications and transformation e g regridding hill et al 2004 however since they are designed for coupling the existing models for multidisciplinary applications they do not provide specific support for the parallelization of spatially explicit watershed modeling as well as for the development of new watershed models kneis 2015 watershed modeling frameworks are those environmental modeling frameworks specifically designed for watershed modeling with remarkable characteristics in terms of flexibility re usability and high performance david et al 2013 kneis 2015 buahin and horsburgh 2018 specifically these characteristics commonly include 1 adopting the object orientated design to provide standard interfaces for the extension of new modeling objects or algorithms 2 assembling existing modules or objects for specific watershed modeling task in a configurable manner 3 hiding the implementation details of the framework as much as possible and only exposing concise interfaces to facilitate rapid development and 4 supporting parallel computing inherently without needing users to handle too much parallel computing programming details object modeling system oms3 david et al 2013 formetta et al 2014 and eco hydrological simulation environment echse kneis 2015 are typical examples however the existing watershed modeling frameworks put emphasis on providing good flexibility rather than good parallel performance in oms3 the simulation of different watershed subprocesses is organized into self contained modules david et al 2013 while in echse several types of spatial units e g subbasin reach lake and node with associated variables and methods are treated as different simulation objects kneis 2015 these frameworks achieve parallel computing by executing modules or objects without dependencies concurrently and are mainly implemented using shared memory multithreaded programming approaches such as open multi processing openmp the de facto standard api application programming interface for parallel computing on shared memory machines which uses notation directives or pragmas to indicate the code region that should be executed concurrently chapman et al 2007 such shared memory multithreaded implementations cannot make good use of more scalable distributed memory parallel platforms such as the symmetric multiprocessing smp clusters which limits the scalability of these frameworks david et al 2013 kneis 2015 wenderholm 2005 in fact several parallelization strategies which can effectively utilize both smp cluster and shared memory parallel platforms have been proposed in recent years liu et al 2016 vivoni et al 2011 wang et al 2011 2013 yalew et al 2013 but have not yet been integrated into flexible watershed modeling frameworks these parallelization strategies often conduct task scheduling based on spatial discretization liu et al 2016 vivoni et al 2011 yalew et al 2013 or spatio temporal discretization wang et al 2013 the implementations are mainly based on the message passing interface mpi programming model or a hybrid of mpi and openmp models liu et al 2016 however to obtain the high efficiency offered by these parallelization strategies requires users to pay a high cost in terms of learning programming skills as well as spending lots of effort on parallel programming details such as domain decomposition task scheduling and data communication liu et al 2016 wang et al 2013 meanwhile these parallelized watershed models often lack standard and concise interfaces for extending new watershed subprocess modules according to corresponding parallelization strategies therefore it is difficult for researchers with less parallel computing experience to add new algorithms for watershed subprocesses e g infiltration to the specific parallelized watershed models especially those in which different watershed subprocesses are tightly coupled by sharing global variables or the code of parallel computing is tightly bound to specific variables or modules besides the flexible modeling structure and the parallelization of the model itself the parallel computing of model level applications such as parameter sensitivity analysis and auto calibration should also be supported by a comprehensive watershed modeling framework currently various tools have been developed for specific watershed models such as the soil and water assessment tool swat arnold et al 1998 which are conducted on multi core computers rouholahnejad et al 2012 smp clusters zhang et al 2013 or grid computing platforms zhao et al 2013 however to the best of our knowledge existing parallelized watershed modeling frameworks often lack the support for model level parallelization on multiple parallel computing platforms buahin and horsburgh 2018 therefore the community still lacks a watershed modeling framework which has a flexible and extensible modular structure and an efficient parallel computing middleware to facilitate rapid development of parallelized watershed models on distributed memory parallel platforms this paper introduces a modular and parallelized watershed modeling framework called spatially explicit integrated modeling system or seims for short to fill this gap the design of seims is introduced in section 2 and its implementation is discussed in section 3 a case study is shown in section 4 to illustrate a typical watershed modeling procedure based on seims including model construction parameter sensitivity analysis and auto calibration the computational efficiencies of the single seims based model and applications that required repeated model runs are also presented conclusions and future perspectives are given in section 5 2 design of seims 2 1 basic idea and overall design seims is currently designed to focus mainly on one type of distributed watershed models those in which both overland and subsurface flow routing and channel flow routing are conducted sequentially and follow upstream downstream orders liu et al 2014 accordingly two common assumptions liu et al 2014 2016 wang et al 2011 2013 for this type of distributed watershed models implemented in seims are presented as follows 1 a watershed can be partitioned into spatial hierarchical units from coarse to fine levels such as subbasins hillslopes slope positions also known as landforms in band 1999 and basic simulation units e g grid cells voronoi polygons or hydrologic response units fig 1 a band 1999 band et al 2000 bieger et al 2016 vivoni et al 2011 a subbasin is defined as a relative closed and independent catchment area which contains two or three types of hillslopes i e source left and right hillslope and drains to one reach or channel of the watershed drainage network fig 1a slope positions are comparatively homogeneous spatial units with physical geographic features at hillslope scale which are often too coarse to be simulation units for distributed watershed modeling and however are suitable as configuration units for the evaluation of watershed management practices qin et al 2018 zhu et al 2019 those finer spatial units with more homogeneous hydrologic responses such as grid cells and irregularly shaped fields are defined as basic simulation units fig 1a in seims on which the hillslope processes at both vertical and horizontal directions are simulated 2 a topography based flow direction is assigned to each spatial unit computing dependencies exist at different spatial levels and often accord with upstream downstream routing relationships such as overland and subsurface flow routing in layers of basic simulation units within each subbasin fig 1b liu et al 2014 and channel flow routing among subbasins fig 1c wang et al 2011 in order to function as a watershed modeling framework with high extendibility and computational efficiency seims is designed with two particular features a a flexible modular structure much like oms3 david et al 2013 seims adopts a modular structure which has the advantages of good flexibility and easy maintenance leavesley et al 2006 david et al 2013 peckham et al 2013 each module corresponds to one simulation algorithm for a watershed subprocess such as the penman monteith method for simulating potential evapotranspiration every module inherits from a standard and concise interface and exposes input and output information via metadata according to the metadata a seims based watershed model for a specific application can be constructed in a loosely coupled manner i e a seims main program dynamically assembles user configured modules according to their input output relationships so to build an application specific workflow and conducts the simulation b a parallel computing middleware a parallel computing middleware is designed to support inside model and model level parallel computation since subbasins can be treated as relative independent spatial units for watershed modeling they are suitable for candidates for being distributed among and executed by the different nodes in an smp cluster or other distributed memory parallel platforms liu et al 2016 vivoni et al 2011 wang et al 2013 wu et al 2013 yalew et al 2013 for basic simulation units inside each subbasin which often interact frequently with each other during the simulation a shared memory programming model such as openmp is suitable and efficient liu et al 2014 thus a two level parallelization strategy proposed by liu et al 2014 2016 is adopted for its exploitation of the parallelizability at both coarse grained i e subbasin level and fine grained i e basic simulation unit level hereafter referred to basic unit level for short levels with such a strategy the simulation of subbasins is treated as relative independent parallel tasks of individual subbasins each of these tasks will be dispatched to one of the multiple nodes of an smp cluster using message passing programming model for execution while for each subbasin the simulation of basic simulation units will be further dispatched to multi core within each node using shared memory programming model and then be executed with the help of the modular interface the details of parallel computing at the subbasin level could be hidden from users and the parallel computing at the basic unit level based on openmp is quite simple and requires little extra programming skill this allows users to develop parallelized watershed models easily besides the two level parallelization strategy adopted for inside model execution a model level parallelization tool based on job management is also designed to speed up those model level applications which require repeated model runs such as spatial optimization of bmp scenarios based on the basic idea above the overall architecture of seims was designed as shown in fig 2 and consists mainly of the seims module library based on the modular structure the seims main programs i e openmp version and mpi openmp version the watershed database and utility tools for watershed model applications such as parameter sensitivity analysis tool and auto calibration tool the parallel computing middleware at multiple levels is implemented at the basic unit level in seims modules the subbasin level in the mpi openmp version of seims main program and the model level in watershed model applications respectively a seims based model consists of one seims main program several customized seims modules and the watershed database each seims module inherits from the base module class i e simulationmodule with standard and concise interfaces e g setdata getdata and execute functions and dependents on base modules such as i o module fig 2 the basic unit level parallelization is achieved using openmp in the execution function of each seims module the basic version of seims main program which is the openmp version is responsible for loading a set of user configured modules to build and execute a simulation workflow the mpi openmp version of seims main program uses the subbasin level domain decomposition and task scheduling to create instances of the openmp versioned seims main program for each individual subbasins and distribute them among different computing nodes with mpi based communication so to achieve subbasin level parallel computing watershed model applications that requires numerous model runs e g sensitivity analysis of a watershed model are parallelized at model level based on job management with the support of parallel computing seims is compatible with common operating systems such as windows and linux and parallel computing platforms such as personal computers with multi core cpu central processing unit and smp clusters qin et al 2014 the detailed design of the modular structure and the parallel computing middleware in current seims is presented as follows 2 2 modular structure each module of seims module library inherits from a unified module interface fig 2 and is compiled as a separate dynamic link library file the user configured modules for a specific watershed modeling application are dynamically loaded and combined as a workflow by seims main programs at runtime to conduct the watershed simulation the unified module interface consists of four types of functions i e metadatainformation setdata and getdata checkinputdata and initialoutputs and execute fig 2 with the unified module interface users can write seims modules in a nearly serial programming manner and achieve parallel computing at both the basic unit level inside these modules and the subbasin level without taking care of the details of data communication in the mpi openmp version of seims main program 2 2 1 metadatainformation function the metadatainformation function provides the metadata of the module and the seims main program can assemble the selected modules according to these metadata the metadata mainly includes basic information about the module such as its designer and description as well as the input and output information the input and output information includes static input parameters recorded in database such as saturated hydraulic conductivity e g lines 1 6 in fig 3 dynamic input variables output from other modules such as surface runoff e g lines 7 9 in fig 3 and output variables for the module such as streamflow at each reach outlet e g lines 10 12 in fig 3 each item of the input and output information mainly includes the name unit description and data type the basic data types of seims include single float value dt single e g global parameters of the watershed 1 dimension float array dt array1d and dt raster1d e g compact rater data by unfolding 2 dimension array excluding no data values liu et al 2014 and 2 dimension float array dt array2d and dt raster2d e g soil properties of multi layers to support the expandability of complex input data with unified interfaces of setting and getting data see section 2 2 2 three complex data types are also designed in seims the first is the reach object dt reach which contains the reach related parameters e g flow routing layer task scheduling group and initial geometric parameters the second is the subbasin object dt subbasin which contains subbasin scale statistical parameters e g area and average slope the third is the scenario object dt scenario that contains bmp scenarios information e g operation schedules of plant management practices with corresponding parameters the extension of these complex objects can be implemented in the base i o modules of seims module library fig 2 since subbasins are modeled independently in the mpi openmp version of seims data transmission occurs between upstream subbasins and downstream subbasins especially for channel routing related modules this means that some variables could be output and input simultaneously hereafter referred to as inoutput variable for example in the channel flow routing module the streamflow is an output variable of the array data type with a length of subbasin count dt array1d while in the mpi openmp version the execution of this module for each subbasin needs the streamflow values from its upstream subbasins as inputs of the single value data type e g line 11 in fig 3 fig 4 shows an example of the calculation of an inoutput variable within the channel flow routing module of the openmp version and mpi openmp version of seims main programs respectively in the openmp version this inoutput variable acts as an ordinary output variable which is firstly initialized as a 1 dimension array then assigned values by the execution order of subbasins which following the order of flow routing layers and finally output as an array with valid simulated values fig 4 in the mpi openmp version which is built on the openmp version the channel flow routing module of a subbasin e g the subbasin d in fig 4 cannot be executed until the output values of this variable of all of its upstream subbasins e g subbasin f and g have been received and the value to this variable of the subbasin has been assigned correspondingly fig 4 therefore an optional argument was designed to specify the data type of transferred input and output variables e g lines 11 in fig 3 among upstream downstream subbasins which made the metadata interface compatible with both the openmp version and the mpi openmp version there are three options for this argument tf none is the default indicating that the variable doesn t need to be transferred and can be omitted tf singlevalue is used for variables of types dt array1d and dt raster1d and tf onearray1d is used for variables of types dt array2d and dt raster2d with the corresponding setting and getting operations see section 2 2 2 the values of transferred variables across subbasins can be dynamically determined after loading the required modules at runtime in the mpi openmp version see section 2 3 3 in this way users can achieve parallel computing at the subbasin level without worrying about the details of data communication in the seims main program 2 2 2 setdata and getdata functions the setdata and getdata functions are responsible for setting and getting parameters or variables listed in the metadata of one module e g fig 3 respectively the setdata functions for basic data types include setvalue set1ddata and set2ddata while the getdata functions include getvalue get1ddata and get2ddata correspondingly for complex data types only setdata functions are needed i e setreaches setsubbasins and setscenario in order to improve data transfer efficiency across modules in the openmp version array type and complex type variables are passed by memory address which means the same variable in different modules points to the same area in memory note that in order to enable the openmp version and mpi openmp version of seims main program to invoke seims modules built by the same module code the variables with certain transferred data types i e tf singlevalue and tf onearray1d should be set and gotten according to the original data type and transferred data type respectively for example the streamflow variable line 11 in fig 3 and the example in fig 4 should be gotten as dt array1d to act as an ordinary output variable for output data of the simulation or input of other modules and should also be set as dt single for receiving the simulated value of this variable from its upstream subbasin and gotten as dt single for its downstream subbasin 2 2 3 checkinputdata and initialoutputs functions the checkinputdata function is responsible for checking the validity of input data for example it verifies that the input array exists and is accessible and raises an exception if any unavailable or illegal data exists the initialoutputs function is used for creating and initializing output variables and temporary variables of the current module so that the output variables required by other modules as inputs are valid 2 2 4 execute function the execute function as the core of a module is for the simulation of a specific watershed subprocess within a time step the outer loop of time steps is handled in seims main program the nested time steps between channel groundwater routing processes and hillslope processes are supported which means the time step of hillslope processes can be finer than channel groundwater routing processes fig 5 generally the customized modules of one seims based model can be divided into three categories which should be executed sequentially i e driver factor related modules e g reading and preprocessing of climate data hillslope processes modules and channel groundwater routing modules fig 5 the inner loop in the execute function is to perform simulation on basic simulation units for hillslope processes and subbasin units or reaches for channel groundwater routing processes see section 2 3 2 2 3 parallel computing middleware decomposing a modeling issue into tasks that can be executed concurrently is the first step to achieving parallelization foster 1995 the parallel computing middleware of seims implements two types of parallelization strategy i e parallelization of inside model execution based on the spatial discretization of the watershed section 2 3 1 2 3 3 and model level parallel job management section 2 3 4 2 3 1 domain decomposition of watershed the two level domain decomposition method proposed by liu et al 2014 2016 is adopted to utilize the parallelizability at both the subbasin and basic unit levels subbasins are partitioned into layers via either an upstream downstream strategy fig 6 b or a downstream upstream strategy fig 6c the subbasin layers are treated as a graph in which one node represents a subbasin and the edges represent the upstream downstream relationships unlike the binary tree code adopted by wang et al 2011 the graph structure adopted in seims can support subbasins with more than two upstream subbasins each node is assigned a computing weight e g by subbasin area in the current version which is useful to account for load balancing in task scheduling liu et al 2016 see section 2 3 2 taking grid cells as an example of basic simulation units fig 7 shows the division of basic simulation units into layers using an upstream downstream strategy fig 7b or downstream upstream strategy fig 7c according to flow directions fig 7a the current version of seims adopts the commonly used d8 single flow direction algorithm o callaghan and mark 1984 while multiple flow direction algorithms such as mfd md algorithm qin et al 2007 could also be supported in future versions in addition support for using irregularly shaped fields such as hydrologic response units arnold et al 1998 and patches tague and band 2004 as basic simulation units is under development 2 3 2 parallelization at the basic unit level at the basic unit level e g grid cells in current version of seims simulation methods for watershed processes can be divided into two types according to their computational characteristics i e local independent methods such as runoff generation processes and plant growth simulations and sequential dependent methods such as overland flow routing using 1 d kinematic wave method liu et al 2014 for local independent methods parallel computing can be conducted simply by dividing simulation units into equal parts while for sequential dependent methods parallel computing can be achieved in the same layer divided by flow directions fig 7 in which there are no upstream and downstream relationships between the corresponding simulation units the openmp programming model is used in current version of seims which makes it quite easy to achieve parallel computing in most circumstances by embedding compiler directives start with pragma in the source code without damaging the original code structure fig 8 other circumstances which need a few more openmp compiler directives such as reduce operations on array type data according to subbasins can be found in https github com lreis2415 seims issues 36 2 3 3 parallelization at the subbasin level seims adopts the mpi programming model to conduct parallel computing at the subbasin level to make the best use of computational resources load imbalance should be reduced by distributing the amount of computation in each computing node as evenly as possible while minimizing the amount of communication e g try to dispatch subbasins with upstream downstream relationships to the same node to reduce communication overhead liu et al 2016 to achieve such a tradeoff between load balance and communication overhead current seims adopts the widely used metis graph partitioning algorithm karypis and kumar 1998 for static task scheduling with the graph of subbasins fig 6b and c as input and estimates the computing weight of each node roughly by subbasin area liu et al 2016 the master slave strategy for task scheduling and data communication which was commonly used in previous studies liu et al 2016 wang et al 2011 may cause unnecessary communication overhead for example with the master slave strategy the transferred values between upstream and downstream subbasins will be sent and receive twice i e from slave processes to master process and from master process to another slave process therefore a static peer to peer communication strategy is designed to ensure that the transferred values only need to be sent and received once from a process to another fig 9 shows the pseudo code of the static task scheduling strategy from the perspective of spatial discretization at the subbasin level the predetermined task scheduling plan is loaded by the master process and then broadcasted to all processes lines 1 6 in fig 9 which makes the peer to peer communication available then each process is responsible for creating and executing the task of subbasins assigned to it lines 7 18 in fig 9 the execution workflow of one subbasin object within a channel routing time step based on the peer to peer communication strategy is described by the pseudo code in fig 10 the hillslope processes and the channel routing processes can proceed separately one after another lines 2 and 11 in fig 10 respectively the transferred data from upstream subbasins should be updated by reading data from the current process or receiving from another process before the execution of channel routing processes lines 3 10 in fig 10 after that the transferred data derived from the current subbasin should be saved for its downstream subbasin if such a subbasin exists lines 13 19 in fig 10 if the downstream subbasin is not assigned to the current process the transferred data will be sent directly to another process which handles the downstream subbasin line 15 in fig 10 finally the specified output data of the current subbasin will be saved line 20 in fig 10 2 3 4 parallelization at the model level parameter sensitivity analysis zhan et al 2013 auto calibration rouholahnejad et al 2012 zhang et al 2013 and scenario analysis based on optimization algorithms qin et al 2018 are common applications which need numerous model runs with different model inputs each model run is independent of others and thus the parallelism at the model level exists although the results of a series of model runs are summarized note that although the multiple model runs can be executed concurrently using multithreading programming techniques on shared memory machines e g rouholahnejad et al 2012 the computational efficiency of each model on individual thread may degrade due to the contention for shared hardware resources especially when the number of threads exceeds the number of processors therefore the model level parallelization is suitable to be conducted through parallel task distribution on distributed memory parallel computing platforms e g smp cluster grid computing platform by job management zhao et al 2013 or mpi zhang et al 2013 current seims provides the model level parallelization based on job management 2 4 watershed database the watershed database for the seims based model is designed to store all data related to watershed modeling such as the input and output data the parameter adjusting data for sensitivity analysis and so on the input data include but are not limited to climate data e g precipitation and meteorological data such as temperature geospatial data e g dem and the derived spatial parameters landuse map soil type map and soil attribute data and management data e g plant management schedules depending on the purpose of watershed modeling two types of output data can be specified by a configuration file i e time series outputs e g discharge at the outlet of the watershed and spatio temporal distribution variables e g the average of soil water storage the output data can also be exported as single files e g plain text and geotiff files all spatial input data are decomposed into small parts according to subbasin boundaries and stored in the database as binary types during data preprocessing so as to improve i o efficiency liu et al 2016 for the execution of the openmp version of the seims based model data for the whole watershed are stored as well with the flexible watershed database design there is no need to duplicate the input data when conducting numerous model runs differentiated by calibrated parameter settings or scenario settings a measure which is required by some other watershed models such as swat zhang et al 2013 3 implementation of seims 3 1 overall implementation the proposed modular and parallelized watershed modeling framework namely seims was implemented using standard c and python programming languages which means it is cross platform compatible seims uses cmake https cmake org a cross platform build tool to manage the entire project in order to achieve compatibility on mainstream compilation environments the compiled c programs include the seims main programs including the openmp version and the mpi openmp version seims module library and executable programs for data preprocessing python is used for utility tools including data preprocessing calibration sensitivity analysis scenario analysis and so on to simplify the implementation a parallel job management strategy based on scoop hold geoffroy et al 2014 using python language was adopted for the current seims scoop automatically distributes tasks among available computing resources using dynamic load balancing hold geoffroy et al 2014 which means it has the potential to be compatible with multiple computing platforms by drawing lessons from the wetspa model water and energy transfer between soil plant and atmosphere liu et al 2003 liu 2004 the swat model arnold et al 1998 and other watershed models or algorithms current seims module library can support the simulation of hydrology soil erosion and nutrient cycling processes for storm event models liu et al 2014 2016 wu et al 2018 and long term e g daily models qin et al 2018 zhu et al 2019 with the parallel middleware of seims at subbasin level and its standard module interfaces modelers need not take care of programming details of mpi and can use serial programming code to develop their own seims modules achieving subbasin level parallelization with high parallel efficiency see section 2 3 3 furthermore modelers can conveniently achieve the openmp based parallelization at basic unit level through programming with a fairly low learning curve in most cases without damaging the serial code structure see section 2 3 2 please refer to the user manual document https github com lreis2415 seims blob master seims usermanual pdf for more detailed information on programming with seims such as the demo of developing a new seims module given the requirements of flexible data structures elastic scalability and high performance a widely used nosql database mongodb https www mongodb com was adopted to manage all kinds of data in the current version of seims seims is still under continuous development please visit https github com lreis2415 seims for more details and latest updates 3 2 utility tools the data preprocessing tools which were written in python and c languages include four categories of functions i e watershed spatial discretization spatial parameters extraction hydroclimate data processing and watershed database management the watershed spatial discretization functions include the functions of delineating spatial units at different scales fig 1a such as subbasins implemented based on taudem v5 3 7 tarboton 2016 hillslopes hydrologically connected fields wu et al 2018 and slope positions zhu et al 2018 and the functions of spatial domain decomposition of the watershed at subbasin and basic unit levels see section 2 3 1 the spatial parameters extraction functions mainly include the calculation of terrain and location related parameters such as depression capacity and the mapping of soil and landuse landcover related parameters based on lookup tables the hydroclimate data processing functions mainly refer to the data formatting such as interpolating irregular precipitation records to be regular time interval data and the statistical calculation such as annual number of heat units the watershed database management functions are responsible for creating the database and importing data with two types of formats the first is spatial parameters unfolded as a 1 dimension array and stored as binary data and the second is plain text data organized and stored as the format of key value pairs i e json format a main python script is provided to perform the whole workflow of data preprocessing present data preprocessing tools in seims are implemented to support the currently available seims modules this means that additional tools should be added if the input data of a newly developed module is not available from the database or outputs of other modules in seims parameter sensitivity analysis is useful for identifying the most important or influential parameters for a specified simulation target e g discharge of the watershed outlet zhan et al 2013 a typical sensitivity analysis procedure includes sampling from the value ranges of determined model inputs evaluating the model with the generated samples and saving the interested outputs and calculating sensitivity indices by various methods such as the morris screening method morris 1991 and fast fourier amplitude sensitivity test cukier et al 1978 the sensitivity analysis tool in seims is organized as separated functions according to these steps which means it is easy to incorporate any sampling method and sensitivity analysis method the most time consuming step is repetitive model evaluations which is parallelized at model level by parallel job management based on scoop like the parameter sensitivity tool auto calibration and scenario analysis tools also need to generate many different model inputs evaluate models and perform analysis based on model outputs in the current seims version the commonly used non dominated sorting genetic algorithm nsga ii deb et al 2002 was integrated as the auto calibration and scenario analysis tool the support for other popular optimization algorithms is on the development plan running all utility tools requires a corresponding configuration file in which the application related parameters are specified seims provides different templates for the configuration files for data preprocessing parameter sensitivity analysis auto calibration etc more details please refers to the user manual document https github com lreis2415 seims blob master seims usermanual pdf 4 case study 4 1 study area and dataset the youwuzhen watershed 5 39 km2 which is located in changting county of fujian province of china was selected as the case study area fig 11 this area belongs to the typical red soil hilly region in southeastern china and suffers from severe soil erosion chen et al 2013 the study area has hills with steep slopes up to 52 9 and with an average slope of 16 8 and broad alluvial valleys qin et al 2018 the elevation ranges from 295 0 m to 556 5 m the study area is characterized by a mid subtropical monsoon moist climate and has an annual average temperature of 18 3 c the annual average precipitation is 1697 0 mm and intense short duration thunderstorm events contribute about three quarters of annual precipitation from march to august chen et al 2013 the land use types are mainly forest 59 8 paddy field 20 6 and orchard 12 8 the dominant soil type is red earth humic acrisols in fao soil taxonomy or ultisols in us soil taxonomy qin et al 2018 the basic data of the youwuzhen watershed collected for watershed modeling include spatial data i e a gridded dem with 10 m resolution a soil type map with a scale of 1 50 000 and a land use type map interpreted from alos advanced land observation satellite images from 2009 climate data and periodic site monitoring streamflow and sediment discharge data collected at the watershed outlet qin et al 2018 according to an accumulated threshold of 0 185 km2 chen et al 2013 the youwuzhen watershed was delineated into 17 subbasins fig 11 qin et al 2018 soil properties such as mechanical composition and organic matter were derived from field sampling data chen et al 2013 soil erodibility factors cover management factors and conservation practice factors of the usle universal soil loss equation model were drawn from the study conducted in the same area by chen and zha 2016 climate data contains daily meteorological data derived from national meteorological information center of china meteorological administration data and precipitation data derived from local monitoring stations the periodic site monitoring data regarding streamflow and sediment discharge at the watershed outlet from 2012 to 2015 were provided by the soil and water conservation bureau of changting county fujian province china limited by the quality of this data those periods of more than three consecutive days of rainfall with complete records of runoff generation and sediment yield data were selected for modeling as a result the year 2012 was selected as a warm up period for the seims based model the years 2013 and 2014 for calibration and the year 2015 for validation 4 2 construction of the seims based model in this case study seims was used to construct a grid based watershed model for simulating streamflow and sediment discharge at a daily time step for the period from 2012 to 2015 according to the available dataset the simulated hydrologic processes included interception surface depression storage surface runoff potential evapotranspiration percolation interflow groundwater flow and channel flow soil erosion by water on hillslopes and sediment routing in channels were simulated to be a comprehensive watershed model the simulation of plant growth process and nutrient i e nitrogen and phosphorous cycling were also included the combination of adopted modules is the same as that used and manually calibrated by qin et al 2018 which allows for a comparison between the manual calibrated model and the auto calibration conducted by seims see section 4 3 in the study of qin et al 2018 and the follow up study of zhu et al 2019 the calibrated seims based watershed model with a daily time step was used to evaluate the reduction rate of multi year average soil erosion under different bmp scenarios the combination of adopted modules is prepared as a list of module names in a plain text file which will be parsed by the seims main program in order to assembling the corresponding modules as described in section 2 2 for more details about the corresponding simulation algorithms please refer to qin et al 2018 4 3 experiment design 4 3 1 experimental environments a high performance computing platform with 134 computing nodes and a general parallel file system gpfs was used each computing node was equipped with 2 way intel xeon e5650 6 cores cpus i e 12 physical cores in total 24 gb memory and one infiniband 40 gb s network card used for high speed interconnection of parallel computing jobs the red hat enterprise linux server 6 2 was used as operation system the compilation environment of seims included intel c 12 1 compiler with the support of openmp 3 1 intel mpi library 4 0 3 gdal 1 11 5 and mongo c driver 1 6 1 the virtual environment for running seims utility tools was python 2 7 13 with several third party packages such as gdal 1 11 5 numpy 1 12 1 matplotlib 1 5 3 pymongo 3 4 0 deap 1 2 salib 1 1 2 and scoop 0 7 4 3 2 experiment for evaluating parallel performance of seims based model the simulation results of the openmp version and the mpi openmp version were confirmed to be the same before the parallel performance of the seims based model was tested according to the parallelization of seims at the subbasin level the maximum number of processes should not exceed the count of subbasins i e 17 in this case study therefore the openmp version and the mpi openmp version were executed separately using the same number ranging from 1 to 16 of threads and processes using one thread per process respectively for convenience of comparison for the mpi openmp version executions using 2 3 4 6 and 8 threads per process were also tested the actual simulation time excluding the time for data input and output i o is regarded as computing time in this study for the mpi openmp version the computing time is the maximum computing time among all processes each execution was repeated three times to get an average computing time as the result for evaluation the speedup ratio which is defined as the ratio between the serial computing time and the parallel computing time was used to measure the parallel performance for the openmp version the serial computing time was the computing time of the execution using one thread while the computing time of the execution using one process and one thread per process was regarded as the serial computing time for the mpi openmp version the theoretical maximum speedup ratio tmsr was also estimated by the method proposed by liu et al 2013 which is suitable for grid based distributed watershed models the proportion of computing channel routing processes in the total amount of computations was set as 1 for estimating tmsr 4 3 3 experiment of parameter sensitivity analysis in seims the most important step for parameter sensitivity analysis is to determine the involved parameters and their proper change ranges according to prior knowledge zhan et al 2013 the simulation of streamflow in a watershed model compared with that of sediment and nutrient is often the priority in sensitivity analysis and calibration abbaspour et al 2015 therefore this study takes the simulation of streamflow as an example to illustrate the sensitivity analysis and auto calibration by seims a total of 23 parameters related to hydrological processes were selected table 1 lists the basic information such as description absolute maximum and minimum values and default initial value the value changing method and the range of each selected parameter the morris screening method morris 1991 implemented seims was used in this study to qualitatively identify important parameters which impact the model performance of streamflow at the watershed outlet although the morris screening method cannot provide an accurate quantitative estimate of how much a parameter contributes to the model performance indicators or distinguish the non linearity of a parameter from the interaction with others yang 2011 zhan et al 2013 it can be used to exclude non sensitive parameters through one at a time sensitivity analysis for a subsequent quantitative sensitivity analysis process such as fast cukier et al 1978 or auto calibration based on optimization algorithms such as nsga ii deb et al 2002 two parameters should be set for the morris screening method i e the number of replications r and the number of levels p morris 1991 the level number p is for gridding the value range of parameters table 1 and it is normally between 4 and 10 yang 2011 in this study the replication number r was set to 100 and the level number p was 10 which means a total of 2400 i e 100 23 1 models were evaluated for computing the sensitivity indices the simulation period used for parameter sensitivity analysis was 2012 2014 and the results from the 2013 2014 period are used to evaluate the model performance commonly used model performance indicators nse nash sutcliffe efficiency equation 1 rsr root mean square error standard deviation ratio equation 2 and pbias percent bias equation 3 moriasi et al 2007 were used to evaluate the performance of the seims based model in this study 1 n s e 1 i 1 n y i o b s y i s i m 2 i 1 n y i o b s y m e a n 2 2 r s r i 1 n y i o b s y i s i m 2 i 1 n y i o b s y m e a n 2 3 p b i a s i 1 n y i o b s y i s i m 100 i 1 n y i o b s where y i obs and y i sim are the ith observed and simulated values respectively y mean is the average of all observed values n is the number of observed values the job of parameter sensitivity analysis was distributed between 32 computing nodes each computing node was allowed to run 3 model evaluations concurrently except the master computing node which only ran 2 which meant that up to 95 models could be executed simultaneously therefore the speedup ratio at the model level of seims can be estimated by the ratio between the sum of the running time including i o time and computing time of all seims based model and the running time of the parallel job 4 3 4 experiment of auto calibration in seims after parameter sensitivity analysis a small number of the most sensitive parameters were selected for auto calibration the value ranges of parameters may be the same with those defined in the parameter sensitivity analysis or they may be narrowed by excluding values that result in an unacceptable model performance such as in cases where nse is less than zero in the current version of seims the nsga ii algorithm was used for auto calibration during nsga ii execution the latin hypercube sampling method iman and shortencarier 1984 was used to generate initial samples of calibrated parameters the crossover and mutation operations were similar to the original implementations in nsga ii by deb et al 2002 the initial population number was 240 with a selection rate of 0 8 the maximum generation number was 50 the crossover probability and the mutate probability were 0 8 and 0 1 respectively the multi objectives for auto calibration of the seims based model for streamflow simulation in this case study were maximizing the nse and minimizing both the rsr and the absolute value of pbias the settings for the parallel job management of auto calibration were the same as those in the parameter sensitivity analysis experiment 4 4 experimental results and discussions 4 4 1 parallel performance of single run of the seims based model fig 12 presents the computing time and speedup ratios of the openmp version and the mpi openmp version using different combinations of process and thread numbers generally the computing times decreased and the speedup ratios increased with the number of processes for the mpi openmp version or the thread number for the openmp version in all experiments except for the openmp version when the threads number exceeds the physical cores number i e 12 see the line with black squares in fig 12 for the one tier parallel computation at the basic unit level i e the openmp version the speedup ratios reached a plateau after the threads number exceeded 8 and got a maximum of 2 26 with 12 threads in contrast the speedup ratios of the parallel computation at the subbasin level i e the mpi openmp version using 1 thread per process reached stability after the processes number exceeded 10 and got a maximum of 6 24 with 16 processes which is very close to the theoretical maximum speedup ratio in this case i e 6 44 thus the parallelization strategy based on mpi at the subbasin level can achieve a much higher efficiency than the parallelization strategy based on openmp at the basic unit level when the thread number exceeded 1 for the mpi openmp version i e the situation where the two level parallel computation was conducted the speedup ratios significantly improved as shown by liu et al 2016 the stable situation also appeared when the process number exceeded 10 with any number of processes in the experiment the speedup ratios increased with the thread number first and then decreased after it exceeded 4 the maximum speedup ratios reached 19 15 under 14 processes and 4 threads per process which was nearly three times that of the tmsr fig 12b this means that the two level parallelization strategy integrated into seims has much better scalability than the parallelization at either the subbasin level or the basic unit level only liu et al 2016 considering the parallel performance of the mpi openmp version and the computing resources of each computing node the combination of 4 processes and 2 threads per process were used by every single model run in the following model level parallel computation for parameter sensitivity analysis and auto calibration 4 4 2 results of parameter sensitivity analysis by seims the running time of the parallel job which included all 2400 seims based models for parameter sensitivity analysis was 7629 42 s 2 12 h by seims while the sum of the running time including i o time and computing time of individual seims based models was 539195 25 s 149 78 h in this study from the perspective of model level parallelization the speedup ratio was 70 67 which indicates a good parallel efficiency of the parallel computing middleware in seims the parameter sensitivity analysis result from the morris screening method in seims is shown as screening plots fig 13 each point in the screening plot represents one parameter of the seims based model in table 1 the larger the modified means μ campolongo et al 2007 the more sensitive the watershed response is to variation in the parameter yang 2011 zhan et al 2013 as shown in fig 13 the importance ranks of parameters were slightly different depending on the model performance indicators i e nse rsr and pbias for example the four parameters with the highest sensitivity for nse and rsr are base ex df coef kg and sw cap while those for pbias are k pet ch n msk co1 and df coef finally a total of twelve parameters i e base ex df coef kg sw cap ch n k pet msk co1 runoff co ch width conductivity ch depth ki were selected as sensitivity parameters for this case and were used for the auto calibration of streamflow simulation 4 4 3 auto calibration results by seims the value ranges for the calibrated parameters were adopted as same as those in the parameter sensitivity analysis experiment during the evolution portion of the auto calibration process a total of 7916 seims based models were executed the running time of the parallel job which included all 7916 seims based models from all generations was 38648 03 s 10 74 h while the sum of the running times including i o time and computing time of the individual seims based models was 2504051 23 s 695 57 h from the perspective of model level parallelization the speedup ratio was 64 79 the relative low speedup ratio compared with that in the parameter sensitivity analysis experiment i e 70 67 could be attributed to the oversupply of computational resources for example there were about 153 new models which needed to be executed for each generation which means that after the first 95 model runs were finished only 58 models left to be executed on 95 computational resources the hypervolume index zitzler and thiele 1999 is often used to indicate the quality of the near pareto optimal fronts by considering both convergence and diversity zitzler et al 2003 a higher hypervolume indicates a better quality of solutions as shown in fig 14 a the hypervolume index increased slowly continuously until keeping stable after the 32nd generation fig 14 also presents near optimal pareto fronts of the generation 1st 14th and 32nd from which the evolution trend from scattering to concentrate can be obviously observed any solution from the near optimal pareto fronts after the 32nd generation could be selected for further applications fig 15 shows the calibration and validation of the simulated streamflow m3s 1 at the watershed outlet of the study area of one solution selected from the 50th near optimal pareto fronts according to the general performance ratings for streamflow simulations at a monthly time step by moriasi et al 2007 a model simulation can be judged as satisfactory if nse 0 50 rsr 0 70 and pbias is within 25 thus the seims based model performance of streamflow in this case is satisfactory since the nse rsr and pbias for the calibration period are 0 58 0 65 and 1 52 respectively and they are 0 52 0 69 and 11 40 respectively for the validation period in the study of qin et al 2018 a total of 10 streamflow simulation related parameters were determined by trial and error and then half of them were included in the set of parameters selected for auto calibration in this study i e df coef kg sw cap runoff co and conductivity with close calibrated values for example the calibrated value of conductivity was 0 8 in qin et al s 2018 study and 0 777 for the selected solution in this study compared to the manual calibration conducted by qin et al 2018 which had nse rsr and pbias of 0 48 0 72 and 16 24 for the calibration period and 0 28 0 85 and 10 69 for the validation period respectively the result of auto calibration in seims showed better performance in terms of extrapolation 5 conclusions this paper proposes an open source modular and parallelized watershed modeling framework seims which makes it easier for researchers to build their own watershed models effectively and efficiently with a fairly low learning curve of openmp based parallel computing technique users of this framework can add their own algorithms in a nearly serial programming manner and construct parallelized models as a comprehensive modeling framework seims also provides utility tools for parallel computation at the model level to support applications such as sensitivity analysis and auto calibration which need numerous model runs the effectiveness and efficiency of seims were illustrated through a case study in the youwuzhen watershed which included model construction parameter sensitivity analysis based on the morris screening method and auto calibration based on the nsga ii algorithm seims still needs to be improved continuously on the aspect of basic structure the support of irregularly shaped fields as basic simulation units and the multiple flow direction model is under development to extend the scalability and further improve the computing efficiency of seims general purpose computing on graphics processing units gpus at the basic unit level may be a promising direction qin 2015 new task scheduling strategies at the subbasin level which consider load balance based on the concept of the spatial computational domain wang and armstrong 2009 by means of pre experiment or quantity estimation equations might produce a higher speedup ratio than the static task scheduling strategy based on the area of subbasins in current version of seims declaration of competing interest none acknowledgments the work reported here was funded by the national natural science foundation of china no 41431177 41601413 41871362 and 41871300 the innovation project of lreis no o88ra20cya national basic research program of china project no 2015cb954102 papd project no 164320h116 and outstanding innovation team in colleges and universities in jiangsu province supports to a xing zhu through the vilas associate award the hammel faculty fellow award and the manasse chair professorship from the university of wisconsin madison are greatly appreciated cheng zhi qin thanks the support through the excellent young scholars project from national natural science foundation of china the authors thank all contributors to seims project with their brilliant code and advices and a range of open source software used by seims such as gdal https github com osgeo gdal mongo c driver https github com mongodb mongo c driver metis http glaros dtc umn edu gkhome metis metis overview taudem https github com dtarb taudem tinyxml https www sourceforge net projects tinyxml salib https github com salib salib deap https github com deap deap and scoop https github com soravux scoop etc which motivate seims to be an open source framework the authors appreciate the valuable suggestions and comments from the anonymous reviewers which help us to improve the manuscript 
26110,it is necessary to develop a flexible and extensible watershed modeling framework with the support of parallel computing to conduct long term high resolution simulations over large areas with diverse watershed characteristics this paper introduced an open source modular and parallelized watershed modeling framework called seims short for spatially explicit integrated modeling system to meet this need first a flexible modular structure with standard interfaces was designed in which each module corresponds to one simulation algorithm for a watershed subprocess then a parallel computing middleware based on an improved two level parallel computing approach was constructed to speed up the computational efficiency with seims users can add their own algorithms in a nearly serial programming manner and construct parallelized watershed models seims also supports model level parallel computation for applications which need numerous model runs the effectiveness and efficiency of seims were illustrated through the simulation of streamflow in the youwuzhen watershed southeastern china keywords watershed modeling framework modular parallelization watershed process simulation seims software availability software seims spatially explicit integrated modeling system operating systems supported windows linux and macos language c and python availability seims is open sourced on github https github com lreis2415 seims 1 introduction watershed modeling is the process of building computer based simulation models to aid users in understanding watershed processes such as hydrology soil erosion and nutrient cycling and has been widely used in applications such as the flood prediction and the evaluation of watershed management practices a typical procedure for watershed modeling can be summarized as follows 1 select or customize watershed models according to the physical characteristics of the study area and the requirements of applications 2 collect and preprocess input data including both the basic data such as climate landuse soil and digital elevation model dem data and application specific data such as the environmental efficiency data for the best management practices bmps under consideration qin et al 2018 3 perform sensitivity analysis and calibration manually or automatically arnold et al 2012 rouholahnejad et al 2012 zhan et al 2013 zhang et al 2013 4 analyze the model outputs or apply the calibrated model to evaluation purposes such as decision making support based on bmp scenarios analysis maringanti et al 2009 qin et al 2018 zhu et al 2019 two major challenges need to be overcome in order to carry out the above procedures effectively and efficiently on the one hand a flexible and extensible modeling system is needed to meet varied modeling purposes since an ideal and all purpose watershed model does not exist kneis 2015 on the other hand parallel computing is required because a large amount of computation is needed by both the model itself and the evaluation applications that require numerous model runs clark et al 2017 david et al 2013 zhang et al 2013 especially given the emerging trends of increasing complexity of process based models finer spatial and temporal resolutions larger study areas and longer simulation periods freeze and harlan 1969 liu et al 2016 environmental modeling frameworks provide an effective way to address the above mentioned challenges david et al 2013 formetta et al 2014 kneis 2015 wagener et al 2001 general purpose environmental modeling frameworks define standard interfaces to couple the models for different processes which make them flexible and extensible this type of framework includes the earth system modeling framework esmf hill et al 2004 open modeling interface openmi moore and tindall 2005 community surface dynamics modeling system csdms peckham et al 2013 and so on these general purpose frameworks often provide parallel computing support for common operations such as data communications and transformation e g regridding hill et al 2004 however since they are designed for coupling the existing models for multidisciplinary applications they do not provide specific support for the parallelization of spatially explicit watershed modeling as well as for the development of new watershed models kneis 2015 watershed modeling frameworks are those environmental modeling frameworks specifically designed for watershed modeling with remarkable characteristics in terms of flexibility re usability and high performance david et al 2013 kneis 2015 buahin and horsburgh 2018 specifically these characteristics commonly include 1 adopting the object orientated design to provide standard interfaces for the extension of new modeling objects or algorithms 2 assembling existing modules or objects for specific watershed modeling task in a configurable manner 3 hiding the implementation details of the framework as much as possible and only exposing concise interfaces to facilitate rapid development and 4 supporting parallel computing inherently without needing users to handle too much parallel computing programming details object modeling system oms3 david et al 2013 formetta et al 2014 and eco hydrological simulation environment echse kneis 2015 are typical examples however the existing watershed modeling frameworks put emphasis on providing good flexibility rather than good parallel performance in oms3 the simulation of different watershed subprocesses is organized into self contained modules david et al 2013 while in echse several types of spatial units e g subbasin reach lake and node with associated variables and methods are treated as different simulation objects kneis 2015 these frameworks achieve parallel computing by executing modules or objects without dependencies concurrently and are mainly implemented using shared memory multithreaded programming approaches such as open multi processing openmp the de facto standard api application programming interface for parallel computing on shared memory machines which uses notation directives or pragmas to indicate the code region that should be executed concurrently chapman et al 2007 such shared memory multithreaded implementations cannot make good use of more scalable distributed memory parallel platforms such as the symmetric multiprocessing smp clusters which limits the scalability of these frameworks david et al 2013 kneis 2015 wenderholm 2005 in fact several parallelization strategies which can effectively utilize both smp cluster and shared memory parallel platforms have been proposed in recent years liu et al 2016 vivoni et al 2011 wang et al 2011 2013 yalew et al 2013 but have not yet been integrated into flexible watershed modeling frameworks these parallelization strategies often conduct task scheduling based on spatial discretization liu et al 2016 vivoni et al 2011 yalew et al 2013 or spatio temporal discretization wang et al 2013 the implementations are mainly based on the message passing interface mpi programming model or a hybrid of mpi and openmp models liu et al 2016 however to obtain the high efficiency offered by these parallelization strategies requires users to pay a high cost in terms of learning programming skills as well as spending lots of effort on parallel programming details such as domain decomposition task scheduling and data communication liu et al 2016 wang et al 2013 meanwhile these parallelized watershed models often lack standard and concise interfaces for extending new watershed subprocess modules according to corresponding parallelization strategies therefore it is difficult for researchers with less parallel computing experience to add new algorithms for watershed subprocesses e g infiltration to the specific parallelized watershed models especially those in which different watershed subprocesses are tightly coupled by sharing global variables or the code of parallel computing is tightly bound to specific variables or modules besides the flexible modeling structure and the parallelization of the model itself the parallel computing of model level applications such as parameter sensitivity analysis and auto calibration should also be supported by a comprehensive watershed modeling framework currently various tools have been developed for specific watershed models such as the soil and water assessment tool swat arnold et al 1998 which are conducted on multi core computers rouholahnejad et al 2012 smp clusters zhang et al 2013 or grid computing platforms zhao et al 2013 however to the best of our knowledge existing parallelized watershed modeling frameworks often lack the support for model level parallelization on multiple parallel computing platforms buahin and horsburgh 2018 therefore the community still lacks a watershed modeling framework which has a flexible and extensible modular structure and an efficient parallel computing middleware to facilitate rapid development of parallelized watershed models on distributed memory parallel platforms this paper introduces a modular and parallelized watershed modeling framework called spatially explicit integrated modeling system or seims for short to fill this gap the design of seims is introduced in section 2 and its implementation is discussed in section 3 a case study is shown in section 4 to illustrate a typical watershed modeling procedure based on seims including model construction parameter sensitivity analysis and auto calibration the computational efficiencies of the single seims based model and applications that required repeated model runs are also presented conclusions and future perspectives are given in section 5 2 design of seims 2 1 basic idea and overall design seims is currently designed to focus mainly on one type of distributed watershed models those in which both overland and subsurface flow routing and channel flow routing are conducted sequentially and follow upstream downstream orders liu et al 2014 accordingly two common assumptions liu et al 2014 2016 wang et al 2011 2013 for this type of distributed watershed models implemented in seims are presented as follows 1 a watershed can be partitioned into spatial hierarchical units from coarse to fine levels such as subbasins hillslopes slope positions also known as landforms in band 1999 and basic simulation units e g grid cells voronoi polygons or hydrologic response units fig 1 a band 1999 band et al 2000 bieger et al 2016 vivoni et al 2011 a subbasin is defined as a relative closed and independent catchment area which contains two or three types of hillslopes i e source left and right hillslope and drains to one reach or channel of the watershed drainage network fig 1a slope positions are comparatively homogeneous spatial units with physical geographic features at hillslope scale which are often too coarse to be simulation units for distributed watershed modeling and however are suitable as configuration units for the evaluation of watershed management practices qin et al 2018 zhu et al 2019 those finer spatial units with more homogeneous hydrologic responses such as grid cells and irregularly shaped fields are defined as basic simulation units fig 1a in seims on which the hillslope processes at both vertical and horizontal directions are simulated 2 a topography based flow direction is assigned to each spatial unit computing dependencies exist at different spatial levels and often accord with upstream downstream routing relationships such as overland and subsurface flow routing in layers of basic simulation units within each subbasin fig 1b liu et al 2014 and channel flow routing among subbasins fig 1c wang et al 2011 in order to function as a watershed modeling framework with high extendibility and computational efficiency seims is designed with two particular features a a flexible modular structure much like oms3 david et al 2013 seims adopts a modular structure which has the advantages of good flexibility and easy maintenance leavesley et al 2006 david et al 2013 peckham et al 2013 each module corresponds to one simulation algorithm for a watershed subprocess such as the penman monteith method for simulating potential evapotranspiration every module inherits from a standard and concise interface and exposes input and output information via metadata according to the metadata a seims based watershed model for a specific application can be constructed in a loosely coupled manner i e a seims main program dynamically assembles user configured modules according to their input output relationships so to build an application specific workflow and conducts the simulation b a parallel computing middleware a parallel computing middleware is designed to support inside model and model level parallel computation since subbasins can be treated as relative independent spatial units for watershed modeling they are suitable for candidates for being distributed among and executed by the different nodes in an smp cluster or other distributed memory parallel platforms liu et al 2016 vivoni et al 2011 wang et al 2013 wu et al 2013 yalew et al 2013 for basic simulation units inside each subbasin which often interact frequently with each other during the simulation a shared memory programming model such as openmp is suitable and efficient liu et al 2014 thus a two level parallelization strategy proposed by liu et al 2014 2016 is adopted for its exploitation of the parallelizability at both coarse grained i e subbasin level and fine grained i e basic simulation unit level hereafter referred to basic unit level for short levels with such a strategy the simulation of subbasins is treated as relative independent parallel tasks of individual subbasins each of these tasks will be dispatched to one of the multiple nodes of an smp cluster using message passing programming model for execution while for each subbasin the simulation of basic simulation units will be further dispatched to multi core within each node using shared memory programming model and then be executed with the help of the modular interface the details of parallel computing at the subbasin level could be hidden from users and the parallel computing at the basic unit level based on openmp is quite simple and requires little extra programming skill this allows users to develop parallelized watershed models easily besides the two level parallelization strategy adopted for inside model execution a model level parallelization tool based on job management is also designed to speed up those model level applications which require repeated model runs such as spatial optimization of bmp scenarios based on the basic idea above the overall architecture of seims was designed as shown in fig 2 and consists mainly of the seims module library based on the modular structure the seims main programs i e openmp version and mpi openmp version the watershed database and utility tools for watershed model applications such as parameter sensitivity analysis tool and auto calibration tool the parallel computing middleware at multiple levels is implemented at the basic unit level in seims modules the subbasin level in the mpi openmp version of seims main program and the model level in watershed model applications respectively a seims based model consists of one seims main program several customized seims modules and the watershed database each seims module inherits from the base module class i e simulationmodule with standard and concise interfaces e g setdata getdata and execute functions and dependents on base modules such as i o module fig 2 the basic unit level parallelization is achieved using openmp in the execution function of each seims module the basic version of seims main program which is the openmp version is responsible for loading a set of user configured modules to build and execute a simulation workflow the mpi openmp version of seims main program uses the subbasin level domain decomposition and task scheduling to create instances of the openmp versioned seims main program for each individual subbasins and distribute them among different computing nodes with mpi based communication so to achieve subbasin level parallel computing watershed model applications that requires numerous model runs e g sensitivity analysis of a watershed model are parallelized at model level based on job management with the support of parallel computing seims is compatible with common operating systems such as windows and linux and parallel computing platforms such as personal computers with multi core cpu central processing unit and smp clusters qin et al 2014 the detailed design of the modular structure and the parallel computing middleware in current seims is presented as follows 2 2 modular structure each module of seims module library inherits from a unified module interface fig 2 and is compiled as a separate dynamic link library file the user configured modules for a specific watershed modeling application are dynamically loaded and combined as a workflow by seims main programs at runtime to conduct the watershed simulation the unified module interface consists of four types of functions i e metadatainformation setdata and getdata checkinputdata and initialoutputs and execute fig 2 with the unified module interface users can write seims modules in a nearly serial programming manner and achieve parallel computing at both the basic unit level inside these modules and the subbasin level without taking care of the details of data communication in the mpi openmp version of seims main program 2 2 1 metadatainformation function the metadatainformation function provides the metadata of the module and the seims main program can assemble the selected modules according to these metadata the metadata mainly includes basic information about the module such as its designer and description as well as the input and output information the input and output information includes static input parameters recorded in database such as saturated hydraulic conductivity e g lines 1 6 in fig 3 dynamic input variables output from other modules such as surface runoff e g lines 7 9 in fig 3 and output variables for the module such as streamflow at each reach outlet e g lines 10 12 in fig 3 each item of the input and output information mainly includes the name unit description and data type the basic data types of seims include single float value dt single e g global parameters of the watershed 1 dimension float array dt array1d and dt raster1d e g compact rater data by unfolding 2 dimension array excluding no data values liu et al 2014 and 2 dimension float array dt array2d and dt raster2d e g soil properties of multi layers to support the expandability of complex input data with unified interfaces of setting and getting data see section 2 2 2 three complex data types are also designed in seims the first is the reach object dt reach which contains the reach related parameters e g flow routing layer task scheduling group and initial geometric parameters the second is the subbasin object dt subbasin which contains subbasin scale statistical parameters e g area and average slope the third is the scenario object dt scenario that contains bmp scenarios information e g operation schedules of plant management practices with corresponding parameters the extension of these complex objects can be implemented in the base i o modules of seims module library fig 2 since subbasins are modeled independently in the mpi openmp version of seims data transmission occurs between upstream subbasins and downstream subbasins especially for channel routing related modules this means that some variables could be output and input simultaneously hereafter referred to as inoutput variable for example in the channel flow routing module the streamflow is an output variable of the array data type with a length of subbasin count dt array1d while in the mpi openmp version the execution of this module for each subbasin needs the streamflow values from its upstream subbasins as inputs of the single value data type e g line 11 in fig 3 fig 4 shows an example of the calculation of an inoutput variable within the channel flow routing module of the openmp version and mpi openmp version of seims main programs respectively in the openmp version this inoutput variable acts as an ordinary output variable which is firstly initialized as a 1 dimension array then assigned values by the execution order of subbasins which following the order of flow routing layers and finally output as an array with valid simulated values fig 4 in the mpi openmp version which is built on the openmp version the channel flow routing module of a subbasin e g the subbasin d in fig 4 cannot be executed until the output values of this variable of all of its upstream subbasins e g subbasin f and g have been received and the value to this variable of the subbasin has been assigned correspondingly fig 4 therefore an optional argument was designed to specify the data type of transferred input and output variables e g lines 11 in fig 3 among upstream downstream subbasins which made the metadata interface compatible with both the openmp version and the mpi openmp version there are three options for this argument tf none is the default indicating that the variable doesn t need to be transferred and can be omitted tf singlevalue is used for variables of types dt array1d and dt raster1d and tf onearray1d is used for variables of types dt array2d and dt raster2d with the corresponding setting and getting operations see section 2 2 2 the values of transferred variables across subbasins can be dynamically determined after loading the required modules at runtime in the mpi openmp version see section 2 3 3 in this way users can achieve parallel computing at the subbasin level without worrying about the details of data communication in the seims main program 2 2 2 setdata and getdata functions the setdata and getdata functions are responsible for setting and getting parameters or variables listed in the metadata of one module e g fig 3 respectively the setdata functions for basic data types include setvalue set1ddata and set2ddata while the getdata functions include getvalue get1ddata and get2ddata correspondingly for complex data types only setdata functions are needed i e setreaches setsubbasins and setscenario in order to improve data transfer efficiency across modules in the openmp version array type and complex type variables are passed by memory address which means the same variable in different modules points to the same area in memory note that in order to enable the openmp version and mpi openmp version of seims main program to invoke seims modules built by the same module code the variables with certain transferred data types i e tf singlevalue and tf onearray1d should be set and gotten according to the original data type and transferred data type respectively for example the streamflow variable line 11 in fig 3 and the example in fig 4 should be gotten as dt array1d to act as an ordinary output variable for output data of the simulation or input of other modules and should also be set as dt single for receiving the simulated value of this variable from its upstream subbasin and gotten as dt single for its downstream subbasin 2 2 3 checkinputdata and initialoutputs functions the checkinputdata function is responsible for checking the validity of input data for example it verifies that the input array exists and is accessible and raises an exception if any unavailable or illegal data exists the initialoutputs function is used for creating and initializing output variables and temporary variables of the current module so that the output variables required by other modules as inputs are valid 2 2 4 execute function the execute function as the core of a module is for the simulation of a specific watershed subprocess within a time step the outer loop of time steps is handled in seims main program the nested time steps between channel groundwater routing processes and hillslope processes are supported which means the time step of hillslope processes can be finer than channel groundwater routing processes fig 5 generally the customized modules of one seims based model can be divided into three categories which should be executed sequentially i e driver factor related modules e g reading and preprocessing of climate data hillslope processes modules and channel groundwater routing modules fig 5 the inner loop in the execute function is to perform simulation on basic simulation units for hillslope processes and subbasin units or reaches for channel groundwater routing processes see section 2 3 2 2 3 parallel computing middleware decomposing a modeling issue into tasks that can be executed concurrently is the first step to achieving parallelization foster 1995 the parallel computing middleware of seims implements two types of parallelization strategy i e parallelization of inside model execution based on the spatial discretization of the watershed section 2 3 1 2 3 3 and model level parallel job management section 2 3 4 2 3 1 domain decomposition of watershed the two level domain decomposition method proposed by liu et al 2014 2016 is adopted to utilize the parallelizability at both the subbasin and basic unit levels subbasins are partitioned into layers via either an upstream downstream strategy fig 6 b or a downstream upstream strategy fig 6c the subbasin layers are treated as a graph in which one node represents a subbasin and the edges represent the upstream downstream relationships unlike the binary tree code adopted by wang et al 2011 the graph structure adopted in seims can support subbasins with more than two upstream subbasins each node is assigned a computing weight e g by subbasin area in the current version which is useful to account for load balancing in task scheduling liu et al 2016 see section 2 3 2 taking grid cells as an example of basic simulation units fig 7 shows the division of basic simulation units into layers using an upstream downstream strategy fig 7b or downstream upstream strategy fig 7c according to flow directions fig 7a the current version of seims adopts the commonly used d8 single flow direction algorithm o callaghan and mark 1984 while multiple flow direction algorithms such as mfd md algorithm qin et al 2007 could also be supported in future versions in addition support for using irregularly shaped fields such as hydrologic response units arnold et al 1998 and patches tague and band 2004 as basic simulation units is under development 2 3 2 parallelization at the basic unit level at the basic unit level e g grid cells in current version of seims simulation methods for watershed processes can be divided into two types according to their computational characteristics i e local independent methods such as runoff generation processes and plant growth simulations and sequential dependent methods such as overland flow routing using 1 d kinematic wave method liu et al 2014 for local independent methods parallel computing can be conducted simply by dividing simulation units into equal parts while for sequential dependent methods parallel computing can be achieved in the same layer divided by flow directions fig 7 in which there are no upstream and downstream relationships between the corresponding simulation units the openmp programming model is used in current version of seims which makes it quite easy to achieve parallel computing in most circumstances by embedding compiler directives start with pragma in the source code without damaging the original code structure fig 8 other circumstances which need a few more openmp compiler directives such as reduce operations on array type data according to subbasins can be found in https github com lreis2415 seims issues 36 2 3 3 parallelization at the subbasin level seims adopts the mpi programming model to conduct parallel computing at the subbasin level to make the best use of computational resources load imbalance should be reduced by distributing the amount of computation in each computing node as evenly as possible while minimizing the amount of communication e g try to dispatch subbasins with upstream downstream relationships to the same node to reduce communication overhead liu et al 2016 to achieve such a tradeoff between load balance and communication overhead current seims adopts the widely used metis graph partitioning algorithm karypis and kumar 1998 for static task scheduling with the graph of subbasins fig 6b and c as input and estimates the computing weight of each node roughly by subbasin area liu et al 2016 the master slave strategy for task scheduling and data communication which was commonly used in previous studies liu et al 2016 wang et al 2011 may cause unnecessary communication overhead for example with the master slave strategy the transferred values between upstream and downstream subbasins will be sent and receive twice i e from slave processes to master process and from master process to another slave process therefore a static peer to peer communication strategy is designed to ensure that the transferred values only need to be sent and received once from a process to another fig 9 shows the pseudo code of the static task scheduling strategy from the perspective of spatial discretization at the subbasin level the predetermined task scheduling plan is loaded by the master process and then broadcasted to all processes lines 1 6 in fig 9 which makes the peer to peer communication available then each process is responsible for creating and executing the task of subbasins assigned to it lines 7 18 in fig 9 the execution workflow of one subbasin object within a channel routing time step based on the peer to peer communication strategy is described by the pseudo code in fig 10 the hillslope processes and the channel routing processes can proceed separately one after another lines 2 and 11 in fig 10 respectively the transferred data from upstream subbasins should be updated by reading data from the current process or receiving from another process before the execution of channel routing processes lines 3 10 in fig 10 after that the transferred data derived from the current subbasin should be saved for its downstream subbasin if such a subbasin exists lines 13 19 in fig 10 if the downstream subbasin is not assigned to the current process the transferred data will be sent directly to another process which handles the downstream subbasin line 15 in fig 10 finally the specified output data of the current subbasin will be saved line 20 in fig 10 2 3 4 parallelization at the model level parameter sensitivity analysis zhan et al 2013 auto calibration rouholahnejad et al 2012 zhang et al 2013 and scenario analysis based on optimization algorithms qin et al 2018 are common applications which need numerous model runs with different model inputs each model run is independent of others and thus the parallelism at the model level exists although the results of a series of model runs are summarized note that although the multiple model runs can be executed concurrently using multithreading programming techniques on shared memory machines e g rouholahnejad et al 2012 the computational efficiency of each model on individual thread may degrade due to the contention for shared hardware resources especially when the number of threads exceeds the number of processors therefore the model level parallelization is suitable to be conducted through parallel task distribution on distributed memory parallel computing platforms e g smp cluster grid computing platform by job management zhao et al 2013 or mpi zhang et al 2013 current seims provides the model level parallelization based on job management 2 4 watershed database the watershed database for the seims based model is designed to store all data related to watershed modeling such as the input and output data the parameter adjusting data for sensitivity analysis and so on the input data include but are not limited to climate data e g precipitation and meteorological data such as temperature geospatial data e g dem and the derived spatial parameters landuse map soil type map and soil attribute data and management data e g plant management schedules depending on the purpose of watershed modeling two types of output data can be specified by a configuration file i e time series outputs e g discharge at the outlet of the watershed and spatio temporal distribution variables e g the average of soil water storage the output data can also be exported as single files e g plain text and geotiff files all spatial input data are decomposed into small parts according to subbasin boundaries and stored in the database as binary types during data preprocessing so as to improve i o efficiency liu et al 2016 for the execution of the openmp version of the seims based model data for the whole watershed are stored as well with the flexible watershed database design there is no need to duplicate the input data when conducting numerous model runs differentiated by calibrated parameter settings or scenario settings a measure which is required by some other watershed models such as swat zhang et al 2013 3 implementation of seims 3 1 overall implementation the proposed modular and parallelized watershed modeling framework namely seims was implemented using standard c and python programming languages which means it is cross platform compatible seims uses cmake https cmake org a cross platform build tool to manage the entire project in order to achieve compatibility on mainstream compilation environments the compiled c programs include the seims main programs including the openmp version and the mpi openmp version seims module library and executable programs for data preprocessing python is used for utility tools including data preprocessing calibration sensitivity analysis scenario analysis and so on to simplify the implementation a parallel job management strategy based on scoop hold geoffroy et al 2014 using python language was adopted for the current seims scoop automatically distributes tasks among available computing resources using dynamic load balancing hold geoffroy et al 2014 which means it has the potential to be compatible with multiple computing platforms by drawing lessons from the wetspa model water and energy transfer between soil plant and atmosphere liu et al 2003 liu 2004 the swat model arnold et al 1998 and other watershed models or algorithms current seims module library can support the simulation of hydrology soil erosion and nutrient cycling processes for storm event models liu et al 2014 2016 wu et al 2018 and long term e g daily models qin et al 2018 zhu et al 2019 with the parallel middleware of seims at subbasin level and its standard module interfaces modelers need not take care of programming details of mpi and can use serial programming code to develop their own seims modules achieving subbasin level parallelization with high parallel efficiency see section 2 3 3 furthermore modelers can conveniently achieve the openmp based parallelization at basic unit level through programming with a fairly low learning curve in most cases without damaging the serial code structure see section 2 3 2 please refer to the user manual document https github com lreis2415 seims blob master seims usermanual pdf for more detailed information on programming with seims such as the demo of developing a new seims module given the requirements of flexible data structures elastic scalability and high performance a widely used nosql database mongodb https www mongodb com was adopted to manage all kinds of data in the current version of seims seims is still under continuous development please visit https github com lreis2415 seims for more details and latest updates 3 2 utility tools the data preprocessing tools which were written in python and c languages include four categories of functions i e watershed spatial discretization spatial parameters extraction hydroclimate data processing and watershed database management the watershed spatial discretization functions include the functions of delineating spatial units at different scales fig 1a such as subbasins implemented based on taudem v5 3 7 tarboton 2016 hillslopes hydrologically connected fields wu et al 2018 and slope positions zhu et al 2018 and the functions of spatial domain decomposition of the watershed at subbasin and basic unit levels see section 2 3 1 the spatial parameters extraction functions mainly include the calculation of terrain and location related parameters such as depression capacity and the mapping of soil and landuse landcover related parameters based on lookup tables the hydroclimate data processing functions mainly refer to the data formatting such as interpolating irregular precipitation records to be regular time interval data and the statistical calculation such as annual number of heat units the watershed database management functions are responsible for creating the database and importing data with two types of formats the first is spatial parameters unfolded as a 1 dimension array and stored as binary data and the second is plain text data organized and stored as the format of key value pairs i e json format a main python script is provided to perform the whole workflow of data preprocessing present data preprocessing tools in seims are implemented to support the currently available seims modules this means that additional tools should be added if the input data of a newly developed module is not available from the database or outputs of other modules in seims parameter sensitivity analysis is useful for identifying the most important or influential parameters for a specified simulation target e g discharge of the watershed outlet zhan et al 2013 a typical sensitivity analysis procedure includes sampling from the value ranges of determined model inputs evaluating the model with the generated samples and saving the interested outputs and calculating sensitivity indices by various methods such as the morris screening method morris 1991 and fast fourier amplitude sensitivity test cukier et al 1978 the sensitivity analysis tool in seims is organized as separated functions according to these steps which means it is easy to incorporate any sampling method and sensitivity analysis method the most time consuming step is repetitive model evaluations which is parallelized at model level by parallel job management based on scoop like the parameter sensitivity tool auto calibration and scenario analysis tools also need to generate many different model inputs evaluate models and perform analysis based on model outputs in the current seims version the commonly used non dominated sorting genetic algorithm nsga ii deb et al 2002 was integrated as the auto calibration and scenario analysis tool the support for other popular optimization algorithms is on the development plan running all utility tools requires a corresponding configuration file in which the application related parameters are specified seims provides different templates for the configuration files for data preprocessing parameter sensitivity analysis auto calibration etc more details please refers to the user manual document https github com lreis2415 seims blob master seims usermanual pdf 4 case study 4 1 study area and dataset the youwuzhen watershed 5 39 km2 which is located in changting county of fujian province of china was selected as the case study area fig 11 this area belongs to the typical red soil hilly region in southeastern china and suffers from severe soil erosion chen et al 2013 the study area has hills with steep slopes up to 52 9 and with an average slope of 16 8 and broad alluvial valleys qin et al 2018 the elevation ranges from 295 0 m to 556 5 m the study area is characterized by a mid subtropical monsoon moist climate and has an annual average temperature of 18 3 c the annual average precipitation is 1697 0 mm and intense short duration thunderstorm events contribute about three quarters of annual precipitation from march to august chen et al 2013 the land use types are mainly forest 59 8 paddy field 20 6 and orchard 12 8 the dominant soil type is red earth humic acrisols in fao soil taxonomy or ultisols in us soil taxonomy qin et al 2018 the basic data of the youwuzhen watershed collected for watershed modeling include spatial data i e a gridded dem with 10 m resolution a soil type map with a scale of 1 50 000 and a land use type map interpreted from alos advanced land observation satellite images from 2009 climate data and periodic site monitoring streamflow and sediment discharge data collected at the watershed outlet qin et al 2018 according to an accumulated threshold of 0 185 km2 chen et al 2013 the youwuzhen watershed was delineated into 17 subbasins fig 11 qin et al 2018 soil properties such as mechanical composition and organic matter were derived from field sampling data chen et al 2013 soil erodibility factors cover management factors and conservation practice factors of the usle universal soil loss equation model were drawn from the study conducted in the same area by chen and zha 2016 climate data contains daily meteorological data derived from national meteorological information center of china meteorological administration data and precipitation data derived from local monitoring stations the periodic site monitoring data regarding streamflow and sediment discharge at the watershed outlet from 2012 to 2015 were provided by the soil and water conservation bureau of changting county fujian province china limited by the quality of this data those periods of more than three consecutive days of rainfall with complete records of runoff generation and sediment yield data were selected for modeling as a result the year 2012 was selected as a warm up period for the seims based model the years 2013 and 2014 for calibration and the year 2015 for validation 4 2 construction of the seims based model in this case study seims was used to construct a grid based watershed model for simulating streamflow and sediment discharge at a daily time step for the period from 2012 to 2015 according to the available dataset the simulated hydrologic processes included interception surface depression storage surface runoff potential evapotranspiration percolation interflow groundwater flow and channel flow soil erosion by water on hillslopes and sediment routing in channels were simulated to be a comprehensive watershed model the simulation of plant growth process and nutrient i e nitrogen and phosphorous cycling were also included the combination of adopted modules is the same as that used and manually calibrated by qin et al 2018 which allows for a comparison between the manual calibrated model and the auto calibration conducted by seims see section 4 3 in the study of qin et al 2018 and the follow up study of zhu et al 2019 the calibrated seims based watershed model with a daily time step was used to evaluate the reduction rate of multi year average soil erosion under different bmp scenarios the combination of adopted modules is prepared as a list of module names in a plain text file which will be parsed by the seims main program in order to assembling the corresponding modules as described in section 2 2 for more details about the corresponding simulation algorithms please refer to qin et al 2018 4 3 experiment design 4 3 1 experimental environments a high performance computing platform with 134 computing nodes and a general parallel file system gpfs was used each computing node was equipped with 2 way intel xeon e5650 6 cores cpus i e 12 physical cores in total 24 gb memory and one infiniband 40 gb s network card used for high speed interconnection of parallel computing jobs the red hat enterprise linux server 6 2 was used as operation system the compilation environment of seims included intel c 12 1 compiler with the support of openmp 3 1 intel mpi library 4 0 3 gdal 1 11 5 and mongo c driver 1 6 1 the virtual environment for running seims utility tools was python 2 7 13 with several third party packages such as gdal 1 11 5 numpy 1 12 1 matplotlib 1 5 3 pymongo 3 4 0 deap 1 2 salib 1 1 2 and scoop 0 7 4 3 2 experiment for evaluating parallel performance of seims based model the simulation results of the openmp version and the mpi openmp version were confirmed to be the same before the parallel performance of the seims based model was tested according to the parallelization of seims at the subbasin level the maximum number of processes should not exceed the count of subbasins i e 17 in this case study therefore the openmp version and the mpi openmp version were executed separately using the same number ranging from 1 to 16 of threads and processes using one thread per process respectively for convenience of comparison for the mpi openmp version executions using 2 3 4 6 and 8 threads per process were also tested the actual simulation time excluding the time for data input and output i o is regarded as computing time in this study for the mpi openmp version the computing time is the maximum computing time among all processes each execution was repeated three times to get an average computing time as the result for evaluation the speedup ratio which is defined as the ratio between the serial computing time and the parallel computing time was used to measure the parallel performance for the openmp version the serial computing time was the computing time of the execution using one thread while the computing time of the execution using one process and one thread per process was regarded as the serial computing time for the mpi openmp version the theoretical maximum speedup ratio tmsr was also estimated by the method proposed by liu et al 2013 which is suitable for grid based distributed watershed models the proportion of computing channel routing processes in the total amount of computations was set as 1 for estimating tmsr 4 3 3 experiment of parameter sensitivity analysis in seims the most important step for parameter sensitivity analysis is to determine the involved parameters and their proper change ranges according to prior knowledge zhan et al 2013 the simulation of streamflow in a watershed model compared with that of sediment and nutrient is often the priority in sensitivity analysis and calibration abbaspour et al 2015 therefore this study takes the simulation of streamflow as an example to illustrate the sensitivity analysis and auto calibration by seims a total of 23 parameters related to hydrological processes were selected table 1 lists the basic information such as description absolute maximum and minimum values and default initial value the value changing method and the range of each selected parameter the morris screening method morris 1991 implemented seims was used in this study to qualitatively identify important parameters which impact the model performance of streamflow at the watershed outlet although the morris screening method cannot provide an accurate quantitative estimate of how much a parameter contributes to the model performance indicators or distinguish the non linearity of a parameter from the interaction with others yang 2011 zhan et al 2013 it can be used to exclude non sensitive parameters through one at a time sensitivity analysis for a subsequent quantitative sensitivity analysis process such as fast cukier et al 1978 or auto calibration based on optimization algorithms such as nsga ii deb et al 2002 two parameters should be set for the morris screening method i e the number of replications r and the number of levels p morris 1991 the level number p is for gridding the value range of parameters table 1 and it is normally between 4 and 10 yang 2011 in this study the replication number r was set to 100 and the level number p was 10 which means a total of 2400 i e 100 23 1 models were evaluated for computing the sensitivity indices the simulation period used for parameter sensitivity analysis was 2012 2014 and the results from the 2013 2014 period are used to evaluate the model performance commonly used model performance indicators nse nash sutcliffe efficiency equation 1 rsr root mean square error standard deviation ratio equation 2 and pbias percent bias equation 3 moriasi et al 2007 were used to evaluate the performance of the seims based model in this study 1 n s e 1 i 1 n y i o b s y i s i m 2 i 1 n y i o b s y m e a n 2 2 r s r i 1 n y i o b s y i s i m 2 i 1 n y i o b s y m e a n 2 3 p b i a s i 1 n y i o b s y i s i m 100 i 1 n y i o b s where y i obs and y i sim are the ith observed and simulated values respectively y mean is the average of all observed values n is the number of observed values the job of parameter sensitivity analysis was distributed between 32 computing nodes each computing node was allowed to run 3 model evaluations concurrently except the master computing node which only ran 2 which meant that up to 95 models could be executed simultaneously therefore the speedup ratio at the model level of seims can be estimated by the ratio between the sum of the running time including i o time and computing time of all seims based model and the running time of the parallel job 4 3 4 experiment of auto calibration in seims after parameter sensitivity analysis a small number of the most sensitive parameters were selected for auto calibration the value ranges of parameters may be the same with those defined in the parameter sensitivity analysis or they may be narrowed by excluding values that result in an unacceptable model performance such as in cases where nse is less than zero in the current version of seims the nsga ii algorithm was used for auto calibration during nsga ii execution the latin hypercube sampling method iman and shortencarier 1984 was used to generate initial samples of calibrated parameters the crossover and mutation operations were similar to the original implementations in nsga ii by deb et al 2002 the initial population number was 240 with a selection rate of 0 8 the maximum generation number was 50 the crossover probability and the mutate probability were 0 8 and 0 1 respectively the multi objectives for auto calibration of the seims based model for streamflow simulation in this case study were maximizing the nse and minimizing both the rsr and the absolute value of pbias the settings for the parallel job management of auto calibration were the same as those in the parameter sensitivity analysis experiment 4 4 experimental results and discussions 4 4 1 parallel performance of single run of the seims based model fig 12 presents the computing time and speedup ratios of the openmp version and the mpi openmp version using different combinations of process and thread numbers generally the computing times decreased and the speedup ratios increased with the number of processes for the mpi openmp version or the thread number for the openmp version in all experiments except for the openmp version when the threads number exceeds the physical cores number i e 12 see the line with black squares in fig 12 for the one tier parallel computation at the basic unit level i e the openmp version the speedup ratios reached a plateau after the threads number exceeded 8 and got a maximum of 2 26 with 12 threads in contrast the speedup ratios of the parallel computation at the subbasin level i e the mpi openmp version using 1 thread per process reached stability after the processes number exceeded 10 and got a maximum of 6 24 with 16 processes which is very close to the theoretical maximum speedup ratio in this case i e 6 44 thus the parallelization strategy based on mpi at the subbasin level can achieve a much higher efficiency than the parallelization strategy based on openmp at the basic unit level when the thread number exceeded 1 for the mpi openmp version i e the situation where the two level parallel computation was conducted the speedup ratios significantly improved as shown by liu et al 2016 the stable situation also appeared when the process number exceeded 10 with any number of processes in the experiment the speedup ratios increased with the thread number first and then decreased after it exceeded 4 the maximum speedup ratios reached 19 15 under 14 processes and 4 threads per process which was nearly three times that of the tmsr fig 12b this means that the two level parallelization strategy integrated into seims has much better scalability than the parallelization at either the subbasin level or the basic unit level only liu et al 2016 considering the parallel performance of the mpi openmp version and the computing resources of each computing node the combination of 4 processes and 2 threads per process were used by every single model run in the following model level parallel computation for parameter sensitivity analysis and auto calibration 4 4 2 results of parameter sensitivity analysis by seims the running time of the parallel job which included all 2400 seims based models for parameter sensitivity analysis was 7629 42 s 2 12 h by seims while the sum of the running time including i o time and computing time of individual seims based models was 539195 25 s 149 78 h in this study from the perspective of model level parallelization the speedup ratio was 70 67 which indicates a good parallel efficiency of the parallel computing middleware in seims the parameter sensitivity analysis result from the morris screening method in seims is shown as screening plots fig 13 each point in the screening plot represents one parameter of the seims based model in table 1 the larger the modified means μ campolongo et al 2007 the more sensitive the watershed response is to variation in the parameter yang 2011 zhan et al 2013 as shown in fig 13 the importance ranks of parameters were slightly different depending on the model performance indicators i e nse rsr and pbias for example the four parameters with the highest sensitivity for nse and rsr are base ex df coef kg and sw cap while those for pbias are k pet ch n msk co1 and df coef finally a total of twelve parameters i e base ex df coef kg sw cap ch n k pet msk co1 runoff co ch width conductivity ch depth ki were selected as sensitivity parameters for this case and were used for the auto calibration of streamflow simulation 4 4 3 auto calibration results by seims the value ranges for the calibrated parameters were adopted as same as those in the parameter sensitivity analysis experiment during the evolution portion of the auto calibration process a total of 7916 seims based models were executed the running time of the parallel job which included all 7916 seims based models from all generations was 38648 03 s 10 74 h while the sum of the running times including i o time and computing time of the individual seims based models was 2504051 23 s 695 57 h from the perspective of model level parallelization the speedup ratio was 64 79 the relative low speedup ratio compared with that in the parameter sensitivity analysis experiment i e 70 67 could be attributed to the oversupply of computational resources for example there were about 153 new models which needed to be executed for each generation which means that after the first 95 model runs were finished only 58 models left to be executed on 95 computational resources the hypervolume index zitzler and thiele 1999 is often used to indicate the quality of the near pareto optimal fronts by considering both convergence and diversity zitzler et al 2003 a higher hypervolume indicates a better quality of solutions as shown in fig 14 a the hypervolume index increased slowly continuously until keeping stable after the 32nd generation fig 14 also presents near optimal pareto fronts of the generation 1st 14th and 32nd from which the evolution trend from scattering to concentrate can be obviously observed any solution from the near optimal pareto fronts after the 32nd generation could be selected for further applications fig 15 shows the calibration and validation of the simulated streamflow m3s 1 at the watershed outlet of the study area of one solution selected from the 50th near optimal pareto fronts according to the general performance ratings for streamflow simulations at a monthly time step by moriasi et al 2007 a model simulation can be judged as satisfactory if nse 0 50 rsr 0 70 and pbias is within 25 thus the seims based model performance of streamflow in this case is satisfactory since the nse rsr and pbias for the calibration period are 0 58 0 65 and 1 52 respectively and they are 0 52 0 69 and 11 40 respectively for the validation period in the study of qin et al 2018 a total of 10 streamflow simulation related parameters were determined by trial and error and then half of them were included in the set of parameters selected for auto calibration in this study i e df coef kg sw cap runoff co and conductivity with close calibrated values for example the calibrated value of conductivity was 0 8 in qin et al s 2018 study and 0 777 for the selected solution in this study compared to the manual calibration conducted by qin et al 2018 which had nse rsr and pbias of 0 48 0 72 and 16 24 for the calibration period and 0 28 0 85 and 10 69 for the validation period respectively the result of auto calibration in seims showed better performance in terms of extrapolation 5 conclusions this paper proposes an open source modular and parallelized watershed modeling framework seims which makes it easier for researchers to build their own watershed models effectively and efficiently with a fairly low learning curve of openmp based parallel computing technique users of this framework can add their own algorithms in a nearly serial programming manner and construct parallelized models as a comprehensive modeling framework seims also provides utility tools for parallel computation at the model level to support applications such as sensitivity analysis and auto calibration which need numerous model runs the effectiveness and efficiency of seims were illustrated through a case study in the youwuzhen watershed which included model construction parameter sensitivity analysis based on the morris screening method and auto calibration based on the nsga ii algorithm seims still needs to be improved continuously on the aspect of basic structure the support of irregularly shaped fields as basic simulation units and the multiple flow direction model is under development to extend the scalability and further improve the computing efficiency of seims general purpose computing on graphics processing units gpus at the basic unit level may be a promising direction qin 2015 new task scheduling strategies at the subbasin level which consider load balance based on the concept of the spatial computational domain wang and armstrong 2009 by means of pre experiment or quantity estimation equations might produce a higher speedup ratio than the static task scheduling strategy based on the area of subbasins in current version of seims declaration of competing interest none acknowledgments the work reported here was funded by the national natural science foundation of china no 41431177 41601413 41871362 and 41871300 the innovation project of lreis no o88ra20cya national basic research program of china project no 2015cb954102 papd project no 164320h116 and outstanding innovation team in colleges and universities in jiangsu province supports to a xing zhu through the vilas associate award the hammel faculty fellow award and the manasse chair professorship from the university of wisconsin madison are greatly appreciated cheng zhi qin thanks the support through the excellent young scholars project from national natural science foundation of china the authors thank all contributors to seims project with their brilliant code and advices and a range of open source software used by seims such as gdal https github com osgeo gdal mongo c driver https github com mongodb mongo c driver metis http glaros dtc umn edu gkhome metis metis overview taudem https github com dtarb taudem tinyxml https www sourceforge net projects tinyxml salib https github com salib salib deap https github com deap deap and scoop https github com soravux scoop etc which motivate seims to be an open source framework the authors appreciate the valuable suggestions and comments from the anonymous reviewers which help us to improve the manuscript 
26111,coastsat is an open source software toolkit written in python that enables the user to obtain time series of shoreline position at any sandy coastline worldwide from 30 years and growing of publicly available satellite imagery the toolkit exploits the capabilities of google earth engine to efficiently retrieve landsat and sentinel 2 images cropped to any user defined region of interest the resulting images are pre processed to remove cloudy pixels and enhance spatial resolution before applying a robust and generic shoreline detection algorithm this novel shoreline detection technique combines a supervised image classification and a sub pixel resolution border segmentation to map the position of the shoreline with an accuracy of 10 m the purpose of coastsat is to provide coastal managers engineers and scientists a user friendly and practical toolkit to monitor and explore their coastlines the software is freely available on github https github com kvos coastsat and is accompanied by guided examples jupyter notebook plus step by step readme documentation keywords google earth engine shoreline mapping landsat sentinel 2 sub pixel resolution software availability software name coastsat developer kilian vos year first official release 2018 hardware requirements pc system requirements windows linux mac program language python program size 1 mb availability https github com kvos coastsat license gpl 3 0 documentation readme in github repository and guided example in the form of an editable jupyter notebook 1 introduction the coastal region is the most heavily urbanised land zone in the world small et al 2011 and is regarded as a critical resource in view of its recreational environmental and economic importance yet ocean coasts are affected by variations in mean sea level extreme waves storm surges and river flow through a range of physical processes ranasinghe 2016 for a review recent intensification in mean wave energy reguero et al 2019 extreme coastal wave energy mentaschi et al 2017 and oceanic wind speeds young and ribal 2019 coupled with rising sea levels suggest that coastal areas will be exposed to increasing hazards in coming decades li et al 2018 it is therefore critical for coastal managers and policy makers regulating coastal development to observe and quantify these changes along coastlines vulnerable to extreme as well as subtle changes in oceanographic forcing e g barnard et al 2017 harley et al 2017 masselink et al 2016 data scarcity however remains an ongoing challenge as long term coastal monitoring programs based on in situ measurements are limited to only a few sites around the world e g barnard et al 2015 pianca et al 2015 turner et al 2016 indeed shoreline data is key to understand past beach behaviour splinter et al 2018 and calibrate numerical models capable of predicting future shoreline changes splinter et al 2013 publicly available remote sensing data from earth observation satellites provides a low cost solution to obtain long term observations of coastline changes over the past three decades at many sites worldwide crucially the advent of google earth engine gorelick et al 2017 has facilitated access to the growing archive of publicly available satellite imagery providing the opportunity for global scale analyses stretching back decades e g donchyts et al 2016 li et al 2019 luijendijk et al 2018 mentaschi et al 2018 space borne observations have been employed in a wide range of change detection applications including the analysis of meandering river morphodynamics monegaglia et al 2018 delineation of wetland footprints quinn and epshtein 2014 and identification of oil spills keramitsoglou et al 2006 recently optical imaging satellites have begun to be used to measure the location of the shoreline e g garcía rubio et al 2015 kuleli et al 2011 liu et al 2017 pardo pascual et al 2012 which is regarded by coastal managers planners engineers and scientists as a key indicator of how coastlines vary and evolve over time for instance hagenaars et al 2018 employed landsat 5 landsat 8 and sentinel 2 image composites to map the position of the shoreline with a horizontal accuracy of the order of half a pixel i e 15 m for landsat images and 5 m for sentinel 2 images while pardo pascual et al 2018 evaluated a sub pixel resolution shoreline detection technique at a low energy microtidal beach and reported horizontal accuracies of less than 10 m using landsat 7 landsat 8 and sentinel 2 images the aim of this short communication is to detail a new open source software toolkit called coastsat to obtain 30 year time series of approximately biweekly shoreline positions with a horizontal accuracy of 10 m at any sandy coastline worldwide demonstrated in the accompanying jupyter notebook and readme file is the suggested workflow to guide users to easily and efficiently retrieve the required satellite imagery from the google earth engine gee archive of publicly available satellite imagery apply a robust and generic sub pixel resolution shoreline detection algorithm and extract the required shoreline positions through time example practical applications of the coastsat toolkit applied at contrasting sandy coastlines tide range wave climate spanning europe the usa australia and new zealand are presented in vos et al 2019 where a range of storm seasonal and inter annual shorelines changes are illustrated 2 software capabilities the toolkit is compatible with python version 3 6 or higher and combines and extends the functionality of a number of other freely available python software packages the google earth engine gee python api package is used to access the satellite imagery while other machine learning and image processing packages namely scikit learn pedregosa et al 2011 and scikit image van der walt et al 2014 are employed to automatically extract the position of the shoreline from the multispectral imagery as is illustrated by a practical example provided in the accompanying jupyter notebook the sequence of functionalities provided within the single coastsat toolkit can be summarised as follows i retrieval of the images from the gee archive ii pre processing of the multispectral images cloud masking pansharpening and down sampling iii sub pixel resolution shoreline extraction and iv time series of shoreline position along shore normal transects coastsat outputs the mapped shorelines as a geojson file that can be imported into a gis application a flow chart of the different steps involved to obtain satellite derived shorelines at a user defined site is summarised in fig 1 and now described in further detail below 2 1 access to publicly available satellite imagery the coastsat toolkit provides an interface to the gee api to enable easy access to all the toa top of atmosphere reflectance images from the landsat 5 tm landsat 7 emt and landsat 8 oli tier 1 collections as well as sentinel 2 msi level 1c products which also represent toa reflectance toa images are calibrated to provide a standardised comparison between images acquired by different sensors on different dates chander et al 2009 thus they are suitable for time series analysis the user can choose which satellite mission s to use and all four missions can be used at the same time to maximise temporal resolution to significantly reduce the size of the individual files from several gbs to less than a mb per 5 km2 region a user defined region of interest roi is used to crop the images while still on the gee server and prior download in addition only the spectral bands that are required for the shoreline detection namely the three visible bands r g b the near infrared band nir and the short wave infrared band swir1 are included in the downloaded cropped image this implementation significantly increases processing speed and reduces memory usage on the local computer for example 30 years of biweekly images of a 5 km2 roi can be downloaded and processed in 2 h on a standard laptop 4 gb ram intel core i5 without running into memory allocation problems users should avoid requesting very large rois 100 km2 as this will significantly slow down the processing instead multiple smaller rois should be requested table 1 summarises the satellite data retrieved by coastsat including the gee collections in which the images are continuously stored and the time coverage revisit period and pixel size of each satellite mission 2 2 pre processing prior to extracting the shoreline the toa images are pre processed as follows cloud masking each image is supplemented by a quality assessment band pre computed by the data provider usgs for landsat or esa for sentinel 2 which contains a per pixel cloud mask with this information the percentage of cloud cover is calculated based on the number of cloudy pixels inside the region of interest and a user defined cloud cover threshold is used to discard all the images that exceed a certain percentage of cloud cover panchromatic image sharpening and down sampling in this next pre processing step the spatial resolution of the satellite image is enhanced to achieve an optimal shoreline detection for landsat 7 etm and landsat 8 oli images the higher resolution panchromatic band see table 1 is used to increase the resolution of the bands whose portion of the electromagnetic spectrum is covered by the panchromatic band from 30 to 15 m by applying a data fusion method based on principal component analysis tu et al 2001 in this method the multispectral bands are down sampled to 15 m by bilinear interpolation and decomposed in principal components then the first principal component is replaced by the panchromatic band after matching the histograms and retransformed back into the original multispectral space landsat 5 tm images do not have a panchromatic band however the 30 m bands are down sampled to 15 m by bilinear interpolation as this was found to improve the accuracy of the shoreline detection for sentinel 2 images the 20 m swir1 band is down sampled to 10 m by bilinear interpolation so that all the bands are at 10 m resolution it should be noted that the images are orthorectified by the data provider and no further geometric correction i e co registration is applied in coastsat the georeferencing accuracy of each image is stored in the output file containing the mapped shorelines 2 3 shoreline detection the shoreline is defined here as the instantaneous interface between water and sand captured at the instant of image acquisition the coastsat toolkit applies a robust sub pixel resolution shoreline detection algorithm to the pre processed satellite images the algorithm builds upon the sub pixel border segmentation by liu et al 2017 as was originally proposed by cipolletti et al 2012 as a further refinement of these techniques the method that is implemented within coastsat introduces a new image classification component that refines the shoreline detection there are two core steps in this algorithm i image classification into four classes ii sub pixel resolution border segmentation these steps are further described below furthermore before running the batch shoreline detection on all the downloaded images the user has the option to manually digitise the position of a reference shoreline that approximates this sand water interface on one of the cloud free images the coordinates of this shoreline provide a reference for the subsequent automatically detected shorelines and assist to exclude outliers and false detections 2 3 1 image classification a neural network classifier civco 1993 referred to as multilayer perceptron in scikit learn is used to label each pixel of the image with one of four classes sand water white water other land features e g vegetation buildings rocky headlands a set of 20 features i e explanatory variables are used as inputs for the classifier these include the pixel intensity in 5 multispectral bands i e r g b nir swir1 5 commonly used spectral indices e g water index vegetation index etc and the variance calculated using a 3 3 moving window of each multispectral band and spectral index the image variance proved to be very efficient at discriminating sand pixels which have a low variance as the neighbouring pixels are also sand from rooftops and other isolated bright objects which tend to have a higher variance as they are surrounded by darker pixels e g roads and trees the classifier was trained beforehand the user does not have to train it with a set of 5 000 pixels in each class manually digitised from 50 satellite images covering 5 different sites training data available in vos 2019 a 10 fold cross validation was performed to test the accuracy of the classifier resulting in 99 of the pixels being correctly classified fig 2 includes an example sentinel 2 msi satellite image fig 2a and the corresponding classified image fig 2b additionally if the user s site of interest is made of dark sand a second classifier specifically trained on black grey sand beaches is available in the toolbox 2 3 2 sub pixel resolution border segmentation in this second step of shoreline detection the boundary between sand and water is extracted using the modified normalized difference water index mndwi applied to each of the classified images section 2 3 1 the mndwi xu 2006 is calculated as follows 1 m n d w i s w i r 1 g s w i r 1 g where swir1 and g are the pixel intensity in the short wave infrared band and green band respectively the mndwi values range between 1 and 1 as shown in fig 2c next a histogram of mndwi values is constructed with the labelled pixels located within a pre defined distance nominally set to 150 m in the toolkit from sand pixels as illustrated in fig 3 in the resulting histogram the probability density function pdf of the sand pixels is centred around positive values of mndwi while the water pixels have negative mndwi values accordingly the sand water threshold is computed applying otsu s thresholding algorithm otsu 1979 to find the mndwi value that maximises inter class variance between the sand and water distributions excluding upfront the pixels belonging to the white water and other land features class the presence of white water is known to cause errors in the shoreline detection hagenaars et al 2018 report seaward offsets of up to 40 m in the presence of wave induced foam and pardo pascual et al 2018 notes white water as being one of the largest sources of error indeed the histogram of mndwi values in fig 3 reveals that the pixels belonging to the white water class span a broad range of values with no distinct peak and therefore do not assist in discriminating the shoreline and may in fact lead to false detections the final step in the detection of individual shorelines is to compute the iso valued contour on the mndwi image for a level equal to the sand water threshold this operation is done at sub pixel resolution by applying the marching squares algorithm cipolletti et al 2012 lorensen and cline 1987 this iso valued contour represents the shoreline fig 2d 2e and 2f and matches with the sand water interface 2 4 post processing coastsat outputs for the user each mapped shoreline as a vector of 2d coordinates in a user defined spatial reference system associated with the exact acquisition date and time in coordinated universal time utc for quality control coastsat offers the option to visualise and accept reject each detected shoreline by the application of an interactive graphical tool once the shorelines have been extracted the coastsat toolkit offers the option to intersect the 2d shorelines with shore normal transects and extract time series of cross shore shoreline change along those transects the user can either manually digitise the transects with an ad hoc interactive tool or import the transect coordinates from a separate file to minimise the effect of localised water levels due to wave runup on the beach face i e swash the intersection between the instantaneous shoreline and the shore normal transects is calculated as the alongshore average of the shoreline points located within 25 m default value which can be modified by the user of the transect it is recommended that along coastlines with large tide ranges and or flatter beach profiles all shorelines be adjusted to a standard reference elevation since every image is acquired at a different stage of the tide a linear tidal correction can be applied using measured water levels and characteristic beach face slope to translate each shoreline to a reference elevation as follows 2 δ x z r e f z w l m where δ x is the cross shore horizontal shift along the shore normal transects z r e f is the reference elevation e g 0 m above mean sea level z w l is the local water level tide residuals at the time of image acquisition and m is a characteristic beach face slope the precise time of image acquisition to obtain the local water level is stored in the image metadata for sites where measurements of local water levels and beach slope may not be available practical guidance on tidal correction is provided in section 3 3 note that the tidal correction is not implemented in the toolbox as it is site specific 3 results and discussion 3 1 validation example this short communication describes the use of the coastsat software package to extract shorelines using gee images at any user defined coastline location and region of interest around the world for illustration the accuracy of the resulting satellite derived shoreline time series is compared here to in situ field measurements that have been obtained since the mid 1970 s at a survey transect located toward the north end of the narrabeen collaroy embayment in southeast australia and labelled as profile 1 pf1 in fig 2a narrabeen collaroy is a world renowned coastal monitoring site situated on the northern beaches of sydney the 3 6 km long embayment has been surveyed monthly since 1976 at five cross shore transects and the uninterrupted dataset is publicly available and described in turner et al 2016 the tidal regime is microtidal mean spring tide range 1 3 m and the characteristic beach face slope is 0 1 all suitable and publicly available satellite images covering narrabeen collaroy were retrieved and processed using coastsat as described in section 2 a total of 502 satellite derived shorelines were extracted between 1987 and 2018 a time series of shoreline cross shore position was obtained by computing the intersection between the extracted shorelines and the shore normal transect pf1 see fig 2d post processing of the shoreline data was undertaken to translate all data to zref 0 7 m relative to the australian height datum corresponding to mean high water springs at narrabeen fig 4 compares the time series of cross shore position obtained from in situ field measurements and satellite derived shorelines at this location the satellite derived shorelines match very well with the in situ observations with a coefficient of determination of 0 8 a rmse of 7 2 m a slight seaward bias of 1 4 m and 90 of the errors falling within 11 2 m the single validation example presented here for the purpose of illustration is representative of larger and more comprehensive analyses presented in vos et al 2019 where satellite derived shorelines were compared against long term in situ shoreline measurements at 5 diverse sites the observed horizontal accuracies rmse varied between 7 3 m at a microtidal beach with a steep beach face slope and 12 7 m at a mesotidal beach with a gentle slope 3 2 comparison to previous methods the time series validation example presented above shows that satellite derived shorelines from coastsat capture major erosion events e g 60 m of shoreline recession in june 2016 as well as seasonal to interannual behaviour the methodology is also capable of capturing the effects of engineering interventions and beach rotation see example applications in vos et al 2019 this result indicates that the enhancement of the image resolution section 2 2 and application of a sub pixel resolution technique section 2 3 2 makes it possible to map individual shorelines with sub pixel accuracy i e smaller than the pixel resolution of 10 m for sentinel 2 and 30 m for landsat the sub pixel methodology ensures no significant variability in the accuracy of the final satellite derived shoreline despite changing resolution of the satellites see table 1 in contrast to previous toolboxes using satellite data monegaglia et al 2018 montzka et al 2008 quan et al 2017 the coastsat toolkit leverages the gee software so that the user can simply and efficiently retrieve hundreds of multispectral images cropped around any user defined region of interest using a standard desktop computer this enables the user to bypass the time consuming task of manually downloading large satellite images from the usgs earth explorer or other web portals which are provided as large files of several gbs and may contain irrelevant data i e areas outside the region of interest thermal bands etc coastsat uses a newly developed generic and robust automated shoreline detection algorithm that integrates an image classification component into a sub pixel resolution border segmentation of the mndwi compared to previous studies that map the shoreline by applying a thresholding algorithm to the entire image e g hagenaars et al 2018 kuleli et al 2011 liu et al 2017 this new method uses a four class classification to specifically extract the boundary between pre classified water and sand pixels only removing from this critical process all other pixels corresponding to vegetation buildings roads white water rocky headlands etc this improvement refines the shoreline detection methodology as previous studies that derived satellite shorelines at narrabeen collaroy have reported lower accuracies for the same dataset for example the recent work by luijendijk et al 2018 in which the narrabeen dataset was also used as one of four validation cases reports an rmse of 13 7 m at this same location additionally this improved approach to shoreline detection can be more readily applied to different coastal settings since the algorithm is indifferent to the presence of urban areas forests or large amounts of white water in the image that are effectively excluded from the analyses by the new 4 class classification 3 3 practical guidance on tidal correction the satellites of the landsat and sentinel 2 constellations are launched into sun synchronous orbits which ensure that the angle of sunlight upon the earth s surface is constant for all image acquisitions to achieve this all landsat and sentinel 2 images are acquired between 10 and 10 30 a m local time at constant revisit periods of respectively 16 and 5 days consequently only fixed combinations of the diurnal semi diurnal and neap spring tidal cycles are sampled by the sun synchronous sensors which may result in tidal aliasing bishop taylor et al 2019 eleveld et al 2014 thus the time series of shoreline change along shore normal transects extracted with coastsat may contain a tidal bias that the user should be aware of for instance the non tidally corrected time series along pf1 at narrabeen result in a 7 2 m mean error compared to a 1 4 m mean error for the tidally corrected time series shown in fig 4 when applying the tidal correction with equation 2 users should employ the best water level and slope data that is available to them the unesco sea level station monitoring facility vliz 2018 and global sea level observing system psmsl 2019 provide data from a global network of tide gauges in the absence of a local tide gauge predicted tides can be obtained at any location in the world from global tide models such as fes2014 carrere et al 2016 the predicted tides do not capture tidal anomalies but they will minimise any potential bias in the time series caused by tidal aliasing a representative intertidal beach slope can be obtained from a gps survey or lidar flight of the area at narrabeen pf1 it was found that no significant improvement in the error statistics was obtained when using a time varying beach slope based on biweekly transect surveys instead of an average beach slope 4 conclusions the growing database of 30 years of satellite images freely available via google earth engine gee constitutes the longest continuously running monitoring of coastlines around the world this short communication describes and illustrates an open source python toolkit that enables the user to obtain time series of shoreline position at any coastline worldwide from 30 years and growing of publicly available satellite imagery the software is easy to use freely available and open source importantly the ability to observe and quantify shoreline changes over the past three decades is key to present day coastal management and coastsat represents a valuable tool for a range of potential users including but not limited to policy makers responsible for regulating coastal development coastal managers and engineers designing and implementing coastal protection researchers evaluating the impacts of climate change and oceanographic forcing within the coastal zone finally increasing population pressure on ocean coasts and climate change driven variations in storminess patterns frequency and intensity wave climate and water levels strongly suggest the need to plan for future adaptation of the coastal community mills et al 2018 however successful future coastal planning relies on an improved understanding of the links between spatial patterns of coastal change and environmental forcings regulated by climate drivers barnard et al 2015 in this context coastsat offers the capability to monitor coastline changes globally at short seasonal and long decadal time scales using satellite remote sensing to provide vital understanding of the relationship between climate drivers and coastline response into the future acknowledgments the authors wish to acknowledge the efforts of the united states geological survey and the european space agency in providing high quality open access data to the scientific community and google earth engine for facilitating the access to the archive of publicly available satellite imagery the authors are particularly grateful to sean vitousek and two anonymous reviewers for their insightful comments and suggestions that greatly improved the manuscript the authors would also like to thank chris leaman for helping to improve the coastsat toolbox funding for the routine monitoring at narrabeen is generously provided by northern beaches council formerly warringah council unsw research infrastructure scheme unsw faculty of engineering early career grant the nsw adaptation research hub coastal processes node office of environment and heritage and the australian research council lp0455157 lp100200348 dp150101339 lp170100161 manly hydraulics laboratory provided tide data on behalf of the nsw office of environment and heritage the first author is supported by the unsw scientia phd scholarship scheme appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104528 
26111,coastsat is an open source software toolkit written in python that enables the user to obtain time series of shoreline position at any sandy coastline worldwide from 30 years and growing of publicly available satellite imagery the toolkit exploits the capabilities of google earth engine to efficiently retrieve landsat and sentinel 2 images cropped to any user defined region of interest the resulting images are pre processed to remove cloudy pixels and enhance spatial resolution before applying a robust and generic shoreline detection algorithm this novel shoreline detection technique combines a supervised image classification and a sub pixel resolution border segmentation to map the position of the shoreline with an accuracy of 10 m the purpose of coastsat is to provide coastal managers engineers and scientists a user friendly and practical toolkit to monitor and explore their coastlines the software is freely available on github https github com kvos coastsat and is accompanied by guided examples jupyter notebook plus step by step readme documentation keywords google earth engine shoreline mapping landsat sentinel 2 sub pixel resolution software availability software name coastsat developer kilian vos year first official release 2018 hardware requirements pc system requirements windows linux mac program language python program size 1 mb availability https github com kvos coastsat license gpl 3 0 documentation readme in github repository and guided example in the form of an editable jupyter notebook 1 introduction the coastal region is the most heavily urbanised land zone in the world small et al 2011 and is regarded as a critical resource in view of its recreational environmental and economic importance yet ocean coasts are affected by variations in mean sea level extreme waves storm surges and river flow through a range of physical processes ranasinghe 2016 for a review recent intensification in mean wave energy reguero et al 2019 extreme coastal wave energy mentaschi et al 2017 and oceanic wind speeds young and ribal 2019 coupled with rising sea levels suggest that coastal areas will be exposed to increasing hazards in coming decades li et al 2018 it is therefore critical for coastal managers and policy makers regulating coastal development to observe and quantify these changes along coastlines vulnerable to extreme as well as subtle changes in oceanographic forcing e g barnard et al 2017 harley et al 2017 masselink et al 2016 data scarcity however remains an ongoing challenge as long term coastal monitoring programs based on in situ measurements are limited to only a few sites around the world e g barnard et al 2015 pianca et al 2015 turner et al 2016 indeed shoreline data is key to understand past beach behaviour splinter et al 2018 and calibrate numerical models capable of predicting future shoreline changes splinter et al 2013 publicly available remote sensing data from earth observation satellites provides a low cost solution to obtain long term observations of coastline changes over the past three decades at many sites worldwide crucially the advent of google earth engine gorelick et al 2017 has facilitated access to the growing archive of publicly available satellite imagery providing the opportunity for global scale analyses stretching back decades e g donchyts et al 2016 li et al 2019 luijendijk et al 2018 mentaschi et al 2018 space borne observations have been employed in a wide range of change detection applications including the analysis of meandering river morphodynamics monegaglia et al 2018 delineation of wetland footprints quinn and epshtein 2014 and identification of oil spills keramitsoglou et al 2006 recently optical imaging satellites have begun to be used to measure the location of the shoreline e g garcía rubio et al 2015 kuleli et al 2011 liu et al 2017 pardo pascual et al 2012 which is regarded by coastal managers planners engineers and scientists as a key indicator of how coastlines vary and evolve over time for instance hagenaars et al 2018 employed landsat 5 landsat 8 and sentinel 2 image composites to map the position of the shoreline with a horizontal accuracy of the order of half a pixel i e 15 m for landsat images and 5 m for sentinel 2 images while pardo pascual et al 2018 evaluated a sub pixel resolution shoreline detection technique at a low energy microtidal beach and reported horizontal accuracies of less than 10 m using landsat 7 landsat 8 and sentinel 2 images the aim of this short communication is to detail a new open source software toolkit called coastsat to obtain 30 year time series of approximately biweekly shoreline positions with a horizontal accuracy of 10 m at any sandy coastline worldwide demonstrated in the accompanying jupyter notebook and readme file is the suggested workflow to guide users to easily and efficiently retrieve the required satellite imagery from the google earth engine gee archive of publicly available satellite imagery apply a robust and generic sub pixel resolution shoreline detection algorithm and extract the required shoreline positions through time example practical applications of the coastsat toolkit applied at contrasting sandy coastlines tide range wave climate spanning europe the usa australia and new zealand are presented in vos et al 2019 where a range of storm seasonal and inter annual shorelines changes are illustrated 2 software capabilities the toolkit is compatible with python version 3 6 or higher and combines and extends the functionality of a number of other freely available python software packages the google earth engine gee python api package is used to access the satellite imagery while other machine learning and image processing packages namely scikit learn pedregosa et al 2011 and scikit image van der walt et al 2014 are employed to automatically extract the position of the shoreline from the multispectral imagery as is illustrated by a practical example provided in the accompanying jupyter notebook the sequence of functionalities provided within the single coastsat toolkit can be summarised as follows i retrieval of the images from the gee archive ii pre processing of the multispectral images cloud masking pansharpening and down sampling iii sub pixel resolution shoreline extraction and iv time series of shoreline position along shore normal transects coastsat outputs the mapped shorelines as a geojson file that can be imported into a gis application a flow chart of the different steps involved to obtain satellite derived shorelines at a user defined site is summarised in fig 1 and now described in further detail below 2 1 access to publicly available satellite imagery the coastsat toolkit provides an interface to the gee api to enable easy access to all the toa top of atmosphere reflectance images from the landsat 5 tm landsat 7 emt and landsat 8 oli tier 1 collections as well as sentinel 2 msi level 1c products which also represent toa reflectance toa images are calibrated to provide a standardised comparison between images acquired by different sensors on different dates chander et al 2009 thus they are suitable for time series analysis the user can choose which satellite mission s to use and all four missions can be used at the same time to maximise temporal resolution to significantly reduce the size of the individual files from several gbs to less than a mb per 5 km2 region a user defined region of interest roi is used to crop the images while still on the gee server and prior download in addition only the spectral bands that are required for the shoreline detection namely the three visible bands r g b the near infrared band nir and the short wave infrared band swir1 are included in the downloaded cropped image this implementation significantly increases processing speed and reduces memory usage on the local computer for example 30 years of biweekly images of a 5 km2 roi can be downloaded and processed in 2 h on a standard laptop 4 gb ram intel core i5 without running into memory allocation problems users should avoid requesting very large rois 100 km2 as this will significantly slow down the processing instead multiple smaller rois should be requested table 1 summarises the satellite data retrieved by coastsat including the gee collections in which the images are continuously stored and the time coverage revisit period and pixel size of each satellite mission 2 2 pre processing prior to extracting the shoreline the toa images are pre processed as follows cloud masking each image is supplemented by a quality assessment band pre computed by the data provider usgs for landsat or esa for sentinel 2 which contains a per pixel cloud mask with this information the percentage of cloud cover is calculated based on the number of cloudy pixels inside the region of interest and a user defined cloud cover threshold is used to discard all the images that exceed a certain percentage of cloud cover panchromatic image sharpening and down sampling in this next pre processing step the spatial resolution of the satellite image is enhanced to achieve an optimal shoreline detection for landsat 7 etm and landsat 8 oli images the higher resolution panchromatic band see table 1 is used to increase the resolution of the bands whose portion of the electromagnetic spectrum is covered by the panchromatic band from 30 to 15 m by applying a data fusion method based on principal component analysis tu et al 2001 in this method the multispectral bands are down sampled to 15 m by bilinear interpolation and decomposed in principal components then the first principal component is replaced by the panchromatic band after matching the histograms and retransformed back into the original multispectral space landsat 5 tm images do not have a panchromatic band however the 30 m bands are down sampled to 15 m by bilinear interpolation as this was found to improve the accuracy of the shoreline detection for sentinel 2 images the 20 m swir1 band is down sampled to 10 m by bilinear interpolation so that all the bands are at 10 m resolution it should be noted that the images are orthorectified by the data provider and no further geometric correction i e co registration is applied in coastsat the georeferencing accuracy of each image is stored in the output file containing the mapped shorelines 2 3 shoreline detection the shoreline is defined here as the instantaneous interface between water and sand captured at the instant of image acquisition the coastsat toolkit applies a robust sub pixel resolution shoreline detection algorithm to the pre processed satellite images the algorithm builds upon the sub pixel border segmentation by liu et al 2017 as was originally proposed by cipolletti et al 2012 as a further refinement of these techniques the method that is implemented within coastsat introduces a new image classification component that refines the shoreline detection there are two core steps in this algorithm i image classification into four classes ii sub pixel resolution border segmentation these steps are further described below furthermore before running the batch shoreline detection on all the downloaded images the user has the option to manually digitise the position of a reference shoreline that approximates this sand water interface on one of the cloud free images the coordinates of this shoreline provide a reference for the subsequent automatically detected shorelines and assist to exclude outliers and false detections 2 3 1 image classification a neural network classifier civco 1993 referred to as multilayer perceptron in scikit learn is used to label each pixel of the image with one of four classes sand water white water other land features e g vegetation buildings rocky headlands a set of 20 features i e explanatory variables are used as inputs for the classifier these include the pixel intensity in 5 multispectral bands i e r g b nir swir1 5 commonly used spectral indices e g water index vegetation index etc and the variance calculated using a 3 3 moving window of each multispectral band and spectral index the image variance proved to be very efficient at discriminating sand pixels which have a low variance as the neighbouring pixels are also sand from rooftops and other isolated bright objects which tend to have a higher variance as they are surrounded by darker pixels e g roads and trees the classifier was trained beforehand the user does not have to train it with a set of 5 000 pixels in each class manually digitised from 50 satellite images covering 5 different sites training data available in vos 2019 a 10 fold cross validation was performed to test the accuracy of the classifier resulting in 99 of the pixels being correctly classified fig 2 includes an example sentinel 2 msi satellite image fig 2a and the corresponding classified image fig 2b additionally if the user s site of interest is made of dark sand a second classifier specifically trained on black grey sand beaches is available in the toolbox 2 3 2 sub pixel resolution border segmentation in this second step of shoreline detection the boundary between sand and water is extracted using the modified normalized difference water index mndwi applied to each of the classified images section 2 3 1 the mndwi xu 2006 is calculated as follows 1 m n d w i s w i r 1 g s w i r 1 g where swir1 and g are the pixel intensity in the short wave infrared band and green band respectively the mndwi values range between 1 and 1 as shown in fig 2c next a histogram of mndwi values is constructed with the labelled pixels located within a pre defined distance nominally set to 150 m in the toolkit from sand pixels as illustrated in fig 3 in the resulting histogram the probability density function pdf of the sand pixels is centred around positive values of mndwi while the water pixels have negative mndwi values accordingly the sand water threshold is computed applying otsu s thresholding algorithm otsu 1979 to find the mndwi value that maximises inter class variance between the sand and water distributions excluding upfront the pixels belonging to the white water and other land features class the presence of white water is known to cause errors in the shoreline detection hagenaars et al 2018 report seaward offsets of up to 40 m in the presence of wave induced foam and pardo pascual et al 2018 notes white water as being one of the largest sources of error indeed the histogram of mndwi values in fig 3 reveals that the pixels belonging to the white water class span a broad range of values with no distinct peak and therefore do not assist in discriminating the shoreline and may in fact lead to false detections the final step in the detection of individual shorelines is to compute the iso valued contour on the mndwi image for a level equal to the sand water threshold this operation is done at sub pixel resolution by applying the marching squares algorithm cipolletti et al 2012 lorensen and cline 1987 this iso valued contour represents the shoreline fig 2d 2e and 2f and matches with the sand water interface 2 4 post processing coastsat outputs for the user each mapped shoreline as a vector of 2d coordinates in a user defined spatial reference system associated with the exact acquisition date and time in coordinated universal time utc for quality control coastsat offers the option to visualise and accept reject each detected shoreline by the application of an interactive graphical tool once the shorelines have been extracted the coastsat toolkit offers the option to intersect the 2d shorelines with shore normal transects and extract time series of cross shore shoreline change along those transects the user can either manually digitise the transects with an ad hoc interactive tool or import the transect coordinates from a separate file to minimise the effect of localised water levels due to wave runup on the beach face i e swash the intersection between the instantaneous shoreline and the shore normal transects is calculated as the alongshore average of the shoreline points located within 25 m default value which can be modified by the user of the transect it is recommended that along coastlines with large tide ranges and or flatter beach profiles all shorelines be adjusted to a standard reference elevation since every image is acquired at a different stage of the tide a linear tidal correction can be applied using measured water levels and characteristic beach face slope to translate each shoreline to a reference elevation as follows 2 δ x z r e f z w l m where δ x is the cross shore horizontal shift along the shore normal transects z r e f is the reference elevation e g 0 m above mean sea level z w l is the local water level tide residuals at the time of image acquisition and m is a characteristic beach face slope the precise time of image acquisition to obtain the local water level is stored in the image metadata for sites where measurements of local water levels and beach slope may not be available practical guidance on tidal correction is provided in section 3 3 note that the tidal correction is not implemented in the toolbox as it is site specific 3 results and discussion 3 1 validation example this short communication describes the use of the coastsat software package to extract shorelines using gee images at any user defined coastline location and region of interest around the world for illustration the accuracy of the resulting satellite derived shoreline time series is compared here to in situ field measurements that have been obtained since the mid 1970 s at a survey transect located toward the north end of the narrabeen collaroy embayment in southeast australia and labelled as profile 1 pf1 in fig 2a narrabeen collaroy is a world renowned coastal monitoring site situated on the northern beaches of sydney the 3 6 km long embayment has been surveyed monthly since 1976 at five cross shore transects and the uninterrupted dataset is publicly available and described in turner et al 2016 the tidal regime is microtidal mean spring tide range 1 3 m and the characteristic beach face slope is 0 1 all suitable and publicly available satellite images covering narrabeen collaroy were retrieved and processed using coastsat as described in section 2 a total of 502 satellite derived shorelines were extracted between 1987 and 2018 a time series of shoreline cross shore position was obtained by computing the intersection between the extracted shorelines and the shore normal transect pf1 see fig 2d post processing of the shoreline data was undertaken to translate all data to zref 0 7 m relative to the australian height datum corresponding to mean high water springs at narrabeen fig 4 compares the time series of cross shore position obtained from in situ field measurements and satellite derived shorelines at this location the satellite derived shorelines match very well with the in situ observations with a coefficient of determination of 0 8 a rmse of 7 2 m a slight seaward bias of 1 4 m and 90 of the errors falling within 11 2 m the single validation example presented here for the purpose of illustration is representative of larger and more comprehensive analyses presented in vos et al 2019 where satellite derived shorelines were compared against long term in situ shoreline measurements at 5 diverse sites the observed horizontal accuracies rmse varied between 7 3 m at a microtidal beach with a steep beach face slope and 12 7 m at a mesotidal beach with a gentle slope 3 2 comparison to previous methods the time series validation example presented above shows that satellite derived shorelines from coastsat capture major erosion events e g 60 m of shoreline recession in june 2016 as well as seasonal to interannual behaviour the methodology is also capable of capturing the effects of engineering interventions and beach rotation see example applications in vos et al 2019 this result indicates that the enhancement of the image resolution section 2 2 and application of a sub pixel resolution technique section 2 3 2 makes it possible to map individual shorelines with sub pixel accuracy i e smaller than the pixel resolution of 10 m for sentinel 2 and 30 m for landsat the sub pixel methodology ensures no significant variability in the accuracy of the final satellite derived shoreline despite changing resolution of the satellites see table 1 in contrast to previous toolboxes using satellite data monegaglia et al 2018 montzka et al 2008 quan et al 2017 the coastsat toolkit leverages the gee software so that the user can simply and efficiently retrieve hundreds of multispectral images cropped around any user defined region of interest using a standard desktop computer this enables the user to bypass the time consuming task of manually downloading large satellite images from the usgs earth explorer or other web portals which are provided as large files of several gbs and may contain irrelevant data i e areas outside the region of interest thermal bands etc coastsat uses a newly developed generic and robust automated shoreline detection algorithm that integrates an image classification component into a sub pixel resolution border segmentation of the mndwi compared to previous studies that map the shoreline by applying a thresholding algorithm to the entire image e g hagenaars et al 2018 kuleli et al 2011 liu et al 2017 this new method uses a four class classification to specifically extract the boundary between pre classified water and sand pixels only removing from this critical process all other pixels corresponding to vegetation buildings roads white water rocky headlands etc this improvement refines the shoreline detection methodology as previous studies that derived satellite shorelines at narrabeen collaroy have reported lower accuracies for the same dataset for example the recent work by luijendijk et al 2018 in which the narrabeen dataset was also used as one of four validation cases reports an rmse of 13 7 m at this same location additionally this improved approach to shoreline detection can be more readily applied to different coastal settings since the algorithm is indifferent to the presence of urban areas forests or large amounts of white water in the image that are effectively excluded from the analyses by the new 4 class classification 3 3 practical guidance on tidal correction the satellites of the landsat and sentinel 2 constellations are launched into sun synchronous orbits which ensure that the angle of sunlight upon the earth s surface is constant for all image acquisitions to achieve this all landsat and sentinel 2 images are acquired between 10 and 10 30 a m local time at constant revisit periods of respectively 16 and 5 days consequently only fixed combinations of the diurnal semi diurnal and neap spring tidal cycles are sampled by the sun synchronous sensors which may result in tidal aliasing bishop taylor et al 2019 eleveld et al 2014 thus the time series of shoreline change along shore normal transects extracted with coastsat may contain a tidal bias that the user should be aware of for instance the non tidally corrected time series along pf1 at narrabeen result in a 7 2 m mean error compared to a 1 4 m mean error for the tidally corrected time series shown in fig 4 when applying the tidal correction with equation 2 users should employ the best water level and slope data that is available to them the unesco sea level station monitoring facility vliz 2018 and global sea level observing system psmsl 2019 provide data from a global network of tide gauges in the absence of a local tide gauge predicted tides can be obtained at any location in the world from global tide models such as fes2014 carrere et al 2016 the predicted tides do not capture tidal anomalies but they will minimise any potential bias in the time series caused by tidal aliasing a representative intertidal beach slope can be obtained from a gps survey or lidar flight of the area at narrabeen pf1 it was found that no significant improvement in the error statistics was obtained when using a time varying beach slope based on biweekly transect surveys instead of an average beach slope 4 conclusions the growing database of 30 years of satellite images freely available via google earth engine gee constitutes the longest continuously running monitoring of coastlines around the world this short communication describes and illustrates an open source python toolkit that enables the user to obtain time series of shoreline position at any coastline worldwide from 30 years and growing of publicly available satellite imagery the software is easy to use freely available and open source importantly the ability to observe and quantify shoreline changes over the past three decades is key to present day coastal management and coastsat represents a valuable tool for a range of potential users including but not limited to policy makers responsible for regulating coastal development coastal managers and engineers designing and implementing coastal protection researchers evaluating the impacts of climate change and oceanographic forcing within the coastal zone finally increasing population pressure on ocean coasts and climate change driven variations in storminess patterns frequency and intensity wave climate and water levels strongly suggest the need to plan for future adaptation of the coastal community mills et al 2018 however successful future coastal planning relies on an improved understanding of the links between spatial patterns of coastal change and environmental forcings regulated by climate drivers barnard et al 2015 in this context coastsat offers the capability to monitor coastline changes globally at short seasonal and long decadal time scales using satellite remote sensing to provide vital understanding of the relationship between climate drivers and coastline response into the future acknowledgments the authors wish to acknowledge the efforts of the united states geological survey and the european space agency in providing high quality open access data to the scientific community and google earth engine for facilitating the access to the archive of publicly available satellite imagery the authors are particularly grateful to sean vitousek and two anonymous reviewers for their insightful comments and suggestions that greatly improved the manuscript the authors would also like to thank chris leaman for helping to improve the coastsat toolbox funding for the routine monitoring at narrabeen is generously provided by northern beaches council formerly warringah council unsw research infrastructure scheme unsw faculty of engineering early career grant the nsw adaptation research hub coastal processes node office of environment and heritage and the australian research council lp0455157 lp100200348 dp150101339 lp170100161 manly hydraulics laboratory provided tide data on behalf of the nsw office of environment and heritage the first author is supported by the unsw scientia phd scholarship scheme appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104528 
26112,understanding the spatiotemporal evolution of drought is vital for effective water resources management especially in arid and semi arid regions and under climate change in this study we developed the soil and water assessment tool swat based drought evaluation tool and used it to investigate the spatiotemporal change of drought and its driving factors over the past 50 years 1965 2014 in a typical semi arid area the wei river basin in the loess plateau the temporal trend analysis of precipitation showed an intensified hydrological cycle with a longer dry interval and the substantially decreased wind speed resulted in a significant decrease in the evapotranspiration and a slight increase in the soil water content the spatiotemporal analysis of drought identified the vulnerable areas and indicated that spring drought was exacerbating overall this study can be informative and valuable for the drought assessment and disaster alleviation in the loess plateau area graphical abstract image 1 keywords climate change driving factors drought index swat loess plateau software availability name of software swat based drought evaluation tool description this tool can facilitate the spatiotemporal analysis and presentation of drought e g pdsi spi and spei across a watershed just based on an existing swat project without the need of efforts to prepare input for drought index calculation developers yiping wu shengnan zhang yuzhu sun availability contact the developers 1 introduction drought is an accumulated and recurring natural hazard caused by a sustained deficit of precipitation p or excess evapotranspiration et over an extended period liu et al 2016 mckee et al 1993 touma et al 2015 there is convincing evidence that global climate change has increased the frequency and severity of extreme drought events in recent decades leading to the destruction of ecological environment economic loss including in terms of agriculture and even conflicts among human communities chang et al 2016b dai 2012 the issue of how climate change affects drought has evoked high levels of interest of many researchers in the different fields concerned dai 2011b mishra et al 2010 seneviratne et al 2010 wang et al 2011 sun et al 2019a a key prerequisite for drought research is the identification of drought using credible drought indices chang et al 2016b there are many statistical drought indices that can be used to investigate the drought evolution and hazard evaluation however these indices are relatively simple because they only focus on either climatic or hydrological aspects for example the standardized precipitation index spi guttman 1999 mckee et al 1993 thilakarathne and sridhar 2017 and the standardized precipitation evapotranspiration index spei vicente serrano et al 2010 are generated based on the precipitation or and potential evapotranspiration pet while the streamflow drought index sdi nalbantis and tsakiris 2009 was only calculated based on streamflow simple indices are easy to use but may mean ignoring the driving factors of drought and may lead to unreliable outcomes due to the lack of some other important variables e g soil moisture the palmer drought severity index pdsi was originally proposed by palmer 1965 for the purpose of measuring the cumulative departure in sur face water balance which may be considered superior to other statistically based drought indices as the pdsi can account for the more comprehensive effect of global warming through palmer s water balance model on dry and wet spells dai 2011a b hao and singh 2015 mishra and singh 2010 it takes previous and current moisture supply p and demand pet into a hydrological accounting system which includes a two layer bucket type model for soil moisture calculations chen et al 2012 dai 2011a nevertheless the pet calculated using the thornthwaite equation in the original palmer model could lead to the overestimation of the effect of temperature on drought guttman et al 1992 and inaccuracy in energy limited regions hobbins et al 2008 as the thornthwaite pet is based only on temperature month and latitude to minimize these errors the more advanced penman monteith equation was used to calculate the pet which accounts for the effects of radiation wind speed and humidity self calibrating pdsi sc pdsi replaces empirical constants e g duration factors climatic characteristics coefficient and other environmental constants with dynamically calculated values at any location to make the pdsi comparable spatially nathan et al 2004 and studies have reported that the sc pdsi can describe drought evolution well jia et al 2014 senatore et al 2019 wang et al 2015 however the sc pdsi is still deficient in its soil moisture calculation included in the two layer bucket type model and fails to consider the effects of some factors e g the spatial heterogeneity of soil vegetation cover and topography dai 2011a guttman 1998 zou et al 2017 showing a certain weakness in drought investigation under these situations combining drought indices with hydrological models chattopadhyay et al 2017 kang and sridhar 2017a sun et al 2014 has become attractive for example leng et al 2015 used the variable infiltration capacity vic model liang et al 1994 to project future hydrological changes in china and then calculate the spi standardized runoff index sri and standardized soil moisture index sswi to assess the climate change impact on droughts from meteorological agricultural and hydrologic perspectives li et al 2017 employed the soil and water assessment tool swat arnold et al 1998 to support the calculation of spi sswi and standardized streamflow index ssi kang and sridhar 2018a assessed the climate change impacts on various drought indices e g pdsi ssi and sdi by combining both swat and vic with multiple climate models and found the two hydrological models can be applicable to investigate and forecast the future drought evolution the distributed model swat is a useful tool to simulate the hydrological response to climate and land use changes qiu et al 2017 sun et al 2017 wu et al 2012 2014a 2018 yang et al 2018 zhao et al 2018a in view of the merits and demerits of the pdsi mentioned above combining swat and pdsi can be an effective approach to investigate the drought evolution under climate change because the former could simulate the hydrological processes under dynamic land surface conditions e g climate elevation soil land use and management and yield multiple hydrological components e g water yield recharge and evapotranspiration some of which are difficult to observe but needed in pdsi calculation kang and sridhar 2018b therefore the pdsi combined with swat can be more meaningful compared to the original one and this is also our motivation for the present study the wei river basin wrb located in the southern chinese loess plateau was regarded to be more sensitive to climate change and highly prone to drought zhao et al 2013 2015b for the wrb several studies have found its disastrous drought periods in the past e g 1962 1972 1987 and 1990s and the drought situation in this region has been deteriorating ma et al 2013 sun et al 2019b yuan et al 2016 zhang et al 2013 2015 however most of the drought indicators e g p spi and streamflow for this region have failed to link with climate change chang et al 2016a huang and wang 2016 tan et al 2011 zuo et al 2015 although chang et al 2016b and zhao et al 2015a used an integrated index and a model based drought index respectively to investigate the spatiotemporal characteristics of drought variations there remain questions on the driving factors of drought evolution at a more detailed and systematic level therefore a further study on the drought evolution mechanism is needed the overall goal of this study was to investigate the drought evolution induced by climate change in the wrb for the 50 year 1965 2014 period using pdsi combined with swat the specific objectives of the present study were to 1 combine swat with pdsi calculation and evaluate its performance 2 investigate the spatiotemporal changes of drought and its key driving factors climatic and hydrological factors and 3 identify the most vulnerable susceptible areas that are prone to drought under climate change the study could help understand the impacts of climate change on drought in time and space for a semi arid area and provide a theoretical basis for rational utilization of water resources effective management of farmland and scientific planning of land use for this region 2 materials and methods 2 1 study area the wei river originates from the north side of the niaoshu mountains in the western part of china flows eastward and finally discharges into the yellow river with its total length of 818 km and its drainage area of 134 800 km2 fig 1 the wrb 33 5 n 37 5 n and 103 5 e 110 5 e is mainly located in the southern part of the loess plateau with its elevation ranging from 319 m in the southeast to 3917 m in the western mountainous region based on the past 50 year 1965 2014 hydro climatic data the annual mean temperature t ranged from 6 2 c in the northwest to 13 5 c in the southeast and the annual mean p and et were about 538 mm and 436 mm respectively besides the distribution of p exhibits noticeable spatial difference and annual variations in the wrb there are three main soil types cambisols anthrosols and fluvisols accounting for 65 1 8 9 and 6 5 of the basin respectively the cambisols mainly covers the central north area and the qinling mountains region southern part of the basin the anthrosols is distributed in the middle and downstream area of the basin and the fluvisols is along the river channels there are also three major land use types cropland grassland and woodland accounting for 43 6 32 1 and 16 2 respectively the cropland is mainly located in the low altitude area the grassland characterized by middle and low coverage is relatively evenly distributed and the woodland is mainly located in the qinling mountains region and the southern contiguous area of the jing river subbasin and the beiluo river subbasin 2 2 model description the swat model developed by the agricultural research service of the united states department of agriculture usda ars is a distributed and physically based watershed hydrological model arnold et al 1998 gassman et al 2014 romagnoli et al 2017 wu et al 2018 for exploring the hydrological cycle plant growth transportation of sediment and agricultural chemical yields on a daily time step arnold et al 1998 panagopoulos et al 2012 pyo et al 2017 stefanidis et al 2018 wu et al 2012 zhao et al 2019 the main outputs of swat are surface runoff lateral flow baseflow et soil water content sw water yield wld sediment load and nutrient loads further details about swat are available in its theoretical documentation neitsch et al 2011 the well established swat model has been used in a variety of watershed related studies including climate and land cover changes at watershed scales qiu et al 2017 wu and johnston 2007 wu et al 2014a zhao et al 2018a therefore we chose this model in the present study to obtain long term distributed hydrological variables which are difficult to be measured in reality 2 3 model input and setup a geographic information system gis interface arcswat version 2012 was employed to set up the model the digital elevation model dem data with a 90 m spatial resolution from the shuttle radar topographic mission srtm jarvis et al 2006 was used to delineate the basin into 192 subbasins the land use 1 km 1 km and soil properties 1 km 1 km of this region were obtained from the ecological and environmental science data center for west china http westdc westgis ac cn based on the combination of land use soil and slope the wrb was discretized into 3470 hydrological response units hrus the daily meteorological data from 20 stations for the period 1965 2014 were collected from the data center of the china meteorological administration http data cma cn including p maximum air temperature tx minimum air temperature tn relative humidity rh wind speed ws and sunshine duration the sunshine duration here was used to calculate the solar radiation sr required for the swat 2 4 model calibration and validation in this study we used the swat cup swat calibration and uncertainty programs sufi 2 abbaspour et al 2004 2007 rouholahnejad et al 2012 to calibrate the swat parameters using a 10 year 1981 1990 record of streamflow and validate them with another 10 year 1971 1980 streamflow observations additionally we used a 5 year warm up period 1966 1970 to minimize the impacts of uncertain initial conditions in the model simulation based on the previous publications and our own experience related to swat calibration abbaspour et al 2007 wu et al 2012 2014b zhao et al 2018b zuo et al 2016 we selected six sensitive parameters for the model calibration and the fitted values of the selected parameters were listed in table 1 2 5 combination of swat and pdsi the calculation of pdsi requires p pet et sw and wld it is quite challenging to measure most of these variables at regional scale here we proposed to use the simulated hydrological components of swat as the input for pdsi calculation as listed in table 2 extensive details of the background theory and detailed methodology of the pdsi are already available in the literature nathan et al 2004 palmer 1965 zhai et al 2010 zhao et al 2015a the main steps for pdsi calculation in this study are described below and the classification of pdsi is shown in table a1 the empirical constants involved in this procedure were the recommended values by the china meteorological administration cma 2006 based on this the pdsi calculation in this study can be expressed as 1 x i z 1 63 0 775 x i 1 where x i and x i 1 are the values of pdsi in month i and i 1 z is the moisture anomaly index which can be given as 2 z d k where d is the departure of moisture from normal k is the modified climatic characteristic coefficient given by 3 k 16 84 1 12 d k k where d is the average absolute value of d and k is the estimated value of climatic characteristic coefficient given by 4 k 1 6 log 10 p e r r o p l 2 8 d 0 4 where p e r r o p l and d are the multi year averages of pet recharge actual runoff actual precipitation soil moisture loss and d respectively departure of moisture from normal d is computed by estimating the gaps between actual precipitation p and climatic optimum precipitation p which is described as 5 d p p the variable p as the expected precipitation supply is climatically appropriate for the existing conditions cafec based on the water balance equation the water balance of pdsi can be expressed as follows 6 p et r ro l where et r ro and l are the climatic optimum et recharge runoff and soil moisture loss respectively here the runoff used in the pdsi is not the surface runoff zhao et al 2015a but the wld the climatic optimum values above can be defined as follows 7 et α pe 8 r β pr 9 ro γ pro 10 l δ pl where pe pr pro and pl are the potential values of et recharge runoff and soil moisture loss respectively and the coefficients α β γ and δ are the ratios of the actual multi year averages to the potential multi year averages over the simulation period the evaluation of a drought index is really challenging because there is not a criterion available and different indices may use different input variables nevertheless drought indices e g spi spei and pdsi are used to represent the severity of drought by depicting different aspects of drought e g climatic aspect or hydrological aspect and they are regarded to be dependent hao and singh 2015 thus comparison of a drought index with others has been widely adopted to evaluate the performance or suitability of a drought index hao and aghakouchak 2013 sehgal et al 2017 sun et al 2018 zhang et al 2019 zhao et al 2017 in this study we employed spi and spei for the comparison and verification of the performance of pdsi the spi was calculated for a certain time scale based on the probability distributions of p while the calculation of spei is the same but with one more variable pet the classification thresholds and the drought categories are shown in table a1 the detailed formulation of the spi calculation is available from appendix b 2 6 analysis methods for the key variables and the drought indices we used the pearson chi square normality test to check their distribution pearson 1900 and employed the simple linear regression with least squares method to detect their trends rahimi 2017 zou et al 2003 if the slopes of the fitted linear lines are significantly different from zero t test p 0 05 the trends are considered to be statistically significant student 1908 for the 50 year 1965 2014 study period the trends of key climatic variables were analyzed by site and the inverse distance weighted idw interpolation method shepard 1968 was then used to present their spatial distributions however the trends of key hydrological variables and drought indices were investigated at the subbasin level 3 results 3 1 model examination to evaluate the model performance for the calibration 1981 1990 and validation 1971 1980 periods we performed both graphical representation and statistical analysis using the simulated and observed monthly streamflows fig 2 shows the comparison of simulated monthly streamflows against the observations at two gaging stations weijiapu and huaxian the plots indicate that the simulated and observed monthly streamflows matched well for much of the period considered and followed the p pattern in their variations in addition to the time series we selected four statistical indicators to measure the model performance nash sutcliffe efficiency nse percent bias pbias ratio of the root mean square error to the standard deviation of measured data rsr and squared correlation coefficient r2 based on the criteria set out by moriasi et al 2007 the performance of the model at both the gaging stations table 3 can be judged as good satisfactory in the calibration validation period with nse 0 65 pbias 15 and rsr 0 60 nse 0 50 pbias 25 and rsr 0 70 besides the values of r2 at the two gages were higher than 0 6 for both calibration and validation periods overall the model performance was satisfactory for the subsequent analysis 3 2 spatiotemporal analysis of key climate variables the meteorological factors including p t ws and rh could influence pet and thus et sw and drought conditions actually these factors are required inputs for calculating drought indices e g spi spei and pdsi therefore we studied the changes of p tx maximum air temperature tn minimum air temperature mt mean air temperature ws and rh in this section in addition drought situations could change even if there is no trend for annual precipitation like what we presented in the current study because of the changes of precipitation mode including no rain days nr light rain days lr etc for example the increasing nr and the decreasing lr as shown in this study implied a longer interval between two rainfall events resulting in an exacerbated drought when other variables remain unchanged for all of these we utilized the linear regression and t test serrano et al 1999 to obtain their annual trends and significance levels for the whole 50 year 1965 2014 study period and then obtained the spatial distributions of their annual trends using the idw interpolation in arcgis we also examined the seasonal trends of p nr and lr and analyzed the variations of rainfall intensity 1 1 1 spatial distributions of annual trends of key climate variables as shown in fig 3 p close to the basin outlet had the largest decrease 4 13 mm year p 0 05 despite no trend for the entire basin the nr exhibited an increasing trend especially for the headwater area the far south part and the eastern part of the beiluo river subbasin p 0 05 the results for the lr were found to be opposite to those for nr for instance lr in most areas had a decreasing trend and the region with a significant drop in lr also roughly had a significant rise in nr based on the above analysis more dry days or longer intervals between rainfall events could be expected this was also in agreement with the results obtained in an earlier study for the east river basin wu et al 2014a for t whether it is tx tn or mt there was a significant upward trend 0 01 0 07 c year for the whole basin with the northern area having a greater rise in t for ws most stations showed a downward trend and 12 stations were statistically significant additionally a trend change point of ws from decrease to increase was found around the year 2000 for 7 stations with 6 of these stations having a statistically significant upward trend after 2000 figure s1 for rh there was no obvious temporal trend or spatial features for the whole basin broadly speaking a warmer climate characterized by more dry days was expected despite no trend in p the significant increase in t and the large decrease in ws would have the opposite effects on et 1 1 2 p nr and lr in dry and wet seasons for p nr and lr we examined their trends for the wet season april through september in a year and the dry season january through march and october through december in a year respectively as seen from fig 4 both seasons had the decreasing trend of p in the headwater and the far downstream areas in the dry season a significant decrease of p occurred in the main cropland area which might have exacerbated the spring drought the high correlation between lr and nr mentioned above fig 3 were also presented in both seasons the greater the rise of nr the greater the drop of lr for the trends of p nr and lr their spatial distributions in the wet season were generally in accordance with those on the annual scale fig 3 implying that the variations of p nr and lr in the wet season might dominate the annual trends of them although there was no trend in the annual p for the entire basin the decrease of p in the dry season might cause the aggravation of spring drought especially for the cropland area the conversion of lr to nr was apparent especially in the wet season 1 1 3 variations of rainfall intensity based on different rainfall intensities the statistical analysis of rainfall amount was carried out for the five 10 year periods 1965 1974 1975 1984 1985 1994 1995 2004 and 2005 2014 as shown in fig 5 there was no consistent change in the rainfall amounts among the five listed rainfall intensities however on the annual scale there was a significant downward trend p 0 05 in the light rain 0 10 mm and an increasing trend p 0 1 in the heavy rain respectively in particular a clear increase can be found in the heavy rain 50 mm for the last three 10 year periods 1985 2014 3 3 spatiotemporal analysis of key hydrological variables for p pet et sw and wld we plotted the scatter diagrams with their annual trends and significance levels figure s2 the combined effects of the increased t and decreased ws caused a slight decrease in pet but it is not statistically significant the slight decrease in pet and p resulted in a significant reduction of et 0 71 mm year p 0 05 and a slight rise of sw not significant fig 6 shows the spatial distributions of annual trends of p pet et and sw for the 50 year 1965 2014 study period at the subbasin level assuming that the annual sr did not change the strong spatial variability of the pet trend was attributed to the combined influence of t ws and rh past studies have shown that ws was the primary factor affecting the pet in the wrb gao et al 2006 the significant rise in pet in the headwater can be undoubtedly explained by the decrease in rh and the increase in t and ws considering the negative impact of the fall in ws the significant rise in pet in the source area of the jing river subbasin may have been caused by the rise in t and the fall in rh in the areas near the wugong yaoxian and foping weather stations the significant downward trend in pet resulted from the large decrease of ws considering the positive impact of the rising t compared with pet the et was affected by more factors such as landform and land use the region with a significant downward trend in et was mainly located in the overlap area of the ws falling zone and the p falling zone for the sw its spatial distribution of annual trend was very similar to that of p except for the southern region where the et had a significant decrease to further study the drought conditions every ten years we analyzed the variations of decadal p et and sw at the subbasin level figure c1 a wet period 1975 1984 and a dry period 1995 2004 can be seen from the spatial distributions of decadal p and sw for the et a clear downward trend was found which could mainly be attributed to the decrease in ws 3 4 spatiotemporal analysis of drought to verify the performance of pdsi we calculated the corresponding spi and spei series and then compared pdsi series with them if the pdsi performed well we would apply it to capture the spatiotemporal characteristics of drought evolution in the wrb on the annual and seasonal scales 3 4 1 performance of pdsi to examine the performance of pdsi on the annual scale we used linear regression to analyze the correlations between pdsi and p spi and spei respectively fig 7 it can be easily observed that the pdsi had a very good agreement with p spi and spei with correlation coefficients of 0 70 0 70 and 0 69 respectively in addition the decadal mean value of pdsi had a similar spatial distribution with the corresponding spi and spei series at the subbasin level figure c2 and the line charts of standardized anomalies of p spi spei and pdsi showed quite synchronous fluctuations figure c3 on the seasonal scale fig 8 the trends of pdsi spi and spei in spring march through may in a year and summer june through august in a year were analyzed at the subbasin level to account for and examine the obvious trend characteristics and the strong spatial variability that exist in both seasons the peaks and valleys of pdsi mostly corresponded to those defined by spi and spei in spring all the three drought indices showed a downward trend indicating that the spring drought might have been aggravated in the past five decades however the spatial distribution of the spring trend of pdsi was greatly different from those of spi and spei exept for the far downstream area the reason for this difference may be that the variables involved in the definition of drought indices are different the p is the dominant variable of spi and spei and the decrease in p in spring caused more severe drought that was defined by spi and spei in summer an insignificant upward trend in the three indices was observed and the spatial variability of pdsi trend was very similar to that of the spi and spei trends to assess the effectiveness of pdsi in capturing drought events we compared the annual pdsi values with the historical severe drought events that were recorded inyearbooks during 1965 2014 from fig 9 it can be seen clearly that the values of pdsi in 1972 1987 1995 and 1997 were less than 2 00 moderate drought threshold which corresponded to four serious drought events indicating that the drought in the basin can be well captured by the pdsi overall the pdsi performed well in the wrb on the decadal annual and seasonal scales and therefore it can be used to reliably capture drought events and define the spatiotemporal features of drought evolution 3 4 2 spatiotemporal features of drought evolution for the spatiotemporal features of drought evolution we analyzed the pdsi series using linear regression and graphical representation for the 50 year 1965 2014 study period 3 4 2 1 analysis on annual scale as shown in fig 9 the drought condition was most severe during 1990 2000 gradually softened and turned to wetness after 2000 in addition the year 2000 was also the trend change point of ws for seven weather stations therefore we chose this year as the time point to divide the pdsi series into two parts although there was no change in pdsi for the whole period a significant rise in pdsi occurred after 2000 by comparison figure s2 we found that the pdsi followed the p and sw pattern roughly in its annual variation and the dominant influencing factor of the pdsi was the p in general the spatial distribution of pdsi trend was similar to that of p a significant decrease of pdsi had occurred in the headwater and the far downstream areas and a slight decrease of pdsi was observed in the middle region of the jing river subbasin according to the principle of water balance the reason for the drought aggravation in the above areas was that the negative effects of the falling p and sw eliminated the positive effect of the falling et the significant wetting trend near the yaoxian weather station was mainly due to the significant rise in sw which in turn resulted from the significant decrease of et 3 4 2 2 analysis on seasonal scale in addition to the annual characteristics of drought evolution we analyzed its seasonal features defined by the pdsi fig 8 in spring a decreasing trend of pdsi was found for most areas especially for the headwater and the far downstream areas in summer the spatial distribution of pdsi trend had a clear spatial variability that was in line with that on the annual scale according to the changes in pdsi we found a significant wetting period 2000 2014 and examined four severe drought events in the past 50 year 1965 2014 study period based on the spatial distribution of pdsi annual trend we identified the vulnerable susceptible areas that are prone to drought the downstream area that is quite close to the basin outlet the headwater and the middle region of the jing river subbasin on the seasonal scale it should be noted that the wrb might face the challenge of spring drought and the spatial inhomogeneity of drought changes in summer 4 discussion in this study we developed a process based drought evaluation tool by combining the swat model and pdsi the aim of this tool was to provide such a flexible framework for investigating the drought evolution and evaluation so that the precautions can be effectively made towards drought at the watershed scale the case study in this paper helps us identify where and when the drought event is most likely to occur and what is the primarily driving forces of the drought in the wrb there are a few reasons why we combine swat and pdsi for drought evaluation compared with statistically based drought indices pdsi is more comprehensive in evaluating drought based on water balance however the calculation process of pdsi is complex the required variables are numerous and some of them are difficult to observe also the calculation of original pdsi did not take the influence of underlying surface heterogeneity into account e g dem soil land use and management to make up for the above deficiencies it is a good approach to combine pdsi with hydrological models the swat model is a process based continuous distributed hydrological model which has been widely used in simulating the hydrological cycles around the world once the swat model is verified it can accurately depict the real hydrological processes at a specific watershed compared to the statistical models because the model considers the land surface conditions e g dem climate soil land use and management during the simulation process we then take the spatially distributed hydrological components from the swat model e g pet et sw and wld as the inputs for calculating pdsi in this sense the pdsi based on swat can be regarded as the process based drought index which is more meaningful compared to the original one the high efficiency in drought diagnose is of great importance in efficiently response to drought our developed drought evaluation tool can facilitate the spatiotemporal analysis and presentation of drought across a watershed just based on an existing swat project without the need of efforts to prepare additional inputs allowing the users policy makers easily and timely get the drought information in addition the drought evaluation framework can be flexibly used to investigate the impacts of anthropogenic and climate change on drought evolution for example the users can study the effects of land use change and climate change on drought by setting related scenarios also swat plus pdsi can facilitate drought forecasting and alerting by driving swat with future long term or near term forecasting weather data from gcms and wrf this paper provides a new method in drought index calculation and drought evaluation by combing the physically based distributed hydrological model and pdsi although additional experiments cases are necessary to test this evaluating framework the pdsi combined with swat here was proven to be effective in capturing the drought evolution in such a semi arid region until now there are only few available toolkits based on swat for drought evaluation chattopadhyay et al 2017 kang and sridhar 2017b li et al 2017 if more such toolkit is available it could help timely diagnose the drought evolution and forecast enabling the managers mitigate or even prevent the drought disasters 5 conclusions in the study we combined the pdsi with a well performed swat model to analyze the driving forces of drought and investigate the spatiotemporal features of drought evolution in the wrb over the past 50 year period 1965 2014 using the linear regression method we detected the spatiotemporal variations of observed climate variables and simulated hydrological components with swat though there was no obvious trend in p at the basin level the increased nr and decreased lr 0 10 mm indicated a longer dry spell between rainfall events wet days in spite of the obvious warming trend the decrease in pet over the 50 year study period can be attributed to the substantial reduction in ws compared with pet the downward trend of et was significant and greater this might be due to another factor the dropping p insignificant the region with a significant downward trend in et was mainly located in the overlap area of the zones that witnessed a drop in ws and a drop in p the spatial distribution of sw trend was very similar to that of the p trend except for the southern part affected by the significant decrease in et furthermore we combined pdsi and swat to investigate the spatiotemporal evolution of drought in the wrb overall the pdsi had no obvious trend at the basin level but its trend at the subbasin level presented a substantial heterogeneity especially in summer chiefly caused by the decreasing p the pdsi was found to decrease in the headwater area significant the far downstream area significant and the middle region of the jing river subbasin insignificant our study indicated that there were two serious problems related to drought in the wrb the worsening spring drought and the intense spatial heterogeneity of drought evolution therefore much attention needs to be paid mainly because spring drought could threaten the agricultural production in this area where more than 40 is cropland also the greater heterogeneity of drought evolution may require watershed decision makers to develop specific adaptive strategies for ensuring the sustainability of water resources and food production when facing climate change in the wrb overall this study can help understand the drought trend and its spatial pattern induced by climate change in the wrb and support sustainable management of water resources in this region however there are still other important drought characteristics e g drought duration and drought frequency that are critical for drought evaluation but have not been investigated in this study thilakarathne and sridhar 2017 and this can be a good subject of a future study acknowledgments this study was funded by key laboratory of degraded and unused land consolidation engineering of natural resources ministry of china sxdj2019 5 shaan xi key research and development program of china 2018zdxm gy 030 the national thousand youth talent program of china the hundred youth talent program of shaanxi province the young talent support plan of xi an jiaotong university and the fundamental research funds for the central universities xzy012019011 we also thank the hpcc platform in xi an jiaotong university for computing equipment and computer maintenance appendix asupplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104519 appendix a classification of drought indices table a 1 classification of drought indices table a 1 category pdsi value spi spei value extreme wetness 4 00 2 00 severe wetness 3 00 3 99 1 50 1 99 moderate wetness 2 00 2 99 1 00 1 49 mild wetness 1 00 1 99 0 50 0 99 normal 0 99 0 99 0 49 0 49 mild drought 1 99 1 00 0 99 0 50 moderate drought 2 99 2 00 1 49 1 00 severe drought 3 99 3 00 1 99 1 50 extreme drought 4 00 2 00 b calculations of spi the standardized precipitation index spi is calculated based on k timescale e g k 3 6 12 24 months considering the fact that precipitation p obeys gamma distribution a gamma probability density function is used to fit to the aggregated monthly precipitation series for a specified time period then the spi is calculated by transforming the cumulative probabilities of the gamma distribution to standard normal distribution the formula of spi calculation is as follows b 1 spi s t c 2 t c 1 t c 0 d 3 t d 2 t d 1 t 1 0 where s is the coefficient of probability density t ln 1 h x 2 x is the amount of precipitation at a given time scale h x is the cumulative probability corresponding to x c 0 2 515517 c 1 0 802853 c 2 0 010328 d 1 1 432788 d 2 0 189269 d 3 0 001308 when h x 0 5 s 1 when h x 0 5 s 1 c figures figure c 1 spatial distribution of the annual average precipitation p evapotranspiration et and soil water content sw series of decadal 10 year periods for the 50 year 1965 2014 period in the wei river basin figure c 1 figure c 2 spatial distribution of the annual average pdsi spi and spei series of decadal 10 year periods for the 50 year 1965 2014 period in the wei river basin figure c 2 figure c 3 line charts of standardized anomalies of precipitation p spi spei and pdsi for the 50 year 1965 2014 period in the wei river basin figure c 3 
26112,understanding the spatiotemporal evolution of drought is vital for effective water resources management especially in arid and semi arid regions and under climate change in this study we developed the soil and water assessment tool swat based drought evaluation tool and used it to investigate the spatiotemporal change of drought and its driving factors over the past 50 years 1965 2014 in a typical semi arid area the wei river basin in the loess plateau the temporal trend analysis of precipitation showed an intensified hydrological cycle with a longer dry interval and the substantially decreased wind speed resulted in a significant decrease in the evapotranspiration and a slight increase in the soil water content the spatiotemporal analysis of drought identified the vulnerable areas and indicated that spring drought was exacerbating overall this study can be informative and valuable for the drought assessment and disaster alleviation in the loess plateau area graphical abstract image 1 keywords climate change driving factors drought index swat loess plateau software availability name of software swat based drought evaluation tool description this tool can facilitate the spatiotemporal analysis and presentation of drought e g pdsi spi and spei across a watershed just based on an existing swat project without the need of efforts to prepare input for drought index calculation developers yiping wu shengnan zhang yuzhu sun availability contact the developers 1 introduction drought is an accumulated and recurring natural hazard caused by a sustained deficit of precipitation p or excess evapotranspiration et over an extended period liu et al 2016 mckee et al 1993 touma et al 2015 there is convincing evidence that global climate change has increased the frequency and severity of extreme drought events in recent decades leading to the destruction of ecological environment economic loss including in terms of agriculture and even conflicts among human communities chang et al 2016b dai 2012 the issue of how climate change affects drought has evoked high levels of interest of many researchers in the different fields concerned dai 2011b mishra et al 2010 seneviratne et al 2010 wang et al 2011 sun et al 2019a a key prerequisite for drought research is the identification of drought using credible drought indices chang et al 2016b there are many statistical drought indices that can be used to investigate the drought evolution and hazard evaluation however these indices are relatively simple because they only focus on either climatic or hydrological aspects for example the standardized precipitation index spi guttman 1999 mckee et al 1993 thilakarathne and sridhar 2017 and the standardized precipitation evapotranspiration index spei vicente serrano et al 2010 are generated based on the precipitation or and potential evapotranspiration pet while the streamflow drought index sdi nalbantis and tsakiris 2009 was only calculated based on streamflow simple indices are easy to use but may mean ignoring the driving factors of drought and may lead to unreliable outcomes due to the lack of some other important variables e g soil moisture the palmer drought severity index pdsi was originally proposed by palmer 1965 for the purpose of measuring the cumulative departure in sur face water balance which may be considered superior to other statistically based drought indices as the pdsi can account for the more comprehensive effect of global warming through palmer s water balance model on dry and wet spells dai 2011a b hao and singh 2015 mishra and singh 2010 it takes previous and current moisture supply p and demand pet into a hydrological accounting system which includes a two layer bucket type model for soil moisture calculations chen et al 2012 dai 2011a nevertheless the pet calculated using the thornthwaite equation in the original palmer model could lead to the overestimation of the effect of temperature on drought guttman et al 1992 and inaccuracy in energy limited regions hobbins et al 2008 as the thornthwaite pet is based only on temperature month and latitude to minimize these errors the more advanced penman monteith equation was used to calculate the pet which accounts for the effects of radiation wind speed and humidity self calibrating pdsi sc pdsi replaces empirical constants e g duration factors climatic characteristics coefficient and other environmental constants with dynamically calculated values at any location to make the pdsi comparable spatially nathan et al 2004 and studies have reported that the sc pdsi can describe drought evolution well jia et al 2014 senatore et al 2019 wang et al 2015 however the sc pdsi is still deficient in its soil moisture calculation included in the two layer bucket type model and fails to consider the effects of some factors e g the spatial heterogeneity of soil vegetation cover and topography dai 2011a guttman 1998 zou et al 2017 showing a certain weakness in drought investigation under these situations combining drought indices with hydrological models chattopadhyay et al 2017 kang and sridhar 2017a sun et al 2014 has become attractive for example leng et al 2015 used the variable infiltration capacity vic model liang et al 1994 to project future hydrological changes in china and then calculate the spi standardized runoff index sri and standardized soil moisture index sswi to assess the climate change impact on droughts from meteorological agricultural and hydrologic perspectives li et al 2017 employed the soil and water assessment tool swat arnold et al 1998 to support the calculation of spi sswi and standardized streamflow index ssi kang and sridhar 2018a assessed the climate change impacts on various drought indices e g pdsi ssi and sdi by combining both swat and vic with multiple climate models and found the two hydrological models can be applicable to investigate and forecast the future drought evolution the distributed model swat is a useful tool to simulate the hydrological response to climate and land use changes qiu et al 2017 sun et al 2017 wu et al 2012 2014a 2018 yang et al 2018 zhao et al 2018a in view of the merits and demerits of the pdsi mentioned above combining swat and pdsi can be an effective approach to investigate the drought evolution under climate change because the former could simulate the hydrological processes under dynamic land surface conditions e g climate elevation soil land use and management and yield multiple hydrological components e g water yield recharge and evapotranspiration some of which are difficult to observe but needed in pdsi calculation kang and sridhar 2018b therefore the pdsi combined with swat can be more meaningful compared to the original one and this is also our motivation for the present study the wei river basin wrb located in the southern chinese loess plateau was regarded to be more sensitive to climate change and highly prone to drought zhao et al 2013 2015b for the wrb several studies have found its disastrous drought periods in the past e g 1962 1972 1987 and 1990s and the drought situation in this region has been deteriorating ma et al 2013 sun et al 2019b yuan et al 2016 zhang et al 2013 2015 however most of the drought indicators e g p spi and streamflow for this region have failed to link with climate change chang et al 2016a huang and wang 2016 tan et al 2011 zuo et al 2015 although chang et al 2016b and zhao et al 2015a used an integrated index and a model based drought index respectively to investigate the spatiotemporal characteristics of drought variations there remain questions on the driving factors of drought evolution at a more detailed and systematic level therefore a further study on the drought evolution mechanism is needed the overall goal of this study was to investigate the drought evolution induced by climate change in the wrb for the 50 year 1965 2014 period using pdsi combined with swat the specific objectives of the present study were to 1 combine swat with pdsi calculation and evaluate its performance 2 investigate the spatiotemporal changes of drought and its key driving factors climatic and hydrological factors and 3 identify the most vulnerable susceptible areas that are prone to drought under climate change the study could help understand the impacts of climate change on drought in time and space for a semi arid area and provide a theoretical basis for rational utilization of water resources effective management of farmland and scientific planning of land use for this region 2 materials and methods 2 1 study area the wei river originates from the north side of the niaoshu mountains in the western part of china flows eastward and finally discharges into the yellow river with its total length of 818 km and its drainage area of 134 800 km2 fig 1 the wrb 33 5 n 37 5 n and 103 5 e 110 5 e is mainly located in the southern part of the loess plateau with its elevation ranging from 319 m in the southeast to 3917 m in the western mountainous region based on the past 50 year 1965 2014 hydro climatic data the annual mean temperature t ranged from 6 2 c in the northwest to 13 5 c in the southeast and the annual mean p and et were about 538 mm and 436 mm respectively besides the distribution of p exhibits noticeable spatial difference and annual variations in the wrb there are three main soil types cambisols anthrosols and fluvisols accounting for 65 1 8 9 and 6 5 of the basin respectively the cambisols mainly covers the central north area and the qinling mountains region southern part of the basin the anthrosols is distributed in the middle and downstream area of the basin and the fluvisols is along the river channels there are also three major land use types cropland grassland and woodland accounting for 43 6 32 1 and 16 2 respectively the cropland is mainly located in the low altitude area the grassland characterized by middle and low coverage is relatively evenly distributed and the woodland is mainly located in the qinling mountains region and the southern contiguous area of the jing river subbasin and the beiluo river subbasin 2 2 model description the swat model developed by the agricultural research service of the united states department of agriculture usda ars is a distributed and physically based watershed hydrological model arnold et al 1998 gassman et al 2014 romagnoli et al 2017 wu et al 2018 for exploring the hydrological cycle plant growth transportation of sediment and agricultural chemical yields on a daily time step arnold et al 1998 panagopoulos et al 2012 pyo et al 2017 stefanidis et al 2018 wu et al 2012 zhao et al 2019 the main outputs of swat are surface runoff lateral flow baseflow et soil water content sw water yield wld sediment load and nutrient loads further details about swat are available in its theoretical documentation neitsch et al 2011 the well established swat model has been used in a variety of watershed related studies including climate and land cover changes at watershed scales qiu et al 2017 wu and johnston 2007 wu et al 2014a zhao et al 2018a therefore we chose this model in the present study to obtain long term distributed hydrological variables which are difficult to be measured in reality 2 3 model input and setup a geographic information system gis interface arcswat version 2012 was employed to set up the model the digital elevation model dem data with a 90 m spatial resolution from the shuttle radar topographic mission srtm jarvis et al 2006 was used to delineate the basin into 192 subbasins the land use 1 km 1 km and soil properties 1 km 1 km of this region were obtained from the ecological and environmental science data center for west china http westdc westgis ac cn based on the combination of land use soil and slope the wrb was discretized into 3470 hydrological response units hrus the daily meteorological data from 20 stations for the period 1965 2014 were collected from the data center of the china meteorological administration http data cma cn including p maximum air temperature tx minimum air temperature tn relative humidity rh wind speed ws and sunshine duration the sunshine duration here was used to calculate the solar radiation sr required for the swat 2 4 model calibration and validation in this study we used the swat cup swat calibration and uncertainty programs sufi 2 abbaspour et al 2004 2007 rouholahnejad et al 2012 to calibrate the swat parameters using a 10 year 1981 1990 record of streamflow and validate them with another 10 year 1971 1980 streamflow observations additionally we used a 5 year warm up period 1966 1970 to minimize the impacts of uncertain initial conditions in the model simulation based on the previous publications and our own experience related to swat calibration abbaspour et al 2007 wu et al 2012 2014b zhao et al 2018b zuo et al 2016 we selected six sensitive parameters for the model calibration and the fitted values of the selected parameters were listed in table 1 2 5 combination of swat and pdsi the calculation of pdsi requires p pet et sw and wld it is quite challenging to measure most of these variables at regional scale here we proposed to use the simulated hydrological components of swat as the input for pdsi calculation as listed in table 2 extensive details of the background theory and detailed methodology of the pdsi are already available in the literature nathan et al 2004 palmer 1965 zhai et al 2010 zhao et al 2015a the main steps for pdsi calculation in this study are described below and the classification of pdsi is shown in table a1 the empirical constants involved in this procedure were the recommended values by the china meteorological administration cma 2006 based on this the pdsi calculation in this study can be expressed as 1 x i z 1 63 0 775 x i 1 where x i and x i 1 are the values of pdsi in month i and i 1 z is the moisture anomaly index which can be given as 2 z d k where d is the departure of moisture from normal k is the modified climatic characteristic coefficient given by 3 k 16 84 1 12 d k k where d is the average absolute value of d and k is the estimated value of climatic characteristic coefficient given by 4 k 1 6 log 10 p e r r o p l 2 8 d 0 4 where p e r r o p l and d are the multi year averages of pet recharge actual runoff actual precipitation soil moisture loss and d respectively departure of moisture from normal d is computed by estimating the gaps between actual precipitation p and climatic optimum precipitation p which is described as 5 d p p the variable p as the expected precipitation supply is climatically appropriate for the existing conditions cafec based on the water balance equation the water balance of pdsi can be expressed as follows 6 p et r ro l where et r ro and l are the climatic optimum et recharge runoff and soil moisture loss respectively here the runoff used in the pdsi is not the surface runoff zhao et al 2015a but the wld the climatic optimum values above can be defined as follows 7 et α pe 8 r β pr 9 ro γ pro 10 l δ pl where pe pr pro and pl are the potential values of et recharge runoff and soil moisture loss respectively and the coefficients α β γ and δ are the ratios of the actual multi year averages to the potential multi year averages over the simulation period the evaluation of a drought index is really challenging because there is not a criterion available and different indices may use different input variables nevertheless drought indices e g spi spei and pdsi are used to represent the severity of drought by depicting different aspects of drought e g climatic aspect or hydrological aspect and they are regarded to be dependent hao and singh 2015 thus comparison of a drought index with others has been widely adopted to evaluate the performance or suitability of a drought index hao and aghakouchak 2013 sehgal et al 2017 sun et al 2018 zhang et al 2019 zhao et al 2017 in this study we employed spi and spei for the comparison and verification of the performance of pdsi the spi was calculated for a certain time scale based on the probability distributions of p while the calculation of spei is the same but with one more variable pet the classification thresholds and the drought categories are shown in table a1 the detailed formulation of the spi calculation is available from appendix b 2 6 analysis methods for the key variables and the drought indices we used the pearson chi square normality test to check their distribution pearson 1900 and employed the simple linear regression with least squares method to detect their trends rahimi 2017 zou et al 2003 if the slopes of the fitted linear lines are significantly different from zero t test p 0 05 the trends are considered to be statistically significant student 1908 for the 50 year 1965 2014 study period the trends of key climatic variables were analyzed by site and the inverse distance weighted idw interpolation method shepard 1968 was then used to present their spatial distributions however the trends of key hydrological variables and drought indices were investigated at the subbasin level 3 results 3 1 model examination to evaluate the model performance for the calibration 1981 1990 and validation 1971 1980 periods we performed both graphical representation and statistical analysis using the simulated and observed monthly streamflows fig 2 shows the comparison of simulated monthly streamflows against the observations at two gaging stations weijiapu and huaxian the plots indicate that the simulated and observed monthly streamflows matched well for much of the period considered and followed the p pattern in their variations in addition to the time series we selected four statistical indicators to measure the model performance nash sutcliffe efficiency nse percent bias pbias ratio of the root mean square error to the standard deviation of measured data rsr and squared correlation coefficient r2 based on the criteria set out by moriasi et al 2007 the performance of the model at both the gaging stations table 3 can be judged as good satisfactory in the calibration validation period with nse 0 65 pbias 15 and rsr 0 60 nse 0 50 pbias 25 and rsr 0 70 besides the values of r2 at the two gages were higher than 0 6 for both calibration and validation periods overall the model performance was satisfactory for the subsequent analysis 3 2 spatiotemporal analysis of key climate variables the meteorological factors including p t ws and rh could influence pet and thus et sw and drought conditions actually these factors are required inputs for calculating drought indices e g spi spei and pdsi therefore we studied the changes of p tx maximum air temperature tn minimum air temperature mt mean air temperature ws and rh in this section in addition drought situations could change even if there is no trend for annual precipitation like what we presented in the current study because of the changes of precipitation mode including no rain days nr light rain days lr etc for example the increasing nr and the decreasing lr as shown in this study implied a longer interval between two rainfall events resulting in an exacerbated drought when other variables remain unchanged for all of these we utilized the linear regression and t test serrano et al 1999 to obtain their annual trends and significance levels for the whole 50 year 1965 2014 study period and then obtained the spatial distributions of their annual trends using the idw interpolation in arcgis we also examined the seasonal trends of p nr and lr and analyzed the variations of rainfall intensity 1 1 1 spatial distributions of annual trends of key climate variables as shown in fig 3 p close to the basin outlet had the largest decrease 4 13 mm year p 0 05 despite no trend for the entire basin the nr exhibited an increasing trend especially for the headwater area the far south part and the eastern part of the beiluo river subbasin p 0 05 the results for the lr were found to be opposite to those for nr for instance lr in most areas had a decreasing trend and the region with a significant drop in lr also roughly had a significant rise in nr based on the above analysis more dry days or longer intervals between rainfall events could be expected this was also in agreement with the results obtained in an earlier study for the east river basin wu et al 2014a for t whether it is tx tn or mt there was a significant upward trend 0 01 0 07 c year for the whole basin with the northern area having a greater rise in t for ws most stations showed a downward trend and 12 stations were statistically significant additionally a trend change point of ws from decrease to increase was found around the year 2000 for 7 stations with 6 of these stations having a statistically significant upward trend after 2000 figure s1 for rh there was no obvious temporal trend or spatial features for the whole basin broadly speaking a warmer climate characterized by more dry days was expected despite no trend in p the significant increase in t and the large decrease in ws would have the opposite effects on et 1 1 2 p nr and lr in dry and wet seasons for p nr and lr we examined their trends for the wet season april through september in a year and the dry season january through march and october through december in a year respectively as seen from fig 4 both seasons had the decreasing trend of p in the headwater and the far downstream areas in the dry season a significant decrease of p occurred in the main cropland area which might have exacerbated the spring drought the high correlation between lr and nr mentioned above fig 3 were also presented in both seasons the greater the rise of nr the greater the drop of lr for the trends of p nr and lr their spatial distributions in the wet season were generally in accordance with those on the annual scale fig 3 implying that the variations of p nr and lr in the wet season might dominate the annual trends of them although there was no trend in the annual p for the entire basin the decrease of p in the dry season might cause the aggravation of spring drought especially for the cropland area the conversion of lr to nr was apparent especially in the wet season 1 1 3 variations of rainfall intensity based on different rainfall intensities the statistical analysis of rainfall amount was carried out for the five 10 year periods 1965 1974 1975 1984 1985 1994 1995 2004 and 2005 2014 as shown in fig 5 there was no consistent change in the rainfall amounts among the five listed rainfall intensities however on the annual scale there was a significant downward trend p 0 05 in the light rain 0 10 mm and an increasing trend p 0 1 in the heavy rain respectively in particular a clear increase can be found in the heavy rain 50 mm for the last three 10 year periods 1985 2014 3 3 spatiotemporal analysis of key hydrological variables for p pet et sw and wld we plotted the scatter diagrams with their annual trends and significance levels figure s2 the combined effects of the increased t and decreased ws caused a slight decrease in pet but it is not statistically significant the slight decrease in pet and p resulted in a significant reduction of et 0 71 mm year p 0 05 and a slight rise of sw not significant fig 6 shows the spatial distributions of annual trends of p pet et and sw for the 50 year 1965 2014 study period at the subbasin level assuming that the annual sr did not change the strong spatial variability of the pet trend was attributed to the combined influence of t ws and rh past studies have shown that ws was the primary factor affecting the pet in the wrb gao et al 2006 the significant rise in pet in the headwater can be undoubtedly explained by the decrease in rh and the increase in t and ws considering the negative impact of the fall in ws the significant rise in pet in the source area of the jing river subbasin may have been caused by the rise in t and the fall in rh in the areas near the wugong yaoxian and foping weather stations the significant downward trend in pet resulted from the large decrease of ws considering the positive impact of the rising t compared with pet the et was affected by more factors such as landform and land use the region with a significant downward trend in et was mainly located in the overlap area of the ws falling zone and the p falling zone for the sw its spatial distribution of annual trend was very similar to that of p except for the southern region where the et had a significant decrease to further study the drought conditions every ten years we analyzed the variations of decadal p et and sw at the subbasin level figure c1 a wet period 1975 1984 and a dry period 1995 2004 can be seen from the spatial distributions of decadal p and sw for the et a clear downward trend was found which could mainly be attributed to the decrease in ws 3 4 spatiotemporal analysis of drought to verify the performance of pdsi we calculated the corresponding spi and spei series and then compared pdsi series with them if the pdsi performed well we would apply it to capture the spatiotemporal characteristics of drought evolution in the wrb on the annual and seasonal scales 3 4 1 performance of pdsi to examine the performance of pdsi on the annual scale we used linear regression to analyze the correlations between pdsi and p spi and spei respectively fig 7 it can be easily observed that the pdsi had a very good agreement with p spi and spei with correlation coefficients of 0 70 0 70 and 0 69 respectively in addition the decadal mean value of pdsi had a similar spatial distribution with the corresponding spi and spei series at the subbasin level figure c2 and the line charts of standardized anomalies of p spi spei and pdsi showed quite synchronous fluctuations figure c3 on the seasonal scale fig 8 the trends of pdsi spi and spei in spring march through may in a year and summer june through august in a year were analyzed at the subbasin level to account for and examine the obvious trend characteristics and the strong spatial variability that exist in both seasons the peaks and valleys of pdsi mostly corresponded to those defined by spi and spei in spring all the three drought indices showed a downward trend indicating that the spring drought might have been aggravated in the past five decades however the spatial distribution of the spring trend of pdsi was greatly different from those of spi and spei exept for the far downstream area the reason for this difference may be that the variables involved in the definition of drought indices are different the p is the dominant variable of spi and spei and the decrease in p in spring caused more severe drought that was defined by spi and spei in summer an insignificant upward trend in the three indices was observed and the spatial variability of pdsi trend was very similar to that of the spi and spei trends to assess the effectiveness of pdsi in capturing drought events we compared the annual pdsi values with the historical severe drought events that were recorded inyearbooks during 1965 2014 from fig 9 it can be seen clearly that the values of pdsi in 1972 1987 1995 and 1997 were less than 2 00 moderate drought threshold which corresponded to four serious drought events indicating that the drought in the basin can be well captured by the pdsi overall the pdsi performed well in the wrb on the decadal annual and seasonal scales and therefore it can be used to reliably capture drought events and define the spatiotemporal features of drought evolution 3 4 2 spatiotemporal features of drought evolution for the spatiotemporal features of drought evolution we analyzed the pdsi series using linear regression and graphical representation for the 50 year 1965 2014 study period 3 4 2 1 analysis on annual scale as shown in fig 9 the drought condition was most severe during 1990 2000 gradually softened and turned to wetness after 2000 in addition the year 2000 was also the trend change point of ws for seven weather stations therefore we chose this year as the time point to divide the pdsi series into two parts although there was no change in pdsi for the whole period a significant rise in pdsi occurred after 2000 by comparison figure s2 we found that the pdsi followed the p and sw pattern roughly in its annual variation and the dominant influencing factor of the pdsi was the p in general the spatial distribution of pdsi trend was similar to that of p a significant decrease of pdsi had occurred in the headwater and the far downstream areas and a slight decrease of pdsi was observed in the middle region of the jing river subbasin according to the principle of water balance the reason for the drought aggravation in the above areas was that the negative effects of the falling p and sw eliminated the positive effect of the falling et the significant wetting trend near the yaoxian weather station was mainly due to the significant rise in sw which in turn resulted from the significant decrease of et 3 4 2 2 analysis on seasonal scale in addition to the annual characteristics of drought evolution we analyzed its seasonal features defined by the pdsi fig 8 in spring a decreasing trend of pdsi was found for most areas especially for the headwater and the far downstream areas in summer the spatial distribution of pdsi trend had a clear spatial variability that was in line with that on the annual scale according to the changes in pdsi we found a significant wetting period 2000 2014 and examined four severe drought events in the past 50 year 1965 2014 study period based on the spatial distribution of pdsi annual trend we identified the vulnerable susceptible areas that are prone to drought the downstream area that is quite close to the basin outlet the headwater and the middle region of the jing river subbasin on the seasonal scale it should be noted that the wrb might face the challenge of spring drought and the spatial inhomogeneity of drought changes in summer 4 discussion in this study we developed a process based drought evaluation tool by combining the swat model and pdsi the aim of this tool was to provide such a flexible framework for investigating the drought evolution and evaluation so that the precautions can be effectively made towards drought at the watershed scale the case study in this paper helps us identify where and when the drought event is most likely to occur and what is the primarily driving forces of the drought in the wrb there are a few reasons why we combine swat and pdsi for drought evaluation compared with statistically based drought indices pdsi is more comprehensive in evaluating drought based on water balance however the calculation process of pdsi is complex the required variables are numerous and some of them are difficult to observe also the calculation of original pdsi did not take the influence of underlying surface heterogeneity into account e g dem soil land use and management to make up for the above deficiencies it is a good approach to combine pdsi with hydrological models the swat model is a process based continuous distributed hydrological model which has been widely used in simulating the hydrological cycles around the world once the swat model is verified it can accurately depict the real hydrological processes at a specific watershed compared to the statistical models because the model considers the land surface conditions e g dem climate soil land use and management during the simulation process we then take the spatially distributed hydrological components from the swat model e g pet et sw and wld as the inputs for calculating pdsi in this sense the pdsi based on swat can be regarded as the process based drought index which is more meaningful compared to the original one the high efficiency in drought diagnose is of great importance in efficiently response to drought our developed drought evaluation tool can facilitate the spatiotemporal analysis and presentation of drought across a watershed just based on an existing swat project without the need of efforts to prepare additional inputs allowing the users policy makers easily and timely get the drought information in addition the drought evaluation framework can be flexibly used to investigate the impacts of anthropogenic and climate change on drought evolution for example the users can study the effects of land use change and climate change on drought by setting related scenarios also swat plus pdsi can facilitate drought forecasting and alerting by driving swat with future long term or near term forecasting weather data from gcms and wrf this paper provides a new method in drought index calculation and drought evaluation by combing the physically based distributed hydrological model and pdsi although additional experiments cases are necessary to test this evaluating framework the pdsi combined with swat here was proven to be effective in capturing the drought evolution in such a semi arid region until now there are only few available toolkits based on swat for drought evaluation chattopadhyay et al 2017 kang and sridhar 2017b li et al 2017 if more such toolkit is available it could help timely diagnose the drought evolution and forecast enabling the managers mitigate or even prevent the drought disasters 5 conclusions in the study we combined the pdsi with a well performed swat model to analyze the driving forces of drought and investigate the spatiotemporal features of drought evolution in the wrb over the past 50 year period 1965 2014 using the linear regression method we detected the spatiotemporal variations of observed climate variables and simulated hydrological components with swat though there was no obvious trend in p at the basin level the increased nr and decreased lr 0 10 mm indicated a longer dry spell between rainfall events wet days in spite of the obvious warming trend the decrease in pet over the 50 year study period can be attributed to the substantial reduction in ws compared with pet the downward trend of et was significant and greater this might be due to another factor the dropping p insignificant the region with a significant downward trend in et was mainly located in the overlap area of the zones that witnessed a drop in ws and a drop in p the spatial distribution of sw trend was very similar to that of the p trend except for the southern part affected by the significant decrease in et furthermore we combined pdsi and swat to investigate the spatiotemporal evolution of drought in the wrb overall the pdsi had no obvious trend at the basin level but its trend at the subbasin level presented a substantial heterogeneity especially in summer chiefly caused by the decreasing p the pdsi was found to decrease in the headwater area significant the far downstream area significant and the middle region of the jing river subbasin insignificant our study indicated that there were two serious problems related to drought in the wrb the worsening spring drought and the intense spatial heterogeneity of drought evolution therefore much attention needs to be paid mainly because spring drought could threaten the agricultural production in this area where more than 40 is cropland also the greater heterogeneity of drought evolution may require watershed decision makers to develop specific adaptive strategies for ensuring the sustainability of water resources and food production when facing climate change in the wrb overall this study can help understand the drought trend and its spatial pattern induced by climate change in the wrb and support sustainable management of water resources in this region however there are still other important drought characteristics e g drought duration and drought frequency that are critical for drought evaluation but have not been investigated in this study thilakarathne and sridhar 2017 and this can be a good subject of a future study acknowledgments this study was funded by key laboratory of degraded and unused land consolidation engineering of natural resources ministry of china sxdj2019 5 shaan xi key research and development program of china 2018zdxm gy 030 the national thousand youth talent program of china the hundred youth talent program of shaanxi province the young talent support plan of xi an jiaotong university and the fundamental research funds for the central universities xzy012019011 we also thank the hpcc platform in xi an jiaotong university for computing equipment and computer maintenance appendix asupplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104519 appendix a classification of drought indices table a 1 classification of drought indices table a 1 category pdsi value spi spei value extreme wetness 4 00 2 00 severe wetness 3 00 3 99 1 50 1 99 moderate wetness 2 00 2 99 1 00 1 49 mild wetness 1 00 1 99 0 50 0 99 normal 0 99 0 99 0 49 0 49 mild drought 1 99 1 00 0 99 0 50 moderate drought 2 99 2 00 1 49 1 00 severe drought 3 99 3 00 1 99 1 50 extreme drought 4 00 2 00 b calculations of spi the standardized precipitation index spi is calculated based on k timescale e g k 3 6 12 24 months considering the fact that precipitation p obeys gamma distribution a gamma probability density function is used to fit to the aggregated monthly precipitation series for a specified time period then the spi is calculated by transforming the cumulative probabilities of the gamma distribution to standard normal distribution the formula of spi calculation is as follows b 1 spi s t c 2 t c 1 t c 0 d 3 t d 2 t d 1 t 1 0 where s is the coefficient of probability density t ln 1 h x 2 x is the amount of precipitation at a given time scale h x is the cumulative probability corresponding to x c 0 2 515517 c 1 0 802853 c 2 0 010328 d 1 1 432788 d 2 0 189269 d 3 0 001308 when h x 0 5 s 1 when h x 0 5 s 1 c figures figure c 1 spatial distribution of the annual average precipitation p evapotranspiration et and soil water content sw series of decadal 10 year periods for the 50 year 1965 2014 period in the wei river basin figure c 1 figure c 2 spatial distribution of the annual average pdsi spi and spei series of decadal 10 year periods for the 50 year 1965 2014 period in the wei river basin figure c 2 figure c 3 line charts of standardized anomalies of precipitation p spi spei and pdsi for the 50 year 1965 2014 period in the wei river basin figure c 3 
26113,heuristic multi objective optimization mo algorithms lose their efficiency and performance as the number of objectives increases due to the so called dominance resistance unless they are equipped with a specialized solution archiving strategy like epsilon ε archiving this study introduces an alternative approach to tackle dominance resistance for solving environmental and water resources engineering problems with more than three objectives in the proposed approach objectives are rounded to user defined precision levels before checking the dominance rounded archiving is developed and assessed for pa dds and borg moea and verified for amalgam applied to hydrologic model calibration problems with more than three objectives results show that rounded archiving significantly improves the performance of mo algorithms and is at least as effective as if not better than the ε archiving for solving many objective optimization problems without the need to restructure the algorithm which is the requirement for the implementation of the ε archiving approach keywords many objective optimization rounded archiving epsilon archiving dominance resistance model calibration software availability name of software pa dds with rounded archiving method developer and contact information shahram sahraei masoud asadzadeh department of civil engineering university of manitoba eitc e1 332 15 gillson street winnipeg mb canada r3t 5v6 email address sahraeis myumanitoba ca masoud asadzadeh umanitoba ca year first available 2019 hardware required desktop computer availability go to zdt round folder in http home cc umanitoba ca asadzadm software html to see rounded archiving method in two objective zdt1 test problem software required matlab program size 1 mb 1 introduction many objective optimization i e multi objective mo optimization with four or more conflicting objectives has become an attractive tool in a broad range of environmental and water resources problems such as calibration of environmental models e g shafii and tolson 2015 zhang et al 2008 water distribution network design fu et al 2012 reservoir operation geressu and harou 2019 giuliani et al 2014a b stormwater management di matteo et al 2018 robust decision making eker and kwakkel 2018 singh et al 2015 kasprzyk et al 2013 and crop food production lautenbach et al 2013 for instance the hydrologic model calibration is well known to have a mo nature for which no single solution simultaneously optimizes all the objective functions model performance metrics see for example boyle et al 2000 wagener and gupta 2005 gupta et al 2006 and maier et al 2018 instead a set of alternatives that is often called tradeoff or pareto after vilfredo pareto 1848 1923 exists since improving one objective degrades some other objectives traditionally hydrologic models had been calibrated with two or three objective functions asadzadeh et al 2015 asadzadeh et al 2013 shafii and smedt 2009 gupta et al 1998 but recent developments in the hydrologic models computational resources and mo algorithms is changing the trend toward the many objective model calibration ercan and goodall 2016 wang and brubaker 2015 reed et al 2013 kollat et al 2012 the number of objectives can easily grow beyond three when the calibration problem is formulated to represent internal hydrological processes besides optimizing traditional model performance metrics such as the sum of squared errors or its variants for instance shafii and tolson 2015 formulated and solved a 15 objective model calibration problem and showed that incorporating the objective functions pertaining to the dominant hydrologic processes of a watershed aids in selecting the most reliable and hydrologically consistent model parameter values with respect to the selected dominant hydrologic processes on the other hand the size of the non dominated space grows exponentially as the number of objective functions increases farina and amato 2002 and leads to a two fold difficulty in solving many objective optimization problems by a heuristic mo algorithm unless it is equipped with a specialized archiving approach first identifying any new dominating better solution is very challenging in many objective optimization due to the high similarity in the objective space this is often referred to as the dominance resistance in the literature ikeda et al 2001 purshouse and fleming 2007 second heuristic mo algorithms that have a bounded archive size e g nsga ii deb et al 2002b have to discard some good quality solutions from the current archive and therefore are subject to the oscillating issue that makes them inefficient for solving many objective optimization problems on the other hand mo algorithms that have an unbounded archive size such as the pareto archived dynamically dimensioned search pa dds in asadzadeh et al 2013 might archive a large number of pareto solutions because solutions for further generations are generated based on the archived pareto set such algorithms assign a very low selection probability to high quality solutions that are scattered among a huge number of pareto solutions teytaud 2007 reed et al 2013 maier et al 2014 mo algorithms that have a bounded archive keep at most an a priori specified number of non dominated solutions by the end of optimization the bounded archive set was first suggested in non dominated sorting genetic algorithm nsga srinivas and deb 1994 in order to reduce the computational time for checking the dominance rank accelerating the decision making process and concentrating solely on appealing parts of the pareto front however tightly bounded archive size of mo algorithms results in an oscillating behavior in that some currently non dominated solutions have to be discarded from the archive in the current generation and be replaced by some inferior ones in subsequent generations hanne 1999 to increase the efficiency of mo algorithms in many objective applications several approaches have been developed in the literature including fuzzy optimality farina and amato 2004 order of efficiency di pierro et al 2007 preferability drechsler et al 2001 and epsilon preferability sulflow et al 2007 these methods are conceptually similar preferability is defined as choosing non dominated solutions that are dominating a larger number of objectives when comparing two non dominated solutions sulflow et al 2007 fuzzy optimality is a fuzzy based concept of optimality that considers the number of improved objectives and the order of improvement between two solutions order of efficiency is the minimum size of non dominated objective sub space for a solution compared with the archived ones however these methods must be used with caution as the search may be misled into undesired regions of the decision space especially when the objective functions are not equally important to decision makers coello et al 2007 laumanns et al 2002 suggested the cell epsilon ε archiving approach to reduce the size of non dominated archived solutions and therefore improve the diversity in the objective space deb et al 2003 successfully applied the ε archiving technique to a multi objective evolutionary algorithm called ε moea that outperformed five other moeas for solving mathematical mo test problems with two three and four objective functions in terms of diversity and convergence reed and devireddy 2004 introduced a variant of nsga ii called ε nsgaii that utilizes the ε archiving applied it to solve a bi objective groundwater monitoring problem and concluded that ε archiving significantly increased the efficiency of the optimization algorithm kollat and reed 2006 applied ε nsgaii to a long term groundwater monitoring design problem with four objective functions and showed a superior performance of ε nsgaii against nsga ii as well as other modern mo algorithms hadka and reed 2013 combined a group of mo operators including the ε archiving and introduced the borg moea that showed a superior performance in comparison with six well known mo algorithms for solving benchmark suites of test problems with 2 8 objectives many modern mo algorithms do not have the ε archiving strategy e g pa dds a multi algorithm genetically adaptive multi objective amalgam vrugt and robinson 2007 strength pareto evolutioary algorithm 2 zitzler et al 2001 nsga iii deb and jain 2014 and therefore are anticipated to lose their performance in terms of convergence and solution diversity when applied to many objective problems in this study an alternative approach to ε archiving is introduced this approach that is called rounded archiving is technically the point based dominace archiving of the rounded value of objective functions the main advantage of the proposed approach over ε archiving is that it does not require the user to restructure the mo algorithm computer code the rounded archiving approach is compared to the ε archiving approach for solving hydrologic model calibration problems that are formulated and solved as many objective optimization problems in the literature the proposed approach is not algorithm or case specific and is readily applicable to other mo algorithms such as pa dds and amalgam as shown in this study the remainder of this paper is organized as follows the methods section describes the proposed rounded archiving method applied to mo algorithms including borg moea pa dds and amalgam case studies including the mathematical and hydrologic model calibration problems numerical experiment setup and the results analysis approach next results are presented and a general discussion is given followed by the concluding remarks 2 methods mo algorithms typically start the optimization with a random solution generation and archive solutions based on the dominance check in the objective space then one or a population of archived solutions are selected for perturbation or recombination to create new solution s for subsequent evaluations this process continues until a termination condition is met point based archiving and selection strategies therefore constitute the principal parts of a mo algorithm to guide the search toward a better proximity convergence and diversity of solutions to approximate the pareto front deb 2001 coello et al 2007 pareto approximate front is referred to as the archived non dominated solutions found by a mo algorithm at the end of optimization in point based archiving solution a is said to be non dominated if and only if there is no other solution b that dominates it veldhuizen and lamont 2000 equation 1 shows the matematical representation of point based archiving concept of solution a dominating b for a minimization problem that has n conflicting objective functions in the case of maximization the corresponding objective function needs to be multiplied by 1 to change it to a minimization problem 1 a b f i a f i b i 1 2 n f j a f j b j 1 2 n in general the regular point based dominance check and archiving strategy become ineffective as the number of objective functions grows to more than three farina and amato 2002 mathematically showed by equation 2 that the proportion of non dominated space e exponentially increases as the number of objectives n increases this issue increases the similarity of the archived solutions in the objective space and causes the dominance resistance ikeda et al 2001 purshouse and fleming 2007 that degrades the performance of mo optimization algorithms 2 e 2 n 2 2 n the cell based dominance check and the corresponding ε archiving introduced by laumanns et al 2002 which is based on the ε dominance addresses the dominance resistance issue and improves the performance of mo algorithms for solving many objective optimization problems by definition in a minimization problem with n objectives solution a ε dominates solution b if and only if f i a ε i f i b ε i for all objectives i 1 n with f j a ε j f j b ε j for at least one objective j the symbol returns the floor of a real value ε archiving uses this inequality to develop a mesh grid in the objective space and archive at most one solution in each grid cell if two solutions a and b reside in one cell i e f i a ε i f i b ε i for all objectives i the solution that is closer to the dominating bottom left in minimization problems corner of the cell is archived and the other solution is discarded green points in fig 1 are ε dominated by the red points demonstrating both conditions of ε archiving relation the size of each grid cell is equal to ε which should be determined a priori by the decision maker or systems analyst reed and devireddy 2004 for each objective function based on the desired precision level equation 3 shows the mesh grid formulation for the ε archiving method in the objective space 3 ε f ε 1 ε f ε ε f ε 1 f f 1 f n ε ε 1 ε n n n o o f o b j e c t i v e s as discussed in reed et al 2007 ε is not a parameter of the mo algorithm it is rather a case dependent value determined either by the decision maker or by the systems analyst prior to the optimization based on the required precision in each objective function to ensure that only meaningful diffrences between non dominated solutions are considered in solution archiving ε moea ε nsgaii and borg moea are among algorithms that are equipped with ε archiving 2 1 rounded archiving method in this study an alternative solution archiving approach to ε archiving is proposed and called the rounded archiving in which the objective values are rounded to their desired precision levels before performing the point based dominance check the idea of rounded archiving evolved after observing a significant decrease in the number of non dominated solutions for a hypothetical example where 10 000 points are sampled from a uniformly random distribution in the 0 1 n n dimensional space representing the objective space of an mo problem with n independent objective functions fig 2 by definition in a minimization problem with n objectives solution a dominates solution b in terms of rounded archiving technique if and only if ε i r o u n d f i a ε i ε i r o u n d f i b ε i for all objectives i 1 n and there exists at least one objective j that the inequality ε i r o u n d f i a ε i ε i r o u n d f i b ε i holds the symbol r o u n d rounds a real value to the nearest integer if ε i r o u n d f i a ε i ε i r o u n d f i b ε i for all objectives i both solutions are equally important and only one of them is retained in the archive technically the rounded archiving method archives the solution that has an objective vector value corresponding to the black point i e ε r o u n d f ε in fig 3 this means that the rounded archiving maintains the dominance relation only if it shows a difference that is meaningful at the rounded level of objectives to be more specific if solution a dominates solution b at the full precision of objective values rounded archiving will prefer a over b if a still dominates b after its objective values are rounded however if a and b have the same rounded objective values one of them will be archived by the rounded archiving similar to the ε archiving the size of the grids for the rounded archiving method is equal to ε but its location is not the same as ε archiving the mesh grid formulation for the rounded archiving is shown in equation 4 the symbols and in equation 4 return the ceiling and floor of a real value respectively 4 ε f ε f ε 2 1 ε f ε f ε 2 ε f ε f ε 2 1 f f 1 f n ε ε 1 ε n n n o o f o b j e c t i v e s as noted in reed and devireddy 2004 the resolution precision level in this paper is case dependent and should reflect decision makers opinion about the meaningful difference between different options solutions rounding should be performed in the scaled objective space by the precision resolution level ε in ε r o u n d f ε for example for a cost function that ranges up to billions of dollars solutions need to be rounded to the nearest one million dollar if it is the deal breaker for the decision makers in the case of hydrologic model calibration the systems analyst might want to round the percent bias error metric to the nearest 1 if this resolution defines the meaningful difference between two solutions script 1 shows a function coded in matlab that rounds a set of objective function values to the precision level that is given set by the user this shows how the modeller needs to implement the rounded archiving strategy the rounded archiving method is formulated for the optimization problem formulation and not the mo algorithm therefore the optimization algorithm does not need to be altered to take advantage of the rounded archiving approach for solving many objective optimization problems script 1 the pseudo code function for rounding the exact value of objective function to the user specified precision levels image 1 2 2 optimization algorithms in order to evaluate and compare the effect of the point based rounded and ε archiving methods on proximity and diversity three mo algorithms that are frequently used for solving environmental and water resources mo problem are considered borg moea pa dds and amalgam these algorithms have completely different structures in terms of search selection and solution generation and therefore represent a wide range of different mo algorithms that have similar structures 2 2 1 borg moea borg moea was developed by hadka and reed 2013 for solving many objective optimization problems it is a robust variant of the ε moea algorithm deb et al 2003 and couples multiple components of different mo algorithms including the ε archiving laumanns et al 2002 adaptive population sizing tang et al 2006 adaptive tournament sizing hadka and reed 2013 for preserving the selection probability at a constant rate and multiple recombination operators this algorithm automatically adjusts the population size and selection probability when no meaningful improvement in the objective space is measured after a specified number of solution evaluations the ε archiving feature of borg moea improves its convergence and preserve the diversity of solutions in the objective space during optimization by diminishing the effect of dominance resistance and reducing the number of archived solutions borg moea has been successfully applied to many water resources problems such as a constrained six objective urban water portfolio planning hadka and reed 2015 a four objective lake pollution control quinn et al 2017 and a constrained three objective engineered injection and extraction for enhanced groundwater remediation piscopo et al 2015 2 2 2 pa dds pa dds asadzadeh and tolson 2013 is an efficient mo algorithm for solving environmental and water resources mo problems it is not population based and generates one solution in each iteration by selecting one currently non dominated solution and perturbing it to search for better solutions asadzadeh and tolson 2013 recommended hyper volume contribution as the most effective selection metric for pa dds solving general mo problems convex hull contribution chc is another selection metric developed by asadzadeh et al 2014a for pa dds applied to mo problems with a convex pareto front however chc is not used in this study since it has not been tested on problems with more than three objectives pa dds has only one parameter which is known as the solution perturbation size with a robust suggested value of 0 2 asadzadeh and tolson 2013 pa dds has an unbounded archive set and uses the point based archiving to archive all non dominated solutions throughout the search therefore it does not suffer from the oscillating behavior but it is expected to suffer from dominance resistance when applied to many objective optimization problems in this research ε archiving is implemented for pa dds to be able to test its performance against the point based and rounded archiving approaches outstanding applications of pa dds in water resources include reservoir operation asadzadeh et al 2014b razavi et al 2013 model calibration asadzadeh et al 2016 2015 and water distribution network design asadzadeh et al 2012 2 2 3 amalgam amalgam introduced by vrugt and robinson 2007 is a multi algorithm evolutionary optimization strategy that combines four well known and widely used heuristic optimization algorithms including the non dominated sorting genetic algorithm ii deb et al 2002b particle swarm optimization kennedy et al 2001 adaptive metropolis search haario et al 2001 and differential evolution storn and price 1997 the population size and the number of generations of amalgam are set based on the computational budget of this study and all its other parameters are set to their default values vrugt and robinson 2007 amalgam generates an initial population of solutions by latin hypercube sampling and gives each of its sub algorithms a pre determined portion of the whole population for generating new solutions then the parent and offspring solutions are combined and sorted using the fast non dominated sorting approach introduced in deb et al 2002a b if necessary the crowding distance metric is utilized for archiving solutions that have an equal dominance rank and the ones that have higher distance are retained in the archive the sub algorithm that contributed more to the set of archived solutions is given a higher portion of population size for generating new solutions in subsequent generation amalgam has a point based archiving and a bounded archive therefore it is not reinforced against the dominance resistance and oscillating effect the effectiveness of the proposed rounded archiving is validated on amalgam outstanding recent applications of amalgam include the many objective signature based hydrologic model calibration in shafii et al 2017 and the multi site hydrologic model calibration in zhang et al 2010 2 3 case studies 2 3 1 mathematical test problems benchmark mathematical test problems are designed to challenge the optimization algorithms often times they are quick to evaluate and the closed form of their true pareto front is known therefore they can be used to efficiently and effectively evaluate the performance of the optimization algorithms dtlz suite of test problems are scalable in terms of the number of decision variables and objective functions deb et al 2001 in this paper the bi objective and five objective versions of dtlz1 and r2 dtlz2 are solved to compare the effect of the archiving methods on the archive size at different number of objectives 2 3 1 1 dtlz1 dtlz1 has m variables with the range 0 1 and n objectives that are designed to trap mo algorithms in local fronts the ten variable bi objective and five objective dtlz1 solved in this research are proven to have respectively 1110 2 1 1 and 1110 5 1 1 local fronts with the true pareto front where i 1 n f i x 0 5 2 3 1 2 dtlz2 the five objective r2 dtlz2 test problem is a more complicated version of dtlz2 with a concave true pareto front with 30 decision variables according to zhang et al 2008 the decision variable space of r2 dtlz2 is mapped by a linear transformation orthogonal matrix and the objective space is extended by a stretching function to make it more challenging to solve the true pareto front of r2 dtlz2 is where i 1 n f i 1 2 1 and the range of its variables is 0 1 2 3 2 watershed model calibration 2 3 2 1 raven model of grand river watershed raven is a semi distributed watershed model introduced by craig 2015 capable of simulating short term rainfall runoff events and long term synthesis of hydrologic processes in a basin for resource management and water quality assessment it divides a watershed into several sub watersheds that are further partitioned into multiple hydrologic response units hrus each hru is lumped areas with a unique combination of topography geometry geography land use type and aquifer soil meteorological conditions such as rainfall temperature and wind velocity are then assigned to hrus the vertical water balance and energy balance are used for simulating and then assembling relevant hydrological processes in each hru the flow is then routed downstream and laterally by reconnecting hrus the interesting fact is that raven s level of complexity can change from a lumped model to a distributed model with a myriad of hrus depending on data availability or analyst s desire the raven model of an upstream sub watershed 274 km2 of the greater grand river watershed in south western ontario canada is re calibrated in this study following shafii et al 2017 the hydrometric data for time span of 2009 2014 is considered to calibrate 20 tunable parameters of the model using three quarter of the year 2009 as the warm up period and the rest of the data for calibration the preliminary analysis of this case study showed that some of the 15 objectives calibrated by shafii and tolson 2015 see table 1 are highly correlated none concflicting and therefore can safely be removed from the set of calibration objectives the proportion of non dominated space unnecessarily increases by considering highly correlated objectives in optimization and mo algorithms encounter difficulty in directing the search toward the desired non dominated front in addition according to the definition objective functions of a mo problem should display relatively conflicting behaviors giuliani et al 2014a for example incorporated the principal component analysis in a many objective reservoir operation problem to reduce the objective functions to a few uncorrelated principal objectives vectors that describe high percentile variance of the original mo problem to reach a more consistent and diverse pareto approximate front in this study the calibration objectives in shafii and tolson 2015 are compared in a pairwise manner and a strong linear correlation higher than 0 8 is observed between eight of these to the other seven objectives see table 1 for instance the overall runoff ratio obj 3 is perfectly correlated with the mean of streamflow data obj 8 as they both represent the capability of the model to simulate the water balance the seven objective functions that are used for the calibration of raven in this paper are highlighted in table 1 and briefly introduced next it is well known that an optimal parameter set for a single calibration metric such as nse nash and sutcliffe 1970 does not necessarily guarrantee the model to emulate the detailed processes of the real system gupta et al 1998 this also holds true in the case of multi and many objective calibration to a lesser extent maier et al 2018 which is referred to as overfitting due to incorporating certain characteristics of error distribution in calibration imperfect model structure simplifying assumptions in simulation uncertainty in measurements as well as spatiotemporal variations in model parameters efstratiadis and koutsoyiannis 2010 nse as shown in equation 5 is more sensitive to larger errors that often happen in high flow periods and to the timing and shape of the measured stream flux data due to the presence of three components including mean variance and the correlation coefficient gupta et al 2009 mse denote the mean of squared errors between the observations and simulations and σ 0 2 represents the variance of the observations 5 n s e 1 m s e σ 0 2 hydrologic signatures represented by equations 6 11 are reformulated in the form of absolute bias in simulated versus measured values to be minimized with an ideal value of zero q and q in equations 6 11 signify the streamflow and the mean of streamflow p is the probability of exceedence and k is the number of time steps the metrics in equations 6 8 evaluate the model performance in emulating the flow duration curve fdc fdc is a sorted logarithmic flow rate curve plotted versus cumulative frequency of exceedence following shafii and tolson 2015 fdc is divided into three segments of mid segment slope 25 75 high flow volume 0 2 and low flow volume 75 100 as shown in fig 4 the respective partitioning of fdc represents soil storage capacity quick runoff due to snow melt and or rainfall and base flow components of the streamflow yilmaz et al 2008 median in equation 9 and peak in equation 10 evaulate the performance in simulating mid flow and max flow bennett et al 2013 the one day lag auto correlation coefficient a c l a g 1 in equation 11 captures the repeated periodic patterns in streamflow time series bennett et al 2013 6 m i d s l o p e l o g 10 q 25 l o g 10 q 75 q 25 q p q 0 25 q 75 q p q 0 75 7 h i g h f l o w q h q h q p q 0 02 8 l o w f l o w l o g 10 q l q l q p q 0 75 9 m e d i a n q p q 0 5 10 p e a k q q max q i i 1 2 k 11 a c l a g 1 j 1 k 1 q j q q j 1 q j 1 k q j q 2 2 3 2 2 swat model of the rouge river watershed swat stands for soil and water assessment tool introduced by arnold et al 1990 for long term basin scale simulations of the hydrologic cycle swat combines features of different models to enable estimation of runoff sediment transport rate and water quality constituents such as suspended solids nitrogen and phosphate components at the outlet of each sub watershed neitsch et al 2011 swat is a semi distributed watershed model that divides the watershed into sub watersheds that can have multiple hrus however hrus do not carry any geographical location inside their sub watershed for each sub watershed swat aggregates the simulation components that are contiguously pyramided en route from sub watersheds to the streams and thus to the outlet of the watershed in this paper the swat model of the rouge river watershed 331 km2 ontario canada is used which was developed by asadzadeh et al 2015 the watershed encompasses four main land use classes agricultural urban natural and water bodies agricultural land management in terms of crop rotation cultivation planting tillage and fertilizer applications are simulated by the model daily hydrometric and climatic data from 2006 to 2009 are used to automatically calibrate the 13 most sensitive parameters of the model considering four objectives maximizing nse for daily streamflow and minimizing absolute bias in the volume of high flow segment less than 2 probability of exceedance the volume of low flow segment more than 70 probability of exceedance of fdc and the slope of the mid segment probability of exceedance between 30 and 70 of fdc asadzadeh et al 2015 formulated the swat calibration as a constrained optimization problem to satisfy the estimated values of evapotranspiration 62 of the annual average precipitation and surface flow contribution to streamflow 60 in this study these two constraints are handled by adding a fifth objective function that minimizes the total constraint violation as discussed in asadzadeh et al 2016 rouge river watershed has a sub daily hydrologic response to precpitation events and the daily simulation mode of swat cannot precisely model the timing of peak flows especially for late calendar day precipitation events as a result asadzadeh et al 2016 observed a one day offset issue in simulating some of the peak flow rates of rouge river watershed and added a feature to the calibration of swat to shift the simulated hydrograph for fixing the timing error the swat model calibration in this paper automatically handles the timing error 2 4 numerical experiment setup the three approaches of point based rounded and ε archiving are implemented and tested in pa dds and borg moea for solving problems explained in the previous section table 2 provides details on these problems as well as the ε value or rounding level for each objective the effectiveness of the rounded archiving as an alternative of the ε archiving approach is subsequently validated for the amalgam algorithm the algorithms are compared based on their performance in a multi trial optimization to capture the variation in their performance due to their stochastic nature we considered 10 and 50 trials for hydrologic model calibration and mathematical test problems respectively moreover the performance of the mo algorithms is assessed at a relatively low computational budget of 1000 solution evaluations and a relatively larger budget of 10000 solution evaluations per trial the low budget case is considered to assess the effect of dominance relations in low budget situations and how they help the performance of the algorithm concerning proximity and diversity the high budget case is suggested to minimize the effect of initialization of the algorithms and find good quality solutions in each trial of calibration asadzadeh et al 2015 shafii et al 2017 the parameterization of pa dds borg moea and amalgam has been done according to the default settings recommended by their developers the initial population for borg moea and amalgam is set to 100 solutions and pa dds automatically commences the search with at least five solutions or 0 5 of the total evaluations whichever is higher as the size of initial solutions pa dds uses the hyper volume contribution selection metric ε archiving is an inseparable part of borg moea structure therefore in order to assess its performance with rounded archiving and point based archiving an extremely small ε value 10 6 for all objectives is used in this study amalgam has a bounded archive size equal to its population size as a result archive truncation is highly likely to occur in many objective optimization and may lead to oscillating effect in amalgam 2 5 mo algorithm performance metrics performance metrics are used to quantify the convergence diversity and consistency of the pareto approximate fronts obtained by stochastic optimization algorithms a number of performance indicators exist in the literature including normalized hyper volume fonseca et al 2006 additive epsilon zitzler et al 2003 and generational distance veldhuizen and lamont 1998 that evaluate different characteristics of the pareto approximate front against the true pareto front or a reference pareto front these metrics have been widely used for evaluating mo algorithms since they can quantify the algorithms performance in terms of proximity and solution diversity in the objective space ward et al 2015 yuan et al 2016 2 5 1 normalized hyper volume nhv nhv normalizes the objective space to the unit hypercube using a nadir point an ever dominated point and a utopia point an absolutely non dominated point for a minimization problem fonseca et al 2006 basically nhv for a front is the portion of this unit hypercube that is dominated by the front obviously a higher value of nhv shows better proximity and diversity of the pareto approximate front the computational cost of the nhv calculation increases exponentially as the number of objective functions increases therefore in the many objective cases of this study the approximation version of nhv bader and zitzler 2008 is used 2 5 2 additive epsilon ε indicator the ε indicator zitzler et al 2003 calculates the minimum distance required for shifting a pareto approximate front to dominate a reference set of points as a whole the ε indicator has a minimum value of zero that is ideal when the reference set is the true pareto front and is very sensitive to the gaps in the pareto approximate front therefore it shows if outliers with poor proximity exist in the pareto approximate front the so called inconsistency hadka and reed 2012 2 5 3 generational distance gd gd is the average euclidean distance between each point on the pareto approximate front and its closest point on a reference set the true or best known pareto with minimum value of zero that is ideal when the pareto approximate front perfectly matches the reference set veldhuizen and lamont 1998 gd purely emphasizes on the proximity rather than the diversity preservation and gaps in the pareto approximate front have minimal effect on it the following steps are taken in this study to calculate the performance metrics 1 in order to have a fair comparison between the mo algorithms the rounded objective value of non dominated solutions are converted back to their full precision by re running the case studies for the corresponding solutions 2 the true pareto front is used as the reference set for mathematical test problems the best non dominated points are collected from all calibration runs disregarding the type of algorithm and archiving method and used as reference set for these problems 3 instead of exact nhv the monte carlo approximation nhv bader and zitzler 2008 is calculated for the seven objective raven model calibration problem 10000 random points are generated from the uniform distribution in the 0 1 7 space the portion of these points that are dominated by each pareto approximate front is calculated to reduce the effect of random number generator on the results this process is repeated ten times independently and the average of the portion of dominated points is calculated as the approximate nhv for each algorithm 4 borg moea automatically terminates the optimization when no improvement is measured in objective values after a certain number of iterations results are post processed to make sure that premature convergance did not occure 2 6 optimization algorithm performance comparison approach the mo optimization algorithms in this paper result in different performance metric values in each trial due to their stochastic search behavoiur the mo algorithms are compared based on the empirical cumulative distribution function cdfs that shows the probability of equal to or better performance than the argument of the function cdfs plot of the algorithms are compared by the first degree stochastic dominance levy 1992 the concept of stochastic domination is referred to as better performance at all levels of probability with regard to ε and gd indicators algorithm a stochastically dominates algorithm b if and only if the cdf of a is less than or equal to that of b at each level of probability carrano et al 2011 whereas a better performance in terms of nhv corresponds to an equal or higher metric value of a compared to b the statistical significance of the stochastic dominance is examined using the two sided wilcoxon rank sum test with the 95 confidence level the null hypothesis of this test states no significant difference between algorithms a and b a p value smaller than the significant level 5 in this paper in the two sided wilcoxon rank sum test is preferred to firmly reject the null hypothesis and confirm the difference between a and b similar performance comparison studies are used in hadka and reed 2012 and asadzadeh and tolson 2013 3 results and discussion 3 1 method development borg moea and pa dds 3 1 1 mathematical test problems according to the observations on the dtlz1 2d and r2 dtlz2 2d experiments with low computational budget the three archiving approaches make pa dds and borg moea store a similar and relatively small number of non dominated solutions cdf plots of the performance metrics not shown in this paper show that altering the archiving approach does not alter the mo algorithm performance in the case of 10000 solution evaluations point based archiving makes mo algorithms archive a high number of non dominated solutions relative to the rounded and ε archiving approaches see table 3 while cdf plots not shown in this paper do not reveal any significant preference in the archiving approach fig 5 shows the pareto approximate fronts obtained by the median performing trial of these algorithms based on ε indicator and confirms that rounded and ε archiving reproduce the same quality pareto approximate front as the point based archiving but with fewer solutions according to fig 6 a rounded archiving is preferred for pa dds for solving dtlz1 5d with 10000 solution evaluations even though it helps pa dds archive fewer solutions on average 383 versus 585 and 608 as in table 3 other cdf plots that are not provided in this paper show crossing behavior similar to fig 6 b meaning that changing the archiving technique results in a similar performance of pa dds and borg moea 3 1 2 hydrologic models calibrations according to table 4 the number of archived solutions decreases significantly when the rounded or ε archiving technique is applied results show that the average archive size reduces to about 22 and 50 in pa dds and borg moea respectively when rounded or ε archiving is applied for solving the five objective swat calibrated with 10000 solution evaluations moreover the rounded based borg moea stochastically dominates the cdf plot of the point based borg moea based on ε as illustrated in fig 7 in the high computational budget calibration case the two sided willcoxon rank sum tests confirm the preference toward rounded over point based archiving for borg moea in the swat calibration problem p values of 0 0022 0 000769 and 0 0046 for the ε indicator gd and nhv respectively the preference toward rounded against ε archiving borg moea is only statistically significant based on the gd p value of 0 0312 results did not show any statistially significant difference between alternative archiving approaches for borg moea in the low computational budget case i e 1000 simulation runs for pa dds used for swat calibration no statistically significant preference is observed among the three approaches except for the point based archiving that is preferred over the other two archiving approaches based only on gd for 10000 simulations with a p value of 0 000583 all the three archiving approaches statistically preserve the same level of diversity in high computational budget for pa dds in terms of ε indicator and nhv indicators with considerably smaller archive size of 504 for the rounded and ε based pa dds against 2264 table 5 demonstrates the proximity toward the utopia or the ideal point by finding the solution that has the shortest distance knee point from the utopia point in light of the fact that the extreme events are of greater importance compared with low flow rates a higher weight 0 4 is given to the nse that is more sensitive to larger errors that often occur in high flow periods the rest of the objective functions are equally weighted in general the distance to the utopia point is shorter when rounded archiving is used for swat except for pa dds that performed better with ε archiving with the budget of 1000 solution evaluations this means that the pareto approximate front is bent more towards the ideal point in the case of rounded archiving according to table 4 the point based pa dds archives more than 60 of generated solutions in the case of the seven objective raven model calibration for 10000 function evaluations while the rounded and ε based pa dds respectively archive only about 8 and 7 percents of all generated solutions in the same number of simulations in the 1000 evaluation experiment on pa dds the rounded and ε based techniques store only 207 and 213 solutions in the archive significantly less than 680 solutions that are archived using the original pa dds this means the traditional archiving method retains all the relatively similar solutions that have even slightly better value in only one out of seven objectives that is challenging for the calculation of one final preferred solution fig 8 shows that despite the significant decrease in the number of archived solutions both rounded and ε archiving approaches are preferred over the point based archiving version of the borg moea for 10000 simulations with a p value of 0 0211 according to table 5 rounded archiving is preferred for both pa dds and borg moea for calibrating the seven objective raven model because it helps the mo algorithms find the pareto approximate front with a knee point closer to the ideal point the only exception is the point based archiving that is preferred for borg moea at the limited budget of 1000 solution evaluations 3 2 method validation amalgam the rounded archiving is tested on amalgam for solving the many objective hydrologic model calibrations amalgam uses the point based archiving technique and has a bounded archive size that is defined a priori therefore it is expected to suffer from both deterioration and dominance resistance issues when applied to many objective optimization problems according to fig 9 rounded archiving improves the performance of amalgam for calibrating both of the five objective swat and seven objective raven models with the budget of 10000 solution evaluations this improvement is statistically meaningful with 95 confidence level based on the wilcoxon rank sum test results in table 6 except for the case of raven based on the indicator at the lower budget of 1000 solution evaluations the preference of rounded archiving over the point based archiving for amalgam is statistically significant in case of the raven model calibration but not for the swat model calibration see figure a and table 6 it is understood that rounded archiving decreased the oscillating issue due to the bounded archive of amalgam the bounded archive makes the mo algorithm eliminate some currently non dominated parameter sets and retain inferior ones in future iterations confirming the published study of hanne 1999 in addition the archive is rapidly filled with the initially generated low quality non dominated solutions and the subsequent solution production are based on the perturbation of the low quality archived ones while the rounded archiving method gradually stores the limited archive set by producing higher quality solutions during the optimization and it properly represents the entire range of the reference set if the precision level of each objective function is properly quantified a further experiment on the impact of the rounding level of the five objective r2 dtlz2 mathematical test problem in fig 10 demonstrate that consideration of the low resolution desired precision level results in a significant improvement on the ε value a similar result to fig 10 is expected for the computationally intensive problems such as the model calibration problems of this study however the modeller should be aware that a high rounding level increases the chance of multi modality in optimization 4 conclusions a comparative study of an alternative rounded archiving method to ε archiving was conducted the proposed archiving method of this study can carry out the task of ε archiving for algorithms that are not equiped with ε archiving the rounded archiving is user friendly in that it does not require altering the mo algorithm computer code this method is not algorithm or case specific and is much needed in situations where 1 the algorithm is not designed for solving many objective case studies and 2 the algorithm has a bounded archive structure the rounded based pareto approximate front is expected to properly represent the entire range of the true pareto front if the objectives resolutions are sufficiently quantified similar to ε archiving in reed et al 2007 at the methodology development stage of this study the rounded archiving resulted in significantly smaller archive size especially in high computational budgets for the five objective swat and seven objective raven model calibrations using pa dds and borg moea while maintaining a well diverse set of solutions in the objective space the rounded archiving has at least the same convergence level as ε archiving with no significant difference in the pareto front diversity for solving mathematical and model calibration problems using pa dds and borg moea the calculated knee points resulted from the proposed method have shorter distance to the ideal or utopia point than those obtained by the point based and ε archiving in the majority of numerical experiments it is concluded that the mo optimization algorithms need to be equipped with more specialized archiving strategies such as the ε or the rounded archiving to solve many objective optimizaton problems since the traditional point based archiving method leads to exponential increase in the number of non dominated solutions especially when mo algorithms have an unbounded archive the proposed rounded archiving method is tested on amalgam that has a bounded archive and is not equipped with ε archiving rounded archiving reamarkably improves the level of proximity and diversity of the pareto approximate front compared to the original amalgam results show that rounded archiving made amalgam perform better than the original amalgam in eight out of 12 hypothesis testings the rounded archiving can be easily generalized to other multi objective algorithms for solving many objective optimization problems reducing the number of archived solutions using either rounded or ε archiving assists decision makers and model analysts to find their desired solution or parameter set based on the status quo of their projects with less confusion it should be noted that the resolution rounding level of each objective needs to be consulted with decision makers prior to optimization setting it is shown in this study that using a coarse rounding level leads to a smaller number of archived solutions which confirms findings by reed et al 2007 the main advantage of rounded archiving over ε archiving is that it is readily available for any mo algorithm that is not equipped with any specialized archiving such as pa dds and amalgam in this study moreover rounded archiving is computationally more efficient but this advantage becomes trivial when solving problems with a time consuming simulation and or evaluation models such as the model calibration case studies of this paper ε archiving approach retains the exact values of objective vectors while rounded archiving will increase the probability of multi modality if a very low resolution is set for objectives rounded archiving needs to be applied to other mo algorithms and other water resources case studies but owing to the highly time demanding process of this comparison study three mo algorithms are considered as sufficient for generalizing the concluding remarks of this study acknowledgment we acknowledge the support of the natural sciences and engineering research council of canada nserc discovery grant of the second author rgpin 2016 05896 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104535 appendix fig a empirical cdf plots comparing point based green series versus rounded red series archiving for amalgam for calibrating five objective swat and seven objective raven models with 1000 solution evaluations fig a 
26113,heuristic multi objective optimization mo algorithms lose their efficiency and performance as the number of objectives increases due to the so called dominance resistance unless they are equipped with a specialized solution archiving strategy like epsilon ε archiving this study introduces an alternative approach to tackle dominance resistance for solving environmental and water resources engineering problems with more than three objectives in the proposed approach objectives are rounded to user defined precision levels before checking the dominance rounded archiving is developed and assessed for pa dds and borg moea and verified for amalgam applied to hydrologic model calibration problems with more than three objectives results show that rounded archiving significantly improves the performance of mo algorithms and is at least as effective as if not better than the ε archiving for solving many objective optimization problems without the need to restructure the algorithm which is the requirement for the implementation of the ε archiving approach keywords many objective optimization rounded archiving epsilon archiving dominance resistance model calibration software availability name of software pa dds with rounded archiving method developer and contact information shahram sahraei masoud asadzadeh department of civil engineering university of manitoba eitc e1 332 15 gillson street winnipeg mb canada r3t 5v6 email address sahraeis myumanitoba ca masoud asadzadeh umanitoba ca year first available 2019 hardware required desktop computer availability go to zdt round folder in http home cc umanitoba ca asadzadm software html to see rounded archiving method in two objective zdt1 test problem software required matlab program size 1 mb 1 introduction many objective optimization i e multi objective mo optimization with four or more conflicting objectives has become an attractive tool in a broad range of environmental and water resources problems such as calibration of environmental models e g shafii and tolson 2015 zhang et al 2008 water distribution network design fu et al 2012 reservoir operation geressu and harou 2019 giuliani et al 2014a b stormwater management di matteo et al 2018 robust decision making eker and kwakkel 2018 singh et al 2015 kasprzyk et al 2013 and crop food production lautenbach et al 2013 for instance the hydrologic model calibration is well known to have a mo nature for which no single solution simultaneously optimizes all the objective functions model performance metrics see for example boyle et al 2000 wagener and gupta 2005 gupta et al 2006 and maier et al 2018 instead a set of alternatives that is often called tradeoff or pareto after vilfredo pareto 1848 1923 exists since improving one objective degrades some other objectives traditionally hydrologic models had been calibrated with two or three objective functions asadzadeh et al 2015 asadzadeh et al 2013 shafii and smedt 2009 gupta et al 1998 but recent developments in the hydrologic models computational resources and mo algorithms is changing the trend toward the many objective model calibration ercan and goodall 2016 wang and brubaker 2015 reed et al 2013 kollat et al 2012 the number of objectives can easily grow beyond three when the calibration problem is formulated to represent internal hydrological processes besides optimizing traditional model performance metrics such as the sum of squared errors or its variants for instance shafii and tolson 2015 formulated and solved a 15 objective model calibration problem and showed that incorporating the objective functions pertaining to the dominant hydrologic processes of a watershed aids in selecting the most reliable and hydrologically consistent model parameter values with respect to the selected dominant hydrologic processes on the other hand the size of the non dominated space grows exponentially as the number of objective functions increases farina and amato 2002 and leads to a two fold difficulty in solving many objective optimization problems by a heuristic mo algorithm unless it is equipped with a specialized archiving approach first identifying any new dominating better solution is very challenging in many objective optimization due to the high similarity in the objective space this is often referred to as the dominance resistance in the literature ikeda et al 2001 purshouse and fleming 2007 second heuristic mo algorithms that have a bounded archive size e g nsga ii deb et al 2002b have to discard some good quality solutions from the current archive and therefore are subject to the oscillating issue that makes them inefficient for solving many objective optimization problems on the other hand mo algorithms that have an unbounded archive size such as the pareto archived dynamically dimensioned search pa dds in asadzadeh et al 2013 might archive a large number of pareto solutions because solutions for further generations are generated based on the archived pareto set such algorithms assign a very low selection probability to high quality solutions that are scattered among a huge number of pareto solutions teytaud 2007 reed et al 2013 maier et al 2014 mo algorithms that have a bounded archive keep at most an a priori specified number of non dominated solutions by the end of optimization the bounded archive set was first suggested in non dominated sorting genetic algorithm nsga srinivas and deb 1994 in order to reduce the computational time for checking the dominance rank accelerating the decision making process and concentrating solely on appealing parts of the pareto front however tightly bounded archive size of mo algorithms results in an oscillating behavior in that some currently non dominated solutions have to be discarded from the archive in the current generation and be replaced by some inferior ones in subsequent generations hanne 1999 to increase the efficiency of mo algorithms in many objective applications several approaches have been developed in the literature including fuzzy optimality farina and amato 2004 order of efficiency di pierro et al 2007 preferability drechsler et al 2001 and epsilon preferability sulflow et al 2007 these methods are conceptually similar preferability is defined as choosing non dominated solutions that are dominating a larger number of objectives when comparing two non dominated solutions sulflow et al 2007 fuzzy optimality is a fuzzy based concept of optimality that considers the number of improved objectives and the order of improvement between two solutions order of efficiency is the minimum size of non dominated objective sub space for a solution compared with the archived ones however these methods must be used with caution as the search may be misled into undesired regions of the decision space especially when the objective functions are not equally important to decision makers coello et al 2007 laumanns et al 2002 suggested the cell epsilon ε archiving approach to reduce the size of non dominated archived solutions and therefore improve the diversity in the objective space deb et al 2003 successfully applied the ε archiving technique to a multi objective evolutionary algorithm called ε moea that outperformed five other moeas for solving mathematical mo test problems with two three and four objective functions in terms of diversity and convergence reed and devireddy 2004 introduced a variant of nsga ii called ε nsgaii that utilizes the ε archiving applied it to solve a bi objective groundwater monitoring problem and concluded that ε archiving significantly increased the efficiency of the optimization algorithm kollat and reed 2006 applied ε nsgaii to a long term groundwater monitoring design problem with four objective functions and showed a superior performance of ε nsgaii against nsga ii as well as other modern mo algorithms hadka and reed 2013 combined a group of mo operators including the ε archiving and introduced the borg moea that showed a superior performance in comparison with six well known mo algorithms for solving benchmark suites of test problems with 2 8 objectives many modern mo algorithms do not have the ε archiving strategy e g pa dds a multi algorithm genetically adaptive multi objective amalgam vrugt and robinson 2007 strength pareto evolutioary algorithm 2 zitzler et al 2001 nsga iii deb and jain 2014 and therefore are anticipated to lose their performance in terms of convergence and solution diversity when applied to many objective problems in this study an alternative approach to ε archiving is introduced this approach that is called rounded archiving is technically the point based dominace archiving of the rounded value of objective functions the main advantage of the proposed approach over ε archiving is that it does not require the user to restructure the mo algorithm computer code the rounded archiving approach is compared to the ε archiving approach for solving hydrologic model calibration problems that are formulated and solved as many objective optimization problems in the literature the proposed approach is not algorithm or case specific and is readily applicable to other mo algorithms such as pa dds and amalgam as shown in this study the remainder of this paper is organized as follows the methods section describes the proposed rounded archiving method applied to mo algorithms including borg moea pa dds and amalgam case studies including the mathematical and hydrologic model calibration problems numerical experiment setup and the results analysis approach next results are presented and a general discussion is given followed by the concluding remarks 2 methods mo algorithms typically start the optimization with a random solution generation and archive solutions based on the dominance check in the objective space then one or a population of archived solutions are selected for perturbation or recombination to create new solution s for subsequent evaluations this process continues until a termination condition is met point based archiving and selection strategies therefore constitute the principal parts of a mo algorithm to guide the search toward a better proximity convergence and diversity of solutions to approximate the pareto front deb 2001 coello et al 2007 pareto approximate front is referred to as the archived non dominated solutions found by a mo algorithm at the end of optimization in point based archiving solution a is said to be non dominated if and only if there is no other solution b that dominates it veldhuizen and lamont 2000 equation 1 shows the matematical representation of point based archiving concept of solution a dominating b for a minimization problem that has n conflicting objective functions in the case of maximization the corresponding objective function needs to be multiplied by 1 to change it to a minimization problem 1 a b f i a f i b i 1 2 n f j a f j b j 1 2 n in general the regular point based dominance check and archiving strategy become ineffective as the number of objective functions grows to more than three farina and amato 2002 mathematically showed by equation 2 that the proportion of non dominated space e exponentially increases as the number of objectives n increases this issue increases the similarity of the archived solutions in the objective space and causes the dominance resistance ikeda et al 2001 purshouse and fleming 2007 that degrades the performance of mo optimization algorithms 2 e 2 n 2 2 n the cell based dominance check and the corresponding ε archiving introduced by laumanns et al 2002 which is based on the ε dominance addresses the dominance resistance issue and improves the performance of mo algorithms for solving many objective optimization problems by definition in a minimization problem with n objectives solution a ε dominates solution b if and only if f i a ε i f i b ε i for all objectives i 1 n with f j a ε j f j b ε j for at least one objective j the symbol returns the floor of a real value ε archiving uses this inequality to develop a mesh grid in the objective space and archive at most one solution in each grid cell if two solutions a and b reside in one cell i e f i a ε i f i b ε i for all objectives i the solution that is closer to the dominating bottom left in minimization problems corner of the cell is archived and the other solution is discarded green points in fig 1 are ε dominated by the red points demonstrating both conditions of ε archiving relation the size of each grid cell is equal to ε which should be determined a priori by the decision maker or systems analyst reed and devireddy 2004 for each objective function based on the desired precision level equation 3 shows the mesh grid formulation for the ε archiving method in the objective space 3 ε f ε 1 ε f ε ε f ε 1 f f 1 f n ε ε 1 ε n n n o o f o b j e c t i v e s as discussed in reed et al 2007 ε is not a parameter of the mo algorithm it is rather a case dependent value determined either by the decision maker or by the systems analyst prior to the optimization based on the required precision in each objective function to ensure that only meaningful diffrences between non dominated solutions are considered in solution archiving ε moea ε nsgaii and borg moea are among algorithms that are equipped with ε archiving 2 1 rounded archiving method in this study an alternative solution archiving approach to ε archiving is proposed and called the rounded archiving in which the objective values are rounded to their desired precision levels before performing the point based dominance check the idea of rounded archiving evolved after observing a significant decrease in the number of non dominated solutions for a hypothetical example where 10 000 points are sampled from a uniformly random distribution in the 0 1 n n dimensional space representing the objective space of an mo problem with n independent objective functions fig 2 by definition in a minimization problem with n objectives solution a dominates solution b in terms of rounded archiving technique if and only if ε i r o u n d f i a ε i ε i r o u n d f i b ε i for all objectives i 1 n and there exists at least one objective j that the inequality ε i r o u n d f i a ε i ε i r o u n d f i b ε i holds the symbol r o u n d rounds a real value to the nearest integer if ε i r o u n d f i a ε i ε i r o u n d f i b ε i for all objectives i both solutions are equally important and only one of them is retained in the archive technically the rounded archiving method archives the solution that has an objective vector value corresponding to the black point i e ε r o u n d f ε in fig 3 this means that the rounded archiving maintains the dominance relation only if it shows a difference that is meaningful at the rounded level of objectives to be more specific if solution a dominates solution b at the full precision of objective values rounded archiving will prefer a over b if a still dominates b after its objective values are rounded however if a and b have the same rounded objective values one of them will be archived by the rounded archiving similar to the ε archiving the size of the grids for the rounded archiving method is equal to ε but its location is not the same as ε archiving the mesh grid formulation for the rounded archiving is shown in equation 4 the symbols and in equation 4 return the ceiling and floor of a real value respectively 4 ε f ε f ε 2 1 ε f ε f ε 2 ε f ε f ε 2 1 f f 1 f n ε ε 1 ε n n n o o f o b j e c t i v e s as noted in reed and devireddy 2004 the resolution precision level in this paper is case dependent and should reflect decision makers opinion about the meaningful difference between different options solutions rounding should be performed in the scaled objective space by the precision resolution level ε in ε r o u n d f ε for example for a cost function that ranges up to billions of dollars solutions need to be rounded to the nearest one million dollar if it is the deal breaker for the decision makers in the case of hydrologic model calibration the systems analyst might want to round the percent bias error metric to the nearest 1 if this resolution defines the meaningful difference between two solutions script 1 shows a function coded in matlab that rounds a set of objective function values to the precision level that is given set by the user this shows how the modeller needs to implement the rounded archiving strategy the rounded archiving method is formulated for the optimization problem formulation and not the mo algorithm therefore the optimization algorithm does not need to be altered to take advantage of the rounded archiving approach for solving many objective optimization problems script 1 the pseudo code function for rounding the exact value of objective function to the user specified precision levels image 1 2 2 optimization algorithms in order to evaluate and compare the effect of the point based rounded and ε archiving methods on proximity and diversity three mo algorithms that are frequently used for solving environmental and water resources mo problem are considered borg moea pa dds and amalgam these algorithms have completely different structures in terms of search selection and solution generation and therefore represent a wide range of different mo algorithms that have similar structures 2 2 1 borg moea borg moea was developed by hadka and reed 2013 for solving many objective optimization problems it is a robust variant of the ε moea algorithm deb et al 2003 and couples multiple components of different mo algorithms including the ε archiving laumanns et al 2002 adaptive population sizing tang et al 2006 adaptive tournament sizing hadka and reed 2013 for preserving the selection probability at a constant rate and multiple recombination operators this algorithm automatically adjusts the population size and selection probability when no meaningful improvement in the objective space is measured after a specified number of solution evaluations the ε archiving feature of borg moea improves its convergence and preserve the diversity of solutions in the objective space during optimization by diminishing the effect of dominance resistance and reducing the number of archived solutions borg moea has been successfully applied to many water resources problems such as a constrained six objective urban water portfolio planning hadka and reed 2015 a four objective lake pollution control quinn et al 2017 and a constrained three objective engineered injection and extraction for enhanced groundwater remediation piscopo et al 2015 2 2 2 pa dds pa dds asadzadeh and tolson 2013 is an efficient mo algorithm for solving environmental and water resources mo problems it is not population based and generates one solution in each iteration by selecting one currently non dominated solution and perturbing it to search for better solutions asadzadeh and tolson 2013 recommended hyper volume contribution as the most effective selection metric for pa dds solving general mo problems convex hull contribution chc is another selection metric developed by asadzadeh et al 2014a for pa dds applied to mo problems with a convex pareto front however chc is not used in this study since it has not been tested on problems with more than three objectives pa dds has only one parameter which is known as the solution perturbation size with a robust suggested value of 0 2 asadzadeh and tolson 2013 pa dds has an unbounded archive set and uses the point based archiving to archive all non dominated solutions throughout the search therefore it does not suffer from the oscillating behavior but it is expected to suffer from dominance resistance when applied to many objective optimization problems in this research ε archiving is implemented for pa dds to be able to test its performance against the point based and rounded archiving approaches outstanding applications of pa dds in water resources include reservoir operation asadzadeh et al 2014b razavi et al 2013 model calibration asadzadeh et al 2016 2015 and water distribution network design asadzadeh et al 2012 2 2 3 amalgam amalgam introduced by vrugt and robinson 2007 is a multi algorithm evolutionary optimization strategy that combines four well known and widely used heuristic optimization algorithms including the non dominated sorting genetic algorithm ii deb et al 2002b particle swarm optimization kennedy et al 2001 adaptive metropolis search haario et al 2001 and differential evolution storn and price 1997 the population size and the number of generations of amalgam are set based on the computational budget of this study and all its other parameters are set to their default values vrugt and robinson 2007 amalgam generates an initial population of solutions by latin hypercube sampling and gives each of its sub algorithms a pre determined portion of the whole population for generating new solutions then the parent and offspring solutions are combined and sorted using the fast non dominated sorting approach introduced in deb et al 2002a b if necessary the crowding distance metric is utilized for archiving solutions that have an equal dominance rank and the ones that have higher distance are retained in the archive the sub algorithm that contributed more to the set of archived solutions is given a higher portion of population size for generating new solutions in subsequent generation amalgam has a point based archiving and a bounded archive therefore it is not reinforced against the dominance resistance and oscillating effect the effectiveness of the proposed rounded archiving is validated on amalgam outstanding recent applications of amalgam include the many objective signature based hydrologic model calibration in shafii et al 2017 and the multi site hydrologic model calibration in zhang et al 2010 2 3 case studies 2 3 1 mathematical test problems benchmark mathematical test problems are designed to challenge the optimization algorithms often times they are quick to evaluate and the closed form of their true pareto front is known therefore they can be used to efficiently and effectively evaluate the performance of the optimization algorithms dtlz suite of test problems are scalable in terms of the number of decision variables and objective functions deb et al 2001 in this paper the bi objective and five objective versions of dtlz1 and r2 dtlz2 are solved to compare the effect of the archiving methods on the archive size at different number of objectives 2 3 1 1 dtlz1 dtlz1 has m variables with the range 0 1 and n objectives that are designed to trap mo algorithms in local fronts the ten variable bi objective and five objective dtlz1 solved in this research are proven to have respectively 1110 2 1 1 and 1110 5 1 1 local fronts with the true pareto front where i 1 n f i x 0 5 2 3 1 2 dtlz2 the five objective r2 dtlz2 test problem is a more complicated version of dtlz2 with a concave true pareto front with 30 decision variables according to zhang et al 2008 the decision variable space of r2 dtlz2 is mapped by a linear transformation orthogonal matrix and the objective space is extended by a stretching function to make it more challenging to solve the true pareto front of r2 dtlz2 is where i 1 n f i 1 2 1 and the range of its variables is 0 1 2 3 2 watershed model calibration 2 3 2 1 raven model of grand river watershed raven is a semi distributed watershed model introduced by craig 2015 capable of simulating short term rainfall runoff events and long term synthesis of hydrologic processes in a basin for resource management and water quality assessment it divides a watershed into several sub watersheds that are further partitioned into multiple hydrologic response units hrus each hru is lumped areas with a unique combination of topography geometry geography land use type and aquifer soil meteorological conditions such as rainfall temperature and wind velocity are then assigned to hrus the vertical water balance and energy balance are used for simulating and then assembling relevant hydrological processes in each hru the flow is then routed downstream and laterally by reconnecting hrus the interesting fact is that raven s level of complexity can change from a lumped model to a distributed model with a myriad of hrus depending on data availability or analyst s desire the raven model of an upstream sub watershed 274 km2 of the greater grand river watershed in south western ontario canada is re calibrated in this study following shafii et al 2017 the hydrometric data for time span of 2009 2014 is considered to calibrate 20 tunable parameters of the model using three quarter of the year 2009 as the warm up period and the rest of the data for calibration the preliminary analysis of this case study showed that some of the 15 objectives calibrated by shafii and tolson 2015 see table 1 are highly correlated none concflicting and therefore can safely be removed from the set of calibration objectives the proportion of non dominated space unnecessarily increases by considering highly correlated objectives in optimization and mo algorithms encounter difficulty in directing the search toward the desired non dominated front in addition according to the definition objective functions of a mo problem should display relatively conflicting behaviors giuliani et al 2014a for example incorporated the principal component analysis in a many objective reservoir operation problem to reduce the objective functions to a few uncorrelated principal objectives vectors that describe high percentile variance of the original mo problem to reach a more consistent and diverse pareto approximate front in this study the calibration objectives in shafii and tolson 2015 are compared in a pairwise manner and a strong linear correlation higher than 0 8 is observed between eight of these to the other seven objectives see table 1 for instance the overall runoff ratio obj 3 is perfectly correlated with the mean of streamflow data obj 8 as they both represent the capability of the model to simulate the water balance the seven objective functions that are used for the calibration of raven in this paper are highlighted in table 1 and briefly introduced next it is well known that an optimal parameter set for a single calibration metric such as nse nash and sutcliffe 1970 does not necessarily guarrantee the model to emulate the detailed processes of the real system gupta et al 1998 this also holds true in the case of multi and many objective calibration to a lesser extent maier et al 2018 which is referred to as overfitting due to incorporating certain characteristics of error distribution in calibration imperfect model structure simplifying assumptions in simulation uncertainty in measurements as well as spatiotemporal variations in model parameters efstratiadis and koutsoyiannis 2010 nse as shown in equation 5 is more sensitive to larger errors that often happen in high flow periods and to the timing and shape of the measured stream flux data due to the presence of three components including mean variance and the correlation coefficient gupta et al 2009 mse denote the mean of squared errors between the observations and simulations and σ 0 2 represents the variance of the observations 5 n s e 1 m s e σ 0 2 hydrologic signatures represented by equations 6 11 are reformulated in the form of absolute bias in simulated versus measured values to be minimized with an ideal value of zero q and q in equations 6 11 signify the streamflow and the mean of streamflow p is the probability of exceedence and k is the number of time steps the metrics in equations 6 8 evaluate the model performance in emulating the flow duration curve fdc fdc is a sorted logarithmic flow rate curve plotted versus cumulative frequency of exceedence following shafii and tolson 2015 fdc is divided into three segments of mid segment slope 25 75 high flow volume 0 2 and low flow volume 75 100 as shown in fig 4 the respective partitioning of fdc represents soil storage capacity quick runoff due to snow melt and or rainfall and base flow components of the streamflow yilmaz et al 2008 median in equation 9 and peak in equation 10 evaulate the performance in simulating mid flow and max flow bennett et al 2013 the one day lag auto correlation coefficient a c l a g 1 in equation 11 captures the repeated periodic patterns in streamflow time series bennett et al 2013 6 m i d s l o p e l o g 10 q 25 l o g 10 q 75 q 25 q p q 0 25 q 75 q p q 0 75 7 h i g h f l o w q h q h q p q 0 02 8 l o w f l o w l o g 10 q l q l q p q 0 75 9 m e d i a n q p q 0 5 10 p e a k q q max q i i 1 2 k 11 a c l a g 1 j 1 k 1 q j q q j 1 q j 1 k q j q 2 2 3 2 2 swat model of the rouge river watershed swat stands for soil and water assessment tool introduced by arnold et al 1990 for long term basin scale simulations of the hydrologic cycle swat combines features of different models to enable estimation of runoff sediment transport rate and water quality constituents such as suspended solids nitrogen and phosphate components at the outlet of each sub watershed neitsch et al 2011 swat is a semi distributed watershed model that divides the watershed into sub watersheds that can have multiple hrus however hrus do not carry any geographical location inside their sub watershed for each sub watershed swat aggregates the simulation components that are contiguously pyramided en route from sub watersheds to the streams and thus to the outlet of the watershed in this paper the swat model of the rouge river watershed 331 km2 ontario canada is used which was developed by asadzadeh et al 2015 the watershed encompasses four main land use classes agricultural urban natural and water bodies agricultural land management in terms of crop rotation cultivation planting tillage and fertilizer applications are simulated by the model daily hydrometric and climatic data from 2006 to 2009 are used to automatically calibrate the 13 most sensitive parameters of the model considering four objectives maximizing nse for daily streamflow and minimizing absolute bias in the volume of high flow segment less than 2 probability of exceedance the volume of low flow segment more than 70 probability of exceedance of fdc and the slope of the mid segment probability of exceedance between 30 and 70 of fdc asadzadeh et al 2015 formulated the swat calibration as a constrained optimization problem to satisfy the estimated values of evapotranspiration 62 of the annual average precipitation and surface flow contribution to streamflow 60 in this study these two constraints are handled by adding a fifth objective function that minimizes the total constraint violation as discussed in asadzadeh et al 2016 rouge river watershed has a sub daily hydrologic response to precpitation events and the daily simulation mode of swat cannot precisely model the timing of peak flows especially for late calendar day precipitation events as a result asadzadeh et al 2016 observed a one day offset issue in simulating some of the peak flow rates of rouge river watershed and added a feature to the calibration of swat to shift the simulated hydrograph for fixing the timing error the swat model calibration in this paper automatically handles the timing error 2 4 numerical experiment setup the three approaches of point based rounded and ε archiving are implemented and tested in pa dds and borg moea for solving problems explained in the previous section table 2 provides details on these problems as well as the ε value or rounding level for each objective the effectiveness of the rounded archiving as an alternative of the ε archiving approach is subsequently validated for the amalgam algorithm the algorithms are compared based on their performance in a multi trial optimization to capture the variation in their performance due to their stochastic nature we considered 10 and 50 trials for hydrologic model calibration and mathematical test problems respectively moreover the performance of the mo algorithms is assessed at a relatively low computational budget of 1000 solution evaluations and a relatively larger budget of 10000 solution evaluations per trial the low budget case is considered to assess the effect of dominance relations in low budget situations and how they help the performance of the algorithm concerning proximity and diversity the high budget case is suggested to minimize the effect of initialization of the algorithms and find good quality solutions in each trial of calibration asadzadeh et al 2015 shafii et al 2017 the parameterization of pa dds borg moea and amalgam has been done according to the default settings recommended by their developers the initial population for borg moea and amalgam is set to 100 solutions and pa dds automatically commences the search with at least five solutions or 0 5 of the total evaluations whichever is higher as the size of initial solutions pa dds uses the hyper volume contribution selection metric ε archiving is an inseparable part of borg moea structure therefore in order to assess its performance with rounded archiving and point based archiving an extremely small ε value 10 6 for all objectives is used in this study amalgam has a bounded archive size equal to its population size as a result archive truncation is highly likely to occur in many objective optimization and may lead to oscillating effect in amalgam 2 5 mo algorithm performance metrics performance metrics are used to quantify the convergence diversity and consistency of the pareto approximate fronts obtained by stochastic optimization algorithms a number of performance indicators exist in the literature including normalized hyper volume fonseca et al 2006 additive epsilon zitzler et al 2003 and generational distance veldhuizen and lamont 1998 that evaluate different characteristics of the pareto approximate front against the true pareto front or a reference pareto front these metrics have been widely used for evaluating mo algorithms since they can quantify the algorithms performance in terms of proximity and solution diversity in the objective space ward et al 2015 yuan et al 2016 2 5 1 normalized hyper volume nhv nhv normalizes the objective space to the unit hypercube using a nadir point an ever dominated point and a utopia point an absolutely non dominated point for a minimization problem fonseca et al 2006 basically nhv for a front is the portion of this unit hypercube that is dominated by the front obviously a higher value of nhv shows better proximity and diversity of the pareto approximate front the computational cost of the nhv calculation increases exponentially as the number of objective functions increases therefore in the many objective cases of this study the approximation version of nhv bader and zitzler 2008 is used 2 5 2 additive epsilon ε indicator the ε indicator zitzler et al 2003 calculates the minimum distance required for shifting a pareto approximate front to dominate a reference set of points as a whole the ε indicator has a minimum value of zero that is ideal when the reference set is the true pareto front and is very sensitive to the gaps in the pareto approximate front therefore it shows if outliers with poor proximity exist in the pareto approximate front the so called inconsistency hadka and reed 2012 2 5 3 generational distance gd gd is the average euclidean distance between each point on the pareto approximate front and its closest point on a reference set the true or best known pareto with minimum value of zero that is ideal when the pareto approximate front perfectly matches the reference set veldhuizen and lamont 1998 gd purely emphasizes on the proximity rather than the diversity preservation and gaps in the pareto approximate front have minimal effect on it the following steps are taken in this study to calculate the performance metrics 1 in order to have a fair comparison between the mo algorithms the rounded objective value of non dominated solutions are converted back to their full precision by re running the case studies for the corresponding solutions 2 the true pareto front is used as the reference set for mathematical test problems the best non dominated points are collected from all calibration runs disregarding the type of algorithm and archiving method and used as reference set for these problems 3 instead of exact nhv the monte carlo approximation nhv bader and zitzler 2008 is calculated for the seven objective raven model calibration problem 10000 random points are generated from the uniform distribution in the 0 1 7 space the portion of these points that are dominated by each pareto approximate front is calculated to reduce the effect of random number generator on the results this process is repeated ten times independently and the average of the portion of dominated points is calculated as the approximate nhv for each algorithm 4 borg moea automatically terminates the optimization when no improvement is measured in objective values after a certain number of iterations results are post processed to make sure that premature convergance did not occure 2 6 optimization algorithm performance comparison approach the mo optimization algorithms in this paper result in different performance metric values in each trial due to their stochastic search behavoiur the mo algorithms are compared based on the empirical cumulative distribution function cdfs that shows the probability of equal to or better performance than the argument of the function cdfs plot of the algorithms are compared by the first degree stochastic dominance levy 1992 the concept of stochastic domination is referred to as better performance at all levels of probability with regard to ε and gd indicators algorithm a stochastically dominates algorithm b if and only if the cdf of a is less than or equal to that of b at each level of probability carrano et al 2011 whereas a better performance in terms of nhv corresponds to an equal or higher metric value of a compared to b the statistical significance of the stochastic dominance is examined using the two sided wilcoxon rank sum test with the 95 confidence level the null hypothesis of this test states no significant difference between algorithms a and b a p value smaller than the significant level 5 in this paper in the two sided wilcoxon rank sum test is preferred to firmly reject the null hypothesis and confirm the difference between a and b similar performance comparison studies are used in hadka and reed 2012 and asadzadeh and tolson 2013 3 results and discussion 3 1 method development borg moea and pa dds 3 1 1 mathematical test problems according to the observations on the dtlz1 2d and r2 dtlz2 2d experiments with low computational budget the three archiving approaches make pa dds and borg moea store a similar and relatively small number of non dominated solutions cdf plots of the performance metrics not shown in this paper show that altering the archiving approach does not alter the mo algorithm performance in the case of 10000 solution evaluations point based archiving makes mo algorithms archive a high number of non dominated solutions relative to the rounded and ε archiving approaches see table 3 while cdf plots not shown in this paper do not reveal any significant preference in the archiving approach fig 5 shows the pareto approximate fronts obtained by the median performing trial of these algorithms based on ε indicator and confirms that rounded and ε archiving reproduce the same quality pareto approximate front as the point based archiving but with fewer solutions according to fig 6 a rounded archiving is preferred for pa dds for solving dtlz1 5d with 10000 solution evaluations even though it helps pa dds archive fewer solutions on average 383 versus 585 and 608 as in table 3 other cdf plots that are not provided in this paper show crossing behavior similar to fig 6 b meaning that changing the archiving technique results in a similar performance of pa dds and borg moea 3 1 2 hydrologic models calibrations according to table 4 the number of archived solutions decreases significantly when the rounded or ε archiving technique is applied results show that the average archive size reduces to about 22 and 50 in pa dds and borg moea respectively when rounded or ε archiving is applied for solving the five objective swat calibrated with 10000 solution evaluations moreover the rounded based borg moea stochastically dominates the cdf plot of the point based borg moea based on ε as illustrated in fig 7 in the high computational budget calibration case the two sided willcoxon rank sum tests confirm the preference toward rounded over point based archiving for borg moea in the swat calibration problem p values of 0 0022 0 000769 and 0 0046 for the ε indicator gd and nhv respectively the preference toward rounded against ε archiving borg moea is only statistically significant based on the gd p value of 0 0312 results did not show any statistially significant difference between alternative archiving approaches for borg moea in the low computational budget case i e 1000 simulation runs for pa dds used for swat calibration no statistically significant preference is observed among the three approaches except for the point based archiving that is preferred over the other two archiving approaches based only on gd for 10000 simulations with a p value of 0 000583 all the three archiving approaches statistically preserve the same level of diversity in high computational budget for pa dds in terms of ε indicator and nhv indicators with considerably smaller archive size of 504 for the rounded and ε based pa dds against 2264 table 5 demonstrates the proximity toward the utopia or the ideal point by finding the solution that has the shortest distance knee point from the utopia point in light of the fact that the extreme events are of greater importance compared with low flow rates a higher weight 0 4 is given to the nse that is more sensitive to larger errors that often occur in high flow periods the rest of the objective functions are equally weighted in general the distance to the utopia point is shorter when rounded archiving is used for swat except for pa dds that performed better with ε archiving with the budget of 1000 solution evaluations this means that the pareto approximate front is bent more towards the ideal point in the case of rounded archiving according to table 4 the point based pa dds archives more than 60 of generated solutions in the case of the seven objective raven model calibration for 10000 function evaluations while the rounded and ε based pa dds respectively archive only about 8 and 7 percents of all generated solutions in the same number of simulations in the 1000 evaluation experiment on pa dds the rounded and ε based techniques store only 207 and 213 solutions in the archive significantly less than 680 solutions that are archived using the original pa dds this means the traditional archiving method retains all the relatively similar solutions that have even slightly better value in only one out of seven objectives that is challenging for the calculation of one final preferred solution fig 8 shows that despite the significant decrease in the number of archived solutions both rounded and ε archiving approaches are preferred over the point based archiving version of the borg moea for 10000 simulations with a p value of 0 0211 according to table 5 rounded archiving is preferred for both pa dds and borg moea for calibrating the seven objective raven model because it helps the mo algorithms find the pareto approximate front with a knee point closer to the ideal point the only exception is the point based archiving that is preferred for borg moea at the limited budget of 1000 solution evaluations 3 2 method validation amalgam the rounded archiving is tested on amalgam for solving the many objective hydrologic model calibrations amalgam uses the point based archiving technique and has a bounded archive size that is defined a priori therefore it is expected to suffer from both deterioration and dominance resistance issues when applied to many objective optimization problems according to fig 9 rounded archiving improves the performance of amalgam for calibrating both of the five objective swat and seven objective raven models with the budget of 10000 solution evaluations this improvement is statistically meaningful with 95 confidence level based on the wilcoxon rank sum test results in table 6 except for the case of raven based on the indicator at the lower budget of 1000 solution evaluations the preference of rounded archiving over the point based archiving for amalgam is statistically significant in case of the raven model calibration but not for the swat model calibration see figure a and table 6 it is understood that rounded archiving decreased the oscillating issue due to the bounded archive of amalgam the bounded archive makes the mo algorithm eliminate some currently non dominated parameter sets and retain inferior ones in future iterations confirming the published study of hanne 1999 in addition the archive is rapidly filled with the initially generated low quality non dominated solutions and the subsequent solution production are based on the perturbation of the low quality archived ones while the rounded archiving method gradually stores the limited archive set by producing higher quality solutions during the optimization and it properly represents the entire range of the reference set if the precision level of each objective function is properly quantified a further experiment on the impact of the rounding level of the five objective r2 dtlz2 mathematical test problem in fig 10 demonstrate that consideration of the low resolution desired precision level results in a significant improvement on the ε value a similar result to fig 10 is expected for the computationally intensive problems such as the model calibration problems of this study however the modeller should be aware that a high rounding level increases the chance of multi modality in optimization 4 conclusions a comparative study of an alternative rounded archiving method to ε archiving was conducted the proposed archiving method of this study can carry out the task of ε archiving for algorithms that are not equiped with ε archiving the rounded archiving is user friendly in that it does not require altering the mo algorithm computer code this method is not algorithm or case specific and is much needed in situations where 1 the algorithm is not designed for solving many objective case studies and 2 the algorithm has a bounded archive structure the rounded based pareto approximate front is expected to properly represent the entire range of the true pareto front if the objectives resolutions are sufficiently quantified similar to ε archiving in reed et al 2007 at the methodology development stage of this study the rounded archiving resulted in significantly smaller archive size especially in high computational budgets for the five objective swat and seven objective raven model calibrations using pa dds and borg moea while maintaining a well diverse set of solutions in the objective space the rounded archiving has at least the same convergence level as ε archiving with no significant difference in the pareto front diversity for solving mathematical and model calibration problems using pa dds and borg moea the calculated knee points resulted from the proposed method have shorter distance to the ideal or utopia point than those obtained by the point based and ε archiving in the majority of numerical experiments it is concluded that the mo optimization algorithms need to be equipped with more specialized archiving strategies such as the ε or the rounded archiving to solve many objective optimizaton problems since the traditional point based archiving method leads to exponential increase in the number of non dominated solutions especially when mo algorithms have an unbounded archive the proposed rounded archiving method is tested on amalgam that has a bounded archive and is not equipped with ε archiving rounded archiving reamarkably improves the level of proximity and diversity of the pareto approximate front compared to the original amalgam results show that rounded archiving made amalgam perform better than the original amalgam in eight out of 12 hypothesis testings the rounded archiving can be easily generalized to other multi objective algorithms for solving many objective optimization problems reducing the number of archived solutions using either rounded or ε archiving assists decision makers and model analysts to find their desired solution or parameter set based on the status quo of their projects with less confusion it should be noted that the resolution rounding level of each objective needs to be consulted with decision makers prior to optimization setting it is shown in this study that using a coarse rounding level leads to a smaller number of archived solutions which confirms findings by reed et al 2007 the main advantage of rounded archiving over ε archiving is that it is readily available for any mo algorithm that is not equipped with any specialized archiving such as pa dds and amalgam in this study moreover rounded archiving is computationally more efficient but this advantage becomes trivial when solving problems with a time consuming simulation and or evaluation models such as the model calibration case studies of this paper ε archiving approach retains the exact values of objective vectors while rounded archiving will increase the probability of multi modality if a very low resolution is set for objectives rounded archiving needs to be applied to other mo algorithms and other water resources case studies but owing to the highly time demanding process of this comparison study three mo algorithms are considered as sufficient for generalizing the concluding remarks of this study acknowledgment we acknowledge the support of the natural sciences and engineering research council of canada nserc discovery grant of the second author rgpin 2016 05896 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104535 appendix fig a empirical cdf plots comparing point based green series versus rounded red series archiving for amalgam for calibrating five objective swat and seven objective raven models with 1000 solution evaluations fig a 
26114,in real system simulations the application of cellular automata has been shown as an interesting option because it can represent an emergent behavior and its implementation is simple this paper presents a method for simulating thermodynamic systems such as cloud dynamics with cellular automata in accordance with thermodynamic principles this paper presents an isolated system model that describes temperature dynamics the model uses the von neumann neighborhood of five cells each with two possible states the presence or absence of a cloud or a part of it our model uses three weather properties as follows condensed cloud water particles temperature and outer winds four types of experiments were performed to validate the model proposed i a warm body in the environment ii a cloud dynamic simulation iii a stability analysis and iv a comparison between the dynamic of the proposed model and satellite images of clouds keywords cellular automata thermodynamic systems cloud dynamics dynamical systems simulation 1 introduction simulations are studies of real systems using models that can describe and understand their behaviors and or to evaluate strategies for their operations pedgen et al 1990 simulations can be performed in several areas such as economics biology medicine social sciences and computation there are some situations in which a simulation may be applied for instance when the plant of the process does not exist when it is necessary to understand and analyze a real systems when tests in a real system is not possible when a real system can not be disturbed or for training and education baik 2005 several advantages can be obtained with a simulation in cost and time because it can be repeated at lower cost or can represent a long time period in seconds in order to simulate a real system it is necessary to choose a simulation method fishman 1996 gould and tobochnik 1988 rennard 2002 m et al 2010 f et al 2010 cellular automata cas rennard 2002 are discrete models on which many areas such as computation mathematics physic complexity science and biology are conducting research cas consist of a grid of cells each with a finite number of states for each cell a set of cells called neighborhood is defined for the specified cell at each iteration a new state of each cell arises in accordance with the current state of the cell the states of the cells in its neighborhood and some fixed rules typically the rule to update the state of a cell is the same for all cells and does not change over time and is applied to the whole grid simultaneously in real system simulations the application of cellular automata has been shown as an interesting option because it can represent an emergent behavior and its implementation is simple cas consist of a n dimensional grid of cells with the same behaviors described by a set of transition rules wolfram 2002 cas use a defined number of neighbor cells that interact with each other creating a local interaction and then a global behavior these interactions reflect the system dynamics based on the transitional rules this paper presents a method for simulating thermodynamic systems such as cloud dynamics with cellular automata a thermodynamic system is concerned with the flow and balance of energy three types of thermodynamic systems are distinguished depending on the types of interaction and energy exchange taking place between the system and its surrounding environment i an isolated system is isolated in every way from its environment and it does not exchange heat work or matter with its environment ii a closed system can exchange energy heat and work but not matter with its environment and iii an open system exchanges energy and matter with its environment a boundary that allows the exchange of matter is said to be permeable in isolated systems it is observed that as time passes internal rearrangements decrease and stable conditions are reached properties such as pressures and temperatures tend to equalize and matter arranges itself into one or a few homogeneous phases a system in which all processes of change have ended is considered to be in a state of thermodynamic equilibrium clouds are formed from the condensation of water vapors present in the atmosphere after it is formed a cloud is moved by winds and changes both its location and its properties such as temperature pressure density and humidity these properties strongly influence cloud dynamics in accordance with thermodynamic principles this paper presents an isolated system model that describes cloud dynamics the model proposed uses the von neumann neighborhood of five cells each with two possible states the presence or absence of a cloud or a part of it our model uses three weather properties as follows condensed cloud water particles temperature and outer winds the transition rules are based on thermodynamic principles and weather concepts three types of experiments were performed one with a warm body in the environment other to analyze the system stability and a cloud dynamics analysis the latter experiments compare the evolution of the proposed model with satellite images of clouds the rest of this paper is organized as follows section 2 presents important related works sections 3 and 4 introduce general concepts of cloud dynamics and cellular automata respectively section 5 describes the proposed model of cloud dynamics model with cellular automata section 6 presents several experiments related to dynamics and stability analyses section 7 concludes the paper and suggests new implementations for future studies 2 related works considering clouds and their simulation there are various approaches that can be classified into two main groups physically based and non physically based on heuristics simulations the physically based approach is closely connected to the simulation of fluid dynamics a set of system of partial differential equations is required the simulations based on heuristics use theoretical non mathematical description to create a role based systems some important works which used these two kind of approaches are presented as follow steiner 1973 developed a three dimensional convection model a set of partial differential atmospheric equations for motion conservation thermodynamic and energy were solved using a staggered finite difference technique his model consider phase transition but no precipitation cloud because of the implicit nature of the set of equations for the buoyancy pressure and saturation mixing ratio in deep moist convection he used a shallow convection a three dimensional model was developed by kajiya and herzen 1984 to demonstrate a visual cloud simulation their focus was in rendering cloud using ray tracing algorithm in order to generate cloud they used a model that incorporate the equation of motion continuity condensation and evaporation a set of partial differential equations known as navier stokes equations define the model and these were solve numerically using incompressible flow when it is assumed that the density of a fluid always remains constant over time in this case fluid is the atmospheric air however this model does not include adiabatic cooling and the temperature lapse in simulation space which is important for cumulus dynamics also does not account for variation in pressure within the atmosphere and accounts for viscous effects which are negligible for a standard atmospheric applications thus the results is not so realistic overby et al 2002 presented a similar work but slightly more detailed they combine the fluid solver with a model of the natural processes of cloud formation including buoyancy relative humidity and condensation to solve the navier stokes equations they used the stable fluid simulation algorithm of stam 1999 the authors modified slightly processes to make the fluid model account for the variation in atmospheric pressure in the vertical dimension they make two low level modifications to their solver the velocity field for the expansion of rising air and the conservation of momentum were modified although these effects tend to be minor velocities in our typical simulation are modified by less than 5 the effects are not negligible over the scale of the simulation furthermore their model simulate more than one type of cloud miyazaki et al 2002 proposed a simulation technique based a numerical solution of the partial differential equation of the atmospheric fluid model their model includes the phase transition and adiabatic cooling the used the interaction of the vapor the cloud the temperature and the velocity field in order to solve the partial differential equations they used the boussinesq approximation then assume that the air density is constant so the atmospheric fluid is incompressible to render the cloud the model generate metaballs at the center of voxels the images cloud are generate using the hardware accelerated rendering method proposed by dobashi et al 2000 their results showed that this model can simulate more realistic cumuliform clouds cumulus and cumulonimbus and clouds can be rendered that take the light scattering due to cloud particles into account the model can simulate only cumuliform clouds harris 2003 developed a similar work to the work by kajiya and herzen and overby et al he developed a cloud dynamics simulation based on partial differential equations that model fluid flow thermodynamics and water condensation and evaporation buoyant forces and water phase transitions as well as various forces and other factors that influence these equations but he also accounts for the negative buoyancy effects of condensed water mass and the positive buoyancy effects of water vapor other difference is that his model uses an exponential relationship between saturation and temperature to solve the equations he implemented the discrete form of these equations using programmable floating point graphics hardware all computation and rendering is performed on the gpu the cpu provides only high level control the speed and parallelism of graphics hardware enables simulation of cloud dynamics in real time his cloud model assumes that clouds exist alone cannot represent clouds that are composed of tiny ice crystals or a mix of water and ice the most important limitation of his cloud simulation system is the scale and detail that it can support it is not currently possible to simulate a sky full of clouds the domain size of most of the simulations that i have run is about 3 5 km at each dimension these kind of approaches commonly needs a large amount of computation time because the equations produce a large of calculations and consequently a lot of data to process in order to reduce the computational demands other researchers have tried to simplify the cloud model using heuristic approach nagel and raschke 1992 used a cellular automaton role based model in order to simulate cloud dynamic in the proposed model the authors used a theoretical non mathematical description to develop a cellular automaton starting from the phenomenology of cloud formation the authors used three logical variables humidity upward motion and cloud water content the state of these variables at the k 1 instant is completely determined by the states of their neighbor cells at the k instant nagel and rashke model uses only a small part of cloud physics process nevertheless the results showed important features of the fractal behavior of clouds and may be used as a starting point whenever a model of a cloud is needed for instance for problems of fractals analysis of satellite pictures rodriguez et al 2012 neyret 1997 applied some rule base using a set of heuristics to simulate the dynamic effects of convective clouds his method was fast to simulate but the simulation effects were not realistic compared with physical method dobashi et al 2000 extend four points to nagel and raschke model extinction of clouds wind effects speeding up of the simulation and controlling cloud motion and they added a stochastic rule to evaporation and re creation of clouds improving the model only cumulus clouds are simulated and rendered these are more detailed and visually convincing miyazaki et al 2002 used a simplified numerical model called couple map lattice cml yanagita and kaneko 1997 to simulate cloud formation miyazaki et al using rule based on atmospheric fluid dynamics considering an incompressible flow to solve navier stokes equation to create rules they have to be taken into account viscosity and pressure effects the advection of the state values by the fluid flow diffusion of water vapor thermal diffusion thermal buoyancy the phase transition from vapor to water their model could simulate various types of cloud such as cumulus cumulonimbus stratocumulus altocumulus and cirrocumulus also it obtained efficiently a realistic images of clouds by making use of graphics hardware guo et al 2012 proposed a real time dynamic cloud simulation approach based on multi core and multi thread differently from the works of harris 2003 kajiya and herzen 1984 overby et al 2002 dobashi et al 2000 and miyazaki et al 2001 that focus on taking advantage of gpu computation and rendering ability guo et al 2012 propose a cloud simulation method that both take advantage multi core cpu and modern gpu s parallel ability to achieve better computational performance and realistic dynamic cloud simulation they used the dobashi et al 2000 idea in dynamic cloud modeling but they introduced horizontal single direction wind function to simulate cloud movement to simply computation and enhance real time performance in order to render the clouds they used harris method they adopted multi thread to implement cloud modeling and illumination modeling to run on cpu while the illuminate color of clouds is rendered on gpu the results showed a better performance than other similar work and the better scalable ability on different multi core platform qiu et al 2013 proposed an effective method for simulating 3d cloud they used cml model proposed by miyazaki et al 2001 to simulate cloud generate they presented a new approach to simulate light scattering in clouds based on spherical harmonics and spherical harmonic coefficients that represent incident light distribution in order to improve rendering a frequency domain volume rendering algorithm combine with spherical harmonics was implemented their method was divided into two phases off line and real time to generate cloud data pre compute incident light and transform volume data to the representation of frequency domain is solved off line the work of real time rendering processing is to carry out frequency domain volume rendering the calculation process is divided in two phases cpu processing and gpu processing on cpu is generate a cloud data that is translated into 2d texture slices in the follow calculation process the spherical harmonic coefficients are updated according to iterative calculation results on gpu the rendering process is executed the spherical harmonic coefficients are calculated according to information received from cpu the results showed that their method facilitates computing efficiency while maintained a realistic visual quality this method not considering flying through or inside the clouds then is not very suitable for animation of clouds 3 cloud dynamics clouds are formed from the condensation of water existing in the humid air in the atmosphere the elevation of air is the key process in the production of clouds because when it rises and comes into contact with low temperatures cold air makes it possible for clouds to form this elevation can be produced by convection convergence of air streams topographical elevation or frontal lifting vianello 2000 clouds may be in a liquid or solid state or may be a mixed composition of water and ice the composition of a cloud depends on its altitude after having formed clouds are moved by winds in all directions when a cloud is moved in a vertical direction its altitude changes as do its properties such as temperature pressure kinetic energy density and humidity on rising there is a cooling of condensed cloud water particles that may become partially or completely frozen on the other hand when a cloud goes to a lower altitude it goes to a higher temperature environment therefore precipitation may arise and spread the cloud the dynamics growth motion and dissipation of clouds are complex thus it is important to understand these dynamics in order to allow an efficient implementation of the real system andrews 2000 three type of processes can be accounted for a cloud model as follows dynamic thermodynamic and cloud physical the equations of the cloud dynamics are governed by newton laws of motions and continuity equations applied to the air an equation for temperature and conservation equations for water the basic elements necessary to simulate clouds are air velocity u u x u y u z atmosphere pressure p temperature t water vapor w and condensed cloud water μ these water vapor and condensed cloud water variables are mixed ratios i e the mass of vapor or liquid water per unit mass of air in order to simulate the cloud dynamics in two dimension of a layer of the atmosphere as a function of the environment temperature it is required four partial differential equations rogers and yau 1988 one of them for horizontal air speed 1 u x t u x u x x u y u x y 1 ρ 0 p x f u x one for vertical air speed 2 u y t u x u y x u y u y y 1 ρ 0 p y f u y one for continuity equation 3 0 x ρ 0 u x y ρ 0 u y one for temperature 4 t t u x t x u y t y f t φ t one for water vapor 5 w t u x w x u y w y f w φ w and one for cloud water 6 μ t u x μ x u y μ y f μ φ μ the f i and φ j variables are buoyant force and entropy on i u x u y t w μ and j t w μ variables respectively in the continuity equation 3 the local rate of change of density has set to zero this is called the anelastic assumption which assumes that ρ 0 is everywhere constant except when buoyancy is calculated hence the continuity equation is eliminated 4 cellular automata a cellular automaton is formally defined as a discrete mathematical model implemented in computers automated by deterministic rules and its conduct of an element within a homogeneous set will be based both on the state of its own attributes and those of the neighboring elements wolfram 2002 a ca is characterized by its cell space and its transition rule the cell space is a lattice of n identical cells arranged in a d dimensional grid each with an identical pattern of local connections to other cells when we consider the lattice is of finite length boundary conditions are applied resulting in a circular lattice a transition rule provides the next state for each cell as a function of the configuration of its current neighborhood at each step of time every cell of the lattice updates its states according to this rule as to the ca dimensional rule contained in each cell it is essentially a finite state machine usually specified in the form of a table of rules these are called elementary cellular automata the neighbors of a cell are adjacent cells or cells on the right and left thus a cell is connected to air local neighbors cells where r is related to the radius so that each cell has a neighborhood of 2 r 1 a neighborhood is made up of three cells so there are 23 8 possible patterns for a neighborhood there are therefore 28 256 possible rules wolfram 2002 proposed a numbering scheme for the elementary cas in which the output bits are ordered alphabetically as in the transition rule and are read from right to left to form a base number in decimal notation between 0 and 255 for cas dimensional cells are arranged in a two dimensional space represented in the form of a grid the neighborhood most widely used are the von neumann neighborhood consisting of 5 cells i e a central cell and 4 neighbors up down left and right and the moore neighborhood consisting of 9 cells i e the von neumann neighborhood of more cells in the diagonal cellular automata are used in simulation and emulation of real systems rennard 2002 such as simulation of bacterial or viral behavior crystal growth coral rocks and other natural elements behavior of gases spread of fires population development economic behavior of land rivers and topographies and forecast of plant growth video generating random pictures image filters and distortions music melody generating digital noise and sound mathematics alternative to replacement differential equations by difference equations computer random number generation cryptography and conceptual design of parallel computations mass and 3d animation particle simulation and generating textures 5 cloud dynamics with cellular automaton in our model the interactions between cloud dynamics and microphysics is through two dimension way the cloud water is not present as described by equation 5 but from the condensation of water vapors present in the atmosphere thus a cloud appears when the temperature decreases and water vapor condenses to form cloud water the limit used to condense water vapors is the dew point temperature t d this is the threshold in which the amount of water vapor in the atmosphere is currently at its maximum concentration vianello 2000 the proposed model simulates clouds into a cellular automaton grid the cloud has an initial size that may be modified by weather events such as temperature and wind the latter is represented into the grid by a vectorial field a cell which represents a micro region in the atmosphere has three attributes the temperature the vertical and horizontal components of the wind vector the following sections explain these parameters and the cloud dynamics the model uses the von neuman neighborhood with five cells as shown in fig 1 every cell has a x y coordinate i e its position in the simulated area thus x l x r y a and y b are coordinates of the i th cell neighbors and δ x and δ y are the lengths between two horizontal and vertical neighbors respectively the δ x and δ y values depend on the number of cells used into the grid for an area the higher is the number of cells into a grid the lower δ x and δ y are the next subsections describe the dynamic of the proposed model 5 1 dew point temperature the dew point is defined as the temperature in which the air must be cooled to the condensation of water i e the air becomes saturated with water vapor at the temperature of the dew point the amount of water vapor in the air is currently at its maximum concentration the dew point temperature t d can be calculated as proposed by vianello 2000 7 t d 186 4905 237 3 log e log e 8 2859 where 8 e e s a p t t u is the real vapor pressure e s is the water vapor saturation pressure a is the psychometrics constant p is local atmospheric pressure and t t u is the psychometrics depression in our model a cloud appears into the grid of the cellular automaton when the atmospheric temperature belongs to t d δ t d δ interval where δ is a constant 5 2 wind the wind flow is described by a vectorial field where each vector is implemented into a cell and described by horizontal u x and vertical u y components fig 2 shows a 5 5 cellular automata grid covered by a wind flow with a general direction from the south west to the northeast a general wind flow direction is established but each vector component has a small disturbance to guarantee randomness as can be seen in some vectors which have a bit different direction from the general one the dynamics of the wind flow is defined by equations 1 and 2 in discrete form the partial derivatives of the wind speed and pressure with respect to the variables x and y for the i th cell at a given iteration are approximated respectively by 9 u x x δ u x i δ x i 10 u y y δ u y i δ y i 11 p x δ p i δ x i and 12 p y δ p i δ y i where δ u x i u l u r δ p i p l p r and δ x i x l x r for equation 9 and δ u y i u a u b δ p i p a p b and δ y i y a y b for equation 10 thus the linearization of equations 1 and 2 by euler s method provides the following difference equations 13 u x i t δ t u x i k u x i δ u x i δ x i u y i δ u x i δ y i 1 ρ 0 δ p i δ x i f u x i δ t for the horizontal wind speed and 14 u y i t δ t u y i k u x i δ u y i δ x i u y i δ u y i δ y i 1 ρ 0 δ p i δ y i f u y i δ t for the vertical wind speed 5 3 temperature the i th cell temperature t i t defines the value of this variable into this micro region of the space the value of t i t defines the appearance of cloud into the cell each cell has a cloud or part of it if t i t belongs to t d δ t d δ interval otherwise there is no cloud into the cell the cell temperature behavior is derived from the newton s law of cooling in which heat transfer between the i th cell and its neighborhood is provided by 15 d t i d t q t i t n where t n is the neighborhood cell temperature and q is a positive constant since the i th cell has four neighborhood the model approximates d t i d t as an average of equation 15 for all neighborhood as follows 16 d t i d t q 1 n o 1 t i k t r t i k t l t i t t a t i t t b q n o 1 4 t i t j t j t where j l r a b n o is the number of cells per neighborhood t i k is the temperature into i th cell at iteration k and the subscripts l r a b mean the neighbors on the left on the right above and below neighbors of an i th cell the cell temperature also interacts to the wind flow and its dynamics is approximated in discrete time for each cell according to equation 4 the partial derivatives of t with respect to x and y for the i th cell are approximated by 17 t x δ t i δ x i and 18 t y δ t i δ y i respectively where δ t i t l t r and δ x i x l x r for equation 17 and δ t i t a t b and δ y i y a y b for equation 18 thus the i th cell temperature is updated as follows 19 t i t δ t t i k u x i δ t i δ x i u y i δ t i δ y i f t φ t d t i d t δ t where d t i d t comes from equation 16 5 4 the model dynamics the model dynamics is based on a discrete and iterative system all grid cell transitions are based on equations 13 14 and 19 that change each cell state locally providing the global effect in the grid the pseudo code of our model is presented as follows 1 for a given altitude initialize ρ 0 t d and t i and p i i 2 initialize f i i u x u y t and φ t 3 introduce a vectorial field of wind into the grid 4 introduce a cloud into the grid 5 update cell temperatures by interaction to its neighborhoods and the wind flow according to equation 19 6 execute the wind flow dynamics according to equations 13 and 14 7 go back to step 5 while a stop criterion is not satisfied a microregion of the atmosphere will have a cloud if the i th cell temperature t i belongs to t d δ t d δ dew point interval 6 simulations and result analyses this section presents four different experiments to validate the proposed model the first experiment shows the performance of the model describing an isolated system the second one shows two experiments with a 100 100 grid in order to present the cloud dynamics the third experiment presents a stability analysis of the proposed model and the last experiment compares the evolution of the proposed model with satellite images of clouds 6 1 thermodynamic model with cellular automata this section presents an experiment to describe an isolated system with cellular automata in isolated systems as the time passes internal rearrangements decrease and stable conditions are reached properties e g pressures and temperatures tend to equalize such that the processes of change come to an end and the system reaches the state of thermodynamic equilibrium two simulations of isolated thermodynamic systems are presented in 30 30 and 50 50 grids the model was implemented with the visual studio 2010 using c language for these simulations the temperature interval was 0 50 degrees the initial temperature was 0 c and δ t 0 01 fig 3 c shows the temperature intervals and their respective colors fig 3 shows six iterations of each simulation in the simulation of the 30 30 grid fig 3 a a warm body of 50 c and 50 grid area was initialized into the center of the grid the heat of the warm body spread quickly see iteration 4 in iteration 27 the extreme area of the grid had been warmed by heat from the warm body to a temperature of between 1 and 10 c iteration 48 shows that the center of the warm body started to cool because its heat spread throughout the grid iterations 112 and 369 show that the heat continued to spread thus providing the temperature equalization iteration 472 shows the moment at which the grid temperature was totally equalized between 1 and 10 c in the simulation of the 50 50 grid fig 3 b a warm body of 50 c and 80 grid area was initialized into the center of the grid the system behavior was similar to that of the first simulation 30 30 grid the heat spread quickly throughout the whole grid after iteration 46 the heat of the center of the grid started to decrease iterations 112 and 134 show the beginning of the temperature equalization throughout the whole grid iteration 485 shows the grid with its temperature totally equalized between 20 and 30 c higher than that of the first simulation because of the larger area of the warm body 80 grid area these simulations showed that a cellular automaton model can simulate a thermodynamic system in both of them the heat in the center of grid spread throughout the whole grid until the thermodynamic equilibrium 6 2 cloud dynamics this subsection presents two simulations with a 100 100 grid the first one with the cloud in the center of the grid and other with the cloud in the top right of the grid the grid altitude was 5000 m the current atmospheric temperature was 3 o c the dew point t d 2 o c δ t 0 01 and a fixed atmospheric pressure of 700 hpa the cloud experiment ran 5000 iterations figs 4 a and 5 a show the initial state of the two simulations the cloud dynamics of our model showed an expected behavior regarding some thermodynamics concepts because in all experiments the whole grid reached thermal equilibrium resulting in total cloud dissipation to which the actions of wind also contributed as can be seen in figs 4 and 5 these figures show that the clouds were dispersed and moved due to the thermal equilibrium and wind interactions effects in figs 4 d and 5 d the two clouds are smaller than their initial positions figs 4 a and 5 a by the dissipation and far from it due to the wind interactions in a grid where a given temperature prevails over almost of its whole area the environmental temperature set to 3 c is expected at infinite time into an undisturbed environment such that it reaches thermal equilibrium close to the initial environmental temperature fig 6 shows both the mean and standard deviation sd of the atmospheric temperature in the two experiments fig 6 a shows the mean temperatures of two experiments which started from about 2 4 c and finished to about 3 c almost the initial atmospheric temperature i e both of them tend to thermal equilibrium fig 6 b presents sd of the two temperatures which passed from 1 5 to about zero i e in the two experiments the grid tended to a same temperature close to 3 c these effects resulting from physical phenomena were more prevalent than those of the simulations with smaller grids these results may be expected because of these smaller areas thermodynamic equilibrium tends to be reached faster than in those of the larger grids in addition to which the probability of a wind reaching the cloud is greater 6 3 stability analysis this section presents the computational experiments on stability analysis of the proposed model the stability is defined with respect to the fixed point of the system when the system reaches a fixed point t its state does not change as the time goes to infinity i e t t t as t in this work experiments the stability was analyzed empirically that is we assumed that the system is stable whenever the mean temperature of the matrix does not change until the end of the simulation 20 000 iterations and its value t t is close to the initial atmospheric temperature thus we assume that the system reached the thermal equilibrium some parameters of the system were fixed in all experiments and their values were defined as follow q 0 0243 w m 1 k 1 cell area was 0 01 m2 altitude was 5 km atmosphere temperature was 3 c t d 8 4 c atmospheric pressure was 700 hpa and wind speed was 60 km h in order to analyze the system stability several matrix dimensions and time step size δ t were used totaling 16 experiments the following matrix dimensions 100 100 2000 2000 and 5000 5000 and δ t 1 s 0 1 s 1 ms 0 1 ms were used in discrete models δ t determines the system precision and stability and the real period of simulation a large δ t provides a long period of simulation nevertheless large δ t increases the deviation between the real and simulated system output moreover the system may become unstable on the other side the precision of the system increases as δ t decreases however the period of simulation also decreases becoming the simulation not feasible for real time in this stability analysis we assessed the largest δ t in which the system remain stable thus the larger a stable δ t the longer the period of simulation in order to use an as precise as possible δ t a deviation analysis between outputs from different simulations was performed fig 7 presents the mean temperatures in the 100 100 matrix for different δ t in fig 7 a it is observed an unstable system since t m increases linearly to 1 8 10 24 at iteration 10 several iterations after that t m overflow thus it is expected that t m as t in fig 7 b t m keeps constant around the initial atmospheric temperature until the first 40 iterations after that t m decreases linearly to thus it is expected that t m as t in fig 7 c and d we see a stable system that reaches the thermal equilibrium with final t m close to the initial atmospheric temperature in fig 7 c the mean temperature reaches the atmospheric temperature at 600 iterations in fig 7 d t m stabilized at 5000 iterations fig 8 presents the mean temperatures of the 2000 2000 matrix for different δ t in fig 8 a and b the system had similar unstable behavior of those presented in fig 7 a and b fig 8 a shows that t m performed an inverse behavior with respect to that presented in fig 7 a i e in the former t m decreases linearly to overflow instead of to increase to in the latter similarly in fig 8 b t m has an inverse behavior with respect to that in fig 7 b in fig 8 b the temperature kept constant until 40 iterations after that t m increased linearly to overflow in fig 8 c t m stabilized with 8200 iterations and kept constant until the end of the experiment thus it is possible to assume that the system reaches the thermal equilibrium with δ t 1 ms in fig 8 d it is not possible to reach a conclusion nevertheless likely the system will stabilize since in 20 000 iterations t m decreased a little due to the disturbance caused by the cloud colder than the initial atmospheric temperature by the reason of the matrix size 20 000 iterations were not enough to stabilize the system fig 9 shows the mean temperatures of the 5000 5000 matrix for several δ t fig 9 a shows the experiment where the behavior of the system was unstable with t m higher than 3 1021 in only 10 iterations such as in the last experiments we assume that t m as t in the experiment shown in fig 9 b the system had similar behavior to that in 100 100 matrix with δ t 0 1 s fig 7 b in fig 9 c t m stabilized about 19 000 iterations thus the system stabilized at a slow rate likely due to the matrix dimension in the experiment shown in fig 9 d the system had a similar behavior to that presented in fig 8 d such as in that experiment we assume that the matrix dimension and low δ t did not permitted that 20 000 iterations were enough to stabilize the system in fig 7 δ t equals 1 ms and 0 1 ms provided a stable behavior in figs 8 and 9 only δ t 1 ms provided a stable behavior in all experiments δ t equals 1 s and 0 1 s provided an unstable behavior then these two value can be considered not suitable for the proposed model the system behaved stable in all experiments with δ t 1 ms until 20 000 iterations a deviation analysis of the mean temperatures with δ t equals to 1 ms and 0 1 ms was performed with 100 100 matrix in order to check the evolution of the proposed model as shown in fig 10 until iteration 100 the values of the mean temperatures were similar after that there was a deviation equals to 0 002 until iteration 280 then the deviation increased to 0 0063 until iteration 580 finally the deviation decreased to zero until the end of the simulation this error can be considered acceptable with respect to the magnitude of the temperature table 1 shows the means and standard deviations of the temperatures with δ t 1 s for all matrix dimensions the initial mean temperatures decreased as the matrix dimensions increased due to the cloud area with respect to the matrix dimension i e the larger is the matrix dimension the smaller is the disturb caused by the cloud in the environment the initial standard deviations also show this relation for instance in 100 100 matrix the rate cloud area per matrix area is larger than that in 5000 5000 matrix thus in the former the cloud provides more disturbance it can be seen that the final mean temperatures and standard deviations provided nan not a number value i e these variables went to these results show that the system did not reach the thermal equilibrium in accordance with the analysis on figs 7 to 9 table 2 shows the means and standard deviations of the temperatures with δ t 0 1 s in which they had similar behavior to those with δ t 1 s table 1 table 2 also shows an unstable system where the final t m and standard deviations provided nan i e they tended to table 3 shows the means and standard deviations of the temperatures with δ t 1 ms in all experiments t m and standard deviations tended to the initial atmospheric temperature and zero respectively these data support our analysis based on figs 7 c 8 c and 9 c finally table 4 shows the means and standard deviations of the temperatures with δ t 0 1 ms the final mean temperature and standard deviation in 100 100 matrix tended to the initial atmospheric temperature and zero respectively in this experiment we can assume that the system reached the thermal equilibrium in experiments with 2000 2000 and 5000 5000 matrices the mean temperatures did not change significantly along the simulations in these experiments the standard deviations showed a small decrease along the time there are two reasons for that the very small δ t which evolves a small period of time during the simulation and the matrix dimension which needs many iterations to propagate the temperature disturbance the number of time step size δ t used in the experiments provided the following conclusions i values of δ t equal or larger than 0 1 s likely provide an unstable system such as those used in our experiments δ t 0 1 s 1 s and ii values of δ t smaller than 0 1 ms will provide a small evolution of the system for a long time of processing simulations with small δ t will not produce significant variations on the state of the system especially in large matrix such as those used in the experiments with δ t 0 1 ms for 2000 2000 and 5000 5000 matrices the simulations showed that δ t 1 ms provides an useful step of time for the proposed model this δ t supplies a stable system with lead the area of simulation to the thermal equilibrium 6 4 satellite images this section presents a real case where the performance of the proposed model is compared with the evolution of a clouds from satellite images the images came from inpe instituto nacional de pesquisas espaciais brazil the tests used images from 6 p m to 8 p m fig 11 shows the original images used for the experiment fig 13 shows the cloud images with their respective temperatures in which the scale is given in fig 12 fig 14 shows two images at 7 p m the result of simulating the proposed model fig 14 a and the satellite image at 7 p m fig 14 b fig 14 shows four clouds fig 14 a shows the simulation of the proposed model from 6 p m fig 13 a to 7 p m and fig 14 b presents the satellite image at 7 p m these four clouds were named top t bottom b center c and right r all clouds in fig 14 a tend to homogeneity and disappear because of heat exchange with the atmosphere since the proposed model uses only one layer of the atmosphere thus convection does not act on the clouds moreover the simulated area is isolated in fig 14 a cloud t shows a similar shape to its equivalent one in fig 14 b the satellite image nevertheless in fig 14 b a cold blue temperature cloud appears on the top different to its equivalent in fig 14 b this occurred likely because of the convection between the atmosphere layer where the simulation takes place and the above or below one cloud c in fig 14 a begins to dissolve due to heat exchange cloud b in fig 14 a is seen to have a similar shape to its equivalent one in fig 14 b except that it is less displaced by the wind finally cloud r spreads in the atmosphere more than its equivalent in fig 14 b however their positions and shapes are similar 7 conclusion this article set out to construct a model to simulate thermodynamic systems using cellular automata two types of models were presented an isolated thermodynamic system and a cloud dynamics model the former showed that a cellular automaton can simulate a thermodynamic system this first model was the basis for the second one the cloud dynamics model the second model used a limited representation considering the variables that represent the dynamics of a real cloud the atmospheric temperature and wind flow in the two dimensional grid were included a two dimensional ca with a von neumann neighborhood of 5 cells was used the transition rules were defined based on the thermodynamic principle that defines the thermal equilibrium and the wind interaction the validation of the second model was made a simulation with a 100 100 cellular automaton grid in this preliminary study we did not compare our model with other ones for cloud dynamics thus we only conducted visual validation and graphical analyses considering basic thermodynamic principles the simulations showed that our proposed model presented a satisfactory behavior considering some thermodynamic principles the wind actions also were considered coherent because they moved the clouds until their full dissipation in the experiment the clouds with heterogeneous temperature randomly initialized tended to converge to a uniform temperature reaching the thermal equilibrium another observed behavior was the thermal equilibrium between the cloud and grid which always resulted in cloud dissipation the wind actions also contributed to convergence of cloud temperature to environmental temperature because they spread the clouds accelerating the thermal equilibrium moreover the proposed model was compared with realist data using satellite images the results showed that the dynamic of the proposed model was similar to that of the real satellite images as a follow up to this study other variables will be added such as pressure kinetic energy density and humidity thus making the model more reliable another proposal is to simulate a three dimensional space approximating the model of a real system the proposed model also may be implemented using the parallel computing paradigm improving its performance and consequently its ability to perform in real time the latter proposal is justified by the increase of variables involved which feature a real atmospheric system thus parallel computing may increase the model performance in a more complex scenario acknowledgments the authors gratefully acknowledge the coordination for the improvement of higher education personnel capes and pontifical catholic university of minas gerais puc minas which supported this work and the assistance provided by the center of climatology of puc minas appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104537 
26114,in real system simulations the application of cellular automata has been shown as an interesting option because it can represent an emergent behavior and its implementation is simple this paper presents a method for simulating thermodynamic systems such as cloud dynamics with cellular automata in accordance with thermodynamic principles this paper presents an isolated system model that describes temperature dynamics the model uses the von neumann neighborhood of five cells each with two possible states the presence or absence of a cloud or a part of it our model uses three weather properties as follows condensed cloud water particles temperature and outer winds four types of experiments were performed to validate the model proposed i a warm body in the environment ii a cloud dynamic simulation iii a stability analysis and iv a comparison between the dynamic of the proposed model and satellite images of clouds keywords cellular automata thermodynamic systems cloud dynamics dynamical systems simulation 1 introduction simulations are studies of real systems using models that can describe and understand their behaviors and or to evaluate strategies for their operations pedgen et al 1990 simulations can be performed in several areas such as economics biology medicine social sciences and computation there are some situations in which a simulation may be applied for instance when the plant of the process does not exist when it is necessary to understand and analyze a real systems when tests in a real system is not possible when a real system can not be disturbed or for training and education baik 2005 several advantages can be obtained with a simulation in cost and time because it can be repeated at lower cost or can represent a long time period in seconds in order to simulate a real system it is necessary to choose a simulation method fishman 1996 gould and tobochnik 1988 rennard 2002 m et al 2010 f et al 2010 cellular automata cas rennard 2002 are discrete models on which many areas such as computation mathematics physic complexity science and biology are conducting research cas consist of a grid of cells each with a finite number of states for each cell a set of cells called neighborhood is defined for the specified cell at each iteration a new state of each cell arises in accordance with the current state of the cell the states of the cells in its neighborhood and some fixed rules typically the rule to update the state of a cell is the same for all cells and does not change over time and is applied to the whole grid simultaneously in real system simulations the application of cellular automata has been shown as an interesting option because it can represent an emergent behavior and its implementation is simple cas consist of a n dimensional grid of cells with the same behaviors described by a set of transition rules wolfram 2002 cas use a defined number of neighbor cells that interact with each other creating a local interaction and then a global behavior these interactions reflect the system dynamics based on the transitional rules this paper presents a method for simulating thermodynamic systems such as cloud dynamics with cellular automata a thermodynamic system is concerned with the flow and balance of energy three types of thermodynamic systems are distinguished depending on the types of interaction and energy exchange taking place between the system and its surrounding environment i an isolated system is isolated in every way from its environment and it does not exchange heat work or matter with its environment ii a closed system can exchange energy heat and work but not matter with its environment and iii an open system exchanges energy and matter with its environment a boundary that allows the exchange of matter is said to be permeable in isolated systems it is observed that as time passes internal rearrangements decrease and stable conditions are reached properties such as pressures and temperatures tend to equalize and matter arranges itself into one or a few homogeneous phases a system in which all processes of change have ended is considered to be in a state of thermodynamic equilibrium clouds are formed from the condensation of water vapors present in the atmosphere after it is formed a cloud is moved by winds and changes both its location and its properties such as temperature pressure density and humidity these properties strongly influence cloud dynamics in accordance with thermodynamic principles this paper presents an isolated system model that describes cloud dynamics the model proposed uses the von neumann neighborhood of five cells each with two possible states the presence or absence of a cloud or a part of it our model uses three weather properties as follows condensed cloud water particles temperature and outer winds the transition rules are based on thermodynamic principles and weather concepts three types of experiments were performed one with a warm body in the environment other to analyze the system stability and a cloud dynamics analysis the latter experiments compare the evolution of the proposed model with satellite images of clouds the rest of this paper is organized as follows section 2 presents important related works sections 3 and 4 introduce general concepts of cloud dynamics and cellular automata respectively section 5 describes the proposed model of cloud dynamics model with cellular automata section 6 presents several experiments related to dynamics and stability analyses section 7 concludes the paper and suggests new implementations for future studies 2 related works considering clouds and their simulation there are various approaches that can be classified into two main groups physically based and non physically based on heuristics simulations the physically based approach is closely connected to the simulation of fluid dynamics a set of system of partial differential equations is required the simulations based on heuristics use theoretical non mathematical description to create a role based systems some important works which used these two kind of approaches are presented as follow steiner 1973 developed a three dimensional convection model a set of partial differential atmospheric equations for motion conservation thermodynamic and energy were solved using a staggered finite difference technique his model consider phase transition but no precipitation cloud because of the implicit nature of the set of equations for the buoyancy pressure and saturation mixing ratio in deep moist convection he used a shallow convection a three dimensional model was developed by kajiya and herzen 1984 to demonstrate a visual cloud simulation their focus was in rendering cloud using ray tracing algorithm in order to generate cloud they used a model that incorporate the equation of motion continuity condensation and evaporation a set of partial differential equations known as navier stokes equations define the model and these were solve numerically using incompressible flow when it is assumed that the density of a fluid always remains constant over time in this case fluid is the atmospheric air however this model does not include adiabatic cooling and the temperature lapse in simulation space which is important for cumulus dynamics also does not account for variation in pressure within the atmosphere and accounts for viscous effects which are negligible for a standard atmospheric applications thus the results is not so realistic overby et al 2002 presented a similar work but slightly more detailed they combine the fluid solver with a model of the natural processes of cloud formation including buoyancy relative humidity and condensation to solve the navier stokes equations they used the stable fluid simulation algorithm of stam 1999 the authors modified slightly processes to make the fluid model account for the variation in atmospheric pressure in the vertical dimension they make two low level modifications to their solver the velocity field for the expansion of rising air and the conservation of momentum were modified although these effects tend to be minor velocities in our typical simulation are modified by less than 5 the effects are not negligible over the scale of the simulation furthermore their model simulate more than one type of cloud miyazaki et al 2002 proposed a simulation technique based a numerical solution of the partial differential equation of the atmospheric fluid model their model includes the phase transition and adiabatic cooling the used the interaction of the vapor the cloud the temperature and the velocity field in order to solve the partial differential equations they used the boussinesq approximation then assume that the air density is constant so the atmospheric fluid is incompressible to render the cloud the model generate metaballs at the center of voxels the images cloud are generate using the hardware accelerated rendering method proposed by dobashi et al 2000 their results showed that this model can simulate more realistic cumuliform clouds cumulus and cumulonimbus and clouds can be rendered that take the light scattering due to cloud particles into account the model can simulate only cumuliform clouds harris 2003 developed a similar work to the work by kajiya and herzen and overby et al he developed a cloud dynamics simulation based on partial differential equations that model fluid flow thermodynamics and water condensation and evaporation buoyant forces and water phase transitions as well as various forces and other factors that influence these equations but he also accounts for the negative buoyancy effects of condensed water mass and the positive buoyancy effects of water vapor other difference is that his model uses an exponential relationship between saturation and temperature to solve the equations he implemented the discrete form of these equations using programmable floating point graphics hardware all computation and rendering is performed on the gpu the cpu provides only high level control the speed and parallelism of graphics hardware enables simulation of cloud dynamics in real time his cloud model assumes that clouds exist alone cannot represent clouds that are composed of tiny ice crystals or a mix of water and ice the most important limitation of his cloud simulation system is the scale and detail that it can support it is not currently possible to simulate a sky full of clouds the domain size of most of the simulations that i have run is about 3 5 km at each dimension these kind of approaches commonly needs a large amount of computation time because the equations produce a large of calculations and consequently a lot of data to process in order to reduce the computational demands other researchers have tried to simplify the cloud model using heuristic approach nagel and raschke 1992 used a cellular automaton role based model in order to simulate cloud dynamic in the proposed model the authors used a theoretical non mathematical description to develop a cellular automaton starting from the phenomenology of cloud formation the authors used three logical variables humidity upward motion and cloud water content the state of these variables at the k 1 instant is completely determined by the states of their neighbor cells at the k instant nagel and rashke model uses only a small part of cloud physics process nevertheless the results showed important features of the fractal behavior of clouds and may be used as a starting point whenever a model of a cloud is needed for instance for problems of fractals analysis of satellite pictures rodriguez et al 2012 neyret 1997 applied some rule base using a set of heuristics to simulate the dynamic effects of convective clouds his method was fast to simulate but the simulation effects were not realistic compared with physical method dobashi et al 2000 extend four points to nagel and raschke model extinction of clouds wind effects speeding up of the simulation and controlling cloud motion and they added a stochastic rule to evaporation and re creation of clouds improving the model only cumulus clouds are simulated and rendered these are more detailed and visually convincing miyazaki et al 2002 used a simplified numerical model called couple map lattice cml yanagita and kaneko 1997 to simulate cloud formation miyazaki et al using rule based on atmospheric fluid dynamics considering an incompressible flow to solve navier stokes equation to create rules they have to be taken into account viscosity and pressure effects the advection of the state values by the fluid flow diffusion of water vapor thermal diffusion thermal buoyancy the phase transition from vapor to water their model could simulate various types of cloud such as cumulus cumulonimbus stratocumulus altocumulus and cirrocumulus also it obtained efficiently a realistic images of clouds by making use of graphics hardware guo et al 2012 proposed a real time dynamic cloud simulation approach based on multi core and multi thread differently from the works of harris 2003 kajiya and herzen 1984 overby et al 2002 dobashi et al 2000 and miyazaki et al 2001 that focus on taking advantage of gpu computation and rendering ability guo et al 2012 propose a cloud simulation method that both take advantage multi core cpu and modern gpu s parallel ability to achieve better computational performance and realistic dynamic cloud simulation they used the dobashi et al 2000 idea in dynamic cloud modeling but they introduced horizontal single direction wind function to simulate cloud movement to simply computation and enhance real time performance in order to render the clouds they used harris method they adopted multi thread to implement cloud modeling and illumination modeling to run on cpu while the illuminate color of clouds is rendered on gpu the results showed a better performance than other similar work and the better scalable ability on different multi core platform qiu et al 2013 proposed an effective method for simulating 3d cloud they used cml model proposed by miyazaki et al 2001 to simulate cloud generate they presented a new approach to simulate light scattering in clouds based on spherical harmonics and spherical harmonic coefficients that represent incident light distribution in order to improve rendering a frequency domain volume rendering algorithm combine with spherical harmonics was implemented their method was divided into two phases off line and real time to generate cloud data pre compute incident light and transform volume data to the representation of frequency domain is solved off line the work of real time rendering processing is to carry out frequency domain volume rendering the calculation process is divided in two phases cpu processing and gpu processing on cpu is generate a cloud data that is translated into 2d texture slices in the follow calculation process the spherical harmonic coefficients are updated according to iterative calculation results on gpu the rendering process is executed the spherical harmonic coefficients are calculated according to information received from cpu the results showed that their method facilitates computing efficiency while maintained a realistic visual quality this method not considering flying through or inside the clouds then is not very suitable for animation of clouds 3 cloud dynamics clouds are formed from the condensation of water existing in the humid air in the atmosphere the elevation of air is the key process in the production of clouds because when it rises and comes into contact with low temperatures cold air makes it possible for clouds to form this elevation can be produced by convection convergence of air streams topographical elevation or frontal lifting vianello 2000 clouds may be in a liquid or solid state or may be a mixed composition of water and ice the composition of a cloud depends on its altitude after having formed clouds are moved by winds in all directions when a cloud is moved in a vertical direction its altitude changes as do its properties such as temperature pressure kinetic energy density and humidity on rising there is a cooling of condensed cloud water particles that may become partially or completely frozen on the other hand when a cloud goes to a lower altitude it goes to a higher temperature environment therefore precipitation may arise and spread the cloud the dynamics growth motion and dissipation of clouds are complex thus it is important to understand these dynamics in order to allow an efficient implementation of the real system andrews 2000 three type of processes can be accounted for a cloud model as follows dynamic thermodynamic and cloud physical the equations of the cloud dynamics are governed by newton laws of motions and continuity equations applied to the air an equation for temperature and conservation equations for water the basic elements necessary to simulate clouds are air velocity u u x u y u z atmosphere pressure p temperature t water vapor w and condensed cloud water μ these water vapor and condensed cloud water variables are mixed ratios i e the mass of vapor or liquid water per unit mass of air in order to simulate the cloud dynamics in two dimension of a layer of the atmosphere as a function of the environment temperature it is required four partial differential equations rogers and yau 1988 one of them for horizontal air speed 1 u x t u x u x x u y u x y 1 ρ 0 p x f u x one for vertical air speed 2 u y t u x u y x u y u y y 1 ρ 0 p y f u y one for continuity equation 3 0 x ρ 0 u x y ρ 0 u y one for temperature 4 t t u x t x u y t y f t φ t one for water vapor 5 w t u x w x u y w y f w φ w and one for cloud water 6 μ t u x μ x u y μ y f μ φ μ the f i and φ j variables are buoyant force and entropy on i u x u y t w μ and j t w μ variables respectively in the continuity equation 3 the local rate of change of density has set to zero this is called the anelastic assumption which assumes that ρ 0 is everywhere constant except when buoyancy is calculated hence the continuity equation is eliminated 4 cellular automata a cellular automaton is formally defined as a discrete mathematical model implemented in computers automated by deterministic rules and its conduct of an element within a homogeneous set will be based both on the state of its own attributes and those of the neighboring elements wolfram 2002 a ca is characterized by its cell space and its transition rule the cell space is a lattice of n identical cells arranged in a d dimensional grid each with an identical pattern of local connections to other cells when we consider the lattice is of finite length boundary conditions are applied resulting in a circular lattice a transition rule provides the next state for each cell as a function of the configuration of its current neighborhood at each step of time every cell of the lattice updates its states according to this rule as to the ca dimensional rule contained in each cell it is essentially a finite state machine usually specified in the form of a table of rules these are called elementary cellular automata the neighbors of a cell are adjacent cells or cells on the right and left thus a cell is connected to air local neighbors cells where r is related to the radius so that each cell has a neighborhood of 2 r 1 a neighborhood is made up of three cells so there are 23 8 possible patterns for a neighborhood there are therefore 28 256 possible rules wolfram 2002 proposed a numbering scheme for the elementary cas in which the output bits are ordered alphabetically as in the transition rule and are read from right to left to form a base number in decimal notation between 0 and 255 for cas dimensional cells are arranged in a two dimensional space represented in the form of a grid the neighborhood most widely used are the von neumann neighborhood consisting of 5 cells i e a central cell and 4 neighbors up down left and right and the moore neighborhood consisting of 9 cells i e the von neumann neighborhood of more cells in the diagonal cellular automata are used in simulation and emulation of real systems rennard 2002 such as simulation of bacterial or viral behavior crystal growth coral rocks and other natural elements behavior of gases spread of fires population development economic behavior of land rivers and topographies and forecast of plant growth video generating random pictures image filters and distortions music melody generating digital noise and sound mathematics alternative to replacement differential equations by difference equations computer random number generation cryptography and conceptual design of parallel computations mass and 3d animation particle simulation and generating textures 5 cloud dynamics with cellular automaton in our model the interactions between cloud dynamics and microphysics is through two dimension way the cloud water is not present as described by equation 5 but from the condensation of water vapors present in the atmosphere thus a cloud appears when the temperature decreases and water vapor condenses to form cloud water the limit used to condense water vapors is the dew point temperature t d this is the threshold in which the amount of water vapor in the atmosphere is currently at its maximum concentration vianello 2000 the proposed model simulates clouds into a cellular automaton grid the cloud has an initial size that may be modified by weather events such as temperature and wind the latter is represented into the grid by a vectorial field a cell which represents a micro region in the atmosphere has three attributes the temperature the vertical and horizontal components of the wind vector the following sections explain these parameters and the cloud dynamics the model uses the von neuman neighborhood with five cells as shown in fig 1 every cell has a x y coordinate i e its position in the simulated area thus x l x r y a and y b are coordinates of the i th cell neighbors and δ x and δ y are the lengths between two horizontal and vertical neighbors respectively the δ x and δ y values depend on the number of cells used into the grid for an area the higher is the number of cells into a grid the lower δ x and δ y are the next subsections describe the dynamic of the proposed model 5 1 dew point temperature the dew point is defined as the temperature in which the air must be cooled to the condensation of water i e the air becomes saturated with water vapor at the temperature of the dew point the amount of water vapor in the air is currently at its maximum concentration the dew point temperature t d can be calculated as proposed by vianello 2000 7 t d 186 4905 237 3 log e log e 8 2859 where 8 e e s a p t t u is the real vapor pressure e s is the water vapor saturation pressure a is the psychometrics constant p is local atmospheric pressure and t t u is the psychometrics depression in our model a cloud appears into the grid of the cellular automaton when the atmospheric temperature belongs to t d δ t d δ interval where δ is a constant 5 2 wind the wind flow is described by a vectorial field where each vector is implemented into a cell and described by horizontal u x and vertical u y components fig 2 shows a 5 5 cellular automata grid covered by a wind flow with a general direction from the south west to the northeast a general wind flow direction is established but each vector component has a small disturbance to guarantee randomness as can be seen in some vectors which have a bit different direction from the general one the dynamics of the wind flow is defined by equations 1 and 2 in discrete form the partial derivatives of the wind speed and pressure with respect to the variables x and y for the i th cell at a given iteration are approximated respectively by 9 u x x δ u x i δ x i 10 u y y δ u y i δ y i 11 p x δ p i δ x i and 12 p y δ p i δ y i where δ u x i u l u r δ p i p l p r and δ x i x l x r for equation 9 and δ u y i u a u b δ p i p a p b and δ y i y a y b for equation 10 thus the linearization of equations 1 and 2 by euler s method provides the following difference equations 13 u x i t δ t u x i k u x i δ u x i δ x i u y i δ u x i δ y i 1 ρ 0 δ p i δ x i f u x i δ t for the horizontal wind speed and 14 u y i t δ t u y i k u x i δ u y i δ x i u y i δ u y i δ y i 1 ρ 0 δ p i δ y i f u y i δ t for the vertical wind speed 5 3 temperature the i th cell temperature t i t defines the value of this variable into this micro region of the space the value of t i t defines the appearance of cloud into the cell each cell has a cloud or part of it if t i t belongs to t d δ t d δ interval otherwise there is no cloud into the cell the cell temperature behavior is derived from the newton s law of cooling in which heat transfer between the i th cell and its neighborhood is provided by 15 d t i d t q t i t n where t n is the neighborhood cell temperature and q is a positive constant since the i th cell has four neighborhood the model approximates d t i d t as an average of equation 15 for all neighborhood as follows 16 d t i d t q 1 n o 1 t i k t r t i k t l t i t t a t i t t b q n o 1 4 t i t j t j t where j l r a b n o is the number of cells per neighborhood t i k is the temperature into i th cell at iteration k and the subscripts l r a b mean the neighbors on the left on the right above and below neighbors of an i th cell the cell temperature also interacts to the wind flow and its dynamics is approximated in discrete time for each cell according to equation 4 the partial derivatives of t with respect to x and y for the i th cell are approximated by 17 t x δ t i δ x i and 18 t y δ t i δ y i respectively where δ t i t l t r and δ x i x l x r for equation 17 and δ t i t a t b and δ y i y a y b for equation 18 thus the i th cell temperature is updated as follows 19 t i t δ t t i k u x i δ t i δ x i u y i δ t i δ y i f t φ t d t i d t δ t where d t i d t comes from equation 16 5 4 the model dynamics the model dynamics is based on a discrete and iterative system all grid cell transitions are based on equations 13 14 and 19 that change each cell state locally providing the global effect in the grid the pseudo code of our model is presented as follows 1 for a given altitude initialize ρ 0 t d and t i and p i i 2 initialize f i i u x u y t and φ t 3 introduce a vectorial field of wind into the grid 4 introduce a cloud into the grid 5 update cell temperatures by interaction to its neighborhoods and the wind flow according to equation 19 6 execute the wind flow dynamics according to equations 13 and 14 7 go back to step 5 while a stop criterion is not satisfied a microregion of the atmosphere will have a cloud if the i th cell temperature t i belongs to t d δ t d δ dew point interval 6 simulations and result analyses this section presents four different experiments to validate the proposed model the first experiment shows the performance of the model describing an isolated system the second one shows two experiments with a 100 100 grid in order to present the cloud dynamics the third experiment presents a stability analysis of the proposed model and the last experiment compares the evolution of the proposed model with satellite images of clouds 6 1 thermodynamic model with cellular automata this section presents an experiment to describe an isolated system with cellular automata in isolated systems as the time passes internal rearrangements decrease and stable conditions are reached properties e g pressures and temperatures tend to equalize such that the processes of change come to an end and the system reaches the state of thermodynamic equilibrium two simulations of isolated thermodynamic systems are presented in 30 30 and 50 50 grids the model was implemented with the visual studio 2010 using c language for these simulations the temperature interval was 0 50 degrees the initial temperature was 0 c and δ t 0 01 fig 3 c shows the temperature intervals and their respective colors fig 3 shows six iterations of each simulation in the simulation of the 30 30 grid fig 3 a a warm body of 50 c and 50 grid area was initialized into the center of the grid the heat of the warm body spread quickly see iteration 4 in iteration 27 the extreme area of the grid had been warmed by heat from the warm body to a temperature of between 1 and 10 c iteration 48 shows that the center of the warm body started to cool because its heat spread throughout the grid iterations 112 and 369 show that the heat continued to spread thus providing the temperature equalization iteration 472 shows the moment at which the grid temperature was totally equalized between 1 and 10 c in the simulation of the 50 50 grid fig 3 b a warm body of 50 c and 80 grid area was initialized into the center of the grid the system behavior was similar to that of the first simulation 30 30 grid the heat spread quickly throughout the whole grid after iteration 46 the heat of the center of the grid started to decrease iterations 112 and 134 show the beginning of the temperature equalization throughout the whole grid iteration 485 shows the grid with its temperature totally equalized between 20 and 30 c higher than that of the first simulation because of the larger area of the warm body 80 grid area these simulations showed that a cellular automaton model can simulate a thermodynamic system in both of them the heat in the center of grid spread throughout the whole grid until the thermodynamic equilibrium 6 2 cloud dynamics this subsection presents two simulations with a 100 100 grid the first one with the cloud in the center of the grid and other with the cloud in the top right of the grid the grid altitude was 5000 m the current atmospheric temperature was 3 o c the dew point t d 2 o c δ t 0 01 and a fixed atmospheric pressure of 700 hpa the cloud experiment ran 5000 iterations figs 4 a and 5 a show the initial state of the two simulations the cloud dynamics of our model showed an expected behavior regarding some thermodynamics concepts because in all experiments the whole grid reached thermal equilibrium resulting in total cloud dissipation to which the actions of wind also contributed as can be seen in figs 4 and 5 these figures show that the clouds were dispersed and moved due to the thermal equilibrium and wind interactions effects in figs 4 d and 5 d the two clouds are smaller than their initial positions figs 4 a and 5 a by the dissipation and far from it due to the wind interactions in a grid where a given temperature prevails over almost of its whole area the environmental temperature set to 3 c is expected at infinite time into an undisturbed environment such that it reaches thermal equilibrium close to the initial environmental temperature fig 6 shows both the mean and standard deviation sd of the atmospheric temperature in the two experiments fig 6 a shows the mean temperatures of two experiments which started from about 2 4 c and finished to about 3 c almost the initial atmospheric temperature i e both of them tend to thermal equilibrium fig 6 b presents sd of the two temperatures which passed from 1 5 to about zero i e in the two experiments the grid tended to a same temperature close to 3 c these effects resulting from physical phenomena were more prevalent than those of the simulations with smaller grids these results may be expected because of these smaller areas thermodynamic equilibrium tends to be reached faster than in those of the larger grids in addition to which the probability of a wind reaching the cloud is greater 6 3 stability analysis this section presents the computational experiments on stability analysis of the proposed model the stability is defined with respect to the fixed point of the system when the system reaches a fixed point t its state does not change as the time goes to infinity i e t t t as t in this work experiments the stability was analyzed empirically that is we assumed that the system is stable whenever the mean temperature of the matrix does not change until the end of the simulation 20 000 iterations and its value t t is close to the initial atmospheric temperature thus we assume that the system reached the thermal equilibrium some parameters of the system were fixed in all experiments and their values were defined as follow q 0 0243 w m 1 k 1 cell area was 0 01 m2 altitude was 5 km atmosphere temperature was 3 c t d 8 4 c atmospheric pressure was 700 hpa and wind speed was 60 km h in order to analyze the system stability several matrix dimensions and time step size δ t were used totaling 16 experiments the following matrix dimensions 100 100 2000 2000 and 5000 5000 and δ t 1 s 0 1 s 1 ms 0 1 ms were used in discrete models δ t determines the system precision and stability and the real period of simulation a large δ t provides a long period of simulation nevertheless large δ t increases the deviation between the real and simulated system output moreover the system may become unstable on the other side the precision of the system increases as δ t decreases however the period of simulation also decreases becoming the simulation not feasible for real time in this stability analysis we assessed the largest δ t in which the system remain stable thus the larger a stable δ t the longer the period of simulation in order to use an as precise as possible δ t a deviation analysis between outputs from different simulations was performed fig 7 presents the mean temperatures in the 100 100 matrix for different δ t in fig 7 a it is observed an unstable system since t m increases linearly to 1 8 10 24 at iteration 10 several iterations after that t m overflow thus it is expected that t m as t in fig 7 b t m keeps constant around the initial atmospheric temperature until the first 40 iterations after that t m decreases linearly to thus it is expected that t m as t in fig 7 c and d we see a stable system that reaches the thermal equilibrium with final t m close to the initial atmospheric temperature in fig 7 c the mean temperature reaches the atmospheric temperature at 600 iterations in fig 7 d t m stabilized at 5000 iterations fig 8 presents the mean temperatures of the 2000 2000 matrix for different δ t in fig 8 a and b the system had similar unstable behavior of those presented in fig 7 a and b fig 8 a shows that t m performed an inverse behavior with respect to that presented in fig 7 a i e in the former t m decreases linearly to overflow instead of to increase to in the latter similarly in fig 8 b t m has an inverse behavior with respect to that in fig 7 b in fig 8 b the temperature kept constant until 40 iterations after that t m increased linearly to overflow in fig 8 c t m stabilized with 8200 iterations and kept constant until the end of the experiment thus it is possible to assume that the system reaches the thermal equilibrium with δ t 1 ms in fig 8 d it is not possible to reach a conclusion nevertheless likely the system will stabilize since in 20 000 iterations t m decreased a little due to the disturbance caused by the cloud colder than the initial atmospheric temperature by the reason of the matrix size 20 000 iterations were not enough to stabilize the system fig 9 shows the mean temperatures of the 5000 5000 matrix for several δ t fig 9 a shows the experiment where the behavior of the system was unstable with t m higher than 3 1021 in only 10 iterations such as in the last experiments we assume that t m as t in the experiment shown in fig 9 b the system had similar behavior to that in 100 100 matrix with δ t 0 1 s fig 7 b in fig 9 c t m stabilized about 19 000 iterations thus the system stabilized at a slow rate likely due to the matrix dimension in the experiment shown in fig 9 d the system had a similar behavior to that presented in fig 8 d such as in that experiment we assume that the matrix dimension and low δ t did not permitted that 20 000 iterations were enough to stabilize the system in fig 7 δ t equals 1 ms and 0 1 ms provided a stable behavior in figs 8 and 9 only δ t 1 ms provided a stable behavior in all experiments δ t equals 1 s and 0 1 s provided an unstable behavior then these two value can be considered not suitable for the proposed model the system behaved stable in all experiments with δ t 1 ms until 20 000 iterations a deviation analysis of the mean temperatures with δ t equals to 1 ms and 0 1 ms was performed with 100 100 matrix in order to check the evolution of the proposed model as shown in fig 10 until iteration 100 the values of the mean temperatures were similar after that there was a deviation equals to 0 002 until iteration 280 then the deviation increased to 0 0063 until iteration 580 finally the deviation decreased to zero until the end of the simulation this error can be considered acceptable with respect to the magnitude of the temperature table 1 shows the means and standard deviations of the temperatures with δ t 1 s for all matrix dimensions the initial mean temperatures decreased as the matrix dimensions increased due to the cloud area with respect to the matrix dimension i e the larger is the matrix dimension the smaller is the disturb caused by the cloud in the environment the initial standard deviations also show this relation for instance in 100 100 matrix the rate cloud area per matrix area is larger than that in 5000 5000 matrix thus in the former the cloud provides more disturbance it can be seen that the final mean temperatures and standard deviations provided nan not a number value i e these variables went to these results show that the system did not reach the thermal equilibrium in accordance with the analysis on figs 7 to 9 table 2 shows the means and standard deviations of the temperatures with δ t 0 1 s in which they had similar behavior to those with δ t 1 s table 1 table 2 also shows an unstable system where the final t m and standard deviations provided nan i e they tended to table 3 shows the means and standard deviations of the temperatures with δ t 1 ms in all experiments t m and standard deviations tended to the initial atmospheric temperature and zero respectively these data support our analysis based on figs 7 c 8 c and 9 c finally table 4 shows the means and standard deviations of the temperatures with δ t 0 1 ms the final mean temperature and standard deviation in 100 100 matrix tended to the initial atmospheric temperature and zero respectively in this experiment we can assume that the system reached the thermal equilibrium in experiments with 2000 2000 and 5000 5000 matrices the mean temperatures did not change significantly along the simulations in these experiments the standard deviations showed a small decrease along the time there are two reasons for that the very small δ t which evolves a small period of time during the simulation and the matrix dimension which needs many iterations to propagate the temperature disturbance the number of time step size δ t used in the experiments provided the following conclusions i values of δ t equal or larger than 0 1 s likely provide an unstable system such as those used in our experiments δ t 0 1 s 1 s and ii values of δ t smaller than 0 1 ms will provide a small evolution of the system for a long time of processing simulations with small δ t will not produce significant variations on the state of the system especially in large matrix such as those used in the experiments with δ t 0 1 ms for 2000 2000 and 5000 5000 matrices the simulations showed that δ t 1 ms provides an useful step of time for the proposed model this δ t supplies a stable system with lead the area of simulation to the thermal equilibrium 6 4 satellite images this section presents a real case where the performance of the proposed model is compared with the evolution of a clouds from satellite images the images came from inpe instituto nacional de pesquisas espaciais brazil the tests used images from 6 p m to 8 p m fig 11 shows the original images used for the experiment fig 13 shows the cloud images with their respective temperatures in which the scale is given in fig 12 fig 14 shows two images at 7 p m the result of simulating the proposed model fig 14 a and the satellite image at 7 p m fig 14 b fig 14 shows four clouds fig 14 a shows the simulation of the proposed model from 6 p m fig 13 a to 7 p m and fig 14 b presents the satellite image at 7 p m these four clouds were named top t bottom b center c and right r all clouds in fig 14 a tend to homogeneity and disappear because of heat exchange with the atmosphere since the proposed model uses only one layer of the atmosphere thus convection does not act on the clouds moreover the simulated area is isolated in fig 14 a cloud t shows a similar shape to its equivalent one in fig 14 b the satellite image nevertheless in fig 14 b a cold blue temperature cloud appears on the top different to its equivalent in fig 14 b this occurred likely because of the convection between the atmosphere layer where the simulation takes place and the above or below one cloud c in fig 14 a begins to dissolve due to heat exchange cloud b in fig 14 a is seen to have a similar shape to its equivalent one in fig 14 b except that it is less displaced by the wind finally cloud r spreads in the atmosphere more than its equivalent in fig 14 b however their positions and shapes are similar 7 conclusion this article set out to construct a model to simulate thermodynamic systems using cellular automata two types of models were presented an isolated thermodynamic system and a cloud dynamics model the former showed that a cellular automaton can simulate a thermodynamic system this first model was the basis for the second one the cloud dynamics model the second model used a limited representation considering the variables that represent the dynamics of a real cloud the atmospheric temperature and wind flow in the two dimensional grid were included a two dimensional ca with a von neumann neighborhood of 5 cells was used the transition rules were defined based on the thermodynamic principle that defines the thermal equilibrium and the wind interaction the validation of the second model was made a simulation with a 100 100 cellular automaton grid in this preliminary study we did not compare our model with other ones for cloud dynamics thus we only conducted visual validation and graphical analyses considering basic thermodynamic principles the simulations showed that our proposed model presented a satisfactory behavior considering some thermodynamic principles the wind actions also were considered coherent because they moved the clouds until their full dissipation in the experiment the clouds with heterogeneous temperature randomly initialized tended to converge to a uniform temperature reaching the thermal equilibrium another observed behavior was the thermal equilibrium between the cloud and grid which always resulted in cloud dissipation the wind actions also contributed to convergence of cloud temperature to environmental temperature because they spread the clouds accelerating the thermal equilibrium moreover the proposed model was compared with realist data using satellite images the results showed that the dynamic of the proposed model was similar to that of the real satellite images as a follow up to this study other variables will be added such as pressure kinetic energy density and humidity thus making the model more reliable another proposal is to simulate a three dimensional space approximating the model of a real system the proposed model also may be implemented using the parallel computing paradigm improving its performance and consequently its ability to perform in real time the latter proposal is justified by the increase of variables involved which feature a real atmospheric system thus parallel computing may increase the model performance in a more complex scenario acknowledgments the authors gratefully acknowledge the coordination for the improvement of higher education personnel capes and pontifical catholic university of minas gerais puc minas which supported this work and the assistance provided by the center of climatology of puc minas appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104537 
