index,text
26360,assessing the ability to predict nutrient concentration in streams is important for determining compliance with the numeric nutrient water quality criteria for nitrogen in the u s a evaluation of the usgs s load estimator loadest and the weighted regression on time discharge and season wrtds models in predicting total nitrogen loads over 18 stations from the water quality network show good performance nash sutcliffe efficiency nse 0 8 in capturing the observed variability even for stations with limited data however both models captured only 40 of observed variance in total nitrogen tn concentration nse 0 4 thus the same dataset performed differently in predicting two attributes tn load and concentration questioning the predictive skill of the models this study proposes a non parametric re sampling approach for assessing the performance of water quality models particularly in predicting tn concentration null distributions for three common performance metrics belonging to populations of metrics with no skill in capturing the observed variability are constructed through a bootstrap resampling technique sample metrics from the loadest and wrtds model in predicting tn concentration are used to calculate p values for determining if the sample metrics belongs to the null distributions keywords water quality modeling performance assessment non parametric re sampling software the toolkit for assessing performance in concentration regression models tap crm is available for free online at the following url https github com chwr ncsu water quality toolkit the program is designed to run as a windows executable exe and communicates with the loadest and wrtds program the matlab and r codes are available for individual download details for the software requirements and developer contact information can be found in the readme md file available on the site 1 introduction water quality predictions particularly non point pollutants such as total nitrogen tn typically focus on estimating load and concentration across various spatio temporal scales the primary challenge in assessing water quality models performance over a large spatial extent is due to the limited availability of observed values oh and sankarasubramanian 2012 data networks with observed water quality measurements available at regional scale such as usgs s water quality network wqn alexander et al 1998 have observations over many years but are infrequent with some months having only one observed value as a result measurements such as tn concentration are available for less than 200 days spanning a 20 year period while mechanistic models like the swat model arnold et al 2012 have potential to provide continuous values they are difficult to calibrate and validate using the sparse and non consecutive observations of water quality constituents a problem that doesn t exist for streamflow predictions smith et al 1997 apart from limited and infrequent water quality samples nutrient loading predictions from regression and mechanistic models are able to capture the observed variability well nash sutcliffe efficiency nse 0 6 0 7 smith et al 1997 douglas mankin et al 2010 kim and kaluarachchi 2014 thus model predictions using streamflow as a predictor tend to easily capture the observed variability in loadings loadings is typically the target constituent for regulation management as seen in total maximum daily load tmdl programs equally important are concentration estimates which are required for assessing compliance with instream concentration limits water quality models like the weighted regression on time discharge and season wrtds hirsch et al 2010 and the load estimator loadest cohn et al 1992 1989 cohn 2005 which can be adapted for concentration estimation use observed discharge and time as predictors for predicting concentration however this study shows that these empirical models have difficulty attaining higher accuracies nse 0 4 in predicting observed concentration which could be potentially interpreted as insignificant particularly for stations with limited data even though observed loads are simply the product of observed concentration and streamflow lower values of nse in predicting concentration suggests that different accuracy thresholds apply for determining the statistical significance of concentration estimates thus there is a need for an alternate way of evaluating the skill in predicting concentration by water quality models one such way is proposed here the intent of this study is to develop a robust nonparametric toolkit that does not depend on distributional assumptions for determining if a sample performance metric is likely drawn from a population of metrics having no skill the framework used in the toolkit can be applicable to a wide range of performance metrics and model types but in this study the framework is applied to two types of water quality prediction models the toolkit provides null distributions p values and skill scores for assessing model performances a tool for studying the uncertainty in water quality trends from the wrtds model named the wrtds bootstrap test wbt was developed by hirsch et al 2015 wbt was built to address the uncertainty in water quality trends for example it could provide the 95 confidence interval in the rise of annual mean flow normalized concentration similar to this the tap crm toolkit focuses on determining if concentration predictions from the loadest and wrtds models are statistically different from models with no skill although the differences in model performance between the loadest and wrtds models are presented the purpose of this study is not on comparing the models which has already been discussed comprehensively hirsch 2014 popular statistical measures pearson s correlation ρ and the nse are considered for assessing water quality model performance as part of the toolkit instead of just reporting the model performance on nse or ρ the toolkit also provides the p value in predicting concentration however to estimate p values for a given sample statistic distributional assumption on the water quality data is required gotway et al 1994 for instance if one assumes the data follows normal distribution then nse is assumed to follow a f distribution as a ratio of chi square distribution larsen and marx 1986 given the limited discontinuous water quality data such distributional assumption may not be desirable hence we propose a non parametric re sampling method that eliminates distributional assumption and is robust for evaluating model performance using smaller sample sizes additionally a new skill score is introduced that provides a numerical measurement for comparing the accuracy across models and their performances as opposed to using just the p value which represents a binary response reject null fail to reject null this article describes the algorithm for determining the nonparametric distribution of performance statistics nse and ρ and how they are useful in finding p values and respective skill scores using the toolkit software a case study comparing concentration regression models for 18 locations across the southeast using metrics from the toolkit is also presented 2 data sources 2 1 wqn measurements this study considers 18 stations from region 3 of the southern eastern united states sites that have also been studied in forecasting seasonal nutrients using climate information oh and sankarasubramanian 2012 frequency of sampling across sites started at the monthly timescale in 1962 and reduced to bimonthly and seasonal sampling from 1982 to 1995 the number of daily observations for total nitrogen records for the 18 stations averages about 200 days extended over about 20 years these stations belong to both the national stream water quality monitoring network wqn and the hydro climatic data network hcdn alexander et al 1998 stations belonging to the hcdn have streamflow that is minimally impacted by anthropogenic influences like artificial storage or pumping preserving the climate signal in streamflow slack et al 1993 vogel and sankarasubramanian 2005 the wqn is a combination of two subsets of networks the national stream quality accounting network nasqan with observations from 1962 to 1995 and the hydrologic benchmark network hbn with observations from 1973 to 1995 the nonparametric framework proposed in this study is only applied to models predicting total nitrogen concentration but can be applicable to other water quality constituents having limited data such as total phosphorus or dissolved oxygen 2 2 loadest estimates observed streamflow and total nitrogen concentrations from the wqn records were used to calibrate the usgs s constituent load estimator loadest cohn et al 1992 1989 cohn 2005 model performance is evaluated using concentration or load estimations from each day in the observed period model regression form number 4 was used for load estimation for all 18 stations using maximum likelihood estimation mle to provide context to the inherent difference between concentration and load estimation performance the loadest model was modified for concentration estimation by manually defining the model form in the header inp file using the technique outlined in the manual the internal model selection feature using the akaike information criterion aic is disabled when altering the model form predetermined model forms 1 10 for estimating concentration using mle were tested for goodness of fit using nse as the criteria the final model form used in this study is shown in 1 which is very similar to the preset model form number 4 for load regression 1 ln c i α 1 ˆ α 2 ˆ ln q i α 3 ˆ sin 2 π d t i m e α 4 ˆ cos 2 π d t i m e ε ˆ i where c i is the observed daily total nitrogen concentration on day i q i is the observed daily streamflow on day i d t i m e is the centered decimal time α 1 ˆ α 4 ˆ are model estimated coefficients and ε ˆ i denotes the estimated model residual on day i 2 3 wrtds estimates the wrtds model is an estimation method for water quality constituents included in the exploration and graphics for river trends egret package used for studying long term changes in water quality and streamflow concentration measurements from the wqn data set are manually uploaded into the wrtds model streamflow data from the usgs streamgage location is loaded from the data retrieval process internal to the model package the wrtds package is already set up for concentration estimation using the model form in 2 2 ln c i β 1 ˆ β 2 ˆ ln q i β 3 ˆ ln t β 4 ˆ sin 2 π t β 5 ˆ cos 2 π t ε i ˆ where c i is the observed daily total nitrogen concentration in mg l on day i q i is the mean daily streamflow for day i t is the decimal time β 1 ˆ β 4 ˆ are model estimated coefficients and ε ˆ i denotes the estimated model residual on day i the regression form shown in 2 is fit for each observation using a weighted tobit regression in a leave one out cross validation approach the weight for each observation is determined by the product of individual weights for discharge time and season found using a tricube function hirsch et al 2010 model estimates for each day in the observed period have corresponding model estimated coefficients and standard errors estimates for concentration on each day are transformed back to the original space using a bias correction factor bcf shown in 3 hirsch et al 2010 3 b c f exp s e 2 2 3 methods 3 1 motivation performance metrics like r squared and nse are commonly used for assessing water quantity prediction models ahl et al 2008 alansi et al 2009 kim and kaluarachchi 2014 wang et al 2014 although specific criteria does not exist for determining poor and excellent models using these performance metrics when values of nse become negative or close to zero they are considered unusable predictions instead one can simply use the mean value of the observed values santhi et al 2001 hydrologic models have shown that even calibrated models can result in different values of the nse under different time scales hence a context should be provided when describing the model s performance application of wrtds and loadest models have shown that popular concentration estimation models consistently have model predictions with nse values less than 0 4 fig 1 to provide reference to the wrtds and loadest models performance we propose a non parametric performance assessment toolkit for assessing if the nse and ρ estimated by the water quality models are significantly different than metrics having no skill i e nse 0 ρ 0 jain and sudheer 2008 schaefli and gupta 2007 non parametric hypothesis tests based on re sampling allows for the estimation of the underlying null distribution that may not follow common forms such as bi modal distributions the null distributions of certain performance measures such as the pearson s correlation ρ and nse depend on sample size and the underlying distribution of streamflow standard normal approximations have been used mccuen et al 2006 to calculate the p values of the nse statistic using a transformation very similar to the fisher z transformation fisher 1992 that is used for approximating the distribution of pearson s correlation these transformations are limited to positive nse values given that nse can be negative this approach is not useful in calculating p values for all underlying population nse values under these conditions it is advantageous to consider non parametric hypothesis tests for region wide studies where distributions may change between watersheds and even time periods 3 2 non parametric bootstrapping this toolkit estimates the null distribution for three performance measures to evaluate whether a metric e g nse from either loadest or wrtdsis significantly different than models having no skill in predicting the observed variability null distributions for the metrics used in the toolkit will be closely centered around zero although in most cases there will be some positive bias in each metric coming from the model s random ability to capture the observed variance hence using a p value provides the criteria for testing whether a performance statistic e g nse belongs to a null distribution at a certain alpha level having no skill in predicting the observed variability gronewold et al 2009 specifically the null distribution is comprised of performance metrics from estimates that have no skill in estimating the variability of predictands i e concentration using the given predictors streamflow for instance if we are testing whether the nse from the loadest regression model nseloadest is statistically significant from a null model whose predictors and predictands do not co vary thereby the nse corresponding to no skill equation 4 the corresponding alternative hypothesis is shown in equation 5 this study refrains from defining the null distribution as being equal to zero e g ho nseloadest 0 since the nonparametric resampling creates null distributions that are not exactly centered on zero 4 h 0 n s e l o a d e s t w r t d s n s e n o s k i l l 5 h a n s e l o a d e s t w r t d s n s e n o s k i l l the null distribution of performance measures is created based on n realizations of loadest or wrtds model runs which are fitted uncorrelated predictand and predictor sets each realization having a sample length of n is created by randomly sampling with replacement from the observed concentration dataset with each observation being equally likely realizations are created by randomly sampling from the concentration data since the regression models both take concentration as the calibration data set to fit their respective models this resampling with replacement scheme randomizes the predictand set while keeping the day and observed discharge fixed within the dataset this process removes the time dependency between the predictand concentration and predictors day and streamflow thereby creating realizations whose mean correlation between the predictand and predictor sets approaches zero the detailed outline for the hypothesis testing framework is shown in fig 1 using the uncorrelated predictand and predictors of length n both wrtds and loadest models are fitted first and then the performance of the fitted model were evaluated using nse ρ and index of agreement ioa by comparing with the observed predictand this process is repeated for n realizations nloadest 100 and nwrtds 30 to develop the null distribution of nse ρ and ioa since the predictors and predictands are uncorrelated the performance measures from the null model represent the water quality models random ability to capture the observed variability thereby providing a basis for checking the statistical significance of the water quality model s performance n s e l o a d e s t w r t d s 3 3 performance metrics the three performance measures considered in this toolkit are the pearson s correlation ρ nash sutcliffe efficiency nse and index of agreement ioa pearson s correlation measures the linear correlation between the model estimates pi and the observations oi shown in 6 correlation values range from 1 to 1 with a value of 1 signifying perfect correlation and a value of 0 indicating no correlation between model and observed values 6 ρ i 1 l o i o p i p i 1 l o i o 2 i 1 l p i p 2 nash sutcliffe efficiency measures the squared deviations of the model values pi to the observed values oi with respect to the squared deviations of observations to the mean of observations o as shown in 7 nse is basically defined as one minus the sum of squared residuals normalized by the observed variance nash and sutcliffe 1970 nse values range from negative infinity to 1 with negative values indicating that the mean value of the observations is a better predictor than the model nash and sutcliffe 1970 models that just predict the mean of the observations and do not capture any of the observed variability will result in an nse value of zero in this case we will call this a model as one with no skill 7 n s e 1 i 1 l o i p i 2 i 1 l o i o 2 the reason for presenting both pearson s correlation and nse is to examine the amount of bias deviation between the model and observation since the difference between the two is just the square of the bias as the bias reduces square of the pearson s correlation approaches nse krause et al 2005 in hydrologic predictions the nse can be very sensitive to large deviations in observed and predicted values for high flow which can cause negative values legates and mccabe 1999 the index of agreement ioa was proposed to overcome the sensitivities of the nse statistic by using the potential error in the denominator willmott 1984 seen in 8 the range of ioa is from 0 to 1 with a value of 0 indicating no correlation 8 i o a 1 i 1 l o i p i 2 i 1 l p i o o i o 2 when using the nse statistic for comparison certain conditions will cause the null distribution to center around negative one rather than zero if the expectation of the model estimates y i ˆ from loadest or wrtds approach the observed mean with a variance close to zero then the nse value will approach zero as shown in 9 however if the variance of the model estimates approach that of the observations the null distribution will be centered on negative 1 while still showing a pearson s correlation distribution centered on zero shown in 10 9 e y i ˆ μ o b s v a r y i ˆ 0 n s e 0 10 e y i ˆ μ o b s v a r y i ˆ σ o b s 2 n s e 1 essentially the numerator in 7 will become twice the denominator and the value will become negative one the variance of the wqn data sets for each station are much less than 1 0 09 and the variance of model estimates are an order of magnitude smaller essentially being close to zero 0 009 using the bootstrapping framework to produce concentration estimates results in a scenario shown in 9 where the nse is centered near zero load estimates form loadest or wrtds not only reproduce the observed mean but also the observed variance while still having no correlation under this scenario the nse will be centered on negative 1 shown in 10 the index of agreement has been included as another performance metric since it does not have this issue and is provided for a more robust analysis 3 4 p values and skill score using a non parametric bootstrap outlined in fig 1 allows for the realizations to have null distributions centered towards zero for pearson s correlation nse and ioa since this is a non parametric re sampling technique the p value for the performance statistics are computed empirically instead of assuming an underlying distribution for model estimates and observations although ρ and nse can be positive or negative the p value is calculated using a one tailed test recall that negative values of ρ and nse indicate models estimates that are worse than just the observed mean so the user should only be interested when the performance statistic is positive using a significance level α of 5 any test statistic having a p value less than 0 05 can be considered not belonging to the null distribution a critical value for the performance statistic can be calculated based on α which specifies the threshold value on the right tail of the null distribution statistic values that are to the left of the threshold value are considered to part of the null distribution for that given α thereby insignificant furthermore this study introduces a skill score shown in 11 which can be useful for comparison between two different models that have different null distributions for the same performance statistics ps denotes the performance statistic and ps critical denotes the right tailed critical value of the performance statistic for the chosen α the skill score ranges from 0 to 100 with a score of 0 indicating that performance statistic is centered on the critical value and a score of 100 indicating a perfect performance metric 11 s s p s p s model p s c r i t i c a l 1 p s c r i t i c a l 100 since a value of 1 indicates a perfect performance metric for all three metrics discussed in this paper the form of equation 11 does not change negative values for ssps indicate that the performance metric belongs to the null distribution and will have a corresponding p value greater than α since p values are primarily used for hypothesis testing and not for indicating the strength of performance metrics the ssps was developed to provide a standard scale for comparing performance metrics between different models 3 5 autocorrelation using data sets that have significant autocorrelation should preserve the autocorrelation in the resampling technique concentration measurements from the wqn are from the monthly to seasonal scale over multiple decades and any significant lag correlation can be interpreted as purely coincidentally in some cases where stations have records from the same month they are never on consecutive days or do not occur over the entire period enough to create significant lag correlation if using a data set with consecutive daily measurements or with reasonable lag correlation we recommend moving block bootstrap sampling in the described framework the block size for resampling is set to 1 meaning that the only one observation is resampled at a time from the observations the framework code is provided and allows for the manipulation of the block size for data sets with significant lag correlation a test for autocorrelation is recommended for each data set before using the toolkit the moving block bootstrap option used in this study is very similar to the framework used to preserve lag correlation in studies for addressing uncertainty in trend analyses for concentration and load hirsch et al 2015 vogel and shallcross 1996 for example say daily concentration values for tar river at tarboro nc had significant lag 3 correlation then the resampling technique would resample 3 neighbors of observed concentration in a single sample for creating a realization so to construct observed concentration of length n by resampling we then resample only n 3 times further details of the moving block bootstrap can be found with the source code but is not discussed further as most observed water quality data sets are discontinuous with daily observations being far apart e g once in two months 4 toolkit interface a performance toolkit for the purpose of testing the significance and comparing models was developed for open source distribution to academics and commercial water quality modelers although the toolkit is constructed for assessing concentration regression models outlined in section 2 the bootstrapping framework was applied for testing the significance of loadest load estimations to show that performance was significant using the non parametric re sampling technique outlined and then applying a kernel smoothing function to the histogram reveals a shape and features that may not be so easily discoverable otherwise fig 2 shows the general framework for uploading data and the options users can choose to evaluate model performance users first upload observed streamflow and concentration data using an excel or csv file the number of simulations and the alpha level can be specified with recommended values of n 100 when using loadest and n 30 when using wrtds and with an α 0 05 the toolkit does not run the loadest and wrtds models internally rather it communicates with the fortran executable and an r complier respectively for each model program the wrtds model takes longer to run and requesting a large number of simulations 30 can greatly increase computation time the user can choose to define null distributions for three statistics pearson s correlation ρ nash sutcliffe efficiency nse and index of agreement ioa after the toolkit runs the re sampling method outlined in fig 1 the nonparametric distribution is plotted with visible kernel smoothing curves shown as blue lines and black density dots displayed along the bottom based on the histogram as shown in fig 4 a summary of all performance statistics p values and skill scores are provided in a result after the distributions are plotted the toolkit first runs loadest realizations since these runs are quicker then a model comparison option is offered where the user can choose to run the wrtds model and compare using the p values skill scores akaike information criterion aic and bayesian information criterion bic all presented in a model comparison table as shown in fig 2 akaike 1981 the toolkit interface was built using the guide feature in matlab shown in fig 3 and is offered as a windows executable that can be used stand still 5 application and results loadest and wrtds regression models were used for predicting tn concentrations over 18 stations belonging to the wqn over the southeastern united states for additional details on the 18 stations see supplementary material application of the loadest model in estimating load concentration for 18 stations in fig 5 fig 6 show the nse varies from 0 79 to 0 98 0 01 0 17 indicating performance of load estimation being better than that of concentration estimates in explaining the observed variability of the respective attributes however p values obtained from the application of the nonparametric toolkit show that the nse values are statistically different from zero for 12 of the 18 sites i e reject the null hypothesis fig 6 thus a rigorous hypothesis testing of the reported skill is necessary for reporting the water quality model performance the developed nonparametric toolkit is useful in discerning whether the skill is statistically significant without requiring any distribution assumption to assess the model performance given that the re sampling approach relies on no lag dependency in the model data caution should be placed while using the modeled data with continuous daily values since observations are often discontinuous in the water quality data and the performance statistic estimation requires comparison with observed data the lag correlations are not going to be significant in this case in case if the lag correlations are significant we suggest considering moving block approach for re sampling hirsch et al 2015 vogel and shallcross 1996 application also demonstrated the usefulness of skill score 11 in comparing the model performance using different performance metrics fig 7 and in comparing the performance of two different models fig 8 loadest and wrtds estimated using the same metric nse lower values for the skill score indicate that they are approaching the threshold for failing to reject the null distribution and higher values mean the performance statistics are approaching a value of 1 skill scores fig 7 comparison between correlation and nse obtained in predicting concentration shows tremendous variability across the region but the performance of two statistics are consistent stations having negative skill scores all correspond to having p values above 0 05 and thus fail to reject the null hypothesis specifically station 7 satilla river at atkinson ga has a negative skill score for ρ but a slightly positive skill score for nse since the positive skill score for station 7 is so close to the threshold 1 this station is considered to have insignificant performance the skill scores for stations 6 falling creek near juliette ga for both performance metrics have magnitudes larger than 10 indicating the individual metrics are more toward the center of the null distribution skill scores comparing the two concentration models fig 8 loadest and wrtds show that wrtds model perform slightly better in estimating the observed concentration across all stations except station 17 station 17 has the shortest record of observations 55 among the 18 stations and the wrtds has difficulty making meaningful predictions using short records the primary advantage of using the skill score is that the water quality models performance can be compared and evaluated across performance statistics as well as across models 6 discussion and conclusion this study recognizes the inherent problem of data availability when dealing with water quality observations and stresses the need for longer continuous data sets this study promotes the use of a non parametric re sampling approaches as opposed to using parametric tests for evaluating model performance as it is difficult to accurately quantify the underlying model with infrequent and limited observations this version of the toolkit was developed for water quality concentration regression models and does not easily apply to load estimation models specifically this pertains mostly to the calibration and estimation files for loadest load estimation which are formatted differently a toolkit for the purposes of assessing loading regression models may not be as useful considering that load estimation models perform much higher than concentration estimation models as proven by the case study if the tap crm toolkit is used for data sets that have significant autocorrelation options to use a moving block bootstrapping approach are available in the matlab source code and the block size should be modified using the non parametric re sampling approach for hypothesis testing the toolkit and the proposed skill scores provides a means of comparing a given model s performance with different performance statistics as seen in fig 7 as well as in comparing multiple models performance under a given statistic as seen in fig 8 it is reassuring to see that given the low values of nse the model s skill is still statistically significant the evidence provided by the toolkit helps establish expected results for modelers focusing on concentration which is a critical variable for downstream habitats as more waters become impaired due to anthropogenic influences we can expect more observations to be available for modeling and expect the values of performance to increase but until then water quality modelers must discover ways on how to deal with the currently available data to make the best informed decisions acknowledgments we would like to thank the national science foundation for supporting this project under the career grant climate informed uncertainty analyses for integrated water resources sustainability no 0954405 contributors d a l a s bjr and s a designed the research d l wrote the code for the research d l a s and s a analyzed the results d l a s and s a wrote the paper appendix a supplementary data the following is the supplementary data related to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 05 013 
26360,assessing the ability to predict nutrient concentration in streams is important for determining compliance with the numeric nutrient water quality criteria for nitrogen in the u s a evaluation of the usgs s load estimator loadest and the weighted regression on time discharge and season wrtds models in predicting total nitrogen loads over 18 stations from the water quality network show good performance nash sutcliffe efficiency nse 0 8 in capturing the observed variability even for stations with limited data however both models captured only 40 of observed variance in total nitrogen tn concentration nse 0 4 thus the same dataset performed differently in predicting two attributes tn load and concentration questioning the predictive skill of the models this study proposes a non parametric re sampling approach for assessing the performance of water quality models particularly in predicting tn concentration null distributions for three common performance metrics belonging to populations of metrics with no skill in capturing the observed variability are constructed through a bootstrap resampling technique sample metrics from the loadest and wrtds model in predicting tn concentration are used to calculate p values for determining if the sample metrics belongs to the null distributions keywords water quality modeling performance assessment non parametric re sampling software the toolkit for assessing performance in concentration regression models tap crm is available for free online at the following url https github com chwr ncsu water quality toolkit the program is designed to run as a windows executable exe and communicates with the loadest and wrtds program the matlab and r codes are available for individual download details for the software requirements and developer contact information can be found in the readme md file available on the site 1 introduction water quality predictions particularly non point pollutants such as total nitrogen tn typically focus on estimating load and concentration across various spatio temporal scales the primary challenge in assessing water quality models performance over a large spatial extent is due to the limited availability of observed values oh and sankarasubramanian 2012 data networks with observed water quality measurements available at regional scale such as usgs s water quality network wqn alexander et al 1998 have observations over many years but are infrequent with some months having only one observed value as a result measurements such as tn concentration are available for less than 200 days spanning a 20 year period while mechanistic models like the swat model arnold et al 2012 have potential to provide continuous values they are difficult to calibrate and validate using the sparse and non consecutive observations of water quality constituents a problem that doesn t exist for streamflow predictions smith et al 1997 apart from limited and infrequent water quality samples nutrient loading predictions from regression and mechanistic models are able to capture the observed variability well nash sutcliffe efficiency nse 0 6 0 7 smith et al 1997 douglas mankin et al 2010 kim and kaluarachchi 2014 thus model predictions using streamflow as a predictor tend to easily capture the observed variability in loadings loadings is typically the target constituent for regulation management as seen in total maximum daily load tmdl programs equally important are concentration estimates which are required for assessing compliance with instream concentration limits water quality models like the weighted regression on time discharge and season wrtds hirsch et al 2010 and the load estimator loadest cohn et al 1992 1989 cohn 2005 which can be adapted for concentration estimation use observed discharge and time as predictors for predicting concentration however this study shows that these empirical models have difficulty attaining higher accuracies nse 0 4 in predicting observed concentration which could be potentially interpreted as insignificant particularly for stations with limited data even though observed loads are simply the product of observed concentration and streamflow lower values of nse in predicting concentration suggests that different accuracy thresholds apply for determining the statistical significance of concentration estimates thus there is a need for an alternate way of evaluating the skill in predicting concentration by water quality models one such way is proposed here the intent of this study is to develop a robust nonparametric toolkit that does not depend on distributional assumptions for determining if a sample performance metric is likely drawn from a population of metrics having no skill the framework used in the toolkit can be applicable to a wide range of performance metrics and model types but in this study the framework is applied to two types of water quality prediction models the toolkit provides null distributions p values and skill scores for assessing model performances a tool for studying the uncertainty in water quality trends from the wrtds model named the wrtds bootstrap test wbt was developed by hirsch et al 2015 wbt was built to address the uncertainty in water quality trends for example it could provide the 95 confidence interval in the rise of annual mean flow normalized concentration similar to this the tap crm toolkit focuses on determining if concentration predictions from the loadest and wrtds models are statistically different from models with no skill although the differences in model performance between the loadest and wrtds models are presented the purpose of this study is not on comparing the models which has already been discussed comprehensively hirsch 2014 popular statistical measures pearson s correlation ρ and the nse are considered for assessing water quality model performance as part of the toolkit instead of just reporting the model performance on nse or ρ the toolkit also provides the p value in predicting concentration however to estimate p values for a given sample statistic distributional assumption on the water quality data is required gotway et al 1994 for instance if one assumes the data follows normal distribution then nse is assumed to follow a f distribution as a ratio of chi square distribution larsen and marx 1986 given the limited discontinuous water quality data such distributional assumption may not be desirable hence we propose a non parametric re sampling method that eliminates distributional assumption and is robust for evaluating model performance using smaller sample sizes additionally a new skill score is introduced that provides a numerical measurement for comparing the accuracy across models and their performances as opposed to using just the p value which represents a binary response reject null fail to reject null this article describes the algorithm for determining the nonparametric distribution of performance statistics nse and ρ and how they are useful in finding p values and respective skill scores using the toolkit software a case study comparing concentration regression models for 18 locations across the southeast using metrics from the toolkit is also presented 2 data sources 2 1 wqn measurements this study considers 18 stations from region 3 of the southern eastern united states sites that have also been studied in forecasting seasonal nutrients using climate information oh and sankarasubramanian 2012 frequency of sampling across sites started at the monthly timescale in 1962 and reduced to bimonthly and seasonal sampling from 1982 to 1995 the number of daily observations for total nitrogen records for the 18 stations averages about 200 days extended over about 20 years these stations belong to both the national stream water quality monitoring network wqn and the hydro climatic data network hcdn alexander et al 1998 stations belonging to the hcdn have streamflow that is minimally impacted by anthropogenic influences like artificial storage or pumping preserving the climate signal in streamflow slack et al 1993 vogel and sankarasubramanian 2005 the wqn is a combination of two subsets of networks the national stream quality accounting network nasqan with observations from 1962 to 1995 and the hydrologic benchmark network hbn with observations from 1973 to 1995 the nonparametric framework proposed in this study is only applied to models predicting total nitrogen concentration but can be applicable to other water quality constituents having limited data such as total phosphorus or dissolved oxygen 2 2 loadest estimates observed streamflow and total nitrogen concentrations from the wqn records were used to calibrate the usgs s constituent load estimator loadest cohn et al 1992 1989 cohn 2005 model performance is evaluated using concentration or load estimations from each day in the observed period model regression form number 4 was used for load estimation for all 18 stations using maximum likelihood estimation mle to provide context to the inherent difference between concentration and load estimation performance the loadest model was modified for concentration estimation by manually defining the model form in the header inp file using the technique outlined in the manual the internal model selection feature using the akaike information criterion aic is disabled when altering the model form predetermined model forms 1 10 for estimating concentration using mle were tested for goodness of fit using nse as the criteria the final model form used in this study is shown in 1 which is very similar to the preset model form number 4 for load regression 1 ln c i α 1 ˆ α 2 ˆ ln q i α 3 ˆ sin 2 π d t i m e α 4 ˆ cos 2 π d t i m e ε ˆ i where c i is the observed daily total nitrogen concentration on day i q i is the observed daily streamflow on day i d t i m e is the centered decimal time α 1 ˆ α 4 ˆ are model estimated coefficients and ε ˆ i denotes the estimated model residual on day i 2 3 wrtds estimates the wrtds model is an estimation method for water quality constituents included in the exploration and graphics for river trends egret package used for studying long term changes in water quality and streamflow concentration measurements from the wqn data set are manually uploaded into the wrtds model streamflow data from the usgs streamgage location is loaded from the data retrieval process internal to the model package the wrtds package is already set up for concentration estimation using the model form in 2 2 ln c i β 1 ˆ β 2 ˆ ln q i β 3 ˆ ln t β 4 ˆ sin 2 π t β 5 ˆ cos 2 π t ε i ˆ where c i is the observed daily total nitrogen concentration in mg l on day i q i is the mean daily streamflow for day i t is the decimal time β 1 ˆ β 4 ˆ are model estimated coefficients and ε ˆ i denotes the estimated model residual on day i the regression form shown in 2 is fit for each observation using a weighted tobit regression in a leave one out cross validation approach the weight for each observation is determined by the product of individual weights for discharge time and season found using a tricube function hirsch et al 2010 model estimates for each day in the observed period have corresponding model estimated coefficients and standard errors estimates for concentration on each day are transformed back to the original space using a bias correction factor bcf shown in 3 hirsch et al 2010 3 b c f exp s e 2 2 3 methods 3 1 motivation performance metrics like r squared and nse are commonly used for assessing water quantity prediction models ahl et al 2008 alansi et al 2009 kim and kaluarachchi 2014 wang et al 2014 although specific criteria does not exist for determining poor and excellent models using these performance metrics when values of nse become negative or close to zero they are considered unusable predictions instead one can simply use the mean value of the observed values santhi et al 2001 hydrologic models have shown that even calibrated models can result in different values of the nse under different time scales hence a context should be provided when describing the model s performance application of wrtds and loadest models have shown that popular concentration estimation models consistently have model predictions with nse values less than 0 4 fig 1 to provide reference to the wrtds and loadest models performance we propose a non parametric performance assessment toolkit for assessing if the nse and ρ estimated by the water quality models are significantly different than metrics having no skill i e nse 0 ρ 0 jain and sudheer 2008 schaefli and gupta 2007 non parametric hypothesis tests based on re sampling allows for the estimation of the underlying null distribution that may not follow common forms such as bi modal distributions the null distributions of certain performance measures such as the pearson s correlation ρ and nse depend on sample size and the underlying distribution of streamflow standard normal approximations have been used mccuen et al 2006 to calculate the p values of the nse statistic using a transformation very similar to the fisher z transformation fisher 1992 that is used for approximating the distribution of pearson s correlation these transformations are limited to positive nse values given that nse can be negative this approach is not useful in calculating p values for all underlying population nse values under these conditions it is advantageous to consider non parametric hypothesis tests for region wide studies where distributions may change between watersheds and even time periods 3 2 non parametric bootstrapping this toolkit estimates the null distribution for three performance measures to evaluate whether a metric e g nse from either loadest or wrtdsis significantly different than models having no skill in predicting the observed variability null distributions for the metrics used in the toolkit will be closely centered around zero although in most cases there will be some positive bias in each metric coming from the model s random ability to capture the observed variance hence using a p value provides the criteria for testing whether a performance statistic e g nse belongs to a null distribution at a certain alpha level having no skill in predicting the observed variability gronewold et al 2009 specifically the null distribution is comprised of performance metrics from estimates that have no skill in estimating the variability of predictands i e concentration using the given predictors streamflow for instance if we are testing whether the nse from the loadest regression model nseloadest is statistically significant from a null model whose predictors and predictands do not co vary thereby the nse corresponding to no skill equation 4 the corresponding alternative hypothesis is shown in equation 5 this study refrains from defining the null distribution as being equal to zero e g ho nseloadest 0 since the nonparametric resampling creates null distributions that are not exactly centered on zero 4 h 0 n s e l o a d e s t w r t d s n s e n o s k i l l 5 h a n s e l o a d e s t w r t d s n s e n o s k i l l the null distribution of performance measures is created based on n realizations of loadest or wrtds model runs which are fitted uncorrelated predictand and predictor sets each realization having a sample length of n is created by randomly sampling with replacement from the observed concentration dataset with each observation being equally likely realizations are created by randomly sampling from the concentration data since the regression models both take concentration as the calibration data set to fit their respective models this resampling with replacement scheme randomizes the predictand set while keeping the day and observed discharge fixed within the dataset this process removes the time dependency between the predictand concentration and predictors day and streamflow thereby creating realizations whose mean correlation between the predictand and predictor sets approaches zero the detailed outline for the hypothesis testing framework is shown in fig 1 using the uncorrelated predictand and predictors of length n both wrtds and loadest models are fitted first and then the performance of the fitted model were evaluated using nse ρ and index of agreement ioa by comparing with the observed predictand this process is repeated for n realizations nloadest 100 and nwrtds 30 to develop the null distribution of nse ρ and ioa since the predictors and predictands are uncorrelated the performance measures from the null model represent the water quality models random ability to capture the observed variability thereby providing a basis for checking the statistical significance of the water quality model s performance n s e l o a d e s t w r t d s 3 3 performance metrics the three performance measures considered in this toolkit are the pearson s correlation ρ nash sutcliffe efficiency nse and index of agreement ioa pearson s correlation measures the linear correlation between the model estimates pi and the observations oi shown in 6 correlation values range from 1 to 1 with a value of 1 signifying perfect correlation and a value of 0 indicating no correlation between model and observed values 6 ρ i 1 l o i o p i p i 1 l o i o 2 i 1 l p i p 2 nash sutcliffe efficiency measures the squared deviations of the model values pi to the observed values oi with respect to the squared deviations of observations to the mean of observations o as shown in 7 nse is basically defined as one minus the sum of squared residuals normalized by the observed variance nash and sutcliffe 1970 nse values range from negative infinity to 1 with negative values indicating that the mean value of the observations is a better predictor than the model nash and sutcliffe 1970 models that just predict the mean of the observations and do not capture any of the observed variability will result in an nse value of zero in this case we will call this a model as one with no skill 7 n s e 1 i 1 l o i p i 2 i 1 l o i o 2 the reason for presenting both pearson s correlation and nse is to examine the amount of bias deviation between the model and observation since the difference between the two is just the square of the bias as the bias reduces square of the pearson s correlation approaches nse krause et al 2005 in hydrologic predictions the nse can be very sensitive to large deviations in observed and predicted values for high flow which can cause negative values legates and mccabe 1999 the index of agreement ioa was proposed to overcome the sensitivities of the nse statistic by using the potential error in the denominator willmott 1984 seen in 8 the range of ioa is from 0 to 1 with a value of 0 indicating no correlation 8 i o a 1 i 1 l o i p i 2 i 1 l p i o o i o 2 when using the nse statistic for comparison certain conditions will cause the null distribution to center around negative one rather than zero if the expectation of the model estimates y i ˆ from loadest or wrtds approach the observed mean with a variance close to zero then the nse value will approach zero as shown in 9 however if the variance of the model estimates approach that of the observations the null distribution will be centered on negative 1 while still showing a pearson s correlation distribution centered on zero shown in 10 9 e y i ˆ μ o b s v a r y i ˆ 0 n s e 0 10 e y i ˆ μ o b s v a r y i ˆ σ o b s 2 n s e 1 essentially the numerator in 7 will become twice the denominator and the value will become negative one the variance of the wqn data sets for each station are much less than 1 0 09 and the variance of model estimates are an order of magnitude smaller essentially being close to zero 0 009 using the bootstrapping framework to produce concentration estimates results in a scenario shown in 9 where the nse is centered near zero load estimates form loadest or wrtds not only reproduce the observed mean but also the observed variance while still having no correlation under this scenario the nse will be centered on negative 1 shown in 10 the index of agreement has been included as another performance metric since it does not have this issue and is provided for a more robust analysis 3 4 p values and skill score using a non parametric bootstrap outlined in fig 1 allows for the realizations to have null distributions centered towards zero for pearson s correlation nse and ioa since this is a non parametric re sampling technique the p value for the performance statistics are computed empirically instead of assuming an underlying distribution for model estimates and observations although ρ and nse can be positive or negative the p value is calculated using a one tailed test recall that negative values of ρ and nse indicate models estimates that are worse than just the observed mean so the user should only be interested when the performance statistic is positive using a significance level α of 5 any test statistic having a p value less than 0 05 can be considered not belonging to the null distribution a critical value for the performance statistic can be calculated based on α which specifies the threshold value on the right tail of the null distribution statistic values that are to the left of the threshold value are considered to part of the null distribution for that given α thereby insignificant furthermore this study introduces a skill score shown in 11 which can be useful for comparison between two different models that have different null distributions for the same performance statistics ps denotes the performance statistic and ps critical denotes the right tailed critical value of the performance statistic for the chosen α the skill score ranges from 0 to 100 with a score of 0 indicating that performance statistic is centered on the critical value and a score of 100 indicating a perfect performance metric 11 s s p s p s model p s c r i t i c a l 1 p s c r i t i c a l 100 since a value of 1 indicates a perfect performance metric for all three metrics discussed in this paper the form of equation 11 does not change negative values for ssps indicate that the performance metric belongs to the null distribution and will have a corresponding p value greater than α since p values are primarily used for hypothesis testing and not for indicating the strength of performance metrics the ssps was developed to provide a standard scale for comparing performance metrics between different models 3 5 autocorrelation using data sets that have significant autocorrelation should preserve the autocorrelation in the resampling technique concentration measurements from the wqn are from the monthly to seasonal scale over multiple decades and any significant lag correlation can be interpreted as purely coincidentally in some cases where stations have records from the same month they are never on consecutive days or do not occur over the entire period enough to create significant lag correlation if using a data set with consecutive daily measurements or with reasonable lag correlation we recommend moving block bootstrap sampling in the described framework the block size for resampling is set to 1 meaning that the only one observation is resampled at a time from the observations the framework code is provided and allows for the manipulation of the block size for data sets with significant lag correlation a test for autocorrelation is recommended for each data set before using the toolkit the moving block bootstrap option used in this study is very similar to the framework used to preserve lag correlation in studies for addressing uncertainty in trend analyses for concentration and load hirsch et al 2015 vogel and shallcross 1996 for example say daily concentration values for tar river at tarboro nc had significant lag 3 correlation then the resampling technique would resample 3 neighbors of observed concentration in a single sample for creating a realization so to construct observed concentration of length n by resampling we then resample only n 3 times further details of the moving block bootstrap can be found with the source code but is not discussed further as most observed water quality data sets are discontinuous with daily observations being far apart e g once in two months 4 toolkit interface a performance toolkit for the purpose of testing the significance and comparing models was developed for open source distribution to academics and commercial water quality modelers although the toolkit is constructed for assessing concentration regression models outlined in section 2 the bootstrapping framework was applied for testing the significance of loadest load estimations to show that performance was significant using the non parametric re sampling technique outlined and then applying a kernel smoothing function to the histogram reveals a shape and features that may not be so easily discoverable otherwise fig 2 shows the general framework for uploading data and the options users can choose to evaluate model performance users first upload observed streamflow and concentration data using an excel or csv file the number of simulations and the alpha level can be specified with recommended values of n 100 when using loadest and n 30 when using wrtds and with an α 0 05 the toolkit does not run the loadest and wrtds models internally rather it communicates with the fortran executable and an r complier respectively for each model program the wrtds model takes longer to run and requesting a large number of simulations 30 can greatly increase computation time the user can choose to define null distributions for three statistics pearson s correlation ρ nash sutcliffe efficiency nse and index of agreement ioa after the toolkit runs the re sampling method outlined in fig 1 the nonparametric distribution is plotted with visible kernel smoothing curves shown as blue lines and black density dots displayed along the bottom based on the histogram as shown in fig 4 a summary of all performance statistics p values and skill scores are provided in a result after the distributions are plotted the toolkit first runs loadest realizations since these runs are quicker then a model comparison option is offered where the user can choose to run the wrtds model and compare using the p values skill scores akaike information criterion aic and bayesian information criterion bic all presented in a model comparison table as shown in fig 2 akaike 1981 the toolkit interface was built using the guide feature in matlab shown in fig 3 and is offered as a windows executable that can be used stand still 5 application and results loadest and wrtds regression models were used for predicting tn concentrations over 18 stations belonging to the wqn over the southeastern united states for additional details on the 18 stations see supplementary material application of the loadest model in estimating load concentration for 18 stations in fig 5 fig 6 show the nse varies from 0 79 to 0 98 0 01 0 17 indicating performance of load estimation being better than that of concentration estimates in explaining the observed variability of the respective attributes however p values obtained from the application of the nonparametric toolkit show that the nse values are statistically different from zero for 12 of the 18 sites i e reject the null hypothesis fig 6 thus a rigorous hypothesis testing of the reported skill is necessary for reporting the water quality model performance the developed nonparametric toolkit is useful in discerning whether the skill is statistically significant without requiring any distribution assumption to assess the model performance given that the re sampling approach relies on no lag dependency in the model data caution should be placed while using the modeled data with continuous daily values since observations are often discontinuous in the water quality data and the performance statistic estimation requires comparison with observed data the lag correlations are not going to be significant in this case in case if the lag correlations are significant we suggest considering moving block approach for re sampling hirsch et al 2015 vogel and shallcross 1996 application also demonstrated the usefulness of skill score 11 in comparing the model performance using different performance metrics fig 7 and in comparing the performance of two different models fig 8 loadest and wrtds estimated using the same metric nse lower values for the skill score indicate that they are approaching the threshold for failing to reject the null distribution and higher values mean the performance statistics are approaching a value of 1 skill scores fig 7 comparison between correlation and nse obtained in predicting concentration shows tremendous variability across the region but the performance of two statistics are consistent stations having negative skill scores all correspond to having p values above 0 05 and thus fail to reject the null hypothesis specifically station 7 satilla river at atkinson ga has a negative skill score for ρ but a slightly positive skill score for nse since the positive skill score for station 7 is so close to the threshold 1 this station is considered to have insignificant performance the skill scores for stations 6 falling creek near juliette ga for both performance metrics have magnitudes larger than 10 indicating the individual metrics are more toward the center of the null distribution skill scores comparing the two concentration models fig 8 loadest and wrtds show that wrtds model perform slightly better in estimating the observed concentration across all stations except station 17 station 17 has the shortest record of observations 55 among the 18 stations and the wrtds has difficulty making meaningful predictions using short records the primary advantage of using the skill score is that the water quality models performance can be compared and evaluated across performance statistics as well as across models 6 discussion and conclusion this study recognizes the inherent problem of data availability when dealing with water quality observations and stresses the need for longer continuous data sets this study promotes the use of a non parametric re sampling approaches as opposed to using parametric tests for evaluating model performance as it is difficult to accurately quantify the underlying model with infrequent and limited observations this version of the toolkit was developed for water quality concentration regression models and does not easily apply to load estimation models specifically this pertains mostly to the calibration and estimation files for loadest load estimation which are formatted differently a toolkit for the purposes of assessing loading regression models may not be as useful considering that load estimation models perform much higher than concentration estimation models as proven by the case study if the tap crm toolkit is used for data sets that have significant autocorrelation options to use a moving block bootstrapping approach are available in the matlab source code and the block size should be modified using the non parametric re sampling approach for hypothesis testing the toolkit and the proposed skill scores provides a means of comparing a given model s performance with different performance statistics as seen in fig 7 as well as in comparing multiple models performance under a given statistic as seen in fig 8 it is reassuring to see that given the low values of nse the model s skill is still statistically significant the evidence provided by the toolkit helps establish expected results for modelers focusing on concentration which is a critical variable for downstream habitats as more waters become impaired due to anthropogenic influences we can expect more observations to be available for modeling and expect the values of performance to increase but until then water quality modelers must discover ways on how to deal with the currently available data to make the best informed decisions acknowledgments we would like to thank the national science foundation for supporting this project under the career grant climate informed uncertainty analyses for integrated water resources sustainability no 0954405 contributors d a l a s bjr and s a designed the research d l wrote the code for the research d l a s and s a analyzed the results d l a s and s a wrote the paper appendix a supplementary data the following is the supplementary data related to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 05 013 
26361,we introduce an explicit indicator and the land use management support system to assess the resource use efficiency of land use rue at the landscape scale to estimate rue we relate land use performance with regard to ecosystem services indicators to the maximum possible land use performance based on an optimised land use configuration the test application of the rue assessment in the haean catchment south korea shows that the land use system s rue could be increased by 11 for both nitrate and sediment loss the estimated headroom could indicate whether potential contaminant reduction targets for the downstream water reservoir lake soyang could be achieved with the current land use system the recurring rue assessment for a given region might indicate the effectiveness of spatial planning and policy measures to improve the rue in that region future work should address the integration of rue into a participatory spatial planning or resource management framework keywords resource use efficiency spatial planning policy development haean catchment spatial optimisation lumass software and data availability land use management support system lumass software lumass developer alexander herzig s corresponding author year first available 2012 hardware pc notebook 4 gb ram 1 gb disk space software required windows 64bit or linux program language c program size 30 mb source 312 mb windows 64bit installed licence gnu general public licence v3 gpl availability source code windows 64 bit binaries download https bitbucket org landcareresearch lumass haean dataset lumass spatial optimisation howto the haean dataset and the lumass spatial optimisation settings files used to compute the data discussed in this paper are available as part of the lumass spatial optimisation howto authors optimisation howto alexander herzig haean dataset ganga ram maharjan trung thanh nguyen sebastian arnhold bumsuk seo thomas koellner john tenhunen licence optimisation howto creative commons attribution cc by 4 0 haean dataset creative commons attribution noderivs cc by nd 4 0 download https bitbucket org landcareresearch lumass 1 introduction human population increase and economic growth agendas e g mbie 2015 increase the pressure on natural resources and ecosystems around the world mea 2005 ever more production needs to be realised from the same finite amount of natural resources at the same time environmental conditions need to be improved or at least maintained e g nzgovt 2012 hence natural resources have to be used more efficiently to maximise the provisioning of ecosystem goods and services ecosystem services mea 2005 but how efficiently are we using natural resources in a given landscape how do we measure resource use efficiency at the landscape scale in resource and ecological economics in particular different approaches have been developed to assess and analyse the environmental efficiency of production processes with regard to natural resources these include simple ratios relating resource use or environmental impact to production output or net revenue often referred to as eco efficiency indicators tyetca 1996 oecd 1998 wbcsd 2000 material or energy and exergy balances coelli et al 2007 lauwers 2009 hoang and alauddin 2012 and frontier based efficiency analyses lansik and wall 2014 berre et al 2015 coelli et al 2011 proposed a modified data envelopment analysis dea charnes et al 1978 to assess the environmental efficiency of italian provinces more recently nguyen et al 2012 have used this dea approach to assess the environmental efficiency of south korean rice farms and to determine the trade offs between economic efficiency maximized profit or minimize production costs and environmental efficiency minimize negative environmental impacts while frontier based approaches are often used to analyse and compare the environmental efficiency of individual enterprises of a particular business sector e g coelli et al 2007 hoang and nguyen 2013 eco efficiency indicators are often used to compare the environmental impact of economic growth of different regions or countries e g unescap 2009 despite their success in fostering resource use reduction and decreasing environmental impact of businesses mcdonough and braungart 1998 huppes and ishikawa 2007 point out that there is a disconnection between environmental targets at larger scales and eco efficiency improvements at smaller scales mcdonough and braungart 1998 argue that the concept of eco efficiency does not overcome the conceptual ill design of industrial production therefore eco efficiency could only slow down environmental degradation rather than actually lead the way towards sustainability mcdonough and braungart 1998 in fact eco efficiency indicators and environmental efficiency measures as mentioned above henceforth simply referred to as eco efficiency indicators solely represent the demand for natural resources and ecosystem services they are not linked to the landscape ecological processes delivering the ecosystem goods and services consumed in the production process eco efficiency indicators do not account for the stock of natural resources or the ecosystem s potential to provide ecosystem services for example the abatement of adverse effects of the production process on the environment likewise coelli et al 2007 p 10 note 17 point out that their farm level environmental efficiency score calculated for belgian pig finishing farms does not directly correspond to environmental damage because of the different locational characteristics of the analysed farms such as soil type and topography it is impossible to infer the state of the environment in a particular region from the eco efficiency of the businesses in that region even if all businesses in a particular region improve their eco efficiency the region s natural resources may still be unsustainably used hence eco efficiency indicators provide little information on the resource use efficiency in a given region the challenge is to link farm scale management and technological aspects with landscape scale environmental and socio economic objectives to address this issue several case studies have been presented that employ land use optimisation algorithms to achieve landscape scale environmental and economic objectives while explicitly accounting for farm scale management and technological aspects such as fertiliser regimes crop rotations and irrigation respectively e g seppelt and voinov 2002 roetter et al 2005 lautenbach et al 2013 herzig et al 2016 land use optimisation is driven by the spatially varying ecological conditions across the landscape and optimises the match of land use with the landscape s potential to provide ecosystem goods and services thereby it implicitly improves the productivity of the landscape and also the efficiency with which the landscape s natural resources are used to provide benefit to human well being 2 2 we understand human well being as defined by mea 2005 p v human well being is assumed to have multiple constituents including the basic material for a good life such as secure and adequate livelihoods enough food at all times shelter clothing and access to goods health including feeling well and having a healthy physical environment such as clean air and access to clean water good social relations including social cohesion mutual respect and the ability to help others and provide for children security including secure access to natural and other resources personal safety and security from natural and human made disasters and freedom of choice and action including the opportunity to achieve what an individual values doing and being e g polasky et al 2008 bryan et al 2015 herzig et al 2016 however to our knowledge there is no explicit measure available that quantifies resource use efficiency of land use consequently if we cannot measure resource use efficiency it is difficult to assess whether any land use changes improve resource use efficiency or not moreover it is impossible to estimate the potential headroom for efficiency increases in this paper we introduce a new indicator to explicitly estimate the resource use efficiency of land use rue at the landscape scale sections 2 1 and 2 2 additionally we introduce a sequence of analysis steps to support the exploration of land use performance by multi scenario analysis reflecting different regional objectives and expectations section 2 6 the information derived from this analysis might be used to support regional planning for example to assess the impact of agricultural intensification on the land use performance potential herzig et al 2016 or the impact of urban expansion on the provisioning of ecosystem services curran cournane et al 2014 to facilitate the assessment of rue and multi scenario analysis we introduce the free and open source land use management support system lumass section 2 3 it is intended to enable planning professionals with geo data processing skills to assess rue and conduct multi scenario land use performance analysis we demonstrate the assessment and evaluation of the indicator through an academic case study using a test data set of modelled ecosystem services indicators for the haean catchment south korea sections 2 4 and 2 5 additionally we indicate at which stages of the assessment stakeholder input is required to potentially inform spatial planning or policy development nevertheless stakeholder engagement and strategies to incorporate rue assessment into spatial planning and policy development processes are beyond the scope of this paper and subject to further research intensive agriculture in the haean catchment has been identified as a major source of sediment and nutrient contamination in the downstream water reservoir lake soyang we use the haean test data set to demonstrate in general how the assessment of rue can be applied to i assess the land use performance limits with respect to individual ecosystem services ii assess the potential maximum environmental and iii socio economic land use performance as well as iv identify potential trade off scenarios between environmental and economic land use performance section 2 6 more specifically in the context of the haean test application we show that the rue assessment provides estimates of the maximum possible individual and combined reduction of sediment and nutrients for the current land use this could inform spatial planning and policy development whether the current land use system provides enough headroom to achieve potential sediment and nutrient reduction targets for lake soyang while maintaining or enhancing the livelihood in the haean catchment 2 material and methods 2 1 conceptual overview our well being depends on the goods and services provided to us by the landscape ecosystems such as the provision of food and fibre or the regulation of water de groot et al 2002 mea 2005 nguyen et al 2013 we realise these benefits i e the goods and services by the direct or indirect use of or interaction with the ecosystem for example we pass our leisure time by swimming in a lake or river or we feed ourselves with produces grown by a farmer on fertile soils the provisioning of these ecosystem services is underpinned by ecosystem processes de groot et al 2002 that utilise and or produce natural resources e g water air minerals soils plants etc depending on the type of land use and the associated management practices different types and quantities of resources are required to provide these services for example different crops have different demands for nutrients and water to ensure specific production levels additional water stored in aquifers rivers or lakes in the landscape may be provided to the plants however water abstraction from a river reduces its water level and may impact on recreational benefits downstream e g swimming other uses may have a positive impact on other ecosystem services for example growing trees not only provides fibre but also sequesters carbon and thus contributes to climate regulation and benefits human well being by mitigating climate change hence any type of land use is linked to the natural resources of the ecosystem where the use occurs this also applies to human infrastructure such as houses and roads even though they may not utilise the natural resources in the region in which they occur they may obstruct their use for example the loss of fertile soils through urban development land use provides a certain benefit to human well being and positively and or negatively impacts on the ecosystem s potential to provide ecosystem services in the future by affecting its natural resources society has to decide which benefits i e services are important for people s well being and which land use impacts on the environment are tolerable or intolerable in the context of the call to use natural resources more efficiently to satisfy the increasing demand for ecosystem services two questions arise i how efficiently are we using our natural resources currently and ii how do we measure it to address these questions we have first to clarify what we mean by efficiency since the concept and definition of it is used differently across disciplines heijungs 2007 in this paper we refer to the general formal definition given by heijungs 2007 p 87 efficiency refers to the degree of optimality of a system it can be a quantitative indicator in which case it is a dimensionless pure number measured on a ratio scale bounded between 0 and 1 and higher values signifying a higher degree of optimality in line with this definition we understand the resource use efficiency of land use rue as the degree of optimality with which land use utilises the available natural resources in a landscape to provide benefits to human well being the resource use efficiency of a given region is higher the more benefit the land use in that region provides thereby the benefit is defined by the values expectations and objectives of the stakeholders in that region potential stakeholders could be national or regional government or agencies e g ministries council businesses community representatives or other regional interest groups we estimate rue by relating current land use benefits to the maximum possible land use benefits which are in line with the stakeholders expectations and objectives section 2 2 we note that the maximum possible land use benefit refers to the maximum attainable land use benefit which can be derived from the landscape ecosystem by the given land use it is not equivalent with the theoretical bio physically possible maximum benefit to operationalise the assessment the stakeholders values expectations and objectives have to be translated into a set of environmental and socio economic land use performance indicators and associated thresholds representing minimum performance levels or maximum tolerable environmental impacts for the given region stakeholder input could be best achieved by adopting a collaborative modelling approach e g cockerill et al 2009 voinov and bousquet 2010 voinov et al 2016 in that context for example schiller et al 2001 discuss how to communicate ecological indicators to stakeholders thomson 2005 discusses indicator based knowledge management for participatory decision making and harmsworth et al 2016 describe the use of values based frameworks to derive environmental limits connected to cultural values in agriculturally dominated areas environmental indicators may include nitrate leaching soil erosion greenhouse gas emissions or water demand socio economic indicators may include crop yield gross margin or net revenue because of the quantitative nature of the proposed rue indicator qualitative indicators e g to represent social and cultural values cannot be easily included directly in the assessment such values may either be included as spatial constraints in the assessment of the maximum possible benefit s sections 2 3 and 2 5 for example to exclude cultural heritage sites from land use change or have to be assessed during the evaluation of the rue assessment to assess the maximum possible land use benefit we use multi objective spatial optimisation conceptually this not only allows the linking of farm scale management practices with landscape scale objectives but also enables the integration of regional objectives and expectations into the assessment e g roetter et al 2005 s also section 2 3 because of the different values of different stakeholder groups and or the uncertainties associated with the translation of those values into regional objectives and expectations there is no single optimum land use configuration rather there are many to account for the variety of possible scenarios we employ a step wise procedure comprised of four major steps sections 2 6 and 3 1 to 3 4 i exploring limits ii maximising environmental benefit iii maximising socio economic benefit and iv identifying trade offs in the first step we assess the maximum possible land use performance with respect to each individual socio economic and environmental indicator and estimate the associated headroom collectively the results provide an estimate of the land use performance limits to derive ecosystem services for human well being from the landscape based on these results opportunities for increasing rue across the landscape subject to the specified objectives are explored sections 2 6 and 3 2 to 3 4 the associated land use scenario maps alongside the rue scores of the associated indicators may then be evaluated by the stakeholders to identify more resource use efficient scenarios for land use development in the given region the use of land use scenarios for stakeholder participation is for example discussed by griewald et al 2017 king et al 2015 discuss the application of a framework for assessing ecosystem services trade offs cavendar bares et al 2015 in terms of stakeholder preferences and associated challenges and opportunities 2 2 resource use efficiency of land use we define resource use efficiency as the ability of land use to utilise the natural resources of a landscape to derive benefit for human well being thereby land use represents the allocation of land cover such as vegetation or buildings and human activities such as farming or recreation across a landscape benefit represents the ecosystem services derived from the landscape by the given land use we estimate rue as the ratio of land use performance over maximum land use performance eq 1 the latter is represented by the spatially optimised land use configuration i e the optimum allocation of land cover and human activities across the landscape however the optimum land use configuration is subject to the expected land use performance lup e with regard to a set of criteria the latter comprises landscape specific environmental and socio economic aspects of land use such as the provision of fresh water or food production and is represented by a set of indicators i we distinguish two types of criteria i benefit criteria which associate increasing indicator values with greater benefits such as crop yield t yr 1 and ii cost criteria which associate decreasing indicator values with greater benefits such as total nitrate loss kg n yr 1 land use performance and hence resource use efficiency are assessed individually for each indicator 1 r u e i e l u p i l u p i e o p t w i t h i r e p r e s e n t s b e n e f i t c r i t e r i o n a l u p i e o p t l u p i w i t h i r e p r e s e n t s c o s t c r i t e r i o n b with r u e i e 0 1 i 1 2 lupi gi land use landscape 3 lupi e opt gi opt land use landscape e i set of environmental and socio economic performance indicators e set of expected outcomes objectives and constraints rue i e resource use efficiency with respect to indicator i and expectations e lup i land use performance with respect to indicator i lup i e opt optimum land use performance with respect to indicator i and expectations e g i performance evaluation function for indicator i opt spatial land use optimisation function subject to e the headroom hr i e for increasing resource use efficiency can thus be expressed as the land use to be assessed for resource use efficiency may be the current or any hypothetical land use scenario the optimum land use represents the spatial configuration of land cover and human activities that derives the maximum benefit from a given landscape subject to regional expectations e the headroom represents the unused potential of land use and indicates the maximum possible performance increase subject to regional expectations e for brevity we subsequently use the term land use system to refer to a set of land covers and management practices i e land use and associated regional expectations with regard to a given set of environmental and socio economic criteria in a given region 2 3 spatial land use optimisation with lumass lumass is free and open source software to support scientists in understanding how ecosystems on the landscape scale work and how they react to management activities methodologically system understanding and impact assessment are commonly facilitated by ecological modelling jørgensen 1994 powers et al 2011 ausseil et al 2013 which is supported by lumass through its spatially explicit system dynamics modelling framework herzig and rutledge 2013 it allows users with gis skills to implement complex bio physical models for investigating system behaviour and management impact in addition land mangers need to decide on how land cover and management activities are best allocated across the landscape to maximise planning objectives and meet minimum performance constraints such as minimum production targets or environmental limits in this paper we focus on the latter aspect and discuss the implementation and use of lumass multi objective spatial optimisation framework for spatial planning and policy development support to solve multi objective spatial optimisation problems lumass uses the open source mixed integer linear programming solver lp solve berkelaar et al 2005 consequently lumass represents spatial optimisation problems as linear programs herzig et al 2013a as follows we seek x that maximises c j x which is 4 m a x c j x w i t h x b w h e r e b x ℝ u ax b x 0 b ℝ q where x is the vector of decision variables containing a variable x fl for each land use parcel combination with f f and l l f denotes the set of available parcels and l the set of land uses to be allocated across the landscape c j represents the vector of performance scores for objective j it contains the coefficients of the decision variables expressing the benefit associated with the allocation of one unit of a particular land use e g hectare or square meters to a particular parcel with regard to the particular objective multiple objectives are accounted for either by weighing individual objective functions and adding them to a single objective function 5 m a x j λ j z j w i t h λ j ℝ n λ j 0 j λ j 1 z j c j x x b or by sequentially solving single objective optimisation problems s eq 4 in the order of their relative importance and then adding them one by one as constraint z v ε v to the subsequent optimisation problem 6 m a x z j w i t h x b x ℝ u z j c j x z v ε v v j ε ℝ n the set b of feasible solutions of the optimisation problem is constrained by the inequality cf eq 4 7 ax b with x ℝ u x 0 b ℝ q where the matrix a comprises the coefficients of the decision variables associated with the q constraints and b represents the vector of constraint thresholds lumass supports two types of user defined constraints i allocation constraints and ii performance constraints the spatial validity of allocation and performance constraints may also be restricted to arbitrarily defined zones allocation constraints control the total area b kd of a set k of land uses which is allocated to a set d of parcels 8 d ε d k ε k x k d b k d w i t h x k d b k d ℝ b k d 0 k l d f where x kd denotes the area share of land use k allocated to parcel d this type of constraint can be used for example to control the allowed type of land use conversions or to configure a minimum or maximum threshold of a particular type of land use s to be allocated to a particular region besides the direct control of the land use allocation by area thresholds lumass also supports the indirect control of the land use allocation with performance constraints similar to objective functions eqs 4 to 7 performance constraints link performance indicators e g s kdj of a particular criterion j with the allocated area share of a particular land use k to a particular parcel d performance constraints can also be configured for arbitrary subsets of the available parcels comprising an arbitrary subset of land uses to which they apply 9 d ε d k ε k s k d j x k d b k d j w i t h s k d j x k d b k d j ℝ b k d j 0 k l d f they can be used to ensure a minimum desired performance for example minimum production quantities or they can be used to limit undesirable effects of land use such as soil erosion to facilitate the evaluation of land use optimisation lumass provides for each optimisation scenario a region based summary statistic with regard to the current and optimal land use performance and their absolute and relative difference for each individual criterion j the generated text file comma separated values can then be easily imported into spreadsheet applications to calculate rue additionally lumass calculates a land use change matrix for each computed scenario to support the analysis of spatial effects of land use optimisation for the assessment of sensitivities of land use pattern to input data uncertainties and optimisation constraints herzig et al 2013b lumass provides specific batch optimisation options to automatically compute multiple realisations and or variations of a spatial optimisation problem users can specify the uncertainty associated with input performance scores and the total number of simulations per uncertainty lumass then randomly varies the input data according to the specified uncertainty and computes multiple realisations of the optimal land use pattern a similar configuration is possible to assess the effect of constraint uncertainty it enables the automated systematic change of the optimisation constraints to generate a series of similar optimisation scenarios which help to explore possible trade offs between objectives this mode is essentially equivalent to the calculation of pareto fronts as performed in the application of search heuristics e g lautenbach et al 2013 since lumass employs linear programming individual points on the pareto front can be directly calculated and do not have to be approximated as is the case with search heuristics there is no fixed limit to the size i e the number of objectives land uses parcels or constraints of spatial optimisation problems which can be addressed with lumass nevertheless the larger the problem is the more likely that the underlying optimisation library lp solve will fail to solve it berkelaar et al 2005 the largest problem we have successfully solved with lumass lp solve had 12 land use options 13 322 parcels one objective two objective constraints 25 performance constraints and 26 644 allocation constraints note that in this setting we have used the ɛ constraint model eq 6 to effectively account for three objectives in total depending on the actual values of the constraint thresholds the runtime of an individual scenario on a virtual machine with 2 4 ghz intel processor and 16 gb of ram was in the range of a few to up to 20 min lumass optimisation settings are specified in a user configurable text file and can be used to run optimisation scenarios in the lumass desktop environment or as multi threaded batch processing jobs using the lumass command line programme e g in server or cluster environments 2 4 case study area haean catchment to test the assessment of rue and gauge its potential value for supporting spatial planning and policy development we apply the proposed approach to a test data set of ecosystem services indicators for the haean catchment in south korea it is located in the northeast of the kangwon province south korea fig 2 land cover in the haean catchment is dominated by deciduous and coniferous forests 57 and covers primarily the steep hillslopes agricultural land covers approximately 28 20 dryland fields and 8 rice paddies primarily at hillslopes near the forest edges and flat areas in the catchment centre the remaining 15 are residential water and semi natural areas including grassland field margins and riparian areas maharjan et al 2016 the dominant crops cultivated at hillslopes are annual dryland cash crops primarily soybean potato radish and cabbage and perennials such as ginseng and orchards while the flat areas in the centre are dominated by paddy fields maharjan et al 2016 seo et al 2014 fig 2 the agricultural landscape of the haean catchment is characterised by small fields in most cases 2 ha arnhold et al 2014 nguyen et al 2014 embedded in a mosaic of semi natural landscape elements but also farm roads and a dense network of artificial drainage channels shope et al 2014 the haean catchment is located within the largely forested watershed of lake soyang fig 2 which is the largest reservoir in south korea and important for drinking water provision for a large proportion of the country s population kim et al 2000 maharjan et al 2016 particularly during the monsoon season the reservoir s water quality is affected by high nutrient loads of the soyang river originating primarily from agricultural land within the watershed kim and jung 2007 park et al 2010 the haean catchment has been identified as one of two key contributors to agricultural water pollution with substantial impacts on the trophic state of the lake park et al 2010 main sources are soil erosion and nitrate loss from dryland agricultural fields cultivated with annual crops kettering et al 2012 arnhold et al 2013 2014 ruidisch et al 2013 kim et al 2014 both sediment and nitrate are rapidly transported from agricultural fields to the stream network due to the high connectivity by artificial drainage channels and short travel distances to streams shope et al 2014 2 5 ecosystem services assessment due to its importance for agricultural production and its role as headwater catchment of the soyang river the current management in the haean catchment constitutes a major conflict between food and fresh water provisioning as well as erosion control ecosystem services mea 2005 insufficient erosion prevention and capacity to maintain soil fertility have led to the deterioration of the long term production of provisioning services of the catchment maes et al 2012 raudsepp hearne et al 2010 we apply the rue assessment to demonstrate the potential information it could provide to support decision making in spatial planning and policy development in this context since our test application did not involve stakeholder engagement we selected five indicators to represent the competing demands for erosion control fresh water provisioning and food production i e sediment yield surface runoff total nitrate loss biomass and crop yield because biomass and crop yield alone do not inform about the income and livelihood situation of farmers we additionally introduced gross margin as indicator for the well being and quality of life of haean residents díaz et al 2015 we also accounted indirectly for cultural services in our assessment by acknowledging the importance of rice as main staple crop in korea kettering et al 2012 nguyen et al 2012 hence we left rice paddies unchanged regardless of their ecological or economic performance and our analysis focused instead on the four major cultivated annual cash crops i e soybean potato radish and cabbage these crops also form an important part the korean diet kettering et al 2012 and were identified as major sources of water quality degradation in the haean catchment section 2 4 we used the soil and water assessment tool swat arnold et al 1998 gassman et al 2007 to quantify the six performance indicators underpinning the rue and land use trade off analysis its bio physical output variables can be directly related to provisioning and regulating ecosystem services francesconi et al 2016 vigerstol and aukema 2011 the swat model setup and parameterisation for the haean catchment including input data collection resolution and data sources have been previously described by shope et al 2014 and maharjan et al 2016 however since our rue test application predates the completion of the work described by maharjan et al 2016 we used an earlier model setup it only modelled nitrate loss via surface runoff and interflow and did not account for nitrate leaching to groundwater and did not benefit from a more recent re calibration it shows about 20 and up to 68 different values for the average sediment loss and crop yield per hectare and year for the crops considered in this study as input to the spatial optimisation procedure we also assessed the potential performance of each of the four major cash crops in terms of each of the six indicators for each swat hydrological response unit hru to calculate the gross margin we multiplied the simulated crop yield for each hru with the market price for the individual crops and subtracted the total production costs e g labour costs land rent fertiliser input etc table 1 market price and production costs are averages for the years 2009 2011 and were obtained from an interview survey of more than 300 farmers in the haean catchment nguyen et al 2014 as swat calculates crop yield as dry biomass we converted the simulated yield outputs into fresh biomass equivalents lindner et al 2014 arnhold et al 2014 to make the swat model outputs available to lumass we rasterised the modelled potential performance for each of the four crops with regard to each of the six ecosystem services indicators at the hru level we used a raster resolution of 30 m which equals the resolution of the digital elevation model that was used to delineate the swat hrus for the hean catchment shope et al 2014 we summarised each of the 24 input performance layers at the land parcel level to represent the six vectors c eq 4 of performance scores required by the lumass optimisation component thereby we only included those parcels which were covered by the four cash crops considered in this study 1205 parcels 492 ha to assess the effect of this procedure on the modelled performance of land use scenarios by lumass we compared the performances of the baseline land use scenario modelled with swat and lumass with respect to the considered cropping area the comparison showed deviations of the lumass results from the swat results of about 2 to 6 across the range of indicators and crops the performance of other scenarios may show a bigger deviation from the corresponding swat model result since the swat model setup used in this study does not represent the actual land use performance with regard to the selected indicators well enough the results of the rue test application should not be used to inform actual decision making in the catchment nevertheless the data set reflects the conflicting demands for food and fresh water provisioning as well as erosion control ecosystem services in the catchment and is used here to exemplify the application of rue and the information it could contribute to support spatial planning and policy development in addressing this problem 2 6 resource use efficiency assessment in the haean catchment as outlined in section 2 2 each optimum land use configuration is subject to objectives and constraints in terms of the expected benefit derived from it thus no single optimum land use configuration exists therefore we employed a step wise procedure to characterise the resource use efficiency of the given set of land uses for different objectives table 2 reflecting the competing demand for ecosystem services observed in the catchment section 2 5 2 6 1 exploring limits in a first step we modelled the optimum land use configurations for each individual indicator considered in this study scenarios s1 to s6 i biomass ii crop yield iii gross margin iv total nitrate loss v sediment yield and vi surface runoff we determined the objective of each individual optimisation scenario depending on the type of the performance indicator benefit indicators such as crop yield or gross margin were subject to maximisation whereas cost indicators such as nitrate loss or sediment yield were subject to minimisation 2 6 2 maximising environmental benefit in a second step we modelled the land use configuration which maximises benefit across the set of environmental performance indicators scenario s7 to represent multiple objectives we iteratively employed equation 6 and incorporated the individual objectives indicators in order of decreasing importance i e sediment yield and nitrate loss 2 6 3 maximising economic benefit we represented an economically focused land use scenario by maximising for gross margin scenario s8 while maintaining the environmental performance s above of the land use configuration as at 2010 2 6 4 identifying trade offs scenarios s9 and s10 depict benefit trade offs across the full range of indicators representing a more environmentally focused scenario on one hand scenario s9 and one focused on economic benefit on the other scenario s10 to identify these trade offs we iteratively solved equation 6 with the objective to maximise gross margin the initial objective constraints eq 6 ε were set to represent the maximum environmental benefit achieved in scenario s7 for sediment yield and nitrate loss respectively table 2 s7 then we used lumass batch optimisation capability to automatically relax these constraints by a percent point at a time and solve the associated optimisation problem we configured lumass to continue this procedure until the objective constraints reached their respective baseline performance this yielded a set of optimisation scenarios representing the range of optimal solutions between scenario s7 maximising environmental benefit and scenario s8 maximising economic benefit finally we picked two scenarios i e s9 and s10 from this set to more or less evenly cover the range of gross margin performance between scenarios s7 and s8 and at the same time achieve the best result for both sediment and nitrate loss to account for the current agricultural activity in the catchment we constrained eq 9 each optimisation scenario to achieve at least the same gross margin for each considered land use as in the 2010 land use configuration also to respect cultural values we allowed land use conversions only to happen outside rice cultivation areas and for environmental reasons we disabled the conversion of forested land we evaluated each land use scenario in terms of its resource use efficiency with respect to the individual performance indicators it allowed us to analyse the performance of the current set of land uses in the area and to look for opportunities for land use development towards more efficient use of natural resources in the haean catchment 3 results and discussion 3 1 exploring limits scenarios s1 to s6 table 2 assess the performance limits of the haean catchment with regard to each individual environmental agricultural and economic performance indicator the greatest headroom for efficiency increases fig 3 were modelled for sediment yield 14 and nitrate loss 11 respectively all remaining indicators showed resource use efficiencies of 90 and more the results indicate the limits of the current land use system to derive higher benefits from the natural resources in the haean catchment the estimated limits in fig 3 define the upper boundary for further development of this land use system however optimising land use to maximise the provision of an individual ecosystem service is likely to affect the provision of another ecosystem service this relationship is displayed in fig 4 for example optimising the land use system to solely minimising sediment yield impacts the provision of food fig 4 in that case the land use system uses the natural resources in the haean catchment more efficiently i e 14 relative efficiency increase to reduce sediment yield i e 14 sediment yield compared with baseline table 2 the same land use system fig 4 min sediment yield uses the natural resources in the catchment less efficiently i e 4 relative efficiency increase to produce crops i e 4 crop yield compared with baseline the area indicator in fig 4 represents a cost criterion fig 1 and shows whether individual scenarios can achieve their expected objective and gross margin performance on less area than in the baseline scenario this is the case for the cost criteria nitrate loss sediment yield and surface runoff for example in the nitrate loss scenario s1 fig 4 min no3 loss the gross margin performance can be achieved on 7 less area i e 7 relative efficiency increase with regard to area the unused area cf figs 6 and 9 land use category unused is assumed to be neutral in terms of the considered ecosystem services performance indicators even though this is not realistic in the context of exploring the landscape s potential land use performance it indicates development opportunities for land use types which perform neutral with regard to the considered cost criteria fig 1 in the analysis collectively the upper and lower performance limits of the given land use system define the window of opportunities for further land use development fig 4 in a given area any land use configuration that improves the resource use efficiency of the given land use system with regard to one or more indicators has to lie within this window of opportunities i e the grey area in fig 4 the analysis of the land use performance limits shows whether the given regional objectives and expectations could actually be met with the considered land uses and management practices in a best case scenario in the given region if that is the case the window of opportunities can be searched for feasible trade off scenarios section 2 5 in case any of the objectives or expectations could not be met i e the associated optimisation problem is infeasible the analysis needs to be repeated with a different set of land uses or management practices or reduced expectations in terms of the configured performance constraints in the context of the haean catchment the question would be whether 14 reduction of sediment yield or 11 reduction of nitrate loss would be sufficient to make a significant enough impact on the water quality in lake soyang if that was the case the window of opportunities could be searched for a feasible trade off scenario that could achieve the desired water quality improvements in lake soyang if the maximum possible reduction of sediment and nitrate loss fig 3 were not enough to sufficiently improve the water quality in lake soyang the analysis had to be repeated with a different set of land uses and or management practices e g fertiliser regime or tillage methods or reduced regional expectations such as a lower gross margin threshold to analyse the potential effect on the objectives of sediment and nitrate reduction the results of scenarios s7 to s10 represent a subset of the feasible opportunities and prioritise different ecosystem services along a cross section from an environmental s7 to an economic s8 focus scenarios s9 and s10 characterise the middle ground in this spectrum fig 5 presents the results of these scenarios in terms of the relative efficiency increase cf fig 1 with respect to the individual performance indicators a positive relative efficiency increase indicates that the land use system uses the natural resources in the given region more efficiently and provides an increased performance in terms of the ecosystem services the specific indicator represents for example in scenarios s7 s9 and s10 the land use system uses the natural resources more efficiently to prevent sediment loss which improves the performance of the fresh water provisioning service and achieves a reduction of sediment yield by 11 8 and 7 respectively table 2 chg over baseline 3 2 maximising environmental benefit scenario s7 focuses on the reduction of sediment yield and nitrate loss the two major contaminants in this catchment responsible for environmental problems downstream the results show that the current land use system could be optimised to reduce both sediment yield and nitrate loss in the haean catchment by 11 without any loss of gross margin this improvement is realised by a more productive use of the land by optimising the match of land use with the landscapes potential to provide ecosystem services such as erosion control fresh water provision and food production fig 6a the analysis of figs 6 8 shows that resource use efficiency gains were achieved by a combination of land use conversions radish potato and soybean were moved to areas with relative higher gross margin performance and similar or higher environmental performance fig 7 i e radish to the south east potato to the north north east and south east and soybean to the west and more towards the centre of the catchment fig 6a this allowed for their overall area to be reduced and their performance in terms of sediment and nitrate loss and surface runoff to be improved while still achieving the gross margin of the baseline scenario fig 8 since cabbage shows the overall lowest gross margins compared with the other crops fig 7 it was moved to those areas of the catchment where the lowest gross margins across all crops are achieved to not affect the potential gross margin gains by other crops and hence their opportunity to reduce the overall production area the overall cabbage area was increased fig 8 to achieve the individual gross margin target of the baseline land use scenario consequently cabbage was the only crop whose environmental performance decreased in this scenario furthermore the land use change matrix for scenario s7 table 3 indicates cabbage to be the most out of place crop since only seven percent of the baseline cabbage area remained unchanged and 14 of the cabbage area were taken out of production fig 6a land use category unused potato and radish were the most stable crops with 24 and 26 of their respective areas remaining unchanged as specified by one of the optimisation constraints applied to all scenarios no changes were observed for land use type other nevertheless the area taken out of production of the optimised land use types were added to category other in this statistic in summary the achieved gains in gross margin enabled the reduction of the overall cultivated area fig 8 this in turn allowed for a further reduction of sediment yield and nitrate loss however the estimated environmental benefit is only realised if the land taken out of production is used in a way which is neutral to sediment yield nitrate loss and surface runoff 3 3 maximising economic benefit scenario s8 aims for economic growth of the catchment while maintaining the overall environmental footprint of the baseline land use configuration the optimised land use configuration achieved an 8 increase in gross margin and does not leave any production area unused fig 9a the overall land use configuration looks similar to scenario s7 but its economic focus reveals a slightly different pattern fig 9a and land use conversions fig 8 similar to scenario s7 resource use efficiency of radish and potato were increased to achieve their individual gross margin baseline targets on a smaller area as in 2010 in contrast to scenario s7 soybean area was increased to maximise the overall gross margin performance of this scenario this is also reflected in the land use change matrix of this scenario which shows that between 30 and 36 of the area covered by the individual crops was converted to soybean in contrast to soybean cabbage the least profitable cash crop considered in this study table 1 was moved to areas with relative low gross margins however its overall area was increased even more than in scenario s7 to achieve the baseline gross margin target soybean and potato were the most stable crops in this scenario and 36 and 34 of their respective areas remained unchanged compared with the baseline land use configuration 3 4 identifying trade offs scenarios s9 and s10 represent the middle ground between scenarios s7 and s8 and trade off economic and environmental performance to different degrees fig 5 both improve the overall resource use efficiency of the land use system in terms of all performance indicators except for biomass and crop yield consequently the optimised land use configurations for these scenarios also represent trade offs between those of scenarios s7 and s8 figs 6a and 9a overall the land use patterns of scenarios s9 and s10 are very similar and the differences only subtle in total less area is taken out of production compared with scenario s7 especially in the southern part of the catchment apart from that the overall allocation trends are very similar to scenario s7 the efficiency improvements in terms of sediment 8 and 7 and gross margin 2 and 5 for scenarios s9 and s10 fig 5 is controlled by the different area share of soybean fig 8 the slightly higher soybean production area in scenario s10 leads to a relative higher efficiency increase with regard to gross margin and a slightly reduced efficiency in terms of sediment yield and nitrate loss fig 5 the mediating character of scenarios s9 and s10 is also represented by their respective land use change matrices the differences between scenarios s9 and s10 are very small and the overall character of land use change is here represented by the matrix for scenario s9 table 3 the most stable crop is soybean with 28 and 29 of its area remaining unchanged in scenarios s9 and s10 respectively radish represents the most out of place crop in scenarios s9 and s10 with only 14 and 13 remaining unchanged respectively 3 5 methodological and operational aspects of ruea in sections 3 1 3 4 we summarised and discussed the specific results of the rue assessment for the haean catchment south korea in this section we discuss methodological and operational aspects of the approach with particular focus on its application in land use management in general as described above we used ecosystem services indicators modelled with swat to assess rue for the haean catchment the use of spatially explicit bio physical models for environmental impact assessment and associated challenges such as model sensitivity and uncertainty have been discussed extensively in the literature e g jakeman and letcher 2003 krysanova et al 2004 tennoy et al 2006 de groot et al 2010 discuss the challenge of integrating the ecosystem services concept into planning management and decision making here we focus on the computation of optimal land use configurations as the basis for the assessment of the maximum possible land use performance in particular we address the question how optimal is the optimal land use configuration in the following sections we focus on the methodological and operational aspects of this question with regard to the application of rue in land use management 3 5 1 methodological aspects depending on the type of the objective function constraints and decision variables different algorithms are used to solve spatial optimisation problems e g grabaum and meyer 1998 aerts and heuvelink 2002 holzkämper and seppelt 2007 for example in the case of linear objective functions and constraints linear programming algorithms can be used to efficiently find an optimal solution i e global optimum to the problem steuer 1986 computationally more complex optimisation problems involve non linear objective functions and constraints or integer or binary decision variables i e combinatorial optimisation ehrgott 2005 they are commonly solved using search heuristics such as simulated annealing or genetic algorithms e g aerts and heuvelink 2002 lautenbach et al 2013 which in contrast to linear programming only provide approximated solutions to an optimisation problem with regard to applications in biological conservation prioritisation moilanen 2008 argues that both types of algorithms i e linear programming and search heuristics lead to a suboptimal solution to the real world spatial planning problem although linear programming provides an exact solution suboptimality is introduced by major simplifications in representing the real world problem with linear equations in contrast employing a heuristic algorithm requires only minor simplifications but therefore only yields an approximate i e suboptimal solution moilanen 2008 this analysis is in general also applicable to the optimisation of land use pattern as presented here consequently the degree of suboptimality in calculating the maximum land use performance and hence rue depends on the real world optimisation problem characterised by its type of objectives and constraints and the algorithm used to solve the problem in addition to suboptimality the generated optimal land use pattern and the calculated optimisation benefits are also influenced by the uncertainty associated with the input data and constraints for example herzig et al 2013b have investigated the sensitivity of the optimal land use pattern to variation in the input data for a single objective problem i e minimisation of nitrate leaching they found that with 30 uncertainty associated with the leaching input data the optimisation benefit i e the potential reduction of nitrate leaching was overestimated by about 25 the average allocation probability of a particular land use type to an individual parcel was estimated to be about 75 a study of uncertainty propagation in multi objective optimisation by holzkämper et al 2015 suggests that optimisation results are less robust for multi objective than for single objective problems in summary suboptimality and uncertainty lead to inaccurate optimisation results hence the calculated maximum possible benefit used to estimate rue is subject to uncertainty lumass support to facilitate the assessment of optimisation uncertainties is outlined in section 2 3 3 5 2 operational aspects even if we could compute exactly the global optimum of a land use optimisation problem from an operational point of view the question is does the mathematically optimal solution to an optimisation problem mean the solution is also socio economically or environmentally optimal for example to minimize nitrate loss in the haean catchment mathematically the most optimal scenario would have been to get rid of all land uses considered in this study since each one contributes to the overall nitrate loss however this is clearly not acceptable from a socio economic point of view therefore stakeholder input is crucial to design a representative optimisation problem in the context of the rue assessment for a given region it needs to reflect community and policy objectives for the region which can then be translated into objective functions and constraints of the optimisation problem furthermore regulatory requirements for the particular region such as environmental limits can be integrated as additional constraints into the optimisation problem nevertheless dependence on subjective goals and values means the optimum land use configuration and hence the rue of a given region cannot be calculated objectively while this subjectivity seems to be a weakness of the presented approach to assess rue of land use we argue it is actually its strength and sets it apart from other widely applied indicators or measures for example unlike eco efficiency and environmental efficiency indicators the presented rue assessment provides a means to link community values with the state of the environment it lives in the uncertainty associated with the translation of stakeholder values objectives and expectations into regional objectives and constraints of the optimisation problem section 2 1 creates another source of uncertainty for the calculation of the maximum possible benefit cf section 3 5 1 it adds to the general challenge of the use of spatial optimisation in an operative context to account for the uncertainties associated with regional constraints and objectives we explored trade offs between environmentally and economically focused land use configurations scenarios s7 to s10 using lumass batch processing capabilities outlined in section 2 3 3 6 application of ruea in land use management the rue assessment is targeted at regions and catchments with land uses that utilise natural resources conceptually rue can be applied to any land use type whose performance can be expressed by at least one numerical ecosystem services indicator eq 1 in addition to the land use types tested in this study this includes pastoral farming viticulture and forestry as well as land use type variants characterised by management practices such as irrigation ausseil et al 2013 herzig et al 2013a 2016 the applicability to other land use types e g recreation needs still to be tested the assessment is intended to provide both an estimate of the resource use efficiency and performance limits of a given land use system as well as a sequence of analysis steps to help identify opportunities to increase the resource use efficiency of land use in a given region we suggest an application scale for the assessment ranging from a few hundred to a few hundred thousand hectares at land parcel scale resolution the upper scale limit is defined by the size of the underlying optimisation problem i e the number of land use options parcels objectives and constraints cf section 2 3 to be effective the rue assessment should be embedded in a spatial planning or resource management framework for example at the regional government level the framework should engage stakeholders and planning professionals or science providers in an iterative manner to translate stakeholder values and expectations into performance objectives and constraints of the assessment and to communicate back the evaluated scenario results the recurring assessment of rue over time might indicate the effectiveness of potential policy measures or spatial planning efforts to increase rue of land use in a given region 4 conclusions we presented a novel approach to assess explicitly the resource use efficiency of land use it is estimated as the ratio of land use benefit over the maximum possible land use benefit the resource use efficiency indicates the closeness of a land use system to its optimal performance unlike other eco efficiency indicators it links the farm scale performance of the production process with the landscape scale ecological processes in a given region additionally it can account for regional environmental and socio economic objectives and expectations expressed as numerical values e g herzig et al 2016 furthermore we proposed a systematic step wise procedure that uses rue to explore both the performance limits of a given land use system in a given region and the potential headroom to increase the rue in a given region to inform policy development and spatial planning the proposed step wise procedure should be embedded in a spatial planning or resource management framework that engages stakeholders and incorporates their objectives and expectations into the assessment this would enable testing whether specific stakeholder development objectives are achievable with a given land use system or whether alternative land uses and or management practices needed to be considered to achieve the objectives the test application of the rue assessment in the haean catchment showed for the most important indicators that the rue of the land use system could be improved at maximum by 14 for sediment loss or by 11 for nitrate loss respectively the highest simultaneous relative efficiency increase was modelled for scenario s7 with 11 for both sediment and nitrate loss while maintaining the overall livelihood in the catchment the information on the maximum achievable contaminant reduction could be analysed in terms of its potential impact on the contaminant reduction in lake soyang this in turn would indicate whether the headroom for resource use efficiency increases in the haean catchment is sufficient or whether alternative land uses and or management practices needed to be explored to achieve the targeted contaminant reduction in lake soyang conceptually the presented rue assessment is generic and can be applied in different areas and contexts in particular it is flexible in terms of i the considered set of land uses ii the set of ecosystem services and socio economic indicators iii the bio physical models used to derive ecosystem services indicators iv the region v the objectives and constraints used to represent regional expectations and vi and the spatial optimisation algorithm and software used to generate pareto optimal land use configurations further research is required to assess its applicability across a wider range of land use types and ecosystem services indicators and to operationalise its use in practice acknowledgement we are deeply saddened that our esteemed colleague and co author dr sebastian arnhold passed away in november 2017 we will remember him as a passionate researcher and as a great person we thank the agencies that supported the conceptual work on resource use efficiency including the new zealand ministry for the environment and the new zealand ministry of business innovation and employment s science and innovation group this study was carried out as part of the international research training group terreco grk 1565 1 funded by the deutsche forschungsgemeinschaft dfg at the university of bayreuth germany and the korean research foundation krf at kangwon national university chuncheon south korea 
26361,we introduce an explicit indicator and the land use management support system to assess the resource use efficiency of land use rue at the landscape scale to estimate rue we relate land use performance with regard to ecosystem services indicators to the maximum possible land use performance based on an optimised land use configuration the test application of the rue assessment in the haean catchment south korea shows that the land use system s rue could be increased by 11 for both nitrate and sediment loss the estimated headroom could indicate whether potential contaminant reduction targets for the downstream water reservoir lake soyang could be achieved with the current land use system the recurring rue assessment for a given region might indicate the effectiveness of spatial planning and policy measures to improve the rue in that region future work should address the integration of rue into a participatory spatial planning or resource management framework keywords resource use efficiency spatial planning policy development haean catchment spatial optimisation lumass software and data availability land use management support system lumass software lumass developer alexander herzig s corresponding author year first available 2012 hardware pc notebook 4 gb ram 1 gb disk space software required windows 64bit or linux program language c program size 30 mb source 312 mb windows 64bit installed licence gnu general public licence v3 gpl availability source code windows 64 bit binaries download https bitbucket org landcareresearch lumass haean dataset lumass spatial optimisation howto the haean dataset and the lumass spatial optimisation settings files used to compute the data discussed in this paper are available as part of the lumass spatial optimisation howto authors optimisation howto alexander herzig haean dataset ganga ram maharjan trung thanh nguyen sebastian arnhold bumsuk seo thomas koellner john tenhunen licence optimisation howto creative commons attribution cc by 4 0 haean dataset creative commons attribution noderivs cc by nd 4 0 download https bitbucket org landcareresearch lumass 1 introduction human population increase and economic growth agendas e g mbie 2015 increase the pressure on natural resources and ecosystems around the world mea 2005 ever more production needs to be realised from the same finite amount of natural resources at the same time environmental conditions need to be improved or at least maintained e g nzgovt 2012 hence natural resources have to be used more efficiently to maximise the provisioning of ecosystem goods and services ecosystem services mea 2005 but how efficiently are we using natural resources in a given landscape how do we measure resource use efficiency at the landscape scale in resource and ecological economics in particular different approaches have been developed to assess and analyse the environmental efficiency of production processes with regard to natural resources these include simple ratios relating resource use or environmental impact to production output or net revenue often referred to as eco efficiency indicators tyetca 1996 oecd 1998 wbcsd 2000 material or energy and exergy balances coelli et al 2007 lauwers 2009 hoang and alauddin 2012 and frontier based efficiency analyses lansik and wall 2014 berre et al 2015 coelli et al 2011 proposed a modified data envelopment analysis dea charnes et al 1978 to assess the environmental efficiency of italian provinces more recently nguyen et al 2012 have used this dea approach to assess the environmental efficiency of south korean rice farms and to determine the trade offs between economic efficiency maximized profit or minimize production costs and environmental efficiency minimize negative environmental impacts while frontier based approaches are often used to analyse and compare the environmental efficiency of individual enterprises of a particular business sector e g coelli et al 2007 hoang and nguyen 2013 eco efficiency indicators are often used to compare the environmental impact of economic growth of different regions or countries e g unescap 2009 despite their success in fostering resource use reduction and decreasing environmental impact of businesses mcdonough and braungart 1998 huppes and ishikawa 2007 point out that there is a disconnection between environmental targets at larger scales and eco efficiency improvements at smaller scales mcdonough and braungart 1998 argue that the concept of eco efficiency does not overcome the conceptual ill design of industrial production therefore eco efficiency could only slow down environmental degradation rather than actually lead the way towards sustainability mcdonough and braungart 1998 in fact eco efficiency indicators and environmental efficiency measures as mentioned above henceforth simply referred to as eco efficiency indicators solely represent the demand for natural resources and ecosystem services they are not linked to the landscape ecological processes delivering the ecosystem goods and services consumed in the production process eco efficiency indicators do not account for the stock of natural resources or the ecosystem s potential to provide ecosystem services for example the abatement of adverse effects of the production process on the environment likewise coelli et al 2007 p 10 note 17 point out that their farm level environmental efficiency score calculated for belgian pig finishing farms does not directly correspond to environmental damage because of the different locational characteristics of the analysed farms such as soil type and topography it is impossible to infer the state of the environment in a particular region from the eco efficiency of the businesses in that region even if all businesses in a particular region improve their eco efficiency the region s natural resources may still be unsustainably used hence eco efficiency indicators provide little information on the resource use efficiency in a given region the challenge is to link farm scale management and technological aspects with landscape scale environmental and socio economic objectives to address this issue several case studies have been presented that employ land use optimisation algorithms to achieve landscape scale environmental and economic objectives while explicitly accounting for farm scale management and technological aspects such as fertiliser regimes crop rotations and irrigation respectively e g seppelt and voinov 2002 roetter et al 2005 lautenbach et al 2013 herzig et al 2016 land use optimisation is driven by the spatially varying ecological conditions across the landscape and optimises the match of land use with the landscape s potential to provide ecosystem goods and services thereby it implicitly improves the productivity of the landscape and also the efficiency with which the landscape s natural resources are used to provide benefit to human well being 2 2 we understand human well being as defined by mea 2005 p v human well being is assumed to have multiple constituents including the basic material for a good life such as secure and adequate livelihoods enough food at all times shelter clothing and access to goods health including feeling well and having a healthy physical environment such as clean air and access to clean water good social relations including social cohesion mutual respect and the ability to help others and provide for children security including secure access to natural and other resources personal safety and security from natural and human made disasters and freedom of choice and action including the opportunity to achieve what an individual values doing and being e g polasky et al 2008 bryan et al 2015 herzig et al 2016 however to our knowledge there is no explicit measure available that quantifies resource use efficiency of land use consequently if we cannot measure resource use efficiency it is difficult to assess whether any land use changes improve resource use efficiency or not moreover it is impossible to estimate the potential headroom for efficiency increases in this paper we introduce a new indicator to explicitly estimate the resource use efficiency of land use rue at the landscape scale sections 2 1 and 2 2 additionally we introduce a sequence of analysis steps to support the exploration of land use performance by multi scenario analysis reflecting different regional objectives and expectations section 2 6 the information derived from this analysis might be used to support regional planning for example to assess the impact of agricultural intensification on the land use performance potential herzig et al 2016 or the impact of urban expansion on the provisioning of ecosystem services curran cournane et al 2014 to facilitate the assessment of rue and multi scenario analysis we introduce the free and open source land use management support system lumass section 2 3 it is intended to enable planning professionals with geo data processing skills to assess rue and conduct multi scenario land use performance analysis we demonstrate the assessment and evaluation of the indicator through an academic case study using a test data set of modelled ecosystem services indicators for the haean catchment south korea sections 2 4 and 2 5 additionally we indicate at which stages of the assessment stakeholder input is required to potentially inform spatial planning or policy development nevertheless stakeholder engagement and strategies to incorporate rue assessment into spatial planning and policy development processes are beyond the scope of this paper and subject to further research intensive agriculture in the haean catchment has been identified as a major source of sediment and nutrient contamination in the downstream water reservoir lake soyang we use the haean test data set to demonstrate in general how the assessment of rue can be applied to i assess the land use performance limits with respect to individual ecosystem services ii assess the potential maximum environmental and iii socio economic land use performance as well as iv identify potential trade off scenarios between environmental and economic land use performance section 2 6 more specifically in the context of the haean test application we show that the rue assessment provides estimates of the maximum possible individual and combined reduction of sediment and nutrients for the current land use this could inform spatial planning and policy development whether the current land use system provides enough headroom to achieve potential sediment and nutrient reduction targets for lake soyang while maintaining or enhancing the livelihood in the haean catchment 2 material and methods 2 1 conceptual overview our well being depends on the goods and services provided to us by the landscape ecosystems such as the provision of food and fibre or the regulation of water de groot et al 2002 mea 2005 nguyen et al 2013 we realise these benefits i e the goods and services by the direct or indirect use of or interaction with the ecosystem for example we pass our leisure time by swimming in a lake or river or we feed ourselves with produces grown by a farmer on fertile soils the provisioning of these ecosystem services is underpinned by ecosystem processes de groot et al 2002 that utilise and or produce natural resources e g water air minerals soils plants etc depending on the type of land use and the associated management practices different types and quantities of resources are required to provide these services for example different crops have different demands for nutrients and water to ensure specific production levels additional water stored in aquifers rivers or lakes in the landscape may be provided to the plants however water abstraction from a river reduces its water level and may impact on recreational benefits downstream e g swimming other uses may have a positive impact on other ecosystem services for example growing trees not only provides fibre but also sequesters carbon and thus contributes to climate regulation and benefits human well being by mitigating climate change hence any type of land use is linked to the natural resources of the ecosystem where the use occurs this also applies to human infrastructure such as houses and roads even though they may not utilise the natural resources in the region in which they occur they may obstruct their use for example the loss of fertile soils through urban development land use provides a certain benefit to human well being and positively and or negatively impacts on the ecosystem s potential to provide ecosystem services in the future by affecting its natural resources society has to decide which benefits i e services are important for people s well being and which land use impacts on the environment are tolerable or intolerable in the context of the call to use natural resources more efficiently to satisfy the increasing demand for ecosystem services two questions arise i how efficiently are we using our natural resources currently and ii how do we measure it to address these questions we have first to clarify what we mean by efficiency since the concept and definition of it is used differently across disciplines heijungs 2007 in this paper we refer to the general formal definition given by heijungs 2007 p 87 efficiency refers to the degree of optimality of a system it can be a quantitative indicator in which case it is a dimensionless pure number measured on a ratio scale bounded between 0 and 1 and higher values signifying a higher degree of optimality in line with this definition we understand the resource use efficiency of land use rue as the degree of optimality with which land use utilises the available natural resources in a landscape to provide benefits to human well being the resource use efficiency of a given region is higher the more benefit the land use in that region provides thereby the benefit is defined by the values expectations and objectives of the stakeholders in that region potential stakeholders could be national or regional government or agencies e g ministries council businesses community representatives or other regional interest groups we estimate rue by relating current land use benefits to the maximum possible land use benefits which are in line with the stakeholders expectations and objectives section 2 2 we note that the maximum possible land use benefit refers to the maximum attainable land use benefit which can be derived from the landscape ecosystem by the given land use it is not equivalent with the theoretical bio physically possible maximum benefit to operationalise the assessment the stakeholders values expectations and objectives have to be translated into a set of environmental and socio economic land use performance indicators and associated thresholds representing minimum performance levels or maximum tolerable environmental impacts for the given region stakeholder input could be best achieved by adopting a collaborative modelling approach e g cockerill et al 2009 voinov and bousquet 2010 voinov et al 2016 in that context for example schiller et al 2001 discuss how to communicate ecological indicators to stakeholders thomson 2005 discusses indicator based knowledge management for participatory decision making and harmsworth et al 2016 describe the use of values based frameworks to derive environmental limits connected to cultural values in agriculturally dominated areas environmental indicators may include nitrate leaching soil erosion greenhouse gas emissions or water demand socio economic indicators may include crop yield gross margin or net revenue because of the quantitative nature of the proposed rue indicator qualitative indicators e g to represent social and cultural values cannot be easily included directly in the assessment such values may either be included as spatial constraints in the assessment of the maximum possible benefit s sections 2 3 and 2 5 for example to exclude cultural heritage sites from land use change or have to be assessed during the evaluation of the rue assessment to assess the maximum possible land use benefit we use multi objective spatial optimisation conceptually this not only allows the linking of farm scale management practices with landscape scale objectives but also enables the integration of regional objectives and expectations into the assessment e g roetter et al 2005 s also section 2 3 because of the different values of different stakeholder groups and or the uncertainties associated with the translation of those values into regional objectives and expectations there is no single optimum land use configuration rather there are many to account for the variety of possible scenarios we employ a step wise procedure comprised of four major steps sections 2 6 and 3 1 to 3 4 i exploring limits ii maximising environmental benefit iii maximising socio economic benefit and iv identifying trade offs in the first step we assess the maximum possible land use performance with respect to each individual socio economic and environmental indicator and estimate the associated headroom collectively the results provide an estimate of the land use performance limits to derive ecosystem services for human well being from the landscape based on these results opportunities for increasing rue across the landscape subject to the specified objectives are explored sections 2 6 and 3 2 to 3 4 the associated land use scenario maps alongside the rue scores of the associated indicators may then be evaluated by the stakeholders to identify more resource use efficient scenarios for land use development in the given region the use of land use scenarios for stakeholder participation is for example discussed by griewald et al 2017 king et al 2015 discuss the application of a framework for assessing ecosystem services trade offs cavendar bares et al 2015 in terms of stakeholder preferences and associated challenges and opportunities 2 2 resource use efficiency of land use we define resource use efficiency as the ability of land use to utilise the natural resources of a landscape to derive benefit for human well being thereby land use represents the allocation of land cover such as vegetation or buildings and human activities such as farming or recreation across a landscape benefit represents the ecosystem services derived from the landscape by the given land use we estimate rue as the ratio of land use performance over maximum land use performance eq 1 the latter is represented by the spatially optimised land use configuration i e the optimum allocation of land cover and human activities across the landscape however the optimum land use configuration is subject to the expected land use performance lup e with regard to a set of criteria the latter comprises landscape specific environmental and socio economic aspects of land use such as the provision of fresh water or food production and is represented by a set of indicators i we distinguish two types of criteria i benefit criteria which associate increasing indicator values with greater benefits such as crop yield t yr 1 and ii cost criteria which associate decreasing indicator values with greater benefits such as total nitrate loss kg n yr 1 land use performance and hence resource use efficiency are assessed individually for each indicator 1 r u e i e l u p i l u p i e o p t w i t h i r e p r e s e n t s b e n e f i t c r i t e r i o n a l u p i e o p t l u p i w i t h i r e p r e s e n t s c o s t c r i t e r i o n b with r u e i e 0 1 i 1 2 lupi gi land use landscape 3 lupi e opt gi opt land use landscape e i set of environmental and socio economic performance indicators e set of expected outcomes objectives and constraints rue i e resource use efficiency with respect to indicator i and expectations e lup i land use performance with respect to indicator i lup i e opt optimum land use performance with respect to indicator i and expectations e g i performance evaluation function for indicator i opt spatial land use optimisation function subject to e the headroom hr i e for increasing resource use efficiency can thus be expressed as the land use to be assessed for resource use efficiency may be the current or any hypothetical land use scenario the optimum land use represents the spatial configuration of land cover and human activities that derives the maximum benefit from a given landscape subject to regional expectations e the headroom represents the unused potential of land use and indicates the maximum possible performance increase subject to regional expectations e for brevity we subsequently use the term land use system to refer to a set of land covers and management practices i e land use and associated regional expectations with regard to a given set of environmental and socio economic criteria in a given region 2 3 spatial land use optimisation with lumass lumass is free and open source software to support scientists in understanding how ecosystems on the landscape scale work and how they react to management activities methodologically system understanding and impact assessment are commonly facilitated by ecological modelling jørgensen 1994 powers et al 2011 ausseil et al 2013 which is supported by lumass through its spatially explicit system dynamics modelling framework herzig and rutledge 2013 it allows users with gis skills to implement complex bio physical models for investigating system behaviour and management impact in addition land mangers need to decide on how land cover and management activities are best allocated across the landscape to maximise planning objectives and meet minimum performance constraints such as minimum production targets or environmental limits in this paper we focus on the latter aspect and discuss the implementation and use of lumass multi objective spatial optimisation framework for spatial planning and policy development support to solve multi objective spatial optimisation problems lumass uses the open source mixed integer linear programming solver lp solve berkelaar et al 2005 consequently lumass represents spatial optimisation problems as linear programs herzig et al 2013a as follows we seek x that maximises c j x which is 4 m a x c j x w i t h x b w h e r e b x ℝ u ax b x 0 b ℝ q where x is the vector of decision variables containing a variable x fl for each land use parcel combination with f f and l l f denotes the set of available parcels and l the set of land uses to be allocated across the landscape c j represents the vector of performance scores for objective j it contains the coefficients of the decision variables expressing the benefit associated with the allocation of one unit of a particular land use e g hectare or square meters to a particular parcel with regard to the particular objective multiple objectives are accounted for either by weighing individual objective functions and adding them to a single objective function 5 m a x j λ j z j w i t h λ j ℝ n λ j 0 j λ j 1 z j c j x x b or by sequentially solving single objective optimisation problems s eq 4 in the order of their relative importance and then adding them one by one as constraint z v ε v to the subsequent optimisation problem 6 m a x z j w i t h x b x ℝ u z j c j x z v ε v v j ε ℝ n the set b of feasible solutions of the optimisation problem is constrained by the inequality cf eq 4 7 ax b with x ℝ u x 0 b ℝ q where the matrix a comprises the coefficients of the decision variables associated with the q constraints and b represents the vector of constraint thresholds lumass supports two types of user defined constraints i allocation constraints and ii performance constraints the spatial validity of allocation and performance constraints may also be restricted to arbitrarily defined zones allocation constraints control the total area b kd of a set k of land uses which is allocated to a set d of parcels 8 d ε d k ε k x k d b k d w i t h x k d b k d ℝ b k d 0 k l d f where x kd denotes the area share of land use k allocated to parcel d this type of constraint can be used for example to control the allowed type of land use conversions or to configure a minimum or maximum threshold of a particular type of land use s to be allocated to a particular region besides the direct control of the land use allocation by area thresholds lumass also supports the indirect control of the land use allocation with performance constraints similar to objective functions eqs 4 to 7 performance constraints link performance indicators e g s kdj of a particular criterion j with the allocated area share of a particular land use k to a particular parcel d performance constraints can also be configured for arbitrary subsets of the available parcels comprising an arbitrary subset of land uses to which they apply 9 d ε d k ε k s k d j x k d b k d j w i t h s k d j x k d b k d j ℝ b k d j 0 k l d f they can be used to ensure a minimum desired performance for example minimum production quantities or they can be used to limit undesirable effects of land use such as soil erosion to facilitate the evaluation of land use optimisation lumass provides for each optimisation scenario a region based summary statistic with regard to the current and optimal land use performance and their absolute and relative difference for each individual criterion j the generated text file comma separated values can then be easily imported into spreadsheet applications to calculate rue additionally lumass calculates a land use change matrix for each computed scenario to support the analysis of spatial effects of land use optimisation for the assessment of sensitivities of land use pattern to input data uncertainties and optimisation constraints herzig et al 2013b lumass provides specific batch optimisation options to automatically compute multiple realisations and or variations of a spatial optimisation problem users can specify the uncertainty associated with input performance scores and the total number of simulations per uncertainty lumass then randomly varies the input data according to the specified uncertainty and computes multiple realisations of the optimal land use pattern a similar configuration is possible to assess the effect of constraint uncertainty it enables the automated systematic change of the optimisation constraints to generate a series of similar optimisation scenarios which help to explore possible trade offs between objectives this mode is essentially equivalent to the calculation of pareto fronts as performed in the application of search heuristics e g lautenbach et al 2013 since lumass employs linear programming individual points on the pareto front can be directly calculated and do not have to be approximated as is the case with search heuristics there is no fixed limit to the size i e the number of objectives land uses parcels or constraints of spatial optimisation problems which can be addressed with lumass nevertheless the larger the problem is the more likely that the underlying optimisation library lp solve will fail to solve it berkelaar et al 2005 the largest problem we have successfully solved with lumass lp solve had 12 land use options 13 322 parcels one objective two objective constraints 25 performance constraints and 26 644 allocation constraints note that in this setting we have used the ɛ constraint model eq 6 to effectively account for three objectives in total depending on the actual values of the constraint thresholds the runtime of an individual scenario on a virtual machine with 2 4 ghz intel processor and 16 gb of ram was in the range of a few to up to 20 min lumass optimisation settings are specified in a user configurable text file and can be used to run optimisation scenarios in the lumass desktop environment or as multi threaded batch processing jobs using the lumass command line programme e g in server or cluster environments 2 4 case study area haean catchment to test the assessment of rue and gauge its potential value for supporting spatial planning and policy development we apply the proposed approach to a test data set of ecosystem services indicators for the haean catchment in south korea it is located in the northeast of the kangwon province south korea fig 2 land cover in the haean catchment is dominated by deciduous and coniferous forests 57 and covers primarily the steep hillslopes agricultural land covers approximately 28 20 dryland fields and 8 rice paddies primarily at hillslopes near the forest edges and flat areas in the catchment centre the remaining 15 are residential water and semi natural areas including grassland field margins and riparian areas maharjan et al 2016 the dominant crops cultivated at hillslopes are annual dryland cash crops primarily soybean potato radish and cabbage and perennials such as ginseng and orchards while the flat areas in the centre are dominated by paddy fields maharjan et al 2016 seo et al 2014 fig 2 the agricultural landscape of the haean catchment is characterised by small fields in most cases 2 ha arnhold et al 2014 nguyen et al 2014 embedded in a mosaic of semi natural landscape elements but also farm roads and a dense network of artificial drainage channels shope et al 2014 the haean catchment is located within the largely forested watershed of lake soyang fig 2 which is the largest reservoir in south korea and important for drinking water provision for a large proportion of the country s population kim et al 2000 maharjan et al 2016 particularly during the monsoon season the reservoir s water quality is affected by high nutrient loads of the soyang river originating primarily from agricultural land within the watershed kim and jung 2007 park et al 2010 the haean catchment has been identified as one of two key contributors to agricultural water pollution with substantial impacts on the trophic state of the lake park et al 2010 main sources are soil erosion and nitrate loss from dryland agricultural fields cultivated with annual crops kettering et al 2012 arnhold et al 2013 2014 ruidisch et al 2013 kim et al 2014 both sediment and nitrate are rapidly transported from agricultural fields to the stream network due to the high connectivity by artificial drainage channels and short travel distances to streams shope et al 2014 2 5 ecosystem services assessment due to its importance for agricultural production and its role as headwater catchment of the soyang river the current management in the haean catchment constitutes a major conflict between food and fresh water provisioning as well as erosion control ecosystem services mea 2005 insufficient erosion prevention and capacity to maintain soil fertility have led to the deterioration of the long term production of provisioning services of the catchment maes et al 2012 raudsepp hearne et al 2010 we apply the rue assessment to demonstrate the potential information it could provide to support decision making in spatial planning and policy development in this context since our test application did not involve stakeholder engagement we selected five indicators to represent the competing demands for erosion control fresh water provisioning and food production i e sediment yield surface runoff total nitrate loss biomass and crop yield because biomass and crop yield alone do not inform about the income and livelihood situation of farmers we additionally introduced gross margin as indicator for the well being and quality of life of haean residents díaz et al 2015 we also accounted indirectly for cultural services in our assessment by acknowledging the importance of rice as main staple crop in korea kettering et al 2012 nguyen et al 2012 hence we left rice paddies unchanged regardless of their ecological or economic performance and our analysis focused instead on the four major cultivated annual cash crops i e soybean potato radish and cabbage these crops also form an important part the korean diet kettering et al 2012 and were identified as major sources of water quality degradation in the haean catchment section 2 4 we used the soil and water assessment tool swat arnold et al 1998 gassman et al 2007 to quantify the six performance indicators underpinning the rue and land use trade off analysis its bio physical output variables can be directly related to provisioning and regulating ecosystem services francesconi et al 2016 vigerstol and aukema 2011 the swat model setup and parameterisation for the haean catchment including input data collection resolution and data sources have been previously described by shope et al 2014 and maharjan et al 2016 however since our rue test application predates the completion of the work described by maharjan et al 2016 we used an earlier model setup it only modelled nitrate loss via surface runoff and interflow and did not account for nitrate leaching to groundwater and did not benefit from a more recent re calibration it shows about 20 and up to 68 different values for the average sediment loss and crop yield per hectare and year for the crops considered in this study as input to the spatial optimisation procedure we also assessed the potential performance of each of the four major cash crops in terms of each of the six indicators for each swat hydrological response unit hru to calculate the gross margin we multiplied the simulated crop yield for each hru with the market price for the individual crops and subtracted the total production costs e g labour costs land rent fertiliser input etc table 1 market price and production costs are averages for the years 2009 2011 and were obtained from an interview survey of more than 300 farmers in the haean catchment nguyen et al 2014 as swat calculates crop yield as dry biomass we converted the simulated yield outputs into fresh biomass equivalents lindner et al 2014 arnhold et al 2014 to make the swat model outputs available to lumass we rasterised the modelled potential performance for each of the four crops with regard to each of the six ecosystem services indicators at the hru level we used a raster resolution of 30 m which equals the resolution of the digital elevation model that was used to delineate the swat hrus for the hean catchment shope et al 2014 we summarised each of the 24 input performance layers at the land parcel level to represent the six vectors c eq 4 of performance scores required by the lumass optimisation component thereby we only included those parcels which were covered by the four cash crops considered in this study 1205 parcels 492 ha to assess the effect of this procedure on the modelled performance of land use scenarios by lumass we compared the performances of the baseline land use scenario modelled with swat and lumass with respect to the considered cropping area the comparison showed deviations of the lumass results from the swat results of about 2 to 6 across the range of indicators and crops the performance of other scenarios may show a bigger deviation from the corresponding swat model result since the swat model setup used in this study does not represent the actual land use performance with regard to the selected indicators well enough the results of the rue test application should not be used to inform actual decision making in the catchment nevertheless the data set reflects the conflicting demands for food and fresh water provisioning as well as erosion control ecosystem services in the catchment and is used here to exemplify the application of rue and the information it could contribute to support spatial planning and policy development in addressing this problem 2 6 resource use efficiency assessment in the haean catchment as outlined in section 2 2 each optimum land use configuration is subject to objectives and constraints in terms of the expected benefit derived from it thus no single optimum land use configuration exists therefore we employed a step wise procedure to characterise the resource use efficiency of the given set of land uses for different objectives table 2 reflecting the competing demand for ecosystem services observed in the catchment section 2 5 2 6 1 exploring limits in a first step we modelled the optimum land use configurations for each individual indicator considered in this study scenarios s1 to s6 i biomass ii crop yield iii gross margin iv total nitrate loss v sediment yield and vi surface runoff we determined the objective of each individual optimisation scenario depending on the type of the performance indicator benefit indicators such as crop yield or gross margin were subject to maximisation whereas cost indicators such as nitrate loss or sediment yield were subject to minimisation 2 6 2 maximising environmental benefit in a second step we modelled the land use configuration which maximises benefit across the set of environmental performance indicators scenario s7 to represent multiple objectives we iteratively employed equation 6 and incorporated the individual objectives indicators in order of decreasing importance i e sediment yield and nitrate loss 2 6 3 maximising economic benefit we represented an economically focused land use scenario by maximising for gross margin scenario s8 while maintaining the environmental performance s above of the land use configuration as at 2010 2 6 4 identifying trade offs scenarios s9 and s10 depict benefit trade offs across the full range of indicators representing a more environmentally focused scenario on one hand scenario s9 and one focused on economic benefit on the other scenario s10 to identify these trade offs we iteratively solved equation 6 with the objective to maximise gross margin the initial objective constraints eq 6 ε were set to represent the maximum environmental benefit achieved in scenario s7 for sediment yield and nitrate loss respectively table 2 s7 then we used lumass batch optimisation capability to automatically relax these constraints by a percent point at a time and solve the associated optimisation problem we configured lumass to continue this procedure until the objective constraints reached their respective baseline performance this yielded a set of optimisation scenarios representing the range of optimal solutions between scenario s7 maximising environmental benefit and scenario s8 maximising economic benefit finally we picked two scenarios i e s9 and s10 from this set to more or less evenly cover the range of gross margin performance between scenarios s7 and s8 and at the same time achieve the best result for both sediment and nitrate loss to account for the current agricultural activity in the catchment we constrained eq 9 each optimisation scenario to achieve at least the same gross margin for each considered land use as in the 2010 land use configuration also to respect cultural values we allowed land use conversions only to happen outside rice cultivation areas and for environmental reasons we disabled the conversion of forested land we evaluated each land use scenario in terms of its resource use efficiency with respect to the individual performance indicators it allowed us to analyse the performance of the current set of land uses in the area and to look for opportunities for land use development towards more efficient use of natural resources in the haean catchment 3 results and discussion 3 1 exploring limits scenarios s1 to s6 table 2 assess the performance limits of the haean catchment with regard to each individual environmental agricultural and economic performance indicator the greatest headroom for efficiency increases fig 3 were modelled for sediment yield 14 and nitrate loss 11 respectively all remaining indicators showed resource use efficiencies of 90 and more the results indicate the limits of the current land use system to derive higher benefits from the natural resources in the haean catchment the estimated limits in fig 3 define the upper boundary for further development of this land use system however optimising land use to maximise the provision of an individual ecosystem service is likely to affect the provision of another ecosystem service this relationship is displayed in fig 4 for example optimising the land use system to solely minimising sediment yield impacts the provision of food fig 4 in that case the land use system uses the natural resources in the haean catchment more efficiently i e 14 relative efficiency increase to reduce sediment yield i e 14 sediment yield compared with baseline table 2 the same land use system fig 4 min sediment yield uses the natural resources in the catchment less efficiently i e 4 relative efficiency increase to produce crops i e 4 crop yield compared with baseline the area indicator in fig 4 represents a cost criterion fig 1 and shows whether individual scenarios can achieve their expected objective and gross margin performance on less area than in the baseline scenario this is the case for the cost criteria nitrate loss sediment yield and surface runoff for example in the nitrate loss scenario s1 fig 4 min no3 loss the gross margin performance can be achieved on 7 less area i e 7 relative efficiency increase with regard to area the unused area cf figs 6 and 9 land use category unused is assumed to be neutral in terms of the considered ecosystem services performance indicators even though this is not realistic in the context of exploring the landscape s potential land use performance it indicates development opportunities for land use types which perform neutral with regard to the considered cost criteria fig 1 in the analysis collectively the upper and lower performance limits of the given land use system define the window of opportunities for further land use development fig 4 in a given area any land use configuration that improves the resource use efficiency of the given land use system with regard to one or more indicators has to lie within this window of opportunities i e the grey area in fig 4 the analysis of the land use performance limits shows whether the given regional objectives and expectations could actually be met with the considered land uses and management practices in a best case scenario in the given region if that is the case the window of opportunities can be searched for feasible trade off scenarios section 2 5 in case any of the objectives or expectations could not be met i e the associated optimisation problem is infeasible the analysis needs to be repeated with a different set of land uses or management practices or reduced expectations in terms of the configured performance constraints in the context of the haean catchment the question would be whether 14 reduction of sediment yield or 11 reduction of nitrate loss would be sufficient to make a significant enough impact on the water quality in lake soyang if that was the case the window of opportunities could be searched for a feasible trade off scenario that could achieve the desired water quality improvements in lake soyang if the maximum possible reduction of sediment and nitrate loss fig 3 were not enough to sufficiently improve the water quality in lake soyang the analysis had to be repeated with a different set of land uses and or management practices e g fertiliser regime or tillage methods or reduced regional expectations such as a lower gross margin threshold to analyse the potential effect on the objectives of sediment and nitrate reduction the results of scenarios s7 to s10 represent a subset of the feasible opportunities and prioritise different ecosystem services along a cross section from an environmental s7 to an economic s8 focus scenarios s9 and s10 characterise the middle ground in this spectrum fig 5 presents the results of these scenarios in terms of the relative efficiency increase cf fig 1 with respect to the individual performance indicators a positive relative efficiency increase indicates that the land use system uses the natural resources in the given region more efficiently and provides an increased performance in terms of the ecosystem services the specific indicator represents for example in scenarios s7 s9 and s10 the land use system uses the natural resources more efficiently to prevent sediment loss which improves the performance of the fresh water provisioning service and achieves a reduction of sediment yield by 11 8 and 7 respectively table 2 chg over baseline 3 2 maximising environmental benefit scenario s7 focuses on the reduction of sediment yield and nitrate loss the two major contaminants in this catchment responsible for environmental problems downstream the results show that the current land use system could be optimised to reduce both sediment yield and nitrate loss in the haean catchment by 11 without any loss of gross margin this improvement is realised by a more productive use of the land by optimising the match of land use with the landscapes potential to provide ecosystem services such as erosion control fresh water provision and food production fig 6a the analysis of figs 6 8 shows that resource use efficiency gains were achieved by a combination of land use conversions radish potato and soybean were moved to areas with relative higher gross margin performance and similar or higher environmental performance fig 7 i e radish to the south east potato to the north north east and south east and soybean to the west and more towards the centre of the catchment fig 6a this allowed for their overall area to be reduced and their performance in terms of sediment and nitrate loss and surface runoff to be improved while still achieving the gross margin of the baseline scenario fig 8 since cabbage shows the overall lowest gross margins compared with the other crops fig 7 it was moved to those areas of the catchment where the lowest gross margins across all crops are achieved to not affect the potential gross margin gains by other crops and hence their opportunity to reduce the overall production area the overall cabbage area was increased fig 8 to achieve the individual gross margin target of the baseline land use scenario consequently cabbage was the only crop whose environmental performance decreased in this scenario furthermore the land use change matrix for scenario s7 table 3 indicates cabbage to be the most out of place crop since only seven percent of the baseline cabbage area remained unchanged and 14 of the cabbage area were taken out of production fig 6a land use category unused potato and radish were the most stable crops with 24 and 26 of their respective areas remaining unchanged as specified by one of the optimisation constraints applied to all scenarios no changes were observed for land use type other nevertheless the area taken out of production of the optimised land use types were added to category other in this statistic in summary the achieved gains in gross margin enabled the reduction of the overall cultivated area fig 8 this in turn allowed for a further reduction of sediment yield and nitrate loss however the estimated environmental benefit is only realised if the land taken out of production is used in a way which is neutral to sediment yield nitrate loss and surface runoff 3 3 maximising economic benefit scenario s8 aims for economic growth of the catchment while maintaining the overall environmental footprint of the baseline land use configuration the optimised land use configuration achieved an 8 increase in gross margin and does not leave any production area unused fig 9a the overall land use configuration looks similar to scenario s7 but its economic focus reveals a slightly different pattern fig 9a and land use conversions fig 8 similar to scenario s7 resource use efficiency of radish and potato were increased to achieve their individual gross margin baseline targets on a smaller area as in 2010 in contrast to scenario s7 soybean area was increased to maximise the overall gross margin performance of this scenario this is also reflected in the land use change matrix of this scenario which shows that between 30 and 36 of the area covered by the individual crops was converted to soybean in contrast to soybean cabbage the least profitable cash crop considered in this study table 1 was moved to areas with relative low gross margins however its overall area was increased even more than in scenario s7 to achieve the baseline gross margin target soybean and potato were the most stable crops in this scenario and 36 and 34 of their respective areas remained unchanged compared with the baseline land use configuration 3 4 identifying trade offs scenarios s9 and s10 represent the middle ground between scenarios s7 and s8 and trade off economic and environmental performance to different degrees fig 5 both improve the overall resource use efficiency of the land use system in terms of all performance indicators except for biomass and crop yield consequently the optimised land use configurations for these scenarios also represent trade offs between those of scenarios s7 and s8 figs 6a and 9a overall the land use patterns of scenarios s9 and s10 are very similar and the differences only subtle in total less area is taken out of production compared with scenario s7 especially in the southern part of the catchment apart from that the overall allocation trends are very similar to scenario s7 the efficiency improvements in terms of sediment 8 and 7 and gross margin 2 and 5 for scenarios s9 and s10 fig 5 is controlled by the different area share of soybean fig 8 the slightly higher soybean production area in scenario s10 leads to a relative higher efficiency increase with regard to gross margin and a slightly reduced efficiency in terms of sediment yield and nitrate loss fig 5 the mediating character of scenarios s9 and s10 is also represented by their respective land use change matrices the differences between scenarios s9 and s10 are very small and the overall character of land use change is here represented by the matrix for scenario s9 table 3 the most stable crop is soybean with 28 and 29 of its area remaining unchanged in scenarios s9 and s10 respectively radish represents the most out of place crop in scenarios s9 and s10 with only 14 and 13 remaining unchanged respectively 3 5 methodological and operational aspects of ruea in sections 3 1 3 4 we summarised and discussed the specific results of the rue assessment for the haean catchment south korea in this section we discuss methodological and operational aspects of the approach with particular focus on its application in land use management in general as described above we used ecosystem services indicators modelled with swat to assess rue for the haean catchment the use of spatially explicit bio physical models for environmental impact assessment and associated challenges such as model sensitivity and uncertainty have been discussed extensively in the literature e g jakeman and letcher 2003 krysanova et al 2004 tennoy et al 2006 de groot et al 2010 discuss the challenge of integrating the ecosystem services concept into planning management and decision making here we focus on the computation of optimal land use configurations as the basis for the assessment of the maximum possible land use performance in particular we address the question how optimal is the optimal land use configuration in the following sections we focus on the methodological and operational aspects of this question with regard to the application of rue in land use management 3 5 1 methodological aspects depending on the type of the objective function constraints and decision variables different algorithms are used to solve spatial optimisation problems e g grabaum and meyer 1998 aerts and heuvelink 2002 holzkämper and seppelt 2007 for example in the case of linear objective functions and constraints linear programming algorithms can be used to efficiently find an optimal solution i e global optimum to the problem steuer 1986 computationally more complex optimisation problems involve non linear objective functions and constraints or integer or binary decision variables i e combinatorial optimisation ehrgott 2005 they are commonly solved using search heuristics such as simulated annealing or genetic algorithms e g aerts and heuvelink 2002 lautenbach et al 2013 which in contrast to linear programming only provide approximated solutions to an optimisation problem with regard to applications in biological conservation prioritisation moilanen 2008 argues that both types of algorithms i e linear programming and search heuristics lead to a suboptimal solution to the real world spatial planning problem although linear programming provides an exact solution suboptimality is introduced by major simplifications in representing the real world problem with linear equations in contrast employing a heuristic algorithm requires only minor simplifications but therefore only yields an approximate i e suboptimal solution moilanen 2008 this analysis is in general also applicable to the optimisation of land use pattern as presented here consequently the degree of suboptimality in calculating the maximum land use performance and hence rue depends on the real world optimisation problem characterised by its type of objectives and constraints and the algorithm used to solve the problem in addition to suboptimality the generated optimal land use pattern and the calculated optimisation benefits are also influenced by the uncertainty associated with the input data and constraints for example herzig et al 2013b have investigated the sensitivity of the optimal land use pattern to variation in the input data for a single objective problem i e minimisation of nitrate leaching they found that with 30 uncertainty associated with the leaching input data the optimisation benefit i e the potential reduction of nitrate leaching was overestimated by about 25 the average allocation probability of a particular land use type to an individual parcel was estimated to be about 75 a study of uncertainty propagation in multi objective optimisation by holzkämper et al 2015 suggests that optimisation results are less robust for multi objective than for single objective problems in summary suboptimality and uncertainty lead to inaccurate optimisation results hence the calculated maximum possible benefit used to estimate rue is subject to uncertainty lumass support to facilitate the assessment of optimisation uncertainties is outlined in section 2 3 3 5 2 operational aspects even if we could compute exactly the global optimum of a land use optimisation problem from an operational point of view the question is does the mathematically optimal solution to an optimisation problem mean the solution is also socio economically or environmentally optimal for example to minimize nitrate loss in the haean catchment mathematically the most optimal scenario would have been to get rid of all land uses considered in this study since each one contributes to the overall nitrate loss however this is clearly not acceptable from a socio economic point of view therefore stakeholder input is crucial to design a representative optimisation problem in the context of the rue assessment for a given region it needs to reflect community and policy objectives for the region which can then be translated into objective functions and constraints of the optimisation problem furthermore regulatory requirements for the particular region such as environmental limits can be integrated as additional constraints into the optimisation problem nevertheless dependence on subjective goals and values means the optimum land use configuration and hence the rue of a given region cannot be calculated objectively while this subjectivity seems to be a weakness of the presented approach to assess rue of land use we argue it is actually its strength and sets it apart from other widely applied indicators or measures for example unlike eco efficiency and environmental efficiency indicators the presented rue assessment provides a means to link community values with the state of the environment it lives in the uncertainty associated with the translation of stakeholder values objectives and expectations into regional objectives and constraints of the optimisation problem section 2 1 creates another source of uncertainty for the calculation of the maximum possible benefit cf section 3 5 1 it adds to the general challenge of the use of spatial optimisation in an operative context to account for the uncertainties associated with regional constraints and objectives we explored trade offs between environmentally and economically focused land use configurations scenarios s7 to s10 using lumass batch processing capabilities outlined in section 2 3 3 6 application of ruea in land use management the rue assessment is targeted at regions and catchments with land uses that utilise natural resources conceptually rue can be applied to any land use type whose performance can be expressed by at least one numerical ecosystem services indicator eq 1 in addition to the land use types tested in this study this includes pastoral farming viticulture and forestry as well as land use type variants characterised by management practices such as irrigation ausseil et al 2013 herzig et al 2013a 2016 the applicability to other land use types e g recreation needs still to be tested the assessment is intended to provide both an estimate of the resource use efficiency and performance limits of a given land use system as well as a sequence of analysis steps to help identify opportunities to increase the resource use efficiency of land use in a given region we suggest an application scale for the assessment ranging from a few hundred to a few hundred thousand hectares at land parcel scale resolution the upper scale limit is defined by the size of the underlying optimisation problem i e the number of land use options parcels objectives and constraints cf section 2 3 to be effective the rue assessment should be embedded in a spatial planning or resource management framework for example at the regional government level the framework should engage stakeholders and planning professionals or science providers in an iterative manner to translate stakeholder values and expectations into performance objectives and constraints of the assessment and to communicate back the evaluated scenario results the recurring assessment of rue over time might indicate the effectiveness of potential policy measures or spatial planning efforts to increase rue of land use in a given region 4 conclusions we presented a novel approach to assess explicitly the resource use efficiency of land use it is estimated as the ratio of land use benefit over the maximum possible land use benefit the resource use efficiency indicates the closeness of a land use system to its optimal performance unlike other eco efficiency indicators it links the farm scale performance of the production process with the landscape scale ecological processes in a given region additionally it can account for regional environmental and socio economic objectives and expectations expressed as numerical values e g herzig et al 2016 furthermore we proposed a systematic step wise procedure that uses rue to explore both the performance limits of a given land use system in a given region and the potential headroom to increase the rue in a given region to inform policy development and spatial planning the proposed step wise procedure should be embedded in a spatial planning or resource management framework that engages stakeholders and incorporates their objectives and expectations into the assessment this would enable testing whether specific stakeholder development objectives are achievable with a given land use system or whether alternative land uses and or management practices needed to be considered to achieve the objectives the test application of the rue assessment in the haean catchment showed for the most important indicators that the rue of the land use system could be improved at maximum by 14 for sediment loss or by 11 for nitrate loss respectively the highest simultaneous relative efficiency increase was modelled for scenario s7 with 11 for both sediment and nitrate loss while maintaining the overall livelihood in the catchment the information on the maximum achievable contaminant reduction could be analysed in terms of its potential impact on the contaminant reduction in lake soyang this in turn would indicate whether the headroom for resource use efficiency increases in the haean catchment is sufficient or whether alternative land uses and or management practices needed to be explored to achieve the targeted contaminant reduction in lake soyang conceptually the presented rue assessment is generic and can be applied in different areas and contexts in particular it is flexible in terms of i the considered set of land uses ii the set of ecosystem services and socio economic indicators iii the bio physical models used to derive ecosystem services indicators iv the region v the objectives and constraints used to represent regional expectations and vi and the spatial optimisation algorithm and software used to generate pareto optimal land use configurations further research is required to assess its applicability across a wider range of land use types and ecosystem services indicators and to operationalise its use in practice acknowledgement we are deeply saddened that our esteemed colleague and co author dr sebastian arnhold passed away in november 2017 we will remember him as a passionate researcher and as a great person we thank the agencies that supported the conceptual work on resource use efficiency including the new zealand ministry for the environment and the new zealand ministry of business innovation and employment s science and innovation group this study was carried out as part of the international research training group terreco grk 1565 1 funded by the deutsche forschungsgemeinschaft dfg at the university of bayreuth germany and the korean research foundation krf at kangwon national university chuncheon south korea 
26362,the epanet software for modeling piping networks is widely used for the design and analysis of water systems this short communication describes epanet2toolkit a package for accessing the epanet simulation engine and application programming interface from the r environment for computing and graphics users of the package may run extended period simulations of hydraulics and water quality or create custom applications using the data exploration visualization and analysis capabilities of r epanet2toolkit enables interactive use of epanet on a variety of computing platforms provides integrated error checking and contributes a publicly available test suite the package is available through the comprehensive r archive network and github com keywords water networks r epanet hydraulic modeling software availability epanet2toolkit is available on the world wide web at https cran r project org package epanet2toolkit and https github com bradleyjeck epanet2toolkit license mit system requirements r version 3 0 2 or higher installation using install packages function 1 introduction modeling the behavior of water piping networks is an active area both in industry and academia with published studies covering a very broad range of problems including design optimization and management several pieces of software are available for simulating piping networks including proprietary packages such as watergems r infowater r and kypipe and freely available packages such as epanet rossman 2000 originally developed by the united states environmental protection agency epanet has over 2000 citations on google scholar making it the most popular choice for academic applications the epanet simulation engine is written in the c language for portability and performance bindings have been developed to call the simulation engine from several other languages the programmer s toolkit rossman 2008 includes bindings for visual basic and pascal more recently bindings for matlab eliades et al 2016 and python pathirana 2016 have been developed this paper describes bindings for the r environment r core team 2013 provided in the form of an add on package called epanet2toolkit as an r package for water resources modeling epanet2toolkit joins a growing group of packages for water applications the package epanetreader eck 2016 compliments the simulation capability of epanet2toolkit with support for parsing epanet data files into r objects in a similar vein the package swmmr by leutnant 2017 provides an interface for the storm water management model the dataretrieval package by hirsch and de cicco 2015 supports downloading hydrological and water quality variables from several sources the wq package by jassby and cloern 2015 aims at exploring environmental monitoring data various aspects of water supply reservoirs can be analyzed using packages reservoir by turner and galelli 2016 rssop by arabzadeh et al 2016 and wrss by arabzadeh et al 2017 this growing collection of software aimed at environmental and water resources topics shows that r packages are becoming a popular way to share research outputs in this field two versions of source code for the epanet simulation engine are currently available the latest release on the us epa website https www epa gov water research epanet is version 2 0 12 from 2008 a community group called open water analytics released a version 2 1 in 2016 https github com openwateranalytics epanet releases tag v2 1 since version 2 1 contains some extensions and bug fixes it was chosen for inclusion in epanet2toolkit the remainder of this article is organized as follows first we give a short description of epanet2tookit s design and list of new functions provided by the package according to category section 2 section 3 presents usage examples ranging from toolkit installation and full network simulation to solving more complex problems such as optimization and stochastic modeling finally we present future developments and conclusions section 4 2 design the epanet simulation engine as distinct from the windows r based graphical user interface can be invoked from the command line or through an application programming interface api the epanet api comprises 74 c functions for carrying out customized simulations the r package epanet2toolkit is designed as a wrapper for the epanet api r functions provided by the package have the same name as functions in the api but differ in function arguments return values and handling of error codes functions in the epanet api return an integer error code and provide requested values by reference for example the function engetnodeindex takes two arguments the node id and a pointer to an integer variable for storing the requested value listing 1 the value returned by the function is not the node index but a code indicating whether the function call finished successfully or encountered an error or warning condition to use this function the calling application needs to allocate an int variable for storing the requested index and corresponding pointer for passing to the function checking the error code returned by the function is good practice but not strictly required listing 1 c function prototype from epanet api image 1 epanet2toolkit exposes functions from the epanet api to the r environment through two steps first existing functions of the epanet api are wrapped by new c functions with return type void or sexp so that they can be called from r second new r functions are provided to call these new c functions the r functions check their arguments check the error codes returned by epanet and return the requested value as an example the r function engetnodeindex takes a single argument and returns the node index listing 2 an r script calling this function passes the node id as an argument and stores the returned index in the desired location or prints it to the screen in case the underlying api call returned an error or warning the r function raises an error or warning through r s exception handling system checking the error code returned from epanet happens on every function call and is transparent to the user until an error or warning condition actually occurs listing 2 r function from epanet2toolkit image 2 new functions for the r environment provided by epanet2toolkit are summarized in table 1 users familiar with the epanet api will recognize that the r functions share the name of the underlying epanet api functions they invoke using consistent function names provides a familiar interface to existing users and makes it easy to port existing applications to r running a full simulation of hydraulics and water quality and writing the results to a file is accomplished with the function enepanet the function requires an input file in epanet s inp format and writes output files in text and or binary format accessing all other functions requires opening the epanet engine with enopen and closing it with enclose hydraulic and water quality simulations may be carried out as extended period or stepwise simulations stepwise simulations allow programs to interrogate values at each step extended period simulations encapsulate stepwise simulations only reporting values for the times and network elements specified in the input file programs can use either stepwise or extended period functions for hydraulics and water quality but must be consistent within each category thus enopenh to encloseh are compatible with ensolveq but not with ensolveh enopenq to encloseq are compatible with enopenh to encloseh or ensolveh but not ensolveq information in the simulation engine can be interrogated and changed using an appropriate function beginning with enget or enset all of these have not been tabulated here but are documented in the package manual and symbolized in the table with the wildcard character in epanet2toolkit enget functions return the value of interest and enset functions return null invisibly unless an error occurs 3 example usage and capability 3 1 installation and full network simulation in order to use epanet2toolkit in an r session it must be installed and loaded from within an r session the install packages function downloads packages from a repository and installs them on the local system r selects packages from the repository according to operating system r also keeps track of installed packages so that users do not need to know or configure the package from their working directory this behavior contrasts with bindings for epanet which require compiling epanet for the relevant operating system and architecture and configuring the dll location the default behavior of install packages is to download pre compiled binary packages for windows and mac os r systems resulting in a quick installation for most users package binaries are not available for other platforms e g linux r and so in these cases packages are downloaded as source code and compiled locally local compilation increases install time slightly but still takes only a few minutes and happens automatically passing the installation option install tests includes the package test suite with the installation so that users can run tests as described in section 3 4 once a package is installed calling library loads it into the current r session for example the function enepanet carries out a full simulation and writes results to a report file listing 3 the network described in the inp file and the simulation results written in the rpt file can be further analyzed and visualized within r using the package epanetreader eck 2016 listing 3 installing the package and running a full network simulation image 3 once epanet2toolkit is available within r functions from the package can be used to access and modify network properties and used along with functions provided by r to carry out different types of analysis the following sections provide some examples 3 2 accessing and changing network properties to access or change network properties the first step is to open the epanet engine with enopen the value of a network parameter such as the length of the pipe having index 2 can be accessed using engetlinkvalue the first argument specifies the link index and the second argument specifies the parameter to be retrieved the value of this parameter can be changed using ensetlinkvalue the first two arguments are the same but the third argument specifies the new value for the parameter a subsequent call to engetlinkvalue confirms that the value was changed finally enclose closes the epanet engine further function calls could be added to listing 4 before enclose and run in sequence this workflow contrasts with using the epanet api from c in that calling different functions requires updating a script but not recompiling an application listing 4 accessing and changing network properties image 4 3 3 model calibration by univariate optimization consider a basic model calibration problem epanet s example network 1 which is included with the package was operated under a high demand condition with a view to estimating the pipe roughness conditions were typical for the time 00 00 except that a demand of 2000 gallons per minute was induced at node 23 index 7 under that condition pressure measurements of 112 11 110 87 and 110 32 psi were collected at nodes with indices 4 6 and 8 listing 5 shows one way of estimating pipe roughness assuming the same roughness value applies to all pipes in the network first enopen initializes epanet and processes network information then ensettimeparam and ensetnodevalue update the network to the conditions of this example only one time period is considered and the demand at one node has been changed next the optimize function built into r is used to find a value of pipe roughness that minimizes the sum of squared errors between the measured and modeled pressures the roughness value in this case from the hazen williams formula is constrained to fall in the interval between 50 and 150 the observed pressures and their node indices are passed through to the objective function by optimize finally the epanet engine is closed with a call to enclose listing 6 provides the user defined functions calibobj setallpiperoughness and sse used to implement the objective function for pipe roughness calibration in this example the function calibobj calls the helper function setallpiperoughess to change the roughness value of all pipes in the network to the provided value setallpiperoughness loops over all links in the network only updating the roughness if the link is of type pipe as opposed to pump or valve ensolveh carries out a full hydraulic simulation the function sse computes the sum of squared error between measured and modeled pressures running this example yields a c value of 131 and an objective value of 0 88 note in an r session listing 6 should be run before listing 5 so that the objective function is available to the optimizer listing 5 pipe roughness calibration with univerate optimization image 5 listing 6 objective function for pipe roughness calibration image 6 3 4 stochastic simulation stochastic simulation is often of interest in the context of modeling hydraulics under water demand forecasting scenarios again using example network 1 consider a problem where the water utility is interested in building a demand forecasting model and simulating the water network behavior under forecasts produced by this model the total water demand in the network measured at hourly intervals for a period of approximately one week is available in a comma separated values file named data csv the first line of this file contains the headings time and measurement the subsequent lines contain the corresponding timestamps and values of the measurements a seasonal arima 0 1 4 0 1 1 24 model is considered appropriate arandia et al 2016 and fitted to the data listing 7 shows how the measurement data is read into a data frame using the read csv function provided by the base r distribution and how a demand forecast for the next 24 h period is calculated using the sarima for function of the astsa package stoffer 2017 which must be previously installed a new demand pattern newpattern is computed by dividing the forecasts by their mean value in order to update the epanet input file with the calculated new pattern the toolkit is opened to work on network 1 with enopen the pattern time interval is set to 3600 s with ensettimeparam the existing time pattern is replaced by newpattern using ensetpattern the input file is saved with ensaveinpfile and finally the toolkit is closed with enclose the result is a modified file net1 forecast inp that can be readily used in hydraulic simulation listing 7 stochastic simulation using water demand forecasts image 7 3 5 test suite as part of the development process a collection of tests was created to verify the behavior of each r function provided by epanet2toolkit these tests take the form of examples included with the package manual and as a suite of tests for use with a testing framework examples are accessed through the help system for each function e g enepanet or help enepanet they are designed to run interactively and illustrate usage in contrast the test suite is designed to run automatically thus making it easier to detect changes that alter package behavior and to document bugs and fixes tests include rudimentary behavior checking for functions and implementations of the sample applications described in the documentation for the epanet programmer s toolkit rossman 2008 the sample applications are located in the tests testthat directory under the file names test epanet example 2 r and test epanet example 3 r because the tests were written to verify the performance of epanet2toolkit roughly one third are specific to the r environment the remaining tests represent a publicly available test suite for the epanet api and thus they may prove useful in detecting changes or bugs in future versions of the simulation engine provided tests have been installed as shown in listing 3 the test package function from testthat wickham 2011 runs the suite of tests listing 8 here the test reporter has been specified to generate more verbose output listing 8 running the package test suite image 8 4 conclusions this short communication has described epanet2toolkit a package for making epanet simulations in the r environment with the package two commands are needed to download compile if required and load the epanet simulation engine into the r environment these steps work on windows mac os and linux systems functions provided by the package map directly to functions in the epanet api enabling developers to port applications into and out of r standard epanet simulations can be invoked with a single function and customized applications can be developed to leverage the visualization and analysis capabilities of r including other packages in the r ecosystem advanced users of epanet2toolkit should keep in mind a few known weak points with version 0 2 1 an r object wrapping an epanet simulation is not provided so it is only possible to run one simulation at a time within an r session in its current version epanet2toolkit provides 55 of the 74 functions exposed by version 2 1 of epanet s c api most functions not yet supported in r relate to the curves and demands sections on the inp file future work on the package could address these areas by improving accessibility of water network simulations epanet2toolkit aims to enable more robust data driven decision making for these critical infrastructure systems epanet2toolkit is available under the mit license and welcomes contributions from third party developers via github github com bradleyjeck epanet2toolkit the github page issues tab is the place to raise questions about the package including bugs and proposed enhancements 
26362,the epanet software for modeling piping networks is widely used for the design and analysis of water systems this short communication describes epanet2toolkit a package for accessing the epanet simulation engine and application programming interface from the r environment for computing and graphics users of the package may run extended period simulations of hydraulics and water quality or create custom applications using the data exploration visualization and analysis capabilities of r epanet2toolkit enables interactive use of epanet on a variety of computing platforms provides integrated error checking and contributes a publicly available test suite the package is available through the comprehensive r archive network and github com keywords water networks r epanet hydraulic modeling software availability epanet2toolkit is available on the world wide web at https cran r project org package epanet2toolkit and https github com bradleyjeck epanet2toolkit license mit system requirements r version 3 0 2 or higher installation using install packages function 1 introduction modeling the behavior of water piping networks is an active area both in industry and academia with published studies covering a very broad range of problems including design optimization and management several pieces of software are available for simulating piping networks including proprietary packages such as watergems r infowater r and kypipe and freely available packages such as epanet rossman 2000 originally developed by the united states environmental protection agency epanet has over 2000 citations on google scholar making it the most popular choice for academic applications the epanet simulation engine is written in the c language for portability and performance bindings have been developed to call the simulation engine from several other languages the programmer s toolkit rossman 2008 includes bindings for visual basic and pascal more recently bindings for matlab eliades et al 2016 and python pathirana 2016 have been developed this paper describes bindings for the r environment r core team 2013 provided in the form of an add on package called epanet2toolkit as an r package for water resources modeling epanet2toolkit joins a growing group of packages for water applications the package epanetreader eck 2016 compliments the simulation capability of epanet2toolkit with support for parsing epanet data files into r objects in a similar vein the package swmmr by leutnant 2017 provides an interface for the storm water management model the dataretrieval package by hirsch and de cicco 2015 supports downloading hydrological and water quality variables from several sources the wq package by jassby and cloern 2015 aims at exploring environmental monitoring data various aspects of water supply reservoirs can be analyzed using packages reservoir by turner and galelli 2016 rssop by arabzadeh et al 2016 and wrss by arabzadeh et al 2017 this growing collection of software aimed at environmental and water resources topics shows that r packages are becoming a popular way to share research outputs in this field two versions of source code for the epanet simulation engine are currently available the latest release on the us epa website https www epa gov water research epanet is version 2 0 12 from 2008 a community group called open water analytics released a version 2 1 in 2016 https github com openwateranalytics epanet releases tag v2 1 since version 2 1 contains some extensions and bug fixes it was chosen for inclusion in epanet2toolkit the remainder of this article is organized as follows first we give a short description of epanet2tookit s design and list of new functions provided by the package according to category section 2 section 3 presents usage examples ranging from toolkit installation and full network simulation to solving more complex problems such as optimization and stochastic modeling finally we present future developments and conclusions section 4 2 design the epanet simulation engine as distinct from the windows r based graphical user interface can be invoked from the command line or through an application programming interface api the epanet api comprises 74 c functions for carrying out customized simulations the r package epanet2toolkit is designed as a wrapper for the epanet api r functions provided by the package have the same name as functions in the api but differ in function arguments return values and handling of error codes functions in the epanet api return an integer error code and provide requested values by reference for example the function engetnodeindex takes two arguments the node id and a pointer to an integer variable for storing the requested value listing 1 the value returned by the function is not the node index but a code indicating whether the function call finished successfully or encountered an error or warning condition to use this function the calling application needs to allocate an int variable for storing the requested index and corresponding pointer for passing to the function checking the error code returned by the function is good practice but not strictly required listing 1 c function prototype from epanet api image 1 epanet2toolkit exposes functions from the epanet api to the r environment through two steps first existing functions of the epanet api are wrapped by new c functions with return type void or sexp so that they can be called from r second new r functions are provided to call these new c functions the r functions check their arguments check the error codes returned by epanet and return the requested value as an example the r function engetnodeindex takes a single argument and returns the node index listing 2 an r script calling this function passes the node id as an argument and stores the returned index in the desired location or prints it to the screen in case the underlying api call returned an error or warning the r function raises an error or warning through r s exception handling system checking the error code returned from epanet happens on every function call and is transparent to the user until an error or warning condition actually occurs listing 2 r function from epanet2toolkit image 2 new functions for the r environment provided by epanet2toolkit are summarized in table 1 users familiar with the epanet api will recognize that the r functions share the name of the underlying epanet api functions they invoke using consistent function names provides a familiar interface to existing users and makes it easy to port existing applications to r running a full simulation of hydraulics and water quality and writing the results to a file is accomplished with the function enepanet the function requires an input file in epanet s inp format and writes output files in text and or binary format accessing all other functions requires opening the epanet engine with enopen and closing it with enclose hydraulic and water quality simulations may be carried out as extended period or stepwise simulations stepwise simulations allow programs to interrogate values at each step extended period simulations encapsulate stepwise simulations only reporting values for the times and network elements specified in the input file programs can use either stepwise or extended period functions for hydraulics and water quality but must be consistent within each category thus enopenh to encloseh are compatible with ensolveq but not with ensolveh enopenq to encloseq are compatible with enopenh to encloseh or ensolveh but not ensolveq information in the simulation engine can be interrogated and changed using an appropriate function beginning with enget or enset all of these have not been tabulated here but are documented in the package manual and symbolized in the table with the wildcard character in epanet2toolkit enget functions return the value of interest and enset functions return null invisibly unless an error occurs 3 example usage and capability 3 1 installation and full network simulation in order to use epanet2toolkit in an r session it must be installed and loaded from within an r session the install packages function downloads packages from a repository and installs them on the local system r selects packages from the repository according to operating system r also keeps track of installed packages so that users do not need to know or configure the package from their working directory this behavior contrasts with bindings for epanet which require compiling epanet for the relevant operating system and architecture and configuring the dll location the default behavior of install packages is to download pre compiled binary packages for windows and mac os r systems resulting in a quick installation for most users package binaries are not available for other platforms e g linux r and so in these cases packages are downloaded as source code and compiled locally local compilation increases install time slightly but still takes only a few minutes and happens automatically passing the installation option install tests includes the package test suite with the installation so that users can run tests as described in section 3 4 once a package is installed calling library loads it into the current r session for example the function enepanet carries out a full simulation and writes results to a report file listing 3 the network described in the inp file and the simulation results written in the rpt file can be further analyzed and visualized within r using the package epanetreader eck 2016 listing 3 installing the package and running a full network simulation image 3 once epanet2toolkit is available within r functions from the package can be used to access and modify network properties and used along with functions provided by r to carry out different types of analysis the following sections provide some examples 3 2 accessing and changing network properties to access or change network properties the first step is to open the epanet engine with enopen the value of a network parameter such as the length of the pipe having index 2 can be accessed using engetlinkvalue the first argument specifies the link index and the second argument specifies the parameter to be retrieved the value of this parameter can be changed using ensetlinkvalue the first two arguments are the same but the third argument specifies the new value for the parameter a subsequent call to engetlinkvalue confirms that the value was changed finally enclose closes the epanet engine further function calls could be added to listing 4 before enclose and run in sequence this workflow contrasts with using the epanet api from c in that calling different functions requires updating a script but not recompiling an application listing 4 accessing and changing network properties image 4 3 3 model calibration by univariate optimization consider a basic model calibration problem epanet s example network 1 which is included with the package was operated under a high demand condition with a view to estimating the pipe roughness conditions were typical for the time 00 00 except that a demand of 2000 gallons per minute was induced at node 23 index 7 under that condition pressure measurements of 112 11 110 87 and 110 32 psi were collected at nodes with indices 4 6 and 8 listing 5 shows one way of estimating pipe roughness assuming the same roughness value applies to all pipes in the network first enopen initializes epanet and processes network information then ensettimeparam and ensetnodevalue update the network to the conditions of this example only one time period is considered and the demand at one node has been changed next the optimize function built into r is used to find a value of pipe roughness that minimizes the sum of squared errors between the measured and modeled pressures the roughness value in this case from the hazen williams formula is constrained to fall in the interval between 50 and 150 the observed pressures and their node indices are passed through to the objective function by optimize finally the epanet engine is closed with a call to enclose listing 6 provides the user defined functions calibobj setallpiperoughness and sse used to implement the objective function for pipe roughness calibration in this example the function calibobj calls the helper function setallpiperoughess to change the roughness value of all pipes in the network to the provided value setallpiperoughness loops over all links in the network only updating the roughness if the link is of type pipe as opposed to pump or valve ensolveh carries out a full hydraulic simulation the function sse computes the sum of squared error between measured and modeled pressures running this example yields a c value of 131 and an objective value of 0 88 note in an r session listing 6 should be run before listing 5 so that the objective function is available to the optimizer listing 5 pipe roughness calibration with univerate optimization image 5 listing 6 objective function for pipe roughness calibration image 6 3 4 stochastic simulation stochastic simulation is often of interest in the context of modeling hydraulics under water demand forecasting scenarios again using example network 1 consider a problem where the water utility is interested in building a demand forecasting model and simulating the water network behavior under forecasts produced by this model the total water demand in the network measured at hourly intervals for a period of approximately one week is available in a comma separated values file named data csv the first line of this file contains the headings time and measurement the subsequent lines contain the corresponding timestamps and values of the measurements a seasonal arima 0 1 4 0 1 1 24 model is considered appropriate arandia et al 2016 and fitted to the data listing 7 shows how the measurement data is read into a data frame using the read csv function provided by the base r distribution and how a demand forecast for the next 24 h period is calculated using the sarima for function of the astsa package stoffer 2017 which must be previously installed a new demand pattern newpattern is computed by dividing the forecasts by their mean value in order to update the epanet input file with the calculated new pattern the toolkit is opened to work on network 1 with enopen the pattern time interval is set to 3600 s with ensettimeparam the existing time pattern is replaced by newpattern using ensetpattern the input file is saved with ensaveinpfile and finally the toolkit is closed with enclose the result is a modified file net1 forecast inp that can be readily used in hydraulic simulation listing 7 stochastic simulation using water demand forecasts image 7 3 5 test suite as part of the development process a collection of tests was created to verify the behavior of each r function provided by epanet2toolkit these tests take the form of examples included with the package manual and as a suite of tests for use with a testing framework examples are accessed through the help system for each function e g enepanet or help enepanet they are designed to run interactively and illustrate usage in contrast the test suite is designed to run automatically thus making it easier to detect changes that alter package behavior and to document bugs and fixes tests include rudimentary behavior checking for functions and implementations of the sample applications described in the documentation for the epanet programmer s toolkit rossman 2008 the sample applications are located in the tests testthat directory under the file names test epanet example 2 r and test epanet example 3 r because the tests were written to verify the performance of epanet2toolkit roughly one third are specific to the r environment the remaining tests represent a publicly available test suite for the epanet api and thus they may prove useful in detecting changes or bugs in future versions of the simulation engine provided tests have been installed as shown in listing 3 the test package function from testthat wickham 2011 runs the suite of tests listing 8 here the test reporter has been specified to generate more verbose output listing 8 running the package test suite image 8 4 conclusions this short communication has described epanet2toolkit a package for making epanet simulations in the r environment with the package two commands are needed to download compile if required and load the epanet simulation engine into the r environment these steps work on windows mac os and linux systems functions provided by the package map directly to functions in the epanet api enabling developers to port applications into and out of r standard epanet simulations can be invoked with a single function and customized applications can be developed to leverage the visualization and analysis capabilities of r including other packages in the r ecosystem advanced users of epanet2toolkit should keep in mind a few known weak points with version 0 2 1 an r object wrapping an epanet simulation is not provided so it is only possible to run one simulation at a time within an r session in its current version epanet2toolkit provides 55 of the 74 functions exposed by version 2 1 of epanet s c api most functions not yet supported in r relate to the curves and demands sections on the inp file future work on the package could address these areas by improving accessibility of water network simulations epanet2toolkit aims to enable more robust data driven decision making for these critical infrastructure systems epanet2toolkit is available under the mit license and welcomes contributions from third party developers via github github com bradleyjeck epanet2toolkit the github page issues tab is the place to raise questions about the package including bugs and proposed enhancements 
26363,a gis based procedure for preliminary mapping of pluvial flood risk at metropolitan scale cristina di salvo a francesco pennica a giancarlo ciotoli a b gian paolo cavinato a a cnr istituto geologia ambientale e geoingegneria igag area della ricerca di roma 1 via salaria km 29 300 c p 10 monterotondo 00015 roma italy cnr istituto geologia ambientale e geoingegneria igag area della ricerca di roma 1 via salaria km 29 300 c p 10 monterotondo roma 00015 italy b istituto nazionale di geofisica e vulcanologia sezione roma 2 roma italy istituto nazionale di geofisica e vulcanologia sezione roma 2 roma italy corresponding author a gis based procedure for mapping pluvial flood risk in urban areas is proposed risk is expressed through an index calculated as the sum of susceptibility and potential impact combined in a matrix the susceptibility is defined as the probability of a flooding to occur because of the ground morphology and the spatial probability density of historical floodings the potential impact was evaluated by considering the consequences of damages on human health environment cultural heritages and economic activities and accounts for the potential cost of damage both the susceptibility and the potential impact are calculated by elaborations of base data in gis environment despite many limitations the methodology furnishes a tool for a preliminary screening of areas potentially subjected to pluvial flood useful for a municipal scale mapping it permits comparative analysis for detecting areas higher at risk helping prioritizing the emergency management and the planning of mitigation actions keywords pluvial flood susceptibility analysis impact assessment risk map rome 1 introduction floods are one of the most common and widely distributed natural hazard to life and property flood hazard assessment is explicitly required by the eu floods directive 2007 60 ec in order to prevent damages and plan sustainable management schemes urban areas are particularly vulnerable to flood due to the modification of the natural environment with often deficient or improper land use planning and the high concentration of population buildings economic activities distribution networks and cultural heritages the urban development generally causes the modification of the natural hydrological cycle healthy waterways org fig 1 the sealing of natural surfaces and the underground channeling of drainage network into pipes abruptly reduce the evapotranspiration from plants the water exchange between surface and underground water and thus the infiltration these factors contribute to strongly increase surface water runoff leading to deterioration of the water quality in the case of intense storms rainwater overwhelms the sewer system which becomes unable to drain excess water anymore the role of climate change remains difficult to assess mainly for the complexity of separating non stationarities related to anthropogenic climate from those associated with natural low frequency climate oscillations ishak et al 2013 thus even if there is high confidence that climate change will be responsible of the increase in the number and frequency of extreme rainfall events bates et al 2008 caution should be used in the evaluation of rainfall changes with respect to flood hazard johnson et al 2016 floods can be described and categorized based on a combination of sources causes and impacts into river or fluvial floods pluvial floods coastal floods groundwater floods often the different types of flooding are strongly linked together indeed all types of floods are triggered by the interaction of various physical processes including hydrological pre conditions e g soil saturation meteorological conditions e g amount intensity and spatial distribution of precipitation runoff generation processes e g infiltration and runoff on hillslopes and river routing e g superposition of flood waves increased infiltration and a rise in the water table may result in less water flowing from rivers to ground with consequent rise of river stage which results in more likely overtopping their banks a rise in the water table during periods of higher than normal rainfall may also mean that land drainage networks such as storm sewers cannot function properly if groundwater is able to flow into them underground jha et al 2012 in many cases river groundwater and pluvial flooding are difficult to distinguish moreover the combination of multiple statistically dependent variables or events not necessarily extreme can have a significant influence on the magnitude of the resultant flood and lead to an extreme impact compound events leonard et al 2014 this adds complexity in modelling the impact of floods in particular pluvial flood is very quick and heterogeneous and occurs at a range of spatial and temporal scales much smaller than are usually considered in fluvial and coastal flood risk analysis the spatial temporal characteristics of rainfall can be a significant factor in influencing the pattern of urban flooding luyckx et al 1998 segond et al 2007 haberlandt et al 2008 and the spatial heterogeneity of the resulting inundation can be large apel 2016 this means that the magnitude and extension of related potential damage is hardly predictable well established and tested procedures exist for urban river flood risk assessment e g dawson et al 2008 morita 2008 floodsite 2009 techniques for the assessment of groundwater flooding risk also exist including fine scale evaluations of hydraulic relationships between water table drainage network and sea level associated to groundwater numerical modelling rotiroti et al 2014 being related to water table rising in areas with flat morphology such in lowlands or coastal areas it is often extensive and the location of floods can be highly predictable conversely techniques for the study and the management of flood risk caused by extreme rainfall are actually poorly considered niemann and illgen 2011 zhou et al 2012 and necessarily need to be explored given the rising of the magnitude and frequency of storms the common approach for risk mapping is to use hydrologic numerical models which simulate the water height over urban surface as a function of many variables however this approach requires a deep knowledge about the urban system e g sewer network pipes infrastructure capacity land use soil type at a proper scale as well as a detailed mapping of urban elements driving the water flowpaths e g buildings location sidewalks height manholes position in order to correctly model local run off and surface flow processes the nonlinear response of sewer systems to inflows needs to be effectively coupled to surface flow pathways and damage calculations blanc et al 2012 to simulate the interactive exchange of storm water in the sewer system and on the surface chang et al 2015 djordjevic et al 1999 lee et al 2015 leandro et al 2009 fraga et al 2015 the probability of occurrence of a storm events is traditionally calculated by intensity duration frequency curves characteristic of homogeneous climatic regions stochastic spatially explicit rainfall simulators are also used in order to fully describe possible rainfall intensities and their spatial coverage apel 2016 burton et al 2008 hundecha et al 2009 willems 2001 wheater et al 2005 the simplification adopted of assuming that the likelihood of the urban flooding is equal to that of a rainfall event with certain return period is considered acceptable for intense rainfall events and often this is the adopted procedure due to difficulties in estimating the real return period and subsequently the likelihood of the flooding event leitao et al 2013 however the inherent complexity of defining and quantifying floodwaters often limits the applicability of such detailed models to local scale case studies conversely the management of emergency requires knowledge of flood risk at urban scale in order to assess which areas are higher at risk requiring prior interventions the scarcity of urban scale mapping approaches is pushing to develop suitable definition of risk and a range of methodologies for appropriate assessment blanc et al 2012 indeed due to the high intensity variability and scarce predictability of storms calculating the expected floodwater depths for rainfall thresholds can be highly uncertain and can result in scarce utility for emergency management rather than assessing susceptibility through uncertain flood depth scenarios susceptibility can be defined as the likelihood of a storm occurring in an area on the basis of local terrain conditions santangelo et al 2011 the degree of morphological susceptibility and the spatial probability density of the historical observed floods can be used to perform a first level urban scale pluvial flood mapping and can be useful to conveniently address emergency interventions optimizing resources allocation the morphological susceptibility is the attitude of a portion of terrain to experience flood due to its elevation with respect to surroundings and physical features such as its steepness the spatial probability distribution is the function defining the distribution of observed floodings in the space as further explained in section 3 during recent years the improvement of geographic information systems gis have allowed us to consider alternative risk mapping approaches commonly gis systems are used to collect and elaborate spatial data required as input for hydrological and stromwater models moreover a wide number of gis tools and plugins are rapidly evolving tailored for hydrological analysis functional to the evaluation of flood depth and extension gis tools can also furnish a base for alternative simplified approaches aimed at the evaluation of factors concurring to pluvial flood risk and their combination indeed in gis environment each kind of data can be represented through its spatially distributed values allowing many variables to be included in calculations and making it easy to perform detailed risks evaluation on a large scale djordjevic et al 1999 2005 schmitt et al 2005 obermayer et al 2010 these include building high resolution digital terrain models integrating river bathymetry and the surrounding topography merwade et al 2008 or improving the accuracy of flood risk assessment when developing decision support systems for integrated flood management qi and altinakar 2011 gis based methodologies for pluvial flood risk assessment allow an optimal management and use of available municipal scale data a high degree of automation in the application and a good transferability of methods scheid et al 2013 for example gis allow to represent vector data such as morphological features i e depressions observed floods or assets exposed to risk buildings roads railways both the location shape and related attributes can be easily managed assigning weights or ranking the elements vectors can be translated into raster layer and be used as terms for map algebra calculations allowing appropriate spatial mapping of selected themes the capability to easily perform large scale calculations between raster levels allows to combine characteristics of the natural and anthropic environment concurring to the definition of risk the wide range of tools for spatial relationships evaluation e g neighboring and cluster analysis overlay spatial density permits to explore and quantify spatial correlation between variables concurring to risk in this framework the objectives of this study are 1 to test a chain of gis tools for mapping the susceptibility to pluvial flood at municipal scale 2 to derive impact scenarios compatible with the available data about past flood impact quantifying the related uncertainty 3 to integrate the susceptibility and potential impact in order to derive a preliminary municipal scale map of pluvial flood risk useful for detecting areas higher at risk and prioritize interventions both in the emergency and planning phases following this approach the comprehensive pluvial flood risk assessment is a synthetic evaluation of natural and social aspects concurring to flood zou et al 2013 many of these factors have not total unified quantitative standards which make the flood risk assessment system complex and difficult to operate du et al 2006 jiang et al 2008 li et al 2008 2010 a common technique is to assign proper weights to each variable concurring to flood risk combining those together in a suitable risk matrix niemann and illgen 2011 leitao et al 2013 by which the definition of risk can be done as a purely qualitative additive of multiplicative process scheid et al 2013 the risk matrix usually combines the likelihood of a flood to occur and the estimated consequences or potential impact both likelihood and estimated consequences are expressed by classes the sum of which allow to define risk levels few examples exist of flood risk grade classification matrix in which risk is defined as the sum of the two components zou et al 2013 defined the risk matrix as the sum of hazard accounting for many factors including flood velocity depth land use and average annual precipitation and vulnerability accounting for various elements such as population and industrial density road surface extension in the swiss flood hazard map van alphen et al 2009 hazard classes are defined through a matrix composed by flood intensity as a combination of flow depth and discharge per unit width and probability frequency of flood matrix classes are referred to legal practices for urban planning and building regulations the computation of risk levels through a simple sum matrix leads to many limitations resulting in uncertainties difficult to quantify cox 2009 the poor resolution due to the same rating assigned to quantitatively very different elements exposed the subjective categorization this means that different users may obtain different rating for risks which should be quantitatively similar despite those limitations matrixes are helpful in reducing the subjectivity in risk definition and represent a valid and quick methodology for a preliminary assessment of risk helping the detection of areas which should primarily be the subject of further detailed analysis or structural interventions in our study the pluvial flood risk is defined through an index which is calculated as the sum of the susceptibility of a flood to occur and the potential impact combined in a matrix the susceptibility accounts both for the spatial probability density of documented floodings and the terrain morphology a preliminary screening of areas prone to flood due to morphological features depressions was performed and results were then coupled with historical observed floodings depressions and observed flooded areas constitute the flood prone areas and their degree of susceptibility was assessed on the basis of the magnitude of parameter composing the feature driving the susceptibility through elaborations performed by gis tools the potential impact was assessed accounting for the potential damage related to the intrinsic features of considered elements at risk and the related costs associated to flood damage the impact considers the overall social economic and environmental value allowing a simplified detection of the value of goods buildings infrastructures and activities located in the flood prone areas which primarily would make the community suffer in the case of loss or damage all informative levels used in the analysis are stored in a database and analyzed through existent gis based techniques and then adapting the methodology to the study context 2 case study the case study is the urban area of rome the central sector is characterized by small hills with gentle slope values between 0 5 and 30 carved from a tuff plateau overlaying pleistocene sedimentary rocks by tiber tributaries and the groundwater table largely more than 3 m deep la vigna et al 2016 toward south west the city spreads to the thyrrenian sea coast and is characterized by a flat morphology and shallow groundwater river flooding affects the city of rome along the valleys of tiber and aniene rivers and tributary streams case 1 in fig 2 this type of risk as defined in the tiber river basin masterplan autorità di bacino del tevere 2006 was the subject of many studies and numerical modelling calenda et al 1997 2005 2009 natale and savi 2007 aimed at the construction of structural mitigation measures such as the upstream dams corbara nazzano dams as well as river walls muraglioni recently hydraulic models by natale and savi 2007 and calvo and savi 2009 demonstrated that river flooding with180 and 1000 years return period respectively can affect the city on the other hand the coastal areas i e ostia casal palocco and acilia case 2 in fig 2 close to or below the sea level and where groundwater level is controlled by draining pumps are yearly affected by groundwater flooding this type of flooding is caused by the water table rise due to intense rainfall and or to high stage of the local stream network not compensated by draining pumps groundwater flooding is strongly sensitive to the dynamic interaction between sea level oscillation storm surge water table oscillation groundwater surface channel water interaction as well as the dynamic interaction with artificial withdrawals thus groundwater flood risk cannot be easily assessed through a simple gis based analysis layers overlay but would instead require an adequate monitoring of river and channels level stage groundwater oscillation sub hourly rainfall record for supporting detailed models adopting a simplified assumption in this study the floodings observed in the coastal areas are assumed to be due to intense rainfall overwhelming the drainage network infiltration toward the ground is also neglected since the water table is very close to the ground implication of high river stage and water table rising are not considered this limiting the applicability of the method in coastal areas conversely the described methodology is tailored for pluvial flooding broadly defined as a type of flooding resulting from heavy rainfall generated overland flow and ponding before the runoff enters a natural or man made watercourse drainage system or sewer or when it cannot enter it because the network is full to capacity falconer et al 2009 pluvial flooding is mainly observed in the central densely urbanized areas case 3 fig 2 2 1 rainfall regime rome exhibits a precipitation pattern typical of the mediterranean regime with the maximum in fall winter and the minimum in summer colacino and purini 1986 recent climate studies show that in central italy since the end of the 19th century there is a tendency toward an increase in precipitation intensity 6 and 9 for winter and fall trends respectively despite the significant negative trend of wet days number 15 and total precipitation 10 per century in total yearly precipitation brunetti et al 2004 in particular a marked decrease in the frequency of low intensity precipitation and an increase of high intensity precipitation events have been observed specifically the analysis of long time series 1862 2004 of seasonal rainfall in the city of rome shows a decreasing trend during winter and fall season villarini et al 2010 in general under projected global warming the resulting changes to ocean atmospheric circulation patterns are likely to lead to shifts in the location magnitude and frequency of extreme precipitation events and the flooding associated with them kiem and verdon kidd 2013 moreover multiple local factors such as catchment anthropic modification or the antecedent soil moisture condition introduce non linearities in the rainfall runoff process which are poorly resolved in global circulation models this increases the uncertainty in separating and quantifying the fraction of risk due to climate change from the one due to local setting many studies have documented the violations of the stationarity assumption for rainfall and temperature records over central italy e g brunetti et al 2006 2000 and over rome in particular e g maheras et al 1992 colacino and purini 1986 covariate analyses highlight the role of seasonal and interannual variability of large scale climate forcing as reflected in three teleconnection indexes north atlantic oscillation atlantic multidecadal oscillation and mediterranean index for modeling rainfall and temperature over rome villarini et al 2010 a correlation has been found between the three indexes trends and the behavior of rainfall and temperature records in particular nao tends to be a significant predictor during the winter season while the mediterrean index is a significant predictor for the majority of the cases providing insights to the slowly varying oscillations in maximum temperature observations for rome however despite the important advances in climate studies the evaluation of local variations induced by climate change remains highly uncertain this is also because the regional assessment often focus on percentage changes which does not easily translate into impacts of practical significance johnson et al 2016 in this study the frequency of intense storms is not directly taken into account in susceptibility calculation the reason for this choice is that the available database of observed floods used for elaborations is referred to a very short period 2001 2014 fig 3 which is too small to account for pluvial flood trends indeed it was mentioned that interannual and multidecadal variability affect the climate in the study area introducing non stationarities in rainfall trends estimating water ponding volumes and related flood scenarios capturing such non stationarities would require the collection and trend analysis for longer period in order to assess the return period of pluvial floods as well as comparing the model with extension and depth of floods which are data currently not available in other words this study considers the susceptibility as directly dependent on the ground morphology and the overall spatial density of observed floods instead of relating susceptibility to quantitative flood scenarios 3 materials methods the pluvial flood risk map was developed by means of a susceptibility analysis and an impact assessment fig 4 a and b respectively data collected for the analysis include the following informative layers observed pluvial floodings with polygon geometry 30 polygons representing areas which experienced pluvial floods between 2004 and 2007 collected mapped and published by the civil protection of rome municipality comune di roma ufficio extradipartimentale della protezione civile 2008 polygons correspond to the observed extent of flooding observed pluvial floodings with point geometry these comprise 1430 points representing the location of floodings during storm events occurred between 2001 and 2014 fig 3 coming from intervention database of fire department civil protection of rome municipality and web media the point approximates the location of the required intervention the point database was divided in two dataset on the basis of the date of occurrence points between 2004 and 2008 916 points and between 2009 and 2014 514 points used to build and validate the susceptibility map respectively all the data collected has been imported in a specialized section of a postgis geodatabase previously developed in the framework of the urbisit research project the pluvial flood risk data has been stored in a new postgresql schema in the bdgt geodatabase the audb webgis application fig 5 has been developed with the aim of centralizing and simplifying the management of flooding data through a platform independent interface and to ease the process of updating and improving the risk map the webgis key features include the ability for the user to manage insert update delete information about observed flooding events visualize and filter by a range of dates the flooding data on map and in a table navigate wms web map service layers on google basemaps further details are in reported in additional material 3 1 susceptibility analysis the susceptibility is defined as the probability of a flooding to occur in a certain area area prone to flood because of the ground morphology e g presence of topographic depressions area and fill volume of depressions the spatial probability density of observed floodings and the proximity to sewerage excess storm water flows and accumulates in catchment s lowest elevation areas which are for this reason particularly susceptible to flooding however floods can also be related to insufficient sewer capacity or poor manhole maintenance in those cases it can affect also areas which are not topographic lows thus the susceptibility was assessed starting from a preliminary screening of areas prone to flood due to morphological features depressions coupled with observed floodings flood observations dataset composed by point and polygon geometry features depressions and observed flooded areas constitute the flood prone areas and their relative degree of susceptibility was assessed through elaborations performed by gis tools finally in order to include areas close to sewer pipes assumed to be primarily at risk when sewer network is overwhelmed a hazard area was considered by buffering pipes centerlines and then adding such areas to the susceptibility map the buffering was needed in order to overcome the cartographic uncertainty in lines positioning the susceptibility analysis consists of the following phases fig 4a a dtm reconstruction and pre processing b detection and selection of topographic depression c processing of observed floodings d weighting depressions by the flood density and fill volume e weighting the flooded areas by fill depth f deriving sewerage pipe flood hazard areas g combining flood areas fill depth grid weighted depressions grid and sewer pipe flood hazard areas to obtain the susceptibility map of pluvial flood prone areas 3 1 1 a dtm reconstruction and pre processing a high resolution dtm 2 2 m was built through the anudem algorithm hutchinson and dowling 1991 anu fenner school of environment and society and geoscience australia 2008 by using lines and point ground elevation from 1 5000 vector cartography fig 4a fig 7a this tool interpolates ground elevation values by imposing constraints that ensure a connected drainage structure and the correct representation of ridges and streams from input contour data the choice of the cell resolution is consistent with the recommendations of cell size between 1 1 m and 5 5 m dtm resolution for urban flood analysis mark et al 2004 adeyemo et al 2008 then the filling dtm tool available in gis environment was used to fill small dtm sinks dtm processing errors to obtain a hydrological correct dtm 3 1 2 b detection and selection of topographic depression topographic depressions or ponds are defined as areas having low bottom elevation surrounded by an edge with higher elevation i e difference between filled dtm and the original dtm with no downslope flowpath and which can contain one or many sinks topographic depressions are part of a large framework of flood protection to control water movement in a specific time scale vesakoski et al 2014 the volume of water detention and direct surface runoff are mainly driven by the storage capacity of depressions on a watershed scale depressions were detected starting from the dtm by automated depression evaluation tool available in gis environment this tool is commonly used to identify areas containing small sinks which should be filled to obtain a hydrologically correct surface in this work depressions are considered as areas where excess water accumulates in case of drainage system overwhelming and thus represent flood prone areas in order to exclude negligible and or unrealistic features dtm processing errors depressions were selected following previous experiences adeyemo et al 2008 depressions were selected based on their fill volume and fill depth after performing a sensitivity analysis the sensitivity analysis was aimed at calibrating the threshold values for minimum volume and fill depth which can be assumed for depression selection in order to delete small depressions while minimizing the loss of potentially susceptible area and volume the trend of total depressions area and volume as a function of minimum fill depth and volume thresholds was evaluated through area and volume loss curves fig 6 a and b the volume and area loss curves show a gentle slope for minimum fill depth of 0 001 m and 0 01 m respectively above these fill depth thresholds the curves slope increase this suggesting a significant increase of area loss the volume and area loss as a function of minimum volume is negligible below 10 m3 and 0 1 m3 respectively by using a minimum volume threshold of 0 1 m3 the minimum fill depth of the resulting selected depressions is 0 00015 m which is below the fill depth value minimizing both area and volume loss 0 001 m on this basis the minimum volume of 0 1 m3 was considered as the best threshold allowing a reliable errors filtering for a 2 2 m cell dtm without losing a high percent of depression area and volume 3 1 3 c weighting the depressions layer by the flood density and fill volume the spatial overlay between observed floodings and the layer of depressions highlights that not all floodings occur within topographic depressions this result confirms that pluvial flood may also be due to anthropic causes since occurs where apparently no morphological or hydrological susceptibility exist furthermore it is possible to note that some depressions may contain more than one observed flooding occurred at different times i e higher spatial density the spatial probability density which is a function defining the distribution of observed floodings in the space was used to correctly map flood susceptibility the spatially discontinuous variable point and polygon observed floods is elaborated by means of the kernel density algorithm silverman 1986 obtaining a continuous variable representing the spatial data structure of the observed floods distribution the kernel density is a smoothing non parametric technique used for estimating the probability density function of floods visualizing the underlying distribution of a continuous variable overcoming the often irregularly positioning in space of sampling points therefore in order to correctly map flood susceptibility the kernel density algorithm was used to build the dens grid which represents the number of observed floodings per squared km the density map was then standardized and transformed in a raster map with values ranging between 0 and 1 with higher values corresponding to higher flood density fig 7 b another parameter driving the susceptibility to flood is the fill volume of depressions indeed depression with lower fill volumes means faster complete floodwater filling and thus higher susceptibility to flood for this reason depressions were ranked based on their standardized fill volume with higher score corresponding to lower fill volumes and converted to grid depr grid the dens and depr layers were multiplied in order to weight the depressions the resulting layer was classified according to the quantile distribution to obtain a 5 class layer weighted depressions the higher the class number the higher the flood potential 3 1 4 d processing of observed floodings punctual data represent the location of an observed flood which subtends a flooded area which extent is unknown the real spatial extension of floods is a function of the rainfall intensity mm hour and the terrain morphology in order to evaluate and approximate the potential extension of floods a buffer area around points was considered which elevation range is explored in order to assess the susceptibility based on the relative elevation of each cell with respect to the point elevation a nearest neighbor analysis based on average cluster considering euclidean distance as similarity measurement was accomplished the analysis uses the distance between each point and its closest neighboring point in a dataset to determine if the point pattern is random regular or clustered the analysis was performed by means of average nearest neighbor and near tools the average nearest neighbor hertz 1909 clark and evans 1954 hereafter ann calculates the distances between each feature centroid and its nearest neighbor s centroid location if the distance is minor than the expected average distance with expected average distance being based on a hypothetical random distribution with the same number of features covering the same total area the feature being analyzed is considered as part of a clustered distribution otherwise it is dispersed taniar et al 2010 the ann analysis allows to assess the clustering degree in the entire sample calculating the hypothetical distance between points average expected distance 669 46 m which is then compared with the average nearest neighbor distance the nearest neighbor ratio or index nni clark and evans 1954 is calculated as the observed distance divided by the expected average distance when index is less than 1 the pattern exhibits clustering if the index is greater than 1 the trend is toward dispersion the distance between each clustered point nni 1 and its nearest was then averaged the average near distance h between clustered points is 202 3 m table 1 it is assumed that h equals the theoretical minimum extension of a flood in a condition of flat or gentle slope morphology in other words where no terrain reliefs exist it is assumed that two point floods being at distance less or equal to 202 m are part of the same flood starting from this principle it is assumed that a circular 200 m radius buffer around a point can be considered as area susceptible to flooding this assumption allowed approximating the area reasonably considered as susceptible to flooding due to its proximity to an observed flood then in order to consider the second factor potentially driving the flooding extent terrain morphology a screening was performed on cells in the buffered areas based on the comparison with the elevation of point observation at its center as explained in next paragraph further details about the method for spatial analysis of observed floodings is reported in the additional material 3 1 5 e weighting the flooded areas by fill depth among the factors that affect flood susceptibility the height of water filling the volume of flood prone areas i e flood depth should be considered indeed the threat to personal safety and to gross structural damage caused by floods largely depends upon the flood depth the flood depth in turn is affected by the magnitude of the flood and by the topographic characteristics of the depression the greater the depth of the depression the greater the danger to people vehicles and goods fema 2014 in traditional hydraulic models the flood depth is calculated by subtracting the ground elevation from the water surface elevation for each flood scenario fig 8 a flood scenarios correspond to different return periods and allow the definition of both the maximum expected flood depth and the areal extent of the flood which is a function of the flood magnitude and topography fema 2014 in this work instead of accounting for the modeled flood depth of each return period a potential fill depth is defined corresponding to the elevation of each cell of a flood prone area with respect to its lowest point therefore the potential fill depth is calculated for each cell as the difference between the ground elevation dtm and the lowest elevation of each flood prone area lowel grid fig 4a the higher is the fill depth flad grid fig 4a fig 7c the lower is the degree of susceptibility indeed in a flood prone area heavy rainfall with longer return periods produce higher fill depths i e water levels otherwise rainfall characterized by shorter return periods will produce lower fill depths for example in a flood prone area the probability of reaching a fill depth of 2 m is higher than the probability of reaching a fill depth of 4 m note that in the detected depressions the fill depth values ranges between 0 and 0 12 m therefore variability of the depressions fill depth is considered negligible and is not included in the flad grid the lowest elevation values constituting the lowel grid were calculated and assigned to each flooded area as follows for polygonal observed floodings lowel is assumed to coincide with the lowest elevation of the polygon area a raster statistic of cells in polygons was performed to extract the lowest elevation value of each polygon regarding the buffered areas it is assumed that lowel coincides with the ground elevation value in the location of the point observed floodings cells within the 200 radius buffer were then classified on the base of their fill depth calculated as the difference between the dtm and lowel cells with elevation equal or lower than lowel fill depth 0 are likely to be flooded as well and were considered as highly susceptible indeed excess water is expected to flow preferentially toward them and accumulate in areas having lowest elevation with respect to the elevation of the observed flooding on the contrary cells with elevation higher than lowel fill depth 0 will be flooded or not depending on the storm intensity and consequent rain volume and were considered as less susceptible due to the fact that rainfall with longer return period can provoke flood the potential fill depth was categorized into 5 classes class 5 comprises all cells with fill depth 0 fill depths from 1 to 3 m were classified by using a constant increment of 1 m while class 1 comprises fill depth above 3 m class 1 fill depth between 5 and 3 m class 2 fill depth between 3 and 2 m class 3 fill depth between 2 and 1 m class 4 fill depth between 1 and 0 m class 5 fill depth 0 fill depths higher than 5 m were considered unlikely to occur and were excluded higher values of flad grid correspond to higher susceptibility 3 1 6 f deriving areas of potential sewerage flood hazard actually the drainage capacity of sewer system in the study area is unknown thus for the purpose of our work the underground drainage is neglected according with the assumption that sewerage overwhelming occurs very quickly during intense storms however many floods observed during the considered storms are located few meters close to drainage pipes fig 9 in particular floods occur in correspondence of sewerage manholes which interrupt their function of draining rainwater toward sewers when rainfall exceeds the drainage capacity sewer pipes traces were digitized and overlaid to the manholes vector layer this operation highlighted poor matching between manholes and sewer centerline due to uncertainty in cartographic lines positioning giving errors in pipes digitizing in order to include sewer pipes hazard in the susceptibility map centerlines were buffered by a distance which is the average distance between couples of manholes bounding a sewer pipe on each side 30 m the vector layer representing sewerage with buffer distance was the converted into a grid with cell value 4 in the sewerage hazard areas and 0 in the other cells 3 1 7 g combining fill depth grid weighted depressions grid and sewer pipe flood hazard areas to obtain the map of the susceptibility to pluvial flood reclassflad weighted depressions and sewer pipes grid were summed to obtain a 10 class map of susceptibility to pluvial flood with higher classes corresponding to higher susceptibility the number of classes were then reduced to five to facilitate further risk calculation the map of susceptibility to pluvial flood fig 10 comprises 5 classes 1 very low 2 low 3 medium 4 high 5 very high areas with low fill depth or high density of observed floods and small fill volume belong to the very high susceptibility class the areas of river flooding risk defined in the tiber river basin masterplan autorità di bacino del fiume tevere 2013 were added to the pluvial flood susceptibility map and highlight areas with combined fluvial and pluvial risk type 3 2 impact assessment the impact assessment aims at evaluating and rank the flood damage potentially affecting the urban elements according to the european guidance for reporting under the floods directive 2007 60 ec european commission 2013 the impact of pluvial flood should be evaluated as damage consequence to human health the environment cultural heritages and economic activities commonly the impact assessment considers a sum of social environmental and economic aspects social aspects such as social segregation level or degree of scholar instruction have a direct influence on communities resilience and need to be investigated in deep by specific competencies in order to correctly assess the vulnerability of communities in economic terms assets are considered more vulnerable depending on the expected degree of damage which is linked to technical issues such as the conservation status or level of floors in a building as well as the unit cost for railway or roads damage renard 2016 the impact is then evaluated by combining the value of element at risk and its vulnerability usually these type of approaches consider aspects such as flood characteristics e g flood depth water velocity economic and cultural value of elements and vulnerability of elements at risk stage damage curves are often built in order to relate flood intensity to damage cost however the vulnerability to flood of assets is hard to estimate where no information exists on the damage behavior of each object such a detailed assessment would require a huge effort when poor information are available simplified approaches are used and elements are grouped into classes based on the qualitative evaluation of potential expected impact the vulnerability to flood is often assumed to be constant and equal to 1 so that the element categories can immediately be referred to a relative impact this is the case of the flood risk management plan of tiber river basin authority autorità di bacino del fiume tevere 2013 compiled following the guidelines of italian ministry of environment which is based on turn on the of the european flood directive 2007 60 european commission 2007 other approaches to overcome limitations in data availability and knowledge on damage mechanisms is to group exposed assets with similar intrinsic characteristics into categories and assign a relative value for potential expected damage to each category merz et al 2010 a certain degree of expert judgment is required in order to adapt the calculation in the specific context this is not a limitation itself and is needed when a specific measure of the required data is not available emanuelsson et al 2014 in this case the associated limitations and uncertainties must be evaluated and specified in this work the potential impact is calculated by assigning a class of potential damage to each exposed element and a weighted class of damage cost urban elements considered in the impact assessment are building subdivided by their utilization e g residential education health care religious worship public office public order and safety industrial plant wastewater treatment plant service or transport networks e g main and secondary roads railway and subway lines land use e g crops urban gardens presence of critical points e g underpasses the factors which can potentially influence the damage are examined e g value of exposed goods or number of people potentially involved as well as the risk induced as a consequence of damage table 3 these aspects are defined through potential damage classes the considered damage classes comprise 1 buildings e g residential public offices schools hospitals strategic infrastructures places for recreational religious activities 2 commercial activities e g including sport facilities and crops 3 transport and service networks and other critical urban elements e g roads subways underpasses service delivery networks 4 elements with high pollution potential e g industrial plants wastewater treatment plants landfills 5 environmental cultural and archaeological heritages e g urban parks and gardens natural protected areas 6 areas with no assets or secondary importance assets exposed the criteria for qualitative estimation of elements intrinsic characteristics include the potential loss of human lives environmental and pollution damage the loss of cultural heritages and the degree of dysfunctions provoked by assets damage the potential damage of residential buildings and the economic losses for commercial activities were assumed as proportional to the number of inhabitants and people employed respectively criteria used to categorize damage classes are listed below table 2 3 2 1 a potential loss of human lives or problems for people safety residential buildings where a constant presence of people is assumed were categorized depending on the number of inhabitants declared in census data istat 2015 into three classes by means of the quantile distribution up to 16 000 between 16 000 and 20 000 above 20 000 corresponding to damage class scores 3 4 5 all the elements considered as indispensable and strategic for people safety such as hospitals or military buildings were considered as having highest score score 5 this is also because of their strategic importance in the emergency phases also public buildings with potential high people concentration such as schools and public offices were considered as having the highest score score 5 a low potential damage class score 2 was assigned to recreational and religious activities where the presence of people is concentrated in limited hours days 3 2 2 b potential economic losses for economic and commercial activities three classes of commercial activities were considered proportional to the number of people employed istat 2015 up to 7 between 7 and 40 above 40 corresponding to scores 3 4 5 respectively scores were assigned following the quantile distribution 3 2 3 c environmental damage assumed to be constant for the elements potentially dangerous for the environment in the specific context the elements potentially responsible of environmental pollution such as wastewater treatment plants industrial plants landfills were grouped in the same damage class score 4 the maximum damage class score score 5 was assigned to relevant industries assuming a high potential damage for environmental hazard 3 2 4 d value of the environmental archaeological and cultural heritages assumed to be constant for the elements in the specific context even if allow people concentration for limited hours a high potential damage class score 4 was assigned to cultural and archaeological heritages due to their cultural and social importance 3 2 5 e degree of dysfunctions for interruption of vehicle traffic transportation networks and the general city s mobility interruption of water and energy delivery services streets railways and subways are elements of primary importance indeed the flood of main streets can impede the movement of vehicles and people during emergency intervention as well as block the fluxes toward schools hospitals and generally workplaces a high potential damage class score 5 was assigned to subway and railway lines and roads in order to assign a relative cost to each damage class available data about economic losses due to past floods where examined cna roma 2015 data about damage costs are extremely scarce and are available for just two intense storm event 20 october 2011 and 16 19 june 2014 this is a diffuse limitation when performing damage cost estimation as discussed by many authors e g emanuelsson et al 2014 jongman et al 2012 potential damage costs must then properly account for the uncertainty related to data scarcity the damage costs estimated for the two events are very different and even if highly uncertain are assumed to be representative of a near full range of relevant flood events data provide only total economic loss for each category of damage table 3 a and does not report the unitary cost of assets such as the cost for squared meter of residential cleaning and repairing for the 20 october 2011 event the higher costs 69 are due to refunding for individual losses in private building both commercial and residential relative minor costs 27 8 are associated to public buildings and infrastructures damages other costs 2 8 are associated to urgent interventions and emergency management for the 16 19 june 2014 storm event the majority of reported costs 91 are due to the damage in public buildings infrastructures and delivery networks while minor costs are related to urgent interventions and private losses 4 9 and 2 8 respectively in order to evaluate and map the relative cost of flood damage the observed costs were translated into three cost classes with higher score classes corresponding to higher damage cost table 3b for the two scenarios the costs of emergency management cannot be related to specific element categories and are assumed to be part of the uncertainty in potential impact calculation for both scenarios a damage cost class was associated to each exposed element then damage cost classes were weighted proportionally to the percent contribution of each element to the total damage cost class following previous experiences in the field of weighted cost class estimation emanuelsson et al 2014 for example considering scenario 1 the class cost 3 private losses includes both flood damage to crops and industrial plants since are all private properties however it can be reasonably assumed that for equal pluvial flood magnitude the contribution to cost class for crops is lower than for main industrial plants the weights associated to cost class of crops and main industrial plants were then set to 0 1 and 1 5 respectively this operation was performed by means of expert judgement and for this reason is highly subjective although it is reasonably consistent over assets and events many uncertainty sources can affect the choice of damage cost class depending on the variability of the specific feature of each element type for example the damage recovery cost for an archaeological site can depend upon its typology e g ruin ancient building ancient street as well as upon its conservation status an uncertainty score ranging from 0 1 to 0 5 was assigned to each element accounting for the degree of damage cost class variability due to element features table 4 only the principal sources of uncertainty were considered where multiple uncertainty sources exist for one element damage cost the sum of uncertainty score constitutes the overall uncertainty the estimated potential impact map was then calculated for both scenarios as 1 potential damage class damage cost class cost class weight ʃcost class uncertainty categories of exposed assets and assigned potential damage class damage cost class and potential impact class score for both flood scenarios are listed in table 5 fig 11 shows potential damage and cost classes cost weight and uncertainty for the categories of exposed assets cost classes are referred to scenario 1 the potential impact maps for the considered scenarios as calculated by the expression 1 are shown in figs 12 and 13 the layer of potential impact was clipped by using the flood prone areas vector layer 3 3 pluvial flood risk assessment 3 3 1 pluvial flood risk map the pluvial flood risk is expressed through an index which is calculated as the sum of susceptibility and potential impact combined in a matrix as the sum of rasters representing the susceptibility and the potential impact these two elements are combined in risk levels according to a defined matrix see leitao et al 2012 niemann and illgen 2011 resulting from the sum of the 5 classes of susceptibility left column of matrix plus the 5 classes of potential impact top row of matrix ten classes of risk result from the algebraic sum the highest risk class score 10 occurs in areas where a potentially highly impacted element e g an underpass is coincident with a depression having high density of observed flooding or with a low fill depth area fig 14 shows the risk map obtained by the sum of susceptibility and potential impact for scenario 1 4 results and discussion results were evaluated by comparing the maps of susceptibility and potential impact with a subdataset of 513 punctual observations of pluvial floodings occurred between 2009 and 2014 the comparison between detected risk levels and observed floods constitutes a validation strategy kandilioti and makropoulos 2012 diakakis et al 2011 hall et al 2005 rudari et al 2014 that although being based on a qualitative comparison often with a limited number of observed floods is useful for detecting whether the criteria used were able to capture the flood occurrence it is assumed that the quality of susceptibility prediction is inversely proportional to the distance between susceptible areas and observed floods in the validation dataset the near tool was used to calculate the distance between points in the validation dataset and the susceptible areas results show that 445 validation points out of 513 86 7 are within a susceptible area the number of validation points not coincident with a susceptible area failed predictions is 68 13 the maximum distance from failed predictions and susceptible areas is 284 7 m while the mean distance is 13 m the standardized near distance values were interpolated by means of ordinary kriging the standard near map and the associated standard error fig 15a and 15b respectively allow visualizing how distance values are distributed in the map failed predictions coincide largely with crops 55 fig 16 the uncertainty related to crop damage cost is low 0 2 this meaning that the damage cost would not vary much depending on crop type a minor number of missed points 18 is coincident with residential buildings equally distributed into high medium and low number of inhabitants the associated high cost uncertainty 0 8 is due to unknown features e g number of basements building vulnerability retained to have a strong influence on damage cost other 8 missing points are coincident with sport plants roads and minor industrial plants 4 3 4 3 2 9 respectively single failed predictions are located in urban parks public buildings stations and networks commercial activities and other element types among those public buildings exhibit the highest cost uncertainty 0 9 and is then responsible for the 5 8 of the overall failed costs despite corresponding to one single point finally failed predictions were evaluated in the light of the reported damage costs for both flood scenarios table 6 for scenario 1 the majority of failed predictions 62 are within the lower class of potential impact score 1 a minor number of failed predictions 10 4 and 21 respectively are located within classes 2 3 and 4 one single failed prediction is located in the higher impact score class score 5 results for scenario 2 confirm that the majority of missed points 72 falls within the lower potential impact class score 1 while a low percent of elements 1 falls in the highest potential impact class score 5 a minor number of missed points 3 14 and 9 respectively are located within potential impact classes 2 3 and 4 one of the missed points is located in the highest impact score class score 5 in monetary terms the cost of missed prediction can be evaluated by comparing the weighted damage cost class of failed prediction with the associated observed damage cost the weighted cost class of elements coincident with failing predictions was expressed in percent of the total failed prediction cost by multiplying it times the percent number of failing points in the correspondent class the percent weighted cost of each single group was then translated into quantitative cost euros by reporting it to the overall cost in other words the cost of the 13 failed prediction corresponding to 1 300 000 euros for scenario 1 and 31 831 280 euros for scenario 2 was partitioned into damage costs classes proportionally to the calculated weighted cost of each element type estimated cost of failed prediction for scenarios 1 and 2 are shown in fig 16 by considering scenario 1 the 76 of costs are due to residential buildings 780 000 euros in total 60 and crops 214 783 euros 16 minor costs are associated to roads and industrial plants 113 000 euros 17 public buildings 75 000 euros 5 8 and stations 37 000 euros 2 9 other costs 47 000 euros 3 5 are mainly related to sport facilities urban parks and commercial activities results for scenario 2 confirms higher costs 51 in total associated to residential buildings 12 732 512 euros 40 and crops 3 506 054 euros 11 main differences with respect to scenario 1 are due to roads and public buildings costs 4 151 906 and 2 767 937 euros accounting for 13 and 8 7 respectively thus for each of the considered scenarios residential buildings and crops are responsible for the highest costs due to pluvial flood damage this result suggests that the features of residential buildings accounting for highest uncertainty need to be more detailed in order to properly allocate resources both for emergency management and for damage recovery secondarily the economic value of potential agricultural losses must be correctly evaluated and cannot be neglected indeed even if damage costs of crops are lower with respect to other element types crops account for the majority of failed predictions finally the type and conservation status of public buildings need to be specified indeed the damage costs associated to public buildings accounts for a high percent in both scenarios 1 and 2 5 8 and 8 7 respectively even if only one single building is involved roads constitutes the element with highest variability in terms of potential impact which is mainly responsible for the differences in potential impact classes between the two scenarios the estimation of roads damage cost needs to be improved introducing a ranking based on the relative importance highway main road local road and split it into different damage classes by doing so main roads accounting for higher damage cost and main traffic dysfunctions when flooded can be set to class cost 3 while minor roads can be set to lower class score 5 limitations and future perspectives many limitations can be ascribed to this work the majority of these are listed below the procedure does not follow the consolidated approach of risk calculation as a combination of hazard vulnerability and exposure in fact the described approach gives a semi quantitative classification and ranking of areas prone to flood due to morphological susceptibility and according to the relative importance and expected impact risk classes are not associated to the probability of flood occurrence for certain return period and this limits the possibility to compare the risk levels with other risk maps obtained with traditional approaches the analysis of rainfall trends is strictly needed in order to add quantitative parameters to the risk computation and must be included in further advances of this work the use of potential impact instead of vulnerability and exposure adds subjectivity to the way the risk is calculated since it is largely based on qualitative parameters and on the costs of damage at the present stage the map does not provide any element useful for projecting and dimensioning mitigation interventions but only furnishes a screening of areas higher at risk in a municipal context in the areas detected as highly at risk by the preliminary analysis quantitative information including the excess water velocity as well as the flood depth for different flooding scenarios must be calculated by means of local scale hydraulic models in order to perform an effective and detailed flood risk mapping at local scale and to obtain parameters useful for interventions planning the computation of risk levels through a sum matrix allow a synthetic simplified evaluation of natural and social aspects concurring to flood the poor resolution the subjective categorization of variables and non numeric calculation process are non negligible unresolved limitations the small time window covered by the flood observation dataset 2001 2015 is too short to account for interannual and multidecadal variability as well as for climate change effects this impedes to perform a correct storm trend analysis indeed the risk is dependent on the application and length of time over which it is assessed as the non stationarities related to anthropogenic climate and natural climate oscillations may influence the estimation of rainfall intensity frequency duration this on turn implies difficulties in linking the flood extension and damage to a rainfall amount thus basing flood risk estimates on an unsuitable time period has the potential to significantly underestimate or overestimate the true risks as demonstrated by many studies kiem et al 2003 leonard et al 2008 to overcome this problem the usage of data sets of similar length ensuring that they span a sufficient number of years which likely capture high or low annual maxima is strongly recommended verdon kidd and kiem 2015 furthermore an uncertainty analysis accounting for both the data availability and variability within the observation period should be performed in order to provide relevant information to practitioners about the reliability of rainfall intensity frequency duration estimates koutsoyiannis 2006 therefore at the present stage the risk assessment can be referred only to the observation period and this methodology cannot be considered as suitable for assessing pluvial flood risk over longer period the collection of rainfall data for a trend analysis capturing such non stationarities will be the object of further advances the validity of the susceptibility assessments is demonstrated only for pluvial flood while it is not demonstrated for floods depending on groundwater table rising nor river floods this poses strong limitations to the transferability of the work as explained in section 6 also the coincidence of many types of floods e g pluvial flood coincident with river flood is not taken into account this limiting the comprehension of the impact related to compound events results of potential impact assessment suggest that actually the uncertainties on elements features could lead to wrong economic allocation of effective costs indeed by testing the procedure with two flood scenarios with very different damage costs results of impact assessment show very similar percent of damage cost however the poor resolution of potential impact due to the same rating assigned to quantitatively very different elements exposed limits the capability to quantify and allocate resources for risk reduction measures on the base of risk matrix categories these issue can be partially solved by incrementing the number of damage cost classes and the refinement of elements features indeed linking element types with very different features to just 3 cost categories can lead to poor cost estimation accuracy which is not completely overcome by using weights and uncertainty scores the results of the risk assessments are strictly dependent on the availability of a proper observation period the longer the period the higher the degree of confidence of the model improvements in the knowledge of how the contexts experienced flood which location and fill depth as well as related damage are especially needed in areas that are rarely affected by flooding actually the potential impact assessment does not consider the increase of precautionary measures the increased risk awareness and the effectiveness of early warning systems neglecting these elements can concur to the risk degree overestimation when transposing the method to other contexts where similar categories of assets can have very different response in terms of flood risk resilience the map validation performed by simply evaluating the distance between risk areas and observed floods only gives a qualitative measure of the reliability of risk map it needs to be improved after a proper data collection by i evaluating how reliably the flood hazard models estimate the probability and the characteristics of a flood event e g flow velocity flood extent flood duration and ii comparing the potential impact map with observed damage and vulnerability of affected items 6 remarks on the transferability of the method some remark can be made about the transferability of the procedure to other regions and problems the validity of the susceptibility assessments is demonstrated only for pluvial flood in the specific case study it is suitable for the central area where stormwater ponding is assumed to be scarcely correlated with river and groundwater dynamics conversely caution should be used when transposing it to coastal areas and floodplains in floodplains and coastal delta the differences in elevation are small due to the flat morphology depressions can be large with very small depth and the water table is shallow in those cases the storage capacity of depressions and thus the susceptibility to flood must account for specific hydraulic and hydrogeological features such as water table oscillation and river stage levels in such contexts the usage of the methodology is limited to flood due to rainfall ponding assuming that river flood or groundwater flood are absent otherwise in upstream mountainous regions where the floodwater velocity can be high due to the steep slope and high hydraulic potential parameters accounting for the water velocity catchment area and the interconnection between draining branches at sub basin scale cannot be neglected consequently the method for susceptibility assessment can be replicated in urban areas with morphological setting similar to the case study however additional and site specific parameters accounting for surface and groundwater dynamics are still not yet sufficiently represented and must be included to improve the predictive capability when the methodology is transferred to different regions and different flood events with respect to the potential impact it should be noted that mostly of traditional damage models are derived for a certain study area with specific building characteristics and specific relationships between losses and flood impact factors cammerer et al 2013 reliable model application is assumed to be restricted to its region of origin oliveri and santoro 2000 thus these models cannot be easily transferred to other regions without any model adaptation and validation merz et al 2010 meyer et al 2013 as building types asset values and their vulnerability on the contrary the simplified approach described in this work for potential impact calculation based on the allocation of a damage cost percent to each class of asset can be easily transposed to different regions even affected by other types of hazards in particular the general and qualitative criteria used to group the exposed assets into potential damage classes such as loss of human lives environmental pollution loss of cultural heritages traffic dysfunctions can be transferred adapting the available information such as population census data destination of use of buildings to the local context moreover the method for ranking and assigning the damage cost classes derived by the evaluation of alternative scenarios of historical flood damage cost can be transposed to other contexts by adapting the score and rank classes to the declared and reported damage cost in the specific context and or for the specific hazard thus even if it is not sufficient to derive detailed costs in monetary terms the method for potential impact assessment allows estimating the relative degree of impact useful for a preliminary grouping of exposed assets in contexts where scarce information about elements are available future works should therefore include the investigation about the potential variability of flood losses due to different process characteristics and or different types of flood a systematic collection of quantitative flood loss data is needed to apply and validate flood loss models as well as testing the transferability to other study areas finally the transferability of the method is strictly dependent on the availability of base data concerning the territory e g elevation location and type of buildings in the european continent there is a rapidly increasing trend towards the availability and accessibility of spatial data and improvements of their properties in terms of resolution european commission 2016 however this is not the same throughout the world this limiting the capability to transfer the method to other country were data lack may impede to perform this kind of municipal scale risk mapping 7 conclusions despite many assumptions and limitations the methodology furnishes a comprehensive preliminary analysis of pluvial flood risk in urban areas differently from approaches based on hydraulic modelling and related stage damage curves requiring detailed data and long computational times the proposed methodology uses data found in local administration offices or open geodata services stored in a dedicated database and processed in a simplified procedure by means of tools in gis environment gis tools allowed to easily derive informative levels accounting for the spatial distribution of each variable concurring to pluvial flood risk the map allows the visualization of susceptibility to pluvial flood at municipal scale showing a good agreement with the validation dataset and of elements potentially impacted ranked on the basis of the expected cost in terms of human lives economic terms and cultural social aspects the main limitations of this work are due to the small time window covered by the flood observation period this impedes to account for climate non stationarities due both to natural variability and climate change when evaluating the components of rainfall intensity frequency duration moreover the poor resolution of damage cost evaluation largely performed by subjective judgment limits an effective allocation of costs for emergency management and mitigation measures in order to improve the map elements features should be better detailed this is particularly important for residential buildings crops and roads which features constitute the higher uncertainty source also the application of the methodology in areas subjected to groundwater floodings or combined river groundwater and pluvial floods need to be considered as well as testing the methodology in different urban areas by improving the impact assessment the comparative large scale pluvial flood risk assessment provided by this map can represent a tool for local administrators for prioritization of investment for reducing and preventing damages the map should be intended as a preliminary pluvial flood analysis useful for mapping the risk at municipal scale even if it does not allow to derive precise quantitative expected damage costs it permits comparative analysis for detecting areas higher at risk which should be the subject of deeper investigations functional to quantitative assessment of both susceptibility and potential impact in such areas the calculation of excess water velocity as well as the flood depth for different flooding scenarios must be performed by means of hydraulic modelling it is indispensable for correctly assessing quantitative susceptibility and relate it to assets vulnerability software all the elaborations described in the text were performed by means of esri arcgis 10 2 acknowledgments this study represents an independent research following the preliminary analysis lead in the framework of the urban georisk project supporto tecnico e indirizzi operativi ai fini della valutazione di pericolosità geologiche nelle aree urbane per la pianificazione di interventi di protezione civile 2014 financially supported by dpc the italian civil protection department cnr igag project manager g p cavinato dpc referents p pagliara a corazza f leone authors would like to thank dpc referents and gianluca ferri ufficio extradipartimentale di protezione civile di roma capitale for supporting this phase of preliminary analyses annex i math formulae potential damage class damage cost class cost class weight ʃcost class uncertainty 1 2 nni do de where do observed mean distance between each point and its nearest neighbor 3 do ʃdi n de expected mean distance for the points given in a random pattern 4 de 0 5 n a n number of studied pointsa size of the studied area 5 z do de se where 6 se 0 26136 n2 a annex ii processing observed floodings punctual data represent the location of an observed flood which subtends a flooded area which extent is unknown the real spatial extension of floods is a function of the rainfall intensity mm hour and the terrain morphology in order to evaluate and approximate the potential extension of floods a buffer area around points was considered which elevation is explored in order to assess the relative susceptibility based on the relative elevation of each cell with respect to the point elevation buffer distance was chosen on the base of the spatial relationships between points the analysis of spatial relationships was aimed at identifying clusters of points which can be considered as part of the same flooded area it was assumed that the average distance between clustered points can approximate the minimum extent of a flood area a nearest neighbor analysis based on average cluster considering euclidean distance as similarity measurement was accomplished the analysis uses the distance between each point and its closest neighboring point in a dataset to determine if the point pattern is random regular or clustered the analysis was performed by means of average nearest neighbor and near tools the average nearest neighbor hertz 1909 clark and evans 1954 hereafter ann calculates the distances between each feature centroid and its nearest neighbor s centroid location if the distance is less than the expected average distance with expected average distance being based on a hypothetical random distribution with the same number of features covering the same total area the feature being analyzed is considered as part of a clustered distribution otherwise it is dispersed if the average distance is greater than a hypothetical random distribution the feature is considered dispersed the ann analysis allows to assess the clustering degree in the entire sample calculating the hypothetical distance between points average expected distance 669 46 m which is then compared with the average nearest neighbor distance results of the average nearest neighbor analysis are summarized in table 1 table a1 average nearest neighbor summary table a1 number of input points 916 distance method euclidean study area m2 1642159239 observed mean distance or average nearest neighbor m 312 48 expected mean distance m 669 46 nearest neighbor ratio 0 466 z score 30 87 p value 0 0000 the nearest neighbor ratio or index nni clark and evans 1954 is calculated as the observed distance divided by the expected average distance when index is less than 1 the pattern exhibits clustering if the index is greater than 1 the trend is toward dispersion 7 nni do de wheredo observed mean distance between each point and its nearest neighbor 8 do ʃdi n where di equals the distance between feature i and its nearest neighboring featurede expected mean distance for the points given in a random pattern 9 de 0 5 n a n number of studied pointsa size of the studied area being the average nearest neighbor ratio 1 table 1 the distribution is clustered the z score and p value returned by the ann tool are measures of statistical significance which tell you whether or not to reject the null hypothesis for the ann statistic the null hypothesis states that features are randomly distributed the average nearest neighbor z score for the statistic is calculated as 10 z do de se where 11 se 0 26136 n2 a given the z score of 58 1560049237 there is a less than 1 likelihood that this clustered pattern could be the result of random chance the distance between each clustered point nni 1 to its nearest was then averaged statistics of near distance analysis for points exhibiting clustering are listed in table 2 table a2 near distance statistics for points with nni 1 table a2 minimum near distance m 8 51 maximum near distance m 491 95 sum m 101990 mean near distance m 202 29 standard diviation m 133 52 the average near distance h between clustered points is 202 3 m it is assumed that h equals the theoretical minimum extension of a flood in a condition of flat or gentle slope morphology in other words where no terrain reliefs exist it is assumed that two point floods being at distance less or equal to 202 m are part of the same flood starting from this principle it is assumed that a circular 200 m radius buffer around all the points can define an area susceptible to flooding subtended by each observed flood point this allowed approximating the area reasonably considered as susceptible to flooding due to its proximity to an observed flood then in order to consider the second factors potentially responsible for flooding extent terrain morphology a screening was performed on cells in the buffered areas based on the comparison with the elevation of point observation at its center as explained in next paragraph 
26363,a gis based procedure for preliminary mapping of pluvial flood risk at metropolitan scale cristina di salvo a francesco pennica a giancarlo ciotoli a b gian paolo cavinato a a cnr istituto geologia ambientale e geoingegneria igag area della ricerca di roma 1 via salaria km 29 300 c p 10 monterotondo 00015 roma italy cnr istituto geologia ambientale e geoingegneria igag area della ricerca di roma 1 via salaria km 29 300 c p 10 monterotondo roma 00015 italy b istituto nazionale di geofisica e vulcanologia sezione roma 2 roma italy istituto nazionale di geofisica e vulcanologia sezione roma 2 roma italy corresponding author a gis based procedure for mapping pluvial flood risk in urban areas is proposed risk is expressed through an index calculated as the sum of susceptibility and potential impact combined in a matrix the susceptibility is defined as the probability of a flooding to occur because of the ground morphology and the spatial probability density of historical floodings the potential impact was evaluated by considering the consequences of damages on human health environment cultural heritages and economic activities and accounts for the potential cost of damage both the susceptibility and the potential impact are calculated by elaborations of base data in gis environment despite many limitations the methodology furnishes a tool for a preliminary screening of areas potentially subjected to pluvial flood useful for a municipal scale mapping it permits comparative analysis for detecting areas higher at risk helping prioritizing the emergency management and the planning of mitigation actions keywords pluvial flood susceptibility analysis impact assessment risk map rome 1 introduction floods are one of the most common and widely distributed natural hazard to life and property flood hazard assessment is explicitly required by the eu floods directive 2007 60 ec in order to prevent damages and plan sustainable management schemes urban areas are particularly vulnerable to flood due to the modification of the natural environment with often deficient or improper land use planning and the high concentration of population buildings economic activities distribution networks and cultural heritages the urban development generally causes the modification of the natural hydrological cycle healthy waterways org fig 1 the sealing of natural surfaces and the underground channeling of drainage network into pipes abruptly reduce the evapotranspiration from plants the water exchange between surface and underground water and thus the infiltration these factors contribute to strongly increase surface water runoff leading to deterioration of the water quality in the case of intense storms rainwater overwhelms the sewer system which becomes unable to drain excess water anymore the role of climate change remains difficult to assess mainly for the complexity of separating non stationarities related to anthropogenic climate from those associated with natural low frequency climate oscillations ishak et al 2013 thus even if there is high confidence that climate change will be responsible of the increase in the number and frequency of extreme rainfall events bates et al 2008 caution should be used in the evaluation of rainfall changes with respect to flood hazard johnson et al 2016 floods can be described and categorized based on a combination of sources causes and impacts into river or fluvial floods pluvial floods coastal floods groundwater floods often the different types of flooding are strongly linked together indeed all types of floods are triggered by the interaction of various physical processes including hydrological pre conditions e g soil saturation meteorological conditions e g amount intensity and spatial distribution of precipitation runoff generation processes e g infiltration and runoff on hillslopes and river routing e g superposition of flood waves increased infiltration and a rise in the water table may result in less water flowing from rivers to ground with consequent rise of river stage which results in more likely overtopping their banks a rise in the water table during periods of higher than normal rainfall may also mean that land drainage networks such as storm sewers cannot function properly if groundwater is able to flow into them underground jha et al 2012 in many cases river groundwater and pluvial flooding are difficult to distinguish moreover the combination of multiple statistically dependent variables or events not necessarily extreme can have a significant influence on the magnitude of the resultant flood and lead to an extreme impact compound events leonard et al 2014 this adds complexity in modelling the impact of floods in particular pluvial flood is very quick and heterogeneous and occurs at a range of spatial and temporal scales much smaller than are usually considered in fluvial and coastal flood risk analysis the spatial temporal characteristics of rainfall can be a significant factor in influencing the pattern of urban flooding luyckx et al 1998 segond et al 2007 haberlandt et al 2008 and the spatial heterogeneity of the resulting inundation can be large apel 2016 this means that the magnitude and extension of related potential damage is hardly predictable well established and tested procedures exist for urban river flood risk assessment e g dawson et al 2008 morita 2008 floodsite 2009 techniques for the assessment of groundwater flooding risk also exist including fine scale evaluations of hydraulic relationships between water table drainage network and sea level associated to groundwater numerical modelling rotiroti et al 2014 being related to water table rising in areas with flat morphology such in lowlands or coastal areas it is often extensive and the location of floods can be highly predictable conversely techniques for the study and the management of flood risk caused by extreme rainfall are actually poorly considered niemann and illgen 2011 zhou et al 2012 and necessarily need to be explored given the rising of the magnitude and frequency of storms the common approach for risk mapping is to use hydrologic numerical models which simulate the water height over urban surface as a function of many variables however this approach requires a deep knowledge about the urban system e g sewer network pipes infrastructure capacity land use soil type at a proper scale as well as a detailed mapping of urban elements driving the water flowpaths e g buildings location sidewalks height manholes position in order to correctly model local run off and surface flow processes the nonlinear response of sewer systems to inflows needs to be effectively coupled to surface flow pathways and damage calculations blanc et al 2012 to simulate the interactive exchange of storm water in the sewer system and on the surface chang et al 2015 djordjevic et al 1999 lee et al 2015 leandro et al 2009 fraga et al 2015 the probability of occurrence of a storm events is traditionally calculated by intensity duration frequency curves characteristic of homogeneous climatic regions stochastic spatially explicit rainfall simulators are also used in order to fully describe possible rainfall intensities and their spatial coverage apel 2016 burton et al 2008 hundecha et al 2009 willems 2001 wheater et al 2005 the simplification adopted of assuming that the likelihood of the urban flooding is equal to that of a rainfall event with certain return period is considered acceptable for intense rainfall events and often this is the adopted procedure due to difficulties in estimating the real return period and subsequently the likelihood of the flooding event leitao et al 2013 however the inherent complexity of defining and quantifying floodwaters often limits the applicability of such detailed models to local scale case studies conversely the management of emergency requires knowledge of flood risk at urban scale in order to assess which areas are higher at risk requiring prior interventions the scarcity of urban scale mapping approaches is pushing to develop suitable definition of risk and a range of methodologies for appropriate assessment blanc et al 2012 indeed due to the high intensity variability and scarce predictability of storms calculating the expected floodwater depths for rainfall thresholds can be highly uncertain and can result in scarce utility for emergency management rather than assessing susceptibility through uncertain flood depth scenarios susceptibility can be defined as the likelihood of a storm occurring in an area on the basis of local terrain conditions santangelo et al 2011 the degree of morphological susceptibility and the spatial probability density of the historical observed floods can be used to perform a first level urban scale pluvial flood mapping and can be useful to conveniently address emergency interventions optimizing resources allocation the morphological susceptibility is the attitude of a portion of terrain to experience flood due to its elevation with respect to surroundings and physical features such as its steepness the spatial probability distribution is the function defining the distribution of observed floodings in the space as further explained in section 3 during recent years the improvement of geographic information systems gis have allowed us to consider alternative risk mapping approaches commonly gis systems are used to collect and elaborate spatial data required as input for hydrological and stromwater models moreover a wide number of gis tools and plugins are rapidly evolving tailored for hydrological analysis functional to the evaluation of flood depth and extension gis tools can also furnish a base for alternative simplified approaches aimed at the evaluation of factors concurring to pluvial flood risk and their combination indeed in gis environment each kind of data can be represented through its spatially distributed values allowing many variables to be included in calculations and making it easy to perform detailed risks evaluation on a large scale djordjevic et al 1999 2005 schmitt et al 2005 obermayer et al 2010 these include building high resolution digital terrain models integrating river bathymetry and the surrounding topography merwade et al 2008 or improving the accuracy of flood risk assessment when developing decision support systems for integrated flood management qi and altinakar 2011 gis based methodologies for pluvial flood risk assessment allow an optimal management and use of available municipal scale data a high degree of automation in the application and a good transferability of methods scheid et al 2013 for example gis allow to represent vector data such as morphological features i e depressions observed floods or assets exposed to risk buildings roads railways both the location shape and related attributes can be easily managed assigning weights or ranking the elements vectors can be translated into raster layer and be used as terms for map algebra calculations allowing appropriate spatial mapping of selected themes the capability to easily perform large scale calculations between raster levels allows to combine characteristics of the natural and anthropic environment concurring to the definition of risk the wide range of tools for spatial relationships evaluation e g neighboring and cluster analysis overlay spatial density permits to explore and quantify spatial correlation between variables concurring to risk in this framework the objectives of this study are 1 to test a chain of gis tools for mapping the susceptibility to pluvial flood at municipal scale 2 to derive impact scenarios compatible with the available data about past flood impact quantifying the related uncertainty 3 to integrate the susceptibility and potential impact in order to derive a preliminary municipal scale map of pluvial flood risk useful for detecting areas higher at risk and prioritize interventions both in the emergency and planning phases following this approach the comprehensive pluvial flood risk assessment is a synthetic evaluation of natural and social aspects concurring to flood zou et al 2013 many of these factors have not total unified quantitative standards which make the flood risk assessment system complex and difficult to operate du et al 2006 jiang et al 2008 li et al 2008 2010 a common technique is to assign proper weights to each variable concurring to flood risk combining those together in a suitable risk matrix niemann and illgen 2011 leitao et al 2013 by which the definition of risk can be done as a purely qualitative additive of multiplicative process scheid et al 2013 the risk matrix usually combines the likelihood of a flood to occur and the estimated consequences or potential impact both likelihood and estimated consequences are expressed by classes the sum of which allow to define risk levels few examples exist of flood risk grade classification matrix in which risk is defined as the sum of the two components zou et al 2013 defined the risk matrix as the sum of hazard accounting for many factors including flood velocity depth land use and average annual precipitation and vulnerability accounting for various elements such as population and industrial density road surface extension in the swiss flood hazard map van alphen et al 2009 hazard classes are defined through a matrix composed by flood intensity as a combination of flow depth and discharge per unit width and probability frequency of flood matrix classes are referred to legal practices for urban planning and building regulations the computation of risk levels through a simple sum matrix leads to many limitations resulting in uncertainties difficult to quantify cox 2009 the poor resolution due to the same rating assigned to quantitatively very different elements exposed the subjective categorization this means that different users may obtain different rating for risks which should be quantitatively similar despite those limitations matrixes are helpful in reducing the subjectivity in risk definition and represent a valid and quick methodology for a preliminary assessment of risk helping the detection of areas which should primarily be the subject of further detailed analysis or structural interventions in our study the pluvial flood risk is defined through an index which is calculated as the sum of the susceptibility of a flood to occur and the potential impact combined in a matrix the susceptibility accounts both for the spatial probability density of documented floodings and the terrain morphology a preliminary screening of areas prone to flood due to morphological features depressions was performed and results were then coupled with historical observed floodings depressions and observed flooded areas constitute the flood prone areas and their degree of susceptibility was assessed on the basis of the magnitude of parameter composing the feature driving the susceptibility through elaborations performed by gis tools the potential impact was assessed accounting for the potential damage related to the intrinsic features of considered elements at risk and the related costs associated to flood damage the impact considers the overall social economic and environmental value allowing a simplified detection of the value of goods buildings infrastructures and activities located in the flood prone areas which primarily would make the community suffer in the case of loss or damage all informative levels used in the analysis are stored in a database and analyzed through existent gis based techniques and then adapting the methodology to the study context 2 case study the case study is the urban area of rome the central sector is characterized by small hills with gentle slope values between 0 5 and 30 carved from a tuff plateau overlaying pleistocene sedimentary rocks by tiber tributaries and the groundwater table largely more than 3 m deep la vigna et al 2016 toward south west the city spreads to the thyrrenian sea coast and is characterized by a flat morphology and shallow groundwater river flooding affects the city of rome along the valleys of tiber and aniene rivers and tributary streams case 1 in fig 2 this type of risk as defined in the tiber river basin masterplan autorità di bacino del tevere 2006 was the subject of many studies and numerical modelling calenda et al 1997 2005 2009 natale and savi 2007 aimed at the construction of structural mitigation measures such as the upstream dams corbara nazzano dams as well as river walls muraglioni recently hydraulic models by natale and savi 2007 and calvo and savi 2009 demonstrated that river flooding with180 and 1000 years return period respectively can affect the city on the other hand the coastal areas i e ostia casal palocco and acilia case 2 in fig 2 close to or below the sea level and where groundwater level is controlled by draining pumps are yearly affected by groundwater flooding this type of flooding is caused by the water table rise due to intense rainfall and or to high stage of the local stream network not compensated by draining pumps groundwater flooding is strongly sensitive to the dynamic interaction between sea level oscillation storm surge water table oscillation groundwater surface channel water interaction as well as the dynamic interaction with artificial withdrawals thus groundwater flood risk cannot be easily assessed through a simple gis based analysis layers overlay but would instead require an adequate monitoring of river and channels level stage groundwater oscillation sub hourly rainfall record for supporting detailed models adopting a simplified assumption in this study the floodings observed in the coastal areas are assumed to be due to intense rainfall overwhelming the drainage network infiltration toward the ground is also neglected since the water table is very close to the ground implication of high river stage and water table rising are not considered this limiting the applicability of the method in coastal areas conversely the described methodology is tailored for pluvial flooding broadly defined as a type of flooding resulting from heavy rainfall generated overland flow and ponding before the runoff enters a natural or man made watercourse drainage system or sewer or when it cannot enter it because the network is full to capacity falconer et al 2009 pluvial flooding is mainly observed in the central densely urbanized areas case 3 fig 2 2 1 rainfall regime rome exhibits a precipitation pattern typical of the mediterranean regime with the maximum in fall winter and the minimum in summer colacino and purini 1986 recent climate studies show that in central italy since the end of the 19th century there is a tendency toward an increase in precipitation intensity 6 and 9 for winter and fall trends respectively despite the significant negative trend of wet days number 15 and total precipitation 10 per century in total yearly precipitation brunetti et al 2004 in particular a marked decrease in the frequency of low intensity precipitation and an increase of high intensity precipitation events have been observed specifically the analysis of long time series 1862 2004 of seasonal rainfall in the city of rome shows a decreasing trend during winter and fall season villarini et al 2010 in general under projected global warming the resulting changes to ocean atmospheric circulation patterns are likely to lead to shifts in the location magnitude and frequency of extreme precipitation events and the flooding associated with them kiem and verdon kidd 2013 moreover multiple local factors such as catchment anthropic modification or the antecedent soil moisture condition introduce non linearities in the rainfall runoff process which are poorly resolved in global circulation models this increases the uncertainty in separating and quantifying the fraction of risk due to climate change from the one due to local setting many studies have documented the violations of the stationarity assumption for rainfall and temperature records over central italy e g brunetti et al 2006 2000 and over rome in particular e g maheras et al 1992 colacino and purini 1986 covariate analyses highlight the role of seasonal and interannual variability of large scale climate forcing as reflected in three teleconnection indexes north atlantic oscillation atlantic multidecadal oscillation and mediterranean index for modeling rainfall and temperature over rome villarini et al 2010 a correlation has been found between the three indexes trends and the behavior of rainfall and temperature records in particular nao tends to be a significant predictor during the winter season while the mediterrean index is a significant predictor for the majority of the cases providing insights to the slowly varying oscillations in maximum temperature observations for rome however despite the important advances in climate studies the evaluation of local variations induced by climate change remains highly uncertain this is also because the regional assessment often focus on percentage changes which does not easily translate into impacts of practical significance johnson et al 2016 in this study the frequency of intense storms is not directly taken into account in susceptibility calculation the reason for this choice is that the available database of observed floods used for elaborations is referred to a very short period 2001 2014 fig 3 which is too small to account for pluvial flood trends indeed it was mentioned that interannual and multidecadal variability affect the climate in the study area introducing non stationarities in rainfall trends estimating water ponding volumes and related flood scenarios capturing such non stationarities would require the collection and trend analysis for longer period in order to assess the return period of pluvial floods as well as comparing the model with extension and depth of floods which are data currently not available in other words this study considers the susceptibility as directly dependent on the ground morphology and the overall spatial density of observed floods instead of relating susceptibility to quantitative flood scenarios 3 materials methods the pluvial flood risk map was developed by means of a susceptibility analysis and an impact assessment fig 4 a and b respectively data collected for the analysis include the following informative layers observed pluvial floodings with polygon geometry 30 polygons representing areas which experienced pluvial floods between 2004 and 2007 collected mapped and published by the civil protection of rome municipality comune di roma ufficio extradipartimentale della protezione civile 2008 polygons correspond to the observed extent of flooding observed pluvial floodings with point geometry these comprise 1430 points representing the location of floodings during storm events occurred between 2001 and 2014 fig 3 coming from intervention database of fire department civil protection of rome municipality and web media the point approximates the location of the required intervention the point database was divided in two dataset on the basis of the date of occurrence points between 2004 and 2008 916 points and between 2009 and 2014 514 points used to build and validate the susceptibility map respectively all the data collected has been imported in a specialized section of a postgis geodatabase previously developed in the framework of the urbisit research project the pluvial flood risk data has been stored in a new postgresql schema in the bdgt geodatabase the audb webgis application fig 5 has been developed with the aim of centralizing and simplifying the management of flooding data through a platform independent interface and to ease the process of updating and improving the risk map the webgis key features include the ability for the user to manage insert update delete information about observed flooding events visualize and filter by a range of dates the flooding data on map and in a table navigate wms web map service layers on google basemaps further details are in reported in additional material 3 1 susceptibility analysis the susceptibility is defined as the probability of a flooding to occur in a certain area area prone to flood because of the ground morphology e g presence of topographic depressions area and fill volume of depressions the spatial probability density of observed floodings and the proximity to sewerage excess storm water flows and accumulates in catchment s lowest elevation areas which are for this reason particularly susceptible to flooding however floods can also be related to insufficient sewer capacity or poor manhole maintenance in those cases it can affect also areas which are not topographic lows thus the susceptibility was assessed starting from a preliminary screening of areas prone to flood due to morphological features depressions coupled with observed floodings flood observations dataset composed by point and polygon geometry features depressions and observed flooded areas constitute the flood prone areas and their relative degree of susceptibility was assessed through elaborations performed by gis tools finally in order to include areas close to sewer pipes assumed to be primarily at risk when sewer network is overwhelmed a hazard area was considered by buffering pipes centerlines and then adding such areas to the susceptibility map the buffering was needed in order to overcome the cartographic uncertainty in lines positioning the susceptibility analysis consists of the following phases fig 4a a dtm reconstruction and pre processing b detection and selection of topographic depression c processing of observed floodings d weighting depressions by the flood density and fill volume e weighting the flooded areas by fill depth f deriving sewerage pipe flood hazard areas g combining flood areas fill depth grid weighted depressions grid and sewer pipe flood hazard areas to obtain the susceptibility map of pluvial flood prone areas 3 1 1 a dtm reconstruction and pre processing a high resolution dtm 2 2 m was built through the anudem algorithm hutchinson and dowling 1991 anu fenner school of environment and society and geoscience australia 2008 by using lines and point ground elevation from 1 5000 vector cartography fig 4a fig 7a this tool interpolates ground elevation values by imposing constraints that ensure a connected drainage structure and the correct representation of ridges and streams from input contour data the choice of the cell resolution is consistent with the recommendations of cell size between 1 1 m and 5 5 m dtm resolution for urban flood analysis mark et al 2004 adeyemo et al 2008 then the filling dtm tool available in gis environment was used to fill small dtm sinks dtm processing errors to obtain a hydrological correct dtm 3 1 2 b detection and selection of topographic depression topographic depressions or ponds are defined as areas having low bottom elevation surrounded by an edge with higher elevation i e difference between filled dtm and the original dtm with no downslope flowpath and which can contain one or many sinks topographic depressions are part of a large framework of flood protection to control water movement in a specific time scale vesakoski et al 2014 the volume of water detention and direct surface runoff are mainly driven by the storage capacity of depressions on a watershed scale depressions were detected starting from the dtm by automated depression evaluation tool available in gis environment this tool is commonly used to identify areas containing small sinks which should be filled to obtain a hydrologically correct surface in this work depressions are considered as areas where excess water accumulates in case of drainage system overwhelming and thus represent flood prone areas in order to exclude negligible and or unrealistic features dtm processing errors depressions were selected following previous experiences adeyemo et al 2008 depressions were selected based on their fill volume and fill depth after performing a sensitivity analysis the sensitivity analysis was aimed at calibrating the threshold values for minimum volume and fill depth which can be assumed for depression selection in order to delete small depressions while minimizing the loss of potentially susceptible area and volume the trend of total depressions area and volume as a function of minimum fill depth and volume thresholds was evaluated through area and volume loss curves fig 6 a and b the volume and area loss curves show a gentle slope for minimum fill depth of 0 001 m and 0 01 m respectively above these fill depth thresholds the curves slope increase this suggesting a significant increase of area loss the volume and area loss as a function of minimum volume is negligible below 10 m3 and 0 1 m3 respectively by using a minimum volume threshold of 0 1 m3 the minimum fill depth of the resulting selected depressions is 0 00015 m which is below the fill depth value minimizing both area and volume loss 0 001 m on this basis the minimum volume of 0 1 m3 was considered as the best threshold allowing a reliable errors filtering for a 2 2 m cell dtm without losing a high percent of depression area and volume 3 1 3 c weighting the depressions layer by the flood density and fill volume the spatial overlay between observed floodings and the layer of depressions highlights that not all floodings occur within topographic depressions this result confirms that pluvial flood may also be due to anthropic causes since occurs where apparently no morphological or hydrological susceptibility exist furthermore it is possible to note that some depressions may contain more than one observed flooding occurred at different times i e higher spatial density the spatial probability density which is a function defining the distribution of observed floodings in the space was used to correctly map flood susceptibility the spatially discontinuous variable point and polygon observed floods is elaborated by means of the kernel density algorithm silverman 1986 obtaining a continuous variable representing the spatial data structure of the observed floods distribution the kernel density is a smoothing non parametric technique used for estimating the probability density function of floods visualizing the underlying distribution of a continuous variable overcoming the often irregularly positioning in space of sampling points therefore in order to correctly map flood susceptibility the kernel density algorithm was used to build the dens grid which represents the number of observed floodings per squared km the density map was then standardized and transformed in a raster map with values ranging between 0 and 1 with higher values corresponding to higher flood density fig 7 b another parameter driving the susceptibility to flood is the fill volume of depressions indeed depression with lower fill volumes means faster complete floodwater filling and thus higher susceptibility to flood for this reason depressions were ranked based on their standardized fill volume with higher score corresponding to lower fill volumes and converted to grid depr grid the dens and depr layers were multiplied in order to weight the depressions the resulting layer was classified according to the quantile distribution to obtain a 5 class layer weighted depressions the higher the class number the higher the flood potential 3 1 4 d processing of observed floodings punctual data represent the location of an observed flood which subtends a flooded area which extent is unknown the real spatial extension of floods is a function of the rainfall intensity mm hour and the terrain morphology in order to evaluate and approximate the potential extension of floods a buffer area around points was considered which elevation range is explored in order to assess the susceptibility based on the relative elevation of each cell with respect to the point elevation a nearest neighbor analysis based on average cluster considering euclidean distance as similarity measurement was accomplished the analysis uses the distance between each point and its closest neighboring point in a dataset to determine if the point pattern is random regular or clustered the analysis was performed by means of average nearest neighbor and near tools the average nearest neighbor hertz 1909 clark and evans 1954 hereafter ann calculates the distances between each feature centroid and its nearest neighbor s centroid location if the distance is minor than the expected average distance with expected average distance being based on a hypothetical random distribution with the same number of features covering the same total area the feature being analyzed is considered as part of a clustered distribution otherwise it is dispersed taniar et al 2010 the ann analysis allows to assess the clustering degree in the entire sample calculating the hypothetical distance between points average expected distance 669 46 m which is then compared with the average nearest neighbor distance the nearest neighbor ratio or index nni clark and evans 1954 is calculated as the observed distance divided by the expected average distance when index is less than 1 the pattern exhibits clustering if the index is greater than 1 the trend is toward dispersion the distance between each clustered point nni 1 and its nearest was then averaged the average near distance h between clustered points is 202 3 m table 1 it is assumed that h equals the theoretical minimum extension of a flood in a condition of flat or gentle slope morphology in other words where no terrain reliefs exist it is assumed that two point floods being at distance less or equal to 202 m are part of the same flood starting from this principle it is assumed that a circular 200 m radius buffer around a point can be considered as area susceptible to flooding this assumption allowed approximating the area reasonably considered as susceptible to flooding due to its proximity to an observed flood then in order to consider the second factor potentially driving the flooding extent terrain morphology a screening was performed on cells in the buffered areas based on the comparison with the elevation of point observation at its center as explained in next paragraph further details about the method for spatial analysis of observed floodings is reported in the additional material 3 1 5 e weighting the flooded areas by fill depth among the factors that affect flood susceptibility the height of water filling the volume of flood prone areas i e flood depth should be considered indeed the threat to personal safety and to gross structural damage caused by floods largely depends upon the flood depth the flood depth in turn is affected by the magnitude of the flood and by the topographic characteristics of the depression the greater the depth of the depression the greater the danger to people vehicles and goods fema 2014 in traditional hydraulic models the flood depth is calculated by subtracting the ground elevation from the water surface elevation for each flood scenario fig 8 a flood scenarios correspond to different return periods and allow the definition of both the maximum expected flood depth and the areal extent of the flood which is a function of the flood magnitude and topography fema 2014 in this work instead of accounting for the modeled flood depth of each return period a potential fill depth is defined corresponding to the elevation of each cell of a flood prone area with respect to its lowest point therefore the potential fill depth is calculated for each cell as the difference between the ground elevation dtm and the lowest elevation of each flood prone area lowel grid fig 4a the higher is the fill depth flad grid fig 4a fig 7c the lower is the degree of susceptibility indeed in a flood prone area heavy rainfall with longer return periods produce higher fill depths i e water levels otherwise rainfall characterized by shorter return periods will produce lower fill depths for example in a flood prone area the probability of reaching a fill depth of 2 m is higher than the probability of reaching a fill depth of 4 m note that in the detected depressions the fill depth values ranges between 0 and 0 12 m therefore variability of the depressions fill depth is considered negligible and is not included in the flad grid the lowest elevation values constituting the lowel grid were calculated and assigned to each flooded area as follows for polygonal observed floodings lowel is assumed to coincide with the lowest elevation of the polygon area a raster statistic of cells in polygons was performed to extract the lowest elevation value of each polygon regarding the buffered areas it is assumed that lowel coincides with the ground elevation value in the location of the point observed floodings cells within the 200 radius buffer were then classified on the base of their fill depth calculated as the difference between the dtm and lowel cells with elevation equal or lower than lowel fill depth 0 are likely to be flooded as well and were considered as highly susceptible indeed excess water is expected to flow preferentially toward them and accumulate in areas having lowest elevation with respect to the elevation of the observed flooding on the contrary cells with elevation higher than lowel fill depth 0 will be flooded or not depending on the storm intensity and consequent rain volume and were considered as less susceptible due to the fact that rainfall with longer return period can provoke flood the potential fill depth was categorized into 5 classes class 5 comprises all cells with fill depth 0 fill depths from 1 to 3 m were classified by using a constant increment of 1 m while class 1 comprises fill depth above 3 m class 1 fill depth between 5 and 3 m class 2 fill depth between 3 and 2 m class 3 fill depth between 2 and 1 m class 4 fill depth between 1 and 0 m class 5 fill depth 0 fill depths higher than 5 m were considered unlikely to occur and were excluded higher values of flad grid correspond to higher susceptibility 3 1 6 f deriving areas of potential sewerage flood hazard actually the drainage capacity of sewer system in the study area is unknown thus for the purpose of our work the underground drainage is neglected according with the assumption that sewerage overwhelming occurs very quickly during intense storms however many floods observed during the considered storms are located few meters close to drainage pipes fig 9 in particular floods occur in correspondence of sewerage manholes which interrupt their function of draining rainwater toward sewers when rainfall exceeds the drainage capacity sewer pipes traces were digitized and overlaid to the manholes vector layer this operation highlighted poor matching between manholes and sewer centerline due to uncertainty in cartographic lines positioning giving errors in pipes digitizing in order to include sewer pipes hazard in the susceptibility map centerlines were buffered by a distance which is the average distance between couples of manholes bounding a sewer pipe on each side 30 m the vector layer representing sewerage with buffer distance was the converted into a grid with cell value 4 in the sewerage hazard areas and 0 in the other cells 3 1 7 g combining fill depth grid weighted depressions grid and sewer pipe flood hazard areas to obtain the map of the susceptibility to pluvial flood reclassflad weighted depressions and sewer pipes grid were summed to obtain a 10 class map of susceptibility to pluvial flood with higher classes corresponding to higher susceptibility the number of classes were then reduced to five to facilitate further risk calculation the map of susceptibility to pluvial flood fig 10 comprises 5 classes 1 very low 2 low 3 medium 4 high 5 very high areas with low fill depth or high density of observed floods and small fill volume belong to the very high susceptibility class the areas of river flooding risk defined in the tiber river basin masterplan autorità di bacino del fiume tevere 2013 were added to the pluvial flood susceptibility map and highlight areas with combined fluvial and pluvial risk type 3 2 impact assessment the impact assessment aims at evaluating and rank the flood damage potentially affecting the urban elements according to the european guidance for reporting under the floods directive 2007 60 ec european commission 2013 the impact of pluvial flood should be evaluated as damage consequence to human health the environment cultural heritages and economic activities commonly the impact assessment considers a sum of social environmental and economic aspects social aspects such as social segregation level or degree of scholar instruction have a direct influence on communities resilience and need to be investigated in deep by specific competencies in order to correctly assess the vulnerability of communities in economic terms assets are considered more vulnerable depending on the expected degree of damage which is linked to technical issues such as the conservation status or level of floors in a building as well as the unit cost for railway or roads damage renard 2016 the impact is then evaluated by combining the value of element at risk and its vulnerability usually these type of approaches consider aspects such as flood characteristics e g flood depth water velocity economic and cultural value of elements and vulnerability of elements at risk stage damage curves are often built in order to relate flood intensity to damage cost however the vulnerability to flood of assets is hard to estimate where no information exists on the damage behavior of each object such a detailed assessment would require a huge effort when poor information are available simplified approaches are used and elements are grouped into classes based on the qualitative evaluation of potential expected impact the vulnerability to flood is often assumed to be constant and equal to 1 so that the element categories can immediately be referred to a relative impact this is the case of the flood risk management plan of tiber river basin authority autorità di bacino del fiume tevere 2013 compiled following the guidelines of italian ministry of environment which is based on turn on the of the european flood directive 2007 60 european commission 2007 other approaches to overcome limitations in data availability and knowledge on damage mechanisms is to group exposed assets with similar intrinsic characteristics into categories and assign a relative value for potential expected damage to each category merz et al 2010 a certain degree of expert judgment is required in order to adapt the calculation in the specific context this is not a limitation itself and is needed when a specific measure of the required data is not available emanuelsson et al 2014 in this case the associated limitations and uncertainties must be evaluated and specified in this work the potential impact is calculated by assigning a class of potential damage to each exposed element and a weighted class of damage cost urban elements considered in the impact assessment are building subdivided by their utilization e g residential education health care religious worship public office public order and safety industrial plant wastewater treatment plant service or transport networks e g main and secondary roads railway and subway lines land use e g crops urban gardens presence of critical points e g underpasses the factors which can potentially influence the damage are examined e g value of exposed goods or number of people potentially involved as well as the risk induced as a consequence of damage table 3 these aspects are defined through potential damage classes the considered damage classes comprise 1 buildings e g residential public offices schools hospitals strategic infrastructures places for recreational religious activities 2 commercial activities e g including sport facilities and crops 3 transport and service networks and other critical urban elements e g roads subways underpasses service delivery networks 4 elements with high pollution potential e g industrial plants wastewater treatment plants landfills 5 environmental cultural and archaeological heritages e g urban parks and gardens natural protected areas 6 areas with no assets or secondary importance assets exposed the criteria for qualitative estimation of elements intrinsic characteristics include the potential loss of human lives environmental and pollution damage the loss of cultural heritages and the degree of dysfunctions provoked by assets damage the potential damage of residential buildings and the economic losses for commercial activities were assumed as proportional to the number of inhabitants and people employed respectively criteria used to categorize damage classes are listed below table 2 3 2 1 a potential loss of human lives or problems for people safety residential buildings where a constant presence of people is assumed were categorized depending on the number of inhabitants declared in census data istat 2015 into three classes by means of the quantile distribution up to 16 000 between 16 000 and 20 000 above 20 000 corresponding to damage class scores 3 4 5 all the elements considered as indispensable and strategic for people safety such as hospitals or military buildings were considered as having highest score score 5 this is also because of their strategic importance in the emergency phases also public buildings with potential high people concentration such as schools and public offices were considered as having the highest score score 5 a low potential damage class score 2 was assigned to recreational and religious activities where the presence of people is concentrated in limited hours days 3 2 2 b potential economic losses for economic and commercial activities three classes of commercial activities were considered proportional to the number of people employed istat 2015 up to 7 between 7 and 40 above 40 corresponding to scores 3 4 5 respectively scores were assigned following the quantile distribution 3 2 3 c environmental damage assumed to be constant for the elements potentially dangerous for the environment in the specific context the elements potentially responsible of environmental pollution such as wastewater treatment plants industrial plants landfills were grouped in the same damage class score 4 the maximum damage class score score 5 was assigned to relevant industries assuming a high potential damage for environmental hazard 3 2 4 d value of the environmental archaeological and cultural heritages assumed to be constant for the elements in the specific context even if allow people concentration for limited hours a high potential damage class score 4 was assigned to cultural and archaeological heritages due to their cultural and social importance 3 2 5 e degree of dysfunctions for interruption of vehicle traffic transportation networks and the general city s mobility interruption of water and energy delivery services streets railways and subways are elements of primary importance indeed the flood of main streets can impede the movement of vehicles and people during emergency intervention as well as block the fluxes toward schools hospitals and generally workplaces a high potential damage class score 5 was assigned to subway and railway lines and roads in order to assign a relative cost to each damage class available data about economic losses due to past floods where examined cna roma 2015 data about damage costs are extremely scarce and are available for just two intense storm event 20 october 2011 and 16 19 june 2014 this is a diffuse limitation when performing damage cost estimation as discussed by many authors e g emanuelsson et al 2014 jongman et al 2012 potential damage costs must then properly account for the uncertainty related to data scarcity the damage costs estimated for the two events are very different and even if highly uncertain are assumed to be representative of a near full range of relevant flood events data provide only total economic loss for each category of damage table 3 a and does not report the unitary cost of assets such as the cost for squared meter of residential cleaning and repairing for the 20 october 2011 event the higher costs 69 are due to refunding for individual losses in private building both commercial and residential relative minor costs 27 8 are associated to public buildings and infrastructures damages other costs 2 8 are associated to urgent interventions and emergency management for the 16 19 june 2014 storm event the majority of reported costs 91 are due to the damage in public buildings infrastructures and delivery networks while minor costs are related to urgent interventions and private losses 4 9 and 2 8 respectively in order to evaluate and map the relative cost of flood damage the observed costs were translated into three cost classes with higher score classes corresponding to higher damage cost table 3b for the two scenarios the costs of emergency management cannot be related to specific element categories and are assumed to be part of the uncertainty in potential impact calculation for both scenarios a damage cost class was associated to each exposed element then damage cost classes were weighted proportionally to the percent contribution of each element to the total damage cost class following previous experiences in the field of weighted cost class estimation emanuelsson et al 2014 for example considering scenario 1 the class cost 3 private losses includes both flood damage to crops and industrial plants since are all private properties however it can be reasonably assumed that for equal pluvial flood magnitude the contribution to cost class for crops is lower than for main industrial plants the weights associated to cost class of crops and main industrial plants were then set to 0 1 and 1 5 respectively this operation was performed by means of expert judgement and for this reason is highly subjective although it is reasonably consistent over assets and events many uncertainty sources can affect the choice of damage cost class depending on the variability of the specific feature of each element type for example the damage recovery cost for an archaeological site can depend upon its typology e g ruin ancient building ancient street as well as upon its conservation status an uncertainty score ranging from 0 1 to 0 5 was assigned to each element accounting for the degree of damage cost class variability due to element features table 4 only the principal sources of uncertainty were considered where multiple uncertainty sources exist for one element damage cost the sum of uncertainty score constitutes the overall uncertainty the estimated potential impact map was then calculated for both scenarios as 1 potential damage class damage cost class cost class weight ʃcost class uncertainty categories of exposed assets and assigned potential damage class damage cost class and potential impact class score for both flood scenarios are listed in table 5 fig 11 shows potential damage and cost classes cost weight and uncertainty for the categories of exposed assets cost classes are referred to scenario 1 the potential impact maps for the considered scenarios as calculated by the expression 1 are shown in figs 12 and 13 the layer of potential impact was clipped by using the flood prone areas vector layer 3 3 pluvial flood risk assessment 3 3 1 pluvial flood risk map the pluvial flood risk is expressed through an index which is calculated as the sum of susceptibility and potential impact combined in a matrix as the sum of rasters representing the susceptibility and the potential impact these two elements are combined in risk levels according to a defined matrix see leitao et al 2012 niemann and illgen 2011 resulting from the sum of the 5 classes of susceptibility left column of matrix plus the 5 classes of potential impact top row of matrix ten classes of risk result from the algebraic sum the highest risk class score 10 occurs in areas where a potentially highly impacted element e g an underpass is coincident with a depression having high density of observed flooding or with a low fill depth area fig 14 shows the risk map obtained by the sum of susceptibility and potential impact for scenario 1 4 results and discussion results were evaluated by comparing the maps of susceptibility and potential impact with a subdataset of 513 punctual observations of pluvial floodings occurred between 2009 and 2014 the comparison between detected risk levels and observed floods constitutes a validation strategy kandilioti and makropoulos 2012 diakakis et al 2011 hall et al 2005 rudari et al 2014 that although being based on a qualitative comparison often with a limited number of observed floods is useful for detecting whether the criteria used were able to capture the flood occurrence it is assumed that the quality of susceptibility prediction is inversely proportional to the distance between susceptible areas and observed floods in the validation dataset the near tool was used to calculate the distance between points in the validation dataset and the susceptible areas results show that 445 validation points out of 513 86 7 are within a susceptible area the number of validation points not coincident with a susceptible area failed predictions is 68 13 the maximum distance from failed predictions and susceptible areas is 284 7 m while the mean distance is 13 m the standardized near distance values were interpolated by means of ordinary kriging the standard near map and the associated standard error fig 15a and 15b respectively allow visualizing how distance values are distributed in the map failed predictions coincide largely with crops 55 fig 16 the uncertainty related to crop damage cost is low 0 2 this meaning that the damage cost would not vary much depending on crop type a minor number of missed points 18 is coincident with residential buildings equally distributed into high medium and low number of inhabitants the associated high cost uncertainty 0 8 is due to unknown features e g number of basements building vulnerability retained to have a strong influence on damage cost other 8 missing points are coincident with sport plants roads and minor industrial plants 4 3 4 3 2 9 respectively single failed predictions are located in urban parks public buildings stations and networks commercial activities and other element types among those public buildings exhibit the highest cost uncertainty 0 9 and is then responsible for the 5 8 of the overall failed costs despite corresponding to one single point finally failed predictions were evaluated in the light of the reported damage costs for both flood scenarios table 6 for scenario 1 the majority of failed predictions 62 are within the lower class of potential impact score 1 a minor number of failed predictions 10 4 and 21 respectively are located within classes 2 3 and 4 one single failed prediction is located in the higher impact score class score 5 results for scenario 2 confirm that the majority of missed points 72 falls within the lower potential impact class score 1 while a low percent of elements 1 falls in the highest potential impact class score 5 a minor number of missed points 3 14 and 9 respectively are located within potential impact classes 2 3 and 4 one of the missed points is located in the highest impact score class score 5 in monetary terms the cost of missed prediction can be evaluated by comparing the weighted damage cost class of failed prediction with the associated observed damage cost the weighted cost class of elements coincident with failing predictions was expressed in percent of the total failed prediction cost by multiplying it times the percent number of failing points in the correspondent class the percent weighted cost of each single group was then translated into quantitative cost euros by reporting it to the overall cost in other words the cost of the 13 failed prediction corresponding to 1 300 000 euros for scenario 1 and 31 831 280 euros for scenario 2 was partitioned into damage costs classes proportionally to the calculated weighted cost of each element type estimated cost of failed prediction for scenarios 1 and 2 are shown in fig 16 by considering scenario 1 the 76 of costs are due to residential buildings 780 000 euros in total 60 and crops 214 783 euros 16 minor costs are associated to roads and industrial plants 113 000 euros 17 public buildings 75 000 euros 5 8 and stations 37 000 euros 2 9 other costs 47 000 euros 3 5 are mainly related to sport facilities urban parks and commercial activities results for scenario 2 confirms higher costs 51 in total associated to residential buildings 12 732 512 euros 40 and crops 3 506 054 euros 11 main differences with respect to scenario 1 are due to roads and public buildings costs 4 151 906 and 2 767 937 euros accounting for 13 and 8 7 respectively thus for each of the considered scenarios residential buildings and crops are responsible for the highest costs due to pluvial flood damage this result suggests that the features of residential buildings accounting for highest uncertainty need to be more detailed in order to properly allocate resources both for emergency management and for damage recovery secondarily the economic value of potential agricultural losses must be correctly evaluated and cannot be neglected indeed even if damage costs of crops are lower with respect to other element types crops account for the majority of failed predictions finally the type and conservation status of public buildings need to be specified indeed the damage costs associated to public buildings accounts for a high percent in both scenarios 1 and 2 5 8 and 8 7 respectively even if only one single building is involved roads constitutes the element with highest variability in terms of potential impact which is mainly responsible for the differences in potential impact classes between the two scenarios the estimation of roads damage cost needs to be improved introducing a ranking based on the relative importance highway main road local road and split it into different damage classes by doing so main roads accounting for higher damage cost and main traffic dysfunctions when flooded can be set to class cost 3 while minor roads can be set to lower class score 5 limitations and future perspectives many limitations can be ascribed to this work the majority of these are listed below the procedure does not follow the consolidated approach of risk calculation as a combination of hazard vulnerability and exposure in fact the described approach gives a semi quantitative classification and ranking of areas prone to flood due to morphological susceptibility and according to the relative importance and expected impact risk classes are not associated to the probability of flood occurrence for certain return period and this limits the possibility to compare the risk levels with other risk maps obtained with traditional approaches the analysis of rainfall trends is strictly needed in order to add quantitative parameters to the risk computation and must be included in further advances of this work the use of potential impact instead of vulnerability and exposure adds subjectivity to the way the risk is calculated since it is largely based on qualitative parameters and on the costs of damage at the present stage the map does not provide any element useful for projecting and dimensioning mitigation interventions but only furnishes a screening of areas higher at risk in a municipal context in the areas detected as highly at risk by the preliminary analysis quantitative information including the excess water velocity as well as the flood depth for different flooding scenarios must be calculated by means of local scale hydraulic models in order to perform an effective and detailed flood risk mapping at local scale and to obtain parameters useful for interventions planning the computation of risk levels through a sum matrix allow a synthetic simplified evaluation of natural and social aspects concurring to flood the poor resolution the subjective categorization of variables and non numeric calculation process are non negligible unresolved limitations the small time window covered by the flood observation dataset 2001 2015 is too short to account for interannual and multidecadal variability as well as for climate change effects this impedes to perform a correct storm trend analysis indeed the risk is dependent on the application and length of time over which it is assessed as the non stationarities related to anthropogenic climate and natural climate oscillations may influence the estimation of rainfall intensity frequency duration this on turn implies difficulties in linking the flood extension and damage to a rainfall amount thus basing flood risk estimates on an unsuitable time period has the potential to significantly underestimate or overestimate the true risks as demonstrated by many studies kiem et al 2003 leonard et al 2008 to overcome this problem the usage of data sets of similar length ensuring that they span a sufficient number of years which likely capture high or low annual maxima is strongly recommended verdon kidd and kiem 2015 furthermore an uncertainty analysis accounting for both the data availability and variability within the observation period should be performed in order to provide relevant information to practitioners about the reliability of rainfall intensity frequency duration estimates koutsoyiannis 2006 therefore at the present stage the risk assessment can be referred only to the observation period and this methodology cannot be considered as suitable for assessing pluvial flood risk over longer period the collection of rainfall data for a trend analysis capturing such non stationarities will be the object of further advances the validity of the susceptibility assessments is demonstrated only for pluvial flood while it is not demonstrated for floods depending on groundwater table rising nor river floods this poses strong limitations to the transferability of the work as explained in section 6 also the coincidence of many types of floods e g pluvial flood coincident with river flood is not taken into account this limiting the comprehension of the impact related to compound events results of potential impact assessment suggest that actually the uncertainties on elements features could lead to wrong economic allocation of effective costs indeed by testing the procedure with two flood scenarios with very different damage costs results of impact assessment show very similar percent of damage cost however the poor resolution of potential impact due to the same rating assigned to quantitatively very different elements exposed limits the capability to quantify and allocate resources for risk reduction measures on the base of risk matrix categories these issue can be partially solved by incrementing the number of damage cost classes and the refinement of elements features indeed linking element types with very different features to just 3 cost categories can lead to poor cost estimation accuracy which is not completely overcome by using weights and uncertainty scores the results of the risk assessments are strictly dependent on the availability of a proper observation period the longer the period the higher the degree of confidence of the model improvements in the knowledge of how the contexts experienced flood which location and fill depth as well as related damage are especially needed in areas that are rarely affected by flooding actually the potential impact assessment does not consider the increase of precautionary measures the increased risk awareness and the effectiveness of early warning systems neglecting these elements can concur to the risk degree overestimation when transposing the method to other contexts where similar categories of assets can have very different response in terms of flood risk resilience the map validation performed by simply evaluating the distance between risk areas and observed floods only gives a qualitative measure of the reliability of risk map it needs to be improved after a proper data collection by i evaluating how reliably the flood hazard models estimate the probability and the characteristics of a flood event e g flow velocity flood extent flood duration and ii comparing the potential impact map with observed damage and vulnerability of affected items 6 remarks on the transferability of the method some remark can be made about the transferability of the procedure to other regions and problems the validity of the susceptibility assessments is demonstrated only for pluvial flood in the specific case study it is suitable for the central area where stormwater ponding is assumed to be scarcely correlated with river and groundwater dynamics conversely caution should be used when transposing it to coastal areas and floodplains in floodplains and coastal delta the differences in elevation are small due to the flat morphology depressions can be large with very small depth and the water table is shallow in those cases the storage capacity of depressions and thus the susceptibility to flood must account for specific hydraulic and hydrogeological features such as water table oscillation and river stage levels in such contexts the usage of the methodology is limited to flood due to rainfall ponding assuming that river flood or groundwater flood are absent otherwise in upstream mountainous regions where the floodwater velocity can be high due to the steep slope and high hydraulic potential parameters accounting for the water velocity catchment area and the interconnection between draining branches at sub basin scale cannot be neglected consequently the method for susceptibility assessment can be replicated in urban areas with morphological setting similar to the case study however additional and site specific parameters accounting for surface and groundwater dynamics are still not yet sufficiently represented and must be included to improve the predictive capability when the methodology is transferred to different regions and different flood events with respect to the potential impact it should be noted that mostly of traditional damage models are derived for a certain study area with specific building characteristics and specific relationships between losses and flood impact factors cammerer et al 2013 reliable model application is assumed to be restricted to its region of origin oliveri and santoro 2000 thus these models cannot be easily transferred to other regions without any model adaptation and validation merz et al 2010 meyer et al 2013 as building types asset values and their vulnerability on the contrary the simplified approach described in this work for potential impact calculation based on the allocation of a damage cost percent to each class of asset can be easily transposed to different regions even affected by other types of hazards in particular the general and qualitative criteria used to group the exposed assets into potential damage classes such as loss of human lives environmental pollution loss of cultural heritages traffic dysfunctions can be transferred adapting the available information such as population census data destination of use of buildings to the local context moreover the method for ranking and assigning the damage cost classes derived by the evaluation of alternative scenarios of historical flood damage cost can be transposed to other contexts by adapting the score and rank classes to the declared and reported damage cost in the specific context and or for the specific hazard thus even if it is not sufficient to derive detailed costs in monetary terms the method for potential impact assessment allows estimating the relative degree of impact useful for a preliminary grouping of exposed assets in contexts where scarce information about elements are available future works should therefore include the investigation about the potential variability of flood losses due to different process characteristics and or different types of flood a systematic collection of quantitative flood loss data is needed to apply and validate flood loss models as well as testing the transferability to other study areas finally the transferability of the method is strictly dependent on the availability of base data concerning the territory e g elevation location and type of buildings in the european continent there is a rapidly increasing trend towards the availability and accessibility of spatial data and improvements of their properties in terms of resolution european commission 2016 however this is not the same throughout the world this limiting the capability to transfer the method to other country were data lack may impede to perform this kind of municipal scale risk mapping 7 conclusions despite many assumptions and limitations the methodology furnishes a comprehensive preliminary analysis of pluvial flood risk in urban areas differently from approaches based on hydraulic modelling and related stage damage curves requiring detailed data and long computational times the proposed methodology uses data found in local administration offices or open geodata services stored in a dedicated database and processed in a simplified procedure by means of tools in gis environment gis tools allowed to easily derive informative levels accounting for the spatial distribution of each variable concurring to pluvial flood risk the map allows the visualization of susceptibility to pluvial flood at municipal scale showing a good agreement with the validation dataset and of elements potentially impacted ranked on the basis of the expected cost in terms of human lives economic terms and cultural social aspects the main limitations of this work are due to the small time window covered by the flood observation period this impedes to account for climate non stationarities due both to natural variability and climate change when evaluating the components of rainfall intensity frequency duration moreover the poor resolution of damage cost evaluation largely performed by subjective judgment limits an effective allocation of costs for emergency management and mitigation measures in order to improve the map elements features should be better detailed this is particularly important for residential buildings crops and roads which features constitute the higher uncertainty source also the application of the methodology in areas subjected to groundwater floodings or combined river groundwater and pluvial floods need to be considered as well as testing the methodology in different urban areas by improving the impact assessment the comparative large scale pluvial flood risk assessment provided by this map can represent a tool for local administrators for prioritization of investment for reducing and preventing damages the map should be intended as a preliminary pluvial flood analysis useful for mapping the risk at municipal scale even if it does not allow to derive precise quantitative expected damage costs it permits comparative analysis for detecting areas higher at risk which should be the subject of deeper investigations functional to quantitative assessment of both susceptibility and potential impact in such areas the calculation of excess water velocity as well as the flood depth for different flooding scenarios must be performed by means of hydraulic modelling it is indispensable for correctly assessing quantitative susceptibility and relate it to assets vulnerability software all the elaborations described in the text were performed by means of esri arcgis 10 2 acknowledgments this study represents an independent research following the preliminary analysis lead in the framework of the urban georisk project supporto tecnico e indirizzi operativi ai fini della valutazione di pericolosità geologiche nelle aree urbane per la pianificazione di interventi di protezione civile 2014 financially supported by dpc the italian civil protection department cnr igag project manager g p cavinato dpc referents p pagliara a corazza f leone authors would like to thank dpc referents and gianluca ferri ufficio extradipartimentale di protezione civile di roma capitale for supporting this phase of preliminary analyses annex i math formulae potential damage class damage cost class cost class weight ʃcost class uncertainty 1 2 nni do de where do observed mean distance between each point and its nearest neighbor 3 do ʃdi n de expected mean distance for the points given in a random pattern 4 de 0 5 n a n number of studied pointsa size of the studied area 5 z do de se where 6 se 0 26136 n2 a annex ii processing observed floodings punctual data represent the location of an observed flood which subtends a flooded area which extent is unknown the real spatial extension of floods is a function of the rainfall intensity mm hour and the terrain morphology in order to evaluate and approximate the potential extension of floods a buffer area around points was considered which elevation is explored in order to assess the relative susceptibility based on the relative elevation of each cell with respect to the point elevation buffer distance was chosen on the base of the spatial relationships between points the analysis of spatial relationships was aimed at identifying clusters of points which can be considered as part of the same flooded area it was assumed that the average distance between clustered points can approximate the minimum extent of a flood area a nearest neighbor analysis based on average cluster considering euclidean distance as similarity measurement was accomplished the analysis uses the distance between each point and its closest neighboring point in a dataset to determine if the point pattern is random regular or clustered the analysis was performed by means of average nearest neighbor and near tools the average nearest neighbor hertz 1909 clark and evans 1954 hereafter ann calculates the distances between each feature centroid and its nearest neighbor s centroid location if the distance is less than the expected average distance with expected average distance being based on a hypothetical random distribution with the same number of features covering the same total area the feature being analyzed is considered as part of a clustered distribution otherwise it is dispersed if the average distance is greater than a hypothetical random distribution the feature is considered dispersed the ann analysis allows to assess the clustering degree in the entire sample calculating the hypothetical distance between points average expected distance 669 46 m which is then compared with the average nearest neighbor distance results of the average nearest neighbor analysis are summarized in table 1 table a1 average nearest neighbor summary table a1 number of input points 916 distance method euclidean study area m2 1642159239 observed mean distance or average nearest neighbor m 312 48 expected mean distance m 669 46 nearest neighbor ratio 0 466 z score 30 87 p value 0 0000 the nearest neighbor ratio or index nni clark and evans 1954 is calculated as the observed distance divided by the expected average distance when index is less than 1 the pattern exhibits clustering if the index is greater than 1 the trend is toward dispersion 7 nni do de wheredo observed mean distance between each point and its nearest neighbor 8 do ʃdi n where di equals the distance between feature i and its nearest neighboring featurede expected mean distance for the points given in a random pattern 9 de 0 5 n a n number of studied pointsa size of the studied area being the average nearest neighbor ratio 1 table 1 the distribution is clustered the z score and p value returned by the ann tool are measures of statistical significance which tell you whether or not to reject the null hypothesis for the ann statistic the null hypothesis states that features are randomly distributed the average nearest neighbor z score for the statistic is calculated as 10 z do de se where 11 se 0 26136 n2 a given the z score of 58 1560049237 there is a less than 1 likelihood that this clustered pattern could be the result of random chance the distance between each clustered point nni 1 to its nearest was then averaged statistics of near distance analysis for points exhibiting clustering are listed in table 2 table a2 near distance statistics for points with nni 1 table a2 minimum near distance m 8 51 maximum near distance m 491 95 sum m 101990 mean near distance m 202 29 standard diviation m 133 52 the average near distance h between clustered points is 202 3 m it is assumed that h equals the theoretical minimum extension of a flood in a condition of flat or gentle slope morphology in other words where no terrain reliefs exist it is assumed that two point floods being at distance less or equal to 202 m are part of the same flood starting from this principle it is assumed that a circular 200 m radius buffer around all the points can define an area susceptible to flooding subtended by each observed flood point this allowed approximating the area reasonably considered as susceptible to flooding due to its proximity to an observed flood then in order to consider the second factors potentially responsible for flooding extent terrain morphology a screening was performed on cells in the buffered areas based on the comparison with the elevation of point observation at its center as explained in next paragraph 
26364,high accuracy models are required for informed decision making in urban flood management this paper develops a new holistic framework for using information collected from multiple sources for setting parameters of a 2d flood model this illustrates the importance of identifying key urban features from the terrain data for capturing high resolution flood processes a cellular automata based model caddies was used to simulate surface water flood inundation existing reports and flood photos obtained via social media were used to set model parameters and investigate different approaches for representing infiltration and drainage system capacity in urban flood modelling the results of different approaches to processing terrain datasets indicate that the representation of urban micro features is critical to the accuracy of modelling results the constant infiltration approach is better than the rainfall reduction approach in representing soil infiltration and drainage capacity as it describes the flood recession process better this study provides an in depth insight into high resolution flood modelling keywords caddies dem resolution drainage capacity flood modelling multi information urban feature 1 introduction urban flooding has become one of the most significant natural hazards due to climate change and rapid urbanization di paola et al 2014 fu et al 2011 vacondio et al 2016 yang et al 2016 yin et al 2016b the growing trends of the frequency and the intensity of extreme rainfall events have increased the likelihood that the surface runoff overwhelms the drainage capacity as a result greater flood impacts to human society are expected to happen curebal et al 2016 korichi et al 2016 rosso and rulli 2002 for example the july 2012 flood event in beijing led to 79 deaths and an estimated economic loss of us 1 86 109 yin et al 2016a to develop effective strategies for flood risk management better understanding of flood dynamics is essential in an urban area not only the terrain elevation but also the existence of artificial structures above the ground and the drainage network underground affect the runoff propagation significantly therefore assessing the flood movements in urban area requires a modelling approach that can reflect the influences of these factors significant efforts have been made during the last few decades to improve accuracy and efficiency of urban flood modelling through enhanced methodology and numerical methods bates et al 2010 chen et al 2007 2010 nguyen et al 2006 and applications of parallel computing technologies ghimire et al 2013 glenis et al 2013 lamb et al 2009 smith and liang 2013 however modelling accuracy is still affected by four main issues 1 the level of details available in the topographic representations of terrain and urban key features haile and rientjes 2005 horritt and bates 2001 leandro et al 2016 rafieeinasab et al 2015 2 the lack of calibration and validation data fu et al 2011 hall et al 2005 horritt 2000 leandro et al 2011 3 the approach used to consider the effects of underground urban drainage infrastructure drainage capacity chen et al 2009 environment agency 2013b and 4 the uncertainty of accelerated land use changes de moel and aerts 2011 du et al 2015 shi et al 2007 micro urban features such as buildings roads and underpasses can change the flow patterns and lead to erroneous simulation results allitt et al 2009 chen et al 2012a 2012b haile and rientjes 2005 horritt and bates 2001 priestnall et al 2000 vojinovic and tutulic 2009 for example depending on how the buildings are represented in a model the water may flow around buildings when the movement is restricted by building walls or it may enter buildings when the water level exceeds the heights of their entrances in recent years the availability of light detection and ranging lidar data has enabled modelling using high resolution terrain data with a horizontal spatial resolution ranging from 0 25 m to 2 m and a vertical accuracy between 5 cm and 15 cm bates et al 2003 chen et al 2012a deshpande 2013 the improved quality of topographic datasets allows hydraulic models to better describe the flow dynamics affected by buildings in urban areas the original lidar data often in the form of a digital surface model dsm which includes buildings trees bridges over main roads and any other objects and features can significantly influence the flow direction in modelling filtering algorithms are applied to produce a digital elevation model dem that represents the ground surface only priestnall et al 2000 nevertheless ability of a generic filtering procedure to capture the complex situations in an urban environment is limited such that a better processing is necessary to build a suitable terrain model for urban flood simulations pluvial flooding in urban areas often occurs rapidly such that it is difficult to obtain good measurements of flood extents and depths for model calibration and validation to bridge this gap multiple sources of information can be used such as the existing reports or the historical flood extent maps as an alternative approach to reconstruct an accurate representation of reality chau and lee 1991 mark et al 2014 although satellite imagery was used for delineating flood extents and calibrating model parameters to simulate fluvial events di baldassarre et al 2009 domeneghetti et al 2014 horritt 2000 mason et al 2009 matgen et al 2004 oberstadler et al 1997 it is not feasible to implement such an approach for short lived pluvial events in another study dendrogeomorphic evidence i e scars on trees was used as benchmarks in roughness calibration ballesteros et al 2011 in urban areas the wide availability of smart phones digital photos and social media provides an opportunity to obtain flood related information where direct measurements are not available rené et al 2015 which can support model verification for example platforms such as twitter or crowd sourcing web portals now carry a wealth of information regarding on going or past flood events smith and liang 2013 wang et al 2018 yu et al 2016 however most of the applications can only underpin the locations and timing of flooding and require human labour to extract flood depth or extent information fohringer et al 2015 in recent years computer vision has received increasing attention in many engineering studies including water level measurement sewer overflow monitoring urban flood warning du et al 2017 narayanan et al 2014 ridolfi and manciola 2018 yu and hahn 2010 for example du et al 2017 put forward a new grey scale image processing method for fluid edge analysis which can overcome many of the inherent challenges of fluid edge measurement yu and hahn 2010 proposed a difference image based jpeg communication scheme and water level measurement scheme using sparsely sampled images in time domain the correct representation of the infiltration in permeable areas and the drainage capacity of the underground pipe system can significantly influence accuracy of urban surface flood modelling leandro et al 2016 without considering the soil infiltration and the function of drainage systems the flood simulations may be less accurate however the availability of drainage network data is very limited in many areas such that a new approach to account for the factor is needed several applications have proposed a discounted rainfall rate or a fixed infiltration rate to account for the influence of soil infiltration and drainage in urban flood modelling chang et al 2015 chen et al 2009 environment agency 2013b henonin et al 2013 this paper aims to present a new holistic framework for high resolution 2d urban flood modelling that utilizes information from multiple sources and takes into account the influences of critical urban features on flood propagation the new framework integrates the methodology to address the key challenge in improving the accuracy of urban flood simulations including extraction of flood information handling of urban key feature and model assessment methods more specifically an original procedure is developed to extract flood inundation extent and depth from social media photographs collected during flood events and cross validated using terrain analysis urban key features such as building layouts and underpasses are identified using different terrain data sets the confusion matrix is used as a model assessment approach to consider the impact of model uncertainties and determine the values of key parameters in this paper the cellular automata dual drainage simulation caddies model guidolin et al 2016 was applied to a case study in wallington london uk for comparison of two approaches for representing soil infiltration and drainage capacity the storm event of 7 june 2016 was simulated to investigate how the urban feature representations in different terrain data settings affect flood modelling the results obtained from the case study show the important role of using multi information sources in setting the parameters of the model and the impact of urban key features on the performance of 2d flood simulation 2 methodology fig 1 summarizes the new framework developed in this study which consists of three main components dem revision flood information extraction and flood modelling these are explained in detail below 2 1 terrain data revision as mentioned above the generic dsm filtering algorithms for producing bare earth dem has severe limitations in representing the actual urban environment in the environment agency s ea surface water mapping the terrain elevations of building footprints were raised by up to 0 3 m to reflect the floor level of buildings while the elevations of roads were lowered by 0 125 m environment agency 2013b we first adopted the same approach shown in fig 1 to alter dem for flood modelling and the results show a discrepancy with the filed observations therefore we further investigated different thresholds for raising the terrain elevations of building footprints and other detailed modification to better present urban micro features in flood modelling 1 the topography polygon data from the ordnance survey mastermap 2015 are converted into a land cover types map in raster format to identify the cells representing building footprints and roads 2 both the raster files of dem and land cover types overlapped such that the elevation data of building and road cells are revised accordingly 3 dsm is used to identify some other critical micro features i e underpasses by comparing their elevation differences between dem and dsm and also verifying the micro features through google map and mastermap the elevations of the cells which represent these features are further revised according to the real flow patterns near the key micro features 2 2 reconstructing flood scenarios from multiple sources although there were no detailed level or extent measurements at the location in wallington during the 2016 flood event many photos and videos were taken by the public and shared via social media or reported in the news most of the information were automatically time stamped via the devices or platforms being used the flood information is manually processed in stages fig 1 1 related flood photos and videos at different timings during the event were collected from social media and news websites 2 the landmarks near the flood boundary such as lamp posts and pavement fences in the photos were used as reference points to identify the boundary of the flood extent by comparing them to the google street view photos and satellite images then the locations of the boundary are determined using the google map service and geo referenced in gis 3 the ground elevations at those boundary points were extracted from lidar data and used as the water level to delineate the boundary of flooding assuming the cells within the boundary with lower elevations were submerged during the event the flood extent obtained from the terrain analysis was cross validated again using the above data 4 finally water depths within the flood extent were obtained by subtracting the ground elevation of each cell from the water level extracted in step 3 and then inundation volume and area can be estimated furthermore the inundation area and volume were calculated with simplified formulas 1 and 2 respectively which are then used to compare with the simulation results 1 s n δ s 2 v i 1 n h w h i δ s where s is the inundation area δ s is the area of the cell n is the number of flooded cells h w is the water level h i is the ground elevation of each cell 2 3 accounting for infiltration and drainage capacity in urban flood modelling the methods used to consider infiltration and drainage capacity in urban flood modelling are different chang et al 2015 leandro et al 2009 vojinovic and tutulic 2009 the highly efficient one dimensional 1d model is the most commonly used tool to simulate the hydraulic performance of urban drainage systems and infiltration during the simulation process of the rainfall runoff is usually calculated by using an additional module for example storm water management model swmm provides choices for modelling infiltration i e horton method and green ampt method leandro et al 2016 introduced a modified green ampt equation for handling compacted urban soils with limited storage capacity when modelling rainfall runoff in urbanized areas however difficulties exist in obtaining drainage system data to build the sewer network model liu et al 2015 zhang and pan 2014 for example in some cities there is a lack of sufficient and accurate knowledge and data on the sewer system such as pipe layout and diameters in this paper two approaches rainfall reduction and constant infiltration were used to represent soil infiltration and urban drainage network capacity in the rainfall reduction approach a fixed percentage reduction is applied to the design rainfall before input to the model to reflect the infiltration and drainage capacity in urban areas i e the design rainfall is reduced to represent infiltration over pervious areas and then a further reduction of rainfall is applied to represent the effect of the drainage system environment agency 2013a in the constant infiltration approach the soil infiltration and the function of sewer drainage system are represented as constant infiltration rates in the 2d overland flow model the design rainfall is applied directly onto the surface without any reduction 2 4 flood modelling using caddies caddies is a fast 2d urban flood simulation model based on the principle of cellular automata ca ghimire et al 2013 gibson et al 2016 guidolin et al 2012 2016 this model s effectiveness has been proven on the ea s 2d benchmark test cases and real world case studies guidolin et al 2016 in this paper the dem dsm and revised dem data were used as input to the caddies model to analyze the impact of terrain data on the simulation results 2 5 performance assessment to evaluate the performance of various model settings we adopted two indicators the true positive rate tpr and positive predictive value ppv from a confusion matrix chang et al 2015 a confusion matrix is a table with two rows and two columns which shows the number of false positives false negatives true positives and true negatives and can be used to calculate tpr ppv true negative rate and negative predictive rate as flooded cells are concerned so tpr and ppv are selected for use in this study and are calculated as below 3 t p r t p t p f n 4 p p v t p t p f p where tp represents the number of cells for which the model correctly predicted flooding fp is the number of cells incorrectly predicted as flooded and fn denotes the number of flooded cells that the model failed to predict correctly higher tpr and ppv values indicate that the model better approximated the observed flooding for example the maximum tpr and ppv are 1 under the situation when fn and fp are equal to 0 i e the predicted flooding extent from the model is the same as the observed flooding this framework was applied to a case study in london using a high performance desktop machine which has an intel core i7 4770k cpu having four physical cores at 3 50 ghz 32 gb of main memory and a tesla k20c graphics card with 2496 cuda cores and 5 gb of video memory the use of the gpu approach significantly improved computational performance while achieving required accuracy for example the simulation time for the study area 1 m 1 m resolution is less than 100 s which enables flood modelling to be undertaken while considering a large number of scenarios such as different storm events and different combinations of infiltration rates 3 case study 3 1 study area in this paper the wallington area in the london borough of sutton was used as the case study fig 2 the topography data ordance survey 2015 was classified into six different land cover types including building green land manmade surface rail road and road side areas to set up the parameters for infiltration and roughness estimates the total area is 0 25 km2 69 4 of which is occupied by buildings and impervious surfaces while 30 6 of the area remains as permeable green land although there are no rivers or watercourses wallington has suffered flooding from pluvial events frequently for example 44 mm of rainfall fell on the morning of 20 july 2007 that overwhelmed the local drainage system such that the surface water flowed along the roads from the surrounding areas towards the underpass at wallington station sutton 2010 therefore flooding in the wallington station road bridge area is due to a combination of insufficient capacity of the local drainage network and low lying terrain 3 2 terrain data the ea s 1 m resolution lidar dem bare terrain and dsm terrain with buildings and vegetation with a vertical accuracy of 0 15 m environment agency 2016 were used to represent the terrain elevation as shown in fig 3 a and b two approaches for revising the dem were applied for modelling the building blockage effect 1 dem i that the elevations of building footprints were raised by 0 3 m and roads were lowered by 0 125 m environment agency 2013b and 2 dem ii that the elevations of building footprints were raised by 5 0 m and roads were lowered by 0 125 m as shown in fig 3 c and d these two approaches are in line with recommendations in literature environment agency 2013b vojinovic and tutulic 2009 the first approach assumes that the water can flow through the building once the depth exceeds the threshold height of 0 3 m while the second approach literally forces the water to flow around buildings furthermore several problems with dem were resolved prior to modelling for example the difference between the elevation of the pavement and the road underpass is 1 5 m but is not correctly represented in the dem therefore the elevations of the pavement at the underpass were also revised to provide an accurate digital representation 3 3 drainage capacity ea developed the updated flood map for surface water for england and wales environment agency 2013a with 2 m resolution using the hydraulic model jflow 2d bradbrook 2006 in the ea s modelling the design rainfall in the urban area was reduced by 30 to represent infiltration losses in pervious surfaces and a fixed rate of rainfall reduction for most cases 12 mm h was used as the fixed rate but it was varied between 6 mm h and 20 mm h was further applied to represent the effect of the drainage system as shown in fig 4 before being input to the model to reflect the infiltration and drainage capacity in urban areas even though the released maps only provide the flood extent information i e no detailed flood depths are given they can be used as reference for evaluating caddies modelling results this was performed using the exactly same rainfall treatment settings the rainfall reduction approach assumes that soil infiltration and drainage capacity are accounted for in the model indirectly via this simplified methodology however the flood tends to recede via the drainage pipe systems once the rainfall intensity is less than the sewer capacity the rainfall reduction approach fails to correctly represent the process of flood evolution over time thus a different approach was introduced as caddies allows spatially varying infiltration rates to be specified for different land cover different constant infiltration rates were applied to each land cover category a constant infiltration approach the same is in the ea studies to reflect both urban drainage capacity and soil infiltration 3 4 rainfall events 3 4 1 design rainfall events the rainfall with three durations 1 3 and 6 h of 30 100 and 1000 year return periods using the intensity duration frequency idf curves from the flood estimation handbook feh ceh 2015 were modelled and compared to the ea s surface water mapping however after comparing the numerical simulation results of events of different durations with the same return period we found that rainfall events of 1 h duration consistently led to worse surface water flooding than events with longer durations which was not surprising considering relatively small catchment longest distance less than 1 km the design rainfall depth of 1 h and peak rainfall intensity using a 2 min time step under different return periods are shown in table 1 3 4 2 the 7 june 2016 event on 7 june 2016 a high intensity precipitation event lasting for 40 min caused flash flooding with more than 2 m water depth and three cars were completely submerged under the bridge on wallington high street the radar rainfall data were collected from the british atmospheric data centre badc archive with spatial and temporal resolutions as 1 km and 5 min respectively and used as the input into the caddies model for this event the study area is covered by four radar cells as shown in fig 2 fig 5 shows the rainfall hyetographs of the 7 june 2016 event for the four radar cells the event began around 14 00 and lasted about 1 5 h and more than 90 of the rainfall occurred in the first 40 min for example the total rainfall registered in radar cell 1 during 1 5 h was about 58 mm with the peak rainfall intensity of 163 mm h which occurred between 14 20 and 14 25 3 5 inundation data fig 6 a b and c show three photos collected from twitter featuring the flood event in the area of wallington station at 14 50 on 7 june 2016 the flood boundary was determined from these flood pictures for example as shown in fig 6 a and b the signpost and pavement fence near the boundary line were identified in the photos as the reference points by comparing these landmarks with the google street view photos and satellite images the spatial relationship between the buildings and the landmarks can be identified which helped delineate the boundary of the flooding the water level was obtained using the elevation of the boundary assuming the horizon water level covering the neighbor areas with lower elevations the flood extent was determined through terrain analysis as the red outlined polygon shown in fig 6 d furthermore the high resolution mastermap information including the detailed layouts of buildings roads and roadsides were used to cross validate the flood boundary identified from terrain analysis a water level of 51 4 m was determined at 14 50 and then the water depth of each cell was calculated by subtracting its ground elevation from the water level 4 results and discussion 4 1 comparison with the ea results we adopted the same approaches of building footprint treatment rainfall reduction and roughness setting to the ones used in the ea s surface water mapping for comparing our modelling results variable manning s roughness values were assigned to different land cover types 1 0 05 s m1 3 for the building areas 2 0 03 s m1 3 for green lands 3 0 025 s m1 3 for manmade surface areas 4 0 05 s m1 3 for rails and 5 0 02 s m1 3 for roads pavements environment agency 2013a the spatial results for the maximum inundation extent and depth in the study area considering 30 100 and 1000 year return periods are presented in fig 7 the red lines represent the flood extents from the ea maps extracted from the wms service and the blue shades represent the maximum water depths obtained by caddies as expected the underpass at wallington station had the highest water depth of about 1 5 m thus the simulation results of a sub area underneath the wallington station are highlighted and shown in detail in the rainfall reduction approach the caddies results show a good agreement between the modelled maximum inundation extent and depth and the ea results for all return periods when the drainage capacity was set to 6 mm h as shown in fig 8 a b and c this demonstrates that caddies is capable of reproducing results that are in agreement with the ones obtained from the jflow 2d model the spatially variable infiltration rates were set and validated by comparing model predictions with the ea results using the rainfall reduction approach different combinations of infiltration rates were investigated for example the rate for green land use type was chosen from 15 20 25 30 35 40 and 45 mm h and the rate for other layers such as roads manmade surfaces and similar areas was chosen from 10 15 20 25 and 30 mm h tp fp and fn in wallington were counted by comparing the flood extent obtained from the model and the identified flood extent from the ea maps the tpr and ppv were obtained from the 35 combinations of infiltration rates for 30 100 and 1000 year return periods the resulting relationships between tpr and ppv are shown in fig 9 for example each circle data point corresponds to one combination of the infiltration rate for the rainfall with the 30 year return period however trade offs exist between the two objectives and even more extensive solution sets can be generated using various assumptions in order to select the most appropriate combination of infiltration rates from the corresponding parameter sets in fig 9 we further compared the flood area of the underpass at wallington station with the reported flood areas 1084 m2 1756 m2 and 2280 m2 for the 30 100 and 1000 year design rainfall events respectively from ea flood maps as this is a critical area with potentially severe consequences table 2 shows the ppv tpr and flood area of 4 combinations of infiltration rates when using the combination of 25 mm h and 15 mm h there are minimum differences between the flood areas obtained from the model and those from ea maps as shown in fig 9 triangles a combination of infiltration rates i e 25 mm h and 15 mm h for the green land cover and other land covers respectively were chosen using the confusion matrix analysis in combination with the inundation extent comparisons for the underpass at wallington station the results for the inundation extent obtained from the constant infiltration approach show a better agreement with the ea results as shown in fig 8 d e and f than the rainfall reduction approach in addition the temporal evolution of flood volumes and areas for the study region and the underpass at wallington station for a design rainfall of 30 year return period are presented in fig 10 the flood volumes during the first 30 min of the simulations are low for both approaches and then they increase rapidly the peak rainfall occurs at about 30 min from the start the flood volume obtained using the constant infiltration approach begins to decrease after 60 min due to the continuous drainage however the flood volume obtained using the rainfall reduction approach stays relatively flat after reaching its peak because drainage is not allowed in the model setting the results demonstrate the approach using constant infiltration rate can better reflect the physical phenomena where the excess runoff is absorbed by the sewer system but the peak volumes from both approaches are similar for example the maximum flood volumes are 1643 m3 and 1784 m3 for the constant infiltration and the rainfall reduction approaches respectively the flood area curves in fig 10 b show a similar trend to the flood volume curves and the maximum flood areas are 4108 m2 and 4252 m2 respectively similarly to fig 10 a and b the flood volumes and areas in the underpass at wallington station under two different infiltration conditions are shown in fig 10 c and d the flood volume and area from the rainfall reduction approach show the same trends as with the entire study area however the changes for the constant infiltration approach are slow because the limited inlet drainage capacity cannot quickly drain away the amount of ponding water that has concentrated into the area the peak values of volume and area obtained using both approaches are similar i e the maximum flood volume and area from constant infiltration and the rainfall reduction approaches are 984 m3 and 1308 m2 and 1080 m3 and 1360 m2 respectively 4 2 the impact of terrain data to investigate the influence of the terrain data on flood simulation results at wallington station four different terrain data versions described earlier i e dem dsm dem i and dem ii with the grid size of 1 m 1 m were used spatially varied rainfall hyetographs were applied to different parts of the catchment covered by four radar cells in caddies fig 11 shows the modelled inundation depth and extent results at 14 50 on 7 june 2016 which can be compared to the inundation identified from the photos in fig 6 furthermore the identified inundation volume and area calculated with simplified formulas 1 and 2 are 1023 m3 and 1285 m2 the use of the bare dem results in a significant loss of urban feature information such as buildings and roads as shown in fig 11 a the flood extent is larger than that estimated from the photos and there is flooding inside buildings this is because the water can flow over the building layer when using the dem data for modelling however the photos in fig 6 show no flooding inside buildings clearly output is affected to a large extent by the bare dem without considering the blockage effect of buildings the dsm data includes terrain features such as road and railway embankments bridges and tunnels which may change flow paths and can influence the flood evolution over time vacondio et al 2016 for example as shown in fig 11 b the flood was divided into two parts by the railway bridge thus topography is one of the critical factors affecting the simulation results in order to achieve a satisfactory output terrain data were revised to achieve correct description of these important features fig 11 c and d show the inundation depth and extent obtained using dem i and dem ii the main difference between the two dems is the building height raised to reflect the blockage effect i e 0 3 m or 5 0 m the flood volume and area of the underpass at wallington station with dem i are 2137 m3 and 2113 m2 respectively they are however 1894 m3 and 1932 m2 respectively for dem ii the simulation results show that the inundation obtained using dem i terrain data is larger than that with dem ii however the results obtained using dem ii are closer to observations so they are considered better than those obtained with dem i this is particularly relevant to the treatment of buildings in high resolution urban flood modelling the building height used dem i is 0 3 m so the flow through buildings is allowed to occur once the depth exceeds the assumed depth threshold this confirms the finding that the raised building height has an effect on simulation results environment agency 2013b 4 3 the impact of drainage capacity as discussed above the combination of constant infiltration rates of 25 mm h and 15 mm h for green land and other land covers respectively was selected to get similar results with the ea study however when we applied that scenario to the 7 june 2016 event the modelled results showed differences from the inundation identified from collected photos i e it was larger than the identified inundation which indicated that the infiltration rates used in the model could be lower than the real drainage capacity therefore three more scenarios of infiltration rates were chosen for further impact analysis of drainage capacity infiltration i with 30 and 20 mm h infiltration ii with 40 and 30 mm h and infiltration iii with 50 and 40 mm h with the first value used for the green land and the second for other land covers the flood maps at wallington station under different infiltration scenarios are shown in fig 12 and all these flood maps were obtained with terrain data of dem ii table 3 shows the resulting inundation volume and area for the underpass at 14 50 for the simulations using the different drainage capacity scenarios the inundation volume and area for the underpass gradually reduce with increasing drainage capacity i e 1894 1686 1314 and 1014 m3 for the scenarios as infiltration infiltration i infiltration ii and infiltration iii respectively the results show that the infiltration iii produced the flood volume and area by far closest to observations with minor differences of 9 m3 and 87 m2 0 9 and 6 8 respectively finally the scenario of infiltration iii settings is used with the design rainfall of 30 100 and 1000 year return periods fig 13 shows that the modelled flood extent estimates for different return periods are smaller than that from the ea study values the results indicate that the drainage capacity in the ea study might be underestimated which is also supported by the ea s latest review of approaches to represent drainage capacity for surface water mapping environment agency 2017 5 conclusions in this paper a holistic framework was developed to utilize the publicly available data to extract flood related information for model calibration and validation the proposed procedure allows the cross validation of the derived data that improves the quality and reliability of the information we also compared different parameter settings to investigate how to represent the influences of urban key features on flood propagations in high resolution modelling approach two methods rainfall reduction and constant infiltration were investigated and compared to reflect the soil infiltration and urban drainage network capacity when those information sources are absent furthermore the impact of four different methods for extracting relevant terrain data was investigated through numerical simulations the framework was tested on a case study of the wallington london storm event on 7 june 2016 specifically the following conclusions could be drawn 1 the identified flood information obtained from social media is a useful source for setting the model parameters 2 two approaches for representing soil infiltration and drainage capacity which were tested in this work lead to different flood evolution results the results of the rainfall reduction approach are not capable of reproducing in full the expected behaviour where the flood volume and area recede gradually after the flood peak occurs the constant infiltration approach can describe the recession process better and should therefore be used for urban flood modelling 3 urban micro features including buildings and underpasses have significant influence on the inundation extent and depth the results from the study suggest the building height should be raised by 5 0 m and the underpass elevations should be revised according to the actual flow condition this analysis improves understanding of urban flood processes and helps decision making in flood risk management even though this study is developed for a small area the knowledge generated from the caddies application can be scaled up to the city scale application by addressing the basic questions of how to set different drainage capacity and how to identify the important urban features in a large scale terrain data an automatic methodology for extracting flood information from photos should be further developed and we will study this in the future furthermore many 1d 2d coupled models have been developed in recent years chen et al 2007 leandro et al 2009 which can be used for identifying which of the simplified approaches for taking into account infiltration for urban flood modelling there is a need to further compare the results of 1d 2d with the 2d models using the rainfall reduction and constant infiltration approaches acknowledgments this research was partially funded by the british council through the global innovation initiative gii206 the uk engineering and physical sciences research council under the building resilience into risk management project ep n010329 1 and the sinatra project of the nerc flooding from intense rainfall programme ne k008765 1 the first author was funded by the china scholarship council the authors would also like to thank the uk environment agency for the lidar datasets uk met office badc for the radar rainfall data ordnance survey for the master maps and nvidia corporation for the tesla k20c gpu used in this research software availability name of software caddies caflood developers michele guidolin albert s chen et al contact address centre for water systems college of engineering mathematics and physical sciences university of exeter harrison building north park road exeter ex4 4qf uk email a s chen exeter ac uk software required openmp opencl libraries hardware required multi core cpu or opencl capable graphics card gpu programming language c c opencl program size around 20 mb availability open source mit license 
26364,high accuracy models are required for informed decision making in urban flood management this paper develops a new holistic framework for using information collected from multiple sources for setting parameters of a 2d flood model this illustrates the importance of identifying key urban features from the terrain data for capturing high resolution flood processes a cellular automata based model caddies was used to simulate surface water flood inundation existing reports and flood photos obtained via social media were used to set model parameters and investigate different approaches for representing infiltration and drainage system capacity in urban flood modelling the results of different approaches to processing terrain datasets indicate that the representation of urban micro features is critical to the accuracy of modelling results the constant infiltration approach is better than the rainfall reduction approach in representing soil infiltration and drainage capacity as it describes the flood recession process better this study provides an in depth insight into high resolution flood modelling keywords caddies dem resolution drainage capacity flood modelling multi information urban feature 1 introduction urban flooding has become one of the most significant natural hazards due to climate change and rapid urbanization di paola et al 2014 fu et al 2011 vacondio et al 2016 yang et al 2016 yin et al 2016b the growing trends of the frequency and the intensity of extreme rainfall events have increased the likelihood that the surface runoff overwhelms the drainage capacity as a result greater flood impacts to human society are expected to happen curebal et al 2016 korichi et al 2016 rosso and rulli 2002 for example the july 2012 flood event in beijing led to 79 deaths and an estimated economic loss of us 1 86 109 yin et al 2016a to develop effective strategies for flood risk management better understanding of flood dynamics is essential in an urban area not only the terrain elevation but also the existence of artificial structures above the ground and the drainage network underground affect the runoff propagation significantly therefore assessing the flood movements in urban area requires a modelling approach that can reflect the influences of these factors significant efforts have been made during the last few decades to improve accuracy and efficiency of urban flood modelling through enhanced methodology and numerical methods bates et al 2010 chen et al 2007 2010 nguyen et al 2006 and applications of parallel computing technologies ghimire et al 2013 glenis et al 2013 lamb et al 2009 smith and liang 2013 however modelling accuracy is still affected by four main issues 1 the level of details available in the topographic representations of terrain and urban key features haile and rientjes 2005 horritt and bates 2001 leandro et al 2016 rafieeinasab et al 2015 2 the lack of calibration and validation data fu et al 2011 hall et al 2005 horritt 2000 leandro et al 2011 3 the approach used to consider the effects of underground urban drainage infrastructure drainage capacity chen et al 2009 environment agency 2013b and 4 the uncertainty of accelerated land use changes de moel and aerts 2011 du et al 2015 shi et al 2007 micro urban features such as buildings roads and underpasses can change the flow patterns and lead to erroneous simulation results allitt et al 2009 chen et al 2012a 2012b haile and rientjes 2005 horritt and bates 2001 priestnall et al 2000 vojinovic and tutulic 2009 for example depending on how the buildings are represented in a model the water may flow around buildings when the movement is restricted by building walls or it may enter buildings when the water level exceeds the heights of their entrances in recent years the availability of light detection and ranging lidar data has enabled modelling using high resolution terrain data with a horizontal spatial resolution ranging from 0 25 m to 2 m and a vertical accuracy between 5 cm and 15 cm bates et al 2003 chen et al 2012a deshpande 2013 the improved quality of topographic datasets allows hydraulic models to better describe the flow dynamics affected by buildings in urban areas the original lidar data often in the form of a digital surface model dsm which includes buildings trees bridges over main roads and any other objects and features can significantly influence the flow direction in modelling filtering algorithms are applied to produce a digital elevation model dem that represents the ground surface only priestnall et al 2000 nevertheless ability of a generic filtering procedure to capture the complex situations in an urban environment is limited such that a better processing is necessary to build a suitable terrain model for urban flood simulations pluvial flooding in urban areas often occurs rapidly such that it is difficult to obtain good measurements of flood extents and depths for model calibration and validation to bridge this gap multiple sources of information can be used such as the existing reports or the historical flood extent maps as an alternative approach to reconstruct an accurate representation of reality chau and lee 1991 mark et al 2014 although satellite imagery was used for delineating flood extents and calibrating model parameters to simulate fluvial events di baldassarre et al 2009 domeneghetti et al 2014 horritt 2000 mason et al 2009 matgen et al 2004 oberstadler et al 1997 it is not feasible to implement such an approach for short lived pluvial events in another study dendrogeomorphic evidence i e scars on trees was used as benchmarks in roughness calibration ballesteros et al 2011 in urban areas the wide availability of smart phones digital photos and social media provides an opportunity to obtain flood related information where direct measurements are not available rené et al 2015 which can support model verification for example platforms such as twitter or crowd sourcing web portals now carry a wealth of information regarding on going or past flood events smith and liang 2013 wang et al 2018 yu et al 2016 however most of the applications can only underpin the locations and timing of flooding and require human labour to extract flood depth or extent information fohringer et al 2015 in recent years computer vision has received increasing attention in many engineering studies including water level measurement sewer overflow monitoring urban flood warning du et al 2017 narayanan et al 2014 ridolfi and manciola 2018 yu and hahn 2010 for example du et al 2017 put forward a new grey scale image processing method for fluid edge analysis which can overcome many of the inherent challenges of fluid edge measurement yu and hahn 2010 proposed a difference image based jpeg communication scheme and water level measurement scheme using sparsely sampled images in time domain the correct representation of the infiltration in permeable areas and the drainage capacity of the underground pipe system can significantly influence accuracy of urban surface flood modelling leandro et al 2016 without considering the soil infiltration and the function of drainage systems the flood simulations may be less accurate however the availability of drainage network data is very limited in many areas such that a new approach to account for the factor is needed several applications have proposed a discounted rainfall rate or a fixed infiltration rate to account for the influence of soil infiltration and drainage in urban flood modelling chang et al 2015 chen et al 2009 environment agency 2013b henonin et al 2013 this paper aims to present a new holistic framework for high resolution 2d urban flood modelling that utilizes information from multiple sources and takes into account the influences of critical urban features on flood propagation the new framework integrates the methodology to address the key challenge in improving the accuracy of urban flood simulations including extraction of flood information handling of urban key feature and model assessment methods more specifically an original procedure is developed to extract flood inundation extent and depth from social media photographs collected during flood events and cross validated using terrain analysis urban key features such as building layouts and underpasses are identified using different terrain data sets the confusion matrix is used as a model assessment approach to consider the impact of model uncertainties and determine the values of key parameters in this paper the cellular automata dual drainage simulation caddies model guidolin et al 2016 was applied to a case study in wallington london uk for comparison of two approaches for representing soil infiltration and drainage capacity the storm event of 7 june 2016 was simulated to investigate how the urban feature representations in different terrain data settings affect flood modelling the results obtained from the case study show the important role of using multi information sources in setting the parameters of the model and the impact of urban key features on the performance of 2d flood simulation 2 methodology fig 1 summarizes the new framework developed in this study which consists of three main components dem revision flood information extraction and flood modelling these are explained in detail below 2 1 terrain data revision as mentioned above the generic dsm filtering algorithms for producing bare earth dem has severe limitations in representing the actual urban environment in the environment agency s ea surface water mapping the terrain elevations of building footprints were raised by up to 0 3 m to reflect the floor level of buildings while the elevations of roads were lowered by 0 125 m environment agency 2013b we first adopted the same approach shown in fig 1 to alter dem for flood modelling and the results show a discrepancy with the filed observations therefore we further investigated different thresholds for raising the terrain elevations of building footprints and other detailed modification to better present urban micro features in flood modelling 1 the topography polygon data from the ordnance survey mastermap 2015 are converted into a land cover types map in raster format to identify the cells representing building footprints and roads 2 both the raster files of dem and land cover types overlapped such that the elevation data of building and road cells are revised accordingly 3 dsm is used to identify some other critical micro features i e underpasses by comparing their elevation differences between dem and dsm and also verifying the micro features through google map and mastermap the elevations of the cells which represent these features are further revised according to the real flow patterns near the key micro features 2 2 reconstructing flood scenarios from multiple sources although there were no detailed level or extent measurements at the location in wallington during the 2016 flood event many photos and videos were taken by the public and shared via social media or reported in the news most of the information were automatically time stamped via the devices or platforms being used the flood information is manually processed in stages fig 1 1 related flood photos and videos at different timings during the event were collected from social media and news websites 2 the landmarks near the flood boundary such as lamp posts and pavement fences in the photos were used as reference points to identify the boundary of the flood extent by comparing them to the google street view photos and satellite images then the locations of the boundary are determined using the google map service and geo referenced in gis 3 the ground elevations at those boundary points were extracted from lidar data and used as the water level to delineate the boundary of flooding assuming the cells within the boundary with lower elevations were submerged during the event the flood extent obtained from the terrain analysis was cross validated again using the above data 4 finally water depths within the flood extent were obtained by subtracting the ground elevation of each cell from the water level extracted in step 3 and then inundation volume and area can be estimated furthermore the inundation area and volume were calculated with simplified formulas 1 and 2 respectively which are then used to compare with the simulation results 1 s n δ s 2 v i 1 n h w h i δ s where s is the inundation area δ s is the area of the cell n is the number of flooded cells h w is the water level h i is the ground elevation of each cell 2 3 accounting for infiltration and drainage capacity in urban flood modelling the methods used to consider infiltration and drainage capacity in urban flood modelling are different chang et al 2015 leandro et al 2009 vojinovic and tutulic 2009 the highly efficient one dimensional 1d model is the most commonly used tool to simulate the hydraulic performance of urban drainage systems and infiltration during the simulation process of the rainfall runoff is usually calculated by using an additional module for example storm water management model swmm provides choices for modelling infiltration i e horton method and green ampt method leandro et al 2016 introduced a modified green ampt equation for handling compacted urban soils with limited storage capacity when modelling rainfall runoff in urbanized areas however difficulties exist in obtaining drainage system data to build the sewer network model liu et al 2015 zhang and pan 2014 for example in some cities there is a lack of sufficient and accurate knowledge and data on the sewer system such as pipe layout and diameters in this paper two approaches rainfall reduction and constant infiltration were used to represent soil infiltration and urban drainage network capacity in the rainfall reduction approach a fixed percentage reduction is applied to the design rainfall before input to the model to reflect the infiltration and drainage capacity in urban areas i e the design rainfall is reduced to represent infiltration over pervious areas and then a further reduction of rainfall is applied to represent the effect of the drainage system environment agency 2013a in the constant infiltration approach the soil infiltration and the function of sewer drainage system are represented as constant infiltration rates in the 2d overland flow model the design rainfall is applied directly onto the surface without any reduction 2 4 flood modelling using caddies caddies is a fast 2d urban flood simulation model based on the principle of cellular automata ca ghimire et al 2013 gibson et al 2016 guidolin et al 2012 2016 this model s effectiveness has been proven on the ea s 2d benchmark test cases and real world case studies guidolin et al 2016 in this paper the dem dsm and revised dem data were used as input to the caddies model to analyze the impact of terrain data on the simulation results 2 5 performance assessment to evaluate the performance of various model settings we adopted two indicators the true positive rate tpr and positive predictive value ppv from a confusion matrix chang et al 2015 a confusion matrix is a table with two rows and two columns which shows the number of false positives false negatives true positives and true negatives and can be used to calculate tpr ppv true negative rate and negative predictive rate as flooded cells are concerned so tpr and ppv are selected for use in this study and are calculated as below 3 t p r t p t p f n 4 p p v t p t p f p where tp represents the number of cells for which the model correctly predicted flooding fp is the number of cells incorrectly predicted as flooded and fn denotes the number of flooded cells that the model failed to predict correctly higher tpr and ppv values indicate that the model better approximated the observed flooding for example the maximum tpr and ppv are 1 under the situation when fn and fp are equal to 0 i e the predicted flooding extent from the model is the same as the observed flooding this framework was applied to a case study in london using a high performance desktop machine which has an intel core i7 4770k cpu having four physical cores at 3 50 ghz 32 gb of main memory and a tesla k20c graphics card with 2496 cuda cores and 5 gb of video memory the use of the gpu approach significantly improved computational performance while achieving required accuracy for example the simulation time for the study area 1 m 1 m resolution is less than 100 s which enables flood modelling to be undertaken while considering a large number of scenarios such as different storm events and different combinations of infiltration rates 3 case study 3 1 study area in this paper the wallington area in the london borough of sutton was used as the case study fig 2 the topography data ordance survey 2015 was classified into six different land cover types including building green land manmade surface rail road and road side areas to set up the parameters for infiltration and roughness estimates the total area is 0 25 km2 69 4 of which is occupied by buildings and impervious surfaces while 30 6 of the area remains as permeable green land although there are no rivers or watercourses wallington has suffered flooding from pluvial events frequently for example 44 mm of rainfall fell on the morning of 20 july 2007 that overwhelmed the local drainage system such that the surface water flowed along the roads from the surrounding areas towards the underpass at wallington station sutton 2010 therefore flooding in the wallington station road bridge area is due to a combination of insufficient capacity of the local drainage network and low lying terrain 3 2 terrain data the ea s 1 m resolution lidar dem bare terrain and dsm terrain with buildings and vegetation with a vertical accuracy of 0 15 m environment agency 2016 were used to represent the terrain elevation as shown in fig 3 a and b two approaches for revising the dem were applied for modelling the building blockage effect 1 dem i that the elevations of building footprints were raised by 0 3 m and roads were lowered by 0 125 m environment agency 2013b and 2 dem ii that the elevations of building footprints were raised by 5 0 m and roads were lowered by 0 125 m as shown in fig 3 c and d these two approaches are in line with recommendations in literature environment agency 2013b vojinovic and tutulic 2009 the first approach assumes that the water can flow through the building once the depth exceeds the threshold height of 0 3 m while the second approach literally forces the water to flow around buildings furthermore several problems with dem were resolved prior to modelling for example the difference between the elevation of the pavement and the road underpass is 1 5 m but is not correctly represented in the dem therefore the elevations of the pavement at the underpass were also revised to provide an accurate digital representation 3 3 drainage capacity ea developed the updated flood map for surface water for england and wales environment agency 2013a with 2 m resolution using the hydraulic model jflow 2d bradbrook 2006 in the ea s modelling the design rainfall in the urban area was reduced by 30 to represent infiltration losses in pervious surfaces and a fixed rate of rainfall reduction for most cases 12 mm h was used as the fixed rate but it was varied between 6 mm h and 20 mm h was further applied to represent the effect of the drainage system as shown in fig 4 before being input to the model to reflect the infiltration and drainage capacity in urban areas even though the released maps only provide the flood extent information i e no detailed flood depths are given they can be used as reference for evaluating caddies modelling results this was performed using the exactly same rainfall treatment settings the rainfall reduction approach assumes that soil infiltration and drainage capacity are accounted for in the model indirectly via this simplified methodology however the flood tends to recede via the drainage pipe systems once the rainfall intensity is less than the sewer capacity the rainfall reduction approach fails to correctly represent the process of flood evolution over time thus a different approach was introduced as caddies allows spatially varying infiltration rates to be specified for different land cover different constant infiltration rates were applied to each land cover category a constant infiltration approach the same is in the ea studies to reflect both urban drainage capacity and soil infiltration 3 4 rainfall events 3 4 1 design rainfall events the rainfall with three durations 1 3 and 6 h of 30 100 and 1000 year return periods using the intensity duration frequency idf curves from the flood estimation handbook feh ceh 2015 were modelled and compared to the ea s surface water mapping however after comparing the numerical simulation results of events of different durations with the same return period we found that rainfall events of 1 h duration consistently led to worse surface water flooding than events with longer durations which was not surprising considering relatively small catchment longest distance less than 1 km the design rainfall depth of 1 h and peak rainfall intensity using a 2 min time step under different return periods are shown in table 1 3 4 2 the 7 june 2016 event on 7 june 2016 a high intensity precipitation event lasting for 40 min caused flash flooding with more than 2 m water depth and three cars were completely submerged under the bridge on wallington high street the radar rainfall data were collected from the british atmospheric data centre badc archive with spatial and temporal resolutions as 1 km and 5 min respectively and used as the input into the caddies model for this event the study area is covered by four radar cells as shown in fig 2 fig 5 shows the rainfall hyetographs of the 7 june 2016 event for the four radar cells the event began around 14 00 and lasted about 1 5 h and more than 90 of the rainfall occurred in the first 40 min for example the total rainfall registered in radar cell 1 during 1 5 h was about 58 mm with the peak rainfall intensity of 163 mm h which occurred between 14 20 and 14 25 3 5 inundation data fig 6 a b and c show three photos collected from twitter featuring the flood event in the area of wallington station at 14 50 on 7 june 2016 the flood boundary was determined from these flood pictures for example as shown in fig 6 a and b the signpost and pavement fence near the boundary line were identified in the photos as the reference points by comparing these landmarks with the google street view photos and satellite images the spatial relationship between the buildings and the landmarks can be identified which helped delineate the boundary of the flooding the water level was obtained using the elevation of the boundary assuming the horizon water level covering the neighbor areas with lower elevations the flood extent was determined through terrain analysis as the red outlined polygon shown in fig 6 d furthermore the high resolution mastermap information including the detailed layouts of buildings roads and roadsides were used to cross validate the flood boundary identified from terrain analysis a water level of 51 4 m was determined at 14 50 and then the water depth of each cell was calculated by subtracting its ground elevation from the water level 4 results and discussion 4 1 comparison with the ea results we adopted the same approaches of building footprint treatment rainfall reduction and roughness setting to the ones used in the ea s surface water mapping for comparing our modelling results variable manning s roughness values were assigned to different land cover types 1 0 05 s m1 3 for the building areas 2 0 03 s m1 3 for green lands 3 0 025 s m1 3 for manmade surface areas 4 0 05 s m1 3 for rails and 5 0 02 s m1 3 for roads pavements environment agency 2013a the spatial results for the maximum inundation extent and depth in the study area considering 30 100 and 1000 year return periods are presented in fig 7 the red lines represent the flood extents from the ea maps extracted from the wms service and the blue shades represent the maximum water depths obtained by caddies as expected the underpass at wallington station had the highest water depth of about 1 5 m thus the simulation results of a sub area underneath the wallington station are highlighted and shown in detail in the rainfall reduction approach the caddies results show a good agreement between the modelled maximum inundation extent and depth and the ea results for all return periods when the drainage capacity was set to 6 mm h as shown in fig 8 a b and c this demonstrates that caddies is capable of reproducing results that are in agreement with the ones obtained from the jflow 2d model the spatially variable infiltration rates were set and validated by comparing model predictions with the ea results using the rainfall reduction approach different combinations of infiltration rates were investigated for example the rate for green land use type was chosen from 15 20 25 30 35 40 and 45 mm h and the rate for other layers such as roads manmade surfaces and similar areas was chosen from 10 15 20 25 and 30 mm h tp fp and fn in wallington were counted by comparing the flood extent obtained from the model and the identified flood extent from the ea maps the tpr and ppv were obtained from the 35 combinations of infiltration rates for 30 100 and 1000 year return periods the resulting relationships between tpr and ppv are shown in fig 9 for example each circle data point corresponds to one combination of the infiltration rate for the rainfall with the 30 year return period however trade offs exist between the two objectives and even more extensive solution sets can be generated using various assumptions in order to select the most appropriate combination of infiltration rates from the corresponding parameter sets in fig 9 we further compared the flood area of the underpass at wallington station with the reported flood areas 1084 m2 1756 m2 and 2280 m2 for the 30 100 and 1000 year design rainfall events respectively from ea flood maps as this is a critical area with potentially severe consequences table 2 shows the ppv tpr and flood area of 4 combinations of infiltration rates when using the combination of 25 mm h and 15 mm h there are minimum differences between the flood areas obtained from the model and those from ea maps as shown in fig 9 triangles a combination of infiltration rates i e 25 mm h and 15 mm h for the green land cover and other land covers respectively were chosen using the confusion matrix analysis in combination with the inundation extent comparisons for the underpass at wallington station the results for the inundation extent obtained from the constant infiltration approach show a better agreement with the ea results as shown in fig 8 d e and f than the rainfall reduction approach in addition the temporal evolution of flood volumes and areas for the study region and the underpass at wallington station for a design rainfall of 30 year return period are presented in fig 10 the flood volumes during the first 30 min of the simulations are low for both approaches and then they increase rapidly the peak rainfall occurs at about 30 min from the start the flood volume obtained using the constant infiltration approach begins to decrease after 60 min due to the continuous drainage however the flood volume obtained using the rainfall reduction approach stays relatively flat after reaching its peak because drainage is not allowed in the model setting the results demonstrate the approach using constant infiltration rate can better reflect the physical phenomena where the excess runoff is absorbed by the sewer system but the peak volumes from both approaches are similar for example the maximum flood volumes are 1643 m3 and 1784 m3 for the constant infiltration and the rainfall reduction approaches respectively the flood area curves in fig 10 b show a similar trend to the flood volume curves and the maximum flood areas are 4108 m2 and 4252 m2 respectively similarly to fig 10 a and b the flood volumes and areas in the underpass at wallington station under two different infiltration conditions are shown in fig 10 c and d the flood volume and area from the rainfall reduction approach show the same trends as with the entire study area however the changes for the constant infiltration approach are slow because the limited inlet drainage capacity cannot quickly drain away the amount of ponding water that has concentrated into the area the peak values of volume and area obtained using both approaches are similar i e the maximum flood volume and area from constant infiltration and the rainfall reduction approaches are 984 m3 and 1308 m2 and 1080 m3 and 1360 m2 respectively 4 2 the impact of terrain data to investigate the influence of the terrain data on flood simulation results at wallington station four different terrain data versions described earlier i e dem dsm dem i and dem ii with the grid size of 1 m 1 m were used spatially varied rainfall hyetographs were applied to different parts of the catchment covered by four radar cells in caddies fig 11 shows the modelled inundation depth and extent results at 14 50 on 7 june 2016 which can be compared to the inundation identified from the photos in fig 6 furthermore the identified inundation volume and area calculated with simplified formulas 1 and 2 are 1023 m3 and 1285 m2 the use of the bare dem results in a significant loss of urban feature information such as buildings and roads as shown in fig 11 a the flood extent is larger than that estimated from the photos and there is flooding inside buildings this is because the water can flow over the building layer when using the dem data for modelling however the photos in fig 6 show no flooding inside buildings clearly output is affected to a large extent by the bare dem without considering the blockage effect of buildings the dsm data includes terrain features such as road and railway embankments bridges and tunnels which may change flow paths and can influence the flood evolution over time vacondio et al 2016 for example as shown in fig 11 b the flood was divided into two parts by the railway bridge thus topography is one of the critical factors affecting the simulation results in order to achieve a satisfactory output terrain data were revised to achieve correct description of these important features fig 11 c and d show the inundation depth and extent obtained using dem i and dem ii the main difference between the two dems is the building height raised to reflect the blockage effect i e 0 3 m or 5 0 m the flood volume and area of the underpass at wallington station with dem i are 2137 m3 and 2113 m2 respectively they are however 1894 m3 and 1932 m2 respectively for dem ii the simulation results show that the inundation obtained using dem i terrain data is larger than that with dem ii however the results obtained using dem ii are closer to observations so they are considered better than those obtained with dem i this is particularly relevant to the treatment of buildings in high resolution urban flood modelling the building height used dem i is 0 3 m so the flow through buildings is allowed to occur once the depth exceeds the assumed depth threshold this confirms the finding that the raised building height has an effect on simulation results environment agency 2013b 4 3 the impact of drainage capacity as discussed above the combination of constant infiltration rates of 25 mm h and 15 mm h for green land and other land covers respectively was selected to get similar results with the ea study however when we applied that scenario to the 7 june 2016 event the modelled results showed differences from the inundation identified from collected photos i e it was larger than the identified inundation which indicated that the infiltration rates used in the model could be lower than the real drainage capacity therefore three more scenarios of infiltration rates were chosen for further impact analysis of drainage capacity infiltration i with 30 and 20 mm h infiltration ii with 40 and 30 mm h and infiltration iii with 50 and 40 mm h with the first value used for the green land and the second for other land covers the flood maps at wallington station under different infiltration scenarios are shown in fig 12 and all these flood maps were obtained with terrain data of dem ii table 3 shows the resulting inundation volume and area for the underpass at 14 50 for the simulations using the different drainage capacity scenarios the inundation volume and area for the underpass gradually reduce with increasing drainage capacity i e 1894 1686 1314 and 1014 m3 for the scenarios as infiltration infiltration i infiltration ii and infiltration iii respectively the results show that the infiltration iii produced the flood volume and area by far closest to observations with minor differences of 9 m3 and 87 m2 0 9 and 6 8 respectively finally the scenario of infiltration iii settings is used with the design rainfall of 30 100 and 1000 year return periods fig 13 shows that the modelled flood extent estimates for different return periods are smaller than that from the ea study values the results indicate that the drainage capacity in the ea study might be underestimated which is also supported by the ea s latest review of approaches to represent drainage capacity for surface water mapping environment agency 2017 5 conclusions in this paper a holistic framework was developed to utilize the publicly available data to extract flood related information for model calibration and validation the proposed procedure allows the cross validation of the derived data that improves the quality and reliability of the information we also compared different parameter settings to investigate how to represent the influences of urban key features on flood propagations in high resolution modelling approach two methods rainfall reduction and constant infiltration were investigated and compared to reflect the soil infiltration and urban drainage network capacity when those information sources are absent furthermore the impact of four different methods for extracting relevant terrain data was investigated through numerical simulations the framework was tested on a case study of the wallington london storm event on 7 june 2016 specifically the following conclusions could be drawn 1 the identified flood information obtained from social media is a useful source for setting the model parameters 2 two approaches for representing soil infiltration and drainage capacity which were tested in this work lead to different flood evolution results the results of the rainfall reduction approach are not capable of reproducing in full the expected behaviour where the flood volume and area recede gradually after the flood peak occurs the constant infiltration approach can describe the recession process better and should therefore be used for urban flood modelling 3 urban micro features including buildings and underpasses have significant influence on the inundation extent and depth the results from the study suggest the building height should be raised by 5 0 m and the underpass elevations should be revised according to the actual flow condition this analysis improves understanding of urban flood processes and helps decision making in flood risk management even though this study is developed for a small area the knowledge generated from the caddies application can be scaled up to the city scale application by addressing the basic questions of how to set different drainage capacity and how to identify the important urban features in a large scale terrain data an automatic methodology for extracting flood information from photos should be further developed and we will study this in the future furthermore many 1d 2d coupled models have been developed in recent years chen et al 2007 leandro et al 2009 which can be used for identifying which of the simplified approaches for taking into account infiltration for urban flood modelling there is a need to further compare the results of 1d 2d with the 2d models using the rainfall reduction and constant infiltration approaches acknowledgments this research was partially funded by the british council through the global innovation initiative gii206 the uk engineering and physical sciences research council under the building resilience into risk management project ep n010329 1 and the sinatra project of the nerc flooding from intense rainfall programme ne k008765 1 the first author was funded by the china scholarship council the authors would also like to thank the uk environment agency for the lidar datasets uk met office badc for the radar rainfall data ordnance survey for the master maps and nvidia corporation for the tesla k20c gpu used in this research software availability name of software caddies caflood developers michele guidolin albert s chen et al contact address centre for water systems college of engineering mathematics and physical sciences university of exeter harrison building north park road exeter ex4 4qf uk email a s chen exeter ac uk software required openmp opencl libraries hardware required multi core cpu or opencl capable graphics card gpu programming language c c opencl program size around 20 mb availability open source mit license 
