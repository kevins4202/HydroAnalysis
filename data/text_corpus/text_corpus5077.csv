index,text
25385,we investigate the effects of physics based constraints added to the loss function of a long short term memory lstm network on its performance in daily streamflow prediction three types of constraints mass balance energy balance and storage discharge relationship along with their combinations are tested across 34 river basins in nebraska we found that the addition of constraints improves the model performance in several basins but there are also cases where the performance drops or does not differ significantly mass and energy balance constraints improve the performance in 38 and 32 of catchments respectively while storage discharge constraints improve the performance in 12 of catchments the combination of mass and energy balance constraints has a positive effect on 41 of catchments while the combination of mass balance and storage discharge constraints improves the performance in 26 of catchments we recommend the use of constraints in cases where they boost the lstm performance keywords lstm physics based machine learning streamflow prediction data availability data will be made available on request 1 introduction accurate streamflow prediction is of critical importance for various sectors such as water resources planning and management agricultural water management disaster mitigation efforts etc dong et al 2020 gauch et al 2021b ghimire et al 2021 hunt et al 2022 as the demand for water resources continues to increase the focus on modeling streamflow has intensified with the proliferation of high resolution mapping and other remote sensing datasets together with the availability of datasets at high spatial and temporal resolutions we have seen a rise in novel methods for modeling streamflow across different scales machine learning ml models in particular have shown significant promise in this area with applications including flood forecasting castangia et al 2023 krajewski et al 2017 lin et al 2013 nevo et al 2021 yuan et al 2018 flood early warning systems moon et al 2019 pyayt et al 2014 sanders et al 2022 flood inundation chang et al 2018 kim and han 2020 tanoue et al 2016 flood prediction tongal and booij 2018 runoff prediction kratzert et al 2018 and streamflow prediction parisouj et al 2020 tongal and booij 2018 xiang and demir 2020 yaseen et al 2016 despite the advancement due to their restricted interpretability anderson and radić 2022 bhasme et al 2021 gilpin et al 2019 reichstein et al 2019 physical inconsistencies karpatne et al 2017 willard et al 2020 and enduring problems with equifinality troch et al 2015 ml algorithms are often criticized by domain experts long short term memory lstm networks hochreiter and schmidhuber 1997 have been widely implemented in the field of hydrology of late these networks can store and regulate information over time gauch et al 2021a hochreiter and schmidhuber 1997 kratzert et al 2019 yang et al 2023 which makes them particularly suitable for simulating the memory effects of different types of hydrological variables while traditional recurrent neural networks rnns are limited in their ability to remember long term sequences in modeling time dependent and sequential data bengio et al 1994 lstms can leverage information from both short term and long term dependencies which is advantageous for hydrologic applications like streamflow prediction khandelwal et al 2020 kratzert et al 2018 majeske et al 2022 song et al 2020 currently there have been efforts to combine physical understanding with ml models to improve their performance and interpretability daw et al 2017 2019 hoedt et al 2021 karpatne et al 2017 young et al 2017 for example to estimate lake temperature daw et al 2017 presented a physics guided neural network pgnn model which fuses a physics based model with a neural network they established a framework for developing neural networks that utilize physical equations and principles in which a crucial link between the temperature density and depth of water is incorporated into the physics based loss function of the lstm model jia et al 2019 extended this work by including a physics based penalty for energy conservation in the loss function to ensure consistency between the lake thermal energy gain over time and the net thermodynamic fluxes into and out of the lake thereby capturing more complex and general physical relationships it is our understanding that there has been limited research on adding physics based constraints into the loss function of the ml model and what type of constraints to include for streamflow prediction incorporating physics based constraints such as water balance energy balance and storage discharge relationship into the loss function of a ml model for streamflow prediction is important because it allows the model to consider real world physical processes that affect the amount of water available for streamflow these constraints play a vital role in accurately predicting streamflow by considering the interactions between various components of the hydrological system such as precipitation evaporation and storage understanding these elements is essential for effective streamflow prediction since they all influence the amount of water that is available for streamflow however there is a lack of research on the specific types of physics based constraints that should be included in the loss function of a machine learning model for streamflow prediction in this study we present a framework for modeling streamflow in 34 basins in nebraska using high resolution datasets that incorporates three types of physics based constraints water balance energy balance and storage discharge relationship into the loss function of an lstm model we evaluate the effectiveness of these constraints and examine their individual and combined effects on streamflow prediction by independently training the lstm model for each catchment we aim to provide a new approach for combining machine learning and physics based constraints in streamflow prediction that can contribute to the field of hydrology 2 study area and data 2 1 study area the study area for this research was the hydrologic unit code huc 8 basins of nebraska out of the 72 huc 8 basins in the state 34 were selected for this analysis based on the availability of stream gage data for at least 20 years the locations of the selected huc 8 basins within nebraska are shown in fig 1 the selected basins represent a range of geographies and hydrological conditions within the state stream gage data for these basins were used as the primary source of observations for the study basins that did not have a gage station at the outlet or had limited years of data were excluded from the analysis this study aims to provide a framework for improving streamflow prediction in these basins by incorporating physics based constraints into the loss function of lstm 2 2 data the atmospheric and land data for this study was obtained from the era5 dataset hersbach et al 2020 which is the fifth generation atmospheric reanalysis from the european centre for medium range weather forecasts ecmwf era5 provides global coverage on a 30 km grid with 137 levels of atmospheric resolution from the ground up to 80 km in altitude and includes hourly data for a wide range of atmospheric land and oceanic climate variables in addition to era5 streamflow data were collected from the united states geological survey usgs and the nebraska department of natural resources nednr the variables considered in this study include wind speed components dew point temperature temperature skin temperature boundary layer height convective available potential energy actual evaporation leaf area index snow albedo snow depth soil temperature streamflow surface solar and thermal radiation surface pressure surface sensible heat flux total column water total precipitation and volumetric soil water the gridded era5 variables were lumped over the basin extent to form a time series the variables listed in the study are relevant in predicting streamflow because they all play a role in the hydrological cycle and understanding these variables and their interactions is crucial in predicting streamflow for example precipitation evaporation and snow elements in the hydrological cycle affect the amount of water available for streamflow similarly temperature and soil related variables affect the evaporation and infiltration rate which affects the streamflow the wind components affect the evaporation rate and the transport of moisture which are important factors in the formation of precipitation the selection of the datasets for this study was motivated by reviewing previous studies in streamflow prediction dalkiliç and hashimi 2020 damavandi et al 2019 ha et al 2021 hadi and tombul 2018 ni et al 2020 pellicciotti et al 2010 peng et al 2017 samit thapa zebin zhao bo li 2020 shortridge et al 2016 tongal and booij 2018 3 methodology 3 1 long short term memory lstm long short term memory lstm networks are a type of recurrent neural network rnn that are capable of learning both short term and long term dependencies as originally introduced by hochreiter and schmidhuber 1997 lstm networks were developed to address the problem of vanishing and exploding gradients in traditional rnns allowing them to effectively model sequences with long time lags the architecture of an lstm network is shown in fig 2 and consists of three main gate mechanisms the forget gate the input gate and the output gate the forget gate determines which pieces of the long term memory should be discarded based on the previous hidden state and the current input data the input gate determines what new information should be added to the network s long term memory given the previous hidden state and current input data the output gate determines the next hidden state and is used to make final predictions for a more detailed explanation of lstm networks see hrnjica mehr 2020 the ability of a deep learning model such as an lstm network to effectively learn and make predictions is highly dependent on the appropriate selection of hyperparameters these hyperparameters including the number of nodes or neurons in the network the dropout rate the activation function the optimization algorithm the learning rate and the loss function can significantly impact the model s performance in this study we implemented a network architecture comprising three stacked lstm layers including an input layer and two hidden layers the input layer contained 80 neurons while the hidden layers contained 50 and 10 neurons respectively the output layer was composed of a single neuron we used relu activation function for all the stacked layers of lstm and linear for the output layer to prevent overfitting a dropout rate of 30 0 3 was applied before and after the final hidden layer overfitting occurs when a model fits too closely to the training data and is unable to generalize to new datasets the use of dropout layers which randomly ignore a subset of neurons during training can help mitigate this issue we used a lookback window of seven days for all independently trained lstm models training of lstm model in this study the models were trained using a mini batch training approach with a batch size of 50 and an epoch size of 200 the adam optimization algorithm was used for training which combines the momentum and root mean square propagation rmsp gradient descent techniques and is known to be effective for a wide range of deep learning models including lstms the mean square error mse was used as the loss function for the standalone lstm model for models that included physics based constraints the mse loss function was modified as described in later sections to ensure the input features including meteorological variables and discharge were properly scaled the lecun et al 2012 and minns hall 2009 normalization method was applied which involves subtracting the mean and dividing by the standard deviation the mean and standard deviation were calculated using only the training period data the normalized output of the network was then transformed using the normalization parameters from the training period to produce the final discharge predictions 3 2 physics based loss function for prediction of streamflow the loss function is a key component of any neural network as it measures the error between the network s predictions and the true values mean squared error mse is a widely used loss function for regression problems in machine learning and it is calculated as the mean of the squared differences between the actual values y true and the predicted values y pred over a dataset of size n m s e 1 n i 1 n y t r u e y p r e d 2 where mse mean square error n number of data points y true actual true values y pred predicted values in this study mse was employed as the loss function for the standalone lstm model however in order to incorporate physics based constraints into the model the loss function was modified as follows l o s s l o s s t r n y t r u e y p r e d λ l o s s p h y y p r e d where the training loss l o s s t r n measures a supervised error such as mse between true y t r u e and predicted labels y p r e d the hyperparameter λ is a penalty factor that controls the relative importance of the physical constraint term l phy in the overall loss function by adjusting λ appropriately it is possible to enforce consistency with known physical laws in the model s predictions 3 2 1 conservation of mass water balance wb conservation of mass also known as the law of mass balance dictates that the inflow to a system must be equal to the outflow plus any changes in storage within the system in the context of a watershed water is received through precipitation and snowmelt and is lost through processes such as evaporation streamflow and groundwater recharge the difference between the amount of water received and lost determines the amount of water stored within the basin δs p e q gw where δs is the change in storage p is precipitation q is runoff e is evaporation and gw is net groundwater flows out of the catchment to apply the conservation of mass principle to a watershed it is necessary to consider the physical variables that govern the hydrological cycle including precipitation surface runoff groundwater infiltration evaporation and transpiration by assuming that the watershed is a closed system it is possible to use the conservation of mass principle to track the amount of water stored within the system in this study we developed two approaches based on the conservation of mass principle which are described in further detail below approach 1 in our first approach we incorporated a physical constraint into the loss function of the lstm model to ensure consistency with the conservation of mass principle this principle dictates that the inflow to a system must be equal to the outflow plus any changes in storage within the system over a given time period in a watershed long term storage refers to the total amount of water that is stored over an extended period of time typically several months to several years to enforce this constraint we defined a condition such that the difference between the long term simulated and observed storage changes must be less than or equal to 10 of the observed storage change simulated storage is calculated by subtracting from the total precipitation the sum over the entire data length of evaporation and simulated discharge in the training dataset whereas observed storage is determined by subtracting from the total precipitation the sum of evaporation and actual observed discharge in the same dataset due to the complexity and high variability of hydrological systems a 10 difference between simulated and observed storage changes is considered acceptable in many applications safeeq et al 2021 in addition measurement errors and uncertainties in input data can also result in differences between simulated and observed results the 10 difference implies that the model is able to reasonably approximate the observed storage changes if this condition is met the regular mse loss function is used however if the condition is not met the product of the absolute difference between the simulated and observed storage changes and a weight factor is added to the loss function to penalize the model loss weight absolute difference between the simulated and observed change in storage mse this approach allows us to ensure that the lstm model adheres to the principle of mass conservation in its predictions of streamflow approach 2 in the second approach we introduced a physical constraint that limits the change in the simulated long term storage to be less than 1 of total precipitation to ensure a water balance closure the simulated storage is calculated by subtracting from the total precipitation the sum of evaporation and simulated discharge in the training dataset a change in simulated storage of less than 1 of total precipitation can be considered an acceptable level of precision for many hydrological applications this is because precipitation is a major input in the water balance equation and small changes in precipitation can result in large changes in water storage for example if the total precipitation is 1000 mm a change in simulated storage of less than 1 would be less than 10 mm this small change in storage is a relatively small margin of error and suggests that the model is able to reasonably ensure a water balance closure if the condition is satisfied the standard mse loss function is applied on the other hand if the condition is not met the loss function is modified to include a penalty term which is calculated by multiplying the absolute change in simulated storage by a weight factor and adding it to the mse loss the resulting loss function can be expressed as loss weight absolute change in simulated storage mse this approach utilizes the principle that the net change in storage for long time periods in a watershed tends to be close to zero and aims to enforce this constraint through the use of the penalty term in the loss function 3 2 2 energy balance eb the principle of energy conservation dictates that energy cannot be created nor destroyed in the context of a watershed this implies that net radiation r n balance among incoming shortwave outgoing shortwave incoming longwave and outgoing longwave radiations either increases the temperature sensible heat h or causes phase change latent heat l e or is lost from the system this can be represented as r n h l e l o s s where an example of l o s s would be the energy lost due to ground heat flux the ground heat flux term tends to be insignificant over a 24 h period so it can be ignored in the loss function formulation thus the energy balance equation becomes r n h l e in the water balance equation the incoming component is precipitation and the outgoing component is evaporation and streamflow evaporation serves as the link between the water and energy balance to ensure consistency with the principle of energy conservation we formulated a loss function equating the difference between net radiation and sensible heat to latent heat total evaporation with a tolerance of 1 of the net radiation sum we assumed a tolerance of 1 of the sum of net radiation because energy balance is complex and is dependent on many factors such as temperature pressure radiation and small changes in these factors can result in large changes in the energy balance therefore a small change in the energy balance suggests that the model is able to accurately capture the energy balance of the system additionally 1 tolerance allows some of the model uncertainties and is not either overly under conservative if this condition is met the normal mse loss function is used however if the condition is not met the model is penalized by taking the absolute difference in incoming and outgoing energy multiplying it by a weight and adding it to the mse the resulting loss function is as follows loss weight absolute difference in energy mse 3 2 3 storage discharge sd this constraint is based on the storage discharge relationship which is given as s k q where s is the storage q is the discharge and k is the storage time constant the calculation of k for each time step involves determining the ratio of the simulated and observed storage values to the corresponding simulated and observed discharge values the final k value for the entire watershed is obtained by averaging the k values across all time steps the difference between the average simulated k ksim and the average observed k kobs is calculated and is then divided by the kobs to capture the relative change in the ratio to incorporate this physical constraint into the loss function of our model we added a condition such that the ratio should be less than or equal to 1 a small difference in the storage time constant suggests that the model can accurately capture the dynamics of the system and simulate the observed storage changes if this condition is met the normal mse loss function is used otherwise we penalized the model by taking the absolute change in this ratio multiplying it by a weight and adding the resulting value to the mse loss the modified loss function can be expressed as loss weight ksim kobs kobs mse 3 3 evaluation metrics the nash sutcliffe efficiency nse nash and sutcliffe 1970 is a measure of the accuracy and efficiency of a model in predicting variables that differ from the mean it represents the proportion of the original variance accounted for by the model and is calculated as follows n s e 1 t 1 t q m t q o t t 1 t q o t q o where q o t and q m t are observed and modeled discharges at time t q o is the mean of observed discharges the nse can range from to 1 with a value of 1 indicating perfect correspondence between simulation and observation a value of 0 indicating that the model simulation has the same explanatory power as the mean of the observations and a value of less than 0 indicating that the model is a worse predictor than the mean of the observations 4 results and discussion in this study the predictability performance of standalone lstm models and lstm models with the addition of physics based constraints in the loss function was evaluated for one day ahead streamflow prediction at 34 basins the model predictions were used to evaluate the model s ability to simulate streamflow data during the testing phase fig 3 shows the prediction performance of standalone lstm models and lstm models with physics based constraints in the loss function for simplicity lstm is represented as lstm water balance approach 1 is represented as w1 water balance approach 2 is represented as w2 energy balance is represented as e and storage discharge is represented as sd the prediction efficiency of standalone lstm models at all basins was found to be between 0 16 nse 0 95 out of the 34 basins nine basins had low prediction efficiency for streamflow with an nse value of less than 0 5 to examine the effect of physics based constraints on prediction accuracy we developed 15 lstm models each incorporating a single physical constraint or a combination of physics based constraints the impact of adding physics based constraints is seen to be minimal where there is already good performance from the lstm model for example in big papillion mosquito the nse is 0 95 from the lstm model indicating less room for improvement in prediction the results of our analysis showed that the impact of physics based constraints on prediction accuracy varied among the different basins studied fig 3 in some cases the incorporation of physics based constraints resulted in an improvement in prediction accuracy while in other cases it had no effect or even decreased prediction accuracy for example the incorporation of the w1 constraint improved prediction accuracy for the red willow basin by 43 but had no effect on the calamus basin in contrast the imposition of the w2 constraint improved prediction accuracy for the red willow basin by 18 but decreased prediction accuracy by 15 for the little nemaha basin and had no effect on the calamus basin the incorporation of the e constraint decreased prediction accuracy for the red willow and little nemaha basins by 162 and 14 respectively however the incorporation of the e constraint improved prediction accuracy for the west fork big blue basin by 20 additionally the imposition of the sd constraint improved prediction accuracy in some basins such as lower niobrara but had no effect on the majority of the basins studied in our analysis of 34 basins the incorporation of the w1 constraint improved prediction efficiency for 38 of the basins with an average increase in efficiency of 9 additionally the combination of w1 with e and w2 improved prediction efficiency for 41 and 38 of the basins respectively with average increases in efficiency of 7 and 6 the incorporation of the w2 constraint improved prediction efficiency for 32 of the basins with an average increase in efficiency of 7 the combination of w2 with e and w1 increased the percentage of basins with improved prediction efficiency from 32 to 41 for both combinations with average increases in efficiency of 9 and 6 respectively it was observed that for a subset of catchments the imposition of constraints on the lstm model resulted in minimal or no effect on model efficiency specifically in 15 of catchments the imposition of w2 in combination with w1 and e constraints did not yield a statistically significant change in efficiency as measured by the nse metric furthermore in 9 of catchments the imposition of w1 in combination with w2 and e constraints w1 and sd constraints w1 and w2 constraints and w2 e and sd constraints resulted in a similar lack of change in nse notably the imposition of sd constraints as well as combinations of sd constraints with w2 resulted in zero percent catchment with no effect the incorporation of the sd constraint improved prediction efficiency for 12 of the basins with an average increase in efficiency of 5 the combination of sd with w1 w2 and e increased the number of basins with improved prediction efficiency with the percentage increasing from 12 to 26 for combinations with w1 and w2 and 29 for combinations with e overall these results suggest that multiple physics based constraints can be used separately or in combinations depending on the characteristics and behaviors of the particular catchment in order to improve prediction efficiency fig 4 our results showed that the incorporation of the sd constraint decreased prediction efficiency for 88 of the basins with an average decrease in efficiency of 60 additionally the combination of sd with other physics based constraints was found to decrease the efficiency of those constraints in improving prediction accuracy for example in the cedar basin the incorporation of the w1 constraint alone improved prediction efficiency by 22 however when combined with the sd constraint the improvement in efficiency was reduced to only 8 indicating a 14 decrease these findings suggest that caution should be exercised when implementing the sd constraint in streamflow prediction models 5 conclusions in this study we investigated the effects of incorporating physics based constraints including water balance energy balance and storage discharge relationship into lstm for predicting daily streamflow in 34 huc 8 basins in nebraska the performance of the model was thoroughly assessed for cases with and without the inclusion of these constraints and their combinations our results showed that the inclusion of certain physics based constraints can lead to improved performance of the lstm model in streamflow prediction in particular the use of water balance constraints was found to have the most significant positive impact on model performance with an increase in efficiency for 38 of catchments additionally the combination of water balance w1 and energy balance constraints resulted in the greatest improvement among all constraint combinations examined with an increase in efficiency from 38 to 41 of catchments on the other hand the storage discharge constraint and its combinations displayed less consistent results and may be more relevant in catchments where flow is dominated by catchment characteristics such as topography soil types land use and hydrography these characteristics can affect the hydrological response of a catchment area such as the amount and timing of streamflow and are important to consider when modeling and predicting streamflow in a catchment it was also noted that the specific method of imposing constraints can affect model performance for example w1 improves efficiency for 38 of the catchment while w2 improves efficiency for 32 these findings highlight the potential value of using physics based constraints in machine learning models for streamflow prediction particularly in data scarce regions these findings suggest that the application of constraints on lstm models may not always result in a significant improvement across all catchments and that further research is needed to understand the conditions under which constraints are most effective additionally the results indicate that the specific constraints combination and their effect may vary depending on the catchment while it may not always result in a significant improvement in the overall performance the incorporation of physics based constraints will enhance the model s ability to capture underlying physical processes that govern streamflow which can potentially lead to the improved generalization capability of the model thus physics based constraints can increase the robustness of the model to unseen data and make it more representative of real world scenarios this is an important step towards physically relevant streamflow prediction by infusing physical laws into the lstm model via a simple but powerful alteration to the loss function this approach provides a good balance between capturing the watershed specific uniqueness through the training of lstm models for each catchment and incorporating generalizable knowledge across all catchments through the use of constraints beven 2014 troch et al 2015 in practice it is important to first test the efficacy of constraints for the problem at hand individual constraints and their combinations need to be thoroughly assessed before being used it is important to approach this process with a mindset of continuous evaluation and improvement because the effectiveness of constraining can change depending on the variables under consideration datasets used unique and generalizable characteristics of the catchments among others overall this study provides valuable insights into the use of lstm models for streamflow prediction and the potential benefits and limitations of imposing constraints on their loss function additionally this study highlights the importance of considering both the unique and generalizable characteristics of a catchment when imposing constraints on the lstm model while this study has focused on the use of these specific physics based constraints it is important to note that other forms of physical relationships can also be explored in lstm and other machine learning models for streamflow prediction finally it would be beneficial to dig deeper into the factors influencing the success or failure of different constraints and combinations of constraints to predict flow in different catchments this type of research can help identify common characteristics that affect the effectiveness of these constraints and their combinations in streamflow prediction this can open new possibilities for improving the performance and robustness of these models and make them more useful for predictions and decision making funding funding for this research was provided by the mid america transportation center via a grant from the u s department of transportation s university transportation centers program usdot utc grant number for matc 69a3551747107 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2023 105730 
25385,we investigate the effects of physics based constraints added to the loss function of a long short term memory lstm network on its performance in daily streamflow prediction three types of constraints mass balance energy balance and storage discharge relationship along with their combinations are tested across 34 river basins in nebraska we found that the addition of constraints improves the model performance in several basins but there are also cases where the performance drops or does not differ significantly mass and energy balance constraints improve the performance in 38 and 32 of catchments respectively while storage discharge constraints improve the performance in 12 of catchments the combination of mass and energy balance constraints has a positive effect on 41 of catchments while the combination of mass balance and storage discharge constraints improves the performance in 26 of catchments we recommend the use of constraints in cases where they boost the lstm performance keywords lstm physics based machine learning streamflow prediction data availability data will be made available on request 1 introduction accurate streamflow prediction is of critical importance for various sectors such as water resources planning and management agricultural water management disaster mitigation efforts etc dong et al 2020 gauch et al 2021b ghimire et al 2021 hunt et al 2022 as the demand for water resources continues to increase the focus on modeling streamflow has intensified with the proliferation of high resolution mapping and other remote sensing datasets together with the availability of datasets at high spatial and temporal resolutions we have seen a rise in novel methods for modeling streamflow across different scales machine learning ml models in particular have shown significant promise in this area with applications including flood forecasting castangia et al 2023 krajewski et al 2017 lin et al 2013 nevo et al 2021 yuan et al 2018 flood early warning systems moon et al 2019 pyayt et al 2014 sanders et al 2022 flood inundation chang et al 2018 kim and han 2020 tanoue et al 2016 flood prediction tongal and booij 2018 runoff prediction kratzert et al 2018 and streamflow prediction parisouj et al 2020 tongal and booij 2018 xiang and demir 2020 yaseen et al 2016 despite the advancement due to their restricted interpretability anderson and radić 2022 bhasme et al 2021 gilpin et al 2019 reichstein et al 2019 physical inconsistencies karpatne et al 2017 willard et al 2020 and enduring problems with equifinality troch et al 2015 ml algorithms are often criticized by domain experts long short term memory lstm networks hochreiter and schmidhuber 1997 have been widely implemented in the field of hydrology of late these networks can store and regulate information over time gauch et al 2021a hochreiter and schmidhuber 1997 kratzert et al 2019 yang et al 2023 which makes them particularly suitable for simulating the memory effects of different types of hydrological variables while traditional recurrent neural networks rnns are limited in their ability to remember long term sequences in modeling time dependent and sequential data bengio et al 1994 lstms can leverage information from both short term and long term dependencies which is advantageous for hydrologic applications like streamflow prediction khandelwal et al 2020 kratzert et al 2018 majeske et al 2022 song et al 2020 currently there have been efforts to combine physical understanding with ml models to improve their performance and interpretability daw et al 2017 2019 hoedt et al 2021 karpatne et al 2017 young et al 2017 for example to estimate lake temperature daw et al 2017 presented a physics guided neural network pgnn model which fuses a physics based model with a neural network they established a framework for developing neural networks that utilize physical equations and principles in which a crucial link between the temperature density and depth of water is incorporated into the physics based loss function of the lstm model jia et al 2019 extended this work by including a physics based penalty for energy conservation in the loss function to ensure consistency between the lake thermal energy gain over time and the net thermodynamic fluxes into and out of the lake thereby capturing more complex and general physical relationships it is our understanding that there has been limited research on adding physics based constraints into the loss function of the ml model and what type of constraints to include for streamflow prediction incorporating physics based constraints such as water balance energy balance and storage discharge relationship into the loss function of a ml model for streamflow prediction is important because it allows the model to consider real world physical processes that affect the amount of water available for streamflow these constraints play a vital role in accurately predicting streamflow by considering the interactions between various components of the hydrological system such as precipitation evaporation and storage understanding these elements is essential for effective streamflow prediction since they all influence the amount of water that is available for streamflow however there is a lack of research on the specific types of physics based constraints that should be included in the loss function of a machine learning model for streamflow prediction in this study we present a framework for modeling streamflow in 34 basins in nebraska using high resolution datasets that incorporates three types of physics based constraints water balance energy balance and storage discharge relationship into the loss function of an lstm model we evaluate the effectiveness of these constraints and examine their individual and combined effects on streamflow prediction by independently training the lstm model for each catchment we aim to provide a new approach for combining machine learning and physics based constraints in streamflow prediction that can contribute to the field of hydrology 2 study area and data 2 1 study area the study area for this research was the hydrologic unit code huc 8 basins of nebraska out of the 72 huc 8 basins in the state 34 were selected for this analysis based on the availability of stream gage data for at least 20 years the locations of the selected huc 8 basins within nebraska are shown in fig 1 the selected basins represent a range of geographies and hydrological conditions within the state stream gage data for these basins were used as the primary source of observations for the study basins that did not have a gage station at the outlet or had limited years of data were excluded from the analysis this study aims to provide a framework for improving streamflow prediction in these basins by incorporating physics based constraints into the loss function of lstm 2 2 data the atmospheric and land data for this study was obtained from the era5 dataset hersbach et al 2020 which is the fifth generation atmospheric reanalysis from the european centre for medium range weather forecasts ecmwf era5 provides global coverage on a 30 km grid with 137 levels of atmospheric resolution from the ground up to 80 km in altitude and includes hourly data for a wide range of atmospheric land and oceanic climate variables in addition to era5 streamflow data were collected from the united states geological survey usgs and the nebraska department of natural resources nednr the variables considered in this study include wind speed components dew point temperature temperature skin temperature boundary layer height convective available potential energy actual evaporation leaf area index snow albedo snow depth soil temperature streamflow surface solar and thermal radiation surface pressure surface sensible heat flux total column water total precipitation and volumetric soil water the gridded era5 variables were lumped over the basin extent to form a time series the variables listed in the study are relevant in predicting streamflow because they all play a role in the hydrological cycle and understanding these variables and their interactions is crucial in predicting streamflow for example precipitation evaporation and snow elements in the hydrological cycle affect the amount of water available for streamflow similarly temperature and soil related variables affect the evaporation and infiltration rate which affects the streamflow the wind components affect the evaporation rate and the transport of moisture which are important factors in the formation of precipitation the selection of the datasets for this study was motivated by reviewing previous studies in streamflow prediction dalkiliç and hashimi 2020 damavandi et al 2019 ha et al 2021 hadi and tombul 2018 ni et al 2020 pellicciotti et al 2010 peng et al 2017 samit thapa zebin zhao bo li 2020 shortridge et al 2016 tongal and booij 2018 3 methodology 3 1 long short term memory lstm long short term memory lstm networks are a type of recurrent neural network rnn that are capable of learning both short term and long term dependencies as originally introduced by hochreiter and schmidhuber 1997 lstm networks were developed to address the problem of vanishing and exploding gradients in traditional rnns allowing them to effectively model sequences with long time lags the architecture of an lstm network is shown in fig 2 and consists of three main gate mechanisms the forget gate the input gate and the output gate the forget gate determines which pieces of the long term memory should be discarded based on the previous hidden state and the current input data the input gate determines what new information should be added to the network s long term memory given the previous hidden state and current input data the output gate determines the next hidden state and is used to make final predictions for a more detailed explanation of lstm networks see hrnjica mehr 2020 the ability of a deep learning model such as an lstm network to effectively learn and make predictions is highly dependent on the appropriate selection of hyperparameters these hyperparameters including the number of nodes or neurons in the network the dropout rate the activation function the optimization algorithm the learning rate and the loss function can significantly impact the model s performance in this study we implemented a network architecture comprising three stacked lstm layers including an input layer and two hidden layers the input layer contained 80 neurons while the hidden layers contained 50 and 10 neurons respectively the output layer was composed of a single neuron we used relu activation function for all the stacked layers of lstm and linear for the output layer to prevent overfitting a dropout rate of 30 0 3 was applied before and after the final hidden layer overfitting occurs when a model fits too closely to the training data and is unable to generalize to new datasets the use of dropout layers which randomly ignore a subset of neurons during training can help mitigate this issue we used a lookback window of seven days for all independently trained lstm models training of lstm model in this study the models were trained using a mini batch training approach with a batch size of 50 and an epoch size of 200 the adam optimization algorithm was used for training which combines the momentum and root mean square propagation rmsp gradient descent techniques and is known to be effective for a wide range of deep learning models including lstms the mean square error mse was used as the loss function for the standalone lstm model for models that included physics based constraints the mse loss function was modified as described in later sections to ensure the input features including meteorological variables and discharge were properly scaled the lecun et al 2012 and minns hall 2009 normalization method was applied which involves subtracting the mean and dividing by the standard deviation the mean and standard deviation were calculated using only the training period data the normalized output of the network was then transformed using the normalization parameters from the training period to produce the final discharge predictions 3 2 physics based loss function for prediction of streamflow the loss function is a key component of any neural network as it measures the error between the network s predictions and the true values mean squared error mse is a widely used loss function for regression problems in machine learning and it is calculated as the mean of the squared differences between the actual values y true and the predicted values y pred over a dataset of size n m s e 1 n i 1 n y t r u e y p r e d 2 where mse mean square error n number of data points y true actual true values y pred predicted values in this study mse was employed as the loss function for the standalone lstm model however in order to incorporate physics based constraints into the model the loss function was modified as follows l o s s l o s s t r n y t r u e y p r e d λ l o s s p h y y p r e d where the training loss l o s s t r n measures a supervised error such as mse between true y t r u e and predicted labels y p r e d the hyperparameter λ is a penalty factor that controls the relative importance of the physical constraint term l phy in the overall loss function by adjusting λ appropriately it is possible to enforce consistency with known physical laws in the model s predictions 3 2 1 conservation of mass water balance wb conservation of mass also known as the law of mass balance dictates that the inflow to a system must be equal to the outflow plus any changes in storage within the system in the context of a watershed water is received through precipitation and snowmelt and is lost through processes such as evaporation streamflow and groundwater recharge the difference between the amount of water received and lost determines the amount of water stored within the basin δs p e q gw where δs is the change in storage p is precipitation q is runoff e is evaporation and gw is net groundwater flows out of the catchment to apply the conservation of mass principle to a watershed it is necessary to consider the physical variables that govern the hydrological cycle including precipitation surface runoff groundwater infiltration evaporation and transpiration by assuming that the watershed is a closed system it is possible to use the conservation of mass principle to track the amount of water stored within the system in this study we developed two approaches based on the conservation of mass principle which are described in further detail below approach 1 in our first approach we incorporated a physical constraint into the loss function of the lstm model to ensure consistency with the conservation of mass principle this principle dictates that the inflow to a system must be equal to the outflow plus any changes in storage within the system over a given time period in a watershed long term storage refers to the total amount of water that is stored over an extended period of time typically several months to several years to enforce this constraint we defined a condition such that the difference between the long term simulated and observed storage changes must be less than or equal to 10 of the observed storage change simulated storage is calculated by subtracting from the total precipitation the sum over the entire data length of evaporation and simulated discharge in the training dataset whereas observed storage is determined by subtracting from the total precipitation the sum of evaporation and actual observed discharge in the same dataset due to the complexity and high variability of hydrological systems a 10 difference between simulated and observed storage changes is considered acceptable in many applications safeeq et al 2021 in addition measurement errors and uncertainties in input data can also result in differences between simulated and observed results the 10 difference implies that the model is able to reasonably approximate the observed storage changes if this condition is met the regular mse loss function is used however if the condition is not met the product of the absolute difference between the simulated and observed storage changes and a weight factor is added to the loss function to penalize the model loss weight absolute difference between the simulated and observed change in storage mse this approach allows us to ensure that the lstm model adheres to the principle of mass conservation in its predictions of streamflow approach 2 in the second approach we introduced a physical constraint that limits the change in the simulated long term storage to be less than 1 of total precipitation to ensure a water balance closure the simulated storage is calculated by subtracting from the total precipitation the sum of evaporation and simulated discharge in the training dataset a change in simulated storage of less than 1 of total precipitation can be considered an acceptable level of precision for many hydrological applications this is because precipitation is a major input in the water balance equation and small changes in precipitation can result in large changes in water storage for example if the total precipitation is 1000 mm a change in simulated storage of less than 1 would be less than 10 mm this small change in storage is a relatively small margin of error and suggests that the model is able to reasonably ensure a water balance closure if the condition is satisfied the standard mse loss function is applied on the other hand if the condition is not met the loss function is modified to include a penalty term which is calculated by multiplying the absolute change in simulated storage by a weight factor and adding it to the mse loss the resulting loss function can be expressed as loss weight absolute change in simulated storage mse this approach utilizes the principle that the net change in storage for long time periods in a watershed tends to be close to zero and aims to enforce this constraint through the use of the penalty term in the loss function 3 2 2 energy balance eb the principle of energy conservation dictates that energy cannot be created nor destroyed in the context of a watershed this implies that net radiation r n balance among incoming shortwave outgoing shortwave incoming longwave and outgoing longwave radiations either increases the temperature sensible heat h or causes phase change latent heat l e or is lost from the system this can be represented as r n h l e l o s s where an example of l o s s would be the energy lost due to ground heat flux the ground heat flux term tends to be insignificant over a 24 h period so it can be ignored in the loss function formulation thus the energy balance equation becomes r n h l e in the water balance equation the incoming component is precipitation and the outgoing component is evaporation and streamflow evaporation serves as the link between the water and energy balance to ensure consistency with the principle of energy conservation we formulated a loss function equating the difference between net radiation and sensible heat to latent heat total evaporation with a tolerance of 1 of the net radiation sum we assumed a tolerance of 1 of the sum of net radiation because energy balance is complex and is dependent on many factors such as temperature pressure radiation and small changes in these factors can result in large changes in the energy balance therefore a small change in the energy balance suggests that the model is able to accurately capture the energy balance of the system additionally 1 tolerance allows some of the model uncertainties and is not either overly under conservative if this condition is met the normal mse loss function is used however if the condition is not met the model is penalized by taking the absolute difference in incoming and outgoing energy multiplying it by a weight and adding it to the mse the resulting loss function is as follows loss weight absolute difference in energy mse 3 2 3 storage discharge sd this constraint is based on the storage discharge relationship which is given as s k q where s is the storage q is the discharge and k is the storage time constant the calculation of k for each time step involves determining the ratio of the simulated and observed storage values to the corresponding simulated and observed discharge values the final k value for the entire watershed is obtained by averaging the k values across all time steps the difference between the average simulated k ksim and the average observed k kobs is calculated and is then divided by the kobs to capture the relative change in the ratio to incorporate this physical constraint into the loss function of our model we added a condition such that the ratio should be less than or equal to 1 a small difference in the storage time constant suggests that the model can accurately capture the dynamics of the system and simulate the observed storage changes if this condition is met the normal mse loss function is used otherwise we penalized the model by taking the absolute change in this ratio multiplying it by a weight and adding the resulting value to the mse loss the modified loss function can be expressed as loss weight ksim kobs kobs mse 3 3 evaluation metrics the nash sutcliffe efficiency nse nash and sutcliffe 1970 is a measure of the accuracy and efficiency of a model in predicting variables that differ from the mean it represents the proportion of the original variance accounted for by the model and is calculated as follows n s e 1 t 1 t q m t q o t t 1 t q o t q o where q o t and q m t are observed and modeled discharges at time t q o is the mean of observed discharges the nse can range from to 1 with a value of 1 indicating perfect correspondence between simulation and observation a value of 0 indicating that the model simulation has the same explanatory power as the mean of the observations and a value of less than 0 indicating that the model is a worse predictor than the mean of the observations 4 results and discussion in this study the predictability performance of standalone lstm models and lstm models with the addition of physics based constraints in the loss function was evaluated for one day ahead streamflow prediction at 34 basins the model predictions were used to evaluate the model s ability to simulate streamflow data during the testing phase fig 3 shows the prediction performance of standalone lstm models and lstm models with physics based constraints in the loss function for simplicity lstm is represented as lstm water balance approach 1 is represented as w1 water balance approach 2 is represented as w2 energy balance is represented as e and storage discharge is represented as sd the prediction efficiency of standalone lstm models at all basins was found to be between 0 16 nse 0 95 out of the 34 basins nine basins had low prediction efficiency for streamflow with an nse value of less than 0 5 to examine the effect of physics based constraints on prediction accuracy we developed 15 lstm models each incorporating a single physical constraint or a combination of physics based constraints the impact of adding physics based constraints is seen to be minimal where there is already good performance from the lstm model for example in big papillion mosquito the nse is 0 95 from the lstm model indicating less room for improvement in prediction the results of our analysis showed that the impact of physics based constraints on prediction accuracy varied among the different basins studied fig 3 in some cases the incorporation of physics based constraints resulted in an improvement in prediction accuracy while in other cases it had no effect or even decreased prediction accuracy for example the incorporation of the w1 constraint improved prediction accuracy for the red willow basin by 43 but had no effect on the calamus basin in contrast the imposition of the w2 constraint improved prediction accuracy for the red willow basin by 18 but decreased prediction accuracy by 15 for the little nemaha basin and had no effect on the calamus basin the incorporation of the e constraint decreased prediction accuracy for the red willow and little nemaha basins by 162 and 14 respectively however the incorporation of the e constraint improved prediction accuracy for the west fork big blue basin by 20 additionally the imposition of the sd constraint improved prediction accuracy in some basins such as lower niobrara but had no effect on the majority of the basins studied in our analysis of 34 basins the incorporation of the w1 constraint improved prediction efficiency for 38 of the basins with an average increase in efficiency of 9 additionally the combination of w1 with e and w2 improved prediction efficiency for 41 and 38 of the basins respectively with average increases in efficiency of 7 and 6 the incorporation of the w2 constraint improved prediction efficiency for 32 of the basins with an average increase in efficiency of 7 the combination of w2 with e and w1 increased the percentage of basins with improved prediction efficiency from 32 to 41 for both combinations with average increases in efficiency of 9 and 6 respectively it was observed that for a subset of catchments the imposition of constraints on the lstm model resulted in minimal or no effect on model efficiency specifically in 15 of catchments the imposition of w2 in combination with w1 and e constraints did not yield a statistically significant change in efficiency as measured by the nse metric furthermore in 9 of catchments the imposition of w1 in combination with w2 and e constraints w1 and sd constraints w1 and w2 constraints and w2 e and sd constraints resulted in a similar lack of change in nse notably the imposition of sd constraints as well as combinations of sd constraints with w2 resulted in zero percent catchment with no effect the incorporation of the sd constraint improved prediction efficiency for 12 of the basins with an average increase in efficiency of 5 the combination of sd with w1 w2 and e increased the number of basins with improved prediction efficiency with the percentage increasing from 12 to 26 for combinations with w1 and w2 and 29 for combinations with e overall these results suggest that multiple physics based constraints can be used separately or in combinations depending on the characteristics and behaviors of the particular catchment in order to improve prediction efficiency fig 4 our results showed that the incorporation of the sd constraint decreased prediction efficiency for 88 of the basins with an average decrease in efficiency of 60 additionally the combination of sd with other physics based constraints was found to decrease the efficiency of those constraints in improving prediction accuracy for example in the cedar basin the incorporation of the w1 constraint alone improved prediction efficiency by 22 however when combined with the sd constraint the improvement in efficiency was reduced to only 8 indicating a 14 decrease these findings suggest that caution should be exercised when implementing the sd constraint in streamflow prediction models 5 conclusions in this study we investigated the effects of incorporating physics based constraints including water balance energy balance and storage discharge relationship into lstm for predicting daily streamflow in 34 huc 8 basins in nebraska the performance of the model was thoroughly assessed for cases with and without the inclusion of these constraints and their combinations our results showed that the inclusion of certain physics based constraints can lead to improved performance of the lstm model in streamflow prediction in particular the use of water balance constraints was found to have the most significant positive impact on model performance with an increase in efficiency for 38 of catchments additionally the combination of water balance w1 and energy balance constraints resulted in the greatest improvement among all constraint combinations examined with an increase in efficiency from 38 to 41 of catchments on the other hand the storage discharge constraint and its combinations displayed less consistent results and may be more relevant in catchments where flow is dominated by catchment characteristics such as topography soil types land use and hydrography these characteristics can affect the hydrological response of a catchment area such as the amount and timing of streamflow and are important to consider when modeling and predicting streamflow in a catchment it was also noted that the specific method of imposing constraints can affect model performance for example w1 improves efficiency for 38 of the catchment while w2 improves efficiency for 32 these findings highlight the potential value of using physics based constraints in machine learning models for streamflow prediction particularly in data scarce regions these findings suggest that the application of constraints on lstm models may not always result in a significant improvement across all catchments and that further research is needed to understand the conditions under which constraints are most effective additionally the results indicate that the specific constraints combination and their effect may vary depending on the catchment while it may not always result in a significant improvement in the overall performance the incorporation of physics based constraints will enhance the model s ability to capture underlying physical processes that govern streamflow which can potentially lead to the improved generalization capability of the model thus physics based constraints can increase the robustness of the model to unseen data and make it more representative of real world scenarios this is an important step towards physically relevant streamflow prediction by infusing physical laws into the lstm model via a simple but powerful alteration to the loss function this approach provides a good balance between capturing the watershed specific uniqueness through the training of lstm models for each catchment and incorporating generalizable knowledge across all catchments through the use of constraints beven 2014 troch et al 2015 in practice it is important to first test the efficacy of constraints for the problem at hand individual constraints and their combinations need to be thoroughly assessed before being used it is important to approach this process with a mindset of continuous evaluation and improvement because the effectiveness of constraining can change depending on the variables under consideration datasets used unique and generalizable characteristics of the catchments among others overall this study provides valuable insights into the use of lstm models for streamflow prediction and the potential benefits and limitations of imposing constraints on their loss function additionally this study highlights the importance of considering both the unique and generalizable characteristics of a catchment when imposing constraints on the lstm model while this study has focused on the use of these specific physics based constraints it is important to note that other forms of physical relationships can also be explored in lstm and other machine learning models for streamflow prediction finally it would be beneficial to dig deeper into the factors influencing the success or failure of different constraints and combinations of constraints to predict flow in different catchments this type of research can help identify common characteristics that affect the effectiveness of these constraints and their combinations in streamflow prediction this can open new possibilities for improving the performance and robustness of these models and make them more useful for predictions and decision making funding funding for this research was provided by the mid america transportation center via a grant from the u s department of transportation s university transportation centers program usdot utc grant number for matc 69a3551747107 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2023 105730 
25386,a major challenge in compound flooding simulation is representing small rivers in the model mesh a parallel python based tool is developed to support the construction of unstructured grids ugs at continental scales the tool is driven by digital elevation model dem ensuring accurate representation of geomorphic features in the resulting mesh its first component pydem extends an existing tool to detect river thalwegs from dems the second component rivermapper uses the thalwegs to generate river arcs which can be directly ingested into meshing tools e g the surface water modeling system the novelty lies in the explicit 2d representation of rivers in both along and cross channel directions making it ideal for accurate efficient and high resolution continental scale research the tool is employed to create a ug for a 3d creek to ocean model encompassing the us east and gulf coasts and it greatly improves the flow exchange between the watershed and the coastal zone keywords compound flooding coastal transition zone cross scale modeling unstructured mesh generation data availability i have shared the link to my code data in the manuscript software and data availability 1 python based tools pydem co developed by linlin cui lcui vims edu and zhengui wang wangzg vims edu and rivermapper developed by fei ye feiye vims edu both tools along with the user manual and sample applications are freely accessible from the schism development github repository https github com schism dev rivermeshtools the standalone pydem package originally developed by zhengui wang is also available at https github com wzhengui pydem 2 mesh generation tool sms v13 2 or newer commercial software developed and maintained by aquaveo aquaveo com with a community version freely available at https www aquaveo com software sms community the tool requires windows 10 and above 3 compound flood model schism open source community model developed by yinglong j zhang vims freely accessible at schism wiki v5 10 0 1 introduction the increasing probabilities of compound floods i e high water levels caused by multiple flood drivers under the projected climate change present an urgent need for comprehending this type of high risk events wahl et al 2015 bevacqua et al 2019 physical models represent a powerful means for analyzing the processes and predicting the extent of compound floods xu et al 2022 ideally a single modeling framework should be used to simulate the nonlinear interactions among flood drivers santiago collazo et al 2019 which include rainfall runoff and storm surge wahl et al 2015 bilskie and hagen 2018 as well as high tides jang and chang 2022 waves qiang et al 2021 off shore oceanic processes ezer 2018 and sea level rise moftakhari et al 2017 del rosal salido et al 2021 this modeling technique is termed full coupling if all relevant equations from hydrology hydraulics and hydrodynamics are solved simultaneously santiago collazo et al 2019 the study conducted by stephens et al 2022 at galveston bay demonstrated the superior accuracy of the full coupling approach in our previous studies huang et al 2021 ye et al 2021 and noaa 1 1 national oceanic and atmospheric administration u s department of commerce s operational forecast stofs 3d atlantic 2 2 stofs 3d atlantic 3 d surge and tide operational forecast system for the atlantic basin url https polar ncep noaa gov estofs last accessed apr 6 2023 we confirmed the feasibility of efficiently applying a full coupling approach at the continental scale by employing a single model to simulate hydrodynamic hydraulic and hydrologic processes excluding groundwater flow and evapotranspiration the computational cost was effectively mitigated by the model s implicit solver for the shallow water equation which enables the use of large time steps even on very high mesh resolutions zhang et al 2016 2020 ye et al 2020 given the geomorphic complexity of coastal transition zones the full coupling approach strongly favors unstructured grid ug models and the remaining challenges of large scale compound flood modeling are often associated with the complexity of generating an appropriate model mesh the importance of accurately representing river channels in the mesh is highlighted by both uncertainty analysis willis et al 2019 and model applications on specific events ye et al 2021 stephens et al 2022 as these channels serve as the main conduit for floodwater from all flood drivers while the use of raster based sub grid approach has become popular recently casulli and stelling 2011 costabile et al 2020 it faces difficulties in representing defense structures such as levees and dams zhang 2021 additionally although the finite volume based sub grid models can utilize ugs they require strict orthogonal ugs that are extremely challenging to generate for complex coastal topography and bathymetry zhang 2021 translating a model domain into a ug often requires automated and manual steps a mesh generator can discretize a given geometric entity based on certain algorithms such as sweep line methods delaunay voronoi methods and others explained by thompson et al 1998 examples of mesh generators for free surface environmental and geophysical flows include surface water modeling system sms 3 3 url https www aquaveo com software sms surface water modeling system introduction last accessed apr 6 2023 ocsmesh mani et al 2022 mike mesh builder 4 4 url https www mikepoweredbydhi com products mike cloud mesh builder last accessed apr 6 2023 oceanmesh2d roberts et al 2019 jigsaw engwirda 2017 deltares meshkernel 5 5 url https github com deltares meshkernel last accessed apr 6 2023 etc furthermore optimizing a ug for coastal applications often involves manually placing polygons or polylines that follow topographic and bathymetric features such as levees thalwegs and characteristic contours overall mesh generation for large scale compound flood modeling is an arduous undertaking given the geometric complexity of nearshore and watershed regions which are characterized by a multitude of fine scale geomorphic features despite the capability of ugs achieving high resolution of these features while ensuring good resolution for other critical features essential for flow routing pushes both the mesh generator and model to the limit as an illustration in the case study presented in section 3 the total number of polylines employed to guide the watershed mesh generation is approximately 350 000 furthermore optimizing the ug to reduce the mesh size for efficient model performance would inevitably require sharp transitions in mesh resolution in the end incorporating fine scale features into the unstructured grid ug requires a heavily constrained triangulation which can be challenging for many meshing tools recent studies have been exploring more efficient meshing procedures that are capable of automatic local refinements conroy et al 2012 araújo et al 2013 roberts et al 2019 bacopoulos and hagen 2022 mani et al 2022 these methods enable automatic control of the spatially varying mesh resolution using size functions which are based on the length scale of geomorphic features or physical phenomena however despite the popularity of size function based methods our experience suggests that they often produce excessively large meshes due to a preference for smooth transitions in mesh resolution in addition many of these tools lack the ability to implement anisotropic elements i e elements with different length scales in different directions an exception is the strategy proposed by legrand et al 2007 which was used to create anisotropic meshes with skew elements across a continental slope in this paper we propose a direct meshing approach for watershed rivers in which all essential features are explicitly specified using a python tool this approach is designed to provide greater control over the final mesh size and is entirely dem driven after mesh generation the bathy topo dems are linearly interpolated onto the resultant mesh without any subjective manipulations e g bathymetry smoothing on the grid depth consequently this approach directly links dem quality to model accuracy it also greatly reduces the turnaround time from new releases of bathy topo datasets to their integration into operational forecasts to facilitate continental scale applications the tool is parallelized using mpi4py version 3 1 3 the first component of the tool pydem finds the 1d thalweg network of river channels using the hydrological network methodology section 2 1 then the 1d thalweg network is expanded by the second tool component rivermapper to form a 2d representation of rivers the 2d representation consists of polylines cf fig 9 to guide the delineation of river channels such that all line segments and vertices should be exactly reproduced in the final mesh the polylines can be output as linestrings in the esri shapefile format for general uses with a user s preferred meshing tool or in this study as feature arcs in the sms map format the meshing tool sms is used for triangulation section 2 3 because it strictly preserves the user specified polylines feature arcs which usually represent distinctive features of a terrain such as shorelines thalwegs banks and the outlines of hydraulic structures at the same time it also allows for resolution relaxation where the topographic gradient is small such as in the non river portion of a watershed where pluvial floods may occur this helps maintain a reasonable mesh size thus improving simulation efficiency the resultant mesh can resolve very narrow channels often found in upstream rivers down to the native dem resolution to ensure channel continuity and unimpeded flow routing in section 3 the meshing methodology and its products are tested with a 3d creek to ocean cross scale model on a domain covering the us east and gulf coasts model results during hurricanes demonstrate that the new mesh can more accurately route the river flow than a previously generated mesh same as the one used in ye et al 2021 that heavily relied on manual editing of channels major findings from this study are summarized in section 4 2 methods the new tool consists of two major components both written in parallel python the first component generates a 1d network of river thalwegs from the dems the second component utilizes existing thalweg information and dems to detect riverbanks then places polylines river arcs based on bank positions the river arcs are directly imported to a mesh generator for example sms to guide the meshing for watershed rivers these steps are illustrated in fig 1 and expounded in detail below further information and sample applications can be found in the online documentation see software and data availability 2 1 thalweg detection by pydem in this section we use a spatial domain that covers the coastal zones along the us east and gulf coasts fig 2 to illustrate the thalweg detection procedure the primary dem source used is noaa national centers for environmental information ncei s ninth arc second resolution bathymetric topographic dem tiles cudem 6 6 url https chs coast noaa gov htdata raster2 elevation ncei ninth topobathy 2014 8483 last accessed apr 6 2023 augmented by the coastal relief model crm 7 7 url https www ncei noaa gov products coastal relief model last accessed apr 6 2023 that fills in gaps in a few regions where cudem tiles are missing there are a total of 754 cudem tiles with 8112 8112 raster cells per tile the original resolution of the crm dataset is three arc seconds 90 m and we interpolate crm into the ninth arc second resolution because the thalweg detection algorithm presented in this subsection requires uniform dem tiles that dovetail with each other the completed coverage consisting of 1135 dem tiles is displayed in fig 2 for the parallel algorithm to work we first locally augment each dem tile with its 8 neighboring tiles this is done to avoid gaps between the calculated thalwegs and the border of each tile after the thalwegs are calculated from the local 9 tile dems only the thalwegs within the center tile are used for merging extracting channel networks from a locally merged 9 tile dem consists of three steps 1 fill or breach depressions in the dems 2 calculate flow direction based on the given flow metric 3 calculate flow accumulation numbers which are the total number of flow cells passing through each raster cell and then apply a prescribed flow accumulation threshold to determine which cells should be included in the final digital stream network the details of the three steps are elaborated below 2 1 1 filling depression the algorithm used in this paper is alg 3 priority flood ε in barnes et al 2014 for completeness this algorithm is illustrated along a dem s cross section fig 3 basically it fills any local depressions cells 3 and 4 in fig 3 to its lowest outlet cell 2 in fig 3 two queues are used to track the cells being processed q1 is a priority queue where cells with lower elevations are placed towards the head left of the sorted list and will be popped out first and q2 is a first in first out queue for storing elevated cells dotted lines in fig 3 the procedure starts from inserting edge cells into q1 fig 3a while either q1 or q2 is not empty the algorithm pops out a cell from either queue prioritizing q2 as the source if it is not empty for example in fig 3a the cell to be popped is 6 from q1 because q2 is empty in fig 3d the cell to be popped is 3 from q2 then push the un processed neighbors if any of the popped cell to either q1 or q2 the destination queue is determined by comparing the neighbor s elevation to the modified elevation of the popped cell i e the elevation of the popped cell plus a small positive number ε the neighbor cell is pushed to q1 if its elevation is higher than the modified elevation e g fig 3a and b and fig 3b and c otherwise the neighbor cell is raised to the modified elevation then pushed to q2 e g fig 3c and d and fig 3d and e the algorithm terminates when both q1 and q2 are empty 2 1 2 calculating flow direction the d8 flow method o callaghan and mark 1984 is used to assign a flow direction from each cell to its steepest downslope neighbor there is only one such neighbor after the fill depression step the flow coordinate system is shown in fig 4 for example if the direction of steepest drop is to the north of the cell of interest blue its flow direction value would be 64 the output of d8 flow direction calculation is an integer array associated with each cell in addition to the indices shown in fig 4 there are two more special indices reserved for some special cases with 1 representing no dem data cells and 0 representing cells on the edges of the locally merged dem tile flowing outwards 2 1 3 calculating flow accumulation this process starts from the outlets cells on the boundary of a locally merged 9 tile dem where the flow direction is flagged as zero and then recursively searches upstream for large dems the regular recursion algorithm may fail because of the maximum recursion depth allowed in python therefore we implement a multi level recursive search when the number of search steps exceeds 100 the search will restart from the current location for another round of search and so on until the boundary is reached all channel segment information is saved along the search so that we can calculate the total accumulation number for any given raster cell the boundary cells 0 will have the largest accumulation number to avoid excessive details of the thalweg network we set an accumulation threshold beyond which we stop the search and thus remove all upstream cells from the final network therefore the flow accumulation threshold can be regarded as a parameter that determines the granularity of the thalweg network using cudem tiles along the georgia coast as an example thalwegs extracted with three different flow accumulation thresholds are compared in fig 5 for the purpose of compound flood simulation a threshold of 107 is too large because it only retains major rivers but not smaller tributaries that are also important in floodwater routing fig 5a on the other hand a threshold of 105 includes too many small streams that requires an excessive amount of mesh elements to resolve fig 5c in this study we applied a threshold of 106 to the whole domain of us east and gulf coasts to obtain a reasonably accurate representation of the river network fig 5b we do not attempt to give a recommended scope of the accumulation threshold because it is affected by 1 different configurations of dem tiles 2 the granularity required for a particular study or application and 3 diverse topographical features for instance mountainous areas may exhibit narrower river channels compared to coastal plains necessitating a smaller threshold if other conditions and requirements are similar as a result identifying the ideal threshold requires a case by case approach incorporating both trial and error iterations and visual evaluations the run time to process one augmented dem tile with 73008 73008 cells 2 3 gigabytes augmented with its 8 neighbors is about half an hour the processing is parallelized by distributing dem tiles across multiple compute cores allowing each core to handle more than one tile if necessary with the parallelization the run time for the whole domain of us east and gulf coasts is about 6 h using 20 computing nodes on w m s vortex cluster even though each node has twelve cores we could only use two cores per node as the large tile size requires large memory obviously using more cores can significantly reduce the time given the nearly perfect parallel scaling 2 2 explicit 2d river representation by rivermapper rivermapper takes existing 1d river segments and dem tiles as inputs and generates polylines that constitute a 2d representation of the rivers in the watershed it is worth noting that the input river segments are not restricted to the thalwegs generated in the previous step section 2 1 instead any reasonable approximation of the river thalwegs can be used as input such as the 1d channel network from an existing hydrological model or manually drawn polylines for local mesh improvements rivermapper can run either in serial mode or in parallel mode with an optional parallel driver 2 2 1 workflow the procedure of rivermapper s core routine make river map is illustrated in fig 6 a with each phase colored differently in the preparation phase the geometries curvature and direction of the input thalweg segments and the initial water elevation on all thalweg points are calculated in addition a pair of bank points are located for each thalweg point fig 6b which provides river width information for the next step in the second phase the information of river curvature and width obtained in the first phase is used to redistribute the thalweg points fig 6c and the bank positions are updated based on the new thalweg points additionally this phase involves correcting thalweg positions and recording the cross channel resolution at each thalweg point upon completion of this phase the final corrected thalwegs are obtained which are useful as a standalone product and are also utilized in subsequent steps in the third phase the bank positions are updated again which is necessary because the thalwegs have been modified depending on user specifications this phase may also apply final touch ups on bank positions e g fig 6d these optional edits can help generate a more visually appealing mesh but may not have a direct impact on model accuracy the fourth phase involves placing additional river polylines based on the final bank positions fig 6e and cleaning up the geometry to avoid crowded polyline vertices the spatially varying cross channel resolution is utilized to determine the acceptable level of crowding among vertices finally the output that contains all polylines for guiding the river meshing is produced the remaining part of this sub section provides further details on the crucial steps required to achieve high accuracy and efficiency for additional information readers can refer to the online documentation available in the github repository see software and data availability 2 2 1 1 efficient query of ground elevation during the execution of the tool the locations of thalweg points and bank points are frequently calculated and adjusted on a continental scale this can involve millions of points and thousands of dem tiles moreover the tiles can be from one or more products thus having different spatial coverages and resolutions consequently an efficient method for querying river points z values from dem tiles is essential for the overall computational efficiency in this tool the nearest neighbor interpolation is implemented by directly computing the corresponding 2d indices of the points in multiple dem tiles a more precise interpolation is not required based on our experience because the primary dem sources typically have a resolution of a few meters which is fine enough for delineating rivers secondly since a specific thalweg typically resides within a limited number of dem tiles grouping thalwegs by their parent tiles is preferable for large applications a parallel driver see section 2 2 2 is provided to automatically generate an optimized grouping where each group consists of a small subset of thalwegs and dem tiles these groups are then distributed across multiple compute cores which accelerates the search process and reduces memory consumption furthermore the code utilizes numpy harris et al 2020 vectorized operations when dealing with large vectors and arrays these operations are significantly faster than python s for loops and offer performance comparable to that of c or fortran programs 2 2 1 2 detecting bank positions bank positions are calculated multiple times in the workflow fig 6a because multiple steps depend on the estimated channel width which must be updated following any changes in the thalwegs given a thalweg point two bank edges where the mean water depth is 0 fig 6b on both sides of the thalweg are searched in the cross channel direction the search process is straightforward but determining the water depth requires knowing the time varying free water surface which is not trivial although the free surface can be assumed to be the same as the local mean sea level for nearshore regions this approximation is no longer valid for inland rivers where the riverbed is higher than the local sea level a number of options were considered to improve the approximation one potential solution is to interpolate the surface elevations from the river stage observations here we propose a simpler alternative that is more straightforward to implement as illustrated in fig 7 the domain is divided into three parts upland region transitional region and coastal region by two representative ground elevation values z u p l a n d and z c o a s t a l in the coastal zone a constant water level e g the mean sea level is assumed in the upland zone a constant water depth is assumed since the free water surface should be smoother than the underlying riverbed the ground elevation along each thalweg is smoothed with a 100 m moving window filter then the water surface is found by adding the assumed constant water depth to the smoothed thalweg elevation in the transitional zone the water level varies linearly from the coastal value to the upland value as 1 η t r a n s i t i o n a l z s z c o a s t a l z u p l a n d z c o a s t a l η u p l a n d z u p l a n d z s z u p l a n d z c o a s t a l η c o a s t a l with the symbols defined in fig 7 in this study we choose z u p l a n d 3 m navd 88 and z c o a s t a l 0 m navd 88 the choice of the constant water depth h in the upland region fig 7 is somewhat arbitrary from our experience a small water depth e g 1 0 m is preferred as it usually results in a clean delineation of river channels the resultant bank positions from this algorithm are generally satisfactory an example is shown in fig 9 additionally the margins of errors in the bank delineation can be partially accounted for by inner and outer arcs as described in section 2 2 1 5 2 2 1 3 optimizing thalweg points distribution the spatial resolution of the extracted thalwegs depends on the dem sources and the typical spacing between two adjacent thalweg points is 1 5 m this resolution is too fine for resolving rivers in the along channel direction in watershed rivers we recommend using quasi 1d elements which are anisotropic elements longer in the along channel direction and shorter in the cross channel direction compared to the size function based approach this quasi 1d river representation significantly reduces the final mesh size while maintaining high resolution especially in the cross channel direction regardless of the river width the shape of the elements is controlled by a user specified ratio between the along thalweg point spacing to the cross channel resolution in addition the along channel resolution should also account for river curvatures because more points are needed at sharp river bends than along straight channels to fit the river geometry with these considerations the script redistributes the thalweg points based on the along channel resolution d l calculated at each thalweg points as 2 d l min w r 1 n r 2 κ where w is river width n is the user specified number of cross channel segments which can be constant or dependent on river width so w n is the cross channel resolution r 1 is the user prescribed ratio between the along thalweg point spacing and the cross channel resolution r 2 is the user prescribed ratio between the along thalweg point spacing and the radius of the thalweg s curve which is the reciprocal of the curvature κ in practice we typically use a value of 4 0 5 0 for r 1 and 0 4 for r 2 before calculating the curvature κ at each thalweg point the script also automatically smoothes the thalweg in the horizontal dimension with a 30 point moving average this smoothing process eliminates the impact of minor zigzags e g on a scale of a few meters on the computed river curvature note that the horizontally smoothed thalweg is only an intermediate product for calculating curvatures the point redistribution is still processed on the original thalweg 2 2 1 4 correcting thalweg positions the thalwegs whether extracted from dems or imported from other sources may exhibit deviations from their true positions for instance some thalwegs produced in step 1 have a tendency to cling to one side of the channel along a sinuous river stretch fig 8 a and in the case of the national water model 8 8 url https water noaa gov about nwm last accessed apr 6 2023 nwm some segments can even be slightly outside the channel shown in the satellite imagery fig 8b as the quality of the generated river map relies on the thalwegs position which should ideally be situated between the riverbanks a thalweg correction is applied prior to determining the final bank positions this is done by relocating each thalweg point to the deepest location along the cross channel transect fig 8c in practice we typically expand the transect i e the search range to three times the estimated channel width to accommodate potential inaccuracies in the initial thalweg position 2 2 1 5 specifying inner and outer arc locations after determining the positions of both riverbanks users have the option to add one or more arcs inside and or outside the channel as illustrated in fig 6e typically the number of inner arcs should be set to increase with river widths with a minimum of one inner arc placed in very narrow channels since the bank points always come in pairs specifying the relative position of inner arcs between banks is straightforward fig 9 provides an example where inner arcs evenly divide the cross channel transect on the other hand implementing a pair of outer arcs is advisable due to two advantages i it facilitates a smoother transition between the quasi 1d river elements which are elongated in the along channel direction and the uniform watershed elements which closely resemble equilateral triangles and ii it accommodates the uncertainty in the estimated water levels and allows for a broader channel during high flow events 2 2 2 parallel driver an optional parallel driver is provided for large applications the driver first divides thalwegs into groups based on their parent dem tiles note that tiles from heterogeneous dem products of different resolutions are allowed in rivermapper and then automatically allocates an optimized workload to the worker routine make river map on each compute core the workflow is illustrated in fig 10 2 2 3 execution for a small or medium size application e g with a spatial coverage of one or two states the serial function make river map is efficient enough the runtime is on the order of a few minutes the function call looks like make river map tif fnames thalweg shp fname output dir the first input argument tif fnames is a list of names of dem files in tif format the second input argument is the name of the shapefile that contains all thalwegs as linestrings for a large application on the continental scale cf the case study in section 3 it is preferable to call the serial function by the parallel driver a sample script is provided in the github repository see software and data availability which can be executed as mpirun n 20 sample parallel py the function call to the parallel driver looks like river map mpi driver dems json file thalweg shp fname output dir the input argument dems json file simplifies the process of specifying numerous tiles from multiple dem products by using an input file instead of a list the runtime for the us east and gulf coasts domain is about 1 5 h with one computation node 20 cores on w m s bora cluster in comparison the serial mode takes roughly 16 h the default output format is esri shapefile and the main file is named total arcs shp it assumes a linestring geometry type and contains all polylines for guiding river meshing the shapefile should be readable by common meshing tools such as sms it is worth noting that the mesh generator is not limited to the one used in this study the only requirement is that the mesh generation process must reproduce the original polylines without adding new vertices or removing existing vertices in other words the final mesh must preserve the original line segments as element sides 2 3 mesh generation of the unstructured grids the high resolution polylines generated in section 2 2 are directly fed into sms to generate the final mesh note that the tool used here only creates polylines arcs for the watershed rivers and the users are responsible for generating the sms maps for other parts of the domain which have simpler geometry constraints and are thus easier to create also defense structures available in shapefile format e g from the national levee database 9 9 url https levees sec usace army mil last accessed apr 6 2023 are imported into sms various maps are then merged inside sms due to the sheer number of arcs 350 000 and vertices 1 14 million involved the meshing is so heavily constrained that in our experience few mesh generators can handle it the current tool is capable of generating ugs that capture rivers in extremely high resolution with the only limitation being the resolution of the underlying dem with the availability of high quality dems in most us coastal areas we are able to resolve small rivers of 10 m wide with 2 3 cross channel elements cf fig 15c in general the accuracy of the river channels depicted in the mesh is quite satisfactory when visually compared to satellite imagery cf fig 9 obviously the quality of the dem is the primary factor influencing this level of fidelity due to the highly constrained triangulation the generated mesh inevitably includes some skewed elements that necessitate a robust model such as schism to manage effectively courtesy of the quasi 1d river representation the mesh size is moderate with 2 9 million nodes for the domain of us east and gulf coasts in contrast methods that use the size function approach often result in an excessive amount of resolution in small channels leading to mesh sizes that are 5 10 times larger however even with larger mesh sizes these methods still have difficulty in providing adequate cross channel resolution for narrow channels that are a few meters wide therefore the current method should lead to significant improvements in both the efficiency and accuracy of the model simulation 3 application to a continent scale study of compound flooding the mesh generated by the new tool is ideal for compound flooding studies because it explicitly resolves river channels and other important features in the watersheds previously we attempted the same goal by extensive manual editing cf fig 16 which is time consuming and labor intensive moreover it is prone to errors and can easily mis represent narrow channels severe consequences of such errors include 1 impeded river flow and tidal movements and 2 inaccurately elevated surface levels due to flow obstruction the results of this case study demonstrate that the newly developed tool represents a major step forward in simulating watershed flows in the context of compound flooding 3 1 creek to ocean model the simulation is carried out using an open source cross scale creek to ocean 3d model schism zhang et al 2016 schism wiki which applies a hybrid finite element finite volume method to solve the reynolds averaged navier stokes equation together with tracer transport a major feature of schism that is crucial for the current study is its implicit time stepping method that guarantees numerical stability free of cfl restriction even with skew elements and very fine resolution its robustness is also beneficial to the automatic generation of on demand forecast systems for example by opencoasts oliveira et al 2020 2021 another major feature that has great implications for the compound flood study is that schism does not smooth bathymetry but instead interpolates directly from the original dems using linear interpolation this makes the model much more sensitive and responsive to dem quality than other models that rely on bathymetry smoothing manipulation the detrimental effects of bathymetry smoothing have been demonstrated in ye et al 2018 and cai et al 2020 for physical and biological processes respectively we remark that the commonly used nearest point interpolation method with channel greedy approach i e using the maximum depth from the surrounding dem raster cells would lead to discontinuous behavior as the mesh is revised the linear interpolation method used in our model in all pre and post processing is c1 continuous or in other words the function approximated by the elements has a smooth gradient across element boundaries however care should be taken when extracting the model results at gauge locations via linear interpolation because schism does not allow partial wetting and drying in a cell to avoid interpolating from a dry node the mesh design needs to ensure that all nodes of the gauge s parent element are within the channel the model domain and setup are similar to huang et al 2021 and to stofs 3d atlantic the domain fig 11 includes the entire us east and gulf coasts in high resolution with the land boundary located at 10 m above msl where the river flows calculated from nwm v2 1 are injected into our domain the offshore boundary is located at 60 w the simulation lasts 61 days starting from aug 17 2018 and thus covers both hurricane florence 2018 and hurricane michael 2018 the vertical datum used is navd88 the model uses a non split time step of 150 s with an implicitness factor of 1 0 i e fully implicit and modified mellor yamada scheme k kl umlauf and burchard 2003 as the turbulence closure for tracer transport the 3rd order weno scheme ye et al 2019 is used for eddying regime and upwind scheme is used for the watersheds the biharmonic viscosity is used for the horizontal mixing augmented by a slope dependent shapiro filter huang et al 2021 the 3d model is initialized and boundary forced by the hybrid coordinate ocean model hycom 10 10 url https www hycom org last accessed apr 6 2023 as the non tidal component for the tidal component of the elevation and velocity the global tidal database of fes2014 lyard et al 2021 is used the atmospheric forcing source consists of 1 noaa s high resolution rapid refresh hrrr benjamin et al 2016 numerical weather prediction modeling system with a 3 km and hourly resolution for coastal ocean and watersheds and 2 era5 hersbach et al 2020 with a 30 km and hourly resolution for areas not covered by hrrr the high resolution precipitation from hrrr is used in the model to simulate the overland runoff since we start from a fully dynamic state from hycom the spin up period is only several days our experience suggests that the watershed flows require 10 days to equilibrate therefore the results from the first 20 days are excluded in the analysis using 3360 cores of texas advanced computing center s frontera cluster the model runs 100 times faster than real time in addition to the new mesh generated in section 2 3 for comparison purposes we also include the results from an older mesh which used the 1d river network from nwm to guide the watershed river meshing higher resolution was specified at the thalwegs to enhance the detail near the channels however the representation of the lateral direction of the channel was insufficient as thalwegs are only 1d features in addition considerable manual editing was performed to improve channel connectivity as shown in fig 12 a the resultant mesh still has many broken rivers due to imprecise channel specification the old mesh has 2 7 million nodes in contrast the new mesh is slightly larger with 2 9 million nodes but it offers a significantly improved channel representation fig 12b as compared to the satellite imagery fig 12c 3 2 hurricane study in 2018 the stofs 3d atlantic domain us east and gulf coasts experienced two major hurricanes florence and michael in quick succession hurricane florence was initially a category 4 storm which downgraded to category 1 when making landfall at wrightsville beach north carolina on september 14 2018 the hurricane was characterized by its slow movement and excessive precipitation leading to devastating compound flooding the estimated total damage was 24 billion according to noaa 11 url https www ncei noaa gov access billions events pdf last accessed apr 6 2023 11 hurricane michael the first category 5 hurricane to strike the contiguous united states since hurricane andrew in 1992 made landfall on october 10 2018 at mexico beach florida with peak winds reaching 261 km h this powerful storm devastated homes in mexico beach and panama city florida its total damage was estimated to be 25 billion according to the same noaa source the focus of this study is the total water level while assessment of 3d variables salinity water temperature velocity have been presented in huang et al 2021 and ye et al 2021 multiple us agencies including noaa us geological survey usgs and us army corps of engineers usace maintain many water level gauges along the us east and gulf coasts from coastal regions into the watershed which can be used to assess the model skill since the coastal part of the new mesh is identical to that used in huang et al 2021 we skip the comparisons at noaa stations and focus on usgs gauges and high water marks hwms here accurate simulation at usgs gauges is very challenging especially in small rivers as it requires high quality dems and precise knowledge of the gauge locations while lidar based dems are highly accurate for the watershed topography the bathymetry in some rivers can be questionable this is because lidar captures only the water surface elevation and algorithms used to convert surface elevation to bottom elevation have substantial uncertainties without actual bathymetric survey data only recently did the ground penetrating lidar become available which will greatly reduce the dem uncertainties in the future meanwhile the dems used in this paper were derived from older surveys and are thus subject to this issue in addition the mesh may also need to have higher resolution locally in small rivers via selection of the meshing parameters mentioned in section 2 with the new technique developed in this paper we can arbitrarily increase the mesh resolution locally to capture features subject to the quality of the dems used given the caveats mentioned above we have achieved some success in capturing the total water elevation in the watershed at several usgs gauges in the florida panhandle fig 13 a c and western florida fig 13d and e the locally very high resolution employed to resolve small rivers proved to be quite effective in accurately capturing the tidal propagation into those rivers remaining issues are found in the areas near some gauges where the dem quality is questionable an example is illustrated in fig 13e with the associated dem shown in fig 14 the gauge is located in the upstream of a narrow river and the dem suggests very shallow depths along the channel a constant bed elevation of 0 1 m was found in the river and the inlet that connects the river to the shelf fig 14 which is highly dubious there may be additional uncertainty in the exact gauge location with respect to the channel therefore even though the mesh well resolves the channel as depicted in the dem the simulated tides are more attenuated than the observation fig 13e the most striking differences between the old and new mesh are seen in the simulated flow routing in the watershed for compound flooding studies it is preferable to visualize the flood using the concept of disturbance which is defined as the water surface elevation in the wet coastal portion of the domain and total water depth in the dry hydrologic regime huang et al 2021 3 d η i f h 0 i e w e t r e g i m e η h i f h 0 i e d r y r e g i m e where η is the water surface elevation and h is the positive downward bathymetry disturbance essentially measures the deviation of water level from the initial undisturbed non storm condition despite a great deal of manual editing in the old mesh to align the mesh to the river channels there are many places where the flow is interrupted due to inadequate channel resolution especially in the cross channel direction fig 15a as a result the blocked flow leads to over estimation of lateral inundation upriver fig 15a vs fig 15b this issue has been largely fixed with the new mesh as illustrated in fig 15b in addition fig 15c reveals that the new mesh is able to well resolve narrow stretches 10 m wide of channels thus allowing unimpeded flow the accurate conveyance of river flow from the watershed to the coastal region which essentially establishes the boundary condition for the coastal regime is a crucial prerequisite for the precise simulation of compound flooding a better flood conveyance in the mesh also helps reduce the error of the simulated water surface elevation at hwms as illustrated in a previous hindcast study of hurricane florence ye et al 2021 using thalwegs alone as guiding polylines at the meshing stage did not guarantee channel connectivity of the resultant mesh consequently floodwater tended to accumulate in the upstream region due to the lack of draining conduits resulting in significant overestimations of water surface elevation in the watershed fig 16a as a temporary remedy we manually edited some of the problematic river channels to apply a 2d channel representation which effectively reduced the model error fig 16b however this was only done for limited locations because the amount of manual labor involved can be formidable for large scale applications like stofs 3d atlantic in contrast the newly introduced tools allow achieving mesh quality similar to the manually edited one compare fig 16b and c but in a much more efficient manner for the whole domain note that the model errors in fig 16c are comparable to those in fig 16b on most data points except for the two points highlighted in fig 16c this discrepancy is related to the level of detail in the river network which is prescribed by the user during the thalweg extraction process section 2 1 some small rivers or lakes circled in fig 16c may be neglected if they are outside the coverage of the extracted river network hence the large errors near them similar to the issue with hwms the use of the old mesh also caused unrealistic high waters near the land boundary where the nwm flows were injected fig 17 a in addition to the channel blockage problem mentioned earlier the issue is partially attributed to the momentum less injection of freshwater depending on the magnitude of the injected flow this approach can temporarily lead to a high water elevation at the injection point before the pressure gradient drives the water away the issue has been largely resolved with better channel representations fig 17b which facilitates the downstream transfer of floodwater however minor unphysical accumulation may still occur under sudden high flow events which can be problematic if the hydrodynamic model needs to send information back to the hydrological model to ensure sufficient time and space for the injected flow to reach a dynamic equilibrium feeder channels can be incorporated during the meshing stage this is achieved by extending the thalwegs slightly upstream beyond the original land boundary as highlighted in fig 17c under this configuration incoming flows are imposed at the upstream end of the feeder channels and the elevations at the downstream end are transmitted back to the hydrological model thereby preventing any undesirable boundary problems most of the remaining issues are related to the dem quality to determine the appropriate dem quality for applying the automated tools a simple rule of thumb is that channels should be discernible by visualization as an example the river channel shown in fig 18 a is clearly visible except for a small highlighted portion although in this case the river arcs are still clean fig 18b the simulated flow inevitably shows channel blockage because the channel depth is ultimately dependent on the dem on the other hand if most parts of a river are poorly defined in the dem the resultant river arcs may be chaotic as temporary measures users have the options from rivermapper to 1 discard the messy river arcs locally or 2 generate a pseudo channel of a prescribed channel width to allow artificial dredging in the mesh of course these issues can be more effectively addressed with an updated dem of better quality after all the automated tool is specifically designed to reduce the time required for the development cycle from a new bathymetric survey to a new operational forecast 4 conclusions we have successfully developed a parallel python based tool to effectively support mesh generation for watersheds this tool addresses a key challenge in watershed and compound flood modeling by providing an explicit guide in the form of polylines or equivalently arcs and vertices to the mesh generator to accomplish this the tool first identifies 1d thalwegs from digital elevation models dems and subsequently expands the 1d thalweg networks into 2d river representations a mesh generator sms is then used to triangulate the mesh with explicit constraints from the generated river polylines or feature arcs other mesh generators can also be utilized as long as they preserve the river representation furthermore it is possible to combine the current method with size function based methods an example is the ongoing work with ocsmesh 12 12 url https github com noaa ocs modeling ocsmesh last accessed apr 6 2023 which will be reported in a future publication we used this tool to generate a continental scale mesh for us east and gulf coasts that includes watersheds then used the mesh and an unstructured grid model schism to simulate compound flooding during hurricane events the model skill in the watershed was greatly improved with the new mesh and the water delivery from the watershed to the coastal region showed uninterrupted flow in most places comparisons at usgs gauges in some small rivers confirmed the superior model skill the resultant accuracy from the meshing tool is critically dependent on the dem quality especially for river bathymetry for which the new ground penetrating lidar should help the tool presented in this paper represents a major step forward in watershed modeling and compound flood studies in general declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was funded by noaa grant na21nos0080197 simulations used in this paper were conducted using the following computational facilities 1 william mary research computing url https www wm edu it rc 2 texas advanced computing center tacc at the university of texas at austin 
25386,a major challenge in compound flooding simulation is representing small rivers in the model mesh a parallel python based tool is developed to support the construction of unstructured grids ugs at continental scales the tool is driven by digital elevation model dem ensuring accurate representation of geomorphic features in the resulting mesh its first component pydem extends an existing tool to detect river thalwegs from dems the second component rivermapper uses the thalwegs to generate river arcs which can be directly ingested into meshing tools e g the surface water modeling system the novelty lies in the explicit 2d representation of rivers in both along and cross channel directions making it ideal for accurate efficient and high resolution continental scale research the tool is employed to create a ug for a 3d creek to ocean model encompassing the us east and gulf coasts and it greatly improves the flow exchange between the watershed and the coastal zone keywords compound flooding coastal transition zone cross scale modeling unstructured mesh generation data availability i have shared the link to my code data in the manuscript software and data availability 1 python based tools pydem co developed by linlin cui lcui vims edu and zhengui wang wangzg vims edu and rivermapper developed by fei ye feiye vims edu both tools along with the user manual and sample applications are freely accessible from the schism development github repository https github com schism dev rivermeshtools the standalone pydem package originally developed by zhengui wang is also available at https github com wzhengui pydem 2 mesh generation tool sms v13 2 or newer commercial software developed and maintained by aquaveo aquaveo com with a community version freely available at https www aquaveo com software sms community the tool requires windows 10 and above 3 compound flood model schism open source community model developed by yinglong j zhang vims freely accessible at schism wiki v5 10 0 1 introduction the increasing probabilities of compound floods i e high water levels caused by multiple flood drivers under the projected climate change present an urgent need for comprehending this type of high risk events wahl et al 2015 bevacqua et al 2019 physical models represent a powerful means for analyzing the processes and predicting the extent of compound floods xu et al 2022 ideally a single modeling framework should be used to simulate the nonlinear interactions among flood drivers santiago collazo et al 2019 which include rainfall runoff and storm surge wahl et al 2015 bilskie and hagen 2018 as well as high tides jang and chang 2022 waves qiang et al 2021 off shore oceanic processes ezer 2018 and sea level rise moftakhari et al 2017 del rosal salido et al 2021 this modeling technique is termed full coupling if all relevant equations from hydrology hydraulics and hydrodynamics are solved simultaneously santiago collazo et al 2019 the study conducted by stephens et al 2022 at galveston bay demonstrated the superior accuracy of the full coupling approach in our previous studies huang et al 2021 ye et al 2021 and noaa 1 1 national oceanic and atmospheric administration u s department of commerce s operational forecast stofs 3d atlantic 2 2 stofs 3d atlantic 3 d surge and tide operational forecast system for the atlantic basin url https polar ncep noaa gov estofs last accessed apr 6 2023 we confirmed the feasibility of efficiently applying a full coupling approach at the continental scale by employing a single model to simulate hydrodynamic hydraulic and hydrologic processes excluding groundwater flow and evapotranspiration the computational cost was effectively mitigated by the model s implicit solver for the shallow water equation which enables the use of large time steps even on very high mesh resolutions zhang et al 2016 2020 ye et al 2020 given the geomorphic complexity of coastal transition zones the full coupling approach strongly favors unstructured grid ug models and the remaining challenges of large scale compound flood modeling are often associated with the complexity of generating an appropriate model mesh the importance of accurately representing river channels in the mesh is highlighted by both uncertainty analysis willis et al 2019 and model applications on specific events ye et al 2021 stephens et al 2022 as these channels serve as the main conduit for floodwater from all flood drivers while the use of raster based sub grid approach has become popular recently casulli and stelling 2011 costabile et al 2020 it faces difficulties in representing defense structures such as levees and dams zhang 2021 additionally although the finite volume based sub grid models can utilize ugs they require strict orthogonal ugs that are extremely challenging to generate for complex coastal topography and bathymetry zhang 2021 translating a model domain into a ug often requires automated and manual steps a mesh generator can discretize a given geometric entity based on certain algorithms such as sweep line methods delaunay voronoi methods and others explained by thompson et al 1998 examples of mesh generators for free surface environmental and geophysical flows include surface water modeling system sms 3 3 url https www aquaveo com software sms surface water modeling system introduction last accessed apr 6 2023 ocsmesh mani et al 2022 mike mesh builder 4 4 url https www mikepoweredbydhi com products mike cloud mesh builder last accessed apr 6 2023 oceanmesh2d roberts et al 2019 jigsaw engwirda 2017 deltares meshkernel 5 5 url https github com deltares meshkernel last accessed apr 6 2023 etc furthermore optimizing a ug for coastal applications often involves manually placing polygons or polylines that follow topographic and bathymetric features such as levees thalwegs and characteristic contours overall mesh generation for large scale compound flood modeling is an arduous undertaking given the geometric complexity of nearshore and watershed regions which are characterized by a multitude of fine scale geomorphic features despite the capability of ugs achieving high resolution of these features while ensuring good resolution for other critical features essential for flow routing pushes both the mesh generator and model to the limit as an illustration in the case study presented in section 3 the total number of polylines employed to guide the watershed mesh generation is approximately 350 000 furthermore optimizing the ug to reduce the mesh size for efficient model performance would inevitably require sharp transitions in mesh resolution in the end incorporating fine scale features into the unstructured grid ug requires a heavily constrained triangulation which can be challenging for many meshing tools recent studies have been exploring more efficient meshing procedures that are capable of automatic local refinements conroy et al 2012 araújo et al 2013 roberts et al 2019 bacopoulos and hagen 2022 mani et al 2022 these methods enable automatic control of the spatially varying mesh resolution using size functions which are based on the length scale of geomorphic features or physical phenomena however despite the popularity of size function based methods our experience suggests that they often produce excessively large meshes due to a preference for smooth transitions in mesh resolution in addition many of these tools lack the ability to implement anisotropic elements i e elements with different length scales in different directions an exception is the strategy proposed by legrand et al 2007 which was used to create anisotropic meshes with skew elements across a continental slope in this paper we propose a direct meshing approach for watershed rivers in which all essential features are explicitly specified using a python tool this approach is designed to provide greater control over the final mesh size and is entirely dem driven after mesh generation the bathy topo dems are linearly interpolated onto the resultant mesh without any subjective manipulations e g bathymetry smoothing on the grid depth consequently this approach directly links dem quality to model accuracy it also greatly reduces the turnaround time from new releases of bathy topo datasets to their integration into operational forecasts to facilitate continental scale applications the tool is parallelized using mpi4py version 3 1 3 the first component of the tool pydem finds the 1d thalweg network of river channels using the hydrological network methodology section 2 1 then the 1d thalweg network is expanded by the second tool component rivermapper to form a 2d representation of rivers the 2d representation consists of polylines cf fig 9 to guide the delineation of river channels such that all line segments and vertices should be exactly reproduced in the final mesh the polylines can be output as linestrings in the esri shapefile format for general uses with a user s preferred meshing tool or in this study as feature arcs in the sms map format the meshing tool sms is used for triangulation section 2 3 because it strictly preserves the user specified polylines feature arcs which usually represent distinctive features of a terrain such as shorelines thalwegs banks and the outlines of hydraulic structures at the same time it also allows for resolution relaxation where the topographic gradient is small such as in the non river portion of a watershed where pluvial floods may occur this helps maintain a reasonable mesh size thus improving simulation efficiency the resultant mesh can resolve very narrow channels often found in upstream rivers down to the native dem resolution to ensure channel continuity and unimpeded flow routing in section 3 the meshing methodology and its products are tested with a 3d creek to ocean cross scale model on a domain covering the us east and gulf coasts model results during hurricanes demonstrate that the new mesh can more accurately route the river flow than a previously generated mesh same as the one used in ye et al 2021 that heavily relied on manual editing of channels major findings from this study are summarized in section 4 2 methods the new tool consists of two major components both written in parallel python the first component generates a 1d network of river thalwegs from the dems the second component utilizes existing thalweg information and dems to detect riverbanks then places polylines river arcs based on bank positions the river arcs are directly imported to a mesh generator for example sms to guide the meshing for watershed rivers these steps are illustrated in fig 1 and expounded in detail below further information and sample applications can be found in the online documentation see software and data availability 2 1 thalweg detection by pydem in this section we use a spatial domain that covers the coastal zones along the us east and gulf coasts fig 2 to illustrate the thalweg detection procedure the primary dem source used is noaa national centers for environmental information ncei s ninth arc second resolution bathymetric topographic dem tiles cudem 6 6 url https chs coast noaa gov htdata raster2 elevation ncei ninth topobathy 2014 8483 last accessed apr 6 2023 augmented by the coastal relief model crm 7 7 url https www ncei noaa gov products coastal relief model last accessed apr 6 2023 that fills in gaps in a few regions where cudem tiles are missing there are a total of 754 cudem tiles with 8112 8112 raster cells per tile the original resolution of the crm dataset is three arc seconds 90 m and we interpolate crm into the ninth arc second resolution because the thalweg detection algorithm presented in this subsection requires uniform dem tiles that dovetail with each other the completed coverage consisting of 1135 dem tiles is displayed in fig 2 for the parallel algorithm to work we first locally augment each dem tile with its 8 neighboring tiles this is done to avoid gaps between the calculated thalwegs and the border of each tile after the thalwegs are calculated from the local 9 tile dems only the thalwegs within the center tile are used for merging extracting channel networks from a locally merged 9 tile dem consists of three steps 1 fill or breach depressions in the dems 2 calculate flow direction based on the given flow metric 3 calculate flow accumulation numbers which are the total number of flow cells passing through each raster cell and then apply a prescribed flow accumulation threshold to determine which cells should be included in the final digital stream network the details of the three steps are elaborated below 2 1 1 filling depression the algorithm used in this paper is alg 3 priority flood ε in barnes et al 2014 for completeness this algorithm is illustrated along a dem s cross section fig 3 basically it fills any local depressions cells 3 and 4 in fig 3 to its lowest outlet cell 2 in fig 3 two queues are used to track the cells being processed q1 is a priority queue where cells with lower elevations are placed towards the head left of the sorted list and will be popped out first and q2 is a first in first out queue for storing elevated cells dotted lines in fig 3 the procedure starts from inserting edge cells into q1 fig 3a while either q1 or q2 is not empty the algorithm pops out a cell from either queue prioritizing q2 as the source if it is not empty for example in fig 3a the cell to be popped is 6 from q1 because q2 is empty in fig 3d the cell to be popped is 3 from q2 then push the un processed neighbors if any of the popped cell to either q1 or q2 the destination queue is determined by comparing the neighbor s elevation to the modified elevation of the popped cell i e the elevation of the popped cell plus a small positive number ε the neighbor cell is pushed to q1 if its elevation is higher than the modified elevation e g fig 3a and b and fig 3b and c otherwise the neighbor cell is raised to the modified elevation then pushed to q2 e g fig 3c and d and fig 3d and e the algorithm terminates when both q1 and q2 are empty 2 1 2 calculating flow direction the d8 flow method o callaghan and mark 1984 is used to assign a flow direction from each cell to its steepest downslope neighbor there is only one such neighbor after the fill depression step the flow coordinate system is shown in fig 4 for example if the direction of steepest drop is to the north of the cell of interest blue its flow direction value would be 64 the output of d8 flow direction calculation is an integer array associated with each cell in addition to the indices shown in fig 4 there are two more special indices reserved for some special cases with 1 representing no dem data cells and 0 representing cells on the edges of the locally merged dem tile flowing outwards 2 1 3 calculating flow accumulation this process starts from the outlets cells on the boundary of a locally merged 9 tile dem where the flow direction is flagged as zero and then recursively searches upstream for large dems the regular recursion algorithm may fail because of the maximum recursion depth allowed in python therefore we implement a multi level recursive search when the number of search steps exceeds 100 the search will restart from the current location for another round of search and so on until the boundary is reached all channel segment information is saved along the search so that we can calculate the total accumulation number for any given raster cell the boundary cells 0 will have the largest accumulation number to avoid excessive details of the thalweg network we set an accumulation threshold beyond which we stop the search and thus remove all upstream cells from the final network therefore the flow accumulation threshold can be regarded as a parameter that determines the granularity of the thalweg network using cudem tiles along the georgia coast as an example thalwegs extracted with three different flow accumulation thresholds are compared in fig 5 for the purpose of compound flood simulation a threshold of 107 is too large because it only retains major rivers but not smaller tributaries that are also important in floodwater routing fig 5a on the other hand a threshold of 105 includes too many small streams that requires an excessive amount of mesh elements to resolve fig 5c in this study we applied a threshold of 106 to the whole domain of us east and gulf coasts to obtain a reasonably accurate representation of the river network fig 5b we do not attempt to give a recommended scope of the accumulation threshold because it is affected by 1 different configurations of dem tiles 2 the granularity required for a particular study or application and 3 diverse topographical features for instance mountainous areas may exhibit narrower river channels compared to coastal plains necessitating a smaller threshold if other conditions and requirements are similar as a result identifying the ideal threshold requires a case by case approach incorporating both trial and error iterations and visual evaluations the run time to process one augmented dem tile with 73008 73008 cells 2 3 gigabytes augmented with its 8 neighbors is about half an hour the processing is parallelized by distributing dem tiles across multiple compute cores allowing each core to handle more than one tile if necessary with the parallelization the run time for the whole domain of us east and gulf coasts is about 6 h using 20 computing nodes on w m s vortex cluster even though each node has twelve cores we could only use two cores per node as the large tile size requires large memory obviously using more cores can significantly reduce the time given the nearly perfect parallel scaling 2 2 explicit 2d river representation by rivermapper rivermapper takes existing 1d river segments and dem tiles as inputs and generates polylines that constitute a 2d representation of the rivers in the watershed it is worth noting that the input river segments are not restricted to the thalwegs generated in the previous step section 2 1 instead any reasonable approximation of the river thalwegs can be used as input such as the 1d channel network from an existing hydrological model or manually drawn polylines for local mesh improvements rivermapper can run either in serial mode or in parallel mode with an optional parallel driver 2 2 1 workflow the procedure of rivermapper s core routine make river map is illustrated in fig 6 a with each phase colored differently in the preparation phase the geometries curvature and direction of the input thalweg segments and the initial water elevation on all thalweg points are calculated in addition a pair of bank points are located for each thalweg point fig 6b which provides river width information for the next step in the second phase the information of river curvature and width obtained in the first phase is used to redistribute the thalweg points fig 6c and the bank positions are updated based on the new thalweg points additionally this phase involves correcting thalweg positions and recording the cross channel resolution at each thalweg point upon completion of this phase the final corrected thalwegs are obtained which are useful as a standalone product and are also utilized in subsequent steps in the third phase the bank positions are updated again which is necessary because the thalwegs have been modified depending on user specifications this phase may also apply final touch ups on bank positions e g fig 6d these optional edits can help generate a more visually appealing mesh but may not have a direct impact on model accuracy the fourth phase involves placing additional river polylines based on the final bank positions fig 6e and cleaning up the geometry to avoid crowded polyline vertices the spatially varying cross channel resolution is utilized to determine the acceptable level of crowding among vertices finally the output that contains all polylines for guiding the river meshing is produced the remaining part of this sub section provides further details on the crucial steps required to achieve high accuracy and efficiency for additional information readers can refer to the online documentation available in the github repository see software and data availability 2 2 1 1 efficient query of ground elevation during the execution of the tool the locations of thalweg points and bank points are frequently calculated and adjusted on a continental scale this can involve millions of points and thousands of dem tiles moreover the tiles can be from one or more products thus having different spatial coverages and resolutions consequently an efficient method for querying river points z values from dem tiles is essential for the overall computational efficiency in this tool the nearest neighbor interpolation is implemented by directly computing the corresponding 2d indices of the points in multiple dem tiles a more precise interpolation is not required based on our experience because the primary dem sources typically have a resolution of a few meters which is fine enough for delineating rivers secondly since a specific thalweg typically resides within a limited number of dem tiles grouping thalwegs by their parent tiles is preferable for large applications a parallel driver see section 2 2 2 is provided to automatically generate an optimized grouping where each group consists of a small subset of thalwegs and dem tiles these groups are then distributed across multiple compute cores which accelerates the search process and reduces memory consumption furthermore the code utilizes numpy harris et al 2020 vectorized operations when dealing with large vectors and arrays these operations are significantly faster than python s for loops and offer performance comparable to that of c or fortran programs 2 2 1 2 detecting bank positions bank positions are calculated multiple times in the workflow fig 6a because multiple steps depend on the estimated channel width which must be updated following any changes in the thalwegs given a thalweg point two bank edges where the mean water depth is 0 fig 6b on both sides of the thalweg are searched in the cross channel direction the search process is straightforward but determining the water depth requires knowing the time varying free water surface which is not trivial although the free surface can be assumed to be the same as the local mean sea level for nearshore regions this approximation is no longer valid for inland rivers where the riverbed is higher than the local sea level a number of options were considered to improve the approximation one potential solution is to interpolate the surface elevations from the river stage observations here we propose a simpler alternative that is more straightforward to implement as illustrated in fig 7 the domain is divided into three parts upland region transitional region and coastal region by two representative ground elevation values z u p l a n d and z c o a s t a l in the coastal zone a constant water level e g the mean sea level is assumed in the upland zone a constant water depth is assumed since the free water surface should be smoother than the underlying riverbed the ground elevation along each thalweg is smoothed with a 100 m moving window filter then the water surface is found by adding the assumed constant water depth to the smoothed thalweg elevation in the transitional zone the water level varies linearly from the coastal value to the upland value as 1 η t r a n s i t i o n a l z s z c o a s t a l z u p l a n d z c o a s t a l η u p l a n d z u p l a n d z s z u p l a n d z c o a s t a l η c o a s t a l with the symbols defined in fig 7 in this study we choose z u p l a n d 3 m navd 88 and z c o a s t a l 0 m navd 88 the choice of the constant water depth h in the upland region fig 7 is somewhat arbitrary from our experience a small water depth e g 1 0 m is preferred as it usually results in a clean delineation of river channels the resultant bank positions from this algorithm are generally satisfactory an example is shown in fig 9 additionally the margins of errors in the bank delineation can be partially accounted for by inner and outer arcs as described in section 2 2 1 5 2 2 1 3 optimizing thalweg points distribution the spatial resolution of the extracted thalwegs depends on the dem sources and the typical spacing between two adjacent thalweg points is 1 5 m this resolution is too fine for resolving rivers in the along channel direction in watershed rivers we recommend using quasi 1d elements which are anisotropic elements longer in the along channel direction and shorter in the cross channel direction compared to the size function based approach this quasi 1d river representation significantly reduces the final mesh size while maintaining high resolution especially in the cross channel direction regardless of the river width the shape of the elements is controlled by a user specified ratio between the along thalweg point spacing to the cross channel resolution in addition the along channel resolution should also account for river curvatures because more points are needed at sharp river bends than along straight channels to fit the river geometry with these considerations the script redistributes the thalweg points based on the along channel resolution d l calculated at each thalweg points as 2 d l min w r 1 n r 2 κ where w is river width n is the user specified number of cross channel segments which can be constant or dependent on river width so w n is the cross channel resolution r 1 is the user prescribed ratio between the along thalweg point spacing and the cross channel resolution r 2 is the user prescribed ratio between the along thalweg point spacing and the radius of the thalweg s curve which is the reciprocal of the curvature κ in practice we typically use a value of 4 0 5 0 for r 1 and 0 4 for r 2 before calculating the curvature κ at each thalweg point the script also automatically smoothes the thalweg in the horizontal dimension with a 30 point moving average this smoothing process eliminates the impact of minor zigzags e g on a scale of a few meters on the computed river curvature note that the horizontally smoothed thalweg is only an intermediate product for calculating curvatures the point redistribution is still processed on the original thalweg 2 2 1 4 correcting thalweg positions the thalwegs whether extracted from dems or imported from other sources may exhibit deviations from their true positions for instance some thalwegs produced in step 1 have a tendency to cling to one side of the channel along a sinuous river stretch fig 8 a and in the case of the national water model 8 8 url https water noaa gov about nwm last accessed apr 6 2023 nwm some segments can even be slightly outside the channel shown in the satellite imagery fig 8b as the quality of the generated river map relies on the thalwegs position which should ideally be situated between the riverbanks a thalweg correction is applied prior to determining the final bank positions this is done by relocating each thalweg point to the deepest location along the cross channel transect fig 8c in practice we typically expand the transect i e the search range to three times the estimated channel width to accommodate potential inaccuracies in the initial thalweg position 2 2 1 5 specifying inner and outer arc locations after determining the positions of both riverbanks users have the option to add one or more arcs inside and or outside the channel as illustrated in fig 6e typically the number of inner arcs should be set to increase with river widths with a minimum of one inner arc placed in very narrow channels since the bank points always come in pairs specifying the relative position of inner arcs between banks is straightforward fig 9 provides an example where inner arcs evenly divide the cross channel transect on the other hand implementing a pair of outer arcs is advisable due to two advantages i it facilitates a smoother transition between the quasi 1d river elements which are elongated in the along channel direction and the uniform watershed elements which closely resemble equilateral triangles and ii it accommodates the uncertainty in the estimated water levels and allows for a broader channel during high flow events 2 2 2 parallel driver an optional parallel driver is provided for large applications the driver first divides thalwegs into groups based on their parent dem tiles note that tiles from heterogeneous dem products of different resolutions are allowed in rivermapper and then automatically allocates an optimized workload to the worker routine make river map on each compute core the workflow is illustrated in fig 10 2 2 3 execution for a small or medium size application e g with a spatial coverage of one or two states the serial function make river map is efficient enough the runtime is on the order of a few minutes the function call looks like make river map tif fnames thalweg shp fname output dir the first input argument tif fnames is a list of names of dem files in tif format the second input argument is the name of the shapefile that contains all thalwegs as linestrings for a large application on the continental scale cf the case study in section 3 it is preferable to call the serial function by the parallel driver a sample script is provided in the github repository see software and data availability which can be executed as mpirun n 20 sample parallel py the function call to the parallel driver looks like river map mpi driver dems json file thalweg shp fname output dir the input argument dems json file simplifies the process of specifying numerous tiles from multiple dem products by using an input file instead of a list the runtime for the us east and gulf coasts domain is about 1 5 h with one computation node 20 cores on w m s bora cluster in comparison the serial mode takes roughly 16 h the default output format is esri shapefile and the main file is named total arcs shp it assumes a linestring geometry type and contains all polylines for guiding river meshing the shapefile should be readable by common meshing tools such as sms it is worth noting that the mesh generator is not limited to the one used in this study the only requirement is that the mesh generation process must reproduce the original polylines without adding new vertices or removing existing vertices in other words the final mesh must preserve the original line segments as element sides 2 3 mesh generation of the unstructured grids the high resolution polylines generated in section 2 2 are directly fed into sms to generate the final mesh note that the tool used here only creates polylines arcs for the watershed rivers and the users are responsible for generating the sms maps for other parts of the domain which have simpler geometry constraints and are thus easier to create also defense structures available in shapefile format e g from the national levee database 9 9 url https levees sec usace army mil last accessed apr 6 2023 are imported into sms various maps are then merged inside sms due to the sheer number of arcs 350 000 and vertices 1 14 million involved the meshing is so heavily constrained that in our experience few mesh generators can handle it the current tool is capable of generating ugs that capture rivers in extremely high resolution with the only limitation being the resolution of the underlying dem with the availability of high quality dems in most us coastal areas we are able to resolve small rivers of 10 m wide with 2 3 cross channel elements cf fig 15c in general the accuracy of the river channels depicted in the mesh is quite satisfactory when visually compared to satellite imagery cf fig 9 obviously the quality of the dem is the primary factor influencing this level of fidelity due to the highly constrained triangulation the generated mesh inevitably includes some skewed elements that necessitate a robust model such as schism to manage effectively courtesy of the quasi 1d river representation the mesh size is moderate with 2 9 million nodes for the domain of us east and gulf coasts in contrast methods that use the size function approach often result in an excessive amount of resolution in small channels leading to mesh sizes that are 5 10 times larger however even with larger mesh sizes these methods still have difficulty in providing adequate cross channel resolution for narrow channels that are a few meters wide therefore the current method should lead to significant improvements in both the efficiency and accuracy of the model simulation 3 application to a continent scale study of compound flooding the mesh generated by the new tool is ideal for compound flooding studies because it explicitly resolves river channels and other important features in the watersheds previously we attempted the same goal by extensive manual editing cf fig 16 which is time consuming and labor intensive moreover it is prone to errors and can easily mis represent narrow channels severe consequences of such errors include 1 impeded river flow and tidal movements and 2 inaccurately elevated surface levels due to flow obstruction the results of this case study demonstrate that the newly developed tool represents a major step forward in simulating watershed flows in the context of compound flooding 3 1 creek to ocean model the simulation is carried out using an open source cross scale creek to ocean 3d model schism zhang et al 2016 schism wiki which applies a hybrid finite element finite volume method to solve the reynolds averaged navier stokes equation together with tracer transport a major feature of schism that is crucial for the current study is its implicit time stepping method that guarantees numerical stability free of cfl restriction even with skew elements and very fine resolution its robustness is also beneficial to the automatic generation of on demand forecast systems for example by opencoasts oliveira et al 2020 2021 another major feature that has great implications for the compound flood study is that schism does not smooth bathymetry but instead interpolates directly from the original dems using linear interpolation this makes the model much more sensitive and responsive to dem quality than other models that rely on bathymetry smoothing manipulation the detrimental effects of bathymetry smoothing have been demonstrated in ye et al 2018 and cai et al 2020 for physical and biological processes respectively we remark that the commonly used nearest point interpolation method with channel greedy approach i e using the maximum depth from the surrounding dem raster cells would lead to discontinuous behavior as the mesh is revised the linear interpolation method used in our model in all pre and post processing is c1 continuous or in other words the function approximated by the elements has a smooth gradient across element boundaries however care should be taken when extracting the model results at gauge locations via linear interpolation because schism does not allow partial wetting and drying in a cell to avoid interpolating from a dry node the mesh design needs to ensure that all nodes of the gauge s parent element are within the channel the model domain and setup are similar to huang et al 2021 and to stofs 3d atlantic the domain fig 11 includes the entire us east and gulf coasts in high resolution with the land boundary located at 10 m above msl where the river flows calculated from nwm v2 1 are injected into our domain the offshore boundary is located at 60 w the simulation lasts 61 days starting from aug 17 2018 and thus covers both hurricane florence 2018 and hurricane michael 2018 the vertical datum used is navd88 the model uses a non split time step of 150 s with an implicitness factor of 1 0 i e fully implicit and modified mellor yamada scheme k kl umlauf and burchard 2003 as the turbulence closure for tracer transport the 3rd order weno scheme ye et al 2019 is used for eddying regime and upwind scheme is used for the watersheds the biharmonic viscosity is used for the horizontal mixing augmented by a slope dependent shapiro filter huang et al 2021 the 3d model is initialized and boundary forced by the hybrid coordinate ocean model hycom 10 10 url https www hycom org last accessed apr 6 2023 as the non tidal component for the tidal component of the elevation and velocity the global tidal database of fes2014 lyard et al 2021 is used the atmospheric forcing source consists of 1 noaa s high resolution rapid refresh hrrr benjamin et al 2016 numerical weather prediction modeling system with a 3 km and hourly resolution for coastal ocean and watersheds and 2 era5 hersbach et al 2020 with a 30 km and hourly resolution for areas not covered by hrrr the high resolution precipitation from hrrr is used in the model to simulate the overland runoff since we start from a fully dynamic state from hycom the spin up period is only several days our experience suggests that the watershed flows require 10 days to equilibrate therefore the results from the first 20 days are excluded in the analysis using 3360 cores of texas advanced computing center s frontera cluster the model runs 100 times faster than real time in addition to the new mesh generated in section 2 3 for comparison purposes we also include the results from an older mesh which used the 1d river network from nwm to guide the watershed river meshing higher resolution was specified at the thalwegs to enhance the detail near the channels however the representation of the lateral direction of the channel was insufficient as thalwegs are only 1d features in addition considerable manual editing was performed to improve channel connectivity as shown in fig 12 a the resultant mesh still has many broken rivers due to imprecise channel specification the old mesh has 2 7 million nodes in contrast the new mesh is slightly larger with 2 9 million nodes but it offers a significantly improved channel representation fig 12b as compared to the satellite imagery fig 12c 3 2 hurricane study in 2018 the stofs 3d atlantic domain us east and gulf coasts experienced two major hurricanes florence and michael in quick succession hurricane florence was initially a category 4 storm which downgraded to category 1 when making landfall at wrightsville beach north carolina on september 14 2018 the hurricane was characterized by its slow movement and excessive precipitation leading to devastating compound flooding the estimated total damage was 24 billion according to noaa 11 url https www ncei noaa gov access billions events pdf last accessed apr 6 2023 11 hurricane michael the first category 5 hurricane to strike the contiguous united states since hurricane andrew in 1992 made landfall on october 10 2018 at mexico beach florida with peak winds reaching 261 km h this powerful storm devastated homes in mexico beach and panama city florida its total damage was estimated to be 25 billion according to the same noaa source the focus of this study is the total water level while assessment of 3d variables salinity water temperature velocity have been presented in huang et al 2021 and ye et al 2021 multiple us agencies including noaa us geological survey usgs and us army corps of engineers usace maintain many water level gauges along the us east and gulf coasts from coastal regions into the watershed which can be used to assess the model skill since the coastal part of the new mesh is identical to that used in huang et al 2021 we skip the comparisons at noaa stations and focus on usgs gauges and high water marks hwms here accurate simulation at usgs gauges is very challenging especially in small rivers as it requires high quality dems and precise knowledge of the gauge locations while lidar based dems are highly accurate for the watershed topography the bathymetry in some rivers can be questionable this is because lidar captures only the water surface elevation and algorithms used to convert surface elevation to bottom elevation have substantial uncertainties without actual bathymetric survey data only recently did the ground penetrating lidar become available which will greatly reduce the dem uncertainties in the future meanwhile the dems used in this paper were derived from older surveys and are thus subject to this issue in addition the mesh may also need to have higher resolution locally in small rivers via selection of the meshing parameters mentioned in section 2 with the new technique developed in this paper we can arbitrarily increase the mesh resolution locally to capture features subject to the quality of the dems used given the caveats mentioned above we have achieved some success in capturing the total water elevation in the watershed at several usgs gauges in the florida panhandle fig 13 a c and western florida fig 13d and e the locally very high resolution employed to resolve small rivers proved to be quite effective in accurately capturing the tidal propagation into those rivers remaining issues are found in the areas near some gauges where the dem quality is questionable an example is illustrated in fig 13e with the associated dem shown in fig 14 the gauge is located in the upstream of a narrow river and the dem suggests very shallow depths along the channel a constant bed elevation of 0 1 m was found in the river and the inlet that connects the river to the shelf fig 14 which is highly dubious there may be additional uncertainty in the exact gauge location with respect to the channel therefore even though the mesh well resolves the channel as depicted in the dem the simulated tides are more attenuated than the observation fig 13e the most striking differences between the old and new mesh are seen in the simulated flow routing in the watershed for compound flooding studies it is preferable to visualize the flood using the concept of disturbance which is defined as the water surface elevation in the wet coastal portion of the domain and total water depth in the dry hydrologic regime huang et al 2021 3 d η i f h 0 i e w e t r e g i m e η h i f h 0 i e d r y r e g i m e where η is the water surface elevation and h is the positive downward bathymetry disturbance essentially measures the deviation of water level from the initial undisturbed non storm condition despite a great deal of manual editing in the old mesh to align the mesh to the river channels there are many places where the flow is interrupted due to inadequate channel resolution especially in the cross channel direction fig 15a as a result the blocked flow leads to over estimation of lateral inundation upriver fig 15a vs fig 15b this issue has been largely fixed with the new mesh as illustrated in fig 15b in addition fig 15c reveals that the new mesh is able to well resolve narrow stretches 10 m wide of channels thus allowing unimpeded flow the accurate conveyance of river flow from the watershed to the coastal region which essentially establishes the boundary condition for the coastal regime is a crucial prerequisite for the precise simulation of compound flooding a better flood conveyance in the mesh also helps reduce the error of the simulated water surface elevation at hwms as illustrated in a previous hindcast study of hurricane florence ye et al 2021 using thalwegs alone as guiding polylines at the meshing stage did not guarantee channel connectivity of the resultant mesh consequently floodwater tended to accumulate in the upstream region due to the lack of draining conduits resulting in significant overestimations of water surface elevation in the watershed fig 16a as a temporary remedy we manually edited some of the problematic river channels to apply a 2d channel representation which effectively reduced the model error fig 16b however this was only done for limited locations because the amount of manual labor involved can be formidable for large scale applications like stofs 3d atlantic in contrast the newly introduced tools allow achieving mesh quality similar to the manually edited one compare fig 16b and c but in a much more efficient manner for the whole domain note that the model errors in fig 16c are comparable to those in fig 16b on most data points except for the two points highlighted in fig 16c this discrepancy is related to the level of detail in the river network which is prescribed by the user during the thalweg extraction process section 2 1 some small rivers or lakes circled in fig 16c may be neglected if they are outside the coverage of the extracted river network hence the large errors near them similar to the issue with hwms the use of the old mesh also caused unrealistic high waters near the land boundary where the nwm flows were injected fig 17 a in addition to the channel blockage problem mentioned earlier the issue is partially attributed to the momentum less injection of freshwater depending on the magnitude of the injected flow this approach can temporarily lead to a high water elevation at the injection point before the pressure gradient drives the water away the issue has been largely resolved with better channel representations fig 17b which facilitates the downstream transfer of floodwater however minor unphysical accumulation may still occur under sudden high flow events which can be problematic if the hydrodynamic model needs to send information back to the hydrological model to ensure sufficient time and space for the injected flow to reach a dynamic equilibrium feeder channels can be incorporated during the meshing stage this is achieved by extending the thalwegs slightly upstream beyond the original land boundary as highlighted in fig 17c under this configuration incoming flows are imposed at the upstream end of the feeder channels and the elevations at the downstream end are transmitted back to the hydrological model thereby preventing any undesirable boundary problems most of the remaining issues are related to the dem quality to determine the appropriate dem quality for applying the automated tools a simple rule of thumb is that channels should be discernible by visualization as an example the river channel shown in fig 18 a is clearly visible except for a small highlighted portion although in this case the river arcs are still clean fig 18b the simulated flow inevitably shows channel blockage because the channel depth is ultimately dependent on the dem on the other hand if most parts of a river are poorly defined in the dem the resultant river arcs may be chaotic as temporary measures users have the options from rivermapper to 1 discard the messy river arcs locally or 2 generate a pseudo channel of a prescribed channel width to allow artificial dredging in the mesh of course these issues can be more effectively addressed with an updated dem of better quality after all the automated tool is specifically designed to reduce the time required for the development cycle from a new bathymetric survey to a new operational forecast 4 conclusions we have successfully developed a parallel python based tool to effectively support mesh generation for watersheds this tool addresses a key challenge in watershed and compound flood modeling by providing an explicit guide in the form of polylines or equivalently arcs and vertices to the mesh generator to accomplish this the tool first identifies 1d thalwegs from digital elevation models dems and subsequently expands the 1d thalweg networks into 2d river representations a mesh generator sms is then used to triangulate the mesh with explicit constraints from the generated river polylines or feature arcs other mesh generators can also be utilized as long as they preserve the river representation furthermore it is possible to combine the current method with size function based methods an example is the ongoing work with ocsmesh 12 12 url https github com noaa ocs modeling ocsmesh last accessed apr 6 2023 which will be reported in a future publication we used this tool to generate a continental scale mesh for us east and gulf coasts that includes watersheds then used the mesh and an unstructured grid model schism to simulate compound flooding during hurricane events the model skill in the watershed was greatly improved with the new mesh and the water delivery from the watershed to the coastal region showed uninterrupted flow in most places comparisons at usgs gauges in some small rivers confirmed the superior model skill the resultant accuracy from the meshing tool is critically dependent on the dem quality especially for river bathymetry for which the new ground penetrating lidar should help the tool presented in this paper represents a major step forward in watershed modeling and compound flood studies in general declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was funded by noaa grant na21nos0080197 simulations used in this paper were conducted using the following computational facilities 1 william mary research computing url https www wm edu it rc 2 texas advanced computing center tacc at the university of texas at austin 
25387,agent based modeling abm has been widely used in numerous disciplines and practice domains subject to many eulogies and criticisms this article presents key advances and challenges in agent based modeling over the last two decades and shows that understanding agents behaviors is a major priority for various research fields we demonstrate that artificial intelligence and data science will likely generate revolutionary impacts for science and technology towards understanding agent decisions and behaviors in complex systems we propose an innovative approach that leverages reinforcement learning and convolutional neural networks to equip agents with the intelligence of self learning their behavior rules directly from data we call for further developments of abm especially modeling agent behaviors in the light of data science and artificial intelligence keywords agent based modeling modeling agent decisions and actions artificial intelligence machine learning data science abbreviations abm agent base modeling ai artificial intelligence rl reinforcement learning gnn graph neural networks cnn convolutional neural network data availability no data were used for the research described in the article although agent based modeling abm abms for agent based models emerged as early as the 1970s schelling 1971 or even earlier w zhang et al 2021 it has been extensively applied in ecology where it is usually referred to as individual based modeling grimm 1999 and numerous other disciplines since the 1990s vincenot 2018 subsequently abm has exploded in applications fig 1 an indication of its usefulness across multiple sciences milestones in abm development include the overview design concepts and details odd protocol for the model documentation grimm et al 2010 2020 and the pattern oriented modeling pom paradigm grimm et al 2005 abm received a major institutional endorsement in 2001 when it was featured by the u s national academy of sciences sackler colloquium and the resultant special issue in the proceedings of the national academy of sciences bonabeau 2002 since then abm has been hailed with both enthusiasm and optimism because of its potential to create a revolution among the social ecological behavioral and complexity sciences yet progress has been slower than initially anticipated in several critical areas of development and application grimm and berger 2016 lorscheid et al 2019 leading to various criticisms of abm couclelis 2002 roughgarden 2012 this slower progress likely leading to the fade of enthusiasm in abm is embedded in the context of a big issue for abm or any process based modeling effort the need to balance 1 the pattern informed top down approach which reproduces macro level patterns without adequate explanatory power such as the well reproduced flight patterns of hawks without an explanation of the mechanistic processes behind the patterns conte and paolucci 2014 and 2 the theory driven approach which aims at generating macro patterns from bottom up processes abm developers aim to not only accurately predict or replicate the observed patterns in question but also to understand and explain the mechanisms behind such patterns in this paper we propose a third approach based on artificial intelligence ai and data science to detect formulate and test mechanistic processes e g structures rules parameters that complement the above two traditional approaches the paper first briefly reviews historic advances in modeling human behavior followed by an overview of the challenges of modeling agent human behavior next we show how artificial intelligence and machine learning in combination with data science can help reveal mechanistic processes and model agent human behavior in the appendix we provide more details about the most promising methods in relation to what we propose in this paper 1 background about modeling agent human behavior in abm historically modeling agent human decision making and behavior in abm was largely based on economic theories such as benefit maximization or cost minimization by rational actors and largely ignored other approaches such as those in psychology and neurology groeneveld et al 2017 advances in relevant social and cognitive sciences have greatly enhanced the capacity of abm to model agent human in particular behavior filatova et al 2013 niamir et al 2020 generating normative models cognitive models and psychologically especially emotional models and neurologically inspired models that are instrumental for understanding and modeling human behavior for excellent review papers see balke and gilbert 2014 huber et al 2018 and bourgais et al 2018 along this thread one prominent example is the belief desire intentions bdi framework inspired by logical and psychological principles e g michael bratman s theory of human practical reasoning bratman 1999 the bdi framework models practical reasoning and subsequent action of resource bounded rational agents these agents carry 1 beliefs which are facts agents believe about the environment 2 desires which involve desired end state or goals to achieve and 3 intentions which are intended commitments to accomplish the corresponding desires goals under this framework intentions are important elements and precursors of planned actions considered an improvement compared with the bdi framework the physical emotional cognitive and social factors pecs conte paolucci 2014 schmidt 2002 framework aims to explain or predict human behavior from a common deep structure which has four fundamental elements physical conditions emotional state cognitive capabilities and social status once a pecs deep structure is constructed and tested as a reference model for real systems or agents various superficial qualities can be imposed on the structure to represent a set of local heterogenous characteristics and better predict the behavior schmidt 2002 2 challenges in modeling agent behavior abm faces several major challenges detailed in appendix 1 and several assessments an et al 2021 a b crooks et al 2008 mcdowall and geels 2017 o sullivan et al 2016 these challenges arise from abm s greater complexity in comparison to traditional models sun et al 2016 which is the price paid for abm s superior flexibility and capacity to capture the corresponding processes or mechanisms filatova et al 2013 abms tend to be data hungry and difficult to understand common solutions deployed to date include simplifying assumptions theoretical representation of processes and inverse parameterization using sets of observed patterns among all challenges that abm faces we highlight the seriousness of two related problems equifinality and multifinality abm suffers from these two problems although we acknowledge that they are not endemic to abm but involve any mechanistic modeling efforts the equifinality problem may blind the true pathway or mechanistic process that generates the observed macro level pattern or outcome because the end state can be reproduced through multiple pathways von bertalanffy 1968 for example the prisoner s dilemma can be related to several seemingly plausible mechanisms including group selection di tosto et al 2007 strong reciprocity boyd et al 2003 tit for tat retaliation axelrod 1997 and others conte and paolucci 2014 while equifinality may be considered a curse by way of the processes involved in the outcome it may also be viewed as a blessing providing a means to explore alternative explanatory pathways and falsification of existing theories such exploration is facilitated when data at multiple spatial temporal or organizational scales are available to filter out those theories that cannot explain all patterns simultaneously the pattern oriented modeling framework grimm et al 2005 a related challenge is the multifinality problem in which the same causes and or starting conditions give rise to very different trajectories or ultimate outcomes this problem may arise from uncertainties in key processes and or parameters a related challenge is that verbally formulated theories of agent behavior often leave too much room for interpretation when it comes to formalize them in an abm so that different implementations of the same theory can lead to different results and conclusions muelder and filatova 2018 to demonstrate abm s unmatched potential to provide alternative pathways or mechanisms for explaining or predicting observed patterns as well as the need to handle the equifinality problem we provide an exemplar abm that aims at understanding the dynamics of firms in the u s axtell 2001 2015 the abm with absence of many parameters and assumptions included in traditional economic models is sufficient to endogenously produce the kinds of macro pattern dynamics in firm sizes ages growth rates job tenure wages networks and so on that agree with empirical data appendix 2 this seemingly successful abm though providing unique useful mechanisms can still be questioned due to the equifinality problem to break new ground for improving the science and art of modeling agent decision and behavior approaches for systematic abm development and testing such as the mohub schlüter et al 2017a and pom grimm et al 2005 frameworks have been suggested in addition we have developed a set of guidelines for modelers and reviewers and for novices an grimm sullivan turner ii et al 2021b which include a comparison of commonly used abm toolkits and software packages an grimm sullivan turner ii et al 2021b given the existence of 85 platforms or toolkits for abm abar et al 2017 also we promote more effective abm education and communication through several means such as developing and sharing curricula promoting the reusability of abm modules e g abm lego or mr potatohead pieces and engaging a broader abm community in collaborative education efforts an grimm sullivan turner ii et al 2021b these and other endeavors though useful have their own limitations we therefore propose a new pathway for modeling agent decisions and behavior which is based on artificial intelligence ai and data science over the last decade or so ai data science and their applications for abm have led to a critical mass of tools applications and insights so that the potential of this new pathway has become clearly visible this paper focuses on data collection processing and methods or algorithms that support modelling decisions and behavior recognizing the importance of other important issues such as verification of the mechanisms or rules thus derived 3 opportunities from artificial intelligence and machine learning traditional artificial intelligence leverages machines to understand and mimic human intelligence machine learning an essential element of artificial intelligence can be as simple as standard linear regression models on the other hand machine learning can leverage more advanced models and reveal non linear complex processes through for example neural networks genetic algorithms decision trees naive bayes and bayesian networks for instance the data driven agent based crowd model by tan et al 2019 adopts a standard differential evolution genetic algorithm to calibrate model parameters e g pedestrian speed turn angle based on video and virtual reality vr experiment data in a study that features a data driven agent based model taghikhah et al 2022 machine learning is leveraged to identify automatically the causal relationships and derive decision rules for agents from microdata on behavior furthermore machine learning can be employed to detect patterns in model output which may help to evaluate the robustness of the model below we focus on illustrating the usefulness of neural networks inspired by the structure of human and animal brains neural networks have emerged as one of the most versatile algorithms in machine learning neural networks are increasingly employed in consequential decision making processes in many domains such as banking medicine and criminal justice by 2030 artificial intelligence could boost the global economy by 15 7 trillion which includes massive decisions made by neural networks west and allen 2018 the huge explosion of neural networks presents an unparalleled opportunity to augment individual human life learning intelligence and productivity a neural network consists of layers of nodes that are connected by links here nodes may be interpreted differently which may be analogous to agents in the context of complex systems variables or decision points abdulkareem et al 2019 while links could be agent agent or agent environment relationships cranmer et al 2020 kipf and welling 2016 as input data are fed into the machine learning algorithm s nodes receive messages from parent sending nodes and pass messages to their child receiving nodes depending on whether some conditions are met with sufficient data and an appropriate model structure the trained models can offer high predictive power offering significant opportunities to calibrate and or validate abms for instance modelers can assign and implement each agent with its own unique regression equation or neural network h zhang et al 2016 then the process of understanding and envisioning agent behavior entails optimizing the regression equations or neural networks for all the agents see the example below models trained in this way the behavior rules of agents in particular are relatively rare for many reasons such as the difficulty of independently training a large number of convolutional neural networks another critical issue concerning neural networks centers on the difficulty of interpretation such models are often like a black box offering little or no understanding of the mechanisms governing the processes below we propose a reinforcement learning rl plus convolutional neural network cnn based approach i e rl cnn approach to equip agents with the intelligence of self uncovering and self learning behavior mechanisms instead of relying on the modeler to hardcode w zhang et al 2021 behavioral rules beforehand among the three ways machine learning can contribute to abm analysis i e prior to running the abm during the running of the abm and post running the abm to analyze abm output abdulkareem et al 2019 the one related to empowering agents to self learn and formulate mechanisms during the running of the abm is most challenging e g computationally intense and promising the most common practice is that abm modelers hardcode agents behavioral rules prior to running abm w zhang et al 2021 in a recent multi agent model integrated with reinforcement learning effective preventive maintenance policies i e rules governing agent actions can be learned directly from data without any knowledge about the environment and maintenance strategies ensuring smooth and efficient production for large scale manufacturing systems su et al 2022 traditional machine learning is powerful in understanding and simulating agents decision making and behavior but tends to suffer from insufficient data and or data handling capabilities gil and selman 2019 to identify the correct model structure and parameters and therefore appropriately calibrate abms srikrishnan and keller 2021 the advent of data science and its methods tools and data infrastructures has powerfully enriched machine learning to derive processes behind patterns of interest verifying or rebutting the underlying hypothetical mechanisms behind such patterns reinforcement learning rl through a certain set of reward and or penalty rules is a promising tool in this regard su et al 2022 specifically rl can be assigned to the agents under investigation with little or no pre knowledge about such mechanisms rl enabled agents can learn the best behavioral rules from data so that the learned rules can maximizes the rl s reward or minimize the penalty when dealing with other agents and the environment one successful rl application is the multi agent reinforcement learning marl buşoniu et al 2010 under which a computer go program called alphago is developed and can beat a human professional player on a full sized board silver et al 2016 recently a newer version called katago can even beat world class human go players edwards 2022 take an example of theorizing from or seeking mechanisms of animal behavioral science as shown in fig 2 we begin with rl without pre knowledge or hypothesis on the mechanisms the term mechanism is often called policy in the machine learning domain as data panel a are used as input to train the rl neural network a built in capacity of each agent panel b the agent s rl neural network can then learn and establish a set of nodes and links which can maximize the reward function with compliance to the state for detail about state see appendix 4 to reveal the thus established yet hidden nodes and links a regression tree panel c can be used to translate them into a set of visible decision tree links arrows in panel c and nodes e g c1 c2 d3 in panel c in turn these nodes and links in the decision tree with the aid of domain knowledge can be used and interpreted as meaningful and understandable mechanisms panel d helping theorize and understand the processes generating the macro patterns e g data in panel a alternatively the above process may start with panel d where we have pre knowledge or hypotheses regarding the mechanisms of interest that need to be verified or polished in this case the rl process starts from both data panel a and such specified mechanisms panel d where the dashed arrow indicates the extra input to train the rl network in panel b all the remaining steps remain the same as above the outcome is that the pre knowledge or hypotheses regarding the mechanisms including parameters and structure may be partially or fully modified according to the nodes and links obtained in the decision tree for instance the parameter 15 km in panel d may be changed to be 20 km and go to lake to stay where it is the above example takes the data for granted which may or may not reflect the actual conditions we may leverage a so called convolutional neural network cnn a data extraction method see appendix 3 for detail to prepare data that are useable in the above rl procedure fig 2 in the above example cnn can be leveraged to identify detect animals based on images from different sources e g gps collars or drone imaging the rl cnn approach though promising and exciting does not imply that ai machine learning and data science are not unbiased nor does it exhaust the potentials that ai and machine learning can contribute to modelling agent behavior first we still emphasize the importance of domain knowledge and theory that are obtained elsewhere taghikhah et al 2022 the mechanism specification in panel d of fig 2 if employed as a starting point for rl network panel b reflects this importance the mechanisms or rules thus derived for example cause effects and feedback loops in many instances should be subject to continued examination by domain knowledge and theory also as new data become available the above rl cnn or other approaches should be continually used to polish or revise existing rules even establish new rules therefore continuous real time data collection is important for not only deriving but also for validating and renewing such rules the concept of digital twins dt is based on this idea of updating in regular intervals the data underlying a realistic model used for forecasting this principle is well known from weather forecast and widely used in industry singh et al 2022 but has also become the basis of large initiatives to support decision making regarding climate ocean and biodiversity such as the destination earth program of the european commission nativi et al 2021 while neural networks and rl are among the most flexible and powerful tools there are many other useful ai and machine learning algorithms for instance it is reported that bayesian networks abdulkareem et al 2019 and artificial neural networks van der hoog 2019 represent viable alternatives for small training datasets such alternatives are illustrated here by an example regarding graph neural networks gnns which have recently emerged to link nodes horizontally and improved predictive tasks in this context a graph is a structure frequently a mathematical function that models pairwise relations between nodes in which all nodes agents are connected by edges or links in one recent application gnn was leveraged to derive successfully the closed form symbolic expression of newton s law of motion based on data from the experiment simply put the machine mining approach can be used to exactly recover newton s formula f g m 1 m 2 r 2 without any previous clue or assumption regarding its form note that f g m1 m2 and r represent the force between particles 1 and 2 the gravitational constant the mass of particle 1 the mass of particle 2 and the distance between the two particles cranmer et al 2020 this success has boosted ai s potential to recover laws or mechanisms in other domains we present a potential way as an example to recover mechanisms or behavioral rules in agent based complex systems see appendix 5 4 opportunities from new forms of data traditional ai s capability to nourish abm rules is also constrained because new forms of data including data in high volumes are either unavailable or too difficult to handle using traditional data processing and analytic methods in applications machine learning will be much more empowered if aided with some non traditional datasets such as big data or qualitative data such challenges are effectively addressed with recent advances in data science big data have several unique features that distinguish them from traditional data largely in terms of huge volume high velocity wide variety variable veracity and value big data are increasingly nourishing quick detection and understanding of processes or patterns in many scientific fields de mauro et al 2016 on the other hand qualitative data could provide essential insights into understanding the above processes or patterns qualitative data take the form of text images videos audio documents and the like yet both big data and qualitative data are very different when compared to such traditional data as census data and survey data marcus 2018 for example in an instance of social sensing analysis of the impacts of disasters c zhang et al 2020 twitter data are used to reveal the dynamic emotions e g disgust fear joy sadness anger and surprise in relation to a hurricane outbreak and related rescue activities in houston tx during august 25 30 2017 numeric emotion scores are derived from tweets describing certain types of events e g help and rescue events or flood control infrastructures these emotion scores expressed as the relative abundance of words related to a certain emotion out of all words can be used to verify or debut related abm rules or outcomes c zhang et al 2020 such data can also help in the above mechanism retrieval steps for instance the emotional scores can help at step 1 see appendix 5 by ruling out some unrealistic functions or at step 3 by casting out unreasonable outcomes and the corresponding functions at step 1 5 conclusion with the advent of the digital industrial revolution new technologies and data forms are exploding in biophysical human anthropocene and many other realms among these artificial intelligence and data science machine learning in particular should be among the top priority areas for future research which will likely bring in revolutionary impacts on the science and technology addressing agent decisions and behaviors in complex systems it must be pointed out that we do not downplay the importance of traditional scientific investigations and the related findings on the contrary the artificial intelligence and data science approach should build on and complement such traditional investigations through for example experimenting fieldwork inductive and deductive reasoning hypothesis testing and theorization and vice versa for instance the data fig 2a and pre knowledge hypotheses fig 2d may come from traditional investigations at the same time it is worth emphasizing the unique potential of this artificial intelligence and data science approach to detecting internal theory relevant mechanisms for instance the links and nodes in the decision tree fig 2c translated from the hidden network fig 2b may reveal unique factors structures e g causal relationships and parameter values fig 2d that would not be imagined and or included in traditional scientific investigations and will likely be used to stimulate formulate new theory development or improve an existing theory we do not intend to say that such factors structures and parameters are completely free of bias and right instead we seek to provide alternative related to traditional scientific investigations thinking and modeling choices therefore these innovative approaches will likely pave unprecedented ways for not only formulating agent behavior mechanisms or rules but also forming new more robust theories or rebutting existing theories thus making equifinality less problematic this approach may also be conducive to better understanding commonalities and differences between theories and addressing the degree of formalization problems schlüter et al 2017b there is abundant literature regarding pathways to uncover or formulate mechanisms or rules behind agent behavior or decisions such as the inverse generative social science vu et al 2019 and the mr potatohead parker et al 2008 frameworks correspondingly there exist a large amount of ai and data science tools algorithms or models we can leverage for good reviews in this regard we refer to w zhang et al 2021 in the context of such literature and tools this paper does not seek to provide a comprehensive review of them instead we aim to call for more attention and efforts towards uncovering agent decision and behavior mechanisms in the light of data science and artificial intelligence towards using and advancing this ai and data science approach barriers may exist for many reasons such as its demanding computational power difficulties in multi and inter disciplinary learning conversing and understanding and coding some vague theoretical frameworks muelder and filatova 2018 however we envision this approach will be increasingly recognized used and advanced in many aeras of research and practical applications related to understanding agent behavior and decision making declaration of competing interest we declare that the authors have no conflicts of interest acknowledgements we are indebted to financial support from the national science foundation nsf through the method measure statistics and geography and spatial sciences bcs 1638446 and the dynamics of integrated socio environmental systems programs bcs 1826839 and deb 1212183 we thank the participants of the abm 17 symposium sponsored by the above nsf grant http complexities org abm17 for input and comments this project has received funding from the european research council erc under the european union s horizon 2020 research and innovation programme grant agreement no 757455 and through an esrc alan turing joint fellowship es r007918 1 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2023 105713 
25387,agent based modeling abm has been widely used in numerous disciplines and practice domains subject to many eulogies and criticisms this article presents key advances and challenges in agent based modeling over the last two decades and shows that understanding agents behaviors is a major priority for various research fields we demonstrate that artificial intelligence and data science will likely generate revolutionary impacts for science and technology towards understanding agent decisions and behaviors in complex systems we propose an innovative approach that leverages reinforcement learning and convolutional neural networks to equip agents with the intelligence of self learning their behavior rules directly from data we call for further developments of abm especially modeling agent behaviors in the light of data science and artificial intelligence keywords agent based modeling modeling agent decisions and actions artificial intelligence machine learning data science abbreviations abm agent base modeling ai artificial intelligence rl reinforcement learning gnn graph neural networks cnn convolutional neural network data availability no data were used for the research described in the article although agent based modeling abm abms for agent based models emerged as early as the 1970s schelling 1971 or even earlier w zhang et al 2021 it has been extensively applied in ecology where it is usually referred to as individual based modeling grimm 1999 and numerous other disciplines since the 1990s vincenot 2018 subsequently abm has exploded in applications fig 1 an indication of its usefulness across multiple sciences milestones in abm development include the overview design concepts and details odd protocol for the model documentation grimm et al 2010 2020 and the pattern oriented modeling pom paradigm grimm et al 2005 abm received a major institutional endorsement in 2001 when it was featured by the u s national academy of sciences sackler colloquium and the resultant special issue in the proceedings of the national academy of sciences bonabeau 2002 since then abm has been hailed with both enthusiasm and optimism because of its potential to create a revolution among the social ecological behavioral and complexity sciences yet progress has been slower than initially anticipated in several critical areas of development and application grimm and berger 2016 lorscheid et al 2019 leading to various criticisms of abm couclelis 2002 roughgarden 2012 this slower progress likely leading to the fade of enthusiasm in abm is embedded in the context of a big issue for abm or any process based modeling effort the need to balance 1 the pattern informed top down approach which reproduces macro level patterns without adequate explanatory power such as the well reproduced flight patterns of hawks without an explanation of the mechanistic processes behind the patterns conte and paolucci 2014 and 2 the theory driven approach which aims at generating macro patterns from bottom up processes abm developers aim to not only accurately predict or replicate the observed patterns in question but also to understand and explain the mechanisms behind such patterns in this paper we propose a third approach based on artificial intelligence ai and data science to detect formulate and test mechanistic processes e g structures rules parameters that complement the above two traditional approaches the paper first briefly reviews historic advances in modeling human behavior followed by an overview of the challenges of modeling agent human behavior next we show how artificial intelligence and machine learning in combination with data science can help reveal mechanistic processes and model agent human behavior in the appendix we provide more details about the most promising methods in relation to what we propose in this paper 1 background about modeling agent human behavior in abm historically modeling agent human decision making and behavior in abm was largely based on economic theories such as benefit maximization or cost minimization by rational actors and largely ignored other approaches such as those in psychology and neurology groeneveld et al 2017 advances in relevant social and cognitive sciences have greatly enhanced the capacity of abm to model agent human in particular behavior filatova et al 2013 niamir et al 2020 generating normative models cognitive models and psychologically especially emotional models and neurologically inspired models that are instrumental for understanding and modeling human behavior for excellent review papers see balke and gilbert 2014 huber et al 2018 and bourgais et al 2018 along this thread one prominent example is the belief desire intentions bdi framework inspired by logical and psychological principles e g michael bratman s theory of human practical reasoning bratman 1999 the bdi framework models practical reasoning and subsequent action of resource bounded rational agents these agents carry 1 beliefs which are facts agents believe about the environment 2 desires which involve desired end state or goals to achieve and 3 intentions which are intended commitments to accomplish the corresponding desires goals under this framework intentions are important elements and precursors of planned actions considered an improvement compared with the bdi framework the physical emotional cognitive and social factors pecs conte paolucci 2014 schmidt 2002 framework aims to explain or predict human behavior from a common deep structure which has four fundamental elements physical conditions emotional state cognitive capabilities and social status once a pecs deep structure is constructed and tested as a reference model for real systems or agents various superficial qualities can be imposed on the structure to represent a set of local heterogenous characteristics and better predict the behavior schmidt 2002 2 challenges in modeling agent behavior abm faces several major challenges detailed in appendix 1 and several assessments an et al 2021 a b crooks et al 2008 mcdowall and geels 2017 o sullivan et al 2016 these challenges arise from abm s greater complexity in comparison to traditional models sun et al 2016 which is the price paid for abm s superior flexibility and capacity to capture the corresponding processes or mechanisms filatova et al 2013 abms tend to be data hungry and difficult to understand common solutions deployed to date include simplifying assumptions theoretical representation of processes and inverse parameterization using sets of observed patterns among all challenges that abm faces we highlight the seriousness of two related problems equifinality and multifinality abm suffers from these two problems although we acknowledge that they are not endemic to abm but involve any mechanistic modeling efforts the equifinality problem may blind the true pathway or mechanistic process that generates the observed macro level pattern or outcome because the end state can be reproduced through multiple pathways von bertalanffy 1968 for example the prisoner s dilemma can be related to several seemingly plausible mechanisms including group selection di tosto et al 2007 strong reciprocity boyd et al 2003 tit for tat retaliation axelrod 1997 and others conte and paolucci 2014 while equifinality may be considered a curse by way of the processes involved in the outcome it may also be viewed as a blessing providing a means to explore alternative explanatory pathways and falsification of existing theories such exploration is facilitated when data at multiple spatial temporal or organizational scales are available to filter out those theories that cannot explain all patterns simultaneously the pattern oriented modeling framework grimm et al 2005 a related challenge is the multifinality problem in which the same causes and or starting conditions give rise to very different trajectories or ultimate outcomes this problem may arise from uncertainties in key processes and or parameters a related challenge is that verbally formulated theories of agent behavior often leave too much room for interpretation when it comes to formalize them in an abm so that different implementations of the same theory can lead to different results and conclusions muelder and filatova 2018 to demonstrate abm s unmatched potential to provide alternative pathways or mechanisms for explaining or predicting observed patterns as well as the need to handle the equifinality problem we provide an exemplar abm that aims at understanding the dynamics of firms in the u s axtell 2001 2015 the abm with absence of many parameters and assumptions included in traditional economic models is sufficient to endogenously produce the kinds of macro pattern dynamics in firm sizes ages growth rates job tenure wages networks and so on that agree with empirical data appendix 2 this seemingly successful abm though providing unique useful mechanisms can still be questioned due to the equifinality problem to break new ground for improving the science and art of modeling agent decision and behavior approaches for systematic abm development and testing such as the mohub schlüter et al 2017a and pom grimm et al 2005 frameworks have been suggested in addition we have developed a set of guidelines for modelers and reviewers and for novices an grimm sullivan turner ii et al 2021b which include a comparison of commonly used abm toolkits and software packages an grimm sullivan turner ii et al 2021b given the existence of 85 platforms or toolkits for abm abar et al 2017 also we promote more effective abm education and communication through several means such as developing and sharing curricula promoting the reusability of abm modules e g abm lego or mr potatohead pieces and engaging a broader abm community in collaborative education efforts an grimm sullivan turner ii et al 2021b these and other endeavors though useful have their own limitations we therefore propose a new pathway for modeling agent decisions and behavior which is based on artificial intelligence ai and data science over the last decade or so ai data science and their applications for abm have led to a critical mass of tools applications and insights so that the potential of this new pathway has become clearly visible this paper focuses on data collection processing and methods or algorithms that support modelling decisions and behavior recognizing the importance of other important issues such as verification of the mechanisms or rules thus derived 3 opportunities from artificial intelligence and machine learning traditional artificial intelligence leverages machines to understand and mimic human intelligence machine learning an essential element of artificial intelligence can be as simple as standard linear regression models on the other hand machine learning can leverage more advanced models and reveal non linear complex processes through for example neural networks genetic algorithms decision trees naive bayes and bayesian networks for instance the data driven agent based crowd model by tan et al 2019 adopts a standard differential evolution genetic algorithm to calibrate model parameters e g pedestrian speed turn angle based on video and virtual reality vr experiment data in a study that features a data driven agent based model taghikhah et al 2022 machine learning is leveraged to identify automatically the causal relationships and derive decision rules for agents from microdata on behavior furthermore machine learning can be employed to detect patterns in model output which may help to evaluate the robustness of the model below we focus on illustrating the usefulness of neural networks inspired by the structure of human and animal brains neural networks have emerged as one of the most versatile algorithms in machine learning neural networks are increasingly employed in consequential decision making processes in many domains such as banking medicine and criminal justice by 2030 artificial intelligence could boost the global economy by 15 7 trillion which includes massive decisions made by neural networks west and allen 2018 the huge explosion of neural networks presents an unparalleled opportunity to augment individual human life learning intelligence and productivity a neural network consists of layers of nodes that are connected by links here nodes may be interpreted differently which may be analogous to agents in the context of complex systems variables or decision points abdulkareem et al 2019 while links could be agent agent or agent environment relationships cranmer et al 2020 kipf and welling 2016 as input data are fed into the machine learning algorithm s nodes receive messages from parent sending nodes and pass messages to their child receiving nodes depending on whether some conditions are met with sufficient data and an appropriate model structure the trained models can offer high predictive power offering significant opportunities to calibrate and or validate abms for instance modelers can assign and implement each agent with its own unique regression equation or neural network h zhang et al 2016 then the process of understanding and envisioning agent behavior entails optimizing the regression equations or neural networks for all the agents see the example below models trained in this way the behavior rules of agents in particular are relatively rare for many reasons such as the difficulty of independently training a large number of convolutional neural networks another critical issue concerning neural networks centers on the difficulty of interpretation such models are often like a black box offering little or no understanding of the mechanisms governing the processes below we propose a reinforcement learning rl plus convolutional neural network cnn based approach i e rl cnn approach to equip agents with the intelligence of self uncovering and self learning behavior mechanisms instead of relying on the modeler to hardcode w zhang et al 2021 behavioral rules beforehand among the three ways machine learning can contribute to abm analysis i e prior to running the abm during the running of the abm and post running the abm to analyze abm output abdulkareem et al 2019 the one related to empowering agents to self learn and formulate mechanisms during the running of the abm is most challenging e g computationally intense and promising the most common practice is that abm modelers hardcode agents behavioral rules prior to running abm w zhang et al 2021 in a recent multi agent model integrated with reinforcement learning effective preventive maintenance policies i e rules governing agent actions can be learned directly from data without any knowledge about the environment and maintenance strategies ensuring smooth and efficient production for large scale manufacturing systems su et al 2022 traditional machine learning is powerful in understanding and simulating agents decision making and behavior but tends to suffer from insufficient data and or data handling capabilities gil and selman 2019 to identify the correct model structure and parameters and therefore appropriately calibrate abms srikrishnan and keller 2021 the advent of data science and its methods tools and data infrastructures has powerfully enriched machine learning to derive processes behind patterns of interest verifying or rebutting the underlying hypothetical mechanisms behind such patterns reinforcement learning rl through a certain set of reward and or penalty rules is a promising tool in this regard su et al 2022 specifically rl can be assigned to the agents under investigation with little or no pre knowledge about such mechanisms rl enabled agents can learn the best behavioral rules from data so that the learned rules can maximizes the rl s reward or minimize the penalty when dealing with other agents and the environment one successful rl application is the multi agent reinforcement learning marl buşoniu et al 2010 under which a computer go program called alphago is developed and can beat a human professional player on a full sized board silver et al 2016 recently a newer version called katago can even beat world class human go players edwards 2022 take an example of theorizing from or seeking mechanisms of animal behavioral science as shown in fig 2 we begin with rl without pre knowledge or hypothesis on the mechanisms the term mechanism is often called policy in the machine learning domain as data panel a are used as input to train the rl neural network a built in capacity of each agent panel b the agent s rl neural network can then learn and establish a set of nodes and links which can maximize the reward function with compliance to the state for detail about state see appendix 4 to reveal the thus established yet hidden nodes and links a regression tree panel c can be used to translate them into a set of visible decision tree links arrows in panel c and nodes e g c1 c2 d3 in panel c in turn these nodes and links in the decision tree with the aid of domain knowledge can be used and interpreted as meaningful and understandable mechanisms panel d helping theorize and understand the processes generating the macro patterns e g data in panel a alternatively the above process may start with panel d where we have pre knowledge or hypotheses regarding the mechanisms of interest that need to be verified or polished in this case the rl process starts from both data panel a and such specified mechanisms panel d where the dashed arrow indicates the extra input to train the rl network in panel b all the remaining steps remain the same as above the outcome is that the pre knowledge or hypotheses regarding the mechanisms including parameters and structure may be partially or fully modified according to the nodes and links obtained in the decision tree for instance the parameter 15 km in panel d may be changed to be 20 km and go to lake to stay where it is the above example takes the data for granted which may or may not reflect the actual conditions we may leverage a so called convolutional neural network cnn a data extraction method see appendix 3 for detail to prepare data that are useable in the above rl procedure fig 2 in the above example cnn can be leveraged to identify detect animals based on images from different sources e g gps collars or drone imaging the rl cnn approach though promising and exciting does not imply that ai machine learning and data science are not unbiased nor does it exhaust the potentials that ai and machine learning can contribute to modelling agent behavior first we still emphasize the importance of domain knowledge and theory that are obtained elsewhere taghikhah et al 2022 the mechanism specification in panel d of fig 2 if employed as a starting point for rl network panel b reflects this importance the mechanisms or rules thus derived for example cause effects and feedback loops in many instances should be subject to continued examination by domain knowledge and theory also as new data become available the above rl cnn or other approaches should be continually used to polish or revise existing rules even establish new rules therefore continuous real time data collection is important for not only deriving but also for validating and renewing such rules the concept of digital twins dt is based on this idea of updating in regular intervals the data underlying a realistic model used for forecasting this principle is well known from weather forecast and widely used in industry singh et al 2022 but has also become the basis of large initiatives to support decision making regarding climate ocean and biodiversity such as the destination earth program of the european commission nativi et al 2021 while neural networks and rl are among the most flexible and powerful tools there are many other useful ai and machine learning algorithms for instance it is reported that bayesian networks abdulkareem et al 2019 and artificial neural networks van der hoog 2019 represent viable alternatives for small training datasets such alternatives are illustrated here by an example regarding graph neural networks gnns which have recently emerged to link nodes horizontally and improved predictive tasks in this context a graph is a structure frequently a mathematical function that models pairwise relations between nodes in which all nodes agents are connected by edges or links in one recent application gnn was leveraged to derive successfully the closed form symbolic expression of newton s law of motion based on data from the experiment simply put the machine mining approach can be used to exactly recover newton s formula f g m 1 m 2 r 2 without any previous clue or assumption regarding its form note that f g m1 m2 and r represent the force between particles 1 and 2 the gravitational constant the mass of particle 1 the mass of particle 2 and the distance between the two particles cranmer et al 2020 this success has boosted ai s potential to recover laws or mechanisms in other domains we present a potential way as an example to recover mechanisms or behavioral rules in agent based complex systems see appendix 5 4 opportunities from new forms of data traditional ai s capability to nourish abm rules is also constrained because new forms of data including data in high volumes are either unavailable or too difficult to handle using traditional data processing and analytic methods in applications machine learning will be much more empowered if aided with some non traditional datasets such as big data or qualitative data such challenges are effectively addressed with recent advances in data science big data have several unique features that distinguish them from traditional data largely in terms of huge volume high velocity wide variety variable veracity and value big data are increasingly nourishing quick detection and understanding of processes or patterns in many scientific fields de mauro et al 2016 on the other hand qualitative data could provide essential insights into understanding the above processes or patterns qualitative data take the form of text images videos audio documents and the like yet both big data and qualitative data are very different when compared to such traditional data as census data and survey data marcus 2018 for example in an instance of social sensing analysis of the impacts of disasters c zhang et al 2020 twitter data are used to reveal the dynamic emotions e g disgust fear joy sadness anger and surprise in relation to a hurricane outbreak and related rescue activities in houston tx during august 25 30 2017 numeric emotion scores are derived from tweets describing certain types of events e g help and rescue events or flood control infrastructures these emotion scores expressed as the relative abundance of words related to a certain emotion out of all words can be used to verify or debut related abm rules or outcomes c zhang et al 2020 such data can also help in the above mechanism retrieval steps for instance the emotional scores can help at step 1 see appendix 5 by ruling out some unrealistic functions or at step 3 by casting out unreasonable outcomes and the corresponding functions at step 1 5 conclusion with the advent of the digital industrial revolution new technologies and data forms are exploding in biophysical human anthropocene and many other realms among these artificial intelligence and data science machine learning in particular should be among the top priority areas for future research which will likely bring in revolutionary impacts on the science and technology addressing agent decisions and behaviors in complex systems it must be pointed out that we do not downplay the importance of traditional scientific investigations and the related findings on the contrary the artificial intelligence and data science approach should build on and complement such traditional investigations through for example experimenting fieldwork inductive and deductive reasoning hypothesis testing and theorization and vice versa for instance the data fig 2a and pre knowledge hypotheses fig 2d may come from traditional investigations at the same time it is worth emphasizing the unique potential of this artificial intelligence and data science approach to detecting internal theory relevant mechanisms for instance the links and nodes in the decision tree fig 2c translated from the hidden network fig 2b may reveal unique factors structures e g causal relationships and parameter values fig 2d that would not be imagined and or included in traditional scientific investigations and will likely be used to stimulate formulate new theory development or improve an existing theory we do not intend to say that such factors structures and parameters are completely free of bias and right instead we seek to provide alternative related to traditional scientific investigations thinking and modeling choices therefore these innovative approaches will likely pave unprecedented ways for not only formulating agent behavior mechanisms or rules but also forming new more robust theories or rebutting existing theories thus making equifinality less problematic this approach may also be conducive to better understanding commonalities and differences between theories and addressing the degree of formalization problems schlüter et al 2017b there is abundant literature regarding pathways to uncover or formulate mechanisms or rules behind agent behavior or decisions such as the inverse generative social science vu et al 2019 and the mr potatohead parker et al 2008 frameworks correspondingly there exist a large amount of ai and data science tools algorithms or models we can leverage for good reviews in this regard we refer to w zhang et al 2021 in the context of such literature and tools this paper does not seek to provide a comprehensive review of them instead we aim to call for more attention and efforts towards uncovering agent decision and behavior mechanisms in the light of data science and artificial intelligence towards using and advancing this ai and data science approach barriers may exist for many reasons such as its demanding computational power difficulties in multi and inter disciplinary learning conversing and understanding and coding some vague theoretical frameworks muelder and filatova 2018 however we envision this approach will be increasingly recognized used and advanced in many aeras of research and practical applications related to understanding agent behavior and decision making declaration of competing interest we declare that the authors have no conflicts of interest acknowledgements we are indebted to financial support from the national science foundation nsf through the method measure statistics and geography and spatial sciences bcs 1638446 and the dynamics of integrated socio environmental systems programs bcs 1826839 and deb 1212183 we thank the participants of the abm 17 symposium sponsored by the above nsf grant http complexities org abm17 for input and comments this project has received funding from the european research council erc under the european union s horizon 2020 research and innovation programme grant agreement no 757455 and through an esrc alan turing joint fellowship es r007918 1 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2023 105713 
25388,computer based coupled soilscape landform evolution models provide an avenue for estimating the erosion potential of a landform over geological timescales they also provide tools to understand soil landform relationships in the context of both field and modelled pedogenesis processes in this study we used the state space soil production and assessment model ssspam and caesar lisflood landform evolution model to simulate the evolution of a natural catchment where soil erosion rates have been closely monitored over the last 20 years both models predict comparable erosion rates to that of field measurements and produce similar catchment scale geomorphological patterns rock content data measured in the field along two transects were compared with ssspam simulations at the same representative points the results showed that ssspam produced similar rock content variation along the respective transects the findings demonstrate ssspam s ability to predict catchment scale erosion and surface soil distribution the insights demonstrated here provide confidence in the ssspam model for soilscape predictions for natural agricultural and post mining landform assessment graphical abstract image 1 keywords sediment transport landform evolution model soil evolution caesar lisflood ssspam geomorphology natural catchment data availability data will be made available on request 1 introduction landforms are the product of various physical and chemical processes acting on the earth s crust for extended time periods young and mcdougall 1993 changes in global processes also change the properties of landforms cooke 1980 coulthard 2001 phillips 2006 vandenberghe 2003 however directly linking a change in a landform to a particular process is extremely difficult due to competing influences of climate tectonics and biological activity involved in landform evolution across a range of both spatial and temporal scales computer based landform evolution models are one method of overcoming these spatial and temporal barriers merritt et al 2003 they provide a tool to gain a deeper understanding of the underlying factors influencing the landform evolution process hancock et al 2019 tucker and hancock 2010 using such models it is possible to predict the evolutionary path of landforms that will arise in the future due to changing global processes such as climate dietrich et al 2003 landform evolution models have extensive applications in the mining industry where they are being used to assess the erosional stability of post mining landforms evans 2000 howard et al 2011 martin duque et al 2021 there are several landform evolution models formulated throughout the years such as siberia willgoose et al 1991a 1991b caesar coulthard et al 2002 child tucker et al 2001 milesd vanwalleghem et al 2013 and ssspam welivitiya et al 2019 natural catena scale landforms are at or near equilibrium conditions renwick 1992 where the evolution of the landform is very slow there have been few attempts to numerically analyse the evolutionary dynamics of natural catena scale landforms cohen et al 2009 parker 1999 willgoose and sharmeen 2006 coupled soilscape landform evolution modelling of natural catchments can produce important insights into the evolution of soil that could influence the ecological productivity of the landform de alba et al 2004 hancock et al 2010 together with insights into relationships between the soil characteristics and geomorphology of a landform ma et al 2019 welivitiya et al 2016 although these landform soilscape evolution modelling studies have produced plausible results there is uncertainty regarding the accuracy of these model predictions the reason behind this uncertainty is the lack of field data that can be compared with the model results to assess their accuracy skinner et al 2018 wong et al 2021 the aim of this study is to evaluate the ssspam coupled soilscape landform evolution model s performance in simulating the evolution of a natural catchment scale landform and compare the simulation results with field data and results from a more established landform evolution model the state space soil production assessment model ssspam is coupled soilscape landform evolution model capable of simultaneously evolving the landform and the soil profile welivitiya 2017 welivitiya et al 2016 in ssspam the state space matrix approach is used to represent physical processes of pedogenesis such as erosion and armouring diffusion sediment deposition and weathering ssspam can characterise the particle size distribution at any point at any modelled soil depth at any time it also tracks the particle size distribution of the entrained sediment in the flow ssspam can also operate at different time steps ranging from days to years allowing it to model long term landform evolution based on annual rainfall averages or simulate erosion caused by short term storm events ssspam also can simulate additional pedogenesis processes such as soil armouring and weathering compared to existing landform evolution models these capabilities make ssspam a suitable platform for analysing evolutionary dynamics and soil characteristics geomorphology relationships in natural catena scale landforms in a previous study welivitiya and hancock 2022 used the ssspam soilscape landform evolution model to simulate the evolution of gullies on a mining waste rock repository they showed that after site specific parameter calibration ssspam was able to predict the geomorphological development of gullies in the study site with reasonable accuracy by comparing the geomorphological attributes of simulated gullies to those observed in the field although ssspam has been shown to accurately predict the evolutionary dynamics of manufactured landforms such as post mining structures its performance in simulating natural catena scale landforms has not been evaluated that is erosion rates predicted by ssspam can be compared with field measured erosion rates however this comparison does not allow us to validate the predicted dynamics of the landform evolution i e we can evaluate the end results but are unable to determine how we got there this is because data on historical erosion rates and catchment geomorphology is largely unavailable which would facilitate the comparison of model predictions to the natural environment one method to evaluate the dynamic performance of ssspam is to compare model results with other well established models here we use ssspam and caesar lisflood erosion models to simulate the evolution of a natural catchment in western arnhem land northern territory there were 2 considerations for selecting the caesar lisflood model for this study the first selection consideration was the compatibility of the model formulation similar to ssspam caesar lisflood uses variable rainfall time series data to drive the erosion model both models use particle size distribution data to represent the soil profile and are able to account for erosion reducing processes such as surface armouring the second selection consideration was the availability of parameters required to run the models in the study area in a previous study coulthard 2019 calibrated caesar lisflood parameters for the study site using runoff erosion data collected by evans and proudfoot 2017 similarities and differences between ssspam and caesar lisflood models are shown in table 1 in this study simulation results from the ssspam and caesar lisflood models are compared to each other and with field determined soil erosion values ssspam simulation results were further analysed to identify any linkage between surface soil characteristics median particle size diameter d50 and rock content particle size 2 mm and geomorphological attributes slope and contributing area furthermore field measured rock content data was compared with the ssspam simulated rock content data to assess the predictive accuracy of ssspam in the context of surface soil evolution 2 study site the site tin camp creek tcc is a natural catchment that is a part of the alligator river system located in western arnhem land northern territory australia the site lies in the myra falls inlier in lover member cahill formation and the area is tectonically inactive needham and stuart smith 1980 the geology of the catchment consists of metamorphosed schists located in the wet dry tropics of northern australia with an annual mean rainfall of 1400 mm most precipitation occurs as high intensity tropical storms during the wet season which lasts from november to april with fluvial erosion as the dominant erosion process in this region saynor et al 2004 for this study a representative sub catchment with an approximate area of 50 ha was selected from the greater tcc fig 1 the selected sub catchment consists of closely dissected short steep slopes 10 100 m long with slope gradients between 15 and 50 hancock et al 2010 the soils in the study sub catchment consist of red loamy earths and shallow gravely loam with some micaceous silty yellow earths and minor solodic soils on alluvial flats riley and williams 1991 the surface slopes and hill crests are covered with gravelly cobble quartz lag the study sub catchment is covered with open dry sclerophyll forests dominated by eucalyptus and acacia species mixed with other native vegetation story et al 1976 in addition the low lying riparian areas are populated by heteropogon contortus and sorghum spp mixed with some melaleuca spp and pandanus spiralus during the dry season the vegetation cover is often reduced by fire leading to high fluvial erosion rates immediately after bushfires saynor et al 2004 during the early part of the wet season there is vigorous growth of annual grasses which falls over during the latter part of the wet season producing a thick mulch that significantly reduces erosion of exposed soil hancock et al 2010 various methods have established erosion and denudation rates of the study region net soil redistribution rates between 2 and 13 t ha 1year 1 0 013 0 86 mm year 1 were established through an assessment employing the fallout environmental radioisotope cesium 137 137cs hancock et al 2008 these measured erosion rates are comparable with the overall denudation rates for the area 0 01 to 0 04 mm year 1 and 0 062 0 088 mm year 1 determined using stream sediment data from a range of catchments of different sizes in the general region cull et al 1992 erskine and saynor 2000 hancock et al 2008 also estimated an erosion rate of 14 t ha 1year 1 1 mm year 1 for the lower hillslopes during a two year study that used erosion pins located at the base of hillslopes erosion estimates using revised universal soil loss equation rusle with site specific parameters parameter values derived from field data collected from the area produced erosion rates of 10 t ha 1year 1 0 67 mm year 1 this area is tectonically inactive and has not been disturbed by european activity hancock et al 2002b as a result it has been used to validate previously developed landform evolution models hancock et al 2002b 2010 hancock 2003 willgoose et al 2003b in addition the tin camp creek catchment has been used to calibrate siberia parameters to assess the stability and evolution of the erarm post mining landform for ranger uranium mine rehabilitation evans 2000 evans and willgoose 2000 willgoose and riley 1998 3 ssspam coupled soilscape landform evolution model ssspam is a coupled soilscape landform evolution model developed by combining a pedogenesis model and a landform evolution model welivitiya et al 2019 2021 it can simulate fluvial erosion and armouring diffusive erosion surface sediment deposition and weathering within the soil profile ssspam uses several layers to simulate the soil profile enabling it to model the evolution of the soil profile ssspam can describe the particle size distribution of the entire soil profile at each node pixel at any modelled soil depth while tracing the eroded sediment grading distribution in the water flow the operational flow diagram identifying model inputs and outputs is shown in fig 2 the potential fluvial erosion rate in ssspam is calculated based on discharge per unit width and slope in this calculation both detachment and transport limited erosion are considered welivitiya et al 2016 the potential rate of fluvial erosion per unit width m3 s 1m 1 is given by 1 e f k e q α 1 s α 2 t where e is the edibility factor q is discharge per unit width m3 s 1m 1 s is the slope α1 and α 2 are exponents governing the erosion process and t is the length of time step used s the parameter k is a factor that determines the maximum volume of erosion that could potentially occur from a particular pixel depending on the surface soil grading and the threshold entrainment diameter ted ted is the maximum particle diameter that the overland water flow could entrain it is calculated using shield s shear stress criteria ted is given by 2 2 t e d c t h n q 0 6 s 0 7 where c t h is a parameter that needs to be calibrated and n is the manning friction factor soil particles residing on the surface with diameters less than the ted can erode while particles larger than ted remain on the surface leading to armouring in ssspam discharge per unit width q is calculated using the runoff excess generation factor r m s 1 and the contributing area of the pixel a m2 and flow width typically equal to the dem resolution w f m as 3 q r a w f the selective entrainment of soil particles through erosion leads to armouring of the soil surface characterisation of this size selectivity of the erosion process is achieved using the erosion transition matrix a ted mentioned earlier is used to generate the entries of erosion transition matrix a 4 a k k a d k m g k v f o r k m 0 f o r k m where d k is the mean diameter of particle size class k m and v are exponent factors for the mean diameter of particle size class and grading fraction g k respectively a is a scaling factor that governs the erosion from each grading class m is the grading class that the ted falls into i e the largest particle that the water flow can entrain ssspam uses a simple diffusion model to determine the diffusive erosion e d where 5 e d d s f o r s d s x 3 s t h s s t h x 2 f o r s d s x 3 s t h where d m3 s 1 m 1 width is diffusivity s is the slope s th is the maximum slope threshold for the modelled material i e the maximum slope value the soil material can withstand without soil mass movement diffusion is modelled as a process dependent only on material availability without any considerations for size selectivity i e particle size distribution of the diffused material is the same as the soil surface ssspam uses a fragmentation mechanism and a depth dependent weathering rate function to simulate physical weathering the fragmentation model determines the geometry of the weathered particles while the depth depend weathering rate function determines the rate at which particles fragment in each soil layer if a spherical parent particle of diameter d m breaks into a daughter particle with diameters d 1 m accounting for α fraction and n 1 daughter particles with diameter d 2 m the sizes of the daughter particles can be determined in terms of the parent particle diameter as 6 d 1 α 1 3 d 7 d 2 1 α n 1 1 3 d in this study a symmetric fragmentation mechanism was used as the weathering mechanism where a single parent particle disintegrates into two identical daughter particles n 2 0 and α 0 5 in this study the exponential depth dependent weathering function hereafter mentioned as exponential function humphreys and wilkinson 2007 was used which is given by 8 w h β e δ 1 h where w h is the weathering rate at the soil layer at a depth of h m below the surface the constant β determines the maximum value w h can take here we used β 1 and δ 1 is the depth scaling factor we used δ 1 1 738 in the exponential function the highest weathering rate is at the soil surface and the weathering rate decreases exponentially with increasing soil depth ssspam tracks the sediment eroded from upstream areas and is used to calculate the erosion or deposition at any given pixel in the case of deposition the total deposition mass and the psd of the deposited material is determined using the deposition mass vector and the deposition transition matrix a quantity called the critical immersion depth is used to calculate the entries of the deposition transition matrix the critical immersion depth is referred to the vertical distance a particle will fall through the water column with the settling velocity while moving the horizontal distance of pixel width with the flow velocity at the pixel assuming all the sediment particles are distributed equally throughout the water column at a pixel the critical immersion depth is greater for larger particles and less for smaller particles leading to size selective deposition further details of the formulation and implementation of the deposition module in ssspam can be found in welivitiya 2017 and welivitiya et al 2019 more information on the weathering model available in ssspam and its implementation can be found elsewhere welivitiya 2017 welivitiya et al 2016 welivitiya et al 2019 4 caesar lisflood landscape and erosion model the landform in caesar lisflood is represented using a regular grid of elevation values other information such as sediment grading vegetation parameters and hydrological parameters discharge infiltration pertaining to each node is stored in the same grid structure according to three primary considerations all these parameters are changed at every model iteration these are hydraulic routing fluvial erosion and deposition and diffusive slope processes hancock et al 2011 surface and subsurface discharge are modelled using hourly rainfall as input through a modified version of topmodel beven and kirkby 1979 a modified adaptation of manning s equation is used to calculate the flow depth using the discharge rate produced by the inbuilt hydrological model the equation for flow depth calculation is 9 d q n s 1 2 3 5 where d is flow depth m q is the discharge m3 s n is manning s coefficient and s is the slope m m once the water depth is calculated the discharge routing is done by comparing the elevation of the neighbouring cells and the flow depth the determination of flow depth allows caesar lisflood to route the flow over or around obstacles in the dem once the flow routing and the flow depth calculation are complete the einstein brown formula einstein 1950 is used to calculate the fluvial erosion expressed as 10 ψ ρ s ρ ρ d s d where ψ is the balance between the forces moving and restraining the particle ρ s ρ is the relative density kg m3 of the submerged sediment ρ s is the bulk density of the sediments and ρ is the density of water d is the grain size m d is the flow depth m and s the energy slope m m the bedload transport rate φ is then calculated using 11 φ q s ρ ρ s ρ g d 3 einstein 1950 found that the bedload transport rate φ is related to the balance between the forces moving and restraining the particle ψ by 12 φ 40 1 ψ 3 the volumetric sediment load q s is then calculated by rearranging equations 11 and 12 the total potential erosion from each cell is distributed amongst nine grading size classes used in caesar lisflood using sediment entrainment algorithms in this way caesar lisflood can represent the river bed as an active layer consisting of different size sediments this ability allows it to model selective entrainment of sediments leading to river bed armouring the sediment entrainment is allowed in the surface layer on an erosive node if the surface layer depth becomes less than a threshold depth material from the subsurface layers is added to the surface active layer in contrast the excess material is added to the subsurface layer in the case of deposition this method of using multiple layers allows caesar lisflood to develop limited sediment stratification van de wiel et al 2007 in addition to the fluvial processes soil creep always termed diffusion is also included in caesar lisflood soil creep is modelled as a mass movement triggered once a critical slope threshold is exceeded according to the following equation a slope based diffusion process is also used to characterise soil creep c r 13 c r λ s d x where s is the slope m m d x is the size of the grid cell and λ is a coefficient governing the diffusion process once the fluvial erosion deposition and diffusion quantities are calculated the corresponding elevation grid and the grading distribution of each grid cell are updated to reflect the evolution of the landform caesar lisflood has been used to study the evolution of catchments under different conditions coulthard et al 2002 2006 2012 hancock et al 2010 2020 peleg et al 2021 in addition since its ability to model rudimentary sediment stratification it has also been used for modelling fluvial fan development clevis et al 2003 coulthard et al 2002 caesar lisflood has also been used to study river meandering dynamics coulthard et al 2006 recently it has also been employed to model the failure of check dams in alpine rivers ramirez et al 2022 for the simulation presented in this study we used the same caesar lisflood parameters used by hancock et al 2015 caesar lisflood is well described in coulthard et al 2012 coulthard and wiel 2006 5 materials and methods 5 1 tin camp creek digital elevation model caesar lisflood and ssspam use dems to characterise the landform s geomorphology in terms of slope flow direction and contributing areas landform evolution is achieved by changing the elevation values of this dem according to erosion or deposition occurring at each pixel the dem used in this study was generated by re gridding 240000 irregularly spaced elevation data points generated through digital photogrammetry into a 10m by 10m regularly spaced grid using a simple kriging technique the tcc study area examined here is a very stable gently sloping catena scale landform without any sudden elevation changes hancock 2005 showed that the 10m by 10m dem can effectively capture the geomorphological properties of the tcc area this same dem has been extensively used in past studies in the same area hancock et al 2002a 2010 hancock 2005 willgoose et al 2003a the study catchment used here is a mature landform with a fully linked drainage system which is not located within a floodplain it has no natural depressions that would constitute a natural pit the dem was processed through a pit filling algorithm senevirathne and willgoose 2013 to remove the pits anomalous low elevation pixels surrounded by high elevation pixels and ensure the hydrological connectivity throughout the dem fig 3 shows a vertically exaggerated 4 x view of the modelled catchment the digital elevation model used in this study is approximately 1 3 km long and 1 2 km wide with a horizontal grid spacing of 10 m 5 2 ssspam and caesar lisflood parameter calibration ssspam and caesar lisflood require site specific parameterisation for optimum results in this study we used runoff and erosion data from evans and proudfoot 2017 collected from a flume in the study catchment evans and proudfoot 2017 assembled a 10m 0 5m concentrated flow race on a 20 hillslope and measured the erosion rate for different discharge rates using the methodology presented by welivitiya and hancock 2022 erosion parameters for the tcc site e α 1 and α 2 were determined using the runoff and erosion measurement data gathered by evans and proudfoot 2017 coulthard 2019 also determined parameters for caesar lisflood for the tcc site these predetermined caesar lisflood sediment transport and hydrological parameters were used in this study calculating ssspam armouring parameters c th m and n requires long term sediment transport data and precise sediment particle size distribution from study sites such data were not available for this particular study site however welivitiya et al 2021 determined ssspam armouring parameters using runoff erosion and eroded sediment particle size distribution data collected by hancock et al 2016b from erosion test plots located on trial rehabilitated landform at the ranger mine for several earlier studies hancock et al 2016b moliere et al 2002 wasson et al 2021 the tcc catchment used in this study has been used as an analogue site for the natural geomorphological and soil conditions existed in the vicinity of the ranger mine before mining operations so the armouring parameters determined in the previous study by welivitiya et al 2021 were deemed acceptable to simulate the evolution of the tcc catchment used in the current study erosion and armouring parameters used for ssspam simulations are given in table 2 and caesar lisflood parameters used in this study are given in table 3 although parameters for fluvial erosion can be determined from the field data determining the diffusion or hillslope creep is difficult since no field data exists for the diffusivity parameter d hancock et al 2010 used a value of 0 0025 with length units being metres and time units being years in their simulations the value mentioned above has been successfully used as the diffusivity parameter in earlier studies concerning the same area 5 3 tcc soil particle size distribution data in addition to the process parameters caesar lisflood and ssspam require soil particle size distribution data soil grading for surface and subsurface layers to evolve the soil profile for this study we used the soil grading developed from a soil pit within the catchment glindeman 1992 this soil pit qt1 had rock fragments covering approximately 30 40 of the surface and a soil profile rich in quartz derived from the parent mica schist formation soil grading was determined using sieving and hydrometer methods hall et al 1992 this soil grading extracted from the qt1 soil pit was also used by hancock et al 2010 for their comparative performance study between siberia and caesar the qt1 soil grading used for ssspam simulations is given in table 4 5 4 field measured rock content data hillslope field measured rock content data was compared with the data generated by ssspam simulations the field rock content data coarse fraction of soil material greater than 2 mm was sampled on two transects along opposite facing hillslopes within the tin camp creek catchment as shown in fig 4 a soil core was used to sample the top 200 mm of the soil layer at regular intervals 15 for transect 1 and 10 for transect 2 along these transects three samples were collected at each point measured rock content percentage data of the two transects are given in table 5 6 model setup and simulations ssspam and caesar lisflood were used to simulate the evolution of the tin camp creek catchment over 10000 years four different simulations were performed using ssspam to study how the process of armouring and diffusion influence the evolution of the landform two simulations were carried out using caesar lisflood to study the influence of diffusion on landform evolution the simulation setup for each simulation with respect to diffusion and armouring is given in table 6 during the field data collection in the study catchment it was found that the soil profile of the study catchment was mostly homogeneous with deep soil so to reflect these findings the tcc catchment soil profile was simulated in this study as a 5m thick homogeneous soil column 100 layers of soil 10 mm thick each for ssspam with qt1 grading the bottommost layer is assumed to be a semi infinite non weathering layer providing unlimited materials to the upper layers with the same grading all ssspam simulations used annual average precipitation values to drive the erosion process 7 results and discussion 7 1 geomorphology of simulated landforms for the first 1000 years there was very little geomorphic difference between the caesar lisflood and ssspam simulations however after 10000 years subtle differences between the two simulated results can be observed fig 5 table 7 shows the changes in morphometric attributes of the tcc landform after 10000 years of simulated evolution by ssspam and caesar lisflood models although they look similar there are subtle differences in these landforms in both simulated landforms ssspam and caesar lisflood the main channel meandering through the landform has been excavated through erosion the incision is slightly more evident in caesar lisflood simulations than in the ssspam simulation this fact is confirmed by the average primary channel incision rate calculated for both models table 7 the hillslopes morphology seems smoother than the initial landform in ssspam simulations with this smoothing effect being more pronounced in the caesar lisflood topographic roughness index tri riley et al 1999 was used to quantify the smoothness of the resultant landforms the topographic roughness index is defined as a measure of elevation difference between neighbouring pixels of a digital elevation model interestingly the topographic roughness index tri table 7 has increased in both ssspam and caesar lisflood simulations this increase in tri is slightly higher for caesar lisflood simulations the rise in tri can be attributed to the main channel incision of the tcc landform in both ssspam and caesar lisflood simulations while not shown in fig 5 simulations without diffusion produce more incised landform results for both models ssspam and caesar lisflood hypsometric curves and area slope plots after 10 000 years of simulated evolution are presented in fig 6 the hypsometric curve langbein 1947 depicts the relationship between non dimensional contributing area and elevation due to its non dimensionality the hypsometric curve provides a means to compare catchments with different sizes and steepness it has been used to represent the geomorphic maturity of landforms area slope plots show the relationship between the contributing area draining through a particular point and the local slope of that point on fluvial landforms it quantifies the local topographic gradient as a function of the drainage area hancock et al 2010 the initial landform s hypsometric curve is typical for a mature landform at or near equilibrium the change in the hypsometric curve in both ssspam and caesar lisflood simulations is minimal fig 6 a both models hypsometric curve almost falls on the initial landform curve this result is consistent with the notion that a mature landform near equilibrium would evolve very slowly and the hypsometric curve would be minimal fig 6 b demonstrates that the area slope relationship has only slightly varied after 10 000 years of evolution by either ssspam or caesar lisflood this reflects that the initial landform is mature and has achieved steady state conditions where the erosion is largely balanced by bedrock weathering the only change from the initial landform appears to be the reduction of the scattering of the points observed in the area slope plot the reduction of the scatter of the area slope relationship represents the evolution and streamlining of the drainage system in the catchment and the choice of diffusion values 7 2 erosion and deposition characteristics to aid in comparing results between the two models and various initial conditions erosion rate maximum erosion depth and maximum deposition height values were calculated for each simulation at different time frames fig 7 there is a considerable difference between the caesar lisflood and ssspam simulation results in the first 100 years both ssspam and caesar lisflood have spinup periods at the start of the simulations during the spinup period the development of the drainage network and initial removal of fine material from the surface occurs since we used a pit filled dem with a linked drainage network only the initial fine material removal occurs during the spinup time however compared with caesar lisflood ssspam seems to have a small spinup period in this study this is evident during the 0 10 year period where caesar lisflood produced an extremely high erosion pulse approximately 2 orders of magnitude larger than the erosion rate produced by ssspam during the same period ssspam s erosion rate falls within the upper and lower bounds of field measured land denudation rates calculated using various methods within the first 10 years the difference between the initial erosion rates between the two models is a consequence of how the models handle time steps rainfall events and armouring caesar lisflood performs its erosion calculation based on hourly time steps it also requires hourly rainfall data ssspam can operate at different time steps ranging from days to years both models simulate the armouring of the soil surface due to erosion because caesar lisflood used hourly rainfall data it can simulate erosion due to high intensity short term rainfall events since the discharge rate of such storm events is high the ted and the rate of erosion calculated by caesar lisflood are high leading to the rapid removal of fine particles smaller than ted this leads to the higher initial erosion rates produced by caesar lisflood this means that caesar lisflood has an initial spin up time at the start of the simulation where the surface material is reconfigured through armouring hancock et al 2010 with time the removal of fine material leads to armouring and the erosion rate reduces to a more or less steady state subject to rainfall variation this study used the annual average rainfall rate for our ssspam simulation however the daily rainfall variability of the study site was considered when determining ssspam yearly erosion parameters for the tcc catchment through the calibration process since the high intensity storm events are averaged over the year ssspam does not produce extremely high erosion rates produced by caesar lisflood long term erosion rates 0 10000 produced by ssspam and caesar lisflood are of the same magnitude however ssspam erosion rates for all scenarios are higher than caesar lisflood although the absolute erosion rates are different both models display similar trends in erosion rate change over time the erosion rates of both models start relatively high 0 10 years and reduce quickly to a relatively constant level in the long term 1000 10000 years interestingly long term 0 10000 years erosion rates predicted by both caesar lisflood and ssspam match the regional denudation rates measured in the field by cull et al 1992 and wasson et al 2021 it also compares well with local denudation rates calculated by hancock et al 2008 using the radioisotope 137cs method and estimated by the revised universal soil loss equation rusle ssspam erosion rate values fall near the estimated regional denudation rate range after approximately 100 years of evolution the ssspam simulation with active armouring produces slightly lower erosion rates than those without armouring a similar comparison with and without armouring between ssspam simulations on a proposed post mining landform was made by welivitiya et al 2021 the difference in long term erosion rates they found was much more significant than the results presented here the reason behind this difference is the coarseness of the soil gradings used for the simulations welivitiya et al 2021 used a relatively coarse soil grading with median grain size d50 of approximately 8 mm in their corridor creek catchment simulations in our tcc catchment simulations we used the qt1 grading with an approximate d50 of 0 5 mm a high concentration of coarser material in the soil grading used by welivitiya et al 2021 led to stronger armouring and considerable erosion reduction the high concentration of fine material in the grading we used for our tcc simulations led to weak armouring and a slight decrease in erosion rate maximum erosion depths and deposition heights are of the same magnitude in both caesar lisflood and ssspam simulations however caesar lisflood simulations produce slightly higher values than ssspam within the first 100 years interestingly after 1000 years the maximum erosion depth values produced by ssspam are higher than caesar lisflood simulations some interesting observations regarding long term maximum erosion depth can be made while comparing the ssspam simulations using armouring and without armouring fig 7 middle although armouring appears to reduce the overall erosion rate it has also increased the maximum erosion depth the results seem to suggest that armouring leads to a more incised drainage network high channel depths it might seem counterintuitive that the process of armouring which reduces the landform s overall erosion rate would increase channel incision depths this apparent discrepancy can be explained by considering the sediment concentration of the flow in a non armouring environment some sediments from upstream regions with low contributing areas and low slopes hill crests and hillslopes could occur eroded sediment increases the sediment load in the downstream flows while decreasing transport capacity the reduction of the transport capacity of the downstream flows causes less erosion leading to lower channel incisions in an armouring environment erosion from the upstream hillslopes is reduced and the downstream flows are less concentrated with sediment hence downstream flows of armouring environments will have relatively higher transport capacity than non armouring settings the increased transport capacity of downstream flows leads to higher erosion and deeper channel incisions diffusion appears to slightly increase the erosion rate for caesar lisflood and ssspam simulations also as observed from fig 7 diffusion decreases the maximum erosion depth and increases the maximum deposition height across all the simulations when diffusion is not active sediment movement only occurs through fluvial erosion fluvial erosion depends on both the slope and the upstream contributing area accordingly some regions such as near the hillcrest have very low fluvial erosion due to low contributing area albeit having steep slopes diffusion only depends on the local slope s steepness and sediment transport will occur in steep slopes regardless of the contributing area the net result of this process is an increase in the erosion rate in addition diffusion increases the sediment load and reduces the flow s transport capacity through sediment saturation due to the reduced transport capacity of the flow the severity of channel incision reduces while promoting downstream sediment deposition erosion deposition plots of ssspam and caesar lisflood 10000year simulation are shown in fig 8 although caesar lisflood exhibits a higher range of erosion and deposition both simulation s erosion deposition patterns have similar trends for both caesar lisflood and ssspam the highest erosion can be observed in the main channel while most deposition occurs in first order channels which receive sediments from surrounding hillslopes compared with ssspam simulations caesar lisflood more severely incises the main drainage channel also ssspam simulations have less deposition compared to caesar lisflood 7 3 comparison of simulated landform cross sections to further analyse the performance of ssspam and caesar lisflood in simulating tcc catchment evolution cross section profiles of the simulated landform after 10000 years were plotted against each model results and the original landform fig 4 shows the locations of these four different cross sections representing different areas of the tcc catchment the comparison of the cross section plots in fig 9 shows that ssspam and caesar lisflood change the catchment morphology after 10000 years of simulation the geomorphological changes in erosion and deposition are very similar in both simulations considering the ssspam and caesar lisflood simulations erosion has occurred in the main channel fig 9 sections 2 and 3 and upper hillslopes the bottom hillslopes near the valley remain unchanged and fall on top of the original landform cross section 7 4 surface particle size distribution characteristics the spatial distribution of the surface d 50 and the percentage rock content after 10 000 years of coupled soilscape landform evolution using ssspam is shown in fig 10 surface d 50 and rock content have a clear correlation with the spatial position of the landform these results are consistent with field observations done by moore et al 1993 and poesen et al 1998 the main channel has developed the highest surface d 50 and rock content it has the highest erosion due to high contributing area the erosion process constantly armours the surface by removing fine particles leading to coarser surface grading the lowest d 50 and rock content values are located at the hillcrests where erosion is the lowest further the surface d 50 coarsens from the hillcrest to the valley bottom along the hillslope following the same pattern the rock content increases from the hill crest toward the valley bottom these observations agree well with the hillslope field observation done by vanwalleghem et al 2013 in the valley bottoms of some upper sub catchments lower d 50 and rock content values can be observed closer examination revealed that significant deposition occurs at these valley bottoms because the contributing area and slope of such upper sub catchments are lower the transport capacity of these valley bottoms is low promoting deposition the deposition of fine particles eroded from upper hillslopes causes the d 50 and the rock content to be relatively lower a similar phenomenon can be observed near the tcc catchment outlet as well the surface d 50 and the rock content are relatively low near the catchment outlet although the transport capacity of the channel near the catchment outlet is high the flow in the channel is saturated with sediments which prevents additional erosion from the channel due to reduced erosion the channel segments armouring effect near the catchment outlet is lower this reduced armouring led to relatively finer d 50 and lower rock contents 7 5 relationships between geomorphology and soils in section 7 4 clear correlations between the surface particle size distribution and the spatial position of the landform were identified this correlation results from erosion deposition characteristics caused by variations of geomorphological attributes of the landform such as contributing area and slope to further analyse these relationships contributing area slope plots were created and the soil properties surface d 50 and percentage rock content were represented using a colour pallet in the same plot in this way the variation of the soil property can be seen with respect to contributing area and slope fig 11 shows the relationship between area slope d 50 fig 11 a and percentage rock content fig 11 b generally surface d 50 and percentage rock content positively correlate with the slope for a given area similarly surface d 50 and percentage rock content positively correlate with contributing area for a given slope however there seems to be an anomaly beyond the 1 105 m2 contributing area position between the 1 105 m2 and 2 105 m2 contributing area there is a sharp increase in both surface d50 and percentage rock content irrespective of the slope also the slopes of these anomalous points are higher than the area slope diagram s general layout these anomalous points represent the main channel s highly armoured bed flowing through the tcc catchment area relative to the surrounding hill slopes the main channel of tcc accounts for the cumulative contributing area of the catchment due to the channel s high erosion rates the fine material is eroded from the channel bed leading to significant armouring and increasing the channel bed coarseness also the channel incision slightly increases the channel bed slope relative to the surrounding area 7 6 comparison of predicted soil properties with field data previous sections examined relationships between the computed soilscape variables and geomorphological attributes this section compares soil properties predicted by ssspam with the measured field soil data to test the model s accuracy the soil core coordinates were transformed to dem grid coordinates to extract the simulated soil profile data and the soil characteristics were extracted the measured average rock content percentage values along transect 1 and transect 2 fig 4 are shown in fig 12 1a and 2a respectively with point 1 being the top of the slope for both transects the dotted lines represent the general trend in the rock content over each transect there seems to be a general trend where the rock content increases from the hillcrest to the valley bottom along the transect however there is considerable variability around the observed trend interestingly the same trends can also be observed in the ssspam simulation data presented in fig 12 1b and 2b while not a perfect match ssspam is able to predict the general trend of increasing rock content moving downslope while the prediction of rock content trends for ssspam is promising there are some limitations the field data collection rock content was done using a soil core with a diameter of approximately 10 cm on regular points on two predetermined transects gps coordinates were recorded to identify the exact location of each sample site the dem used in this study had a spatial resolution of 10m we compared ssspam simulation results with rock content data collected at the field by transforming the gps coordinates to the dem coordinates because the dem resolution is coarse the rock content values extracted from any particular pixel represent an average of rock content near the sample point further field data and high resolution dem data would help refine the findings however the qualitative match suggests that in broad terms ssspsam is able to predict the general surface and near subsurface evolution of a hillslope in this coupled soilscape landform evolution simulation of the tcc catchment physical weathering fluvial erosion and diffusive erosion were considered as the active pedogenesis processes the results suggest that these are the dominant processes however soil evolution depends on other factors such as chemical weathering and biological activity for instance hancock et al 2016a studied the impact of feral pigs on soil development in the same study area they found that these animals contribute significantly to mixing the surface soil layers which changes the in situ soil properties since ssspam nor any other model did not simulate such processes the model s absolute soil properties rock content may not match perfectly with soil properties measured in the field however the fact that ssspam was able to generate the general trends of the soil properties with the limited number of pedogenetic processes increases our confidence in the model 8 study limitations and future work to simulate the weathering of the soil profile we employed a symmetric fragmentation mechanism and exponential depth dependent weathering function the symmetric fragmentation mechanism was selected based on the work done by wells et al 2006 here the exponential weathering function was employed however the weathering mechanism and the depth dependent weathering function needs further evaluation field and or laboratory experimental data on surface and subsurface weathering are necessary at present ssspam simulates a limited number of pedogenesis processes such as armouring due to erosion soil diffusion physical weathering and size selective sediment deposition there are many other processes that contribute to pedogenesis and landform evolution the establishment and development of vegetation olen et al 2016 vanacker et al 2007 and other means of weathering chemical biological mudd and furbish 2004 are some of these important processes identified in the literature incorporating such processes in the ssspam modelling framework would significantly increase its predictive capability including such processes would allow ssspam to be used in other research domains such as soil organic carbon analysis wiaux et al 2014 and environmental quality assessment in addition to geomorphological analysis currently ssspam models spatially homogenous erosion given by a single set of parameters and one set of soil grading i e initial soil grading and parameters are used to determine the soilscape landform evolution of each pixel incorporating other pedogenesis processes especially vegetation evolution would require additional capabilities such as spatially variable soil grading and erosion parameters i e different initial soil grading and parameters at different pixels the authors are currently in the process of developing these additional capabilities in this study the soil profile of the tcc catchment was modelled as a homogenous soil column having a single soil grading the simulations were done using the dem of the current tcc catchment in essence the simulations predict the evolution of the tcc catchment 10000 years into the future since the tcc catchment is at or near the steady state condition comparing the erosion rate and the surface soil grading variation between the field and the simulations is acceptable the reality is that the tcc catchment has evolved through millions of years and has developed its soil profile through weathering and transformation of the in situ bedrock a better methodology for simulating the evolution of the tcc catchment would be creating an approximate initial landform from which the current catchment evolved similar to hancock et al 2002a and running ssspam for several million years of simulated evolution however because of the computations required simulation of tcc catchment evolution for millions of years using ssspam would require an infeasible amount of computer run time to overcome this issue the ssspam code needs to be updated and its computational efficiency increased allowing ssspam to tackle such long term simulations at feasible runtimes modification and optimisation of the ssspam code to tackle this issue is currently being undertaken 9 conclusion the simulations carried out by both models ssspam and caesar lisflood demonstrate that the tin camp creek landform evolves very slowly under present environmental conditions very little change in the hypsometric curve and the area slope diagram supports this observation these observations agree well with the evolution of a catchment reaching steady state conditions both models developed similar erosion and deposition patterns within the catchment the hillcrests had the least erosion while the main channel flowing through the catchment displayed the highest erosion deposition occurred mainly in the upstream sub catchment valleys and downstream flood plains near the catchment outlet comparison of the tin camp creek catchment evolution simulation results produced by the two models revealed that the evolution of the tcc catchment predicted by caesar lisflood and ssspam was very similar although there were some differences in simulated short term erosion rates both models long term erosion rates were similar interestingly the long term erosion rate simulated by ssspam and caesar lisflood is close to the regional land denudation rates determined in the field analysing the ssspam simulations showed that soil diffusion and armouring processes significantly influence long term erosion rates which leads to differences in landform evolution high diffusion rates increase long term erosion rates leading to smoother landforms while low or no diffusion leads to a more incised landform the armouring process lowered the overall erosion rate of the landform and caused the channel flowing through the catchment to develop a coarse soil surface consistent with field observations long term ssspam simulation data revealed strong correlations between soil properties and the geomorphology of the landform both slope and contributing area positively correlate with surface median particle size diameter d 50 we conclude that this correlation arises through erosion and armouring comparison of simulated and field measured rock content data shows that ssspam produces the general trends of soil properties rock content existing at the field site and demonstrates its potential as a robust soilscape landform evolution model software and data availability the ssspam model and ssspam results analysis tools used in this study can be obtained through the following link https zenodo org record 7808748 zdarwxzbxaq the caesar lisflood model used in this study can be obtained through the following link https sourceforge net projects caesar lisflood various data used in this study such as digital elevation models and soil gradings can be obtained through the following link https zenodo org record 7808779 zdarlxzbxaq author contributions both authors welivitiya and hancock conceived the idea for this research while hancock secured the funding for this research both authors jointly carried out the methodology development and simulations using the two numerical models development of the ssspam model was carried out by welivitiya collation of data model results analysis and writing of the draft manuscript was also done by welivitiya supervision of the research project and reviewing of the manuscript was done by hancock declaration of competing interest the authors declare that they have no conflict of interest associated with this publication and the funding agencies which supported this work does not have any influence on the findings of this work acknowledgements this work was supported by australian coal association research program acarp project c27042 adaption of design tools to better design rehabilitation and capping over highly mobile mine waste and australian research council discovery grant dp110101216 
25388,computer based coupled soilscape landform evolution models provide an avenue for estimating the erosion potential of a landform over geological timescales they also provide tools to understand soil landform relationships in the context of both field and modelled pedogenesis processes in this study we used the state space soil production and assessment model ssspam and caesar lisflood landform evolution model to simulate the evolution of a natural catchment where soil erosion rates have been closely monitored over the last 20 years both models predict comparable erosion rates to that of field measurements and produce similar catchment scale geomorphological patterns rock content data measured in the field along two transects were compared with ssspam simulations at the same representative points the results showed that ssspam produced similar rock content variation along the respective transects the findings demonstrate ssspam s ability to predict catchment scale erosion and surface soil distribution the insights demonstrated here provide confidence in the ssspam model for soilscape predictions for natural agricultural and post mining landform assessment graphical abstract image 1 keywords sediment transport landform evolution model soil evolution caesar lisflood ssspam geomorphology natural catchment data availability data will be made available on request 1 introduction landforms are the product of various physical and chemical processes acting on the earth s crust for extended time periods young and mcdougall 1993 changes in global processes also change the properties of landforms cooke 1980 coulthard 2001 phillips 2006 vandenberghe 2003 however directly linking a change in a landform to a particular process is extremely difficult due to competing influences of climate tectonics and biological activity involved in landform evolution across a range of both spatial and temporal scales computer based landform evolution models are one method of overcoming these spatial and temporal barriers merritt et al 2003 they provide a tool to gain a deeper understanding of the underlying factors influencing the landform evolution process hancock et al 2019 tucker and hancock 2010 using such models it is possible to predict the evolutionary path of landforms that will arise in the future due to changing global processes such as climate dietrich et al 2003 landform evolution models have extensive applications in the mining industry where they are being used to assess the erosional stability of post mining landforms evans 2000 howard et al 2011 martin duque et al 2021 there are several landform evolution models formulated throughout the years such as siberia willgoose et al 1991a 1991b caesar coulthard et al 2002 child tucker et al 2001 milesd vanwalleghem et al 2013 and ssspam welivitiya et al 2019 natural catena scale landforms are at or near equilibrium conditions renwick 1992 where the evolution of the landform is very slow there have been few attempts to numerically analyse the evolutionary dynamics of natural catena scale landforms cohen et al 2009 parker 1999 willgoose and sharmeen 2006 coupled soilscape landform evolution modelling of natural catchments can produce important insights into the evolution of soil that could influence the ecological productivity of the landform de alba et al 2004 hancock et al 2010 together with insights into relationships between the soil characteristics and geomorphology of a landform ma et al 2019 welivitiya et al 2016 although these landform soilscape evolution modelling studies have produced plausible results there is uncertainty regarding the accuracy of these model predictions the reason behind this uncertainty is the lack of field data that can be compared with the model results to assess their accuracy skinner et al 2018 wong et al 2021 the aim of this study is to evaluate the ssspam coupled soilscape landform evolution model s performance in simulating the evolution of a natural catchment scale landform and compare the simulation results with field data and results from a more established landform evolution model the state space soil production assessment model ssspam is coupled soilscape landform evolution model capable of simultaneously evolving the landform and the soil profile welivitiya 2017 welivitiya et al 2016 in ssspam the state space matrix approach is used to represent physical processes of pedogenesis such as erosion and armouring diffusion sediment deposition and weathering ssspam can characterise the particle size distribution at any point at any modelled soil depth at any time it also tracks the particle size distribution of the entrained sediment in the flow ssspam can also operate at different time steps ranging from days to years allowing it to model long term landform evolution based on annual rainfall averages or simulate erosion caused by short term storm events ssspam also can simulate additional pedogenesis processes such as soil armouring and weathering compared to existing landform evolution models these capabilities make ssspam a suitable platform for analysing evolutionary dynamics and soil characteristics geomorphology relationships in natural catena scale landforms in a previous study welivitiya and hancock 2022 used the ssspam soilscape landform evolution model to simulate the evolution of gullies on a mining waste rock repository they showed that after site specific parameter calibration ssspam was able to predict the geomorphological development of gullies in the study site with reasonable accuracy by comparing the geomorphological attributes of simulated gullies to those observed in the field although ssspam has been shown to accurately predict the evolutionary dynamics of manufactured landforms such as post mining structures its performance in simulating natural catena scale landforms has not been evaluated that is erosion rates predicted by ssspam can be compared with field measured erosion rates however this comparison does not allow us to validate the predicted dynamics of the landform evolution i e we can evaluate the end results but are unable to determine how we got there this is because data on historical erosion rates and catchment geomorphology is largely unavailable which would facilitate the comparison of model predictions to the natural environment one method to evaluate the dynamic performance of ssspam is to compare model results with other well established models here we use ssspam and caesar lisflood erosion models to simulate the evolution of a natural catchment in western arnhem land northern territory there were 2 considerations for selecting the caesar lisflood model for this study the first selection consideration was the compatibility of the model formulation similar to ssspam caesar lisflood uses variable rainfall time series data to drive the erosion model both models use particle size distribution data to represent the soil profile and are able to account for erosion reducing processes such as surface armouring the second selection consideration was the availability of parameters required to run the models in the study area in a previous study coulthard 2019 calibrated caesar lisflood parameters for the study site using runoff erosion data collected by evans and proudfoot 2017 similarities and differences between ssspam and caesar lisflood models are shown in table 1 in this study simulation results from the ssspam and caesar lisflood models are compared to each other and with field determined soil erosion values ssspam simulation results were further analysed to identify any linkage between surface soil characteristics median particle size diameter d50 and rock content particle size 2 mm and geomorphological attributes slope and contributing area furthermore field measured rock content data was compared with the ssspam simulated rock content data to assess the predictive accuracy of ssspam in the context of surface soil evolution 2 study site the site tin camp creek tcc is a natural catchment that is a part of the alligator river system located in western arnhem land northern territory australia the site lies in the myra falls inlier in lover member cahill formation and the area is tectonically inactive needham and stuart smith 1980 the geology of the catchment consists of metamorphosed schists located in the wet dry tropics of northern australia with an annual mean rainfall of 1400 mm most precipitation occurs as high intensity tropical storms during the wet season which lasts from november to april with fluvial erosion as the dominant erosion process in this region saynor et al 2004 for this study a representative sub catchment with an approximate area of 50 ha was selected from the greater tcc fig 1 the selected sub catchment consists of closely dissected short steep slopes 10 100 m long with slope gradients between 15 and 50 hancock et al 2010 the soils in the study sub catchment consist of red loamy earths and shallow gravely loam with some micaceous silty yellow earths and minor solodic soils on alluvial flats riley and williams 1991 the surface slopes and hill crests are covered with gravelly cobble quartz lag the study sub catchment is covered with open dry sclerophyll forests dominated by eucalyptus and acacia species mixed with other native vegetation story et al 1976 in addition the low lying riparian areas are populated by heteropogon contortus and sorghum spp mixed with some melaleuca spp and pandanus spiralus during the dry season the vegetation cover is often reduced by fire leading to high fluvial erosion rates immediately after bushfires saynor et al 2004 during the early part of the wet season there is vigorous growth of annual grasses which falls over during the latter part of the wet season producing a thick mulch that significantly reduces erosion of exposed soil hancock et al 2010 various methods have established erosion and denudation rates of the study region net soil redistribution rates between 2 and 13 t ha 1year 1 0 013 0 86 mm year 1 were established through an assessment employing the fallout environmental radioisotope cesium 137 137cs hancock et al 2008 these measured erosion rates are comparable with the overall denudation rates for the area 0 01 to 0 04 mm year 1 and 0 062 0 088 mm year 1 determined using stream sediment data from a range of catchments of different sizes in the general region cull et al 1992 erskine and saynor 2000 hancock et al 2008 also estimated an erosion rate of 14 t ha 1year 1 1 mm year 1 for the lower hillslopes during a two year study that used erosion pins located at the base of hillslopes erosion estimates using revised universal soil loss equation rusle with site specific parameters parameter values derived from field data collected from the area produced erosion rates of 10 t ha 1year 1 0 67 mm year 1 this area is tectonically inactive and has not been disturbed by european activity hancock et al 2002b as a result it has been used to validate previously developed landform evolution models hancock et al 2002b 2010 hancock 2003 willgoose et al 2003b in addition the tin camp creek catchment has been used to calibrate siberia parameters to assess the stability and evolution of the erarm post mining landform for ranger uranium mine rehabilitation evans 2000 evans and willgoose 2000 willgoose and riley 1998 3 ssspam coupled soilscape landform evolution model ssspam is a coupled soilscape landform evolution model developed by combining a pedogenesis model and a landform evolution model welivitiya et al 2019 2021 it can simulate fluvial erosion and armouring diffusive erosion surface sediment deposition and weathering within the soil profile ssspam uses several layers to simulate the soil profile enabling it to model the evolution of the soil profile ssspam can describe the particle size distribution of the entire soil profile at each node pixel at any modelled soil depth while tracing the eroded sediment grading distribution in the water flow the operational flow diagram identifying model inputs and outputs is shown in fig 2 the potential fluvial erosion rate in ssspam is calculated based on discharge per unit width and slope in this calculation both detachment and transport limited erosion are considered welivitiya et al 2016 the potential rate of fluvial erosion per unit width m3 s 1m 1 is given by 1 e f k e q α 1 s α 2 t where e is the edibility factor q is discharge per unit width m3 s 1m 1 s is the slope α1 and α 2 are exponents governing the erosion process and t is the length of time step used s the parameter k is a factor that determines the maximum volume of erosion that could potentially occur from a particular pixel depending on the surface soil grading and the threshold entrainment diameter ted ted is the maximum particle diameter that the overland water flow could entrain it is calculated using shield s shear stress criteria ted is given by 2 2 t e d c t h n q 0 6 s 0 7 where c t h is a parameter that needs to be calibrated and n is the manning friction factor soil particles residing on the surface with diameters less than the ted can erode while particles larger than ted remain on the surface leading to armouring in ssspam discharge per unit width q is calculated using the runoff excess generation factor r m s 1 and the contributing area of the pixel a m2 and flow width typically equal to the dem resolution w f m as 3 q r a w f the selective entrainment of soil particles through erosion leads to armouring of the soil surface characterisation of this size selectivity of the erosion process is achieved using the erosion transition matrix a ted mentioned earlier is used to generate the entries of erosion transition matrix a 4 a k k a d k m g k v f o r k m 0 f o r k m where d k is the mean diameter of particle size class k m and v are exponent factors for the mean diameter of particle size class and grading fraction g k respectively a is a scaling factor that governs the erosion from each grading class m is the grading class that the ted falls into i e the largest particle that the water flow can entrain ssspam uses a simple diffusion model to determine the diffusive erosion e d where 5 e d d s f o r s d s x 3 s t h s s t h x 2 f o r s d s x 3 s t h where d m3 s 1 m 1 width is diffusivity s is the slope s th is the maximum slope threshold for the modelled material i e the maximum slope value the soil material can withstand without soil mass movement diffusion is modelled as a process dependent only on material availability without any considerations for size selectivity i e particle size distribution of the diffused material is the same as the soil surface ssspam uses a fragmentation mechanism and a depth dependent weathering rate function to simulate physical weathering the fragmentation model determines the geometry of the weathered particles while the depth depend weathering rate function determines the rate at which particles fragment in each soil layer if a spherical parent particle of diameter d m breaks into a daughter particle with diameters d 1 m accounting for α fraction and n 1 daughter particles with diameter d 2 m the sizes of the daughter particles can be determined in terms of the parent particle diameter as 6 d 1 α 1 3 d 7 d 2 1 α n 1 1 3 d in this study a symmetric fragmentation mechanism was used as the weathering mechanism where a single parent particle disintegrates into two identical daughter particles n 2 0 and α 0 5 in this study the exponential depth dependent weathering function hereafter mentioned as exponential function humphreys and wilkinson 2007 was used which is given by 8 w h β e δ 1 h where w h is the weathering rate at the soil layer at a depth of h m below the surface the constant β determines the maximum value w h can take here we used β 1 and δ 1 is the depth scaling factor we used δ 1 1 738 in the exponential function the highest weathering rate is at the soil surface and the weathering rate decreases exponentially with increasing soil depth ssspam tracks the sediment eroded from upstream areas and is used to calculate the erosion or deposition at any given pixel in the case of deposition the total deposition mass and the psd of the deposited material is determined using the deposition mass vector and the deposition transition matrix a quantity called the critical immersion depth is used to calculate the entries of the deposition transition matrix the critical immersion depth is referred to the vertical distance a particle will fall through the water column with the settling velocity while moving the horizontal distance of pixel width with the flow velocity at the pixel assuming all the sediment particles are distributed equally throughout the water column at a pixel the critical immersion depth is greater for larger particles and less for smaller particles leading to size selective deposition further details of the formulation and implementation of the deposition module in ssspam can be found in welivitiya 2017 and welivitiya et al 2019 more information on the weathering model available in ssspam and its implementation can be found elsewhere welivitiya 2017 welivitiya et al 2016 welivitiya et al 2019 4 caesar lisflood landscape and erosion model the landform in caesar lisflood is represented using a regular grid of elevation values other information such as sediment grading vegetation parameters and hydrological parameters discharge infiltration pertaining to each node is stored in the same grid structure according to three primary considerations all these parameters are changed at every model iteration these are hydraulic routing fluvial erosion and deposition and diffusive slope processes hancock et al 2011 surface and subsurface discharge are modelled using hourly rainfall as input through a modified version of topmodel beven and kirkby 1979 a modified adaptation of manning s equation is used to calculate the flow depth using the discharge rate produced by the inbuilt hydrological model the equation for flow depth calculation is 9 d q n s 1 2 3 5 where d is flow depth m q is the discharge m3 s n is manning s coefficient and s is the slope m m once the water depth is calculated the discharge routing is done by comparing the elevation of the neighbouring cells and the flow depth the determination of flow depth allows caesar lisflood to route the flow over or around obstacles in the dem once the flow routing and the flow depth calculation are complete the einstein brown formula einstein 1950 is used to calculate the fluvial erosion expressed as 10 ψ ρ s ρ ρ d s d where ψ is the balance between the forces moving and restraining the particle ρ s ρ is the relative density kg m3 of the submerged sediment ρ s is the bulk density of the sediments and ρ is the density of water d is the grain size m d is the flow depth m and s the energy slope m m the bedload transport rate φ is then calculated using 11 φ q s ρ ρ s ρ g d 3 einstein 1950 found that the bedload transport rate φ is related to the balance between the forces moving and restraining the particle ψ by 12 φ 40 1 ψ 3 the volumetric sediment load q s is then calculated by rearranging equations 11 and 12 the total potential erosion from each cell is distributed amongst nine grading size classes used in caesar lisflood using sediment entrainment algorithms in this way caesar lisflood can represent the river bed as an active layer consisting of different size sediments this ability allows it to model selective entrainment of sediments leading to river bed armouring the sediment entrainment is allowed in the surface layer on an erosive node if the surface layer depth becomes less than a threshold depth material from the subsurface layers is added to the surface active layer in contrast the excess material is added to the subsurface layer in the case of deposition this method of using multiple layers allows caesar lisflood to develop limited sediment stratification van de wiel et al 2007 in addition to the fluvial processes soil creep always termed diffusion is also included in caesar lisflood soil creep is modelled as a mass movement triggered once a critical slope threshold is exceeded according to the following equation a slope based diffusion process is also used to characterise soil creep c r 13 c r λ s d x where s is the slope m m d x is the size of the grid cell and λ is a coefficient governing the diffusion process once the fluvial erosion deposition and diffusion quantities are calculated the corresponding elevation grid and the grading distribution of each grid cell are updated to reflect the evolution of the landform caesar lisflood has been used to study the evolution of catchments under different conditions coulthard et al 2002 2006 2012 hancock et al 2010 2020 peleg et al 2021 in addition since its ability to model rudimentary sediment stratification it has also been used for modelling fluvial fan development clevis et al 2003 coulthard et al 2002 caesar lisflood has also been used to study river meandering dynamics coulthard et al 2006 recently it has also been employed to model the failure of check dams in alpine rivers ramirez et al 2022 for the simulation presented in this study we used the same caesar lisflood parameters used by hancock et al 2015 caesar lisflood is well described in coulthard et al 2012 coulthard and wiel 2006 5 materials and methods 5 1 tin camp creek digital elevation model caesar lisflood and ssspam use dems to characterise the landform s geomorphology in terms of slope flow direction and contributing areas landform evolution is achieved by changing the elevation values of this dem according to erosion or deposition occurring at each pixel the dem used in this study was generated by re gridding 240000 irregularly spaced elevation data points generated through digital photogrammetry into a 10m by 10m regularly spaced grid using a simple kriging technique the tcc study area examined here is a very stable gently sloping catena scale landform without any sudden elevation changes hancock 2005 showed that the 10m by 10m dem can effectively capture the geomorphological properties of the tcc area this same dem has been extensively used in past studies in the same area hancock et al 2002a 2010 hancock 2005 willgoose et al 2003a the study catchment used here is a mature landform with a fully linked drainage system which is not located within a floodplain it has no natural depressions that would constitute a natural pit the dem was processed through a pit filling algorithm senevirathne and willgoose 2013 to remove the pits anomalous low elevation pixels surrounded by high elevation pixels and ensure the hydrological connectivity throughout the dem fig 3 shows a vertically exaggerated 4 x view of the modelled catchment the digital elevation model used in this study is approximately 1 3 km long and 1 2 km wide with a horizontal grid spacing of 10 m 5 2 ssspam and caesar lisflood parameter calibration ssspam and caesar lisflood require site specific parameterisation for optimum results in this study we used runoff and erosion data from evans and proudfoot 2017 collected from a flume in the study catchment evans and proudfoot 2017 assembled a 10m 0 5m concentrated flow race on a 20 hillslope and measured the erosion rate for different discharge rates using the methodology presented by welivitiya and hancock 2022 erosion parameters for the tcc site e α 1 and α 2 were determined using the runoff and erosion measurement data gathered by evans and proudfoot 2017 coulthard 2019 also determined parameters for caesar lisflood for the tcc site these predetermined caesar lisflood sediment transport and hydrological parameters were used in this study calculating ssspam armouring parameters c th m and n requires long term sediment transport data and precise sediment particle size distribution from study sites such data were not available for this particular study site however welivitiya et al 2021 determined ssspam armouring parameters using runoff erosion and eroded sediment particle size distribution data collected by hancock et al 2016b from erosion test plots located on trial rehabilitated landform at the ranger mine for several earlier studies hancock et al 2016b moliere et al 2002 wasson et al 2021 the tcc catchment used in this study has been used as an analogue site for the natural geomorphological and soil conditions existed in the vicinity of the ranger mine before mining operations so the armouring parameters determined in the previous study by welivitiya et al 2021 were deemed acceptable to simulate the evolution of the tcc catchment used in the current study erosion and armouring parameters used for ssspam simulations are given in table 2 and caesar lisflood parameters used in this study are given in table 3 although parameters for fluvial erosion can be determined from the field data determining the diffusion or hillslope creep is difficult since no field data exists for the diffusivity parameter d hancock et al 2010 used a value of 0 0025 with length units being metres and time units being years in their simulations the value mentioned above has been successfully used as the diffusivity parameter in earlier studies concerning the same area 5 3 tcc soil particle size distribution data in addition to the process parameters caesar lisflood and ssspam require soil particle size distribution data soil grading for surface and subsurface layers to evolve the soil profile for this study we used the soil grading developed from a soil pit within the catchment glindeman 1992 this soil pit qt1 had rock fragments covering approximately 30 40 of the surface and a soil profile rich in quartz derived from the parent mica schist formation soil grading was determined using sieving and hydrometer methods hall et al 1992 this soil grading extracted from the qt1 soil pit was also used by hancock et al 2010 for their comparative performance study between siberia and caesar the qt1 soil grading used for ssspam simulations is given in table 4 5 4 field measured rock content data hillslope field measured rock content data was compared with the data generated by ssspam simulations the field rock content data coarse fraction of soil material greater than 2 mm was sampled on two transects along opposite facing hillslopes within the tin camp creek catchment as shown in fig 4 a soil core was used to sample the top 200 mm of the soil layer at regular intervals 15 for transect 1 and 10 for transect 2 along these transects three samples were collected at each point measured rock content percentage data of the two transects are given in table 5 6 model setup and simulations ssspam and caesar lisflood were used to simulate the evolution of the tin camp creek catchment over 10000 years four different simulations were performed using ssspam to study how the process of armouring and diffusion influence the evolution of the landform two simulations were carried out using caesar lisflood to study the influence of diffusion on landform evolution the simulation setup for each simulation with respect to diffusion and armouring is given in table 6 during the field data collection in the study catchment it was found that the soil profile of the study catchment was mostly homogeneous with deep soil so to reflect these findings the tcc catchment soil profile was simulated in this study as a 5m thick homogeneous soil column 100 layers of soil 10 mm thick each for ssspam with qt1 grading the bottommost layer is assumed to be a semi infinite non weathering layer providing unlimited materials to the upper layers with the same grading all ssspam simulations used annual average precipitation values to drive the erosion process 7 results and discussion 7 1 geomorphology of simulated landforms for the first 1000 years there was very little geomorphic difference between the caesar lisflood and ssspam simulations however after 10000 years subtle differences between the two simulated results can be observed fig 5 table 7 shows the changes in morphometric attributes of the tcc landform after 10000 years of simulated evolution by ssspam and caesar lisflood models although they look similar there are subtle differences in these landforms in both simulated landforms ssspam and caesar lisflood the main channel meandering through the landform has been excavated through erosion the incision is slightly more evident in caesar lisflood simulations than in the ssspam simulation this fact is confirmed by the average primary channel incision rate calculated for both models table 7 the hillslopes morphology seems smoother than the initial landform in ssspam simulations with this smoothing effect being more pronounced in the caesar lisflood topographic roughness index tri riley et al 1999 was used to quantify the smoothness of the resultant landforms the topographic roughness index is defined as a measure of elevation difference between neighbouring pixels of a digital elevation model interestingly the topographic roughness index tri table 7 has increased in both ssspam and caesar lisflood simulations this increase in tri is slightly higher for caesar lisflood simulations the rise in tri can be attributed to the main channel incision of the tcc landform in both ssspam and caesar lisflood simulations while not shown in fig 5 simulations without diffusion produce more incised landform results for both models ssspam and caesar lisflood hypsometric curves and area slope plots after 10 000 years of simulated evolution are presented in fig 6 the hypsometric curve langbein 1947 depicts the relationship between non dimensional contributing area and elevation due to its non dimensionality the hypsometric curve provides a means to compare catchments with different sizes and steepness it has been used to represent the geomorphic maturity of landforms area slope plots show the relationship between the contributing area draining through a particular point and the local slope of that point on fluvial landforms it quantifies the local topographic gradient as a function of the drainage area hancock et al 2010 the initial landform s hypsometric curve is typical for a mature landform at or near equilibrium the change in the hypsometric curve in both ssspam and caesar lisflood simulations is minimal fig 6 a both models hypsometric curve almost falls on the initial landform curve this result is consistent with the notion that a mature landform near equilibrium would evolve very slowly and the hypsometric curve would be minimal fig 6 b demonstrates that the area slope relationship has only slightly varied after 10 000 years of evolution by either ssspam or caesar lisflood this reflects that the initial landform is mature and has achieved steady state conditions where the erosion is largely balanced by bedrock weathering the only change from the initial landform appears to be the reduction of the scattering of the points observed in the area slope plot the reduction of the scatter of the area slope relationship represents the evolution and streamlining of the drainage system in the catchment and the choice of diffusion values 7 2 erosion and deposition characteristics to aid in comparing results between the two models and various initial conditions erosion rate maximum erosion depth and maximum deposition height values were calculated for each simulation at different time frames fig 7 there is a considerable difference between the caesar lisflood and ssspam simulation results in the first 100 years both ssspam and caesar lisflood have spinup periods at the start of the simulations during the spinup period the development of the drainage network and initial removal of fine material from the surface occurs since we used a pit filled dem with a linked drainage network only the initial fine material removal occurs during the spinup time however compared with caesar lisflood ssspam seems to have a small spinup period in this study this is evident during the 0 10 year period where caesar lisflood produced an extremely high erosion pulse approximately 2 orders of magnitude larger than the erosion rate produced by ssspam during the same period ssspam s erosion rate falls within the upper and lower bounds of field measured land denudation rates calculated using various methods within the first 10 years the difference between the initial erosion rates between the two models is a consequence of how the models handle time steps rainfall events and armouring caesar lisflood performs its erosion calculation based on hourly time steps it also requires hourly rainfall data ssspam can operate at different time steps ranging from days to years both models simulate the armouring of the soil surface due to erosion because caesar lisflood used hourly rainfall data it can simulate erosion due to high intensity short term rainfall events since the discharge rate of such storm events is high the ted and the rate of erosion calculated by caesar lisflood are high leading to the rapid removal of fine particles smaller than ted this leads to the higher initial erosion rates produced by caesar lisflood this means that caesar lisflood has an initial spin up time at the start of the simulation where the surface material is reconfigured through armouring hancock et al 2010 with time the removal of fine material leads to armouring and the erosion rate reduces to a more or less steady state subject to rainfall variation this study used the annual average rainfall rate for our ssspam simulation however the daily rainfall variability of the study site was considered when determining ssspam yearly erosion parameters for the tcc catchment through the calibration process since the high intensity storm events are averaged over the year ssspam does not produce extremely high erosion rates produced by caesar lisflood long term erosion rates 0 10000 produced by ssspam and caesar lisflood are of the same magnitude however ssspam erosion rates for all scenarios are higher than caesar lisflood although the absolute erosion rates are different both models display similar trends in erosion rate change over time the erosion rates of both models start relatively high 0 10 years and reduce quickly to a relatively constant level in the long term 1000 10000 years interestingly long term 0 10000 years erosion rates predicted by both caesar lisflood and ssspam match the regional denudation rates measured in the field by cull et al 1992 and wasson et al 2021 it also compares well with local denudation rates calculated by hancock et al 2008 using the radioisotope 137cs method and estimated by the revised universal soil loss equation rusle ssspam erosion rate values fall near the estimated regional denudation rate range after approximately 100 years of evolution the ssspam simulation with active armouring produces slightly lower erosion rates than those without armouring a similar comparison with and without armouring between ssspam simulations on a proposed post mining landform was made by welivitiya et al 2021 the difference in long term erosion rates they found was much more significant than the results presented here the reason behind this difference is the coarseness of the soil gradings used for the simulations welivitiya et al 2021 used a relatively coarse soil grading with median grain size d50 of approximately 8 mm in their corridor creek catchment simulations in our tcc catchment simulations we used the qt1 grading with an approximate d50 of 0 5 mm a high concentration of coarser material in the soil grading used by welivitiya et al 2021 led to stronger armouring and considerable erosion reduction the high concentration of fine material in the grading we used for our tcc simulations led to weak armouring and a slight decrease in erosion rate maximum erosion depths and deposition heights are of the same magnitude in both caesar lisflood and ssspam simulations however caesar lisflood simulations produce slightly higher values than ssspam within the first 100 years interestingly after 1000 years the maximum erosion depth values produced by ssspam are higher than caesar lisflood simulations some interesting observations regarding long term maximum erosion depth can be made while comparing the ssspam simulations using armouring and without armouring fig 7 middle although armouring appears to reduce the overall erosion rate it has also increased the maximum erosion depth the results seem to suggest that armouring leads to a more incised drainage network high channel depths it might seem counterintuitive that the process of armouring which reduces the landform s overall erosion rate would increase channel incision depths this apparent discrepancy can be explained by considering the sediment concentration of the flow in a non armouring environment some sediments from upstream regions with low contributing areas and low slopes hill crests and hillslopes could occur eroded sediment increases the sediment load in the downstream flows while decreasing transport capacity the reduction of the transport capacity of the downstream flows causes less erosion leading to lower channel incisions in an armouring environment erosion from the upstream hillslopes is reduced and the downstream flows are less concentrated with sediment hence downstream flows of armouring environments will have relatively higher transport capacity than non armouring settings the increased transport capacity of downstream flows leads to higher erosion and deeper channel incisions diffusion appears to slightly increase the erosion rate for caesar lisflood and ssspam simulations also as observed from fig 7 diffusion decreases the maximum erosion depth and increases the maximum deposition height across all the simulations when diffusion is not active sediment movement only occurs through fluvial erosion fluvial erosion depends on both the slope and the upstream contributing area accordingly some regions such as near the hillcrest have very low fluvial erosion due to low contributing area albeit having steep slopes diffusion only depends on the local slope s steepness and sediment transport will occur in steep slopes regardless of the contributing area the net result of this process is an increase in the erosion rate in addition diffusion increases the sediment load and reduces the flow s transport capacity through sediment saturation due to the reduced transport capacity of the flow the severity of channel incision reduces while promoting downstream sediment deposition erosion deposition plots of ssspam and caesar lisflood 10000year simulation are shown in fig 8 although caesar lisflood exhibits a higher range of erosion and deposition both simulation s erosion deposition patterns have similar trends for both caesar lisflood and ssspam the highest erosion can be observed in the main channel while most deposition occurs in first order channels which receive sediments from surrounding hillslopes compared with ssspam simulations caesar lisflood more severely incises the main drainage channel also ssspam simulations have less deposition compared to caesar lisflood 7 3 comparison of simulated landform cross sections to further analyse the performance of ssspam and caesar lisflood in simulating tcc catchment evolution cross section profiles of the simulated landform after 10000 years were plotted against each model results and the original landform fig 4 shows the locations of these four different cross sections representing different areas of the tcc catchment the comparison of the cross section plots in fig 9 shows that ssspam and caesar lisflood change the catchment morphology after 10000 years of simulation the geomorphological changes in erosion and deposition are very similar in both simulations considering the ssspam and caesar lisflood simulations erosion has occurred in the main channel fig 9 sections 2 and 3 and upper hillslopes the bottom hillslopes near the valley remain unchanged and fall on top of the original landform cross section 7 4 surface particle size distribution characteristics the spatial distribution of the surface d 50 and the percentage rock content after 10 000 years of coupled soilscape landform evolution using ssspam is shown in fig 10 surface d 50 and rock content have a clear correlation with the spatial position of the landform these results are consistent with field observations done by moore et al 1993 and poesen et al 1998 the main channel has developed the highest surface d 50 and rock content it has the highest erosion due to high contributing area the erosion process constantly armours the surface by removing fine particles leading to coarser surface grading the lowest d 50 and rock content values are located at the hillcrests where erosion is the lowest further the surface d 50 coarsens from the hillcrest to the valley bottom along the hillslope following the same pattern the rock content increases from the hill crest toward the valley bottom these observations agree well with the hillslope field observation done by vanwalleghem et al 2013 in the valley bottoms of some upper sub catchments lower d 50 and rock content values can be observed closer examination revealed that significant deposition occurs at these valley bottoms because the contributing area and slope of such upper sub catchments are lower the transport capacity of these valley bottoms is low promoting deposition the deposition of fine particles eroded from upper hillslopes causes the d 50 and the rock content to be relatively lower a similar phenomenon can be observed near the tcc catchment outlet as well the surface d 50 and the rock content are relatively low near the catchment outlet although the transport capacity of the channel near the catchment outlet is high the flow in the channel is saturated with sediments which prevents additional erosion from the channel due to reduced erosion the channel segments armouring effect near the catchment outlet is lower this reduced armouring led to relatively finer d 50 and lower rock contents 7 5 relationships between geomorphology and soils in section 7 4 clear correlations between the surface particle size distribution and the spatial position of the landform were identified this correlation results from erosion deposition characteristics caused by variations of geomorphological attributes of the landform such as contributing area and slope to further analyse these relationships contributing area slope plots were created and the soil properties surface d 50 and percentage rock content were represented using a colour pallet in the same plot in this way the variation of the soil property can be seen with respect to contributing area and slope fig 11 shows the relationship between area slope d 50 fig 11 a and percentage rock content fig 11 b generally surface d 50 and percentage rock content positively correlate with the slope for a given area similarly surface d 50 and percentage rock content positively correlate with contributing area for a given slope however there seems to be an anomaly beyond the 1 105 m2 contributing area position between the 1 105 m2 and 2 105 m2 contributing area there is a sharp increase in both surface d50 and percentage rock content irrespective of the slope also the slopes of these anomalous points are higher than the area slope diagram s general layout these anomalous points represent the main channel s highly armoured bed flowing through the tcc catchment area relative to the surrounding hill slopes the main channel of tcc accounts for the cumulative contributing area of the catchment due to the channel s high erosion rates the fine material is eroded from the channel bed leading to significant armouring and increasing the channel bed coarseness also the channel incision slightly increases the channel bed slope relative to the surrounding area 7 6 comparison of predicted soil properties with field data previous sections examined relationships between the computed soilscape variables and geomorphological attributes this section compares soil properties predicted by ssspam with the measured field soil data to test the model s accuracy the soil core coordinates were transformed to dem grid coordinates to extract the simulated soil profile data and the soil characteristics were extracted the measured average rock content percentage values along transect 1 and transect 2 fig 4 are shown in fig 12 1a and 2a respectively with point 1 being the top of the slope for both transects the dotted lines represent the general trend in the rock content over each transect there seems to be a general trend where the rock content increases from the hillcrest to the valley bottom along the transect however there is considerable variability around the observed trend interestingly the same trends can also be observed in the ssspam simulation data presented in fig 12 1b and 2b while not a perfect match ssspam is able to predict the general trend of increasing rock content moving downslope while the prediction of rock content trends for ssspam is promising there are some limitations the field data collection rock content was done using a soil core with a diameter of approximately 10 cm on regular points on two predetermined transects gps coordinates were recorded to identify the exact location of each sample site the dem used in this study had a spatial resolution of 10m we compared ssspam simulation results with rock content data collected at the field by transforming the gps coordinates to the dem coordinates because the dem resolution is coarse the rock content values extracted from any particular pixel represent an average of rock content near the sample point further field data and high resolution dem data would help refine the findings however the qualitative match suggests that in broad terms ssspsam is able to predict the general surface and near subsurface evolution of a hillslope in this coupled soilscape landform evolution simulation of the tcc catchment physical weathering fluvial erosion and diffusive erosion were considered as the active pedogenesis processes the results suggest that these are the dominant processes however soil evolution depends on other factors such as chemical weathering and biological activity for instance hancock et al 2016a studied the impact of feral pigs on soil development in the same study area they found that these animals contribute significantly to mixing the surface soil layers which changes the in situ soil properties since ssspam nor any other model did not simulate such processes the model s absolute soil properties rock content may not match perfectly with soil properties measured in the field however the fact that ssspam was able to generate the general trends of the soil properties with the limited number of pedogenetic processes increases our confidence in the model 8 study limitations and future work to simulate the weathering of the soil profile we employed a symmetric fragmentation mechanism and exponential depth dependent weathering function the symmetric fragmentation mechanism was selected based on the work done by wells et al 2006 here the exponential weathering function was employed however the weathering mechanism and the depth dependent weathering function needs further evaluation field and or laboratory experimental data on surface and subsurface weathering are necessary at present ssspam simulates a limited number of pedogenesis processes such as armouring due to erosion soil diffusion physical weathering and size selective sediment deposition there are many other processes that contribute to pedogenesis and landform evolution the establishment and development of vegetation olen et al 2016 vanacker et al 2007 and other means of weathering chemical biological mudd and furbish 2004 are some of these important processes identified in the literature incorporating such processes in the ssspam modelling framework would significantly increase its predictive capability including such processes would allow ssspam to be used in other research domains such as soil organic carbon analysis wiaux et al 2014 and environmental quality assessment in addition to geomorphological analysis currently ssspam models spatially homogenous erosion given by a single set of parameters and one set of soil grading i e initial soil grading and parameters are used to determine the soilscape landform evolution of each pixel incorporating other pedogenesis processes especially vegetation evolution would require additional capabilities such as spatially variable soil grading and erosion parameters i e different initial soil grading and parameters at different pixels the authors are currently in the process of developing these additional capabilities in this study the soil profile of the tcc catchment was modelled as a homogenous soil column having a single soil grading the simulations were done using the dem of the current tcc catchment in essence the simulations predict the evolution of the tcc catchment 10000 years into the future since the tcc catchment is at or near the steady state condition comparing the erosion rate and the surface soil grading variation between the field and the simulations is acceptable the reality is that the tcc catchment has evolved through millions of years and has developed its soil profile through weathering and transformation of the in situ bedrock a better methodology for simulating the evolution of the tcc catchment would be creating an approximate initial landform from which the current catchment evolved similar to hancock et al 2002a and running ssspam for several million years of simulated evolution however because of the computations required simulation of tcc catchment evolution for millions of years using ssspam would require an infeasible amount of computer run time to overcome this issue the ssspam code needs to be updated and its computational efficiency increased allowing ssspam to tackle such long term simulations at feasible runtimes modification and optimisation of the ssspam code to tackle this issue is currently being undertaken 9 conclusion the simulations carried out by both models ssspam and caesar lisflood demonstrate that the tin camp creek landform evolves very slowly under present environmental conditions very little change in the hypsometric curve and the area slope diagram supports this observation these observations agree well with the evolution of a catchment reaching steady state conditions both models developed similar erosion and deposition patterns within the catchment the hillcrests had the least erosion while the main channel flowing through the catchment displayed the highest erosion deposition occurred mainly in the upstream sub catchment valleys and downstream flood plains near the catchment outlet comparison of the tin camp creek catchment evolution simulation results produced by the two models revealed that the evolution of the tcc catchment predicted by caesar lisflood and ssspam was very similar although there were some differences in simulated short term erosion rates both models long term erosion rates were similar interestingly the long term erosion rate simulated by ssspam and caesar lisflood is close to the regional land denudation rates determined in the field analysing the ssspam simulations showed that soil diffusion and armouring processes significantly influence long term erosion rates which leads to differences in landform evolution high diffusion rates increase long term erosion rates leading to smoother landforms while low or no diffusion leads to a more incised landform the armouring process lowered the overall erosion rate of the landform and caused the channel flowing through the catchment to develop a coarse soil surface consistent with field observations long term ssspam simulation data revealed strong correlations between soil properties and the geomorphology of the landform both slope and contributing area positively correlate with surface median particle size diameter d 50 we conclude that this correlation arises through erosion and armouring comparison of simulated and field measured rock content data shows that ssspam produces the general trends of soil properties rock content existing at the field site and demonstrates its potential as a robust soilscape landform evolution model software and data availability the ssspam model and ssspam results analysis tools used in this study can be obtained through the following link https zenodo org record 7808748 zdarwxzbxaq the caesar lisflood model used in this study can be obtained through the following link https sourceforge net projects caesar lisflood various data used in this study such as digital elevation models and soil gradings can be obtained through the following link https zenodo org record 7808779 zdarlxzbxaq author contributions both authors welivitiya and hancock conceived the idea for this research while hancock secured the funding for this research both authors jointly carried out the methodology development and simulations using the two numerical models development of the ssspam model was carried out by welivitiya collation of data model results analysis and writing of the draft manuscript was also done by welivitiya supervision of the research project and reviewing of the manuscript was done by hancock declaration of competing interest the authors declare that they have no conflict of interest associated with this publication and the funding agencies which supported this work does not have any influence on the findings of this work acknowledgements this work was supported by australian coal association research program acarp project c27042 adaption of design tools to better design rehabilitation and capping over highly mobile mine waste and australian research council discovery grant dp110101216 
25389,classic shallow water equations swes and their various simplified approaches provide the theoretical basis for urban flood modeling aiming at the limitations of the local inertial approach lina this paper proposes a hybrid approach that combines the lina and classic swes to eliminate the errors caused by lina results show that the hybrid approach can reduce the error from 7 to 1 5 with slightly reduced computation efficiency in the dam break case in the urban flood case the hybrid approach can reduce the relative error on flood depth from 50 to 20 in a few grids while the relative error on flood velocity is improved from 60 to below 30 the hybrid approach proposed here can provide a practical strategy to eliminate the errors caused by the inherent defects of the simplified approach and still maintain the computation efficiency keywords hybrid approach shallow water equations local inertial approximation urban flood modeling froude number data availability i have share the link to my code and data in the manuscirpt file 1 introduction climate change and urbanization have greatly influenced the hydrological cycle of the natural world in which flood disasters have emerged as a grave consequence of the changing environment vousdoukas et al 2018 zhang et al 2018 numerous studies have emphasized the significant upward trend in the frequency and severity of urban floods in the past few decades azizi et al 2022 despite significant flood management efforts the flood risk is set to continue to increase in the future tellman et al 2021 in addition to engineering measures non engineering measures such as flood risk assessment flood warning forecasting etc provide alternative strategies for flood management the core technology of the above non engineering measures is numerical flood modeling luo et al 2022 which can determine flood characteristics including their extent depth and duration providing reliable data for historical scene reappearance and future scene prediction in a highly developed information era numerical flood simulation is considered the most economical and effective strategy for flood management therefore research on urban flood simulation technology is of great theoretical significance and practical value for flood management work shallow water equations swes are the basic control equations employed in flood modeling garcía navarro et al 2019 zhao and liang 2022 due to their physical significance swes have been comprehensively studied around the world however it is difficult to obtain the analytical solutions of flow characteristics using swes instead the approximate solutions namely the numerical solutions are usually obtained with numerical methods the rapid development of computer technology and numerical computation has facilitated the wide use of the hydrodynamic method with swes in the calculation of water flow evolution on the ground surface according to different ideas of discreteness the numerical methods can be mainly divided into three categories including the finite difference method fdm chu et al 2022 huang et al 2022 the finite volume method fvm glenis et al 2018 liu et al 2022 and the finite element method fem gunzburger et al 2022 zeng et al 2022 due to the large number of matrix calculations involved in the numerical methods the computational efficiency is inverse to the method complexity leading to unsatisfactory model requirements as a result researchers have successively proposed several simplified forms of swes in recent decades by omitting some items in the momentum equation levent kavvas et al 2021 wang et al 2021 aiming at providing comparable simulation results with high computational efficiency according to their scope of application among them the most commonly used simplified models are kinematic wave su and zhang 2022 and diffusion wave approximations caviedes voullième et al 2020 whose characteristics and limitations have been extensively studied previous studies costabile et al 2012 highlighted that the differences among the simulations were not significant when the simulations referred to commonly used ideal tests found in the literature which reduced the topography to a plane surface significant differences were observed in more complicated tests in which only the fully dynamic model was able to provide a good prediction of the observed discharges and water depths other simplified models have also been proposed particularly the local inertial approach lina the theoretical study of the lina began with the work by bates et al 2010 it was then applied in the two dimensional hydrodynamic model lisflood fp a globally recognized flood modeling tool bate found that the lina was the most efficient model with comparable accuracy among the simplified models scholars also solved the lina with the fdm and structured grids finding that neglecting the advection term produced some errors in the results cozzolino et al 2019 neal et al 2012 reported that the lack of an advection term in the momentum equation prevented the model from correctly representing the regions of the flow field characterized by critical or transcritical flow as expected they found that the lina systematically underestimated the wavefront speed in flooding experiments martins et al 2016 supplied the exact solution of a dam break for the lina and compared it with the corresponding solution of the classic swe finding that the lina failed to reproduce supercritical flows cea and bladé 2015 observed that the lina underestimated the extension of the flooded areas with froude numbers greater than about 0 8 according to the theoretical analyses available in the literature cozzolino et al 2019 de almeida and bates 2013 neal et al 2012 it is commonly believed that the lina model should only be applied in sub critical flow conditions and with gradually varying flow characteristics of overland flow change rapidly due to various situations such as terrain alternation and sudden overtopping inflow which introduce strong discontinuities in flow conditions chang et al 2014 2016 due to the omission of one or more terms in classic swes simplified forms always produce less satisfactory results in complicated flow conditions however not all overland flow computations involve such complicated flow conditions calculation with classic swes only takes a small amount of the total computing domain further numerical accuracy tends to reduce efficiency and vice versa castro orgaz and hager 2019 in this situation classic swes are not economically suitable for modeling regular flows that have relatively smooth profiles of water depths and velocities in most computing domains both accuracy and efficiency are crucial when evaluating the numerical performances of various overland flow models a hybrid approach that is capable of analyzing complicated flow conditions with high accuracy low efficiency schemes and simulating regular flows with low accuracy high efficiency schemes is not yet available for a wide range of flood modeling applications the aims of this paper are 1 to develop a lina based urban flood model with fvm and unstructured grids 2 to present a hybrid approach that analyzes complicated flow conditions with classic swes and simulates regular flows with lina 3 to reveal characteristics of the hybrid approach through theoretical and practical case studies in particular we propose a hybrid approach combining the lina and classic swes and construct an urban flood model using this approach the model is applied in a highly urbanized area to simulate an urban pluvial flood the remainder of this paper is organized as follows section 2 introduces the hybrid approach proposed in the study including the governing equations numerical solutions and the hybrid treatment section 3 presents the results including the theoretical testing and the practical urban flood cases section 4 discusses the results and finally conclusions are provided in section 5 2 methodology 2 1 governing equations the hybrid approach utilizes two groups of governing equations including classic swes and the lina previous studies have demonstrated that the lina should only be applied in sub critical flow conditions and with gradually varying flow thus the main framework of the hybrid approach is to apply the lina in cells with a low froude number and the classic swes in cells with a high froude number at each time step 2 1 1 classic swes the swes are derived from the conservation of mass and momentum by assuming hydrostatic pressure distribution considering that water quality is not involved in the current study the diffusion term of the equation is not included thus the classic swes can be written as follows 1 t h x h u y h v 0 2 t u h x h u 2 y h u v 1 2 x g h 2 g h x b x y τ b x 0 3 t v h y h v 2 x h u v 1 2 y g h 2 g h y b x y τ b y 0 where t represents the time x and y are the cartesian coordinates h is water depth u and v are the depth averaged velocity components in the x and y directions respectively g represents the gravitational acceleration b is the elevation of cell bottom τ b x and τ b y are the friction components in the x and y directions respectively equation 1 is the mass conservation equation equations 2 and 3 are the momentum conservation equations in the x and y directions respectively in the momentum equation the first term on the left is the local acceleration term which represents the time dependent change rate of momentum at any fixed position and implies fluid instability the second and third terms are convection terms the former of which is a flow direction term while the latter is a cross term representing the influence on the spatial gradient of flow momentum the fourth and fifth terms are the pressure and the bottom slope terms respectively both of which represent the gravity effect on the fluid movement the last term is the friction item 2 1 2 lina in general numerical simulation takes large amounts of computation time to solve classic swes for less time consuming simulation work the simplified form of classic swes has gained significant interest among researchers worldwide due to its high efficiency and acceptable accuracy the lina first proposed by bates et al 2010 ignores the convective term of the momentum equations it can be written as follows 4 t h x h u y h v 0 5 t u h 1 2 x g h 2 g h x b x y τ b x 0 t v h 1 2 y g h 2 g h y b x y τ b y 0 2 2 numerical solution 2 2 1 finite volume method on unstructured grids the lina is generally solved by fdm with structural grids in previous studies unlike unstructured grids structured grids are characterized by strong topological relationships which are less suitable for urban areas with complex and irregular boundaries therefore an unstructured grid with fvm is utilized to solve the lina and the classic swes here the lina can be written in a vector form as follows 6 u t f x g y s b s f where u represents the conservation variables in the water flow f and g are the flux in the x and y directions respectively s b and s f are the slope source and friction source terms respectively for a control volume the integral form of the lina is obtained through the gaussian divergence theorem and equation 6 the integral form of the lina is shown in equation 7 7 t u d ω ω e n d s s d ω where ω is the control volume e f g t is the flux term n is the unit outward vector that is normal to the boundary s s b s f t is the source term in the finite volume solution the spatial domain of integration is covered by a set of unstructured triangular cells the above formula is used for discrete approximation in a single control volume ω i the volume integral represents the integral of the whole cell area while the area integral represents the total flux passing through each cell boundary using u i to represent the average value of the conservative variables of ω i in the current time step the following equation can be obtained for each cell 8 u i t a i ω e n d s s d ω where the subscript i is the index for the control volume a i is the area of ω i m2 cell center unstructured grids are utilized in this solution of which all physical variables including water depth velocity momentum and mass are stored in the grid center as shown in fig 1 p and q are the centroids of two adjacent cells the shaded part is the control volume ω of cell p and w is the common edge of p and q for cell center grids each grid is a single control volume the physical variables in the control equation are distributed on the centroid of the grid and expressed as piecewise constants two assumptions are applied in equation 8 to facilitate the numerical solution firstly the integral of the time derivative of the conserved variable vector is uniformly distributed throughout the control volume and is the value at the center of the grid secondly the flux on the interface of the control volume can be discretized and uniformly distributed employing the two assumptions the following equation is obtained for grid p 9 u p t 1 ω p q m φ p q q m φ p q where φ p q is the flux term on the cell interface φ p q represents the slope source term and the friction source term m represents all the adjacent cells of cell p 2 2 2 flux calculation for the flux calculation there are some differences between the lina and classic swes the numerical solution for the flux term of the lina and classic swes are illustrated respectively as follows 2 2 2 1 solution for flux term of lina to calculate the numerical flux at the interface between cell p and cell q a one dimensional riemann problem is assumed on the left and right sides of the interface we employ the roe scheme riemann solver to solve the problem of inconsistency between the left and right sides of the interface according to this solver the approximate jacobian matrix for the lina is derived as 10 j r l 0 n x n y c 2 n x 0 0 c 2 n y 0 0 where superscript refers to the roe average value of variables c g h l h r 2 h l and h r represent the water depth on the left and right sides of the interface respectively from equation 10 eigenvector e and eigenvalue λ of j r l can be calculated as follows 11 e 1 1 c n x c n y e 2 0 c n y c n x e 3 1 c n x c n y 12 λ 1 c λ 2 0 λ 3 c the flux at the cell interface can be calculated using the following equation 13 φ p q 1 2 e r e l 1 2 j r l u r u l considering the convenience of calculation and programing matrix j r l can be replaced by the product form of the eigenvector and eigenvalue 14 j r l u r u l k 1 n e λ k α k e k in equation 14 α is calculated using 15 α 1 3 1 2 h r h l 1 2 c h r u r h l u l n x h r v r h l v l n y α 2 1 c h r v r h l v l n x h r u r h l u l n y the flux at the interface between cell p and cell q can be obtained by substituting equations 12 14 and 15 into equation 13 which is shown in equation 16 16 φ p q 1 2 δ h c h l u l h r u r n x h l v l h r v r n y 1 2 g 2 h l 2 h r 2 n x c n x 2 h l u l h r u r c n x n y h l v l h r v r 1 2 g 2 h l 2 h r 2 n y c n y 2 h l v l h r v r c n x n y h l u l h r u r where δ h h r h l m u l and u r are the x direction velocity on the left and right sides of the interface respectively m s v l and v r are the y direction velocities on the left and right side of the interface respectively m s 2 2 2 2 solution for flux term of classic swes the numerical solution of the flux term is almost the same for the lina and the classic swes when dealing with the riemann problem except when the roe scheme jacobian matrix and its related characteristic parameters are different for the classic swes the approximate jacobian matrix is 17 j r l 0 n x n y c 2 u 2 n x u v n y 2 u n x v n y u n y u v n x c 2 v 2 n y v n x u n x 2 v n y the eigenvector and eigenvalue of j r l can be calculated as follows 18 e 1 1 u c n x v c n y e 2 0 c n y c n x e 3 1 u c n x v c n y 19 λ 1 u n x v n y c λ 2 u n x v n y λ 3 u n x v n y c the matrix j r l can then be replaced by the product form of the eigenvector and eigenvalue 20 j r l u r u l k 1 n e λ k α k e k for classic swes α is calculated as 21 α 1 3 1 2 h r h l 1 2 c h r u r h l u l n x h r v r h l v l n y u n x v n y h r h l α 2 1 c h r v r h l v l v h r h l n x h r u r h l u l u h r h l n y the flux can then be obtained by substituting equations 19 21 into equation 13 2 2 3 slope source term we adopt a novel water surface reconstruction method as proposed by xia et al 2017 for the treatment of the slope term this method enables the model to correctly compute slope source terms and maintain numerical stability in areas with small water depths which greatly enhances the robustness of the model the slope source term can be calculated as follows 22 s b p 0 1 a i k 1 3 1 2 g h p h l k b p b f k n k l k where s b p is the slope source term b p is the bottom elevation of cell p m k is the index of the boundary h l k is the water depth on the left side of boundary k m n k and l k are the unit outward vector normal to the boundary k and length of the boundary k m b f k is the revised bottom elevation of boundary k in cell p m 23 b f b f δ b 24 δ b max 0 b f η p i f h q ε h 0 i f h q ε h where b f is the bottom elevation of the boundary m b f is the revised bottom elevation of the boundary h q is the water depth of cell q η p is the water surface elevation of cell p ε h is the minimum threshold value for the dry cell decision 2 2 4 friction source term the novel solution proposed by xia and liang 2018 is adopted to determine the friction source term and a completely implicit scheme is utilized to solve it by deriving and analyzing the solution of the implicit formula each physical variable can then be explicitly updated the following equation is obtained for the x direction 25 u x j 1 m x i f δ h f 10 3 m x m x 1 4 h f 2 h f f i δ h f 10 3 a similar equation is also obtained for the y direction 26 u y j 1 m y i f δ h f 10 3 m y m y 1 4 h f 2 h f i f δ h f 10 3 where m x u x j δ t a x m y u y j δ t a y u x and u y represent the unit flow for the x and y directions respectively m2 s u x h u u y h v a x and a y represent the components of momentum 1 a i k 1 3 e k j n k l k s b i j in the x and y directions respectively m2 s2 2 2 5 time updating a first order euler scheme is adopted for time updating which is calculated using equation 27 27 u j 1 u t δ t a i k 1 n e e k j n k l k δ t s b i j s f i j 1 where superscript j and j 1 represent the current and subsequent time steps δ t is the time step s in general the time step is decided by the courant friedrich levy cfl condition for classic swes the maximum time step that maintains the cfl condition is determined by equation 28 28 δ t c f l min d i g h i u i 2 v i 2 meanwhile for lina the maximum time step is obtained by equation 29 29 δ t c f l min d i g h i where d i is the minimum distance between the center of cell i and its boundaries m u i and v i is the velocity of cell i in the x and y directions respectively h i is the water depth of cell i c f l is the stable factor ranging from 0 to 1 2 3 hybrid treatment to achieve a high accuracy and high efficiency hybrid solution each interface between two cells in the entire research area is assigned a judgment weight w s for the selection of a flux calculation solution the value of w s ranges from 0 to 1 when w s is 1 the flux at the interface is calculated according to the solution of lina when w s is 0 the flux at the interface is calculated according to the solution of the classic swes when w s values are between 0 and 1 the flux at the interface is calculated through the weighted methods of the solution of lina and classic swes which is also a strategy to ensure the stability of the numerical model the hybrid approach consists of two sections including the w s decision and flux calculation a flow chart of the hybrid approach is shown in fig 2 as shown in fig 2 the first step of the hybrid approach is to obtain the flow state of all the calculation cells at the beginning of each time step and thus calculate the judgment weight w s at each interface in this study the froude number is used as the key index to decide the w s of each interface thus a preliminary method is obtained as equation 30 in which the w s for cell i is determined by the threshold value of the froude number 30 w s 0 i f f r δ 1 i f f r δ where f r is the froude number in cell i δ is the threshold value of the froude number in general the w s for all interfaces can be calculated according to the above method through the calculation of equation 30 w s at any interface is a value of 1 or 0 which means that there is a transient switch between the classic swes solution and the lina solution however due to the numerical difference between these two solutions the transient switch will lead to simulation instability wang et al 2016 proposed a continuous weight method to determine the w s between different solutions to reduce the numerical imbalance and result instability in other words several transition layers were used to eliminate the impact of numerical differences however if too many transition layers are utilized this method will lead to a wide range of transition areas in the computing domain both classic swe and lina solutions are used in the transition areas which will increase the calculation time considering the model s accuracy and efficiency we utilize one transition layer in this work firstly we determine the classic swes f r δ and lina f r δ cells according to the froude number of each cell as shown in fig 3 the red cell is the classic swes cell and the three boundaries of the classic swes cell are defined as classic swes interfaces meanwhile the lina cell that is adjacent to the classic swes cell is redefined as the transition cell except for the boundary shared between classic swes cell and transition cell the other two boundaries in the transition are redefined as transition interfaces the w s of the transition interface is decided by the distance from the transition cell center and classic swes cell center to the shared boundary the calculated method is as follows 31 w s d t d t d s where d t is the distance from the transition cell center to the shared boundary m d s is the distance from the classic swes cell center to the shared boundary m in summary the flux solution of all the interfaces can be calculated using the following equation 32 φ f 1 w s f φ f s w e s w s f φ f l i n a where φ f is the flux at the interface f w s f is the judgment weight of interface f φ f s w e s and φ f l i n a are the fluxes calculated by the classic swes solution and the lina solution at interface f respectively it is worth noting that time step is still decided by the courant friedrich levy cfl condition but the classic swes cells are calculated through equation 28 while the other cells are calculated through equation 29 and the time step for hybrid approach is the minimum value of time steps at all cells 3 results 3 1 case study of cylindrical dam break we employed the cylindrical dam break proposed by leveque 2002 in 2002 as the theoretical case study in this work as shown in fig 4 a cylindrical tank of 0 25 m in diameter is located in the center of a 5 m 5 m domain with four solid boundaries the tank and the remaining domain are initially filled with water of which the water levels are 2 and 1 m respectively the tank wall is assumed to be removed instantaneously to produce a 2d circular dam break wave due to the transitory dam break process and for conveniently analyzing and comparing the results we suggest that the gravity acceleration is set as 1 m2 s in this case during the dam break process the water level of the central cylindrical tank will change tempestuously and the flow around the tank will be in a state of supercritical flow it is appropriate to test the hybrid approach using this case as the lina will produce results with significant errors in these areas during this time and the threshold value of froude number in this case is set as 0 5 the classic swes is also adopted as a reference to be compared with the hybrid approach 3 1 1 water depth analysis fig 5 shows the water level profile along the x axis from the dam center to the boundary as simulated by three models including the classic swes lina and the hybrid approach all models strictly maintain the law of water balance during the whole simulation process the flood evolution process for every 0 5 s from t 0 to t 1 5 s is depicted in fig 5 the x axis in fig 5 represents the distance to the dam center while the y axis represents the water level at the beginning of the dam break t 0 5 s the flow regime changes sharply around in the dam center due to the inherent defects of the lina the simulation results have significant errors compared to the classic swes results from the lina model show that the water level at the dam center is about 0 2 m while the classic swes shows that the corresponding water level is about 0 8 m through the improvement of hybrid treatment the performance of the hybrid approach is quite satisfactory of which the corresponding water level is slightly under 0 8 m meanwhile near the dam center 0 0 25 m the simulated water level also has varying degrees of correction there are few or no computational grids using the hybrid approach in the computational domain at t 1 s and t 1 5 s as the flow regime in the study area tends to be stable the simulation results of the hybrid approach are consistent with those of the lina and both are slightly different from the classic swes at the front wave table 1 shows the mean absolute percentage error mape of the lina and the hybrid approach compared to the classic swes the proportion of grids that utilize the hybrid approach at the corresponding time steps is also listed in table 1 it can be seen from the table that the hybrid treatment reduces the error of the simulation results from 6 5 to 1 5 at time step t 0 5 s it is worth noting that the error of the simulation results in a few grids around the dam center from the lina reaches 31 2 while the corresponding error of the hybrid approach is only 6 1 which implies that the hybrid approach plays an important role in this improvement table 1 also shows the proportion of grids that utilize the hybrid approach at time step t 0 5 s the number of grids utilizing the hybrid approach only accounts for 2 49 of the total cells in the calculation domain after time step t 1 s there are no calculation grids using the hybrid approach in the calculation domain this indicates that the hybrid approach can greatly improve the simulation accuracy of the lina by working in a few grids and a few time steps 3 1 2 computation efficiency table 2 shows the calculation time and acceleration ratio of the three models in this case as illustrated in the table the lina is the most efficient among the three models which takes 9 59 s to simulate the theoretical case and increases the efficiency by 42 compared to the classic swes meanwhile the hybrid approach performs as effectively as the lina from the perspective of computation efficiency taking 9 74 s in this case this result indicates that the hybrid approach has little impact on the model efficiency combining the accuracy analysis described above the results of the cylindrical dam break case indicate that the hybrid approach proposed in this paper can significantly improve the simulation accuracy of the lina without reducing the calculation efficiency 3 2 case study of urban pluvial flood to further reveal the feasibility of hybrid approach in urban flood modeling a hybrid approach based coupled model was developed and applied in a catchment scale research region in the coupled model the hybrid approach was utilized to model the water flow on the ground surface while the stormwater management model swmm was utilized to model the water flow in the underground pipes rainfall with a return period of 100a was employed in the research scenario considering that the froude number is the important variable for modeling accuracy three different threshold values for the froude number including 0 3 0 5 and 0 8 are considered in the current simulation the results of the classic swe are regarded as the benchmark for our quantificational analysis of the model results an analysis of the mass balance maximum flood depth maximum flood velocity and computation efficiency is discussed subsequently 3 2 1 mass balance analysis mass balance analysis is an important self check process for numerical modeling the hybrid approach increases the complexity of flux calculation between two different adjacent grids which may cause instability during the calculation process as such it is essential to examine the water balance of the hybrid approach fig 6 a shows the water volumes on the ground surface produced by classic swes and the hybrid approach and fig 6 b shows the water volumes in the underground pipe system according to the results in the figure the water volumes of different models are highly consistent during the calculation process which implies that the hybrid model can maintain the law of mass balance 3 2 2 maximum flood depth analysis the maximum flood depth for each grid is employed for analysis fig 7 shows the relative errors of different models according to fig 7 a the relative errors of maximum flood depths for the lina reach beyond 50 at a few grids while the hybrid approach is less than 20 the height of the data box implies that the dispersion of relative errors is significantly reduced the upper and lower quartiles are within the range of 0 10 under three kinds of froude number threshold values further the accuracy of model results increases as the froude number threshold value decreases fig 7 b e shows the spatial distribution of relative error for different models it is worth noting that the hybrid approach performs inhibitory effect on grids with large errors in summary the results shows that the hybrid approach applied in this case significantly improves the result accuracy compared with the lina table 3 shows the mean absolute percentage error mape of the maximum flood depth for models with different threshold values and grids with different froude numbers the total computation domain is divided into three parts according to the froude number which are 0 0 3 0 3 0 8 and above 0 8 respectively it can be seen from the table that the hybrid approach obtains a significant improvement on the grids with froude numbers that are larger than 0 8 in the case of the hybrid approach with δ 0 3 and for grids with a froude number ranging between 0 3 and 0 8 the mape of the results decreases from 9 53 to 0 97 of which the model accuracy increases by approximately 10 times for the grids with froude numbers that are larger than 0 8 the mape decreases from 19 84 to 0 82 of which the model accuracy increases by 20 times in the case of the hybrid approach with δ 0 8 for the grids with froude numbers that are greater than 0 8 the mape decreases from 19 48 to 4 51 it can also be observed that the improvement in the model accuracy at grids with a froude number that is larger than the threshold value will have a positive impact on the surrounding grids for example for the grids with froude numbers that are less than 0 3 the mape from hybrid approaches with δ 0 3 0 5 and 0 8 decreases from 7 46 to 1 58 3 39 and 6 21 respectively 3 2 3 maximum flood velocity analysis fig 8 a shows the box plot of the relative error for the maximum flood velocity it can be found that the relative errors of maximum flood velocity for the lina reach approximately 60 at a few grids the relative error of the hybrid approach is below 30 and the upper quartiles of the relative error are around 10 indicating that the improvement in the maximum flood velocity using the hybrid approach is significant unlike the maximum flood depth there is no significant difference in the velocity between the cases δ 0 5 and δ 0 8 however from fig 8 d and e it can be found that the model accuracy in some local area of δ 0 8 is not as well as δ 0 5 table 4 shows the mape of the maximum flood velocity for models with different threshold values and grids with different froude numbers it can also be observed that the hybrid approach provides a significantly improved flood velocity in the case of the hybrid approach with δ 0 3 for the grids with froude numbers ranging from 0 3 to 0 8 the mape of the results decreases from 8 12 to 1 2 in the case of the hybrid approach with δ 0 8 for the grids with froude numbers greater than 0 8 the mape of the results decreases from 15 79 to 2 89 the same result is observed for flood depth whereby the improvement of model accuracy at the grids with froude numbers larger than the threshold value has a positive impact on the surrounding grids 3 2 4 computation efficiency analysis in the hybrid approach the numerical method for each grid changes with its flow regime at every time step the time dependent proportion of grids that utilize the hybrid approach is summarized in fig 9 as illustrated for the three models the proportion rises first and falls as time advances the proportion also varies according to the threshold value of the froude number for the hybrid approach with a threshold value of 0 3 the maximum proportion reaches 37 for the threshold value of 0 8 the maximum proportion only accounts for 5 model efficiency is also important in addition to accuracy thus we compared the computation time for the hybrid approach with lina and the classic swes as shown in table 5 the flood results were simulated under a rainstorm with a return period of 100a the hybrid approach with a threshold value of 0 3 0 5 and 0 8 obtains results in 239 38 200 36 and 196 01 s respectively compared to the classic swes the hybrid approach improves efficiency from 113 to 138 combined with the model accuracy a threshold value of 0 5 is recommended in practical cases using this threshold value the hybrid approach can produce results with less than 4 errors and improve the speed by 34 in summary the hybrid approach will be utilized more frequently as the threshold value of the froude number decreases which increases the computation time as well as the accuracy of the results 4 discussion the hybrid approach proposed in this study analyzes complicated flow conditions with classic swes and simulates regular flows with the lina in this way the hybrid approach can produce comparable results to classic swes with a similar computing time to the lina this method takes advantage of the classic swes for complex flow computing and the lina for time saving previous studies have obtained similar results such as yu and chang 2021 who adopted two numerical schemes for different flow conditions which were identified through the sharpness of discontinuity the rusanov scheme was considered as the low accuracy high efficiency scheme while the hllc solver was employed as the high accuracy low efficiency scheme according to their findings the proposed hybrid approach could simulate as accurately as the high accuracy scheme with significant reductions in computational time the same governing equations were employed for the two schemes which made up the hybrid mode however we extend the research in our study by employing two different governing equation systems including lina and classic swes which are combined into the hybrid approach the froude number is used to control the switch between these two governing equation systems which is determined according to previous studies on the lina results show that this attempt is feasible and stable which provides a theoretical guideline for the improvement of other simplified swes however it is worth noting that the control of the switching among different governing equation systems should be based on the characteristics of the simplified equations being utilized the threshold value of the froude number is found to significantly influence the calculation efficiency and result accuracy for a threshold value of 0 3 almost 37 of the total domain utilizes the hybrid approach in the peak time as a result the computation efficiency is only improved by 13 and the mape decreases to below 2 however for the thresholds of 0 5 and 0 8 10 and 4 of the total domain utilize the hybrid approach in the peak time respectively the improvement of computation efficiency increases to approximately 35 and the mape decreases to below 4 and 7 the threshold value is a key judgment factor in the proposed method for balancing simulation efficiency and result accuracy which requires more comprehensive research based on the characteristics of the research region besides the threshold value the efficiency of the hybrid approach is also related to the computation domain that it is utilized in the efficiency level of the proposed approach depends on the flow conditions involved in our study the proposed hybrid approach is applied in a practical case study instead of an entirely flat area the terrain of the research region varies greatly in the southern part in this case the hybrid approach is frequently utilized to handle complex flows thus the speed up ratio in the current research only reaches 13 38 if the research region is flat terrain the acceleration ratio will be as significant as the simplified approach simplified approaches including the lina diffusion wave approximation and kinematic wave approximation have been applied in urban flood modeling for many years as is known such approaches cannot comprehensively reflect the shallow water flows due to the omission of one or several terms by the classic swes which is the inherent defect of simplified approaches as they are always applied according to their limitations to produce acceptable flood results minor errors and excellent efficiency alleviate concerns regarding their defects thus the simplified approaches are widely accepted as the theoretical basis of urban flood modeling despite their limitations marangoz and anilan 2021 savant et al 2019 we propose the hybrid approach in this work to reduce these constraints which can provide a practical strategy to eliminate the errors caused by the inherent defects of the simplified approach while maintaining computation efficiency further according to previous studies and the above results the computation efficiency improvement of the simplified or hybrid approaches is far lower than that of parallel computation but the efficiency improvement of both hardware gpu and cpu and algorithms simplification methods can be overlapped research on efficiency improvement based on theoretical aspect is also important the exploration on the theoretical basis makes sense of the current research the hybrid approach proposed in this research does not change the initial calculation procedures to some extent it gives each cell a tag related to the calculation methods thus parallel computing technology can be performed in this hybrid approach while the hybrid approach proposed here reduces the errors of grids with large froude numbers in lina producing comparable results with slightly lower computation efficiency the speed of the lina frontal shock is slightly reduced compared to the speed of the fastest signal in the classic swes in other words the spread speed of the wet dry front modeled by the lina is slightly lower than the actual situation this problem is not addressed in the current research and will be explored in the future 5 conclusions this work presented a hybrid approach that combined the lina and classic swes where the swes were solved with fvm and unstructured grids this approach was successfully applied in a theoretical cylindrical dam break case and a practical urban pluvial flood case the main conclusions were as follows 1 the lina was successfully solved with fvm and unstructured grids the results of the cylindrical dam break case and urban pluvial flood case showed that the lina could increase the computation efficiency by 42 and 47 with an acceptable error compared to the classic swes 2 the hybrid approach took advantage of the classic swes in complex flow computing and the lina in time saving through hybrid treatment errors were reduced from 6 5 to 1 5 at time step t 0 5 s for the cylindrical dam break case and from 19 48 to 0 82 for the urban flood case in both cases the efficiency of the hybrid approach was similar to that of the lina 3 the threshold value of the froude number greatly influenced the calculation efficiency and result accuracy for a threshold value of 0 3 the computation efficiency was improved by only 13 and the mape decreased to below 2 for thresholds of 0 5 and 0 8 the computation efficiency increased to approximately 35 and the mape decreased to below 4 and 7 respectively software availability name of the software hufm hybrid urban flood model developer weiqi wang wenjie chen guoru huang contact information wangwq iwhr com year first available 2023 program language c cost free software availability https github com scut 777 hybird urban flood model program size 1 mb declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was funded by the open research fund of state key laboratory of simulation and regulation of water cycle in river basin china institute of water resources and hydropower research grant no iwhr skl kf202109 the national natural science foundation of china 52109018 51739011 and the science and technology planning project of guangdong province in china 2022a1515010131 
25389,classic shallow water equations swes and their various simplified approaches provide the theoretical basis for urban flood modeling aiming at the limitations of the local inertial approach lina this paper proposes a hybrid approach that combines the lina and classic swes to eliminate the errors caused by lina results show that the hybrid approach can reduce the error from 7 to 1 5 with slightly reduced computation efficiency in the dam break case in the urban flood case the hybrid approach can reduce the relative error on flood depth from 50 to 20 in a few grids while the relative error on flood velocity is improved from 60 to below 30 the hybrid approach proposed here can provide a practical strategy to eliminate the errors caused by the inherent defects of the simplified approach and still maintain the computation efficiency keywords hybrid approach shallow water equations local inertial approximation urban flood modeling froude number data availability i have share the link to my code and data in the manuscirpt file 1 introduction climate change and urbanization have greatly influenced the hydrological cycle of the natural world in which flood disasters have emerged as a grave consequence of the changing environment vousdoukas et al 2018 zhang et al 2018 numerous studies have emphasized the significant upward trend in the frequency and severity of urban floods in the past few decades azizi et al 2022 despite significant flood management efforts the flood risk is set to continue to increase in the future tellman et al 2021 in addition to engineering measures non engineering measures such as flood risk assessment flood warning forecasting etc provide alternative strategies for flood management the core technology of the above non engineering measures is numerical flood modeling luo et al 2022 which can determine flood characteristics including their extent depth and duration providing reliable data for historical scene reappearance and future scene prediction in a highly developed information era numerical flood simulation is considered the most economical and effective strategy for flood management therefore research on urban flood simulation technology is of great theoretical significance and practical value for flood management work shallow water equations swes are the basic control equations employed in flood modeling garcía navarro et al 2019 zhao and liang 2022 due to their physical significance swes have been comprehensively studied around the world however it is difficult to obtain the analytical solutions of flow characteristics using swes instead the approximate solutions namely the numerical solutions are usually obtained with numerical methods the rapid development of computer technology and numerical computation has facilitated the wide use of the hydrodynamic method with swes in the calculation of water flow evolution on the ground surface according to different ideas of discreteness the numerical methods can be mainly divided into three categories including the finite difference method fdm chu et al 2022 huang et al 2022 the finite volume method fvm glenis et al 2018 liu et al 2022 and the finite element method fem gunzburger et al 2022 zeng et al 2022 due to the large number of matrix calculations involved in the numerical methods the computational efficiency is inverse to the method complexity leading to unsatisfactory model requirements as a result researchers have successively proposed several simplified forms of swes in recent decades by omitting some items in the momentum equation levent kavvas et al 2021 wang et al 2021 aiming at providing comparable simulation results with high computational efficiency according to their scope of application among them the most commonly used simplified models are kinematic wave su and zhang 2022 and diffusion wave approximations caviedes voullième et al 2020 whose characteristics and limitations have been extensively studied previous studies costabile et al 2012 highlighted that the differences among the simulations were not significant when the simulations referred to commonly used ideal tests found in the literature which reduced the topography to a plane surface significant differences were observed in more complicated tests in which only the fully dynamic model was able to provide a good prediction of the observed discharges and water depths other simplified models have also been proposed particularly the local inertial approach lina the theoretical study of the lina began with the work by bates et al 2010 it was then applied in the two dimensional hydrodynamic model lisflood fp a globally recognized flood modeling tool bate found that the lina was the most efficient model with comparable accuracy among the simplified models scholars also solved the lina with the fdm and structured grids finding that neglecting the advection term produced some errors in the results cozzolino et al 2019 neal et al 2012 reported that the lack of an advection term in the momentum equation prevented the model from correctly representing the regions of the flow field characterized by critical or transcritical flow as expected they found that the lina systematically underestimated the wavefront speed in flooding experiments martins et al 2016 supplied the exact solution of a dam break for the lina and compared it with the corresponding solution of the classic swe finding that the lina failed to reproduce supercritical flows cea and bladé 2015 observed that the lina underestimated the extension of the flooded areas with froude numbers greater than about 0 8 according to the theoretical analyses available in the literature cozzolino et al 2019 de almeida and bates 2013 neal et al 2012 it is commonly believed that the lina model should only be applied in sub critical flow conditions and with gradually varying flow characteristics of overland flow change rapidly due to various situations such as terrain alternation and sudden overtopping inflow which introduce strong discontinuities in flow conditions chang et al 2014 2016 due to the omission of one or more terms in classic swes simplified forms always produce less satisfactory results in complicated flow conditions however not all overland flow computations involve such complicated flow conditions calculation with classic swes only takes a small amount of the total computing domain further numerical accuracy tends to reduce efficiency and vice versa castro orgaz and hager 2019 in this situation classic swes are not economically suitable for modeling regular flows that have relatively smooth profiles of water depths and velocities in most computing domains both accuracy and efficiency are crucial when evaluating the numerical performances of various overland flow models a hybrid approach that is capable of analyzing complicated flow conditions with high accuracy low efficiency schemes and simulating regular flows with low accuracy high efficiency schemes is not yet available for a wide range of flood modeling applications the aims of this paper are 1 to develop a lina based urban flood model with fvm and unstructured grids 2 to present a hybrid approach that analyzes complicated flow conditions with classic swes and simulates regular flows with lina 3 to reveal characteristics of the hybrid approach through theoretical and practical case studies in particular we propose a hybrid approach combining the lina and classic swes and construct an urban flood model using this approach the model is applied in a highly urbanized area to simulate an urban pluvial flood the remainder of this paper is organized as follows section 2 introduces the hybrid approach proposed in the study including the governing equations numerical solutions and the hybrid treatment section 3 presents the results including the theoretical testing and the practical urban flood cases section 4 discusses the results and finally conclusions are provided in section 5 2 methodology 2 1 governing equations the hybrid approach utilizes two groups of governing equations including classic swes and the lina previous studies have demonstrated that the lina should only be applied in sub critical flow conditions and with gradually varying flow thus the main framework of the hybrid approach is to apply the lina in cells with a low froude number and the classic swes in cells with a high froude number at each time step 2 1 1 classic swes the swes are derived from the conservation of mass and momentum by assuming hydrostatic pressure distribution considering that water quality is not involved in the current study the diffusion term of the equation is not included thus the classic swes can be written as follows 1 t h x h u y h v 0 2 t u h x h u 2 y h u v 1 2 x g h 2 g h x b x y τ b x 0 3 t v h y h v 2 x h u v 1 2 y g h 2 g h y b x y τ b y 0 where t represents the time x and y are the cartesian coordinates h is water depth u and v are the depth averaged velocity components in the x and y directions respectively g represents the gravitational acceleration b is the elevation of cell bottom τ b x and τ b y are the friction components in the x and y directions respectively equation 1 is the mass conservation equation equations 2 and 3 are the momentum conservation equations in the x and y directions respectively in the momentum equation the first term on the left is the local acceleration term which represents the time dependent change rate of momentum at any fixed position and implies fluid instability the second and third terms are convection terms the former of which is a flow direction term while the latter is a cross term representing the influence on the spatial gradient of flow momentum the fourth and fifth terms are the pressure and the bottom slope terms respectively both of which represent the gravity effect on the fluid movement the last term is the friction item 2 1 2 lina in general numerical simulation takes large amounts of computation time to solve classic swes for less time consuming simulation work the simplified form of classic swes has gained significant interest among researchers worldwide due to its high efficiency and acceptable accuracy the lina first proposed by bates et al 2010 ignores the convective term of the momentum equations it can be written as follows 4 t h x h u y h v 0 5 t u h 1 2 x g h 2 g h x b x y τ b x 0 t v h 1 2 y g h 2 g h y b x y τ b y 0 2 2 numerical solution 2 2 1 finite volume method on unstructured grids the lina is generally solved by fdm with structural grids in previous studies unlike unstructured grids structured grids are characterized by strong topological relationships which are less suitable for urban areas with complex and irregular boundaries therefore an unstructured grid with fvm is utilized to solve the lina and the classic swes here the lina can be written in a vector form as follows 6 u t f x g y s b s f where u represents the conservation variables in the water flow f and g are the flux in the x and y directions respectively s b and s f are the slope source and friction source terms respectively for a control volume the integral form of the lina is obtained through the gaussian divergence theorem and equation 6 the integral form of the lina is shown in equation 7 7 t u d ω ω e n d s s d ω where ω is the control volume e f g t is the flux term n is the unit outward vector that is normal to the boundary s s b s f t is the source term in the finite volume solution the spatial domain of integration is covered by a set of unstructured triangular cells the above formula is used for discrete approximation in a single control volume ω i the volume integral represents the integral of the whole cell area while the area integral represents the total flux passing through each cell boundary using u i to represent the average value of the conservative variables of ω i in the current time step the following equation can be obtained for each cell 8 u i t a i ω e n d s s d ω where the subscript i is the index for the control volume a i is the area of ω i m2 cell center unstructured grids are utilized in this solution of which all physical variables including water depth velocity momentum and mass are stored in the grid center as shown in fig 1 p and q are the centroids of two adjacent cells the shaded part is the control volume ω of cell p and w is the common edge of p and q for cell center grids each grid is a single control volume the physical variables in the control equation are distributed on the centroid of the grid and expressed as piecewise constants two assumptions are applied in equation 8 to facilitate the numerical solution firstly the integral of the time derivative of the conserved variable vector is uniformly distributed throughout the control volume and is the value at the center of the grid secondly the flux on the interface of the control volume can be discretized and uniformly distributed employing the two assumptions the following equation is obtained for grid p 9 u p t 1 ω p q m φ p q q m φ p q where φ p q is the flux term on the cell interface φ p q represents the slope source term and the friction source term m represents all the adjacent cells of cell p 2 2 2 flux calculation for the flux calculation there are some differences between the lina and classic swes the numerical solution for the flux term of the lina and classic swes are illustrated respectively as follows 2 2 2 1 solution for flux term of lina to calculate the numerical flux at the interface between cell p and cell q a one dimensional riemann problem is assumed on the left and right sides of the interface we employ the roe scheme riemann solver to solve the problem of inconsistency between the left and right sides of the interface according to this solver the approximate jacobian matrix for the lina is derived as 10 j r l 0 n x n y c 2 n x 0 0 c 2 n y 0 0 where superscript refers to the roe average value of variables c g h l h r 2 h l and h r represent the water depth on the left and right sides of the interface respectively from equation 10 eigenvector e and eigenvalue λ of j r l can be calculated as follows 11 e 1 1 c n x c n y e 2 0 c n y c n x e 3 1 c n x c n y 12 λ 1 c λ 2 0 λ 3 c the flux at the cell interface can be calculated using the following equation 13 φ p q 1 2 e r e l 1 2 j r l u r u l considering the convenience of calculation and programing matrix j r l can be replaced by the product form of the eigenvector and eigenvalue 14 j r l u r u l k 1 n e λ k α k e k in equation 14 α is calculated using 15 α 1 3 1 2 h r h l 1 2 c h r u r h l u l n x h r v r h l v l n y α 2 1 c h r v r h l v l n x h r u r h l u l n y the flux at the interface between cell p and cell q can be obtained by substituting equations 12 14 and 15 into equation 13 which is shown in equation 16 16 φ p q 1 2 δ h c h l u l h r u r n x h l v l h r v r n y 1 2 g 2 h l 2 h r 2 n x c n x 2 h l u l h r u r c n x n y h l v l h r v r 1 2 g 2 h l 2 h r 2 n y c n y 2 h l v l h r v r c n x n y h l u l h r u r where δ h h r h l m u l and u r are the x direction velocity on the left and right sides of the interface respectively m s v l and v r are the y direction velocities on the left and right side of the interface respectively m s 2 2 2 2 solution for flux term of classic swes the numerical solution of the flux term is almost the same for the lina and the classic swes when dealing with the riemann problem except when the roe scheme jacobian matrix and its related characteristic parameters are different for the classic swes the approximate jacobian matrix is 17 j r l 0 n x n y c 2 u 2 n x u v n y 2 u n x v n y u n y u v n x c 2 v 2 n y v n x u n x 2 v n y the eigenvector and eigenvalue of j r l can be calculated as follows 18 e 1 1 u c n x v c n y e 2 0 c n y c n x e 3 1 u c n x v c n y 19 λ 1 u n x v n y c λ 2 u n x v n y λ 3 u n x v n y c the matrix j r l can then be replaced by the product form of the eigenvector and eigenvalue 20 j r l u r u l k 1 n e λ k α k e k for classic swes α is calculated as 21 α 1 3 1 2 h r h l 1 2 c h r u r h l u l n x h r v r h l v l n y u n x v n y h r h l α 2 1 c h r v r h l v l v h r h l n x h r u r h l u l u h r h l n y the flux can then be obtained by substituting equations 19 21 into equation 13 2 2 3 slope source term we adopt a novel water surface reconstruction method as proposed by xia et al 2017 for the treatment of the slope term this method enables the model to correctly compute slope source terms and maintain numerical stability in areas with small water depths which greatly enhances the robustness of the model the slope source term can be calculated as follows 22 s b p 0 1 a i k 1 3 1 2 g h p h l k b p b f k n k l k where s b p is the slope source term b p is the bottom elevation of cell p m k is the index of the boundary h l k is the water depth on the left side of boundary k m n k and l k are the unit outward vector normal to the boundary k and length of the boundary k m b f k is the revised bottom elevation of boundary k in cell p m 23 b f b f δ b 24 δ b max 0 b f η p i f h q ε h 0 i f h q ε h where b f is the bottom elevation of the boundary m b f is the revised bottom elevation of the boundary h q is the water depth of cell q η p is the water surface elevation of cell p ε h is the minimum threshold value for the dry cell decision 2 2 4 friction source term the novel solution proposed by xia and liang 2018 is adopted to determine the friction source term and a completely implicit scheme is utilized to solve it by deriving and analyzing the solution of the implicit formula each physical variable can then be explicitly updated the following equation is obtained for the x direction 25 u x j 1 m x i f δ h f 10 3 m x m x 1 4 h f 2 h f f i δ h f 10 3 a similar equation is also obtained for the y direction 26 u y j 1 m y i f δ h f 10 3 m y m y 1 4 h f 2 h f i f δ h f 10 3 where m x u x j δ t a x m y u y j δ t a y u x and u y represent the unit flow for the x and y directions respectively m2 s u x h u u y h v a x and a y represent the components of momentum 1 a i k 1 3 e k j n k l k s b i j in the x and y directions respectively m2 s2 2 2 5 time updating a first order euler scheme is adopted for time updating which is calculated using equation 27 27 u j 1 u t δ t a i k 1 n e e k j n k l k δ t s b i j s f i j 1 where superscript j and j 1 represent the current and subsequent time steps δ t is the time step s in general the time step is decided by the courant friedrich levy cfl condition for classic swes the maximum time step that maintains the cfl condition is determined by equation 28 28 δ t c f l min d i g h i u i 2 v i 2 meanwhile for lina the maximum time step is obtained by equation 29 29 δ t c f l min d i g h i where d i is the minimum distance between the center of cell i and its boundaries m u i and v i is the velocity of cell i in the x and y directions respectively h i is the water depth of cell i c f l is the stable factor ranging from 0 to 1 2 3 hybrid treatment to achieve a high accuracy and high efficiency hybrid solution each interface between two cells in the entire research area is assigned a judgment weight w s for the selection of a flux calculation solution the value of w s ranges from 0 to 1 when w s is 1 the flux at the interface is calculated according to the solution of lina when w s is 0 the flux at the interface is calculated according to the solution of the classic swes when w s values are between 0 and 1 the flux at the interface is calculated through the weighted methods of the solution of lina and classic swes which is also a strategy to ensure the stability of the numerical model the hybrid approach consists of two sections including the w s decision and flux calculation a flow chart of the hybrid approach is shown in fig 2 as shown in fig 2 the first step of the hybrid approach is to obtain the flow state of all the calculation cells at the beginning of each time step and thus calculate the judgment weight w s at each interface in this study the froude number is used as the key index to decide the w s of each interface thus a preliminary method is obtained as equation 30 in which the w s for cell i is determined by the threshold value of the froude number 30 w s 0 i f f r δ 1 i f f r δ where f r is the froude number in cell i δ is the threshold value of the froude number in general the w s for all interfaces can be calculated according to the above method through the calculation of equation 30 w s at any interface is a value of 1 or 0 which means that there is a transient switch between the classic swes solution and the lina solution however due to the numerical difference between these two solutions the transient switch will lead to simulation instability wang et al 2016 proposed a continuous weight method to determine the w s between different solutions to reduce the numerical imbalance and result instability in other words several transition layers were used to eliminate the impact of numerical differences however if too many transition layers are utilized this method will lead to a wide range of transition areas in the computing domain both classic swe and lina solutions are used in the transition areas which will increase the calculation time considering the model s accuracy and efficiency we utilize one transition layer in this work firstly we determine the classic swes f r δ and lina f r δ cells according to the froude number of each cell as shown in fig 3 the red cell is the classic swes cell and the three boundaries of the classic swes cell are defined as classic swes interfaces meanwhile the lina cell that is adjacent to the classic swes cell is redefined as the transition cell except for the boundary shared between classic swes cell and transition cell the other two boundaries in the transition are redefined as transition interfaces the w s of the transition interface is decided by the distance from the transition cell center and classic swes cell center to the shared boundary the calculated method is as follows 31 w s d t d t d s where d t is the distance from the transition cell center to the shared boundary m d s is the distance from the classic swes cell center to the shared boundary m in summary the flux solution of all the interfaces can be calculated using the following equation 32 φ f 1 w s f φ f s w e s w s f φ f l i n a where φ f is the flux at the interface f w s f is the judgment weight of interface f φ f s w e s and φ f l i n a are the fluxes calculated by the classic swes solution and the lina solution at interface f respectively it is worth noting that time step is still decided by the courant friedrich levy cfl condition but the classic swes cells are calculated through equation 28 while the other cells are calculated through equation 29 and the time step for hybrid approach is the minimum value of time steps at all cells 3 results 3 1 case study of cylindrical dam break we employed the cylindrical dam break proposed by leveque 2002 in 2002 as the theoretical case study in this work as shown in fig 4 a cylindrical tank of 0 25 m in diameter is located in the center of a 5 m 5 m domain with four solid boundaries the tank and the remaining domain are initially filled with water of which the water levels are 2 and 1 m respectively the tank wall is assumed to be removed instantaneously to produce a 2d circular dam break wave due to the transitory dam break process and for conveniently analyzing and comparing the results we suggest that the gravity acceleration is set as 1 m2 s in this case during the dam break process the water level of the central cylindrical tank will change tempestuously and the flow around the tank will be in a state of supercritical flow it is appropriate to test the hybrid approach using this case as the lina will produce results with significant errors in these areas during this time and the threshold value of froude number in this case is set as 0 5 the classic swes is also adopted as a reference to be compared with the hybrid approach 3 1 1 water depth analysis fig 5 shows the water level profile along the x axis from the dam center to the boundary as simulated by three models including the classic swes lina and the hybrid approach all models strictly maintain the law of water balance during the whole simulation process the flood evolution process for every 0 5 s from t 0 to t 1 5 s is depicted in fig 5 the x axis in fig 5 represents the distance to the dam center while the y axis represents the water level at the beginning of the dam break t 0 5 s the flow regime changes sharply around in the dam center due to the inherent defects of the lina the simulation results have significant errors compared to the classic swes results from the lina model show that the water level at the dam center is about 0 2 m while the classic swes shows that the corresponding water level is about 0 8 m through the improvement of hybrid treatment the performance of the hybrid approach is quite satisfactory of which the corresponding water level is slightly under 0 8 m meanwhile near the dam center 0 0 25 m the simulated water level also has varying degrees of correction there are few or no computational grids using the hybrid approach in the computational domain at t 1 s and t 1 5 s as the flow regime in the study area tends to be stable the simulation results of the hybrid approach are consistent with those of the lina and both are slightly different from the classic swes at the front wave table 1 shows the mean absolute percentage error mape of the lina and the hybrid approach compared to the classic swes the proportion of grids that utilize the hybrid approach at the corresponding time steps is also listed in table 1 it can be seen from the table that the hybrid treatment reduces the error of the simulation results from 6 5 to 1 5 at time step t 0 5 s it is worth noting that the error of the simulation results in a few grids around the dam center from the lina reaches 31 2 while the corresponding error of the hybrid approach is only 6 1 which implies that the hybrid approach plays an important role in this improvement table 1 also shows the proportion of grids that utilize the hybrid approach at time step t 0 5 s the number of grids utilizing the hybrid approach only accounts for 2 49 of the total cells in the calculation domain after time step t 1 s there are no calculation grids using the hybrid approach in the calculation domain this indicates that the hybrid approach can greatly improve the simulation accuracy of the lina by working in a few grids and a few time steps 3 1 2 computation efficiency table 2 shows the calculation time and acceleration ratio of the three models in this case as illustrated in the table the lina is the most efficient among the three models which takes 9 59 s to simulate the theoretical case and increases the efficiency by 42 compared to the classic swes meanwhile the hybrid approach performs as effectively as the lina from the perspective of computation efficiency taking 9 74 s in this case this result indicates that the hybrid approach has little impact on the model efficiency combining the accuracy analysis described above the results of the cylindrical dam break case indicate that the hybrid approach proposed in this paper can significantly improve the simulation accuracy of the lina without reducing the calculation efficiency 3 2 case study of urban pluvial flood to further reveal the feasibility of hybrid approach in urban flood modeling a hybrid approach based coupled model was developed and applied in a catchment scale research region in the coupled model the hybrid approach was utilized to model the water flow on the ground surface while the stormwater management model swmm was utilized to model the water flow in the underground pipes rainfall with a return period of 100a was employed in the research scenario considering that the froude number is the important variable for modeling accuracy three different threshold values for the froude number including 0 3 0 5 and 0 8 are considered in the current simulation the results of the classic swe are regarded as the benchmark for our quantificational analysis of the model results an analysis of the mass balance maximum flood depth maximum flood velocity and computation efficiency is discussed subsequently 3 2 1 mass balance analysis mass balance analysis is an important self check process for numerical modeling the hybrid approach increases the complexity of flux calculation between two different adjacent grids which may cause instability during the calculation process as such it is essential to examine the water balance of the hybrid approach fig 6 a shows the water volumes on the ground surface produced by classic swes and the hybrid approach and fig 6 b shows the water volumes in the underground pipe system according to the results in the figure the water volumes of different models are highly consistent during the calculation process which implies that the hybrid model can maintain the law of mass balance 3 2 2 maximum flood depth analysis the maximum flood depth for each grid is employed for analysis fig 7 shows the relative errors of different models according to fig 7 a the relative errors of maximum flood depths for the lina reach beyond 50 at a few grids while the hybrid approach is less than 20 the height of the data box implies that the dispersion of relative errors is significantly reduced the upper and lower quartiles are within the range of 0 10 under three kinds of froude number threshold values further the accuracy of model results increases as the froude number threshold value decreases fig 7 b e shows the spatial distribution of relative error for different models it is worth noting that the hybrid approach performs inhibitory effect on grids with large errors in summary the results shows that the hybrid approach applied in this case significantly improves the result accuracy compared with the lina table 3 shows the mean absolute percentage error mape of the maximum flood depth for models with different threshold values and grids with different froude numbers the total computation domain is divided into three parts according to the froude number which are 0 0 3 0 3 0 8 and above 0 8 respectively it can be seen from the table that the hybrid approach obtains a significant improvement on the grids with froude numbers that are larger than 0 8 in the case of the hybrid approach with δ 0 3 and for grids with a froude number ranging between 0 3 and 0 8 the mape of the results decreases from 9 53 to 0 97 of which the model accuracy increases by approximately 10 times for the grids with froude numbers that are larger than 0 8 the mape decreases from 19 84 to 0 82 of which the model accuracy increases by 20 times in the case of the hybrid approach with δ 0 8 for the grids with froude numbers that are greater than 0 8 the mape decreases from 19 48 to 4 51 it can also be observed that the improvement in the model accuracy at grids with a froude number that is larger than the threshold value will have a positive impact on the surrounding grids for example for the grids with froude numbers that are less than 0 3 the mape from hybrid approaches with δ 0 3 0 5 and 0 8 decreases from 7 46 to 1 58 3 39 and 6 21 respectively 3 2 3 maximum flood velocity analysis fig 8 a shows the box plot of the relative error for the maximum flood velocity it can be found that the relative errors of maximum flood velocity for the lina reach approximately 60 at a few grids the relative error of the hybrid approach is below 30 and the upper quartiles of the relative error are around 10 indicating that the improvement in the maximum flood velocity using the hybrid approach is significant unlike the maximum flood depth there is no significant difference in the velocity between the cases δ 0 5 and δ 0 8 however from fig 8 d and e it can be found that the model accuracy in some local area of δ 0 8 is not as well as δ 0 5 table 4 shows the mape of the maximum flood velocity for models with different threshold values and grids with different froude numbers it can also be observed that the hybrid approach provides a significantly improved flood velocity in the case of the hybrid approach with δ 0 3 for the grids with froude numbers ranging from 0 3 to 0 8 the mape of the results decreases from 8 12 to 1 2 in the case of the hybrid approach with δ 0 8 for the grids with froude numbers greater than 0 8 the mape of the results decreases from 15 79 to 2 89 the same result is observed for flood depth whereby the improvement of model accuracy at the grids with froude numbers larger than the threshold value has a positive impact on the surrounding grids 3 2 4 computation efficiency analysis in the hybrid approach the numerical method for each grid changes with its flow regime at every time step the time dependent proportion of grids that utilize the hybrid approach is summarized in fig 9 as illustrated for the three models the proportion rises first and falls as time advances the proportion also varies according to the threshold value of the froude number for the hybrid approach with a threshold value of 0 3 the maximum proportion reaches 37 for the threshold value of 0 8 the maximum proportion only accounts for 5 model efficiency is also important in addition to accuracy thus we compared the computation time for the hybrid approach with lina and the classic swes as shown in table 5 the flood results were simulated under a rainstorm with a return period of 100a the hybrid approach with a threshold value of 0 3 0 5 and 0 8 obtains results in 239 38 200 36 and 196 01 s respectively compared to the classic swes the hybrid approach improves efficiency from 113 to 138 combined with the model accuracy a threshold value of 0 5 is recommended in practical cases using this threshold value the hybrid approach can produce results with less than 4 errors and improve the speed by 34 in summary the hybrid approach will be utilized more frequently as the threshold value of the froude number decreases which increases the computation time as well as the accuracy of the results 4 discussion the hybrid approach proposed in this study analyzes complicated flow conditions with classic swes and simulates regular flows with the lina in this way the hybrid approach can produce comparable results to classic swes with a similar computing time to the lina this method takes advantage of the classic swes for complex flow computing and the lina for time saving previous studies have obtained similar results such as yu and chang 2021 who adopted two numerical schemes for different flow conditions which were identified through the sharpness of discontinuity the rusanov scheme was considered as the low accuracy high efficiency scheme while the hllc solver was employed as the high accuracy low efficiency scheme according to their findings the proposed hybrid approach could simulate as accurately as the high accuracy scheme with significant reductions in computational time the same governing equations were employed for the two schemes which made up the hybrid mode however we extend the research in our study by employing two different governing equation systems including lina and classic swes which are combined into the hybrid approach the froude number is used to control the switch between these two governing equation systems which is determined according to previous studies on the lina results show that this attempt is feasible and stable which provides a theoretical guideline for the improvement of other simplified swes however it is worth noting that the control of the switching among different governing equation systems should be based on the characteristics of the simplified equations being utilized the threshold value of the froude number is found to significantly influence the calculation efficiency and result accuracy for a threshold value of 0 3 almost 37 of the total domain utilizes the hybrid approach in the peak time as a result the computation efficiency is only improved by 13 and the mape decreases to below 2 however for the thresholds of 0 5 and 0 8 10 and 4 of the total domain utilize the hybrid approach in the peak time respectively the improvement of computation efficiency increases to approximately 35 and the mape decreases to below 4 and 7 the threshold value is a key judgment factor in the proposed method for balancing simulation efficiency and result accuracy which requires more comprehensive research based on the characteristics of the research region besides the threshold value the efficiency of the hybrid approach is also related to the computation domain that it is utilized in the efficiency level of the proposed approach depends on the flow conditions involved in our study the proposed hybrid approach is applied in a practical case study instead of an entirely flat area the terrain of the research region varies greatly in the southern part in this case the hybrid approach is frequently utilized to handle complex flows thus the speed up ratio in the current research only reaches 13 38 if the research region is flat terrain the acceleration ratio will be as significant as the simplified approach simplified approaches including the lina diffusion wave approximation and kinematic wave approximation have been applied in urban flood modeling for many years as is known such approaches cannot comprehensively reflect the shallow water flows due to the omission of one or several terms by the classic swes which is the inherent defect of simplified approaches as they are always applied according to their limitations to produce acceptable flood results minor errors and excellent efficiency alleviate concerns regarding their defects thus the simplified approaches are widely accepted as the theoretical basis of urban flood modeling despite their limitations marangoz and anilan 2021 savant et al 2019 we propose the hybrid approach in this work to reduce these constraints which can provide a practical strategy to eliminate the errors caused by the inherent defects of the simplified approach while maintaining computation efficiency further according to previous studies and the above results the computation efficiency improvement of the simplified or hybrid approaches is far lower than that of parallel computation but the efficiency improvement of both hardware gpu and cpu and algorithms simplification methods can be overlapped research on efficiency improvement based on theoretical aspect is also important the exploration on the theoretical basis makes sense of the current research the hybrid approach proposed in this research does not change the initial calculation procedures to some extent it gives each cell a tag related to the calculation methods thus parallel computing technology can be performed in this hybrid approach while the hybrid approach proposed here reduces the errors of grids with large froude numbers in lina producing comparable results with slightly lower computation efficiency the speed of the lina frontal shock is slightly reduced compared to the speed of the fastest signal in the classic swes in other words the spread speed of the wet dry front modeled by the lina is slightly lower than the actual situation this problem is not addressed in the current research and will be explored in the future 5 conclusions this work presented a hybrid approach that combined the lina and classic swes where the swes were solved with fvm and unstructured grids this approach was successfully applied in a theoretical cylindrical dam break case and a practical urban pluvial flood case the main conclusions were as follows 1 the lina was successfully solved with fvm and unstructured grids the results of the cylindrical dam break case and urban pluvial flood case showed that the lina could increase the computation efficiency by 42 and 47 with an acceptable error compared to the classic swes 2 the hybrid approach took advantage of the classic swes in complex flow computing and the lina in time saving through hybrid treatment errors were reduced from 6 5 to 1 5 at time step t 0 5 s for the cylindrical dam break case and from 19 48 to 0 82 for the urban flood case in both cases the efficiency of the hybrid approach was similar to that of the lina 3 the threshold value of the froude number greatly influenced the calculation efficiency and result accuracy for a threshold value of 0 3 the computation efficiency was improved by only 13 and the mape decreased to below 2 for thresholds of 0 5 and 0 8 the computation efficiency increased to approximately 35 and the mape decreased to below 4 and 7 respectively software availability name of the software hufm hybrid urban flood model developer weiqi wang wenjie chen guoru huang contact information wangwq iwhr com year first available 2023 program language c cost free software availability https github com scut 777 hybird urban flood model program size 1 mb declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was funded by the open research fund of state key laboratory of simulation and regulation of water cycle in river basin china institute of water resources and hydropower research grant no iwhr skl kf202109 the national natural science foundation of china 52109018 51739011 and the science and technology planning project of guangdong province in china 2022a1515010131 
