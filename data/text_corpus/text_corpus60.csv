index,text
300,cellular automata ca are particularly suitable for physically based modelling of complex hydrological processes due to their inherent aptitude for parallel computing nevertheless other ca specific features like asynchronism could be exploited to enhance further the computational efficiency this note introduces an innovative activation deactivation rule enabling for the first time the practical application of asynchronous ca to subsurface flow problems the asynchronism rule consisting of a threshold depending on the spatial total hydraulic head difference between each automaton and its neighbourhood was implemented in the new opencal parallel software library using the extended ca xca paradigm it was tested performing hundreds of wet front propagation simulations in a comprehensive probabilistic framework considering several thresholds and addressing both two and three dimensional domains with heterogeneous hydraulic properties furthermore realistic simulations of a drip irrigated field entailing combined infiltration exfiltration cycles were carried out the application of the asynchronism rule generated considerable computational benefits for all the test cases even adopting low threshold values reducing the elapsed time by up to 80 85 while preserving numerical accuracy in particular the three dimensional simulation of the drip irrigated field demonstrated the practical relevance of the approach the computational improvement produced by the combined effect of parallel computing and the proposed asynchronism strategy enables increased space time resolution on wider study areas and further developments focusing on the simulation of computationally demanding relevant processes keywords extended cellular automata subsurface flow asynchronous functionality opencal heterogeneous soil properties computational efficiency 1 introduction in the last 50 years the process based approach relying on numerical modelling has become the most common to model hydrological phenomena paniconi and putti 2015 among the various hydrological processes one of the most complex yet most investigated is the subsurface unsaturated flow whose close relationship with the soil properties and dynamics is highly non linear therefore particularly challenging to treat through numerical techniques the partial derivative equation pde traditionally describing unsaturated flow is the richards equation richards 1931 which is a combination of the mass and the momentum conservation principles the latter being expressed by darcy s law this differential equation is elliptic parabolic degenerating list and radu 2016 highly non linear and rather complex to solve numerically miller et al 2013 many different methods have been adopted to deal with it farthing and ogden 2017 such as the finite elements method fem huyakorn et al 1984 kees et al 2008 the finite volumes method fvm eymard et al 1999 manzini and ferraris 2004 or the finite differences method fdm narasimhan and witherspoon 1976 an et al 2010 which are usually solved adopting implicit explicit or mixed time solution schemes nevertheless applying the richards equation in physically based models has been criticized for several reasons fatichi et al 2016 among which the computational burden hampering the use for large scale high resolution applications dealing with computational complexity and resulting constraints for models execution in terms of both time and resources required is a historically relevant problem common to all hydrological models freeze and harlan 1969 despite the increase of computational capabilities this problem is still unsolved because the requirement of physically based models with more detailed space time scales increased as well clark et al 2017 identified different solutions to face computing challenges the first is to adopt massively parallel computation strategies kollet et al 2010 fatichi et al 2016 liu et al 2016 zhu et al 2019 based on either classical cpus or graphical processing units gpus using e g a distributed or shared memory system the second solution relies on advanced and faster numerical solvers that in their turn can be combined with several parallelization strategies lott et al 2012 maxwell 2013 dolejší et al 2019 the third solution proposed focuses on avoiding redundant calculation and is mainly based on the concept of hydrological similarity peters lidard et al 2017 indeed it is possible to describe phenomena bringing together some parts of the domain that are characterized by the same properties significantly reducing the computational cost newman et al 2014 the cellular automata ca model based approach von neumann 1966 chopard and droz 1998 is particularly suitable for implementing the first of the solutions proposed by clark et al 2017 due to its intrinsically parallel nature most explicit schemes among them fdm and fvm can be formalized in terms of cellular automata ca and easily applied as parallel computational models cannataro et al 1995 straface et al 2006 giordano et al 2019 cellular automata are defined as a dynamical system in both space and time adapted for regular structured grids where only local interactions based on specific rules allow the cells also called automata to evolve to a next state within this paradigm di gregorio and serra di gregorio and serra 1999 proposed the macroscopic or extended cellular automata xca which are a particular ca form where the purpose is to investigate the macro scale effect of local interactions the xca are widely used in many research fields e g alemani et al 2012 liu et al 2015 sun et al 2016 de rango et al 2018 d ambrosio et al 2018 tang et al 2018 concerning hydrological applications there are some examples of xca based models used to simulate groundwater flow ravazzani et al 2011 de rango et al 2020 surface flow dottori and todini 2011 cai et al 2014 guidolin et al 2016 caviedes voulliéme et al 2018 subsurface flow folino et al 2006 mendicino et al 2006 and coupled surface subsurface flow cervarolo et al 2010 2011 mendicino et al 2015 ca based models offer other opportunities in addition to their natural predisposition to parallel computing which can be exploited to improve further computational speed in particular the asynchronous functionality allows a not simultaneous update of the cells in the system in literature the term asynchronous associated with the cellular automata has been used with different meanings earlier definitions identified as asynchronous those systems where some rules were simply applied e g every n iterations e g biesiada 1986 schönfisch and de roos 1999 referred to step driven and time driven asynchronous updating methods meaning that the order of evaluation of the cells can be determined by either specific algorithms e g random sweep or fixed updating time intervals for each automaton the definition of asynchronism in this paper instead is closer to that used by worsch 2012 who imposed some rules allowing only part of the automata to evolve step by step to a next state while making quiescent i e not evolving but blocked in its current state the remaining part this type of asynchronism is very similar to that presented by bouré et al 2011 and bouré et al 2012 where an asynchronous information transmission system which is ruled by specific algorithms and can be either stochastic or deterministic allows passing information only between selected neighbour automata therefore making active only certain regions of the domain using an asynchronous ca approach part of the domain composed of either contiguous or spatially separated automata can be quiescent until a given rule allows it to evolve and vice versa i e the same rule forces previously active automata to turn quiescent asynchronous ca can be used to avoid unnecessary calculations related to non active or very weakly evolving parts of the domain an asynchronous technique was applied for the first time to unsaturated flow modelling with xca by mendicino et al 2006 they called it quantization being inspired by a previous work of zeigler 1998 but de facto they applied a rule causing asynchronous xca evolution such rule was based on a threshold representing the minimum admissible total hydraulic head variation in time in each automaton the cell was forced to stay quiescent until the threshold was exceeded then it was allowed to vary its state continuously i e it could evolve to an infinite set of new states mendicino et al 2006 showed that this asynchronous technique can break down the computational burden because it reduces the active domain at the cost however of increasing errors with increasing total head variation threshold values therefore the optimal threshold value should be selected as a trade off limiting both computational costs and numerical errors as much as possible however mendicino et al 2006 evaluated only theoretically the computational gain calculating the reduction of the number of messages they could not assess the benefit in terms of computing time since the development environment used camelot dattilo and spezzano 2003 did not allow to efficiently exclude the inactive portion of the domain from the calculation the limits of the computational environment were overcome with the development of the open computing abstraction layer opencal d ambrosio et al 2018 system in which the asynchronous functionality was efficiently built in among the first opencal applications de rango et al 2021 considered subsurface unsaturated flow modelling and applied a basic asynchronism rule which was evaluated in a simple test case only from a computational point of view i e to compare different computing times without investigating any hydrological consequence in this work we present a new asynchronism rule different from that of mendicino et al 2006 no more depending on the hydraulic head variation in two consecutive time steps but on its spatial gradient in the same time step the new rule requires total head differences comparison in the same computational step avoiding to store and query different matrices at different time steps this new approach more flexible and straightforward allowed for the first time to highlight the benefits of the practical application of asynchronous ca to subsurface flow problems we report and discuss the results of the adoption of the novel asynchronism rule in a xca subsurface flow model stressed with challenging conditions within a comprehensive evaluation framework specifically we adopted a probabilistic approach performing several hundred simulations under different degrees of soil heterogeneity in both two and three dimensional test cases the main aim was evaluating the potential of the xca method coupled with the asynchronous functionality in terms of computational efficiency though constrained by a reasonable numerical accuracy eventually the range of simulations was further enlarged to highlight the relevance of the model in a practical application and the novelty of the asynchronous approach in the field of hydrological modelling the specific goals of this paper are i to define a simple and practical asynchronism rule suitable for the opencal environment managing effectively the activation of the domain and allowing to detect the trade off between numerical accuracy and computational efficiency in an easy way ii to show through a preliminary benchmark modelled with both the synchronous xca model and a widely used fem based commercial model namely comsol multiphysics comsol 2008 the dependence of the xca based model numerical accuracy on the threshold value selected in the asynchronism rule iii to quantify in a probabilistic framework the effects produced by the adoption of the asynchronous technique in both two and three dimensional test cases with different heterogeneous conditions characterized by increased variability of the saturated hydraulic conductivity iv to highlight through a realistic three dimensional infiltration exfiltration test case both the changes induced by asynchronism to the modelling of the hydrological processes and the practical benefit of the asynchronism rule the paper is structured as follows section 2 concerning materials and methods describes the subsurface flow equations adapted in the context of the xca paradigm briefly recalls the opencal library explains the asynchronism rule adopted presents the two dimensional benchmark and provides details about the strategy used to generate the two and three dimensional heterogeneous test cases sections 3 and 4 present and discuss the results obtained for both two and three dimensional test cases including the infiltration exfiltration test case and finally section 5 summarises the main findings and outlines the main perspectives of the research 2 materials and methods 2 1 subsurface flow equations in the framework of extended cellular automata the xca flow model was presented by mendicino et al 2006 and is briefly summarized below it is based on a regular square lattice grid where the volumetric mass balance equation is employed that takes into account the moisture flux incoming from or outgoing to the neighbouring cells using the momentum conservation principle expressed by darcy s law 1 i 1 6 k c i h i h c l i a i v c c c δ h c δ t s c in the equation 1 the subscripts c and i indicate the considered cell and the neighbouring cells along the i directions respectively in a generalized three dimensional space domain 6 directions are eligible adopting the von neumann neighbourhood k c i m s 1 is the averaged hydraulic conductivity between the cell c and its neighbours h c m is the total hydraulic head in the cell c given by the sum of the pressure head ψ c m and elevation z c m h c ψ c z c δ h c δ t is the unknown term where δ h c represents the variation of the variable h in c during the interval δ t the term c c m 1 is defined as the specific retention capacity bear 1972 and represents the derivative of the volumetric moisture content on the pressure head θ h where θ m3 m 3 is defined as the ratio between the water volume m3 and the total volume of the porous medium m3 the volume of the cell is indicated as v c m3 a i is the surface area m2 normal to the water flux l i represents the distance m between the centroids of the cubic cells and is equal to the resolution of the automaton finally s c m3 s 1 in the second member represents a possible mass source term the first term in the first member of the equation considers the volumetric fluxes according to the darcy s law and can be represented as the sum of the mass fluxes along the six directions i 1 6 φ i c m3 s 1 therein k c i is calculated as 2 k c i v c v i v c k c v i k i for each cell the hydraulic conductivity k can be assumed as composed of two terms i e the saturated hydraulic conductivity k s and the relative hydraulic conductivity k r varying between 0 and 1 depending on the moisture content of the cell so that k k s k r in the equation 1 the hydraulic conductivity and the specific retention capacity depend non linearly on the pressure head hence on the total head h there are several constitutive equations proposed in literature to update the volumetric moisture content θ on which c c depends and k r as functions of the pressure head i e θ ψ and k r ψ in this study the approach proposed by van genuchten and nielsen 1985 one of the most commonly adopted in hydrogeology is used moreover to take into account transitions between unsaturated and saturated flow conditions the extended form of the van genuchten s model as defined by paniconi et al 1991 is adopted introducing a continuity parameter ψ 0 and the specific storage s s 3 θ ψ θ r θ s θ r 1 ψ n 1 1 n ψ ψ 0 θ r θ s θ r 1 ψ n 1 1 n s s ψ ψ 0 ψ ψ 0 the main feature of equation 1 is that it comes from a directly discrete formulation of the field equation i e obtained without discretizing a previously defined differential equation in a discrete cell system in which the volume enclosed by each cell has homogeneous characteristics the constitutive law used in the differential context can be used as an approximation mendicino et al 2006 though the equation derives from a directly discrete formulation in this case in practice it is the same as a fdm or fem scheme tonti 2001 because the three dimensional cubic cell system adopted is a particular case of a delaunay tessellation mattiussi 1997 manzini and ferraris 2004 mendicino et al 2006 as an explicit finite scheme the xca based model needs a convergence criterion in particular the courant friederichs lewy condition is calculated as 4 δ t l i 2 c c i 1 6 k c i regarding the asynchronous functionality mendicino et al 2006 proposed a rule based on the hydraulic head difference in time between two consecutive calculation steps since the water flow is generated by a hydraulic difference in space here we propose a new rule based on the difference between the hydraulic heads in the considered cell c and its neighbourhood in particular considering a three dimensional von neumann neighbourhood and assuming no preferential directions if all the 6 differences between c and its neighbours are lower than a fixed threshold called δ hs the automaton turns to be not active contrariwise if one or more differences are higher it turns to be active 5 i 1 6 h i h c δ h s n o t a c t i v e i 1 6 h i h c δ h s a c t i v e the new rule is implementable in the development environment in a straightforward way and is more flexible since it allows accounting for anisotropic flow simply by fixing different thresholds according to different directions the rationale of the functioning of the asynchronism rule is detailed through fig 1 it shows two consecutive time steps in a one dimensional example at time t t the total hydraulic head difference between the cells c and c 1 is higher than the threshold δ hs therefore both c 1 and c are set active on the other hand the cell c 1 is set not active since both hydraulic head differences between the two neighbour automata are lower than the δ hs threshold at time t t 1 the mass transfer between the cells c 1 and c reduces their total hydraulic head difference h c 1 h c to a value lower than δ hs therefore the cell c 1 is set not active on the other hand the new total hydraulic head in the cell c makes h c h c 1 δ h s involving that the cell c is still active and the cell c 1 also activates from fig 1 one can infer that mass transfer is delayed by higher thresholds therefore the higher the δ hs value the slower the propagation rate of a given phenomenon the xca flow model is formally defined as 6 x c a f l o w d s i x q σ φ γ t δ t ω τ n in this definition the terms in the triplet d s i represent the three dimensional computational domain d the corresponding continuum three dimensional space s and the set of domain interfaces including the boundary regions i respectively x represents the von neumann neighbourhood which is defined only over the orthogonal directions q is the finite set of cell states given by the cartesian product of the following substates the volumetric moisture content q θ the hydraulic conductivity q k the saturated hydraulic conductivity q k s the total hydraulic head q h the specific retention capacity q c c and a convergence substate q c o n v σ is the set of the local transition functions both accounting for the boundary conditions and solving eq 1 while φ is the set of functions applying σ to the non local domains defined by i γ is the set of global functions by which in our case the minimum value over the q c o n v substate is identified to define the time step of the system t is the function determining the automaton global transition and δ t is the quantity corresponding to the physical time step simulated ω is the termination criterion and finally τ n is the xca control unit allowing the automaton to evolve over time a more detailed explanation of the xca flow model formal definition is presented in the appendix a 1 2 2 opencal opencal open computing abstraction layer is an open source parallel c c software library based on the extended cellular automata xca computational formalism di gregorio and serra 1999 such formalism allows opencal to support many computational schemes like finite difference finite volume and more in general structured grid based methods opencal implementation is straightforward once the xca formalism has been defined by the user in particular the framework permits to specify the cell states the neighbourhood and the local update transition functions as well as global operations if needed with minimum effort moreover the initial conditions of the entire system and a stop criterion to the simulation can be straightforwardly implemented one of the most important advantages of opencal is the ease of obtaining different parallel versions from the serial version once a serial implementation of the model has been completed many different parallel versions can be derived transparently opencal supports many parallel architectures among them multi gpu and many core cpu shared memory devices as well as distributed memory systems de rango et al 2019 furthermore opencal provides embedded optimization algorithms allowing fine grained control over the simulation d ambrosio et al 2018 among them the asynchronous mechanism implementation can provide a significant speed up in the simulation of topologically connected phenomena 2 3 making cellular automata asynchronous the asynchronous functionality permits to narrow the computation only to the set of active cells allowing to skip the not active cells meaning that not only the transition function is not computed but also that the not active cells are completely skipped i e they are not visited by the loops spanning the domain once a model substate and a static condition threshold are chosen the cells can be easily added and removed from the set of active cells during the computation before the computation starts the set of active cells must be preliminary defined usually at the initialization stage it is worth to note that all the parallel opencal versions already support this optimization algorithm more in details the opencal algorithm considers a dynamic array namely a to memorize the active cells the cells in a identify the subset of the entire domain where the computation takes place a further array of boolean type namely b which has the same dimension of the entire computational domain is considered to track the variation of the active cells over the computational steps at the beginning of the simulation the entire array is set to false at each calculation step including or removing a cell in the set of active cells a is ruled by a process composed of two different steps i modifying the flag value of the cell in the corresponding position of the array b to false deactivation or true activation ii updating the set a according to the previous change over the array b therefore modifying i e increasing or decreasing the size of a this way the elementary processes are executed only over the set a excluding the remaining part of the domain from the computation fig 2 illustrates an example of the asynchronous mechanism applied to a simple 4 4 matrix where the patterns of the gray cells in time represent the evolution of a dynamic process e g infiltration the step t fig 2a represents a given initial stage then at step t 1 fig 2b more cells activate hence the size of the set a increases finally at step t 2 fig 2c one cells deactivate in the top left position therefore reducing the size of the set a 2 4 two dimensional benchmark the goal of this section is to propose a simple benchmarking exercise introducing the issue of the relationship between accuracy decrease and increasing asynchronous thresholds to provide a reference measure useful to assess if the magnitude of the errors with the increased threshold is meaningful the benchmark test is simulated with both the synchronous xca model and the comsol multiphysics r environment comsol 2008 a state of the art finite element model the difference between the models outputs due to the two distinct schematizations can be assumed as a measure of the acceptable output uncertainty since the accuracy of both models was already proved in previous studies the test case investigated is based on an experiment conducted by the university of arizona in the jornada test site near las cruces new mexico and reproduced using a two dimensional numerical modelling by smyth et al 1989 and mccord and goodrich 1994 it concerned a heterogeneous terrain with very dry initial conditions having three main horizontal soil layers and a fourth soil type inserted in the deepest of them fig 3 a the main hydraulic properties of the four soil types are reported in table 1 according to the experiment a domain 8 00 m long and 6 50 m deep with a uniform rain rate of 0 02 m d 1 for a length of 2 25 m along the left side of the upper boundary layer was numerically reconstructed along the other boundaries neumann conditions no flow were imposed the initial conditions on the whole domain consisted of a constant total hydraulic head equal to a negative value of 7 34 m indicating that the soil was unsaturated this test case was already addressed by folino et al 2006 in the framework of the xca paradigm and used by de rango et al 2021 to test the new opencal implementation according to those studies here a cubic lattice with an edge length δ s of 0 05 m was used having nr 130 rows nc 160 columns and only one slice the test case was simulated for a total of 30 days a total of 25 986 computational steps were required resulting in an average δ t 99 75 s for the same test case a two dimensional model domain was created and discretized with a finite element triangular grid in the comsol multiphysics r environment the grid was constituted by elements with sizes between 0 0001 and 0 16 m with a maximum element growth rate of 1 2 fig 3 b shows the hydraulic head h field calculated by comsol multiphysics after 30 simulated days figs 3c e instead show the absolute differences at the same time step between the h fields achieved with comsol multiphysics and the xca flow model using no threshold i e in a synchronous fashion and with δ hs equal to 0 25 m and 1 0 m respectively as fig 3b highlights during the 30 days simulation a relevant fraction of the domain modifies its initial water content up to δ hs 0 25 m figs 3c d the xca flow model behavior is almost identical the main differences between the outputs of the comsol multiphysics model and the two versions of the xca flow model are located along the wetting transient front and are not directly affected by the boundary conditions i e by rain and soil heterogeneity even if the variations in the sharp front of the wetting zone are high sometimes up to almost 2 5 m they do not persist in the same position for a long time since they follow the wetting front evolution higher thresholds produce similar patterns but the region along the wetting front with the highest h differences enlarges with δ hs 1 0 m fig 3e its thickness is clearly higher than the uncertainty emerging from the comparison between the fem and xca flow synchronous models therefore suggesting possible relevant errors the benchmark reveals that increasing the value of the asynchronous threshold induces a non linear increase of the errors until they become not acceptable the direct relationship of the thresholds with errors will be analysed in the next sections as well as the concurrent inverse relationship with the computational speed concerning the computational time required to achieve the solution at 30 days the comsol multiphysics model simulation on an intel i7 8700 cpu required approximately 100 seconds such as the synchronous x flow model did on an intel xeon w 2175a cpu on the same cpu the simulations with δ hs equal to 0 25 m and 1 0 m required 55 and 50 seconds respectively 2 5 two and three dimensional heterogeneous test cases with the asynchronous functionality to extensively evaluate the effects produced by the asynchronous functionality the xca flow model was applied to multiple two and three dimensional heterogeneous test cases 300 two dimensional and 300 three dimensional pseudo random synthetic fields of saturated hydraulic conductivity k s were generated through the application of simple kriging techniques with an exponential semivariogram model pebesma 2004 the field generation method required the assignment of both mean μ and variance σ 2 values of k s due to the wide range of variability of such parameter the logarithmic transform of the variable was interpolated therefore a value of μ equal to 4 19 was assigned obtained as the logarithm of the average k s of the three main areas in the case study presented in 2 4 concerning variance for both the two dimensional and three dimensional generations three different values of σ 2 were imposed 0 5 1 0 2 0 each of them used for generating 100 fields furthermore to avoid unrealistic values of generated k s values upper and lower limits of k s were imposed equal to 0 2 and 1 10 10 m s 1 respectively the two dimensional test case consisted of a rectangular domain 16 m wide and 13 m deep with a mesh size of 0 1 m producing 160 130 grid points as for the boundaries no flow conditions were imposed in all borders except along the first 3 4 m starting from the left of the upper border where a constant infiltration rate of 0 02 m d 1 was imposed simulating a rainfall boundary conditions are easily handled in the xca framework for each cell in the border of the domain it is sufficient to set either a specific h i e fixed total or pressure head or φ i e fixed mass flow value in the outside direction to represent the dirichlet or neumann boundary conditions respectively these flexible settings can vary in time variable boundary conditions and can be combined fig 4 a shows the domain geometric properties as well as the location of the rainfall boundary conditions it also shows an example of the k s pattern generated imposing σ 2 0 5 which can be visually compared with the k s fields generated imposing σ 2 1 0 and σ 2 2 0 figs 4b and 4 c respectively each of the 300 two dimensional configurations simulated 10 days starting from initial conditions of constant total hydraulic head equal to 7 34 m this setting was needed to guarantee initial steady state conditions which emphasizes the application of the asynchronous functionality van genuchten s model parameters were the same adopted in section 2 5 and are reported in table 1 furthermore a continuity parameter ψ 0 fixed to 0 001 m and the relative specific storage s s equal to 10 6 m 1 were used in the three dimensional test case the space domain was composed of a parallelepiped 30 meters long and wide with a depth of 15 meters the mesh size was fixed to 0 3 m generating 100 100 50 grid points also in this case no flow conditions were set for all boundaries except a constant infiltration rate of 0 02 m d 1 interesting the central part of the upper border of the domain specifically infiltration affected a square with a side of 12 m positioned in the centre of the upper face of the domain as showed in fig 5a such as for the two dimensional test case fig 5a also shows an example of the k s field generated with σ 2 0 5 while figs 5b and 5 c show the k s fields generated with σ 2 1 0 and σ 2 2 0 respectively also in the three dimensional test case the 300 configurations simulated 10 days with a constant initial total hydraulic head of 7 34 m all simulations were performed in an hpc infrastructure using an intel xeon gold 6128 and a shared memory parallel openmp based system with 24 threads both the two and three dimensional configurations were used to assess the effects of the asynchronous functionality in terms of computational efficiency and numerical accuracy applying different activation thresholds specifically computational efficiency was assessed through the percentage elapsed time with respect to the synchronous simulations while the statistics accounting for the numerical accuracy were evaluated in terms of both the absolute and percentage errors with respect to the pressure heads calculated with the synchronous simulations furthermore the mean square error mse m2 on the pressure head was calculated for each asynchronous simulation 7 m s e i 1 n ψ i ψ i 2 n in the previous equation the subscript i indicates the automaton where the pressure head difference is calculated with the whole domain containing n automata ψ refers to the value achieved with the synchronous simulation and ψ indicates the value obtained by adopting the asynchronous functionality both ψ and ψ are calculated subtracting the cell elevation z from the values of the state variables h and h respectively to achieve a single mse value for each threshold δ hs and variance both for the two and three dimensional test cases the average mse of the related 100 simulations was calculated finally for each of the three σ 2 values imposed for the three dimensional k s fields i e 0 5 1 0 and 2 0 one of the 100 set ups generated was randomly selected to test the asynchronous technique in a realistic situation characterized by concurring infiltration and exfiltration boundary conditions specifically the upper boundary layer was considered as a drip irrigated field with one irrigated strip of land every 2 1 m along the direction of the x axis i e given the 0 3 m cell resolution one irrigated strip of cells every 7 irrigation occurred 5 days out of 7 from 18 00 to 21 00 3 hours 2 mm h 1 besides the whole upper layer was subject to evapotranspiration every day from 12 00 to 16 00 4 consecutive hours with potential rates of 0 4 0 6 0 6 and 0 4 mm h 1 respectively no flow conditions were set for the lateral boundaries but percolation was allowed through the lower boundary layer furthermore the constant value of the initial total hydraulic head was raised to 1 0 m to enhance actual evapotranspiration the main statistics concerning soil moisture evolution at different depths during a 25 day simulation were evaluated for different threshold values accounting for the related computational times the three operational test cases provided practical examples elucidating the actual relevance of the magnitude of the error induced by different thresholds in water balance 3 results in the probabilistic framework the asynchronism effects were evaluated by considering 7 different activation thresholds δ hs 0 m 0 001 m 0 01 m 0 1 m 0 25 m 0 50 m 1 0 m in the operational infiltration exfiltration test case the thresholds were increased up to 3 5 m to better highlight the effect of the asynchronism to model outputs of practical relevance note that from a theoretical point of view the value δ hs 0 implies that results must be equal to those achieved without using the asynchronous technique indeed this threshold forces a reduction of the computational domain without any loss of information due to neglecting mass exchanges 3 1 two dimensional test cases depending on the values of k s the location of the wet front at the end of the 10 days in the two dimensional test cases was extremely variable in particular the number of automata affected by the infiltration process increased with the growth of the variance the average number of automata activated with the lowest threshold i e δ hs 0 was 1736 8 adopting a value of σ 2 0 5 against 1961 9 5 obtained with σ 2 1 0 and 2387 11 5 achieved with σ 2 2 0 the increase of the cells involved in the mass exchange process with increasing variance depends on the fact that the higher the variance the higher the possibility of occurrence of high saturated conductivity values concurring to the generation of preferential paths which in turn transfer faster the water to regions of the domain further away from the infiltration border fig 6 shows an example of the spatial distribution of the percentage error achieved by varying the activation threshold δ hs at the end of the 10 days simulation period in a configuration where k s was generated with σ 2 1 previously shown in fig 4b errors enhanced increasing the activation threshold because of the higher number of cells whose mass exchange was neglected the propagation of the wet front was slowed down by the delayed activation of the automata in the wet dry border contrariwise adopting low activation thresholds i e near 0 the distributed errors were almost 0 analysing the problem from a statistical point of view every simulation can be considered as an occurrence of a 100 members sample distribution characterized by a specific σ 2 value and activation threshold δ hs fig 7 shows the characteristics of the sample distributions in terms of computational efficiency and numerical accuracy the red pink colours refer to the maximum error achieved in the domain while the black grey colours refer to the computational time the dots indicate the median values while the bands represent the ranges between the 1st and 3rd distribution quartiles the computational time was calculated as the percentage with respect to the synchronous simulations results show that for each σ 2 value even adopting low thresholds the computational time was highly reduced with σ 2 2 0 the simulation time decreased from approximately 300 seconds achieved by synchronous simulations to only 90 seconds with δ hs 0 conversely using high activation thresholds the computational time did not improve significantly further while the maximum error generated increased more than linearly also the computational gain did not change appreciably increasing the σ 2 value but the errors due to the higher thresholds were larger because of the higher possibility that the previously mentioned preferential paths affecting significantly the wet front propagation speed were activated later compared to the reference simulation table 2 shows the mse m2 calculated in the whole domain following eq 7 in agreement with fig 7 the errors grew up by increasing δ hs or σ 2 as a further evidence that imposing a larger number of cells with neglected mass exchange or a higher variability on k s generated greater delay in the wet front propagation 3 2 three dimensional test cases also in the three dimensional test cases infiltration did not propagate all over the domain the number of cells interested by the phenomenon increased adopting higher σ 2 vales in particular the average number of automata activated adopting δ hs 0 was 4580 0 9 41507 8 3 and 85565 17 1 fixing σ 2 0 5 1 0 and 2 0 respectively fig 8 shows some examples of the wet front spread specifically figs a c refer to results achieved using a domain configuration characterized by σ 2 0 5 figs d f by σ 2 1 0 and finally figs g i by σ 2 2 0 furthermore figs a d g refer to synchronous simulations figs b e h concern the threshold δ hs 0 and figs c f i the threshold δ hs 1 0 m related cross sections are provided as supplementary material fig s1 the visual comparison among the rows highlights the increasing wet front spread with higher degrees of heterogeneity instead the comparison among the columns points out the effect of asynchronism which as in the two dimensional test cases produced a slow down in propagation the statistical analysis confirmed the findings achieved with the two dimensional test cases fig 9 shows the distribution of computational time and maximum errors depending on σ 2 and δ hs while table 3 reports the mse m2 calculated with the three dimensional test cases the only appreciable difference between two and three dimensional results is the slower increase of the errors in the latter when higher thresholds and variances were considered this is due to the different grid size adopted 0 1 m and 0 3 m for the two and the three dimensional test cases respectively as indicated in section 2 5 this difference implied that with equal propagation of the wet front fewer automata were involved in the lower resolution three dimensional simulations therefore fewer preferential paths were affected by the phenomenon and a smaller number of automata activation and deactivation occurred with all thresholds in other words the asynchronous simulations were closer to the reference synchronous simulations 3 3 drip irrigation three dimensional test case this test case differs from the previous ones because besides considering also exfiltration it involves a higher percentage of cells in the process the number of cells activated during the 25 day simulations with δ hs 0 was equal to 70 6 86 7 and 84 3 for σ 2 0 5 1 0 and 2 0 respectively since the main objective of the drip irrigation test case was the analysis of the practical implications of the asynchronous approach the investigated variable was the volumetric moisture content θ fig 10 shows the average soil moisture evolution during the 25 simulated days at three soil depths according to different thresholds up to 3 5 m in the case with σ 2 0 5 corresponding figures with σ 2 1 0 and 2 0 are reported in the supplementary material figs s2 and s3 while an example of the three dimensional volumetric moisture content distribution at the end of the simulation period is shown in fig s4 the opposing top boundary conditions given by infiltration in a subset of cells and exfiltration all over the top boundary resulted in a slow drying process therefore although the graphs are designed to highlight as well as possible both θ variation in time and the differences achieved by using different thresholds it is important to acknowledge that overall θ variability is small as the values in the vertical axis highlight nevertheless these small differences explain well how the selected thresholds affected the complex dynamics of the process under investigation though there is not clear moving wet front such as in the previous experiments even this test case highlights that the higher the asynchronous threshold the slower the system response to the boundary forcing fig 10b relative to the shallower layer more affected by exfiltration depth equal to 0 45 m shows that an increase of the threshold causes faster drying because upward subsurface flow allows replacing exfiltrated water less quickly in a complementary way the deeper layer moisture fig 10d depth equal to 1 35 m increases with the threshold because the upward movement does not start or starts very late in the layer at intermediate depth fig 10c 0 75 m with σ 2 0 5 the soil moisture evolution does not change up to δ hs 0 5 m then for higher δ hs values it resembles the evolution of the deeper layer for higher σ 2 values figs s2 and dummytxdummy s3 the intermediate behaviour observed in the layer at depth 0 75 m extends to the deeper layer because the higher k s variability facilitates the creation of preferential paths that speed up the upward water movement for each σ 2 value and each of the three layers at depths of 0 45 m 0 75 m and 1 35 m fig 11 compares the computational saving achieved with the asynchronous functionality with a measure of the error namely the average absolute percentage error of θ in the layer at the end of the simulated 25 days the use of δ hs 0 does not produce many benefits since the number of cells involved in the simulations with this threshold is high nevertheless increasing δ hs to 0 01 m allows reducing the computational times to 20 30 of the initial values keeping the error at very low values selecting higher thresholds such as for the previous experiments does not lead to a further notable reduction of computational times errors remain always well below 1 they tend to grow with thresholds but not always monotonically in agreement with the physical explanation of the effect of the asynchronous functionality on the process dynamics described before the layer that deviates most from the monotonic trend is the intermediate one depth 0 75 m it is particularly affected by the combined effect of exfiltration and infiltration resulting in a less predictable behavior with varying thresholds 4 discussion the proposed two and three dimensional domain configurations refer to a particularly challenging hydrological process unsaturated flow in heterogeneous soils is the subject of numerous recent studies relying on elegant numerical solutions aimed at solving specific problems e g layered soils berardi et al 2020 or fractured porous media kumar et al 2020 and improving computational efficiency to allow large scale watershed simulations suk and park 2019 while keeping a relatively simple directly discrete approach and being naturally predisposed to parallel computing the xca flow model demonstrated to be accurate and that can further increase its efficiency if the asynchronous functionality is well managed the results of the simulations performed within the probabilistic framework suggest that from a hydrological point of view the application of the asynchronous functionality produces a slowing down of the wet front propagation due to the delay of automata activation at the wet dry interface this behaviour leads to highly localised errors in pressure head distribution at the propagation interface contrariwise in the remaining part of the domain where the automata are activated earlier and not deactivated during the simulation errors are very low or null because the system is able to re balance however the strip of the domain with more evident errors near the propagation front increases its thickness as the simulation proceeds obviously in the areas not reached by infiltration there is no error numerical errors are influenced by the choice of the asynchronous threshold δ hs and grow up increasing it the rate of growth changes depend also on the k s values variability in general high k s variability corresponds to the creation of preferential paths that influence significantly the wet front propagation but are activated erratically as the asynchronous threshold increases the operational drip irrigation experiment further confirmed that system evolution is slowed down by higher thresholds in a completely different test case with a higher percentage of cells involved nevertheless the slightly delayed cells activation did not prevent reaching an acceptable accuracy for model outputs of practical relevance such as soil moisture state while computational times reduced significantly with very low asynchronous thresholds also in this test case in analogy with video recording the asynchronous functionality works at a smaller frame rate in those regions of the domain where it is allowed i e where numerical accuracy is not impaired of course in contrast to video recording modelling does not simply film a physical process and the selected level of detail affects process representation the asynchronous functionality herein proposed provides a method for objectively quantifying the minimum resolution to comply with for ensuring acceptable accuracy in subsurface flow modelling the test cases performed both two and three dimensional with increasing heterogeneous hydraulic properties varying boundary conditions and a different number of cells activated highlighted the potential of such an approach the benefit of the asynchronous functionality can be further elucidated relating it to the phenomenon under investigation e g point source vs global perturbation of the system but also its link with soil hydraulic properties is worth further analysis since the hydraulic conductivity affects the timing of hydraulic head redistribution and consequently the activation deactivation rule this research question deserves specific analysis and will be the subject of further research from a computational point of view the xca flow model adopts an explicit finite difference scheme on the one hand the application of a time explicit scheme requires a strict numerical convergence criterion e g courant friedrichs lewy condition that led many researchers to not delve into it on the other hand also the adoption of time implicit solution schemes requires a very intensive cpu effort mcbride et al 2006 due to the time step size adaptation kavetski et al 2001 efforts were made to adapt not only the time step size but also the spatial discretization e g mostaghimi et al 2015 solin and kuraz 2011 in those cases where an increase of the grid resolution for specific regions of the domain is needed however this type of solution is not widely used in practice farthing and ogden 2017 and the applications of high detail physically based models at basin or regional scales are often limited due to the intensive calculation required the alternative solution herein proposed does not rely either on time step or on grid size re adaptations rather a regular grid is considered made of square or cubic automata in our case having a resolution high enough to represent the analysed phenomenon then the active part of this grid i e the portion actually involved in the calculus changes dynamically at each time step according to the evolution of the phenomenon allowing the presence of dynamic regions of the domain where no calculation is executed the proposed approach is flexible and could also integrate other optimization strategies e g the ca grid structure could even be irregular bochenek and tajs zielińska 2017 zeng et al 2010 finally the proposed test cases provide a guideline for the selection of the asynchronous threshold value the experiments performed showed that a good trade off between numerical accuracy and computational performance can be already detected adopting very low thresholds which break down the computational efforts considerably ensuring very small or even null errors 5 conclusions in this note the asynchronous cellular automata functionality is investigated performing several hundreds of subsurface flow simulations under different heterogeneous conditions in both two and three dimensional test cases the subsurface flow model whose original formulation was proposed by mendicino et al 2006 was modified for what concerns the asynchronous functionality and reimplemented exploiting the highly efficient opencal parallel software library d ambrosio et al 2018 the asynchronous functionality was applied adopting an activation deactivation threshold linked to the spatial total hydraulic head difference which is the cause of the water flow through the porous media the effects produced by this functionality were evaluated in terms of both numerical accuracy and computational time reduction a preliminary benchmark comparison with both the synchronous version of the xca flow model having a numerical approximation based on a 1st order truncation scheme and a state of the art finite element model 4th order truncation suggested negligible errors for small thresholds as well as non linear error increase with increasing threshold values then the results achieved through a probabilistic approach highlighted that ca asynchronism generates considerable benefits from the computational point of view reducing the elapsed time by up to 80 even adopting low thresholds while the numerical accuracy is affected by significant errors only with relatively high thresholds finally a realistic three dimensional simulation of a drip irrigated field involving both infiltration and exfiltration processes further confirmed the performance of the model highlighting that an acceptable accuracy of the operational outputs such as soil moisture estimates can be easily achieved together with a significant saving of computational times overall the asynchronous functionality demonstrated to be an efficient computational optimization feature which fits very well the cellular automata paradigm and could be easily coupled with other strategies to speed up computational time such as the adoption of irregular meshes or time step adaptation methods nevertheless it is worth to highlight that first and foremost the model benefits from the xca natural aptitude to parallel computing which can be fully exploited through the opencal library parallelization strategies have been recently enriched with the alternative offered by gpus which are being already adopted for hydrological applications e g lacasta et al 2015 aureli et al 2020 in this regard among its functionalities opencal allows the user to transparently transfer cpu based to gpu based code permitting to explore the possibility of breaking down further the computational burden further research will address the enhancement of computational optimization strategies involving gpus and load balancing techniques giordano et al 2020 which will be tested with improved versions of the xca flow model concerning future modelling advancements it should be noted that though the xca method has been already thoroughly tested against one two and three dimensional literature data the same comparison with the asynchronous functionality was performed only indirectly i e it was compared against the synchronous model and a fem model section 2 4 direct evaluation of the asynchronous functionality against literature data or even with classical numerical richards equation simulations will be performed furthermore the connection between the asynchronous functionality and soil hydraulic properties will be investigated and different asynchronism rules based not only on the spatial difference of the hydraulic head will be tested finally it is worth it to underline that the unsaturated flow model herein proposed is part of a more complex modelling system that is currently being developed including also surface flow surface subsurface interactions and transport processes credit authorship contribution statement luca furnari methodology formal analysis writing original draft alfonso senatore conceptualization methodology formal analysis writing original draft alessio de rango writing original draft michele de biase formal analysis salvatore straface conceptualization methodology formal analysis writing original draft giuseppe mendicino conceptualization formal analysis writing original draft declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements l furnari acknowledges support from the programme por calabria fse fesr 2014 2020 mobilitá internazionale di dottorandi e assegnisti di ricerca ricercatori di tipo a actions 10 5 6 and 10 5 12 m de biase acknowledges the european commission for the financial support activated under the pon research and innovation 2014 2020 action i 2 researchers mobility notice d d 407 of 02 27 2018 aim attraction and international mobility line 1 researchers mobility appendix a a1 formal definition of xca flow model from a mathematical point of view the model is formally defined with the following functional structure de rango et al 2021 d 0 n r 1 0 n c 1 0 n s 1 z 3 is the three dimensional discrete computational domain with n r n c and n s the number of rows columns and slices respectively s 0 n r δ s 0 n c δ s 0 n s δ s r 3 is the continuum three dimensional space corresponding to d subdivided in cubic cells of side δ s the function μ defines the mapping to d as μ d r 3 ι 1 ι 2 ι 3 ι 1 δ s ι 2 δ s n s 1 ι 3 δ s i i ρ i β i τ d is the set of domain interfaces where i ρ π s 0 is the boundary region belonging to the top surface π s 0 that could be affected by the interaction with the surface boundary layer e g rainfall or evapotranspiration i β is the remaining domain boundary region i τ d i ρ i β defines the inner domain where the system evolution occurs x 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 is the von neumann neighbourhood q q θ q k q k s q h q c c q c o n v is the set of states for the cell where q θ is the set of values representing the volumetric moisture content q k is the set of values representing the total hydraulic conductivity q k s is the set of values representing the saturated hydraulic conductivity q h is the set of values representing the total hydraulic head q c c is the set of values representing the specific retention capacity q c o n v is the set of values representing the time step size needed for numerical convergence σ σ ρ σ β σ τ is the set of local transition functions or kernels in particular σ ρ q h q c c q h accounts for the top boundary region belonging to the surface π s 0 that could be affected by the interaction with the surface boundary layer σ β q h q h sets the conditions on the remaining boundary region not directly affected by atmospheric interactions σ τ q x q corresponds to the explicit form of the finite difference scheme in 1 also the substates q θ q k q c c and q c o n v are updated according to equations 3 2 and 4 respectively moreover σ τ can activate deactivate cells of the computational domain if the asynchronous algorithm is exploited φ ϕ ρ ϕ β ϕ τ is the set of functions applying the local transition σ to the non local domains defined by i in particular ϕ ρ q i ρ q i ρ applies the σ ρ local transition to the i ρ interface ϕ β q i β q i β applies the σ β local transition to the i β interface to account for boundary conditions ϕ τ q i τ q i τ applies the σ τ local transition to the i τ interface here i τ denotes the union of i τ and the set of cells belonging to d i τ that are needed to guarantee a complete neighbourhood to each boundary cell of i τ according to the ca definition only states of cells in i τ are updated γ γ t γ t q c o n v d r is the set of global functions where γ t evaluates the minimum value over the q c o n v substate to define the physical time corresponding to the computational step of the system specifically if δ t indicates the time step size it is δ t min ι d q c o n v ι t ϕ ρ ϕ β ϕ τ γ t is the function determining the automaton global transition δ t r is the quantity corresponding to the physical time interval simulated by a state transition of the automaton it is evaluated step by step by considering the γ t function ω r f a l s e t r u e is the termination criterion when the prefixed simulated time interval is complete ω returns f a l s e and the simulation terminates τ n n c c is the xca control unit at step n 0 the xca is in the initial configuration c 0 τ n is then applied at discrete steps by producing a sequence of configurations c 1 c 2 until the ω termination criterion is satisfied supplementary material supplementary material associated with this article can be found in the online version at 10 1016 j advwatres 2021 103952 appendix b supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
300,cellular automata ca are particularly suitable for physically based modelling of complex hydrological processes due to their inherent aptitude for parallel computing nevertheless other ca specific features like asynchronism could be exploited to enhance further the computational efficiency this note introduces an innovative activation deactivation rule enabling for the first time the practical application of asynchronous ca to subsurface flow problems the asynchronism rule consisting of a threshold depending on the spatial total hydraulic head difference between each automaton and its neighbourhood was implemented in the new opencal parallel software library using the extended ca xca paradigm it was tested performing hundreds of wet front propagation simulations in a comprehensive probabilistic framework considering several thresholds and addressing both two and three dimensional domains with heterogeneous hydraulic properties furthermore realistic simulations of a drip irrigated field entailing combined infiltration exfiltration cycles were carried out the application of the asynchronism rule generated considerable computational benefits for all the test cases even adopting low threshold values reducing the elapsed time by up to 80 85 while preserving numerical accuracy in particular the three dimensional simulation of the drip irrigated field demonstrated the practical relevance of the approach the computational improvement produced by the combined effect of parallel computing and the proposed asynchronism strategy enables increased space time resolution on wider study areas and further developments focusing on the simulation of computationally demanding relevant processes keywords extended cellular automata subsurface flow asynchronous functionality opencal heterogeneous soil properties computational efficiency 1 introduction in the last 50 years the process based approach relying on numerical modelling has become the most common to model hydrological phenomena paniconi and putti 2015 among the various hydrological processes one of the most complex yet most investigated is the subsurface unsaturated flow whose close relationship with the soil properties and dynamics is highly non linear therefore particularly challenging to treat through numerical techniques the partial derivative equation pde traditionally describing unsaturated flow is the richards equation richards 1931 which is a combination of the mass and the momentum conservation principles the latter being expressed by darcy s law this differential equation is elliptic parabolic degenerating list and radu 2016 highly non linear and rather complex to solve numerically miller et al 2013 many different methods have been adopted to deal with it farthing and ogden 2017 such as the finite elements method fem huyakorn et al 1984 kees et al 2008 the finite volumes method fvm eymard et al 1999 manzini and ferraris 2004 or the finite differences method fdm narasimhan and witherspoon 1976 an et al 2010 which are usually solved adopting implicit explicit or mixed time solution schemes nevertheless applying the richards equation in physically based models has been criticized for several reasons fatichi et al 2016 among which the computational burden hampering the use for large scale high resolution applications dealing with computational complexity and resulting constraints for models execution in terms of both time and resources required is a historically relevant problem common to all hydrological models freeze and harlan 1969 despite the increase of computational capabilities this problem is still unsolved because the requirement of physically based models with more detailed space time scales increased as well clark et al 2017 identified different solutions to face computing challenges the first is to adopt massively parallel computation strategies kollet et al 2010 fatichi et al 2016 liu et al 2016 zhu et al 2019 based on either classical cpus or graphical processing units gpus using e g a distributed or shared memory system the second solution relies on advanced and faster numerical solvers that in their turn can be combined with several parallelization strategies lott et al 2012 maxwell 2013 dolejší et al 2019 the third solution proposed focuses on avoiding redundant calculation and is mainly based on the concept of hydrological similarity peters lidard et al 2017 indeed it is possible to describe phenomena bringing together some parts of the domain that are characterized by the same properties significantly reducing the computational cost newman et al 2014 the cellular automata ca model based approach von neumann 1966 chopard and droz 1998 is particularly suitable for implementing the first of the solutions proposed by clark et al 2017 due to its intrinsically parallel nature most explicit schemes among them fdm and fvm can be formalized in terms of cellular automata ca and easily applied as parallel computational models cannataro et al 1995 straface et al 2006 giordano et al 2019 cellular automata are defined as a dynamical system in both space and time adapted for regular structured grids where only local interactions based on specific rules allow the cells also called automata to evolve to a next state within this paradigm di gregorio and serra di gregorio and serra 1999 proposed the macroscopic or extended cellular automata xca which are a particular ca form where the purpose is to investigate the macro scale effect of local interactions the xca are widely used in many research fields e g alemani et al 2012 liu et al 2015 sun et al 2016 de rango et al 2018 d ambrosio et al 2018 tang et al 2018 concerning hydrological applications there are some examples of xca based models used to simulate groundwater flow ravazzani et al 2011 de rango et al 2020 surface flow dottori and todini 2011 cai et al 2014 guidolin et al 2016 caviedes voulliéme et al 2018 subsurface flow folino et al 2006 mendicino et al 2006 and coupled surface subsurface flow cervarolo et al 2010 2011 mendicino et al 2015 ca based models offer other opportunities in addition to their natural predisposition to parallel computing which can be exploited to improve further computational speed in particular the asynchronous functionality allows a not simultaneous update of the cells in the system in literature the term asynchronous associated with the cellular automata has been used with different meanings earlier definitions identified as asynchronous those systems where some rules were simply applied e g every n iterations e g biesiada 1986 schönfisch and de roos 1999 referred to step driven and time driven asynchronous updating methods meaning that the order of evaluation of the cells can be determined by either specific algorithms e g random sweep or fixed updating time intervals for each automaton the definition of asynchronism in this paper instead is closer to that used by worsch 2012 who imposed some rules allowing only part of the automata to evolve step by step to a next state while making quiescent i e not evolving but blocked in its current state the remaining part this type of asynchronism is very similar to that presented by bouré et al 2011 and bouré et al 2012 where an asynchronous information transmission system which is ruled by specific algorithms and can be either stochastic or deterministic allows passing information only between selected neighbour automata therefore making active only certain regions of the domain using an asynchronous ca approach part of the domain composed of either contiguous or spatially separated automata can be quiescent until a given rule allows it to evolve and vice versa i e the same rule forces previously active automata to turn quiescent asynchronous ca can be used to avoid unnecessary calculations related to non active or very weakly evolving parts of the domain an asynchronous technique was applied for the first time to unsaturated flow modelling with xca by mendicino et al 2006 they called it quantization being inspired by a previous work of zeigler 1998 but de facto they applied a rule causing asynchronous xca evolution such rule was based on a threshold representing the minimum admissible total hydraulic head variation in time in each automaton the cell was forced to stay quiescent until the threshold was exceeded then it was allowed to vary its state continuously i e it could evolve to an infinite set of new states mendicino et al 2006 showed that this asynchronous technique can break down the computational burden because it reduces the active domain at the cost however of increasing errors with increasing total head variation threshold values therefore the optimal threshold value should be selected as a trade off limiting both computational costs and numerical errors as much as possible however mendicino et al 2006 evaluated only theoretically the computational gain calculating the reduction of the number of messages they could not assess the benefit in terms of computing time since the development environment used camelot dattilo and spezzano 2003 did not allow to efficiently exclude the inactive portion of the domain from the calculation the limits of the computational environment were overcome with the development of the open computing abstraction layer opencal d ambrosio et al 2018 system in which the asynchronous functionality was efficiently built in among the first opencal applications de rango et al 2021 considered subsurface unsaturated flow modelling and applied a basic asynchronism rule which was evaluated in a simple test case only from a computational point of view i e to compare different computing times without investigating any hydrological consequence in this work we present a new asynchronism rule different from that of mendicino et al 2006 no more depending on the hydraulic head variation in two consecutive time steps but on its spatial gradient in the same time step the new rule requires total head differences comparison in the same computational step avoiding to store and query different matrices at different time steps this new approach more flexible and straightforward allowed for the first time to highlight the benefits of the practical application of asynchronous ca to subsurface flow problems we report and discuss the results of the adoption of the novel asynchronism rule in a xca subsurface flow model stressed with challenging conditions within a comprehensive evaluation framework specifically we adopted a probabilistic approach performing several hundred simulations under different degrees of soil heterogeneity in both two and three dimensional test cases the main aim was evaluating the potential of the xca method coupled with the asynchronous functionality in terms of computational efficiency though constrained by a reasonable numerical accuracy eventually the range of simulations was further enlarged to highlight the relevance of the model in a practical application and the novelty of the asynchronous approach in the field of hydrological modelling the specific goals of this paper are i to define a simple and practical asynchronism rule suitable for the opencal environment managing effectively the activation of the domain and allowing to detect the trade off between numerical accuracy and computational efficiency in an easy way ii to show through a preliminary benchmark modelled with both the synchronous xca model and a widely used fem based commercial model namely comsol multiphysics comsol 2008 the dependence of the xca based model numerical accuracy on the threshold value selected in the asynchronism rule iii to quantify in a probabilistic framework the effects produced by the adoption of the asynchronous technique in both two and three dimensional test cases with different heterogeneous conditions characterized by increased variability of the saturated hydraulic conductivity iv to highlight through a realistic three dimensional infiltration exfiltration test case both the changes induced by asynchronism to the modelling of the hydrological processes and the practical benefit of the asynchronism rule the paper is structured as follows section 2 concerning materials and methods describes the subsurface flow equations adapted in the context of the xca paradigm briefly recalls the opencal library explains the asynchronism rule adopted presents the two dimensional benchmark and provides details about the strategy used to generate the two and three dimensional heterogeneous test cases sections 3 and 4 present and discuss the results obtained for both two and three dimensional test cases including the infiltration exfiltration test case and finally section 5 summarises the main findings and outlines the main perspectives of the research 2 materials and methods 2 1 subsurface flow equations in the framework of extended cellular automata the xca flow model was presented by mendicino et al 2006 and is briefly summarized below it is based on a regular square lattice grid where the volumetric mass balance equation is employed that takes into account the moisture flux incoming from or outgoing to the neighbouring cells using the momentum conservation principle expressed by darcy s law 1 i 1 6 k c i h i h c l i a i v c c c δ h c δ t s c in the equation 1 the subscripts c and i indicate the considered cell and the neighbouring cells along the i directions respectively in a generalized three dimensional space domain 6 directions are eligible adopting the von neumann neighbourhood k c i m s 1 is the averaged hydraulic conductivity between the cell c and its neighbours h c m is the total hydraulic head in the cell c given by the sum of the pressure head ψ c m and elevation z c m h c ψ c z c δ h c δ t is the unknown term where δ h c represents the variation of the variable h in c during the interval δ t the term c c m 1 is defined as the specific retention capacity bear 1972 and represents the derivative of the volumetric moisture content on the pressure head θ h where θ m3 m 3 is defined as the ratio between the water volume m3 and the total volume of the porous medium m3 the volume of the cell is indicated as v c m3 a i is the surface area m2 normal to the water flux l i represents the distance m between the centroids of the cubic cells and is equal to the resolution of the automaton finally s c m3 s 1 in the second member represents a possible mass source term the first term in the first member of the equation considers the volumetric fluxes according to the darcy s law and can be represented as the sum of the mass fluxes along the six directions i 1 6 φ i c m3 s 1 therein k c i is calculated as 2 k c i v c v i v c k c v i k i for each cell the hydraulic conductivity k can be assumed as composed of two terms i e the saturated hydraulic conductivity k s and the relative hydraulic conductivity k r varying between 0 and 1 depending on the moisture content of the cell so that k k s k r in the equation 1 the hydraulic conductivity and the specific retention capacity depend non linearly on the pressure head hence on the total head h there are several constitutive equations proposed in literature to update the volumetric moisture content θ on which c c depends and k r as functions of the pressure head i e θ ψ and k r ψ in this study the approach proposed by van genuchten and nielsen 1985 one of the most commonly adopted in hydrogeology is used moreover to take into account transitions between unsaturated and saturated flow conditions the extended form of the van genuchten s model as defined by paniconi et al 1991 is adopted introducing a continuity parameter ψ 0 and the specific storage s s 3 θ ψ θ r θ s θ r 1 ψ n 1 1 n ψ ψ 0 θ r θ s θ r 1 ψ n 1 1 n s s ψ ψ 0 ψ ψ 0 the main feature of equation 1 is that it comes from a directly discrete formulation of the field equation i e obtained without discretizing a previously defined differential equation in a discrete cell system in which the volume enclosed by each cell has homogeneous characteristics the constitutive law used in the differential context can be used as an approximation mendicino et al 2006 though the equation derives from a directly discrete formulation in this case in practice it is the same as a fdm or fem scheme tonti 2001 because the three dimensional cubic cell system adopted is a particular case of a delaunay tessellation mattiussi 1997 manzini and ferraris 2004 mendicino et al 2006 as an explicit finite scheme the xca based model needs a convergence criterion in particular the courant friederichs lewy condition is calculated as 4 δ t l i 2 c c i 1 6 k c i regarding the asynchronous functionality mendicino et al 2006 proposed a rule based on the hydraulic head difference in time between two consecutive calculation steps since the water flow is generated by a hydraulic difference in space here we propose a new rule based on the difference between the hydraulic heads in the considered cell c and its neighbourhood in particular considering a three dimensional von neumann neighbourhood and assuming no preferential directions if all the 6 differences between c and its neighbours are lower than a fixed threshold called δ hs the automaton turns to be not active contrariwise if one or more differences are higher it turns to be active 5 i 1 6 h i h c δ h s n o t a c t i v e i 1 6 h i h c δ h s a c t i v e the new rule is implementable in the development environment in a straightforward way and is more flexible since it allows accounting for anisotropic flow simply by fixing different thresholds according to different directions the rationale of the functioning of the asynchronism rule is detailed through fig 1 it shows two consecutive time steps in a one dimensional example at time t t the total hydraulic head difference between the cells c and c 1 is higher than the threshold δ hs therefore both c 1 and c are set active on the other hand the cell c 1 is set not active since both hydraulic head differences between the two neighbour automata are lower than the δ hs threshold at time t t 1 the mass transfer between the cells c 1 and c reduces their total hydraulic head difference h c 1 h c to a value lower than δ hs therefore the cell c 1 is set not active on the other hand the new total hydraulic head in the cell c makes h c h c 1 δ h s involving that the cell c is still active and the cell c 1 also activates from fig 1 one can infer that mass transfer is delayed by higher thresholds therefore the higher the δ hs value the slower the propagation rate of a given phenomenon the xca flow model is formally defined as 6 x c a f l o w d s i x q σ φ γ t δ t ω τ n in this definition the terms in the triplet d s i represent the three dimensional computational domain d the corresponding continuum three dimensional space s and the set of domain interfaces including the boundary regions i respectively x represents the von neumann neighbourhood which is defined only over the orthogonal directions q is the finite set of cell states given by the cartesian product of the following substates the volumetric moisture content q θ the hydraulic conductivity q k the saturated hydraulic conductivity q k s the total hydraulic head q h the specific retention capacity q c c and a convergence substate q c o n v σ is the set of the local transition functions both accounting for the boundary conditions and solving eq 1 while φ is the set of functions applying σ to the non local domains defined by i γ is the set of global functions by which in our case the minimum value over the q c o n v substate is identified to define the time step of the system t is the function determining the automaton global transition and δ t is the quantity corresponding to the physical time step simulated ω is the termination criterion and finally τ n is the xca control unit allowing the automaton to evolve over time a more detailed explanation of the xca flow model formal definition is presented in the appendix a 1 2 2 opencal opencal open computing abstraction layer is an open source parallel c c software library based on the extended cellular automata xca computational formalism di gregorio and serra 1999 such formalism allows opencal to support many computational schemes like finite difference finite volume and more in general structured grid based methods opencal implementation is straightforward once the xca formalism has been defined by the user in particular the framework permits to specify the cell states the neighbourhood and the local update transition functions as well as global operations if needed with minimum effort moreover the initial conditions of the entire system and a stop criterion to the simulation can be straightforwardly implemented one of the most important advantages of opencal is the ease of obtaining different parallel versions from the serial version once a serial implementation of the model has been completed many different parallel versions can be derived transparently opencal supports many parallel architectures among them multi gpu and many core cpu shared memory devices as well as distributed memory systems de rango et al 2019 furthermore opencal provides embedded optimization algorithms allowing fine grained control over the simulation d ambrosio et al 2018 among them the asynchronous mechanism implementation can provide a significant speed up in the simulation of topologically connected phenomena 2 3 making cellular automata asynchronous the asynchronous functionality permits to narrow the computation only to the set of active cells allowing to skip the not active cells meaning that not only the transition function is not computed but also that the not active cells are completely skipped i e they are not visited by the loops spanning the domain once a model substate and a static condition threshold are chosen the cells can be easily added and removed from the set of active cells during the computation before the computation starts the set of active cells must be preliminary defined usually at the initialization stage it is worth to note that all the parallel opencal versions already support this optimization algorithm more in details the opencal algorithm considers a dynamic array namely a to memorize the active cells the cells in a identify the subset of the entire domain where the computation takes place a further array of boolean type namely b which has the same dimension of the entire computational domain is considered to track the variation of the active cells over the computational steps at the beginning of the simulation the entire array is set to false at each calculation step including or removing a cell in the set of active cells a is ruled by a process composed of two different steps i modifying the flag value of the cell in the corresponding position of the array b to false deactivation or true activation ii updating the set a according to the previous change over the array b therefore modifying i e increasing or decreasing the size of a this way the elementary processes are executed only over the set a excluding the remaining part of the domain from the computation fig 2 illustrates an example of the asynchronous mechanism applied to a simple 4 4 matrix where the patterns of the gray cells in time represent the evolution of a dynamic process e g infiltration the step t fig 2a represents a given initial stage then at step t 1 fig 2b more cells activate hence the size of the set a increases finally at step t 2 fig 2c one cells deactivate in the top left position therefore reducing the size of the set a 2 4 two dimensional benchmark the goal of this section is to propose a simple benchmarking exercise introducing the issue of the relationship between accuracy decrease and increasing asynchronous thresholds to provide a reference measure useful to assess if the magnitude of the errors with the increased threshold is meaningful the benchmark test is simulated with both the synchronous xca model and the comsol multiphysics r environment comsol 2008 a state of the art finite element model the difference between the models outputs due to the two distinct schematizations can be assumed as a measure of the acceptable output uncertainty since the accuracy of both models was already proved in previous studies the test case investigated is based on an experiment conducted by the university of arizona in the jornada test site near las cruces new mexico and reproduced using a two dimensional numerical modelling by smyth et al 1989 and mccord and goodrich 1994 it concerned a heterogeneous terrain with very dry initial conditions having three main horizontal soil layers and a fourth soil type inserted in the deepest of them fig 3 a the main hydraulic properties of the four soil types are reported in table 1 according to the experiment a domain 8 00 m long and 6 50 m deep with a uniform rain rate of 0 02 m d 1 for a length of 2 25 m along the left side of the upper boundary layer was numerically reconstructed along the other boundaries neumann conditions no flow were imposed the initial conditions on the whole domain consisted of a constant total hydraulic head equal to a negative value of 7 34 m indicating that the soil was unsaturated this test case was already addressed by folino et al 2006 in the framework of the xca paradigm and used by de rango et al 2021 to test the new opencal implementation according to those studies here a cubic lattice with an edge length δ s of 0 05 m was used having nr 130 rows nc 160 columns and only one slice the test case was simulated for a total of 30 days a total of 25 986 computational steps were required resulting in an average δ t 99 75 s for the same test case a two dimensional model domain was created and discretized with a finite element triangular grid in the comsol multiphysics r environment the grid was constituted by elements with sizes between 0 0001 and 0 16 m with a maximum element growth rate of 1 2 fig 3 b shows the hydraulic head h field calculated by comsol multiphysics after 30 simulated days figs 3c e instead show the absolute differences at the same time step between the h fields achieved with comsol multiphysics and the xca flow model using no threshold i e in a synchronous fashion and with δ hs equal to 0 25 m and 1 0 m respectively as fig 3b highlights during the 30 days simulation a relevant fraction of the domain modifies its initial water content up to δ hs 0 25 m figs 3c d the xca flow model behavior is almost identical the main differences between the outputs of the comsol multiphysics model and the two versions of the xca flow model are located along the wetting transient front and are not directly affected by the boundary conditions i e by rain and soil heterogeneity even if the variations in the sharp front of the wetting zone are high sometimes up to almost 2 5 m they do not persist in the same position for a long time since they follow the wetting front evolution higher thresholds produce similar patterns but the region along the wetting front with the highest h differences enlarges with δ hs 1 0 m fig 3e its thickness is clearly higher than the uncertainty emerging from the comparison between the fem and xca flow synchronous models therefore suggesting possible relevant errors the benchmark reveals that increasing the value of the asynchronous threshold induces a non linear increase of the errors until they become not acceptable the direct relationship of the thresholds with errors will be analysed in the next sections as well as the concurrent inverse relationship with the computational speed concerning the computational time required to achieve the solution at 30 days the comsol multiphysics model simulation on an intel i7 8700 cpu required approximately 100 seconds such as the synchronous x flow model did on an intel xeon w 2175a cpu on the same cpu the simulations with δ hs equal to 0 25 m and 1 0 m required 55 and 50 seconds respectively 2 5 two and three dimensional heterogeneous test cases with the asynchronous functionality to extensively evaluate the effects produced by the asynchronous functionality the xca flow model was applied to multiple two and three dimensional heterogeneous test cases 300 two dimensional and 300 three dimensional pseudo random synthetic fields of saturated hydraulic conductivity k s were generated through the application of simple kriging techniques with an exponential semivariogram model pebesma 2004 the field generation method required the assignment of both mean μ and variance σ 2 values of k s due to the wide range of variability of such parameter the logarithmic transform of the variable was interpolated therefore a value of μ equal to 4 19 was assigned obtained as the logarithm of the average k s of the three main areas in the case study presented in 2 4 concerning variance for both the two dimensional and three dimensional generations three different values of σ 2 were imposed 0 5 1 0 2 0 each of them used for generating 100 fields furthermore to avoid unrealistic values of generated k s values upper and lower limits of k s were imposed equal to 0 2 and 1 10 10 m s 1 respectively the two dimensional test case consisted of a rectangular domain 16 m wide and 13 m deep with a mesh size of 0 1 m producing 160 130 grid points as for the boundaries no flow conditions were imposed in all borders except along the first 3 4 m starting from the left of the upper border where a constant infiltration rate of 0 02 m d 1 was imposed simulating a rainfall boundary conditions are easily handled in the xca framework for each cell in the border of the domain it is sufficient to set either a specific h i e fixed total or pressure head or φ i e fixed mass flow value in the outside direction to represent the dirichlet or neumann boundary conditions respectively these flexible settings can vary in time variable boundary conditions and can be combined fig 4 a shows the domain geometric properties as well as the location of the rainfall boundary conditions it also shows an example of the k s pattern generated imposing σ 2 0 5 which can be visually compared with the k s fields generated imposing σ 2 1 0 and σ 2 2 0 figs 4b and 4 c respectively each of the 300 two dimensional configurations simulated 10 days starting from initial conditions of constant total hydraulic head equal to 7 34 m this setting was needed to guarantee initial steady state conditions which emphasizes the application of the asynchronous functionality van genuchten s model parameters were the same adopted in section 2 5 and are reported in table 1 furthermore a continuity parameter ψ 0 fixed to 0 001 m and the relative specific storage s s equal to 10 6 m 1 were used in the three dimensional test case the space domain was composed of a parallelepiped 30 meters long and wide with a depth of 15 meters the mesh size was fixed to 0 3 m generating 100 100 50 grid points also in this case no flow conditions were set for all boundaries except a constant infiltration rate of 0 02 m d 1 interesting the central part of the upper border of the domain specifically infiltration affected a square with a side of 12 m positioned in the centre of the upper face of the domain as showed in fig 5a such as for the two dimensional test case fig 5a also shows an example of the k s field generated with σ 2 0 5 while figs 5b and 5 c show the k s fields generated with σ 2 1 0 and σ 2 2 0 respectively also in the three dimensional test case the 300 configurations simulated 10 days with a constant initial total hydraulic head of 7 34 m all simulations were performed in an hpc infrastructure using an intel xeon gold 6128 and a shared memory parallel openmp based system with 24 threads both the two and three dimensional configurations were used to assess the effects of the asynchronous functionality in terms of computational efficiency and numerical accuracy applying different activation thresholds specifically computational efficiency was assessed through the percentage elapsed time with respect to the synchronous simulations while the statistics accounting for the numerical accuracy were evaluated in terms of both the absolute and percentage errors with respect to the pressure heads calculated with the synchronous simulations furthermore the mean square error mse m2 on the pressure head was calculated for each asynchronous simulation 7 m s e i 1 n ψ i ψ i 2 n in the previous equation the subscript i indicates the automaton where the pressure head difference is calculated with the whole domain containing n automata ψ refers to the value achieved with the synchronous simulation and ψ indicates the value obtained by adopting the asynchronous functionality both ψ and ψ are calculated subtracting the cell elevation z from the values of the state variables h and h respectively to achieve a single mse value for each threshold δ hs and variance both for the two and three dimensional test cases the average mse of the related 100 simulations was calculated finally for each of the three σ 2 values imposed for the three dimensional k s fields i e 0 5 1 0 and 2 0 one of the 100 set ups generated was randomly selected to test the asynchronous technique in a realistic situation characterized by concurring infiltration and exfiltration boundary conditions specifically the upper boundary layer was considered as a drip irrigated field with one irrigated strip of land every 2 1 m along the direction of the x axis i e given the 0 3 m cell resolution one irrigated strip of cells every 7 irrigation occurred 5 days out of 7 from 18 00 to 21 00 3 hours 2 mm h 1 besides the whole upper layer was subject to evapotranspiration every day from 12 00 to 16 00 4 consecutive hours with potential rates of 0 4 0 6 0 6 and 0 4 mm h 1 respectively no flow conditions were set for the lateral boundaries but percolation was allowed through the lower boundary layer furthermore the constant value of the initial total hydraulic head was raised to 1 0 m to enhance actual evapotranspiration the main statistics concerning soil moisture evolution at different depths during a 25 day simulation were evaluated for different threshold values accounting for the related computational times the three operational test cases provided practical examples elucidating the actual relevance of the magnitude of the error induced by different thresholds in water balance 3 results in the probabilistic framework the asynchronism effects were evaluated by considering 7 different activation thresholds δ hs 0 m 0 001 m 0 01 m 0 1 m 0 25 m 0 50 m 1 0 m in the operational infiltration exfiltration test case the thresholds were increased up to 3 5 m to better highlight the effect of the asynchronism to model outputs of practical relevance note that from a theoretical point of view the value δ hs 0 implies that results must be equal to those achieved without using the asynchronous technique indeed this threshold forces a reduction of the computational domain without any loss of information due to neglecting mass exchanges 3 1 two dimensional test cases depending on the values of k s the location of the wet front at the end of the 10 days in the two dimensional test cases was extremely variable in particular the number of automata affected by the infiltration process increased with the growth of the variance the average number of automata activated with the lowest threshold i e δ hs 0 was 1736 8 adopting a value of σ 2 0 5 against 1961 9 5 obtained with σ 2 1 0 and 2387 11 5 achieved with σ 2 2 0 the increase of the cells involved in the mass exchange process with increasing variance depends on the fact that the higher the variance the higher the possibility of occurrence of high saturated conductivity values concurring to the generation of preferential paths which in turn transfer faster the water to regions of the domain further away from the infiltration border fig 6 shows an example of the spatial distribution of the percentage error achieved by varying the activation threshold δ hs at the end of the 10 days simulation period in a configuration where k s was generated with σ 2 1 previously shown in fig 4b errors enhanced increasing the activation threshold because of the higher number of cells whose mass exchange was neglected the propagation of the wet front was slowed down by the delayed activation of the automata in the wet dry border contrariwise adopting low activation thresholds i e near 0 the distributed errors were almost 0 analysing the problem from a statistical point of view every simulation can be considered as an occurrence of a 100 members sample distribution characterized by a specific σ 2 value and activation threshold δ hs fig 7 shows the characteristics of the sample distributions in terms of computational efficiency and numerical accuracy the red pink colours refer to the maximum error achieved in the domain while the black grey colours refer to the computational time the dots indicate the median values while the bands represent the ranges between the 1st and 3rd distribution quartiles the computational time was calculated as the percentage with respect to the synchronous simulations results show that for each σ 2 value even adopting low thresholds the computational time was highly reduced with σ 2 2 0 the simulation time decreased from approximately 300 seconds achieved by synchronous simulations to only 90 seconds with δ hs 0 conversely using high activation thresholds the computational time did not improve significantly further while the maximum error generated increased more than linearly also the computational gain did not change appreciably increasing the σ 2 value but the errors due to the higher thresholds were larger because of the higher possibility that the previously mentioned preferential paths affecting significantly the wet front propagation speed were activated later compared to the reference simulation table 2 shows the mse m2 calculated in the whole domain following eq 7 in agreement with fig 7 the errors grew up by increasing δ hs or σ 2 as a further evidence that imposing a larger number of cells with neglected mass exchange or a higher variability on k s generated greater delay in the wet front propagation 3 2 three dimensional test cases also in the three dimensional test cases infiltration did not propagate all over the domain the number of cells interested by the phenomenon increased adopting higher σ 2 vales in particular the average number of automata activated adopting δ hs 0 was 4580 0 9 41507 8 3 and 85565 17 1 fixing σ 2 0 5 1 0 and 2 0 respectively fig 8 shows some examples of the wet front spread specifically figs a c refer to results achieved using a domain configuration characterized by σ 2 0 5 figs d f by σ 2 1 0 and finally figs g i by σ 2 2 0 furthermore figs a d g refer to synchronous simulations figs b e h concern the threshold δ hs 0 and figs c f i the threshold δ hs 1 0 m related cross sections are provided as supplementary material fig s1 the visual comparison among the rows highlights the increasing wet front spread with higher degrees of heterogeneity instead the comparison among the columns points out the effect of asynchronism which as in the two dimensional test cases produced a slow down in propagation the statistical analysis confirmed the findings achieved with the two dimensional test cases fig 9 shows the distribution of computational time and maximum errors depending on σ 2 and δ hs while table 3 reports the mse m2 calculated with the three dimensional test cases the only appreciable difference between two and three dimensional results is the slower increase of the errors in the latter when higher thresholds and variances were considered this is due to the different grid size adopted 0 1 m and 0 3 m for the two and the three dimensional test cases respectively as indicated in section 2 5 this difference implied that with equal propagation of the wet front fewer automata were involved in the lower resolution three dimensional simulations therefore fewer preferential paths were affected by the phenomenon and a smaller number of automata activation and deactivation occurred with all thresholds in other words the asynchronous simulations were closer to the reference synchronous simulations 3 3 drip irrigation three dimensional test case this test case differs from the previous ones because besides considering also exfiltration it involves a higher percentage of cells in the process the number of cells activated during the 25 day simulations with δ hs 0 was equal to 70 6 86 7 and 84 3 for σ 2 0 5 1 0 and 2 0 respectively since the main objective of the drip irrigation test case was the analysis of the practical implications of the asynchronous approach the investigated variable was the volumetric moisture content θ fig 10 shows the average soil moisture evolution during the 25 simulated days at three soil depths according to different thresholds up to 3 5 m in the case with σ 2 0 5 corresponding figures with σ 2 1 0 and 2 0 are reported in the supplementary material figs s2 and s3 while an example of the three dimensional volumetric moisture content distribution at the end of the simulation period is shown in fig s4 the opposing top boundary conditions given by infiltration in a subset of cells and exfiltration all over the top boundary resulted in a slow drying process therefore although the graphs are designed to highlight as well as possible both θ variation in time and the differences achieved by using different thresholds it is important to acknowledge that overall θ variability is small as the values in the vertical axis highlight nevertheless these small differences explain well how the selected thresholds affected the complex dynamics of the process under investigation though there is not clear moving wet front such as in the previous experiments even this test case highlights that the higher the asynchronous threshold the slower the system response to the boundary forcing fig 10b relative to the shallower layer more affected by exfiltration depth equal to 0 45 m shows that an increase of the threshold causes faster drying because upward subsurface flow allows replacing exfiltrated water less quickly in a complementary way the deeper layer moisture fig 10d depth equal to 1 35 m increases with the threshold because the upward movement does not start or starts very late in the layer at intermediate depth fig 10c 0 75 m with σ 2 0 5 the soil moisture evolution does not change up to δ hs 0 5 m then for higher δ hs values it resembles the evolution of the deeper layer for higher σ 2 values figs s2 and dummytxdummy s3 the intermediate behaviour observed in the layer at depth 0 75 m extends to the deeper layer because the higher k s variability facilitates the creation of preferential paths that speed up the upward water movement for each σ 2 value and each of the three layers at depths of 0 45 m 0 75 m and 1 35 m fig 11 compares the computational saving achieved with the asynchronous functionality with a measure of the error namely the average absolute percentage error of θ in the layer at the end of the simulated 25 days the use of δ hs 0 does not produce many benefits since the number of cells involved in the simulations with this threshold is high nevertheless increasing δ hs to 0 01 m allows reducing the computational times to 20 30 of the initial values keeping the error at very low values selecting higher thresholds such as for the previous experiments does not lead to a further notable reduction of computational times errors remain always well below 1 they tend to grow with thresholds but not always monotonically in agreement with the physical explanation of the effect of the asynchronous functionality on the process dynamics described before the layer that deviates most from the monotonic trend is the intermediate one depth 0 75 m it is particularly affected by the combined effect of exfiltration and infiltration resulting in a less predictable behavior with varying thresholds 4 discussion the proposed two and three dimensional domain configurations refer to a particularly challenging hydrological process unsaturated flow in heterogeneous soils is the subject of numerous recent studies relying on elegant numerical solutions aimed at solving specific problems e g layered soils berardi et al 2020 or fractured porous media kumar et al 2020 and improving computational efficiency to allow large scale watershed simulations suk and park 2019 while keeping a relatively simple directly discrete approach and being naturally predisposed to parallel computing the xca flow model demonstrated to be accurate and that can further increase its efficiency if the asynchronous functionality is well managed the results of the simulations performed within the probabilistic framework suggest that from a hydrological point of view the application of the asynchronous functionality produces a slowing down of the wet front propagation due to the delay of automata activation at the wet dry interface this behaviour leads to highly localised errors in pressure head distribution at the propagation interface contrariwise in the remaining part of the domain where the automata are activated earlier and not deactivated during the simulation errors are very low or null because the system is able to re balance however the strip of the domain with more evident errors near the propagation front increases its thickness as the simulation proceeds obviously in the areas not reached by infiltration there is no error numerical errors are influenced by the choice of the asynchronous threshold δ hs and grow up increasing it the rate of growth changes depend also on the k s values variability in general high k s variability corresponds to the creation of preferential paths that influence significantly the wet front propagation but are activated erratically as the asynchronous threshold increases the operational drip irrigation experiment further confirmed that system evolution is slowed down by higher thresholds in a completely different test case with a higher percentage of cells involved nevertheless the slightly delayed cells activation did not prevent reaching an acceptable accuracy for model outputs of practical relevance such as soil moisture state while computational times reduced significantly with very low asynchronous thresholds also in this test case in analogy with video recording the asynchronous functionality works at a smaller frame rate in those regions of the domain where it is allowed i e where numerical accuracy is not impaired of course in contrast to video recording modelling does not simply film a physical process and the selected level of detail affects process representation the asynchronous functionality herein proposed provides a method for objectively quantifying the minimum resolution to comply with for ensuring acceptable accuracy in subsurface flow modelling the test cases performed both two and three dimensional with increasing heterogeneous hydraulic properties varying boundary conditions and a different number of cells activated highlighted the potential of such an approach the benefit of the asynchronous functionality can be further elucidated relating it to the phenomenon under investigation e g point source vs global perturbation of the system but also its link with soil hydraulic properties is worth further analysis since the hydraulic conductivity affects the timing of hydraulic head redistribution and consequently the activation deactivation rule this research question deserves specific analysis and will be the subject of further research from a computational point of view the xca flow model adopts an explicit finite difference scheme on the one hand the application of a time explicit scheme requires a strict numerical convergence criterion e g courant friedrichs lewy condition that led many researchers to not delve into it on the other hand also the adoption of time implicit solution schemes requires a very intensive cpu effort mcbride et al 2006 due to the time step size adaptation kavetski et al 2001 efforts were made to adapt not only the time step size but also the spatial discretization e g mostaghimi et al 2015 solin and kuraz 2011 in those cases where an increase of the grid resolution for specific regions of the domain is needed however this type of solution is not widely used in practice farthing and ogden 2017 and the applications of high detail physically based models at basin or regional scales are often limited due to the intensive calculation required the alternative solution herein proposed does not rely either on time step or on grid size re adaptations rather a regular grid is considered made of square or cubic automata in our case having a resolution high enough to represent the analysed phenomenon then the active part of this grid i e the portion actually involved in the calculus changes dynamically at each time step according to the evolution of the phenomenon allowing the presence of dynamic regions of the domain where no calculation is executed the proposed approach is flexible and could also integrate other optimization strategies e g the ca grid structure could even be irregular bochenek and tajs zielińska 2017 zeng et al 2010 finally the proposed test cases provide a guideline for the selection of the asynchronous threshold value the experiments performed showed that a good trade off between numerical accuracy and computational performance can be already detected adopting very low thresholds which break down the computational efforts considerably ensuring very small or even null errors 5 conclusions in this note the asynchronous cellular automata functionality is investigated performing several hundreds of subsurface flow simulations under different heterogeneous conditions in both two and three dimensional test cases the subsurface flow model whose original formulation was proposed by mendicino et al 2006 was modified for what concerns the asynchronous functionality and reimplemented exploiting the highly efficient opencal parallel software library d ambrosio et al 2018 the asynchronous functionality was applied adopting an activation deactivation threshold linked to the spatial total hydraulic head difference which is the cause of the water flow through the porous media the effects produced by this functionality were evaluated in terms of both numerical accuracy and computational time reduction a preliminary benchmark comparison with both the synchronous version of the xca flow model having a numerical approximation based on a 1st order truncation scheme and a state of the art finite element model 4th order truncation suggested negligible errors for small thresholds as well as non linear error increase with increasing threshold values then the results achieved through a probabilistic approach highlighted that ca asynchronism generates considerable benefits from the computational point of view reducing the elapsed time by up to 80 even adopting low thresholds while the numerical accuracy is affected by significant errors only with relatively high thresholds finally a realistic three dimensional simulation of a drip irrigated field involving both infiltration and exfiltration processes further confirmed the performance of the model highlighting that an acceptable accuracy of the operational outputs such as soil moisture estimates can be easily achieved together with a significant saving of computational times overall the asynchronous functionality demonstrated to be an efficient computational optimization feature which fits very well the cellular automata paradigm and could be easily coupled with other strategies to speed up computational time such as the adoption of irregular meshes or time step adaptation methods nevertheless it is worth to highlight that first and foremost the model benefits from the xca natural aptitude to parallel computing which can be fully exploited through the opencal library parallelization strategies have been recently enriched with the alternative offered by gpus which are being already adopted for hydrological applications e g lacasta et al 2015 aureli et al 2020 in this regard among its functionalities opencal allows the user to transparently transfer cpu based to gpu based code permitting to explore the possibility of breaking down further the computational burden further research will address the enhancement of computational optimization strategies involving gpus and load balancing techniques giordano et al 2020 which will be tested with improved versions of the xca flow model concerning future modelling advancements it should be noted that though the xca method has been already thoroughly tested against one two and three dimensional literature data the same comparison with the asynchronous functionality was performed only indirectly i e it was compared against the synchronous model and a fem model section 2 4 direct evaluation of the asynchronous functionality against literature data or even with classical numerical richards equation simulations will be performed furthermore the connection between the asynchronous functionality and soil hydraulic properties will be investigated and different asynchronism rules based not only on the spatial difference of the hydraulic head will be tested finally it is worth it to underline that the unsaturated flow model herein proposed is part of a more complex modelling system that is currently being developed including also surface flow surface subsurface interactions and transport processes credit authorship contribution statement luca furnari methodology formal analysis writing original draft alfonso senatore conceptualization methodology formal analysis writing original draft alessio de rango writing original draft michele de biase formal analysis salvatore straface conceptualization methodology formal analysis writing original draft giuseppe mendicino conceptualization formal analysis writing original draft declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements l furnari acknowledges support from the programme por calabria fse fesr 2014 2020 mobilitá internazionale di dottorandi e assegnisti di ricerca ricercatori di tipo a actions 10 5 6 and 10 5 12 m de biase acknowledges the european commission for the financial support activated under the pon research and innovation 2014 2020 action i 2 researchers mobility notice d d 407 of 02 27 2018 aim attraction and international mobility line 1 researchers mobility appendix a a1 formal definition of xca flow model from a mathematical point of view the model is formally defined with the following functional structure de rango et al 2021 d 0 n r 1 0 n c 1 0 n s 1 z 3 is the three dimensional discrete computational domain with n r n c and n s the number of rows columns and slices respectively s 0 n r δ s 0 n c δ s 0 n s δ s r 3 is the continuum three dimensional space corresponding to d subdivided in cubic cells of side δ s the function μ defines the mapping to d as μ d r 3 ι 1 ι 2 ι 3 ι 1 δ s ι 2 δ s n s 1 ι 3 δ s i i ρ i β i τ d is the set of domain interfaces where i ρ π s 0 is the boundary region belonging to the top surface π s 0 that could be affected by the interaction with the surface boundary layer e g rainfall or evapotranspiration i β is the remaining domain boundary region i τ d i ρ i β defines the inner domain where the system evolution occurs x 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 is the von neumann neighbourhood q q θ q k q k s q h q c c q c o n v is the set of states for the cell where q θ is the set of values representing the volumetric moisture content q k is the set of values representing the total hydraulic conductivity q k s is the set of values representing the saturated hydraulic conductivity q h is the set of values representing the total hydraulic head q c c is the set of values representing the specific retention capacity q c o n v is the set of values representing the time step size needed for numerical convergence σ σ ρ σ β σ τ is the set of local transition functions or kernels in particular σ ρ q h q c c q h accounts for the top boundary region belonging to the surface π s 0 that could be affected by the interaction with the surface boundary layer σ β q h q h sets the conditions on the remaining boundary region not directly affected by atmospheric interactions σ τ q x q corresponds to the explicit form of the finite difference scheme in 1 also the substates q θ q k q c c and q c o n v are updated according to equations 3 2 and 4 respectively moreover σ τ can activate deactivate cells of the computational domain if the asynchronous algorithm is exploited φ ϕ ρ ϕ β ϕ τ is the set of functions applying the local transition σ to the non local domains defined by i in particular ϕ ρ q i ρ q i ρ applies the σ ρ local transition to the i ρ interface ϕ β q i β q i β applies the σ β local transition to the i β interface to account for boundary conditions ϕ τ q i τ q i τ applies the σ τ local transition to the i τ interface here i τ denotes the union of i τ and the set of cells belonging to d i τ that are needed to guarantee a complete neighbourhood to each boundary cell of i τ according to the ca definition only states of cells in i τ are updated γ γ t γ t q c o n v d r is the set of global functions where γ t evaluates the minimum value over the q c o n v substate to define the physical time corresponding to the computational step of the system specifically if δ t indicates the time step size it is δ t min ι d q c o n v ι t ϕ ρ ϕ β ϕ τ γ t is the function determining the automaton global transition δ t r is the quantity corresponding to the physical time interval simulated by a state transition of the automaton it is evaluated step by step by considering the γ t function ω r f a l s e t r u e is the termination criterion when the prefixed simulated time interval is complete ω returns f a l s e and the simulation terminates τ n n c c is the xca control unit at step n 0 the xca is in the initial configuration c 0 τ n is then applied at discrete steps by producing a sequence of configurations c 1 c 2 until the ω termination criterion is satisfied supplementary material supplementary material associated with this article can be found in the online version at 10 1016 j advwatres 2021 103952 appendix b supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
301,traditional interpolation techniques for particle tracking include binning and convolutional formulas that use pre determined i e closed form parameteric kernels in many instances the particles are introduced as point sources in time and space so the cloud of particles either in space or time is a discrete representation of the green s function of an underlying pde as such each particle is a sample from the green s function therefore each particle should be distributed according to the green s function in short the kernel of a convolutional interpolation of the particle sample cloud should be a replica of the cloud itself this idea gives rise to an iterative method by which the form of the kernel may be discerned in the process of interpolating the green s function when the green s function is a density this method is broadly applicable to interpolating a kernel density estimate based on random data drawn from a single distribution we formulate and construct the algorithm and demonstrate its ability to perform kernel density estimation of skewed and or heavy tailed data including breakthrough curves keywords particle methods kernel density estimation machine learning nonparametric kernel 1 introduction in many applications discrete samples of a continuous and potentially complex random process are generated as output even though a continuous solution is desired some examples are given by particle tracking of passive solute transport e g fernàndez garcia and sanchez vila 2011 pedretti and fernàndez garcia 2013 siirila woodburn et al 2015 carrel et al 2018 reactive particle transport e g ding et al 2012 2017 schmidt et al 2017 sole mari et al 2017 sole mari et al 2019 sole mari and fernàndez garcia 2018 benson et al 2019 perez et al 2019 engdahl et al 2017 2019 and monte carlo and bayesian simulation e g taverniers et al 2020 in short many of the quantities used by hydrologists are probability density functions that are constructed by users even thought there is no concrete and accepted methodology for their construction a long history of statistical estimation has sought to best fit some continuous density function to a sequence of random samples including maximum likelihood estimation brockwell and davis 2016 and kernel density estimation silverman 1986 the former assumes a functional density form and estimates its parameters while the latter fits a continuous function to discrete data tests of functional fits or other statistical properties may be conducted later in hydrology and many other sciences the underlying processes being simulated may be sufficiently uncertain that a functional form for the density function cannot be assumed and kernel density estimation is preferred given a true underlying pdf f x kernel density estimation is based on the convolution like interpolation or extrapolation of discrete random data x 1 x 2 x n with some kernel function k x producing the estimated pdf 1 f x 1 i 1 n w i j 1 n w j h j k x x j h j where w j are weights associated with data points x j which could be prior concentrations that come from binning h j are bandwidths associated with the kernel applied at each data point and k is some pre determined non negative function with the requirement k x d x 1 i e k is a pdf for random samples the weights are equal constants that cancel from expression eq 1 resulting in a factor of 1 n the common forms of k are relatively simple e g triangles or standard gaussians and yield estimates of f x with different properties such as regularity i e number of derivatives or compact support kernels that are symmetric around x 0 are most commonly used but certainly not always see hirukawa 2018 inasmuch as the eventual form of f x including skewness or heavy tails are unknown a priori it is well known that a pre chosen kernel such as a standard gaussian does not perform well if all of the bandwidths are chosen to be the same size silverman 1986 where data is more dense the kernel bandwidths must be made smaller this has led to adaptive bandwidths that are adjusted based on the apparent or estimated density at the data points higher estimated density values at data points are given smaller bandwidths but one may ask should the functional form of the kernel also be adjusted based on the estimated density we suggest and provide evidence in appendix c that the optimal kernel should be the same shape as the underlying true density which is best estimated by the interpolated density but clearly the estimated density changes if the kernel shape changes therefore an iterative procedure is required we define this procedure in section 3 after a brief review of kernel density estimation in section 2 a series of examples are given in sections 4 and 5 and we conclude in section 6 2 classical bandwidth selection intuitively one would like to choose a bandwidth as small as possible because the convolution adds the variance of the kernels to the data itself on the other hand as h 0 the kernels become delta functions and continuity of f x disappears additionally the choice of h i will depend strongly on both the eventual shape of f x and the availability of random samples in any interval x x δ x this has led to expressions that balance the bias and variance of the estimates silverman 1986 that we review here and re derive in appendix a a common place to start is to minimize the mean integrated squared error mise between the estimated and unknown real densities given by 2 mise e f x f x 2 d x taking the expectation inside the integral and realizing that the mean squared error of an estimate is composed of squared bias and variance terms one finds e f f 2 e f f 2 e f e f 2 which gives a target functional for minimization typically a truncated taylor series is used to derive asymptotic h 0 nh expressions for the bias and variance that depend on the properties of the kernel and underlying density silverman 1986 this process appendix a results in approximations for the bias 3 b x bias f x e f x f x h 2 2 f x μ 2 k o h 3 and variance 4 var f x e f e f 2 1 n h f x k 2 x d x o n h 2 all other things held equal letting h 0 minimizes bias but variance grows without bound i e accuracy increases but smoothness decreases while letting h grow large decreases the variance of estimates but accuracy is sacrificed minimizing the sum gives a value for the optimal global bandwidth 5 h 0 d k 2 x d x n μ 2 k 2 f x 2 d x 1 d 4 where d is the number of dimensions of the random variable d 1 herein notice that a finite second moment μ 2 k is necessary to use this method in the estimation of the optimal bandwidth we remove that requirement herein appendices a c without any information at all it is common to assume gaussian f x and gaussian kernels in which case a constant global bandwidth is used with size 6 h 0 1 06 n 1 5 σ where σ is the sample variance greater data density means smaller bandwidth until as n h 0 0 because this estimation of finite h 0 is based in part on an assumption of h 0 0 we might expect significant error in any estimate of the global bandwidth value using eq 5 indeed an exact value of h 0 can instead be derived using the fourier transform appendix b and we show that the result in eq 6 can be significantly erroneous furthermore it is largely recognized e g silverman 1986 that the local data density is a better indicator of bandwidths that should be uniquely defined at each data point in regions where data density is smaller the bandwidth should be greater there are several methods used to estimate local data density e g silverman 1986 wu et al 2007 sole mari and fernàndez garcia 2018 for example silverman 1986 shows that for large n the local data density can be approximated by the value of the estimated pdf so that an adaptive bandwidth can be estimated by 7 h i h 0 λ i h 0 f x i g ξ where the tilde indicates some intermediate estimate of the density and the normalization factor g is the geometric mean of estimated density values namely 8 g exp 1 n i 1 n ln f x i the exponent 0 ξ 1 is an empirical weighting factor shown to be 0 5 under ideal conditions abramson 1982 in a novel way pedretti and fernàndez garcia 2013 investigated the use of the adaptive kernel methods eqs 5 7 and 8 for interpolating breakthrough curves btcs for simulated push pull single well tests with trapping in relatively immobile low velocity zones these btcs are noteworthy for their thin early tails and fat late tails or rapid steep early breakthrough and delayed power law decline of concentration importantly pedretti and fernàndez garcia 2013 found that adjusting the bandwidth based only on particle density tended to overly broaden the early btcs in order to more properly represent the late tail pedretti and fernàndez garcia 2013 then imposed a restriction on broadening the kernel bandwidth based on whether particles concentrations in eq 1 occurred early or late in the btc this of course means that the user must decide how the bandwidths must be adjusted but this is simply a side effect of choosing a priori a non physical and symmetric kernel if each particle were treated as a single realization of the green s function then its highly skewed kernel would transfer little mass to earlier portions of the btc and no adjustment may be needed we investigate that possibility here 3 iterative algorithm we show appendix c that asymptotically as n to minimize the mise the kernel applied to each random sample should be a scaled version of the underlying true density itself this suggests that for a reasonably large number of data n the kernel k should be made functionally similar to the estimated density f as this is the best representation of the true density f of course the shape of the density is not known a priori so the shape of the kernel must be learned during the estimation process we seek to find f to best approximate f and we find f through successive intermediate estimates that we call f our proposed algorithm discovers the kernel shape and size recursively according to the following steps 1 build an initial candidate f 0 x using constant bandwidth h 0 and standard gaussian kernel k x 2 π 1 2 exp x 2 2 in eq 1 2 use f 0 x to interpolate values at data points f x i 3 use the values f x i in eq 7 to estimate adaptive bandwidths h i for the gaussian kernel and re estimate f 1 x this would end classical estimation set counter ℓ 1 4 use f ℓ x as the new kernel k ℓ x f ℓ x 5 adjust kernel k ℓ to have zero mean and unit width 6 use f ℓ x to interpolate values at data points f x i 7 use the values f x i in eq 7 to estimate adaptive bandwidths h i for the new kernel k ℓ x 8 use new kernel function k ℓ and bandwidths h i to estimate f ℓ 1 x using eq 1 9 return to step 4 until desired closure between f ℓ 1 x and f ℓ x upon closure f ℓ 1 x is the best estimate of f x the potentially tricky parts of the algorithm are associated with steps 1 4 5 and 7 for step 1 the distributional qualities of the data are unknown so we use a fourier transform algorithm to estimate the data density function see eq 17 in the appendix by assuming a gaussian kernel the initial h 0 can be easily estimated for step 4 it is important to use a numerical domain for x that is wider than the data values so that the kernel may extrapolate sufficiently before the smallest data point and after the largest furthermore for widely spaced and sparse data the density of calculated points in x must also be chosen to provide sufficient resolution for step 5 it is not always clear that the mean and standard deviation exist or are the proper scaling metrics for the iterated kernel a simple example is a stable density which may have diverging moments and also rescale differently from say a gaussian density here we suggest using the interquartile range of the data and the kernel for a reasonably close and reliable estimate of the scale of many different density functions for step 7 we are now using a kernel that is thought to resemble the underlying density so using k f or f in eqs 3 5 and 7 will give different values of h 0 etc more on these points is provided below in order to find a standard kernel from the previous iteration s density estimate f the kernel must have zero mean so that the subsequent addition of kernels has the same mean as the data the width of the kernel should be standardized such as normalizing by a scale factor equal to the standard deviation of the data or central second moment of f however many densities have diverging second moments so a robust method must be found for situations in which the underlying density is unknown a quick survey of the interquartile range iqr and the scale factor of many densities shows reasonably similar relationships for finite variance distributions we find for example the gaussian has σ iqr 1 35 the exponential σ iqr 1 1 the laplace σ iqr 0 98 infinite variance distributions with closed form distribution functions characterized by scale parameter σ include the symmetric cauchy with σ iqr 2 and the maximally skewed 1 2 stable lévy density with σ iqr 9 using matlab s routine for calculating the cdf of a stable law we find that for a maximally skewed 1 5 stable σ iqr 2 13 with the exception of the lévy density it is a reasonable approximation to say that the width of the density function may be standardized using σ iqr 1 5 therefore in the following to arrive at a standard density from the data kernel we numerically integrate the intermediate density f ℓ 1 to find iqr x 0 75 x 0 25 where x z min x j δ x ℓ 1 n f ℓ 1 x j z and simply shift and rescale the experimental density by its first moment m 1 and a generic multiple of the iqr so that k ℓ x 1 iqr 1 5 f ℓ 1 x m 1 iqr 1 5 as noted before the kernel is allowed to change after each iteration and this kernel is checked against the previous iteration s kernel iteration is terminated when the kernel converges and the difference between the density estimated with those kernels in successive approximations is sufficiently small here we choose to discontinue the algorithm when the l 2 difference between successive iterations is less then 10 9 where 9 l 2 δ x i 1 n f ℓ x f ℓ 1 x 2 if the l 2 difference is found to increase between iterations this indicates too large a global bandwidth h 0 assuming that one starts with a conservatively large value from the fourier transform procedure the too large value of h 0 makes the kernel itself too smooth and also gives it too large a scale based on the iqr so convergence will not occur in practice there may be a range of h 0 values that leads to convergence based on some numerical threshold of the l 2 norm so some care needs to be used when adjusting h 0 when the initial h 0 estimate is far from the correct range one may decrease h 0 by a factor of 0 9 as the algorithm gets nearer the correct kernel and h 0 the minimum of eq 9 gets progressively smaller and there is a danger of overshooting the optimal h 0 so the algorithm slows the adjustment of h 0 by incrementally moving the factor toward unity after h 0 is adjusted iteration resumes in practice if the minimum l 2 eq 9 reached for a given h 0 is on the order of 10 4 the value of h 0 is reasonably far from the optimal and may be decreased by about 10 each order of magnitude improvement in the l 2 convergence is accompanied by moving the factor 2 closer to unity we also add that other h 0 estimators can be used that may underestimate the optimal h 0 and so the same procedure to adjust the value is done in reverse say starting with an adjustment factor of 1 1 that decreases toward unity based on minimum l 2 norm seen with any h 0 value examples can be seen in the matlab code provided at https github com dbenson5225 kernel density estimation another important consideration is the construction of the set of points at which the density is calculated through some experimentation we find that an optimal set of points is made from a union of 1 a set of appropriately spaced points between a desired minimum and maximum that is larger than the measured data range and 2 the set of actual random data values x i the first set is important so that sufficient interpolation between widely spaced data is made the second set is sometimes important so that the weights are accurately calculated within eq 7 we experimented with neglecting the second set and simply interpolating the density at points x i from points in the first set but for spiky densities the results depend too much on the density of points that are specified if the number of points at which the density is calculated becomes large it is a simple matter to parallelize a large part of the procedure because the calculation of f x in eq 1 is independent for any x value 4 examples we investigate the iterative algorithm versus classical assumed gaussian kernel methods for four types of data 1 symmetric and thin tailed 2 maximally skewed and exponentially tailed 3 symmetric and heavy power law tailed and 4 maximally skewed and heavy power law tailed the last is chosen because btc data are strictly positive and often observed to fall off like x 1 α where α is on the order of 0 5 we also investigate how well the estimators perform over a large realization of random samples and a range of population sizes n 100 1000 10 000 inasmuch as large particle numbers and random arrival times are typically used in each case we use estimates of the mise to measure bias and variance of the estimated density versus a known density on a regular grid in x a numerical estimate of the mise is given by an ensemble mean of the l 2 norm namely mise δ x m m 1 m j 1 n f m x j f x j 2 for a set of m realizations of data with an underlying density f and the corresponding estimates of the density f m on a common grid of estimation points x j with spacing δ x for each of the examples we generate m 100 independent realizations of data from known distributions in order to estimate the densities and resulting mise table 1 4 1 gaussian data we start with gaussian random variables in which the data kernel should nearly converge to an a priori gaussian kernel because the underlying data that builds the data kernel is gaussian indeed for a large number of data points 1000 the iterated kde for the data based and gaussian based kernel are nearly identical even in the extreme tails fig 1 this example shows the robust nature of the estimation inasmuch as the gaussian kernel uses the exact width of the kernel σ 1 while the iterated kernel uses a general width estimate of iqr 1 5 in actuality the width of a gaussian is σ iqr 1 34 it is worth noting that closure to the final kernel usually takes between 5 and 7 iterations furthermore because the value of h 0 is not set exactly which would require identifying the data as gaussian before interpolating the iterated kernels have similar magnitudes of mise as single pass adaptive gaussian kernels and convolution with a single value of h 0 table 1 4 2 exponential data next we use a shifted exponential the arbitrary shift is added to ensure functionality of the code with density function f x 1 σ exp x μ 1 σ σ for x μ 1 σ 0 else this density has arbitrary mean μ and variance 1 σ 2 in the plots that follow we set μ 0 and σ 1 for this skewed density it becomes clear that a symmetric gaussian in this case kernel is not an effective interpolant fig 2 while silverman 1986 suggests using a skewed say lognormal kernel for this kind of data our method does not rely on interpretation and user intervention for kernel selection and while a gaussian kernel is not particularly useful for this kind of data the iterated data based kernel typically has mise of about half that of the gaussian table 1 because the underlying optimal h 0 is much smaller than that estimated from assuming a gaussian kernel the iterations do not converge and in fact tend to diverge until h 0 decreases several times requiring on the order of 30 or more iterations for 1000 data points 4 3 cauchy data heavy tailed data present a problem for kernel density estimation because of the extremes that may accompany the data this leads to very wide spacing between extreme data points and difficulty interpolating the density here this also means that the x discretization of the kernel must use a large number of points in order to represent the near origin spikiness of the density as well as the very long range the existence of one or two super extreme values can lead to numerical problems in our 100 realization ensemble of 1000 cauchy data points two realizations failed to converge in 100 iterations with the data based kernel because of data values in the 50 000 range a typical realization shows that the converged data based kernel tends to both interpolate between and extrapolate beyond extreme values better than the gaussian kernel but still represents the fine scale near the origin where most of the data reside fig 3 in the ensemble the mise estimated for the data based kernel is substantially less than for the gaussian kernels table 1 4 4 maximally skewed α stable data stable random variables rv are characterized among many other ways as those to which sums of iid random variables converge samorodnitsky and taqqu 1994 sums of finite variance rvs converge to are in the domain of attraction of a gaussian which is itself a stable rv when for some constant 0 α 2 only those moments of order α and greater are infinite such as for pareto power law distributed rvs then those rvs are in the domain of attraction of a α stable these rvs arise in hydrology quite naturally because they describe waiting times that a particle might take when trapped in a sequence of fractal immobile zones see schumer et al 2003 benson et al 2013 depending on the skewness parameter one or both of the tails of a α stable density function decay like x 1 α therefore all moments of order α and greater diverge the density is only expressible in closed form for a few instances but most statistical packages will readily calculate the density to any desired tolerance and generate sequences of the random variable here we choose a maximally skewed standard 1 5 stable for analysis using the parameterization in the matlab statistics package also called the 0 parameterization in nolan 2018 the ensemble mise for the data based kernel is about 1 5 that of the iterated gaussian kernel suggesting that both the heavy tailed and skewed nature of this example is especially well suited to our proposed method fig 4 5 particle breakthrough concentration data the creation of breakthrough curves btc from particle tracking simulations is a tricky proposition classically histograms are used which means manually choosing either constant or variable bin sizes and locations the variance of the estimated density is inversely proportional to bin size total number of particles and the estimated concentration chakraborty et al 2009 and the histogram based density is discontinuous and may frequently be zero when particle arrival times are widely separated especially in the late time tail the zeros make comparison to non zero data difficult e g using weighted least squares so several methods are typically used to create a non zero pdf interpolation the first set of constructions of arrival time pdfs which we will call naive estimators is based on simple linear interpolation of arrival times for example one may construct by several means an empirical cumulative distribution function ecdf that is strictly increasing and then make a non zero empirical pdf using finite differences on the ecdf in particular order the particle arrival times of n particles t 1 t 2 t n and at each point the ecdf t i i n then the empirical pdf is epdf t i 1 t i 2 ecdf t i 1 ecdf t i t i 1 t i i 1 n 1 the ecdf can also use a regularly spaced time grid and count numbers of particles arriving between grid points i e bins and empty bins are neglected once again giving a strictly increasing ecdf in this section we compare these two naive estimators to the iterative kernel based techniques developed in this paper along with prior deterministic kernel based methods fernàndez garcia and sanchez vila 2011 pedretti and fernàndez garcia 2013 for particle arrival times we solved for the hydraulic head h in the steady state groundwater flow equation k h 0 in 2 d using finite differences on a square 128 128 m grid with constant grid discretization of 1 1 m fig 5a the hydraulic conductivity k is a scalar log normal random variable with a mean of ln k 1 standard deviation of ln k 4 and an exponential autocorrelation function for ln k with correlation length of 5 m the left and right boundaries x 0 and x 128 are dirichlet with h 1 and h 0 respectively the top and bottom boundaries y 0 and y 128 are neumann with h y 0 the resulting velocities take the solved h field and apply v k h ϕ where ϕ is assumed a constant porosity of unity once again using finite differences these velocities vary in magnitude from about 3 10 7 to 2 1 m d fig 5a particles are placed in a line near the left boundary and each particle s position vector tracked via a discretized ito equation x t δ t x t v d 2 δ t b n where d d m a t v i a l a t v v t v is a dispersion tensor that has a decomposition d b b t n is an independent 2 d standard normal vector d m 8 10 5 m 2 d is molecular diffusion a t 10 3 m is transverse dispersivity and a l 5 10 3 m is longitudinal dispersivity the number of particles placed in any cell is proportional to the velocity magnitude in that cell i e a flux weighted source a plot of particle positions at elapsed times of 1 and 250 days just before arrival of first particle at the right hand side suggests the wide range of arrival times that can be expected we ran simulations using 500 5000 and 50 000 particles to judge the efficacy of density estimates the data based kernel estimates developed in this work are remarkably similar for the different particle numbers on a linear plot fig 5b two naive estimates of the epdf using 50 000 particle arrival times fig 5f show considerable noise at late time due to wide separation of late particle arrival times this effect can be counteracted by using much larger particle numbers e g labolle et al 1996 kang et al 2017 carrel et al 2018 this computational burden may be reduced in the case of particle tracking simulations for conservative solutes because they are highly parallelizable rizzo et al 2019 however non linearly reacting solutes have yet to be parallelized in three dimensions engdahl et al 2019 the naive estimators also do not have density weight before the first particle arrival because the ecdf is zero for any time before the first particle this is a commonly accepted but ultimately incorrect feature as the number of particles becomes larger or goes to infinity in the case of kernel density estimates the empirical density of early arrivals should grow in other words by calculating the density on a time grid from zero to 10 6 days the only imposed constraint is that the first arrival is non negative the early time density estimates for larger particle numbers have greater probability for early time than the 500 particle which can be identified using logarithmic time axes fig 5d and e the late time tail estimates are smoothly interpolated and very close for all three particle numbers fig 5e until some time after the final particle arrival time in the 500 particle simulation when that tail starts to drop somewhat compared to the higher particle numbers the 500 particle density is smoothly extrapolated over 50 times longer than the final arrival because of the kernel shape overall it is fair to say that the 5000 particle simulation gives similar enough results to the 50 000 particle simulation that the latter is superfluous a serious problem with the kernel density estimates is that the time grid along which the density hence kernel for subsequent iterations needs to be quite large the density is spiky enough to warrant a grid size of about 20 days or less and the last particle arrives on the order of 10 6 days so a linearly partitioned grid is a vector on the order of 50 000 to 100 000 elements making the convolutions quite slow convergence is also slow because of the high skewness so the estimates are computationally expensive because of this we looked at two alternatives 1 use of a log spaced discretization grid which was used to generate fig 5 and 2 the ad hoc correction of pedretti and fernàndez garcia 2013 which is detailed immediately 5 1 experiments with the universal adaptive bandwidth of pedretti and fernàndez garcia 2013 these authors recognize that early arrival tails of a btc are often much thinner than late arrival tails and seek to adjust the bandwidth assigned to early versus late data accordingly the authors choose to use the smaller global bandwidth h 0 at smaller t i that relatively smoothly transitions to the density adjusted value for later data there are an unlimited number of possible schemes to do this pedretti and fernàndez garcia 2013 suggest constructing the ecdf t i which is monotonically increasing with arrival time t i and constructing a variable bandwidth at each point by taking a weighted average of the single global bandwidth h 0 and the classical adaptive bandwidth 10 h 2 t i 1 ecdf t i h 0 ecdf t i h i where h 2 is their universal global bandwidth uab and h i is the adaptive bandwidth given in eq 7 pedretti and fernàndez garcia 2013 choose a standard gaussian kernel in their paper so we do the same here this leaves only the selection of the global bandwidth h 0 as a potential difference in the implementation pedretti and fernàndez garcia 2013 use a code supplied by engel et al 1994 that uses a prescribed kernel to interpolate data points to predict the value of f x d x only once prior to estimation of f x this value is used in 5 to get a value of h 0 we have shown above that there are several approaches to arriving at a value of h 0 to be used in eqs 5 and 10 for example we may use the fourier methods in appendix c we apply the uab method using fixed estimates of h 0 from the plug in method and from our fourier transform method fig 6 once again we calculate the densities on time points made from a union of two sets 1 a set of 20 000 logarithmically spaced time points between zero and 10 6 days and 2 the set of actual arrival times for the 50 000 particle simulation our fourier transform algorithm gives h 0 2370 days the plug in method of estimating 5 from engel et al 1994 used by pedretti and fernàndez garcia 2013 gives an estimated h 0 22 days table 2 the fact that these two estimates differ by two orders of magnitude is remarkable by itself and points to the potential errors of a priori h 0 estimates using a gaussian kernel with these estimates and the uab 10 gives clearly over smoothed and under smoothed density estimates fig 6a the under smoothing by the plug in value of h 0 used in the uab is shown by the failure to interpolate between the many late time arrivals due to the relatively narrow gaussian kernels there while the over smoothing of the initial fourier h 0 is shown by the relatively high weight of the near zero arrival time pdf similar discrepancies are seen in both the densities and values of h 0 from the ft and plug in methods for the other particle numbers table 2 also shown in table 2 and fig 6a c are the intermediate iterated values of h 0 that accompany the iteration of the kernels from section 5 this analysis of btc did not allow an assessment of which model had the better fit because the underlying true density was unknown the particle arrival time data have several features common to the 1 5 stable density used in section 4 4 including thin leading early time tail fat trailing tail and a high degree of skewness we applied the uab method using a standard gaussian kernel and the iterated kernel algorithm developed in this paper to data taken from a known maximally skewed 1 5 stable distribution with one caveat to eliminate one variable in both methods we use the h 0 from the plug in method engel et al 1994 as did pedretti and fernàndez garcia 2013 an ensemble of 100 data realizations each with 1000 random variables were generated to calculate the ensemble mean mise using eq 4 these values were 1 1 10 4 and 1 5 10 4 for the uab and iterated kernels approaches respectively the uab method paired with the plug in h 0 clearly does a good job in the areas around the peak fig 7 a b where the densities have the greatest weight in eq 4 similar to the btc data above the uab method succeeds in re creating the thin leading tail of the 1 5 stable density but fails to interpolate between large data values or extrapolate beyond the largest data value fig 7a b the iterated kernel method outperforms the gauss kernel method of pedretti and fernàndez garcia 2013 in both interpolating and extrapolating the large data tail fig 7c d but does tend to put too much density weight on the thin tailed small data values relative to the known real density one might also conclude that the uab method could be combined with the iterated kernel method to achieve good estimates of both the early and the late tails indeed iterating the kernel function until closure and then applying the uab does give essentially identical late tail estimates and steeper early tail estimates red curve fig 7 c although we note that the estimated mise using the uab and the iterated kernel was about 20 worse due to slightly poorer fits around the peak of course using the uab requires inspection of the data to decide whether this adjustment is appropriate it is interesting to note that the plug in estimates of h 0 use a method that evaluates the integrals in eq 5 based only on data values our method of iterating the kernel recognizes that the best estimate of the true density f x f x evolves and that f x d x might be improved using intermediate values of f x we implemented this procedure by repeatedly estimating f x d x by finite differences and trapezoidal integration the new value of h 0 was then used in the uab eq 10 until closure was reached in all cases the value of h 0 that was estimated was smaller than the one time plug in estimate and the overall mise was worse so those density estimates are not shown finally in the context of fitting models to data seeking to minimize the mise is not always the most appropriate choice any kernel density estimate constitutes a model of the data and the classical measures of model fit should apply including maximum likelihood estimation mle and entropy considerations that include parametric and computational parsimony e g akaike 1974 benson et al 2020 in particular chakraborty et al 2009 make an argument that the variance of concentration values in a binned density estimate would have a variance proportional to the estimated concentration and that those variances while dependent upon each other could be treated independently in the present case we conjecture that an mle would seek to minimize an integrated weighted squared difference of the estimated and real densities where the weights are 1 f x applying this formula to the data in this section returned machine infinities for the uab method using gaussian kernels because of the machine zeros for the estimates of the density in many places e g fig 7a b the iterated kernel returned finite values for all realizations in the ensemble with an average weighted mise of 0 02 from a standpoint of comparing some estimated btc to real data this coincides with a desire to have good interpolations of particle tracking simulations on the low concentration tails 6 conclusions and recommendations the application of an optimal iterative algorithm for kernel density estimation using an evolving data based kernel is possible with a few caveats first because the underlying data density is in general unknown a method is needed to estimate a mise minimizing global bandwidth h 0 we show that a fourier transform based method can obtain an unbiased estimate for any kernel and the exact value for a gaussian kernel this gaussian kernel starts the new algorithm by generating a first continuous estimate of the density this density is then used to construct the kernel for subsequent density estimates second creating a standard kernel based on the current iterated density estimate requires an estimate of the scale parameter of the density we use a value based on the interquartile range divided by 1 5 this value is intermediate for several known densities and works well for a range of known densities third because the final iterated version does not use a gaussian kernel the initial estimate of h 0 will necessarily be in error we show that for some common densities the fourier transform estimate of h 0 will err on the large side furthermore we show for a wide range of densities that the estimate of h 0 n γ with γ being a minimum for gaussian data and increasing systematically as the tails become heavier including exponential and power law therefore the iterative scheme allows h 0 to decrease if the algorithm fails to demonstrate convergence as expected for gaussian data the data based kernel converges rapidly to a form similar to that given by the gaussian kernel for skewed and or heavy tailed data convergence is slower and only occurs when h 0 is allowed to decrease toward its actual optimal value or range overall the data based iterated kernel gives significantly smaller ensemble mise than either 1 an iterated adaptive bandwidth gaussian kernel 2 a single pass adaptive bandwidth gaussian kernel and 3 a single pass gaussian kernel with a single global value of h 0 the new algorithm is clearly better when the non gaussian aspects of the underlying data increase including skewness and heavy exponential or power law tails when applied to particle arrival times that are heavy tailed the iterated kernel kernel and h 0 provide smooth and continuous interpolation and extrapolation of widely spaced late time arrivals even when few particles 5000 are used the iterated kernel approach does over smooth the early time data and the uab approach can be used to thin the estimated early time tail if a particle tracking model is used to compare to real data whose measurement times will not necessarily correspond to particle arrival times the methods developed here will be key to providing good interpolations between a simulation s widely spaced late time arrivals the derivation of the optimal kernel and global bandwidth solved a minimization problem for one variable h 0 based on kernel shape and the fourier transform of actual data appendix c a more difficult problem of optimizing a separate h i for each data point may be possible using cluster identification wu et al 2007 or multi gaussian kernel localization techniques sole mari and fernàndez garcia 2018 these methods would eliminate the potentially dubious taylor series based assumptions of the power law weighting scheme used in eq 7 to adjust each data point s bandwidth we leave this for a future paper credit authorship contribution statement david a benson conceptualization methodology software formal analysis writing original draft writing review editing diogo bolster formal analysis writing review editing stephen pankavich formal analysis writing original draft writing review editing michael j schmidt software writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we thank the editor and reviewers including daniel fernàndez garcia for extremely helpful comments we also thank daniele pedretti for sending source fortran code of their uab estimator this material is based upon work supported by or in part by the us army research office under contract grant number w911nf 18 1 0338 the authors were also supported by the national science foundation under awards ear 1417145 dms 1614586 dms 1911145 ear 1351625 ear 1417264 ear 1446236 and cbet 1705770 sandia national laboratories is a multi mission laboratory managed and operated by the national technology and engineering solutions of sandia l l c a wholly owned subsidiary of honeywell international inc for the does national nuclear security administration under contract de na0003525 this paper describes objective technical results and analysis any subjective views or opinions that might be expressed in the paper do not necessarily represent the views of the u s department of energy or the united states government matlab codes for generating all results in this paper are held in the public repository https github com dbenson5225 kernel density estimation appendix a mathematical background the idea of optimal global bandwidth silverman 1986 stems from using a truncated taylor series to represent the terms in the mise we begin with the fact that the expectation of the density estimate constructed from a set of independent observations is the sum of the expectations of the weights associated with each observation so that 11 e f x 1 n j 1 n e 1 h k x x j h 1 h k x ξ h f ξ d ξ similarly we compute the variance as var f x var j 1 n 1 n h k x x j h j 1 n 1 n 2 var 1 h k x x j h 1 n 1 h k x ξ h 2 f ξ d ξ 1 n 1 h k x ξ h f ξ d ξ 2 the bias at any point is b x e f x f x 1 h k x ξ h f ξ d ξ f x k z f x h z d z f x k z f x h z f x d z with this the mise is written as 12 mise b x 2 d x var f x d x k z f x h z f x 2 d z d x 1 n 1 h 2 k x ξ h 2 f ξ d ξ d x 1 h k x ξ h f ξ d ξ 2 d x the bias contribution is simply the effect of the kernel smoothing on the real density which does not depend on the sample size n the variance obviously grows smaller as n increases and not completely obviously as h increases this expression is difficult to minimize exactly although for both k and f gaussian the convolutions yield gaussians and an exact result may be computed silverman 1986 the vast majority of work done with kde is to use asymptotic expansions of certain functions with some questionable assumptions regarding their validity and application for example the density at x h z is typically approximated for h z 0 even though the goal is to find a finite h and z may be arbitrarily large in the integral still using a truncated taylor series namely f x h z f x h z f x 1 2 h 2 z 2 f x o h 3 gives b x h f x z k z d z 1 2 h 2 f x z 2 k z d z o h 3 h f x μ 1 k 1 2 h 2 f x μ 2 k o h 3 where μ n k denotes the n th moment of the kernel clearly using a zero mean i e symmetric or properly shifted kernel eliminates the first term on the rhs and indeed letting h 0 eliminates bias altogether but at the cost of increasing the noise in the estimate assuming a finite mean and proper shifting the squared bias is simply after truncation of higher order terms b x 2 d x 1 4 h 2 μ 2 k 2 f 2 d x silverman 1986 uses the bias approximation the substitution z x ξ h and another application of taylor series to reduce the local variance term to var f x 1 n h k z 2 f x z h d z 1 n f x o h 2 2 1 n h k z 2 f x h z f x o h 2 d z o n 1 1 n h f x k z 2 d z which when integrated over x yields var f x d x 1 n h k z 2 d z all told this gives a mise of mise 1 4 h 2 μ 2 k 2 f x 2 d x 1 n h k z 2 d z based on the assumptions of small h large n and n 1 h 2 all of which are likely to be bad assumptions in practice taking d mise d h and setting this expression to zero clearly gives the global estimate eq 5 in one dimension appendix b global bandwidth estimation using fourier methods in this section we implement a method based on the fourier transform that will allow us to create an unbiased estimator for the mise and minimize this function in order to select the optimal bandwidth h throughout we will use the form of the transform common to fast fourier transform routines namely g ω e 2 π i ω x g x d x for any sufficiently smooth function g x recall that the mise can be written as the sum of a bias and variance term as in 12 so that 13 mise b x 2 d x var f x d x where the bias is b x k z f x h z d z f x and the variance is given by var f x 1 n 1 h 2 k x z h 2 f z d z 1 h k x z h f z d z 2 using fourier methods we first compute the bias term taking the transform of the bias and making the change of variables y x h z we find b ω k z f x h z e 2 π i x ω d z d x f ω k z f y e 2 π i ω y h z d y d z f ω k z e 2 π i ω h z d z 1 f ω k h ω 1 f ω therefore by plancherel s theorem the bias term in eq 13 becomes b x 2 d x b ω 2 d ω k h ω 1 2 f ω 2 d ω to compute the associated variance term in eq 13 we first split it into two parts so that var f x d x 1 n i i i the first term is then i 1 h 2 k x z h 2 f z d z d x and satisfies i 1 h k y 2 f x h y d y d x 1 h k y 2 f x h y d x d y 1 h k y 2 d y f ξ d ξ 1 h k y 2 d y due to the change of variables y x z h and then ξ x h y as well as the fact that f x is a pdf to compute i i we write it as i i p x 2 d x where p x 1 h k x z h f z d z k y f x h y d y of course the transform of p x has already been identified in the computation of the integrated bias term in particular it is given by p ω k h ω f ω using plancherel s theorem as before we find i i p ω 2 d ω k h ω 2 f ω 2 d ω with this fourier representation of the bias and variance integrals we may explicitly write the mise in terms of integrals of transformed functions namely 14 mise n h 1 n h k ω 2 d ω k h ω 1 2 1 n k h ω 2 f ω 2 d ω note that we have used k y 2 d y k ω 2 d ω in the first term to write the mise depending upon k rather than k this derivation is similar to previous spectral representations of the mise chiu 1991 wu et al 2007 wu and tsai 2004 unfortunately this expression still requires knowledge of the fourier transform f ω of the unknown pdf and thus cannot be used to choose the optimal bandwidth h instead we will rely on an empirical distribution to approximate f and thus f given n observations of the distribution f x which are denoted x 1 x n we define the empirical or observed distribution f n x 1 n j 1 n δ x x j so that the corresponding transform of this function is 15 f n ω 1 n j 1 n e 2 π i x ω δ x x j d x 1 n j 1 n e 2 π i ω x j now as n we find f n f and f n f in fact we have an asymptotic estimate for the expected value of f n which implies 16 e f n ω 2 1 1 n f ω 2 1 n as n therefore by using the empirical distribution we can define and utilize an unbiased estimator for the mise for fixed n n and any h 0 define 17 ε n h 2 n k h ω d ω 1 1 n k h ω 2 2 k h ω f n ω 2 d ω 2 n h k 0 1 1 n k h ω 2 2 k h ω f n ω 2 d ω then ε n h and mise n h must attain their minimum values at the same h therefore given a sample x 1 x n of n draws from f x we define the optimal bandwidth by h ε h 0 arg min ε n h computationally approximating the global bandwidth using this value of h ε is instrumental to the algorithm proposed in section 2 finally we justify the claim that ε n h is an unbiased estimator of the mise we first note that by the fourier inversion property we have 1 h k 0 1 h k ω e 0 d ω k h ω d ω then taking the expectation of ε n h and inserting the convergence result 16 we find e ε n h 2 n k h ω d ω 1 1 n k h ω 2 2 k h ω e f n ω 2 d ω 2 n k h ω d ω 1 1 n k h ω 2 2 k h ω 1 1 n f ω 2 1 n d ω 1 1 n k h ω 2 1 1 n f ω 2 1 n d ω 2 1 1 n k h ω f ω 2 d ω 1 1 n k h ω 2 1 1 n f ω 2 1 n 2 k h ω f ω 2 d ω 1 1 n k h ω 2 2 k h ω 1 n k h ω 2 f ω 2 1 n k h ω 2 d ω 1 1 n k h ω 1 2 1 n k h ω 2 1 f ω 2 d ω 1 n h k ω 2 d ω 1 1 n mise n h f ω 2 d ω 1 1 n mise n h f x 2 d x this implies that modulo a shifting and scaling factor that are both independent of h the expectation of our estimator is exactly mise n h additionally it becomes clear that this function must attain its minimum at the same value of h as mise n h and modulo a shift we have e ε n h mise n h as n appendix c numerical bandwidth estimation next we outline a numerical approach based on our use of the fourier transform implementing the iterative algorithm of section 3 to compute the approximate distributon let us assume that the algorithm converges then due to the relationship between successive iterates and the previous kernel namely f ℓ 1 x and f ℓ x k ℓ x the final density and the kernel must converge to the same function as ℓ while the bandwidth must also converge to some value h 0 then denoting the converged iteratively estimated kernel based on data by k x this function must satisfy the interesting self similar property using eq 1 k x 1 n j 1 n 1 h k x x j h additionally its fourier transform then satisfies the relationship k ω 1 n h e 2 π i ω x j 1 n k x x j h d x 1 n h j 1 n e 2 π i ω z h x j k z h d z 1 n j 1 n e 2 π i ω x j e 2 π i ω z h k z d z 1 n j 1 n e 2 π i ω x j k h ω f n ω k h ω due to 15 with this and the asymptotic approximation 16 we have 18 k ω f ω k h ω o 1 n for n suitably large evaluating the mise 14 with the approximation k x k x and h h yields mise n h k h ω 1 2 f ω 2 d ω o 1 n for n suitably large finally expanding this expression and using the relationship 18 satisfied by the fourier transforms of the limiting kernel and unknown pdf we find mise n h k h ω 2 f ω 2 2 k h ω f ω 2 f ω 2 d ω o 1 n k ω 2 2 k ω f ω f ω 2 d ω o 1 n k ω f ω 2 d ω o 1 n therefore we see that for large n the iterative algorithm guarantees that the mise is minimized precisely when the kernel k x converges to the unknown distribution in the l 2 sense this suggests that when the algorithm converges as ℓ it must converge to the unknown pdf f x because an unbiased estimator for the mise is minimized at every step furthermore our analysis now demonstrates the appropriate range of taylor series estimates of h because the exact result can be derived from the fourier transform assume a gaussian for the kernel and also assume a priori that the underlying data are gaussian with zero mean and variance σ 2 so that k ω exp 2 π ω 2 2 and f ω exp σ 2 2 π ω 2 2 the first integral in eq 14 can be computed in several ways but is easily performed by recognizing the form of a gaussian so that 1 n h e 2 π ω 2 d ω 1 n h 1 4 π 1 2 π 8 π 2 e ω 2 2 8 π 2 d ω 1 2 π n h owing to the fact that the last integral is of a density in ω similarly the second integral in eq 14 is 1 1 n k 2 h ω 2 k h ω f 2 ω d ω 1 1 n e 2 π h ω 2 2 e 2 π h ω 2 2 e σ 2 2 π ω 2 d ω 1 1 n e σ 2 h 2 2 π ω 2 2 e σ 2 h 2 2 2 π ω 2 d ω 1 1 n 2 π h 2 σ 2 1 π σ 2 h 2 2 where we have rearranged as before to make gaussian densities in ω for each term therefore the resulting quantity to be minimized silverman 1986 is now 19 mise n h 1 2 π n h 1 2 π σ 2 h 2 1 2 n π σ 2 h 2 1 π σ 2 h 2 2 it suffices to approximate the h 0 that minimizes mise n h to any numerical tolerance by taking d mise n h d h setting it to zero and finding the root of the resulting equation as expected the estimate of h 0 based on taylor series is worse for smaller data sets i e n 100 but as n grows large the taylor series solution converges to the exact solution fig 8 however it is important to note that these quantities are the optimal bandwidth when both the kernel and the underlying data density are known to be gaussian if the underlying density is unknown then the data are used to construct the quantity to be minimized ε n h in eq 17 to see how this differs we can imagine that perfectly gaussian data is generated then eq 17 evaluates to 20 ε n h 2 2 π n h 1 2 π σ 2 h 2 1 2 n π σ 2 h 2 1 π σ 2 h 2 2 which has a root approximately 4 1 5 1 32 larger for large n fig 8 the fact that data are imperfect means that the global bandwidth must be about 32 to 70 larger depending on n to achieve additional smoothing when compared to a completely perfect realization of data for a specific example we set n 1000 and σ 2 1 which produces an exact optimal global bandwidth for imperfect data using ε n h in eq 17 of h 0 0 341 fig 9 whereas the estimate based on taylor expansions gives h 0 1 06 σ n 1 5 0 266 it is important to see how well a numerical estimate of the data density gives an estimate of h 0 rather than simply assuming a gaussian density function we may now compare the values of h that are estimated using the fourier transformed data to form an estimate of the density function i e using eq 15 in eq 17 instead of assuming the gaussian form here we show the results for 50 independent runs in which 1000 iid gaussian data are generated and the experimental curve generated and h taken at the curve minimum black curves in fig 9 while there is a large vertical spread in the curves the locations of the minima are fairly tightly constrained the mean of 50 values of h 0 is 0 335 compared to the exact value of 0 341 and the estimated h 0 have a standard deviation of 0 0238 several other characteristic functions fourier transforms of pdfs are easily integrated and illustrate the effect of data distribution on estimation of h 0 for example the standard cauchy density defined by f x σ π σ 2 x 2 has both divergent variance and mean and its associated fourier transform is given by f ω exp 2 π σ ω note that the scale parameter σ commonly used for stable densities is not the standard deviation which is infinite assuming that the kernel was also a perfect copy of the data density so that k ω exp 2 π ω the mise becomes up to an additive factor independent of h mise n h 1 2 π n h 1 1 n 1 2 π h σ 1 π 1 2 h σ therefore calculating the minimum of the mise 14 means solving the root of 21 d mise n d h 1 2 π n 1 1 n 1 2 π 1 σ h 2 2 π 1 2 σ h 2 these values of h 0 n are significantly smaller than those found for gaussian data fig 8 and also decline for large n approximately like n 1 3 this suggests a numerical procedure for simultaneous estimation of the data density and the global bandwidth the ft estimate of h 0 based on gaussian data is the largest of the estimates fig 8 so we begin with that value if the iterated kernel based on the estimated density and using this h 0 fails to converge then we reduce h 0 systematically down to a minimum given by the cauchy h 0 in this procedure the specifics of the data distribution need not be known simply start with an assumption of gaussian like smoothness and data density but allow for cauchy like sparcity of data i e few very large data we may also consider the laplace or double exponential density defined by f x 1 2 σ exp x σ which has mean zero and variance 2 σ 2 but does not possess a continuous derivative at x 0 the fourier transform of this function is given by f ω 1 1 4 π 2 σ 2 ω 2 if the kernel is similarly distributed so that k ω 1 1 4 π 2 ω 2 then the mise becomes mise n h 1 4 n h 1 1 n h 2 3 h σ σ 2 4 h σ 3 2 h σ 2 h σ 2 as before the minimum of the mise 14 can be computed by finding the root of the derivative of this expression namely 22 d mise n d h 1 4 n 1 σ h 2 3 4 1 4 n h σ h σ 1 3 4 1 4 n h σ h σ 1 2 the resulting values of h 0 n are again significantly smaller than those found for gaussian data fig 8 and also decline for large n approximately like n 1 4 finally we consider the family of stable distributions whose density may be defined by f x 1 2 π e c k α 1 i β s g n k tan π α 2 e i k x d k where 0 α 2 is the stability parameter 1 β 1 is a skewness parameter and c is the scale parameter the fourier transform is given by f ω e 2 π c ω α 1 i β s g n ω tan π α 2 as before if the kernel is similarly distributed so that k ω e 2 π ω α 1 i β s g n ω tan π α 2 then the mise becomes mise n h c 2 1 α n h 1 1 n 2 h α 2 c α 1 α 2 h α 2 c α 1 α where c e 2 π ω α 1 i β s g n ω tan π α 2 d ω as before this can be minimized and similar to the distributions explored so far we find that there is a power law decline n γ where γ depends on α as depicted in fig 10 note that the magnitude of mise depends on α and β through the constant c but that this does not impact the minimized value we also note that these calculations may be made for other densities but are not shown additionally some of the integrations must be performed numerically as it may be the case that no closed form expression for the antiderivative exists 
301,traditional interpolation techniques for particle tracking include binning and convolutional formulas that use pre determined i e closed form parameteric kernels in many instances the particles are introduced as point sources in time and space so the cloud of particles either in space or time is a discrete representation of the green s function of an underlying pde as such each particle is a sample from the green s function therefore each particle should be distributed according to the green s function in short the kernel of a convolutional interpolation of the particle sample cloud should be a replica of the cloud itself this idea gives rise to an iterative method by which the form of the kernel may be discerned in the process of interpolating the green s function when the green s function is a density this method is broadly applicable to interpolating a kernel density estimate based on random data drawn from a single distribution we formulate and construct the algorithm and demonstrate its ability to perform kernel density estimation of skewed and or heavy tailed data including breakthrough curves keywords particle methods kernel density estimation machine learning nonparametric kernel 1 introduction in many applications discrete samples of a continuous and potentially complex random process are generated as output even though a continuous solution is desired some examples are given by particle tracking of passive solute transport e g fernàndez garcia and sanchez vila 2011 pedretti and fernàndez garcia 2013 siirila woodburn et al 2015 carrel et al 2018 reactive particle transport e g ding et al 2012 2017 schmidt et al 2017 sole mari et al 2017 sole mari et al 2019 sole mari and fernàndez garcia 2018 benson et al 2019 perez et al 2019 engdahl et al 2017 2019 and monte carlo and bayesian simulation e g taverniers et al 2020 in short many of the quantities used by hydrologists are probability density functions that are constructed by users even thought there is no concrete and accepted methodology for their construction a long history of statistical estimation has sought to best fit some continuous density function to a sequence of random samples including maximum likelihood estimation brockwell and davis 2016 and kernel density estimation silverman 1986 the former assumes a functional density form and estimates its parameters while the latter fits a continuous function to discrete data tests of functional fits or other statistical properties may be conducted later in hydrology and many other sciences the underlying processes being simulated may be sufficiently uncertain that a functional form for the density function cannot be assumed and kernel density estimation is preferred given a true underlying pdf f x kernel density estimation is based on the convolution like interpolation or extrapolation of discrete random data x 1 x 2 x n with some kernel function k x producing the estimated pdf 1 f x 1 i 1 n w i j 1 n w j h j k x x j h j where w j are weights associated with data points x j which could be prior concentrations that come from binning h j are bandwidths associated with the kernel applied at each data point and k is some pre determined non negative function with the requirement k x d x 1 i e k is a pdf for random samples the weights are equal constants that cancel from expression eq 1 resulting in a factor of 1 n the common forms of k are relatively simple e g triangles or standard gaussians and yield estimates of f x with different properties such as regularity i e number of derivatives or compact support kernels that are symmetric around x 0 are most commonly used but certainly not always see hirukawa 2018 inasmuch as the eventual form of f x including skewness or heavy tails are unknown a priori it is well known that a pre chosen kernel such as a standard gaussian does not perform well if all of the bandwidths are chosen to be the same size silverman 1986 where data is more dense the kernel bandwidths must be made smaller this has led to adaptive bandwidths that are adjusted based on the apparent or estimated density at the data points higher estimated density values at data points are given smaller bandwidths but one may ask should the functional form of the kernel also be adjusted based on the estimated density we suggest and provide evidence in appendix c that the optimal kernel should be the same shape as the underlying true density which is best estimated by the interpolated density but clearly the estimated density changes if the kernel shape changes therefore an iterative procedure is required we define this procedure in section 3 after a brief review of kernel density estimation in section 2 a series of examples are given in sections 4 and 5 and we conclude in section 6 2 classical bandwidth selection intuitively one would like to choose a bandwidth as small as possible because the convolution adds the variance of the kernels to the data itself on the other hand as h 0 the kernels become delta functions and continuity of f x disappears additionally the choice of h i will depend strongly on both the eventual shape of f x and the availability of random samples in any interval x x δ x this has led to expressions that balance the bias and variance of the estimates silverman 1986 that we review here and re derive in appendix a a common place to start is to minimize the mean integrated squared error mise between the estimated and unknown real densities given by 2 mise e f x f x 2 d x taking the expectation inside the integral and realizing that the mean squared error of an estimate is composed of squared bias and variance terms one finds e f f 2 e f f 2 e f e f 2 which gives a target functional for minimization typically a truncated taylor series is used to derive asymptotic h 0 nh expressions for the bias and variance that depend on the properties of the kernel and underlying density silverman 1986 this process appendix a results in approximations for the bias 3 b x bias f x e f x f x h 2 2 f x μ 2 k o h 3 and variance 4 var f x e f e f 2 1 n h f x k 2 x d x o n h 2 all other things held equal letting h 0 minimizes bias but variance grows without bound i e accuracy increases but smoothness decreases while letting h grow large decreases the variance of estimates but accuracy is sacrificed minimizing the sum gives a value for the optimal global bandwidth 5 h 0 d k 2 x d x n μ 2 k 2 f x 2 d x 1 d 4 where d is the number of dimensions of the random variable d 1 herein notice that a finite second moment μ 2 k is necessary to use this method in the estimation of the optimal bandwidth we remove that requirement herein appendices a c without any information at all it is common to assume gaussian f x and gaussian kernels in which case a constant global bandwidth is used with size 6 h 0 1 06 n 1 5 σ where σ is the sample variance greater data density means smaller bandwidth until as n h 0 0 because this estimation of finite h 0 is based in part on an assumption of h 0 0 we might expect significant error in any estimate of the global bandwidth value using eq 5 indeed an exact value of h 0 can instead be derived using the fourier transform appendix b and we show that the result in eq 6 can be significantly erroneous furthermore it is largely recognized e g silverman 1986 that the local data density is a better indicator of bandwidths that should be uniquely defined at each data point in regions where data density is smaller the bandwidth should be greater there are several methods used to estimate local data density e g silverman 1986 wu et al 2007 sole mari and fernàndez garcia 2018 for example silverman 1986 shows that for large n the local data density can be approximated by the value of the estimated pdf so that an adaptive bandwidth can be estimated by 7 h i h 0 λ i h 0 f x i g ξ where the tilde indicates some intermediate estimate of the density and the normalization factor g is the geometric mean of estimated density values namely 8 g exp 1 n i 1 n ln f x i the exponent 0 ξ 1 is an empirical weighting factor shown to be 0 5 under ideal conditions abramson 1982 in a novel way pedretti and fernàndez garcia 2013 investigated the use of the adaptive kernel methods eqs 5 7 and 8 for interpolating breakthrough curves btcs for simulated push pull single well tests with trapping in relatively immobile low velocity zones these btcs are noteworthy for their thin early tails and fat late tails or rapid steep early breakthrough and delayed power law decline of concentration importantly pedretti and fernàndez garcia 2013 found that adjusting the bandwidth based only on particle density tended to overly broaden the early btcs in order to more properly represent the late tail pedretti and fernàndez garcia 2013 then imposed a restriction on broadening the kernel bandwidth based on whether particles concentrations in eq 1 occurred early or late in the btc this of course means that the user must decide how the bandwidths must be adjusted but this is simply a side effect of choosing a priori a non physical and symmetric kernel if each particle were treated as a single realization of the green s function then its highly skewed kernel would transfer little mass to earlier portions of the btc and no adjustment may be needed we investigate that possibility here 3 iterative algorithm we show appendix c that asymptotically as n to minimize the mise the kernel applied to each random sample should be a scaled version of the underlying true density itself this suggests that for a reasonably large number of data n the kernel k should be made functionally similar to the estimated density f as this is the best representation of the true density f of course the shape of the density is not known a priori so the shape of the kernel must be learned during the estimation process we seek to find f to best approximate f and we find f through successive intermediate estimates that we call f our proposed algorithm discovers the kernel shape and size recursively according to the following steps 1 build an initial candidate f 0 x using constant bandwidth h 0 and standard gaussian kernel k x 2 π 1 2 exp x 2 2 in eq 1 2 use f 0 x to interpolate values at data points f x i 3 use the values f x i in eq 7 to estimate adaptive bandwidths h i for the gaussian kernel and re estimate f 1 x this would end classical estimation set counter ℓ 1 4 use f ℓ x as the new kernel k ℓ x f ℓ x 5 adjust kernel k ℓ to have zero mean and unit width 6 use f ℓ x to interpolate values at data points f x i 7 use the values f x i in eq 7 to estimate adaptive bandwidths h i for the new kernel k ℓ x 8 use new kernel function k ℓ and bandwidths h i to estimate f ℓ 1 x using eq 1 9 return to step 4 until desired closure between f ℓ 1 x and f ℓ x upon closure f ℓ 1 x is the best estimate of f x the potentially tricky parts of the algorithm are associated with steps 1 4 5 and 7 for step 1 the distributional qualities of the data are unknown so we use a fourier transform algorithm to estimate the data density function see eq 17 in the appendix by assuming a gaussian kernel the initial h 0 can be easily estimated for step 4 it is important to use a numerical domain for x that is wider than the data values so that the kernel may extrapolate sufficiently before the smallest data point and after the largest furthermore for widely spaced and sparse data the density of calculated points in x must also be chosen to provide sufficient resolution for step 5 it is not always clear that the mean and standard deviation exist or are the proper scaling metrics for the iterated kernel a simple example is a stable density which may have diverging moments and also rescale differently from say a gaussian density here we suggest using the interquartile range of the data and the kernel for a reasonably close and reliable estimate of the scale of many different density functions for step 7 we are now using a kernel that is thought to resemble the underlying density so using k f or f in eqs 3 5 and 7 will give different values of h 0 etc more on these points is provided below in order to find a standard kernel from the previous iteration s density estimate f the kernel must have zero mean so that the subsequent addition of kernels has the same mean as the data the width of the kernel should be standardized such as normalizing by a scale factor equal to the standard deviation of the data or central second moment of f however many densities have diverging second moments so a robust method must be found for situations in which the underlying density is unknown a quick survey of the interquartile range iqr and the scale factor of many densities shows reasonably similar relationships for finite variance distributions we find for example the gaussian has σ iqr 1 35 the exponential σ iqr 1 1 the laplace σ iqr 0 98 infinite variance distributions with closed form distribution functions characterized by scale parameter σ include the symmetric cauchy with σ iqr 2 and the maximally skewed 1 2 stable lévy density with σ iqr 9 using matlab s routine for calculating the cdf of a stable law we find that for a maximally skewed 1 5 stable σ iqr 2 13 with the exception of the lévy density it is a reasonable approximation to say that the width of the density function may be standardized using σ iqr 1 5 therefore in the following to arrive at a standard density from the data kernel we numerically integrate the intermediate density f ℓ 1 to find iqr x 0 75 x 0 25 where x z min x j δ x ℓ 1 n f ℓ 1 x j z and simply shift and rescale the experimental density by its first moment m 1 and a generic multiple of the iqr so that k ℓ x 1 iqr 1 5 f ℓ 1 x m 1 iqr 1 5 as noted before the kernel is allowed to change after each iteration and this kernel is checked against the previous iteration s kernel iteration is terminated when the kernel converges and the difference between the density estimated with those kernels in successive approximations is sufficiently small here we choose to discontinue the algorithm when the l 2 difference between successive iterations is less then 10 9 where 9 l 2 δ x i 1 n f ℓ x f ℓ 1 x 2 if the l 2 difference is found to increase between iterations this indicates too large a global bandwidth h 0 assuming that one starts with a conservatively large value from the fourier transform procedure the too large value of h 0 makes the kernel itself too smooth and also gives it too large a scale based on the iqr so convergence will not occur in practice there may be a range of h 0 values that leads to convergence based on some numerical threshold of the l 2 norm so some care needs to be used when adjusting h 0 when the initial h 0 estimate is far from the correct range one may decrease h 0 by a factor of 0 9 as the algorithm gets nearer the correct kernel and h 0 the minimum of eq 9 gets progressively smaller and there is a danger of overshooting the optimal h 0 so the algorithm slows the adjustment of h 0 by incrementally moving the factor toward unity after h 0 is adjusted iteration resumes in practice if the minimum l 2 eq 9 reached for a given h 0 is on the order of 10 4 the value of h 0 is reasonably far from the optimal and may be decreased by about 10 each order of magnitude improvement in the l 2 convergence is accompanied by moving the factor 2 closer to unity we also add that other h 0 estimators can be used that may underestimate the optimal h 0 and so the same procedure to adjust the value is done in reverse say starting with an adjustment factor of 1 1 that decreases toward unity based on minimum l 2 norm seen with any h 0 value examples can be seen in the matlab code provided at https github com dbenson5225 kernel density estimation another important consideration is the construction of the set of points at which the density is calculated through some experimentation we find that an optimal set of points is made from a union of 1 a set of appropriately spaced points between a desired minimum and maximum that is larger than the measured data range and 2 the set of actual random data values x i the first set is important so that sufficient interpolation between widely spaced data is made the second set is sometimes important so that the weights are accurately calculated within eq 7 we experimented with neglecting the second set and simply interpolating the density at points x i from points in the first set but for spiky densities the results depend too much on the density of points that are specified if the number of points at which the density is calculated becomes large it is a simple matter to parallelize a large part of the procedure because the calculation of f x in eq 1 is independent for any x value 4 examples we investigate the iterative algorithm versus classical assumed gaussian kernel methods for four types of data 1 symmetric and thin tailed 2 maximally skewed and exponentially tailed 3 symmetric and heavy power law tailed and 4 maximally skewed and heavy power law tailed the last is chosen because btc data are strictly positive and often observed to fall off like x 1 α where α is on the order of 0 5 we also investigate how well the estimators perform over a large realization of random samples and a range of population sizes n 100 1000 10 000 inasmuch as large particle numbers and random arrival times are typically used in each case we use estimates of the mise to measure bias and variance of the estimated density versus a known density on a regular grid in x a numerical estimate of the mise is given by an ensemble mean of the l 2 norm namely mise δ x m m 1 m j 1 n f m x j f x j 2 for a set of m realizations of data with an underlying density f and the corresponding estimates of the density f m on a common grid of estimation points x j with spacing δ x for each of the examples we generate m 100 independent realizations of data from known distributions in order to estimate the densities and resulting mise table 1 4 1 gaussian data we start with gaussian random variables in which the data kernel should nearly converge to an a priori gaussian kernel because the underlying data that builds the data kernel is gaussian indeed for a large number of data points 1000 the iterated kde for the data based and gaussian based kernel are nearly identical even in the extreme tails fig 1 this example shows the robust nature of the estimation inasmuch as the gaussian kernel uses the exact width of the kernel σ 1 while the iterated kernel uses a general width estimate of iqr 1 5 in actuality the width of a gaussian is σ iqr 1 34 it is worth noting that closure to the final kernel usually takes between 5 and 7 iterations furthermore because the value of h 0 is not set exactly which would require identifying the data as gaussian before interpolating the iterated kernels have similar magnitudes of mise as single pass adaptive gaussian kernels and convolution with a single value of h 0 table 1 4 2 exponential data next we use a shifted exponential the arbitrary shift is added to ensure functionality of the code with density function f x 1 σ exp x μ 1 σ σ for x μ 1 σ 0 else this density has arbitrary mean μ and variance 1 σ 2 in the plots that follow we set μ 0 and σ 1 for this skewed density it becomes clear that a symmetric gaussian in this case kernel is not an effective interpolant fig 2 while silverman 1986 suggests using a skewed say lognormal kernel for this kind of data our method does not rely on interpretation and user intervention for kernel selection and while a gaussian kernel is not particularly useful for this kind of data the iterated data based kernel typically has mise of about half that of the gaussian table 1 because the underlying optimal h 0 is much smaller than that estimated from assuming a gaussian kernel the iterations do not converge and in fact tend to diverge until h 0 decreases several times requiring on the order of 30 or more iterations for 1000 data points 4 3 cauchy data heavy tailed data present a problem for kernel density estimation because of the extremes that may accompany the data this leads to very wide spacing between extreme data points and difficulty interpolating the density here this also means that the x discretization of the kernel must use a large number of points in order to represent the near origin spikiness of the density as well as the very long range the existence of one or two super extreme values can lead to numerical problems in our 100 realization ensemble of 1000 cauchy data points two realizations failed to converge in 100 iterations with the data based kernel because of data values in the 50 000 range a typical realization shows that the converged data based kernel tends to both interpolate between and extrapolate beyond extreme values better than the gaussian kernel but still represents the fine scale near the origin where most of the data reside fig 3 in the ensemble the mise estimated for the data based kernel is substantially less than for the gaussian kernels table 1 4 4 maximally skewed α stable data stable random variables rv are characterized among many other ways as those to which sums of iid random variables converge samorodnitsky and taqqu 1994 sums of finite variance rvs converge to are in the domain of attraction of a gaussian which is itself a stable rv when for some constant 0 α 2 only those moments of order α and greater are infinite such as for pareto power law distributed rvs then those rvs are in the domain of attraction of a α stable these rvs arise in hydrology quite naturally because they describe waiting times that a particle might take when trapped in a sequence of fractal immobile zones see schumer et al 2003 benson et al 2013 depending on the skewness parameter one or both of the tails of a α stable density function decay like x 1 α therefore all moments of order α and greater diverge the density is only expressible in closed form for a few instances but most statistical packages will readily calculate the density to any desired tolerance and generate sequences of the random variable here we choose a maximally skewed standard 1 5 stable for analysis using the parameterization in the matlab statistics package also called the 0 parameterization in nolan 2018 the ensemble mise for the data based kernel is about 1 5 that of the iterated gaussian kernel suggesting that both the heavy tailed and skewed nature of this example is especially well suited to our proposed method fig 4 5 particle breakthrough concentration data the creation of breakthrough curves btc from particle tracking simulations is a tricky proposition classically histograms are used which means manually choosing either constant or variable bin sizes and locations the variance of the estimated density is inversely proportional to bin size total number of particles and the estimated concentration chakraborty et al 2009 and the histogram based density is discontinuous and may frequently be zero when particle arrival times are widely separated especially in the late time tail the zeros make comparison to non zero data difficult e g using weighted least squares so several methods are typically used to create a non zero pdf interpolation the first set of constructions of arrival time pdfs which we will call naive estimators is based on simple linear interpolation of arrival times for example one may construct by several means an empirical cumulative distribution function ecdf that is strictly increasing and then make a non zero empirical pdf using finite differences on the ecdf in particular order the particle arrival times of n particles t 1 t 2 t n and at each point the ecdf t i i n then the empirical pdf is epdf t i 1 t i 2 ecdf t i 1 ecdf t i t i 1 t i i 1 n 1 the ecdf can also use a regularly spaced time grid and count numbers of particles arriving between grid points i e bins and empty bins are neglected once again giving a strictly increasing ecdf in this section we compare these two naive estimators to the iterative kernel based techniques developed in this paper along with prior deterministic kernel based methods fernàndez garcia and sanchez vila 2011 pedretti and fernàndez garcia 2013 for particle arrival times we solved for the hydraulic head h in the steady state groundwater flow equation k h 0 in 2 d using finite differences on a square 128 128 m grid with constant grid discretization of 1 1 m fig 5a the hydraulic conductivity k is a scalar log normal random variable with a mean of ln k 1 standard deviation of ln k 4 and an exponential autocorrelation function for ln k with correlation length of 5 m the left and right boundaries x 0 and x 128 are dirichlet with h 1 and h 0 respectively the top and bottom boundaries y 0 and y 128 are neumann with h y 0 the resulting velocities take the solved h field and apply v k h ϕ where ϕ is assumed a constant porosity of unity once again using finite differences these velocities vary in magnitude from about 3 10 7 to 2 1 m d fig 5a particles are placed in a line near the left boundary and each particle s position vector tracked via a discretized ito equation x t δ t x t v d 2 δ t b n where d d m a t v i a l a t v v t v is a dispersion tensor that has a decomposition d b b t n is an independent 2 d standard normal vector d m 8 10 5 m 2 d is molecular diffusion a t 10 3 m is transverse dispersivity and a l 5 10 3 m is longitudinal dispersivity the number of particles placed in any cell is proportional to the velocity magnitude in that cell i e a flux weighted source a plot of particle positions at elapsed times of 1 and 250 days just before arrival of first particle at the right hand side suggests the wide range of arrival times that can be expected we ran simulations using 500 5000 and 50 000 particles to judge the efficacy of density estimates the data based kernel estimates developed in this work are remarkably similar for the different particle numbers on a linear plot fig 5b two naive estimates of the epdf using 50 000 particle arrival times fig 5f show considerable noise at late time due to wide separation of late particle arrival times this effect can be counteracted by using much larger particle numbers e g labolle et al 1996 kang et al 2017 carrel et al 2018 this computational burden may be reduced in the case of particle tracking simulations for conservative solutes because they are highly parallelizable rizzo et al 2019 however non linearly reacting solutes have yet to be parallelized in three dimensions engdahl et al 2019 the naive estimators also do not have density weight before the first particle arrival because the ecdf is zero for any time before the first particle this is a commonly accepted but ultimately incorrect feature as the number of particles becomes larger or goes to infinity in the case of kernel density estimates the empirical density of early arrivals should grow in other words by calculating the density on a time grid from zero to 10 6 days the only imposed constraint is that the first arrival is non negative the early time density estimates for larger particle numbers have greater probability for early time than the 500 particle which can be identified using logarithmic time axes fig 5d and e the late time tail estimates are smoothly interpolated and very close for all three particle numbers fig 5e until some time after the final particle arrival time in the 500 particle simulation when that tail starts to drop somewhat compared to the higher particle numbers the 500 particle density is smoothly extrapolated over 50 times longer than the final arrival because of the kernel shape overall it is fair to say that the 5000 particle simulation gives similar enough results to the 50 000 particle simulation that the latter is superfluous a serious problem with the kernel density estimates is that the time grid along which the density hence kernel for subsequent iterations needs to be quite large the density is spiky enough to warrant a grid size of about 20 days or less and the last particle arrives on the order of 10 6 days so a linearly partitioned grid is a vector on the order of 50 000 to 100 000 elements making the convolutions quite slow convergence is also slow because of the high skewness so the estimates are computationally expensive because of this we looked at two alternatives 1 use of a log spaced discretization grid which was used to generate fig 5 and 2 the ad hoc correction of pedretti and fernàndez garcia 2013 which is detailed immediately 5 1 experiments with the universal adaptive bandwidth of pedretti and fernàndez garcia 2013 these authors recognize that early arrival tails of a btc are often much thinner than late arrival tails and seek to adjust the bandwidth assigned to early versus late data accordingly the authors choose to use the smaller global bandwidth h 0 at smaller t i that relatively smoothly transitions to the density adjusted value for later data there are an unlimited number of possible schemes to do this pedretti and fernàndez garcia 2013 suggest constructing the ecdf t i which is monotonically increasing with arrival time t i and constructing a variable bandwidth at each point by taking a weighted average of the single global bandwidth h 0 and the classical adaptive bandwidth 10 h 2 t i 1 ecdf t i h 0 ecdf t i h i where h 2 is their universal global bandwidth uab and h i is the adaptive bandwidth given in eq 7 pedretti and fernàndez garcia 2013 choose a standard gaussian kernel in their paper so we do the same here this leaves only the selection of the global bandwidth h 0 as a potential difference in the implementation pedretti and fernàndez garcia 2013 use a code supplied by engel et al 1994 that uses a prescribed kernel to interpolate data points to predict the value of f x d x only once prior to estimation of f x this value is used in 5 to get a value of h 0 we have shown above that there are several approaches to arriving at a value of h 0 to be used in eqs 5 and 10 for example we may use the fourier methods in appendix c we apply the uab method using fixed estimates of h 0 from the plug in method and from our fourier transform method fig 6 once again we calculate the densities on time points made from a union of two sets 1 a set of 20 000 logarithmically spaced time points between zero and 10 6 days and 2 the set of actual arrival times for the 50 000 particle simulation our fourier transform algorithm gives h 0 2370 days the plug in method of estimating 5 from engel et al 1994 used by pedretti and fernàndez garcia 2013 gives an estimated h 0 22 days table 2 the fact that these two estimates differ by two orders of magnitude is remarkable by itself and points to the potential errors of a priori h 0 estimates using a gaussian kernel with these estimates and the uab 10 gives clearly over smoothed and under smoothed density estimates fig 6a the under smoothing by the plug in value of h 0 used in the uab is shown by the failure to interpolate between the many late time arrivals due to the relatively narrow gaussian kernels there while the over smoothing of the initial fourier h 0 is shown by the relatively high weight of the near zero arrival time pdf similar discrepancies are seen in both the densities and values of h 0 from the ft and plug in methods for the other particle numbers table 2 also shown in table 2 and fig 6a c are the intermediate iterated values of h 0 that accompany the iteration of the kernels from section 5 this analysis of btc did not allow an assessment of which model had the better fit because the underlying true density was unknown the particle arrival time data have several features common to the 1 5 stable density used in section 4 4 including thin leading early time tail fat trailing tail and a high degree of skewness we applied the uab method using a standard gaussian kernel and the iterated kernel algorithm developed in this paper to data taken from a known maximally skewed 1 5 stable distribution with one caveat to eliminate one variable in both methods we use the h 0 from the plug in method engel et al 1994 as did pedretti and fernàndez garcia 2013 an ensemble of 100 data realizations each with 1000 random variables were generated to calculate the ensemble mean mise using eq 4 these values were 1 1 10 4 and 1 5 10 4 for the uab and iterated kernels approaches respectively the uab method paired with the plug in h 0 clearly does a good job in the areas around the peak fig 7 a b where the densities have the greatest weight in eq 4 similar to the btc data above the uab method succeeds in re creating the thin leading tail of the 1 5 stable density but fails to interpolate between large data values or extrapolate beyond the largest data value fig 7a b the iterated kernel method outperforms the gauss kernel method of pedretti and fernàndez garcia 2013 in both interpolating and extrapolating the large data tail fig 7c d but does tend to put too much density weight on the thin tailed small data values relative to the known real density one might also conclude that the uab method could be combined with the iterated kernel method to achieve good estimates of both the early and the late tails indeed iterating the kernel function until closure and then applying the uab does give essentially identical late tail estimates and steeper early tail estimates red curve fig 7 c although we note that the estimated mise using the uab and the iterated kernel was about 20 worse due to slightly poorer fits around the peak of course using the uab requires inspection of the data to decide whether this adjustment is appropriate it is interesting to note that the plug in estimates of h 0 use a method that evaluates the integrals in eq 5 based only on data values our method of iterating the kernel recognizes that the best estimate of the true density f x f x evolves and that f x d x might be improved using intermediate values of f x we implemented this procedure by repeatedly estimating f x d x by finite differences and trapezoidal integration the new value of h 0 was then used in the uab eq 10 until closure was reached in all cases the value of h 0 that was estimated was smaller than the one time plug in estimate and the overall mise was worse so those density estimates are not shown finally in the context of fitting models to data seeking to minimize the mise is not always the most appropriate choice any kernel density estimate constitutes a model of the data and the classical measures of model fit should apply including maximum likelihood estimation mle and entropy considerations that include parametric and computational parsimony e g akaike 1974 benson et al 2020 in particular chakraborty et al 2009 make an argument that the variance of concentration values in a binned density estimate would have a variance proportional to the estimated concentration and that those variances while dependent upon each other could be treated independently in the present case we conjecture that an mle would seek to minimize an integrated weighted squared difference of the estimated and real densities where the weights are 1 f x applying this formula to the data in this section returned machine infinities for the uab method using gaussian kernels because of the machine zeros for the estimates of the density in many places e g fig 7a b the iterated kernel returned finite values for all realizations in the ensemble with an average weighted mise of 0 02 from a standpoint of comparing some estimated btc to real data this coincides with a desire to have good interpolations of particle tracking simulations on the low concentration tails 6 conclusions and recommendations the application of an optimal iterative algorithm for kernel density estimation using an evolving data based kernel is possible with a few caveats first because the underlying data density is in general unknown a method is needed to estimate a mise minimizing global bandwidth h 0 we show that a fourier transform based method can obtain an unbiased estimate for any kernel and the exact value for a gaussian kernel this gaussian kernel starts the new algorithm by generating a first continuous estimate of the density this density is then used to construct the kernel for subsequent density estimates second creating a standard kernel based on the current iterated density estimate requires an estimate of the scale parameter of the density we use a value based on the interquartile range divided by 1 5 this value is intermediate for several known densities and works well for a range of known densities third because the final iterated version does not use a gaussian kernel the initial estimate of h 0 will necessarily be in error we show that for some common densities the fourier transform estimate of h 0 will err on the large side furthermore we show for a wide range of densities that the estimate of h 0 n γ with γ being a minimum for gaussian data and increasing systematically as the tails become heavier including exponential and power law therefore the iterative scheme allows h 0 to decrease if the algorithm fails to demonstrate convergence as expected for gaussian data the data based kernel converges rapidly to a form similar to that given by the gaussian kernel for skewed and or heavy tailed data convergence is slower and only occurs when h 0 is allowed to decrease toward its actual optimal value or range overall the data based iterated kernel gives significantly smaller ensemble mise than either 1 an iterated adaptive bandwidth gaussian kernel 2 a single pass adaptive bandwidth gaussian kernel and 3 a single pass gaussian kernel with a single global value of h 0 the new algorithm is clearly better when the non gaussian aspects of the underlying data increase including skewness and heavy exponential or power law tails when applied to particle arrival times that are heavy tailed the iterated kernel kernel and h 0 provide smooth and continuous interpolation and extrapolation of widely spaced late time arrivals even when few particles 5000 are used the iterated kernel approach does over smooth the early time data and the uab approach can be used to thin the estimated early time tail if a particle tracking model is used to compare to real data whose measurement times will not necessarily correspond to particle arrival times the methods developed here will be key to providing good interpolations between a simulation s widely spaced late time arrivals the derivation of the optimal kernel and global bandwidth solved a minimization problem for one variable h 0 based on kernel shape and the fourier transform of actual data appendix c a more difficult problem of optimizing a separate h i for each data point may be possible using cluster identification wu et al 2007 or multi gaussian kernel localization techniques sole mari and fernàndez garcia 2018 these methods would eliminate the potentially dubious taylor series based assumptions of the power law weighting scheme used in eq 7 to adjust each data point s bandwidth we leave this for a future paper credit authorship contribution statement david a benson conceptualization methodology software formal analysis writing original draft writing review editing diogo bolster formal analysis writing review editing stephen pankavich formal analysis writing original draft writing review editing michael j schmidt software writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we thank the editor and reviewers including daniel fernàndez garcia for extremely helpful comments we also thank daniele pedretti for sending source fortran code of their uab estimator this material is based upon work supported by or in part by the us army research office under contract grant number w911nf 18 1 0338 the authors were also supported by the national science foundation under awards ear 1417145 dms 1614586 dms 1911145 ear 1351625 ear 1417264 ear 1446236 and cbet 1705770 sandia national laboratories is a multi mission laboratory managed and operated by the national technology and engineering solutions of sandia l l c a wholly owned subsidiary of honeywell international inc for the does national nuclear security administration under contract de na0003525 this paper describes objective technical results and analysis any subjective views or opinions that might be expressed in the paper do not necessarily represent the views of the u s department of energy or the united states government matlab codes for generating all results in this paper are held in the public repository https github com dbenson5225 kernel density estimation appendix a mathematical background the idea of optimal global bandwidth silverman 1986 stems from using a truncated taylor series to represent the terms in the mise we begin with the fact that the expectation of the density estimate constructed from a set of independent observations is the sum of the expectations of the weights associated with each observation so that 11 e f x 1 n j 1 n e 1 h k x x j h 1 h k x ξ h f ξ d ξ similarly we compute the variance as var f x var j 1 n 1 n h k x x j h j 1 n 1 n 2 var 1 h k x x j h 1 n 1 h k x ξ h 2 f ξ d ξ 1 n 1 h k x ξ h f ξ d ξ 2 the bias at any point is b x e f x f x 1 h k x ξ h f ξ d ξ f x k z f x h z d z f x k z f x h z f x d z with this the mise is written as 12 mise b x 2 d x var f x d x k z f x h z f x 2 d z d x 1 n 1 h 2 k x ξ h 2 f ξ d ξ d x 1 h k x ξ h f ξ d ξ 2 d x the bias contribution is simply the effect of the kernel smoothing on the real density which does not depend on the sample size n the variance obviously grows smaller as n increases and not completely obviously as h increases this expression is difficult to minimize exactly although for both k and f gaussian the convolutions yield gaussians and an exact result may be computed silverman 1986 the vast majority of work done with kde is to use asymptotic expansions of certain functions with some questionable assumptions regarding their validity and application for example the density at x h z is typically approximated for h z 0 even though the goal is to find a finite h and z may be arbitrarily large in the integral still using a truncated taylor series namely f x h z f x h z f x 1 2 h 2 z 2 f x o h 3 gives b x h f x z k z d z 1 2 h 2 f x z 2 k z d z o h 3 h f x μ 1 k 1 2 h 2 f x μ 2 k o h 3 where μ n k denotes the n th moment of the kernel clearly using a zero mean i e symmetric or properly shifted kernel eliminates the first term on the rhs and indeed letting h 0 eliminates bias altogether but at the cost of increasing the noise in the estimate assuming a finite mean and proper shifting the squared bias is simply after truncation of higher order terms b x 2 d x 1 4 h 2 μ 2 k 2 f 2 d x silverman 1986 uses the bias approximation the substitution z x ξ h and another application of taylor series to reduce the local variance term to var f x 1 n h k z 2 f x z h d z 1 n f x o h 2 2 1 n h k z 2 f x h z f x o h 2 d z o n 1 1 n h f x k z 2 d z which when integrated over x yields var f x d x 1 n h k z 2 d z all told this gives a mise of mise 1 4 h 2 μ 2 k 2 f x 2 d x 1 n h k z 2 d z based on the assumptions of small h large n and n 1 h 2 all of which are likely to be bad assumptions in practice taking d mise d h and setting this expression to zero clearly gives the global estimate eq 5 in one dimension appendix b global bandwidth estimation using fourier methods in this section we implement a method based on the fourier transform that will allow us to create an unbiased estimator for the mise and minimize this function in order to select the optimal bandwidth h throughout we will use the form of the transform common to fast fourier transform routines namely g ω e 2 π i ω x g x d x for any sufficiently smooth function g x recall that the mise can be written as the sum of a bias and variance term as in 12 so that 13 mise b x 2 d x var f x d x where the bias is b x k z f x h z d z f x and the variance is given by var f x 1 n 1 h 2 k x z h 2 f z d z 1 h k x z h f z d z 2 using fourier methods we first compute the bias term taking the transform of the bias and making the change of variables y x h z we find b ω k z f x h z e 2 π i x ω d z d x f ω k z f y e 2 π i ω y h z d y d z f ω k z e 2 π i ω h z d z 1 f ω k h ω 1 f ω therefore by plancherel s theorem the bias term in eq 13 becomes b x 2 d x b ω 2 d ω k h ω 1 2 f ω 2 d ω to compute the associated variance term in eq 13 we first split it into two parts so that var f x d x 1 n i i i the first term is then i 1 h 2 k x z h 2 f z d z d x and satisfies i 1 h k y 2 f x h y d y d x 1 h k y 2 f x h y d x d y 1 h k y 2 d y f ξ d ξ 1 h k y 2 d y due to the change of variables y x z h and then ξ x h y as well as the fact that f x is a pdf to compute i i we write it as i i p x 2 d x where p x 1 h k x z h f z d z k y f x h y d y of course the transform of p x has already been identified in the computation of the integrated bias term in particular it is given by p ω k h ω f ω using plancherel s theorem as before we find i i p ω 2 d ω k h ω 2 f ω 2 d ω with this fourier representation of the bias and variance integrals we may explicitly write the mise in terms of integrals of transformed functions namely 14 mise n h 1 n h k ω 2 d ω k h ω 1 2 1 n k h ω 2 f ω 2 d ω note that we have used k y 2 d y k ω 2 d ω in the first term to write the mise depending upon k rather than k this derivation is similar to previous spectral representations of the mise chiu 1991 wu et al 2007 wu and tsai 2004 unfortunately this expression still requires knowledge of the fourier transform f ω of the unknown pdf and thus cannot be used to choose the optimal bandwidth h instead we will rely on an empirical distribution to approximate f and thus f given n observations of the distribution f x which are denoted x 1 x n we define the empirical or observed distribution f n x 1 n j 1 n δ x x j so that the corresponding transform of this function is 15 f n ω 1 n j 1 n e 2 π i x ω δ x x j d x 1 n j 1 n e 2 π i ω x j now as n we find f n f and f n f in fact we have an asymptotic estimate for the expected value of f n which implies 16 e f n ω 2 1 1 n f ω 2 1 n as n therefore by using the empirical distribution we can define and utilize an unbiased estimator for the mise for fixed n n and any h 0 define 17 ε n h 2 n k h ω d ω 1 1 n k h ω 2 2 k h ω f n ω 2 d ω 2 n h k 0 1 1 n k h ω 2 2 k h ω f n ω 2 d ω then ε n h and mise n h must attain their minimum values at the same h therefore given a sample x 1 x n of n draws from f x we define the optimal bandwidth by h ε h 0 arg min ε n h computationally approximating the global bandwidth using this value of h ε is instrumental to the algorithm proposed in section 2 finally we justify the claim that ε n h is an unbiased estimator of the mise we first note that by the fourier inversion property we have 1 h k 0 1 h k ω e 0 d ω k h ω d ω then taking the expectation of ε n h and inserting the convergence result 16 we find e ε n h 2 n k h ω d ω 1 1 n k h ω 2 2 k h ω e f n ω 2 d ω 2 n k h ω d ω 1 1 n k h ω 2 2 k h ω 1 1 n f ω 2 1 n d ω 1 1 n k h ω 2 1 1 n f ω 2 1 n d ω 2 1 1 n k h ω f ω 2 d ω 1 1 n k h ω 2 1 1 n f ω 2 1 n 2 k h ω f ω 2 d ω 1 1 n k h ω 2 2 k h ω 1 n k h ω 2 f ω 2 1 n k h ω 2 d ω 1 1 n k h ω 1 2 1 n k h ω 2 1 f ω 2 d ω 1 n h k ω 2 d ω 1 1 n mise n h f ω 2 d ω 1 1 n mise n h f x 2 d x this implies that modulo a shifting and scaling factor that are both independent of h the expectation of our estimator is exactly mise n h additionally it becomes clear that this function must attain its minimum at the same value of h as mise n h and modulo a shift we have e ε n h mise n h as n appendix c numerical bandwidth estimation next we outline a numerical approach based on our use of the fourier transform implementing the iterative algorithm of section 3 to compute the approximate distributon let us assume that the algorithm converges then due to the relationship between successive iterates and the previous kernel namely f ℓ 1 x and f ℓ x k ℓ x the final density and the kernel must converge to the same function as ℓ while the bandwidth must also converge to some value h 0 then denoting the converged iteratively estimated kernel based on data by k x this function must satisfy the interesting self similar property using eq 1 k x 1 n j 1 n 1 h k x x j h additionally its fourier transform then satisfies the relationship k ω 1 n h e 2 π i ω x j 1 n k x x j h d x 1 n h j 1 n e 2 π i ω z h x j k z h d z 1 n j 1 n e 2 π i ω x j e 2 π i ω z h k z d z 1 n j 1 n e 2 π i ω x j k h ω f n ω k h ω due to 15 with this and the asymptotic approximation 16 we have 18 k ω f ω k h ω o 1 n for n suitably large evaluating the mise 14 with the approximation k x k x and h h yields mise n h k h ω 1 2 f ω 2 d ω o 1 n for n suitably large finally expanding this expression and using the relationship 18 satisfied by the fourier transforms of the limiting kernel and unknown pdf we find mise n h k h ω 2 f ω 2 2 k h ω f ω 2 f ω 2 d ω o 1 n k ω 2 2 k ω f ω f ω 2 d ω o 1 n k ω f ω 2 d ω o 1 n therefore we see that for large n the iterative algorithm guarantees that the mise is minimized precisely when the kernel k x converges to the unknown distribution in the l 2 sense this suggests that when the algorithm converges as ℓ it must converge to the unknown pdf f x because an unbiased estimator for the mise is minimized at every step furthermore our analysis now demonstrates the appropriate range of taylor series estimates of h because the exact result can be derived from the fourier transform assume a gaussian for the kernel and also assume a priori that the underlying data are gaussian with zero mean and variance σ 2 so that k ω exp 2 π ω 2 2 and f ω exp σ 2 2 π ω 2 2 the first integral in eq 14 can be computed in several ways but is easily performed by recognizing the form of a gaussian so that 1 n h e 2 π ω 2 d ω 1 n h 1 4 π 1 2 π 8 π 2 e ω 2 2 8 π 2 d ω 1 2 π n h owing to the fact that the last integral is of a density in ω similarly the second integral in eq 14 is 1 1 n k 2 h ω 2 k h ω f 2 ω d ω 1 1 n e 2 π h ω 2 2 e 2 π h ω 2 2 e σ 2 2 π ω 2 d ω 1 1 n e σ 2 h 2 2 π ω 2 2 e σ 2 h 2 2 2 π ω 2 d ω 1 1 n 2 π h 2 σ 2 1 π σ 2 h 2 2 where we have rearranged as before to make gaussian densities in ω for each term therefore the resulting quantity to be minimized silverman 1986 is now 19 mise n h 1 2 π n h 1 2 π σ 2 h 2 1 2 n π σ 2 h 2 1 π σ 2 h 2 2 it suffices to approximate the h 0 that minimizes mise n h to any numerical tolerance by taking d mise n h d h setting it to zero and finding the root of the resulting equation as expected the estimate of h 0 based on taylor series is worse for smaller data sets i e n 100 but as n grows large the taylor series solution converges to the exact solution fig 8 however it is important to note that these quantities are the optimal bandwidth when both the kernel and the underlying data density are known to be gaussian if the underlying density is unknown then the data are used to construct the quantity to be minimized ε n h in eq 17 to see how this differs we can imagine that perfectly gaussian data is generated then eq 17 evaluates to 20 ε n h 2 2 π n h 1 2 π σ 2 h 2 1 2 n π σ 2 h 2 1 π σ 2 h 2 2 which has a root approximately 4 1 5 1 32 larger for large n fig 8 the fact that data are imperfect means that the global bandwidth must be about 32 to 70 larger depending on n to achieve additional smoothing when compared to a completely perfect realization of data for a specific example we set n 1000 and σ 2 1 which produces an exact optimal global bandwidth for imperfect data using ε n h in eq 17 of h 0 0 341 fig 9 whereas the estimate based on taylor expansions gives h 0 1 06 σ n 1 5 0 266 it is important to see how well a numerical estimate of the data density gives an estimate of h 0 rather than simply assuming a gaussian density function we may now compare the values of h that are estimated using the fourier transformed data to form an estimate of the density function i e using eq 15 in eq 17 instead of assuming the gaussian form here we show the results for 50 independent runs in which 1000 iid gaussian data are generated and the experimental curve generated and h taken at the curve minimum black curves in fig 9 while there is a large vertical spread in the curves the locations of the minima are fairly tightly constrained the mean of 50 values of h 0 is 0 335 compared to the exact value of 0 341 and the estimated h 0 have a standard deviation of 0 0238 several other characteristic functions fourier transforms of pdfs are easily integrated and illustrate the effect of data distribution on estimation of h 0 for example the standard cauchy density defined by f x σ π σ 2 x 2 has both divergent variance and mean and its associated fourier transform is given by f ω exp 2 π σ ω note that the scale parameter σ commonly used for stable densities is not the standard deviation which is infinite assuming that the kernel was also a perfect copy of the data density so that k ω exp 2 π ω the mise becomes up to an additive factor independent of h mise n h 1 2 π n h 1 1 n 1 2 π h σ 1 π 1 2 h σ therefore calculating the minimum of the mise 14 means solving the root of 21 d mise n d h 1 2 π n 1 1 n 1 2 π 1 σ h 2 2 π 1 2 σ h 2 these values of h 0 n are significantly smaller than those found for gaussian data fig 8 and also decline for large n approximately like n 1 3 this suggests a numerical procedure for simultaneous estimation of the data density and the global bandwidth the ft estimate of h 0 based on gaussian data is the largest of the estimates fig 8 so we begin with that value if the iterated kernel based on the estimated density and using this h 0 fails to converge then we reduce h 0 systematically down to a minimum given by the cauchy h 0 in this procedure the specifics of the data distribution need not be known simply start with an assumption of gaussian like smoothness and data density but allow for cauchy like sparcity of data i e few very large data we may also consider the laplace or double exponential density defined by f x 1 2 σ exp x σ which has mean zero and variance 2 σ 2 but does not possess a continuous derivative at x 0 the fourier transform of this function is given by f ω 1 1 4 π 2 σ 2 ω 2 if the kernel is similarly distributed so that k ω 1 1 4 π 2 ω 2 then the mise becomes mise n h 1 4 n h 1 1 n h 2 3 h σ σ 2 4 h σ 3 2 h σ 2 h σ 2 as before the minimum of the mise 14 can be computed by finding the root of the derivative of this expression namely 22 d mise n d h 1 4 n 1 σ h 2 3 4 1 4 n h σ h σ 1 3 4 1 4 n h σ h σ 1 2 the resulting values of h 0 n are again significantly smaller than those found for gaussian data fig 8 and also decline for large n approximately like n 1 4 finally we consider the family of stable distributions whose density may be defined by f x 1 2 π e c k α 1 i β s g n k tan π α 2 e i k x d k where 0 α 2 is the stability parameter 1 β 1 is a skewness parameter and c is the scale parameter the fourier transform is given by f ω e 2 π c ω α 1 i β s g n ω tan π α 2 as before if the kernel is similarly distributed so that k ω e 2 π ω α 1 i β s g n ω tan π α 2 then the mise becomes mise n h c 2 1 α n h 1 1 n 2 h α 2 c α 1 α 2 h α 2 c α 1 α where c e 2 π ω α 1 i β s g n ω tan π α 2 d ω as before this can be minimized and similar to the distributions explored so far we find that there is a power law decline n γ where γ depends on α as depicted in fig 10 note that the magnitude of mise depends on α and β through the constant c but that this does not impact the minimized value we also note that these calculations may be made for other densities but are not shown additionally some of the integrations must be performed numerically as it may be the case that no closed form expression for the antiderivative exists 
302,this study aims to analyze model reproducibility on large wood collisions on moveable channel beds flume experiments were conducted to observe the responses of moveable bed change with the behavior of large wood including wood collisions in the experiments we employed time intervals of 5 and 9 s in wood supply which modified the collision patterns subsequently we conducted simulations based on the experimental conditions herein to analyze the wood collision effect in the simulations we employed the dashpot spring model in a large wood dynamics model subsequently we compared the simulations with and without wood collision and discussed the reproducibility of the model the results of the simulations and experiments revealed that the wood pieces exhibited diverse behaviors including floating sliding deposition plane rotation rolling collision remobilization jam formation and causing local bed changes in particular the results indicate that model reproducibility is dependent on wood collision because this can actively alter wood deposition patterns we also described the piping and tunnel erosion processes linked to wood collision and moveable beds and suggested the corresponding suitable computational treatment this study presents an experimental and modeling methodology on wood collision and the interaction between bed change and large wood thus useful information for solving numerical difficulties linked to large wood dynamics including wood collisions is provided keywords flume experiments with large wood 2 d flow and large wood models large wood deposition on moveable bed large wood collision model 1 introduction in the past few decades studies on large wood length 1 m and diameter 0 1 m have been conducted swanson et al 2020 through field observations e g the queets river in the u s a abbe and montgomery 1996 the drôme river in france piégaya and gurnell 1997 the oyabu basin in japan haga et al 2002 and the tagliamento river in italy gurnell et al 2002 bertoldi et al 2013 flume experiments e g braudrick et al 1997 braudrick and grant 2001 welber et al 2013 bertoldi et al 2014 kang and kimura 2018 schalko et al 2020 and numerical computations e g ruiz villanueva et al 2014 ruiz villanueva et al 2020 v ruiz villanueva et al 2014 persi et al 2019a persi et al 2019b kimura and kitazono 2019 kang et al 2020 studies on large wood have been advanced and documented by several countries e g the u s a japan and european countries through the investigation of the large wood effect and environmental changes in natural systems in the initial stages of research on large wood braudrick et al braudrick et al 1997 and braudrick and grant braudrick and grant 2001 established the basic framework for methodologies on large wood dynamics using flume experiments braudrick et al braudrick et al 1997 conducted diverse experiments on wood transportation in rivers and classified the jamming types as uncongested congested and semi congested additionally braudrick and grant braudrick and grant 2001 conducted flume experiments by assuming that the incipient motion of wood occurs when it begins to slide examining the parameter interactions among hydraulics channel formation travel distance and spatial deposition furthermore bocchiola et al bocchiola et al 2006 investigated large wood motions through tilt testing measurements of the friction angle of each large piece of wood on a fixed channel bed under dry conditions they tested the critical angles that initiate wood motion between the trunk direction and flow direction most studies on large wood have assumed the shape of large wood as cylindrical within their experiments these studies e g braudrick et al 1997 braudrick and grant 2001 bocchiola et al 2006 have also considered that partially submerged wood on a channel bed is subject to drag and friction additionally the cylindrical shape of large wood was examined through a hydrodynamic response in an open channel flow to investigate wood motion in the horizontal orientation as well as the submerged volume and area affecting the drag force the drag force that interacts between large wood and water flow is an important parameter in understanding wood motion several studies e g persi et al 2019a gippel et al 1992 shields and alonso 2012 have examined the changes in the drag coefficient with respect to the orientation angles and cylindrical shape of the wood in addition these studies assessed the drag force of natural wood in rivers gippel et al gippel et al 1992 examined the drag coefficient of the cylindrical shape and the angle between water flow directions and trunk wise flow shields and alonso shields and alonso 2012 assessed the drag force of real wood in a natural river and discussed changes in the drag force for wood profiles such as orientation and submerged volume ruiz villanueva et al ruiz villanueva et al 2014 ruiz villanueva et al 2020 ruiz villanueva et al 2014 ruiz villanueva et al 2016a ruiz villanueva et al 2016b ruiz villanueva et al 2016c contributed a significant amount of information on large wood transport and interacting river systems through various studies on the laboratory experiments and numerical simulations of the dynamics of large wood motion and channel evolution they also observed and analyzed field data related to the transport of large wood depending on wood characteristics and river flow through another approach several numerical simulations for wood dynamics were developed based on a number of flume experiments and observations e g haga et al 2002 braudrick et al 1997 braudrick and grant 2001 welber et al 2013 bertoldi et al 2014 ruiz villanueva et al 2016a ruiz villanueva et al 2016b ruiz villanueva et al 2016c davidson et al 2015 in particular two dimensional 2 d hydrodynamic flow models based on eulerian and lagrangian modeling were proposed to model large wood transport examples of such models are the iber wood model which considers the forces balance between water flow and large wood ruiz villanueva et al 2014 ruiz villanueva et al 2020 the two dimensionally connected spheres model which simulates the bed deformation by large wood kang and kimura 2018 kang et al 2020 and orsa2d wt which calculates the multiple local forces from the four segments of a cylindrical wood body persi et al 2019a persi et al 2019b to simulate wood motion such simulation models have enhanced our understanding of wood dynamics transport and motion however considering the present computational methods large wood modeling is still significantly challenging owing to channel bed deformation swanson et al 2020 in particular moveable bed channels such as braiding network formation and alternating bars can actively show the large fluctuations in bed evolution and drive wood dynamics this increases the complexities of the interaction among water flow bed change and large wood consequently more sophisticated modeling that can precisely reproduce such complexities is required to understand wood transport as for bed deformation studies with large wood flume experiments have been actively conducted welber et al 2013 bertoldi et al 2014 welber et al welber et al 2013 and bertoldi et al bertoldi et al 2014 showed that the bed morphology drove wood transport and deposition in a braided multi thread river reach with shallow flows where they classified deposition patterns via several parameters such as wood supply the rootwad effect and jam formation therefore they provided useful data for understanding the relationship between braiding channels and large wood deposition the authors showed that bar formation could strongly affect wood deposition patterns in a braiding network these studies focused on partially submerged braiding channels there is a need to consider different channel geometries in a submerged moveable bed to understand the diverse morphodynamics of large wood in particular sand bar formation such as alternating bars and mid channel bars is important in fluvial bed deformation so far explorations of sand bars and related large wood behavior patterns in numerical simulations based on laboratory experiments are scarce moreover wood collision has been commonly shown in the above experimental studies and it also plays a role in collapsing and forming wood jams accordingly wood collision is also a significant factor in large wood deposition simulating physical wood collision is one of the difficulties of fluid and wood dynamics because simulating the collision motion requires costly cpu time additionally a collision model is a mechanical approach based on the vector of solid particle motion which is different from water pressure thus model complexity increases further kimura and kitazono 2019 however if wood collision is employed in large wood modeling the model would better reproduce the formation and collapse of wood jams kang and kimura 2018 kimura and kitazono 2019 kang et al 2020 the main aim of this work is to increase our understanding of the computational methodology using large wood dynamics model to reproduce the wood motion on a moveable bed as well as wood collision for this we conducted a study based on flume experiments and numerical simulations and we examined the behavior of large wood including wood collision and bed deformations on a moveable bed we carried out a comparison of the experimental and simulation results to understand the limitations of the large wood dynamics model with respect to a moveable bed which is one of the challenges associated with the computations of wood transport we then discuss the diverse deposition patterns including wood collision and bed deformations this study provides useful data on several phenomena such as the interaction between a wood jam or wood transported in a congested regime on a moveable bed in addition we provide a framework for applying our methodology to numerical and experimental methods 2 methodology 2 1 experimental and computational setup and bed profiles to ensure the simplicity of the model we did not consider a prototype scale of the natural river and large wood instead we arbitrarily decided on the dimensions of the experimental configurations e g flume and flow discharge and grain size which were considered for generating alternating bars particularly we considered the criteria for conditions of the bar formations no bar alternating bar braiding channel according to the given parameters such as the channel slope channel width grain size uniform water depth and non dimensional tractive force proposed by kishi and kuroki kishi and kuroki 1973 based on this criterion we determined the initial water depth 0 01 m using channel width 0 4 m and the initial flatbed with a constant slope of 0 005 m m those parameters showed the acceptable non dimensional tractive force 0 059 which can generate the alternating bar in the criterion for conditions of the bar formations kishi and kuroki 1973 herein the inlet flow discharge was 1 1 l s note that the minimum and maximum values of the available inlet flow discharge of the pump were 0 016 and 10 l s respectively additionally the size of the flume was 12 m 0 4 m fig 1 a b and the height of the sidewall of the flume was 0 3 m the initial sand bed layer was 5 cm and the uniform grain size was 0 51 mm at the upstream section of 0 m in the flume we continuously supplied sediment to maintain the same initial upstream elevation every 90 s under those conditions an alternating bar was able to be generated in 5 h note that an alternating bar can also be generated under a larger inlet flow discharge than 1 1 l s but most wood pieces in pre tested experiments showed the floating motion in such a larger inlet flow discharge two experiments hereafter exp1 and exp2 were conducted for each experiment wood pieces were supplied after the generation of the alternating bars the time intervals of the wood supply were 5 s exp1 and 9 s exp2 this was because the time intervals of wood supply change the collision patters of wood pieces clearly showing the wood collision effect all flume experiments were conducted under the same hydraulic conditions as listed in table 1 four bed profiles were gauged depending on exp1 before after wood supply and exp2 before after wood supply we first gauged bed elevation profiles within 3 and 10 m fig 1 a in both exp1 and exp2 after generating an alternating bar then we gauged the bed elevation again after the wood supply the reason for bed elevation measuring to within 3 0 m of the flume is for the wave effect by the wood input and sediment supply that occurs with 0 3 m of the flume which slightly changes the phase of sediment transport and wood motion patterns sometimes small backwater waves that form due to the step shape at the end of the flume see the longitudinal flume domain in fig 1 a slightly affect the wood deposition patterns within 10 12 m flume thus only the first 3 10 m of the flume was considered thus this phenomenon could be disregarded the numerical simulations were carried out in this study to quantify the channel evolution by large wood deposition and analyze the reproducibility of the wood collision under the bed deformation herein we used measured bed elevations for the simulations which were measured before the wood supply the resolutions x and y of exp1 and exp2 were 0 001 0 02 m2 and 0 03 0 01 m2 respectively as the measuring device was taken and replaced from a point gage at exp2 to a laser profiler at exp1 the grid size of the computational domain was 0 02 0 02 m2 the number of stored wood pieces within this gaging section 3 10 m was counted see fig 1 a c 2 2 wood characteristics we considered the dimensional ratio between the wood profile and flume size together with the hydraulic conditions e g grain size and water depth as we first considered generating the alternating bar flow discharge was the most important factor based on this flow discharge and uniform water depth we considered the critical draft for wood motion cdm which is the minimum water depth to have a floating motion kang and kimura 2018 the length and diameter of the piece of large wood were selected as 0 1 and 0 01 m respectively because we considered the ratio between the flume width and the length of the wood piece as the medium river scale presented by kramer and wohl kramer and wohl 2017 herein the medium river scale considering large wood means that the channel width stem length of large wood is 2 4 or the water depth diameter of large wood is 2 4 based on this scale kang and kimura kang and kimura 2018 also considered a ratio of 1 3 for their river scale this study similarly considered a ratio of 1 4 for this the density of the wood piece was adjusted to 620 kg m3 which allowed us to generate both wood jams and single deposition kang and kimura kang and kimura 2018 experimented with the large wood motions using a wood piece model with rootwad in their work the cross shaped rootwad was based on bertoldi et al bertoldi et al 2014 experiment the diameter of the rootwad was arbitrarily determined as 0 02 m which is a larger size than the stem diameter of the wood piece to examine the rootwad effect based on kang and kimura kang and kimura 2018 this study also considered the diameter of the rootwad modeled as a cross shape was 0 02 m as shown in fig 2 the deposition of this cross shape varied depending on the large wood deposition if the deposition happened at a orientation mostly due to sliding motion and deposition the diameter of the rootwad was assumed to be 0 02 m if the deposition happened at an orientation mostly due to deposition the diameter was assumed to be 0 014 m thus while considering both deposition types in the numerical simulation the averaged rootwad particle diameter was assumed to be 0 017 m 2 3 wood supply characteristics in the experimental and computational cases in the flume experiment the initial angle of the wood pieces was set to 90 against the streamwise direction to permit easy downstream flow these settings also prevent the formation of a wood jam at the wood supply zone the wood piece supply zone in the flume experiment was set to 1 5 m from the upstream of the flume to minimize the generation of a wave by the wood supply fig 1 thus the wood pieces were manually supplied into the wood supply zone in contrast in the computations the generation zone circle shape of the wood pieces was set as shown in fig 1 d in the simulation we assumed that the initial angle of the wood pieces was determined by the gradient of the flow vectors in the transversally neighboring two cells around the center position of the wood piece see gradient between flow vectors of fig 1 d fig 1 d shows how wood pieces were generated in an assigned cross section 0 1 m from upstream for instance when the maximum flow velocity occurred in the center part of the assigned cross section the wood piece was generated in this cell the angle of the wood piece was that of by the gradient of the flow vectors at the center position of the wood piece in the transversally neighboring two cells using grid numbers j and j 1 in the transversal direction in the upstream area the upstream cross section of the computational domain of the simulations the non uniform flow velocity was generated in the transversal direction because the inlet flow velocity in the upstream segment was dependent on the water depth and bed elevation at the upstream cross section kang et al 2020 accordingly the time changes in the upstream flow velocity were predominantly the same in all the simulations this wood generation method showed a deterministic pattern dependent on the water flow rather than the unpredictable random generation method thus it reduced the simulation cases by eliminating the random factor the wood pieces were alternately supplied with and without rootwad we arbitrarily determined the number of wood pieces to be 200 100 rootwad and 100 no rootwad in all experimental cases table 2 lists the scenarios of the simulations there were six runs conducted in this study runs1 3 used the bed profile of exp1 and runs4 6 used exp2 the wood supply time intervals were 5 s runs1 3 and 5 and 9 s runs2 4 and 6 the dashpot spring model simulations were conducted in runs1 2 4 and 5 to consider the wood collision effect 2 4 computational method for the numerical simulation nays2dh shimizu et al 2014 in the international river interface cooperative iric software international river interface cooperative iric 2020 was used to simulate the flows and bed morphology nays2dh is based on the grid eulerian computational domain and the finite difference method to solve the shallow water equation and the 2 d exner equation in particular nays2dh uses the generalized curvilinear coordinate to fit the computational domain into a curved river shape and reproduce the channel evolution e g channel widening meandering the water flow calculation of nays2dh was conducted by employing the third order total variation diminishing monotonic upstream centered scheme for conservation laws tvd muscl to solve the advection of water flow herein turbulence flow was calculated by a zero equation type model for the sediment transport the ashida and michiue ashida and michiue 1972 formula was used to estimate the bedload flux in the streamwise and transversal directions and a bank collapse model based on hasegawa hasegawa 1985 was employed angle of repose 30 the manning roughness coefficient was 0 013 s m1 3 which was estimated using the manning strickler formula from the uniform sediment diameter of 0 51 mm in this study the large wood dynamics model was employed to simulate the motions of large wood in the water flows and for interacting with the channel bed in the large wood dynamics model the motion of the large wood is expressed as a series of spheres and the collision of each sphere is evaluated with the spring and dashpot model like the discrete element method dem approach cundall and strack cundall and strack 1979 in the model the motion of each sphere composing a piece of large wood is calculated with a 2d lagrangian equation in a stepwise manner at each time step the locations of spheres should be rearranged to form the shape of large wood a straight line in this case this method is similar to the moving particle semi implicit mps method simulating a rigid body in a flow proposed by koshizuka koshizuka 1997 the large wood dynamics model reproduces the floating sliding settling collision rotation and rolling motions kang and kimura 2018 kang et al 2020 kimura and kitazono 2019 herein the second order adams bashforth scheme was used to calculate the advection of wood particles moreover this model considers both the change in the draft for wood motion and bed friction force to consider the sliding and settling motion this large wood dynamics model was coupled with nays2dh e g kang and kimura 2018 kang et al 2020 note that the detailed methods of interaction processing between nays2dh and the large wood dynamics model were described in the report by kang and kimura kang and kimura 2018 in addition nays2dh along with the large wood dynamics model simulates the effect of large wood deposition for a changing bedload flux in sediment transport on the exner equation to consider sediment flux by large wood deposition we employed the area occupation ratio σ p of particles against one grid cell acell the occupation ratio σ p increases when wood particles are deposited on the cell area acell the rate of bed elevation change increases for the same amount of sediment flux change if the occupation ratio increases we used a modified exner equation kang et al 2020 for evaluating the bed elevation change 1 q b x x q b x x δ x δ x q b y y q b y y δ y δ y δ ζ δ t 1 λ s 1 σ σ p where qbx and qby denote the sediment discharge flux in the x and y directions respectively δt is the time step δζ is the bed elevation change at δt δx and δy are the cell lengths in the x and y directions respectively λ s is the soil porosity 0 4 and σ p is written as 2 σ p a p δ x δ y a p a c e l l where ap is the total occupation area of the wood particles in one grid cell note that the occupation ratio σ p is not applied to the calculation of water flow instead when the wood particles deposit on a grid cell drag force by the large wood is calculated and it was applied to the water flow calculation of nays2dh called the two way type model for interaction between water flow and large wood a large amount of large wood within a computational cell can decrease the flow velocity by drag force resulting in the increase of water depth we also employed parallel computation openmp in nays2dh and the large wood dynamics model including wood collision to reduce the cpu time the basic equations for large wood dynamics are described in several previous studies kang and kimura 2018 kimura and kitazono 2019 kang et al 2020 2 5 dashpot spring model for computational wood collision the soft spherical particle model for granular flows was initially proposed by cundall and strack cundall and strack 1979 the dashpot spring model is a type of soft spherical particle model it calculates a non linearly incremented collision force depending on the overlap distance between particles based on hertzian contact theory this dashpot spring model was used in the large wood dynamics model this collision model was applied to the particle motion in a two dimensional plane the dashpot is a damper that resists motion via an elastic modulus the resulting force is proportional to the velocity but acts in the opposite direction slowing the motion and absorbing energy the spring is a reflection force that acts to resist displacement to apply such a wood collision system we assumed that the particles of wood pieces are soft spheres young s modulus 0 689 gpa the model of particle motion by collision dashpot spring is widely known in this field e g gotoh et al 2013 kang et al 2019 such that we omit a description of the equations 2 6 particle in cell method for collision detection in the computation in terms of the solid collision model detecting particle contact is dependent on a significantly small computational time step this is because the distance between all the particles of the wood pieces must be estimated to detect the wood collision in the lagrangian method and this estimation requires a lot of cpu time the particle in cell pic method together with a potential collision area was employed to overcome this difficulty fig 3 illustrates the schematic domain for the pic and potential collision area as a first step all wood pieces were assigned with the address of the sub grid cell which is larger than the water flow grid cell to reduce the required computational memory after this process of address assignment the computer can recognize the position of the wood pieces and effectively determine the cell in which wood collision occurs when more than two wood pieces are located in a cell the distances among the center positions of all the wood pieces are calculated if the calculated distance between the wood pieces is smaller than the potential distance the collision between every particle of those wood pieces is calculated by considering the distance between the center positions of the particles here the potential distance was assumed to be the length of each wood piece because the motion velocity of the wood pieces was smaller than the length of the wood at all time steps this process effectively reduces the cpu time for collision detection 3 results 3 1 final patterns of the channel bed and large wood deposition in the experiments and simulations fig 4 shows the experimental results for the wood with and without rootwad deposition patterns and channel variation profile along the flatbed with a constant slope additionally the red and blue rectangular shapes indicate the wood pieces with and without rootwad respectively fig 4 indicates the results of exp1 and exp2 showing that the wood jams occurred at the bar crest or mid channel bar where the water depth was lower than that of the cdm 0 006 m additionally fig 4 b indicates the wood jams i e the sections at 4 6 and 9 10 m and in fig 4 d the sections at 6 7 and 9 10 m all simulation results fig 5 show deposition patterns on the bar crest or mid channel bar a larger flow velocity 0 35 m s occurred near the wall adjacent to the alternating bar in all the simulations a smaller flow velocity 0 1 m s occurred near the location of the wood jam where the water depth was smaller than the cdm 0 006 m considering the deposition section in the simulation cases of runs1 3 most of the wood jams formed within the section of 4 6 m in the computational domain fig 5 the sections of 9 10 m showed wood jams of smaller sizes as shown in runs1 and 2 similarly exp1 fig 4 b shows that the main wood jam was in the section of 4 6 m however run3 without wood collision shows that all wood pieces deposited at 4 6 m in the computational domain and the stored wood pieces were significantly larger than those in runs1 and 2 the simulated result of run4 shows differently distributed wood jams the primary wood jam sections were between 7 8 m and 9 10 m which was slightly different from those in the experimental results 6 7 m and 9 10 m run5 shows that most wood pieces deposited between 9 and 10 m this is because water depth was slightly overestimated compared to the cdm 0 006 m at 6 7 m moreover wood collision disturbed the deposition of wood pieces in this section thus wood pieces moved and then deposited after 7 m where the water depth was lower than the cdm run6 shows several stored wood pieces at 5 6 m this is because the absence of wood collision in the simulation allowed an overlap of wood at 5 6 m and then caused a stable deposition because the overlapping wood pieces significantly decreased the flow velocity and water depth the results of run 3 show the same overlapping and water flow change those results imply that the absence of wood collision has a higher impact than an overestimated water depth table 3 shows the final number of stored wood pieces the experimental results show that the final number of stored wood pieces during 5 s exp1 and 9 s exp2 were 34 and 57 respectively the final number of stored wood pieces in the simulations runs1 6 were 29 33 199 41 18 and 188 respectively herein the cases without wood collision showed significantly larger values than those with collision fig 6 indicates the simulated bed deformations from the gauged bed elevation exp1 and 2 fig 4 a d shows the channel variation along with the channel slope in contrast fig 6 shows the channel variation by the generated bar clearly expressing bed deformation together with the wood pieces the black particles on the domain in fig 6 show the center position of the wood pieces additionally the colors of erosion 0 m and deposition 0 m were narrowed to 0 01 m in this figure here fig 6 shows that most of the wood jams were located in the area where there was a higher elevation 0 m with a lower water depth 0 006 m additionally fig 6 indicates local scour near the wood jams these patterns are the same as those of the experimental results fig 4 3 2 effect of time interval on wood supply fig 7 shows the evolution in the number of stored wood pieces in total a count of nine for the wood pieces was obtained at an interval of 300 s in both exp1 and exp2 during the wood supply period the number of stored wood pieces actively increased in some simulations runs1 2 4 and 5 and all experiments but decreased after the end of the wood supply however runs3 and 6 showed a significant increase of stored wood pieces with wood piece overlapping fig 5 excluding no collision cases runs3 and 6 all of the experiments and simulations with the larger time interval of 9 s i e runs2 and 4 resulted in a larger number of stored wood pieces than in the case of the 5 s interval i e runs1 and 4 as shown in fig 7 3 3 time changes in the rootwad proportion fig 8 illustrates the changes in the proportion of rootwad with time where we compare the effect that rootwad has on wood storage if the value of the proportion is one all the stored wood pieces have rootwad whereas at the other extreme when the proportion is zero none of the stored wood pieces has rootwad the experimental cases fig 8 showed a proportion of 0 5 from 300 s to the final time step 2400 s run1 showed a significantly increased value after wood supply 1000 s with a larger proportion value 0 75 at the final time step because the wood jam had collapsed at approximately 1000 s and several wood pieces without rootwad flowed downstream the final proportions of rootwad in the simulations runs1 6 were 0 79 0 66 0 50 0 50 0 61 and 0 51 respectively the runs without wood collision runs3 and 6 showed a convergence of rootwad proportion to 0 5 the cases for exp1 and 2 showed final values of 0 58 and 0 57 respectively 3 4 quantification of channel deformation patterns the bed relief index bri hoey and sutherland 1991 was calculated using the bed elevation of all the simulation results to compare the channel patterns kang et al 2020 kang et al 2018 if bri 0 the channel bed is a flatbed in all cross sections if bri 0 the channel bed shows uneven bed elevations kang et al 2018 indeed bri also means the averaged value of the mean square deviations on bed elevation in all cross sections thus bri effectively shows the magnitude of channel evolution if bri increases the channel bed shows an active deformation resulting in more extensive erosion on the contrary if bri decreases the channel bed shows an inactive deformation with local erosion moreover bri increasing implies a larger rate of sediment output than sediment input and bri decreasing shows a smaller sediment output than sediment input hoey and sutherland 1991 using the bri the channel patterns in the longitudinal direction were not considered in this direction the local channel slope dramatically increased the complexity of the bri patterns which reduced the readability of the graph fig 9 shows the range of temporal changes in the bri during all simulation times the initial values of the bri 10 5 m2 were 4 9 runs1 3 and 5 84 runs4 6 additionally the final values of the bri 10 5 m2 were 4 23 4 21 3 99 4 31 4 04 and 4 13 respectively the mean values runs1 6 of the bri were 4 46 4 45 4 35 4 61 4 56 and 4 54 respectively showing that the mean values of the bri in runs4 6 were greater than the values in runs1 3 moreover runs without wood collision runs3 and 6 clearly showed lower mean bri values than those with collision cases in each bed profile 3 5 model reproducibility of the large wood deposition number the reproducibility of the simulations depending on the collision model and the time interval of wood supply was evaluated using the coefficient of determination r2 and root mean square error rmse herein the compared values were time changes in the number of stored wood pieces the simulation results with the same time interval and bed profile conditions were selected runs1 3 4 and 6 from fig 7 eight values excluding 0 s were extracted from the values of the time changes in the stored wood pieces in the simulations as data were obtained by counting the wood pieces at a time interval of 300 s then these values were used in the estimation of the two evaluation coefficients table 4 indicates the estimated reproducibility of the simulations in exp1 and the simulations runs1 and 3 the r2 values were 0 820 and 0 781 respectively which showed a high correlation between the simulation and experiment similarly exp2 and the simulations runs4 and 6 showed high correlations 0 924 and 0 935 this was because the time changes in the stored wood pieces were significantly dependent on the wood supply the values of the rmse for all target runs 1 3 4 and 6 were 5 301 133 976 12 297 and 89 71 respectively accordingly this indicates that the wood collision effect had a higher reproducibility than no wood collision therefore the simulation results with wood collision reproduced the experimental results in terms of the number of stored wood pieces based on the time changes in particular r2 showed high correlation values 0 7 and the rmse was much smaller than in the cases without collision this indicates that the present model considering wood collision well reproduced the tendency of time changes in the number of stored wood pieces in the computational domain 4 discussion 4 1 alternating channel with large wood deposition in experiments and simulations we performed simulations considering a sediment transport module able to reproduce the formation of alternating bars and the simulation of the transport and collisions of wood pieces in fig 4 exp1 and exp2 show that the wood jams were located near the bar crest where the water depth was lower than that of the cdm in the simulation of run1 based on exp1 the model reproduced the time changes in the number of stored wood pieces see table 4 and deposition location figs 4 and 5 well although the time changes in stored wood were reproduced well r2 0 7 in run 4 which is based on exp2 the model reproduced the different locations of wood jams as shown in figs 5 and 6 the primary wood jam section was between 7 8 m and 9 10 m which was different from that in the experimental result 6 7 m and 9 10 m this was because the water depth was slightly overestimated compared to the cdm 0 006 m at 6 7 m and the motion of wood pieces was more sensitive to the cdm than to the flow velocity thus wood pieces flowed more in the simulation not only in run4 but also in run5 so that they deposited after 7 m where the water depth was lower than the cdm in particular the wood deposition results of all simulations of both runs3 and 6 without wood collision were significantly different from the experimental results this is because the absence of wood collision allows wood overlapping resulting in an easy deposition of many wood pieces decreasing the flow velocity due to large drag forces moreover those wood pieces can decrease the water depth near the downstream side to capture more wood pieces note that the wood overlapping means that a particle of a wood piece invades into the inside of a particle of another wood piece in the simulations the wood collision in the simulations and experiments caused a significant interaction between the wood pieces 1 when the first wood piece deposits on the bar crest caused by a lower water depth than that of the cdm and when a second wood piece also deposits on the same location these two wood pieces may cause a collision at this time the first wood piece may flow downstream if its depositional area has a larger water depth than that of the cdm or it may laterally deposit otherwise if the depositional area still has a lower water depth and the first wood piece stream wisely deposits the first wood piece can deposit on the bed together with the second wood piece without contacting each other herein the first wood piece disturbs the motion of the second wood piece at the contacted point between both wood pieces such that the second wood piece is rotated by the water flow into a lateral direction against the streamwise direction 2 the second wood piece rotates at the contacted point during the second wood piece rotation the first wood piece indicates a slow sliding motion toward the downstream but the moving distance of the second wood piece is relatively smaller than that of the first wood piece 3 then the first wood piece is finally deposited in the lateral direction after contacting the second wood piece this process shows the formation of a jam on the bar crest sometimes if the first wood piece flows downstream only the second wood piece will remain on the bed due to additional wood collision and water flow 4 those wood pieces may become a wood jam and the wood jam can consistently decrease the flow velocity in the depositional area where the sediment deposits thus the channel bed gradually increased in this area abbe and montgomery 1996 in contrast the flow velocity increased around the wood jams narrowing the channel width such that the increased velocity caused erosion consequently the bar crest was changed into a mid channel by this process as shown in fig 4 b and d fig 9 shows the bri ranges which indicate that the bed deformation by large wood occurred locally if large wood actively changes the whole channel bed e g generating a large thalweg the bri should increase but the final values of the bri in all the cases decreased as shown in fig 9 moreover the experiments and simulations figs 4 6 show the final channel patterns with local bed deformations local scours and depositions which were generated near the wood jams and deposited large wood the wood jam changed the bar migration patterns bar length as shown in figs 4 and 6 the speed of bar migration showed an unclear pattern in the entire domain but the wood jams caused local erosion near the deposit location consequently the effect that the wood jams have on bar migration was not dominant in the entire domain but it locally affected bed deformation channel geometry influences jam formation and single large wood deposition abbe and montgomery 1996 kang et al 2020 cherry and beschta 1989 4 2 large wood deposition by wood supply wood collision rootwad and density fig 7 shows the time dependent changes in the number of stored wood pieces the experimental results exp1 and exp2 and all simulations show that the wood supply time interval controlled the number of stored wood pieces if the time interval of the wood supply increased there was an increase in deposited wood pieces in other words the time interval of the wood supply activated wood collisions for the floating wood pieces except for no wood collisions runs3 and 6 all simulations also reproduced such depositional patterns as the number of floating wood pieces grew due to the shorter time interval they actively caused wood collisions however the absence of wood collisions can cause wood overlapping which results in a non physical deposition with a larger number of wood pieces it also changes the sediment transport by large drag forces which are caused by the deposited wood pieces thus wood collision can be an important parameter when considering the transport of wood and sediment in the simulation in particular this parameter should be carefully considered in the dramatic transport of wood such as woody debris during flood events in the river system we observed such phenomena related to the wood supply and wood collision time interval manners and doyle manners and doyle 2008 presented that upstream conditions can alter the formation of wood jams moreover in rivers where wood is actively mobile wood supply more dominantly controls wood deposition than local environmental conditions e g water flow and channel bed additionally the supply of large wood is more dependent on flood events piegay 1993 pettit et al 2005 in this case a large water flow and sediment transport can cause the generation of several large woods such a large wood jam can generate another large wood jam due to wood collision the deposition patterns are also dependent on the density of large wood this study employed a smaller density for large wood than other experiments e g welber et al 2013 bertoldi et al 2014 davidson et al 2015 we employed the dimensional ratio between the wood profile and flume size together with the hydraulic conditions for generation in an alternating bar based on the size of the wood pieces we adjusted the density of the wood piece 620 kg m3 to allow both the generation of a wood jam and single deposition if the density of the wood piece increases deposition may become considerably stabilized ruiz villanueva et al 2016c costigan et al 2015 chen et al 2019 and the number of stored wood pieces can increase thus this can accelerate the formation of wood jams on the contrary a smaller density of wood pieces relatively increases single wood deposition compared to wood jams as the deposition of large wood becomes unstable in run3 the proportion 0 50 of rootwad was similar to the value 0 57 of exp2 on the contrary in run1 the final proportion of the wood pieces was larger 0 79 than that on exp1 0 57 because many wood pieces without rootwad flowed downstream due to a wood jam collapse which could have been caused by wood collision this result is likely more connected to wood collision rather than to the overestimated rootwad effect if the wood collision is not used in the simulation wood overlapping occurs fig 5 c which then unrealistically captures other wood pieces accordingly the rootwad effect cannot be indicated as shown in runs 3 and 6 the final proportions are 0 5 fig 8 in exp1 and 2 we considered the rootwad size based on kang and kimura kang and kimura 2018 and the rootwad effect had a smaller effect on large wood deposition than that in the experiments of kang and kimura kang and kimura 2018 this is because the rootwad size was insufficient to decrease the draft of a wood piece in the moveable channel bed for a fixed flatbed with a constant slope see kang and kimura kang and kimura 2018 pieces with rootwad are more practical for allowing the wood deposition than that in a moveable bed even if it has the same size and the water flow velocity is large in contrast on a moveable bed rootwad increases the flow velocity resulting in sediment transport around it such that the elevation of the contacted bed decreases and becomes separated from the channel bed thus a moveable bed decreases the rootwad effect more than a fixed bed 4 3 model limitation in terms of bed change piping and tunnel erosion by large wood for an elaborate discussion on the experimental data linked with modeling additional discussion about bed changes by wood deposition together with wood collision is described here piping and tunnel erosion by wood deposition with collision were observed in the experiments initially piping erosion around the trunk was generated as shown in fig 10 the laterally deposited wood pieces can be significantly subjected to water flow owing to the large projection area therefore large wood can significantly obstruct the water flow at this time most of the water flows occurred along the trunk wise direction certain water flows moved between the bed surface and wood piece fig 10 a 1 at the channel bed which consisted of the bedload and had a small gap due to its porosity on the contacted bed with the wood piece such water flow can cause small piping erosion similar to seepage flow fig 10 a 2 moreover this water flow can disturb the deposition of the wood pieces degrading the channel bed as well as yielding an increase in the water depth on the contacted bed abbe and montgomery 1996 cherry and beschta 1989 thus wood pieces can gradually move downstream the size of piping erosion can locally be developed because of increased shear stress jensen et al 1990 on the contacted bed due to the increased water flow fig 10 a 3 thus this process finally becomes tunnel erosion fig 10 a 4 herein the wood piece gradually moved downstream exhibiting a floating motion this was because the water depth and flow velocity were sufficient to move the wood piece moreover when the wood piece was contacting other deposited wood pieces or step shapes of the channel bed it consistently kept floating cherry and beschta cherry and beschta 1989 also observed this type of erosion they referred to it as scour erosion in their experiment showing that a laterally deposited wood piece against the streamwise direction tends to produce more localized scour than the streamwise deposition of wood pieces on the channel bed similarly our study shows that laterally deposited wood can cause more such erosion abbe and montgomery abbe and montgomery 1996 also interpreted that this erosion can lead to pool and bar formation this is because wood initiates the formation of a stable bar apex and meander jams that alter the local flow hydraulics and thereby the spatial characteristics of scour and deposition such erosion by large wood can create pools locally and this helps in developing diverse habitats inoue and nakano 1998 aumen et al 1990 and rehabilitating habitats aumen et al 1990 bilby and ward 1991 thus this phenomenon can be an important factor for ecosystem management however the present model could not reproduce such erosion piping and tunnel this is because these types of erosion are caused by a three dimensional flow structure while the present model was a two dimensional plane model if such erosions need to be simulated a three dimensional model is necessary moreover the minimum gap between the wood and channel bed should be parameterized in the simulation to trigger piping erosion as shown in fig 10 a 1 see jensen et al 1990 otherwise it is difficult for piping erosion to occur in the simulation because the seepage flow between the wood and channel bed does not indicate when the gap is too small 5 conclusions large wood behavior on a moveable bed was investigated through laboratory experiments and computational models the experiments showed diverse interactions between the channel bed and large wood in particular the simulations showed the effect of the time interval of the wood supply together with wood collisions the main findings of the study are summarized below 1 simulation reproducibility and wood collision in the simulations of runs 1 and 4 with collision the time changes in stored wood showed good agreement with the high correlations r2 0 7 particularly runs 1 and 4 indicated significantly lower rmse values than those without wood collision runs 3 and 6 in terms of the spatial pattern run 1 based on exp1 accurately reproduced large wood deposition however run 4 based on exp2 only partially reproduced the spatial deposition patterns this is because the water depth of run 4 was overestimated resulting in wood pieces more easily flowing downstream in the simulations additionally the motion of wood pieces is more sensitive to water depth than flow velocity more importantly runs 3 and 6 demonstrated that the absence of wood collision can allow wood overlapping which results in a significant number of stored wood pieces thus wood collision can be an important parameter in computational modeling to improve model reproducibility 1 deposition patterns of large wood by main factors the time interval of wood supply rootwad and wood collision the time interval of wood supply can affect the wood storage of the domain in all simulations except for run 3 and 6 when the time interval of wood supply decreased wood storage decreased because wood collision became more active by the supplied floating wood pieces which then collapsed wood jams or disturbed the depositions of other wood however the simulations of both runs 3 and 6 without collision showed unclear patterns due to wood overlapping wood collision can affect the rootwad proportion through wood collapse if the simulation does not reproduce wood collision the rootwad proportion may be unclearly indicated additionally the rootwad effect was not significant on the moveable bed this was because the rootwads increased the flow velocity resulting in sediment transport around it on the moveable bed such that the elevation of the contacted bed decreased and became separated from the channel bed thus it is necessary to increase the rootwad size when considering moveable beds in future work credit authorship contribution statement taeun kang data curtion writing original draft visualization investigation ichiro kimura conceptualization methodology software supervision shinichiro onda software validation writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we thank the japan society for the promotion of science jsps for providing funding for this study 18f18359 supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2021 103912 appendix supplementary materials image application 1 
302,this study aims to analyze model reproducibility on large wood collisions on moveable channel beds flume experiments were conducted to observe the responses of moveable bed change with the behavior of large wood including wood collisions in the experiments we employed time intervals of 5 and 9 s in wood supply which modified the collision patterns subsequently we conducted simulations based on the experimental conditions herein to analyze the wood collision effect in the simulations we employed the dashpot spring model in a large wood dynamics model subsequently we compared the simulations with and without wood collision and discussed the reproducibility of the model the results of the simulations and experiments revealed that the wood pieces exhibited diverse behaviors including floating sliding deposition plane rotation rolling collision remobilization jam formation and causing local bed changes in particular the results indicate that model reproducibility is dependent on wood collision because this can actively alter wood deposition patterns we also described the piping and tunnel erosion processes linked to wood collision and moveable beds and suggested the corresponding suitable computational treatment this study presents an experimental and modeling methodology on wood collision and the interaction between bed change and large wood thus useful information for solving numerical difficulties linked to large wood dynamics including wood collisions is provided keywords flume experiments with large wood 2 d flow and large wood models large wood deposition on moveable bed large wood collision model 1 introduction in the past few decades studies on large wood length 1 m and diameter 0 1 m have been conducted swanson et al 2020 through field observations e g the queets river in the u s a abbe and montgomery 1996 the drôme river in france piégaya and gurnell 1997 the oyabu basin in japan haga et al 2002 and the tagliamento river in italy gurnell et al 2002 bertoldi et al 2013 flume experiments e g braudrick et al 1997 braudrick and grant 2001 welber et al 2013 bertoldi et al 2014 kang and kimura 2018 schalko et al 2020 and numerical computations e g ruiz villanueva et al 2014 ruiz villanueva et al 2020 v ruiz villanueva et al 2014 persi et al 2019a persi et al 2019b kimura and kitazono 2019 kang et al 2020 studies on large wood have been advanced and documented by several countries e g the u s a japan and european countries through the investigation of the large wood effect and environmental changes in natural systems in the initial stages of research on large wood braudrick et al braudrick et al 1997 and braudrick and grant braudrick and grant 2001 established the basic framework for methodologies on large wood dynamics using flume experiments braudrick et al braudrick et al 1997 conducted diverse experiments on wood transportation in rivers and classified the jamming types as uncongested congested and semi congested additionally braudrick and grant braudrick and grant 2001 conducted flume experiments by assuming that the incipient motion of wood occurs when it begins to slide examining the parameter interactions among hydraulics channel formation travel distance and spatial deposition furthermore bocchiola et al bocchiola et al 2006 investigated large wood motions through tilt testing measurements of the friction angle of each large piece of wood on a fixed channel bed under dry conditions they tested the critical angles that initiate wood motion between the trunk direction and flow direction most studies on large wood have assumed the shape of large wood as cylindrical within their experiments these studies e g braudrick et al 1997 braudrick and grant 2001 bocchiola et al 2006 have also considered that partially submerged wood on a channel bed is subject to drag and friction additionally the cylindrical shape of large wood was examined through a hydrodynamic response in an open channel flow to investigate wood motion in the horizontal orientation as well as the submerged volume and area affecting the drag force the drag force that interacts between large wood and water flow is an important parameter in understanding wood motion several studies e g persi et al 2019a gippel et al 1992 shields and alonso 2012 have examined the changes in the drag coefficient with respect to the orientation angles and cylindrical shape of the wood in addition these studies assessed the drag force of natural wood in rivers gippel et al gippel et al 1992 examined the drag coefficient of the cylindrical shape and the angle between water flow directions and trunk wise flow shields and alonso shields and alonso 2012 assessed the drag force of real wood in a natural river and discussed changes in the drag force for wood profiles such as orientation and submerged volume ruiz villanueva et al ruiz villanueva et al 2014 ruiz villanueva et al 2020 ruiz villanueva et al 2014 ruiz villanueva et al 2016a ruiz villanueva et al 2016b ruiz villanueva et al 2016c contributed a significant amount of information on large wood transport and interacting river systems through various studies on the laboratory experiments and numerical simulations of the dynamics of large wood motion and channel evolution they also observed and analyzed field data related to the transport of large wood depending on wood characteristics and river flow through another approach several numerical simulations for wood dynamics were developed based on a number of flume experiments and observations e g haga et al 2002 braudrick et al 1997 braudrick and grant 2001 welber et al 2013 bertoldi et al 2014 ruiz villanueva et al 2016a ruiz villanueva et al 2016b ruiz villanueva et al 2016c davidson et al 2015 in particular two dimensional 2 d hydrodynamic flow models based on eulerian and lagrangian modeling were proposed to model large wood transport examples of such models are the iber wood model which considers the forces balance between water flow and large wood ruiz villanueva et al 2014 ruiz villanueva et al 2020 the two dimensionally connected spheres model which simulates the bed deformation by large wood kang and kimura 2018 kang et al 2020 and orsa2d wt which calculates the multiple local forces from the four segments of a cylindrical wood body persi et al 2019a persi et al 2019b to simulate wood motion such simulation models have enhanced our understanding of wood dynamics transport and motion however considering the present computational methods large wood modeling is still significantly challenging owing to channel bed deformation swanson et al 2020 in particular moveable bed channels such as braiding network formation and alternating bars can actively show the large fluctuations in bed evolution and drive wood dynamics this increases the complexities of the interaction among water flow bed change and large wood consequently more sophisticated modeling that can precisely reproduce such complexities is required to understand wood transport as for bed deformation studies with large wood flume experiments have been actively conducted welber et al 2013 bertoldi et al 2014 welber et al welber et al 2013 and bertoldi et al bertoldi et al 2014 showed that the bed morphology drove wood transport and deposition in a braided multi thread river reach with shallow flows where they classified deposition patterns via several parameters such as wood supply the rootwad effect and jam formation therefore they provided useful data for understanding the relationship between braiding channels and large wood deposition the authors showed that bar formation could strongly affect wood deposition patterns in a braiding network these studies focused on partially submerged braiding channels there is a need to consider different channel geometries in a submerged moveable bed to understand the diverse morphodynamics of large wood in particular sand bar formation such as alternating bars and mid channel bars is important in fluvial bed deformation so far explorations of sand bars and related large wood behavior patterns in numerical simulations based on laboratory experiments are scarce moreover wood collision has been commonly shown in the above experimental studies and it also plays a role in collapsing and forming wood jams accordingly wood collision is also a significant factor in large wood deposition simulating physical wood collision is one of the difficulties of fluid and wood dynamics because simulating the collision motion requires costly cpu time additionally a collision model is a mechanical approach based on the vector of solid particle motion which is different from water pressure thus model complexity increases further kimura and kitazono 2019 however if wood collision is employed in large wood modeling the model would better reproduce the formation and collapse of wood jams kang and kimura 2018 kimura and kitazono 2019 kang et al 2020 the main aim of this work is to increase our understanding of the computational methodology using large wood dynamics model to reproduce the wood motion on a moveable bed as well as wood collision for this we conducted a study based on flume experiments and numerical simulations and we examined the behavior of large wood including wood collision and bed deformations on a moveable bed we carried out a comparison of the experimental and simulation results to understand the limitations of the large wood dynamics model with respect to a moveable bed which is one of the challenges associated with the computations of wood transport we then discuss the diverse deposition patterns including wood collision and bed deformations this study provides useful data on several phenomena such as the interaction between a wood jam or wood transported in a congested regime on a moveable bed in addition we provide a framework for applying our methodology to numerical and experimental methods 2 methodology 2 1 experimental and computational setup and bed profiles to ensure the simplicity of the model we did not consider a prototype scale of the natural river and large wood instead we arbitrarily decided on the dimensions of the experimental configurations e g flume and flow discharge and grain size which were considered for generating alternating bars particularly we considered the criteria for conditions of the bar formations no bar alternating bar braiding channel according to the given parameters such as the channel slope channel width grain size uniform water depth and non dimensional tractive force proposed by kishi and kuroki kishi and kuroki 1973 based on this criterion we determined the initial water depth 0 01 m using channel width 0 4 m and the initial flatbed with a constant slope of 0 005 m m those parameters showed the acceptable non dimensional tractive force 0 059 which can generate the alternating bar in the criterion for conditions of the bar formations kishi and kuroki 1973 herein the inlet flow discharge was 1 1 l s note that the minimum and maximum values of the available inlet flow discharge of the pump were 0 016 and 10 l s respectively additionally the size of the flume was 12 m 0 4 m fig 1 a b and the height of the sidewall of the flume was 0 3 m the initial sand bed layer was 5 cm and the uniform grain size was 0 51 mm at the upstream section of 0 m in the flume we continuously supplied sediment to maintain the same initial upstream elevation every 90 s under those conditions an alternating bar was able to be generated in 5 h note that an alternating bar can also be generated under a larger inlet flow discharge than 1 1 l s but most wood pieces in pre tested experiments showed the floating motion in such a larger inlet flow discharge two experiments hereafter exp1 and exp2 were conducted for each experiment wood pieces were supplied after the generation of the alternating bars the time intervals of the wood supply were 5 s exp1 and 9 s exp2 this was because the time intervals of wood supply change the collision patters of wood pieces clearly showing the wood collision effect all flume experiments were conducted under the same hydraulic conditions as listed in table 1 four bed profiles were gauged depending on exp1 before after wood supply and exp2 before after wood supply we first gauged bed elevation profiles within 3 and 10 m fig 1 a in both exp1 and exp2 after generating an alternating bar then we gauged the bed elevation again after the wood supply the reason for bed elevation measuring to within 3 0 m of the flume is for the wave effect by the wood input and sediment supply that occurs with 0 3 m of the flume which slightly changes the phase of sediment transport and wood motion patterns sometimes small backwater waves that form due to the step shape at the end of the flume see the longitudinal flume domain in fig 1 a slightly affect the wood deposition patterns within 10 12 m flume thus only the first 3 10 m of the flume was considered thus this phenomenon could be disregarded the numerical simulations were carried out in this study to quantify the channel evolution by large wood deposition and analyze the reproducibility of the wood collision under the bed deformation herein we used measured bed elevations for the simulations which were measured before the wood supply the resolutions x and y of exp1 and exp2 were 0 001 0 02 m2 and 0 03 0 01 m2 respectively as the measuring device was taken and replaced from a point gage at exp2 to a laser profiler at exp1 the grid size of the computational domain was 0 02 0 02 m2 the number of stored wood pieces within this gaging section 3 10 m was counted see fig 1 a c 2 2 wood characteristics we considered the dimensional ratio between the wood profile and flume size together with the hydraulic conditions e g grain size and water depth as we first considered generating the alternating bar flow discharge was the most important factor based on this flow discharge and uniform water depth we considered the critical draft for wood motion cdm which is the minimum water depth to have a floating motion kang and kimura 2018 the length and diameter of the piece of large wood were selected as 0 1 and 0 01 m respectively because we considered the ratio between the flume width and the length of the wood piece as the medium river scale presented by kramer and wohl kramer and wohl 2017 herein the medium river scale considering large wood means that the channel width stem length of large wood is 2 4 or the water depth diameter of large wood is 2 4 based on this scale kang and kimura kang and kimura 2018 also considered a ratio of 1 3 for their river scale this study similarly considered a ratio of 1 4 for this the density of the wood piece was adjusted to 620 kg m3 which allowed us to generate both wood jams and single deposition kang and kimura kang and kimura 2018 experimented with the large wood motions using a wood piece model with rootwad in their work the cross shaped rootwad was based on bertoldi et al bertoldi et al 2014 experiment the diameter of the rootwad was arbitrarily determined as 0 02 m which is a larger size than the stem diameter of the wood piece to examine the rootwad effect based on kang and kimura kang and kimura 2018 this study also considered the diameter of the rootwad modeled as a cross shape was 0 02 m as shown in fig 2 the deposition of this cross shape varied depending on the large wood deposition if the deposition happened at a orientation mostly due to sliding motion and deposition the diameter of the rootwad was assumed to be 0 02 m if the deposition happened at an orientation mostly due to deposition the diameter was assumed to be 0 014 m thus while considering both deposition types in the numerical simulation the averaged rootwad particle diameter was assumed to be 0 017 m 2 3 wood supply characteristics in the experimental and computational cases in the flume experiment the initial angle of the wood pieces was set to 90 against the streamwise direction to permit easy downstream flow these settings also prevent the formation of a wood jam at the wood supply zone the wood piece supply zone in the flume experiment was set to 1 5 m from the upstream of the flume to minimize the generation of a wave by the wood supply fig 1 thus the wood pieces were manually supplied into the wood supply zone in contrast in the computations the generation zone circle shape of the wood pieces was set as shown in fig 1 d in the simulation we assumed that the initial angle of the wood pieces was determined by the gradient of the flow vectors in the transversally neighboring two cells around the center position of the wood piece see gradient between flow vectors of fig 1 d fig 1 d shows how wood pieces were generated in an assigned cross section 0 1 m from upstream for instance when the maximum flow velocity occurred in the center part of the assigned cross section the wood piece was generated in this cell the angle of the wood piece was that of by the gradient of the flow vectors at the center position of the wood piece in the transversally neighboring two cells using grid numbers j and j 1 in the transversal direction in the upstream area the upstream cross section of the computational domain of the simulations the non uniform flow velocity was generated in the transversal direction because the inlet flow velocity in the upstream segment was dependent on the water depth and bed elevation at the upstream cross section kang et al 2020 accordingly the time changes in the upstream flow velocity were predominantly the same in all the simulations this wood generation method showed a deterministic pattern dependent on the water flow rather than the unpredictable random generation method thus it reduced the simulation cases by eliminating the random factor the wood pieces were alternately supplied with and without rootwad we arbitrarily determined the number of wood pieces to be 200 100 rootwad and 100 no rootwad in all experimental cases table 2 lists the scenarios of the simulations there were six runs conducted in this study runs1 3 used the bed profile of exp1 and runs4 6 used exp2 the wood supply time intervals were 5 s runs1 3 and 5 and 9 s runs2 4 and 6 the dashpot spring model simulations were conducted in runs1 2 4 and 5 to consider the wood collision effect 2 4 computational method for the numerical simulation nays2dh shimizu et al 2014 in the international river interface cooperative iric software international river interface cooperative iric 2020 was used to simulate the flows and bed morphology nays2dh is based on the grid eulerian computational domain and the finite difference method to solve the shallow water equation and the 2 d exner equation in particular nays2dh uses the generalized curvilinear coordinate to fit the computational domain into a curved river shape and reproduce the channel evolution e g channel widening meandering the water flow calculation of nays2dh was conducted by employing the third order total variation diminishing monotonic upstream centered scheme for conservation laws tvd muscl to solve the advection of water flow herein turbulence flow was calculated by a zero equation type model for the sediment transport the ashida and michiue ashida and michiue 1972 formula was used to estimate the bedload flux in the streamwise and transversal directions and a bank collapse model based on hasegawa hasegawa 1985 was employed angle of repose 30 the manning roughness coefficient was 0 013 s m1 3 which was estimated using the manning strickler formula from the uniform sediment diameter of 0 51 mm in this study the large wood dynamics model was employed to simulate the motions of large wood in the water flows and for interacting with the channel bed in the large wood dynamics model the motion of the large wood is expressed as a series of spheres and the collision of each sphere is evaluated with the spring and dashpot model like the discrete element method dem approach cundall and strack cundall and strack 1979 in the model the motion of each sphere composing a piece of large wood is calculated with a 2d lagrangian equation in a stepwise manner at each time step the locations of spheres should be rearranged to form the shape of large wood a straight line in this case this method is similar to the moving particle semi implicit mps method simulating a rigid body in a flow proposed by koshizuka koshizuka 1997 the large wood dynamics model reproduces the floating sliding settling collision rotation and rolling motions kang and kimura 2018 kang et al 2020 kimura and kitazono 2019 herein the second order adams bashforth scheme was used to calculate the advection of wood particles moreover this model considers both the change in the draft for wood motion and bed friction force to consider the sliding and settling motion this large wood dynamics model was coupled with nays2dh e g kang and kimura 2018 kang et al 2020 note that the detailed methods of interaction processing between nays2dh and the large wood dynamics model were described in the report by kang and kimura kang and kimura 2018 in addition nays2dh along with the large wood dynamics model simulates the effect of large wood deposition for a changing bedload flux in sediment transport on the exner equation to consider sediment flux by large wood deposition we employed the area occupation ratio σ p of particles against one grid cell acell the occupation ratio σ p increases when wood particles are deposited on the cell area acell the rate of bed elevation change increases for the same amount of sediment flux change if the occupation ratio increases we used a modified exner equation kang et al 2020 for evaluating the bed elevation change 1 q b x x q b x x δ x δ x q b y y q b y y δ y δ y δ ζ δ t 1 λ s 1 σ σ p where qbx and qby denote the sediment discharge flux in the x and y directions respectively δt is the time step δζ is the bed elevation change at δt δx and δy are the cell lengths in the x and y directions respectively λ s is the soil porosity 0 4 and σ p is written as 2 σ p a p δ x δ y a p a c e l l where ap is the total occupation area of the wood particles in one grid cell note that the occupation ratio σ p is not applied to the calculation of water flow instead when the wood particles deposit on a grid cell drag force by the large wood is calculated and it was applied to the water flow calculation of nays2dh called the two way type model for interaction between water flow and large wood a large amount of large wood within a computational cell can decrease the flow velocity by drag force resulting in the increase of water depth we also employed parallel computation openmp in nays2dh and the large wood dynamics model including wood collision to reduce the cpu time the basic equations for large wood dynamics are described in several previous studies kang and kimura 2018 kimura and kitazono 2019 kang et al 2020 2 5 dashpot spring model for computational wood collision the soft spherical particle model for granular flows was initially proposed by cundall and strack cundall and strack 1979 the dashpot spring model is a type of soft spherical particle model it calculates a non linearly incremented collision force depending on the overlap distance between particles based on hertzian contact theory this dashpot spring model was used in the large wood dynamics model this collision model was applied to the particle motion in a two dimensional plane the dashpot is a damper that resists motion via an elastic modulus the resulting force is proportional to the velocity but acts in the opposite direction slowing the motion and absorbing energy the spring is a reflection force that acts to resist displacement to apply such a wood collision system we assumed that the particles of wood pieces are soft spheres young s modulus 0 689 gpa the model of particle motion by collision dashpot spring is widely known in this field e g gotoh et al 2013 kang et al 2019 such that we omit a description of the equations 2 6 particle in cell method for collision detection in the computation in terms of the solid collision model detecting particle contact is dependent on a significantly small computational time step this is because the distance between all the particles of the wood pieces must be estimated to detect the wood collision in the lagrangian method and this estimation requires a lot of cpu time the particle in cell pic method together with a potential collision area was employed to overcome this difficulty fig 3 illustrates the schematic domain for the pic and potential collision area as a first step all wood pieces were assigned with the address of the sub grid cell which is larger than the water flow grid cell to reduce the required computational memory after this process of address assignment the computer can recognize the position of the wood pieces and effectively determine the cell in which wood collision occurs when more than two wood pieces are located in a cell the distances among the center positions of all the wood pieces are calculated if the calculated distance between the wood pieces is smaller than the potential distance the collision between every particle of those wood pieces is calculated by considering the distance between the center positions of the particles here the potential distance was assumed to be the length of each wood piece because the motion velocity of the wood pieces was smaller than the length of the wood at all time steps this process effectively reduces the cpu time for collision detection 3 results 3 1 final patterns of the channel bed and large wood deposition in the experiments and simulations fig 4 shows the experimental results for the wood with and without rootwad deposition patterns and channel variation profile along the flatbed with a constant slope additionally the red and blue rectangular shapes indicate the wood pieces with and without rootwad respectively fig 4 indicates the results of exp1 and exp2 showing that the wood jams occurred at the bar crest or mid channel bar where the water depth was lower than that of the cdm 0 006 m additionally fig 4 b indicates the wood jams i e the sections at 4 6 and 9 10 m and in fig 4 d the sections at 6 7 and 9 10 m all simulation results fig 5 show deposition patterns on the bar crest or mid channel bar a larger flow velocity 0 35 m s occurred near the wall adjacent to the alternating bar in all the simulations a smaller flow velocity 0 1 m s occurred near the location of the wood jam where the water depth was smaller than the cdm 0 006 m considering the deposition section in the simulation cases of runs1 3 most of the wood jams formed within the section of 4 6 m in the computational domain fig 5 the sections of 9 10 m showed wood jams of smaller sizes as shown in runs1 and 2 similarly exp1 fig 4 b shows that the main wood jam was in the section of 4 6 m however run3 without wood collision shows that all wood pieces deposited at 4 6 m in the computational domain and the stored wood pieces were significantly larger than those in runs1 and 2 the simulated result of run4 shows differently distributed wood jams the primary wood jam sections were between 7 8 m and 9 10 m which was slightly different from those in the experimental results 6 7 m and 9 10 m run5 shows that most wood pieces deposited between 9 and 10 m this is because water depth was slightly overestimated compared to the cdm 0 006 m at 6 7 m moreover wood collision disturbed the deposition of wood pieces in this section thus wood pieces moved and then deposited after 7 m where the water depth was lower than the cdm run6 shows several stored wood pieces at 5 6 m this is because the absence of wood collision in the simulation allowed an overlap of wood at 5 6 m and then caused a stable deposition because the overlapping wood pieces significantly decreased the flow velocity and water depth the results of run 3 show the same overlapping and water flow change those results imply that the absence of wood collision has a higher impact than an overestimated water depth table 3 shows the final number of stored wood pieces the experimental results show that the final number of stored wood pieces during 5 s exp1 and 9 s exp2 were 34 and 57 respectively the final number of stored wood pieces in the simulations runs1 6 were 29 33 199 41 18 and 188 respectively herein the cases without wood collision showed significantly larger values than those with collision fig 6 indicates the simulated bed deformations from the gauged bed elevation exp1 and 2 fig 4 a d shows the channel variation along with the channel slope in contrast fig 6 shows the channel variation by the generated bar clearly expressing bed deformation together with the wood pieces the black particles on the domain in fig 6 show the center position of the wood pieces additionally the colors of erosion 0 m and deposition 0 m were narrowed to 0 01 m in this figure here fig 6 shows that most of the wood jams were located in the area where there was a higher elevation 0 m with a lower water depth 0 006 m additionally fig 6 indicates local scour near the wood jams these patterns are the same as those of the experimental results fig 4 3 2 effect of time interval on wood supply fig 7 shows the evolution in the number of stored wood pieces in total a count of nine for the wood pieces was obtained at an interval of 300 s in both exp1 and exp2 during the wood supply period the number of stored wood pieces actively increased in some simulations runs1 2 4 and 5 and all experiments but decreased after the end of the wood supply however runs3 and 6 showed a significant increase of stored wood pieces with wood piece overlapping fig 5 excluding no collision cases runs3 and 6 all of the experiments and simulations with the larger time interval of 9 s i e runs2 and 4 resulted in a larger number of stored wood pieces than in the case of the 5 s interval i e runs1 and 4 as shown in fig 7 3 3 time changes in the rootwad proportion fig 8 illustrates the changes in the proportion of rootwad with time where we compare the effect that rootwad has on wood storage if the value of the proportion is one all the stored wood pieces have rootwad whereas at the other extreme when the proportion is zero none of the stored wood pieces has rootwad the experimental cases fig 8 showed a proportion of 0 5 from 300 s to the final time step 2400 s run1 showed a significantly increased value after wood supply 1000 s with a larger proportion value 0 75 at the final time step because the wood jam had collapsed at approximately 1000 s and several wood pieces without rootwad flowed downstream the final proportions of rootwad in the simulations runs1 6 were 0 79 0 66 0 50 0 50 0 61 and 0 51 respectively the runs without wood collision runs3 and 6 showed a convergence of rootwad proportion to 0 5 the cases for exp1 and 2 showed final values of 0 58 and 0 57 respectively 3 4 quantification of channel deformation patterns the bed relief index bri hoey and sutherland 1991 was calculated using the bed elevation of all the simulation results to compare the channel patterns kang et al 2020 kang et al 2018 if bri 0 the channel bed is a flatbed in all cross sections if bri 0 the channel bed shows uneven bed elevations kang et al 2018 indeed bri also means the averaged value of the mean square deviations on bed elevation in all cross sections thus bri effectively shows the magnitude of channel evolution if bri increases the channel bed shows an active deformation resulting in more extensive erosion on the contrary if bri decreases the channel bed shows an inactive deformation with local erosion moreover bri increasing implies a larger rate of sediment output than sediment input and bri decreasing shows a smaller sediment output than sediment input hoey and sutherland 1991 using the bri the channel patterns in the longitudinal direction were not considered in this direction the local channel slope dramatically increased the complexity of the bri patterns which reduced the readability of the graph fig 9 shows the range of temporal changes in the bri during all simulation times the initial values of the bri 10 5 m2 were 4 9 runs1 3 and 5 84 runs4 6 additionally the final values of the bri 10 5 m2 were 4 23 4 21 3 99 4 31 4 04 and 4 13 respectively the mean values runs1 6 of the bri were 4 46 4 45 4 35 4 61 4 56 and 4 54 respectively showing that the mean values of the bri in runs4 6 were greater than the values in runs1 3 moreover runs without wood collision runs3 and 6 clearly showed lower mean bri values than those with collision cases in each bed profile 3 5 model reproducibility of the large wood deposition number the reproducibility of the simulations depending on the collision model and the time interval of wood supply was evaluated using the coefficient of determination r2 and root mean square error rmse herein the compared values were time changes in the number of stored wood pieces the simulation results with the same time interval and bed profile conditions were selected runs1 3 4 and 6 from fig 7 eight values excluding 0 s were extracted from the values of the time changes in the stored wood pieces in the simulations as data were obtained by counting the wood pieces at a time interval of 300 s then these values were used in the estimation of the two evaluation coefficients table 4 indicates the estimated reproducibility of the simulations in exp1 and the simulations runs1 and 3 the r2 values were 0 820 and 0 781 respectively which showed a high correlation between the simulation and experiment similarly exp2 and the simulations runs4 and 6 showed high correlations 0 924 and 0 935 this was because the time changes in the stored wood pieces were significantly dependent on the wood supply the values of the rmse for all target runs 1 3 4 and 6 were 5 301 133 976 12 297 and 89 71 respectively accordingly this indicates that the wood collision effect had a higher reproducibility than no wood collision therefore the simulation results with wood collision reproduced the experimental results in terms of the number of stored wood pieces based on the time changes in particular r2 showed high correlation values 0 7 and the rmse was much smaller than in the cases without collision this indicates that the present model considering wood collision well reproduced the tendency of time changes in the number of stored wood pieces in the computational domain 4 discussion 4 1 alternating channel with large wood deposition in experiments and simulations we performed simulations considering a sediment transport module able to reproduce the formation of alternating bars and the simulation of the transport and collisions of wood pieces in fig 4 exp1 and exp2 show that the wood jams were located near the bar crest where the water depth was lower than that of the cdm in the simulation of run1 based on exp1 the model reproduced the time changes in the number of stored wood pieces see table 4 and deposition location figs 4 and 5 well although the time changes in stored wood were reproduced well r2 0 7 in run 4 which is based on exp2 the model reproduced the different locations of wood jams as shown in figs 5 and 6 the primary wood jam section was between 7 8 m and 9 10 m which was different from that in the experimental result 6 7 m and 9 10 m this was because the water depth was slightly overestimated compared to the cdm 0 006 m at 6 7 m and the motion of wood pieces was more sensitive to the cdm than to the flow velocity thus wood pieces flowed more in the simulation not only in run4 but also in run5 so that they deposited after 7 m where the water depth was lower than the cdm in particular the wood deposition results of all simulations of both runs3 and 6 without wood collision were significantly different from the experimental results this is because the absence of wood collision allows wood overlapping resulting in an easy deposition of many wood pieces decreasing the flow velocity due to large drag forces moreover those wood pieces can decrease the water depth near the downstream side to capture more wood pieces note that the wood overlapping means that a particle of a wood piece invades into the inside of a particle of another wood piece in the simulations the wood collision in the simulations and experiments caused a significant interaction between the wood pieces 1 when the first wood piece deposits on the bar crest caused by a lower water depth than that of the cdm and when a second wood piece also deposits on the same location these two wood pieces may cause a collision at this time the first wood piece may flow downstream if its depositional area has a larger water depth than that of the cdm or it may laterally deposit otherwise if the depositional area still has a lower water depth and the first wood piece stream wisely deposits the first wood piece can deposit on the bed together with the second wood piece without contacting each other herein the first wood piece disturbs the motion of the second wood piece at the contacted point between both wood pieces such that the second wood piece is rotated by the water flow into a lateral direction against the streamwise direction 2 the second wood piece rotates at the contacted point during the second wood piece rotation the first wood piece indicates a slow sliding motion toward the downstream but the moving distance of the second wood piece is relatively smaller than that of the first wood piece 3 then the first wood piece is finally deposited in the lateral direction after contacting the second wood piece this process shows the formation of a jam on the bar crest sometimes if the first wood piece flows downstream only the second wood piece will remain on the bed due to additional wood collision and water flow 4 those wood pieces may become a wood jam and the wood jam can consistently decrease the flow velocity in the depositional area where the sediment deposits thus the channel bed gradually increased in this area abbe and montgomery 1996 in contrast the flow velocity increased around the wood jams narrowing the channel width such that the increased velocity caused erosion consequently the bar crest was changed into a mid channel by this process as shown in fig 4 b and d fig 9 shows the bri ranges which indicate that the bed deformation by large wood occurred locally if large wood actively changes the whole channel bed e g generating a large thalweg the bri should increase but the final values of the bri in all the cases decreased as shown in fig 9 moreover the experiments and simulations figs 4 6 show the final channel patterns with local bed deformations local scours and depositions which were generated near the wood jams and deposited large wood the wood jam changed the bar migration patterns bar length as shown in figs 4 and 6 the speed of bar migration showed an unclear pattern in the entire domain but the wood jams caused local erosion near the deposit location consequently the effect that the wood jams have on bar migration was not dominant in the entire domain but it locally affected bed deformation channel geometry influences jam formation and single large wood deposition abbe and montgomery 1996 kang et al 2020 cherry and beschta 1989 4 2 large wood deposition by wood supply wood collision rootwad and density fig 7 shows the time dependent changes in the number of stored wood pieces the experimental results exp1 and exp2 and all simulations show that the wood supply time interval controlled the number of stored wood pieces if the time interval of the wood supply increased there was an increase in deposited wood pieces in other words the time interval of the wood supply activated wood collisions for the floating wood pieces except for no wood collisions runs3 and 6 all simulations also reproduced such depositional patterns as the number of floating wood pieces grew due to the shorter time interval they actively caused wood collisions however the absence of wood collisions can cause wood overlapping which results in a non physical deposition with a larger number of wood pieces it also changes the sediment transport by large drag forces which are caused by the deposited wood pieces thus wood collision can be an important parameter when considering the transport of wood and sediment in the simulation in particular this parameter should be carefully considered in the dramatic transport of wood such as woody debris during flood events in the river system we observed such phenomena related to the wood supply and wood collision time interval manners and doyle manners and doyle 2008 presented that upstream conditions can alter the formation of wood jams moreover in rivers where wood is actively mobile wood supply more dominantly controls wood deposition than local environmental conditions e g water flow and channel bed additionally the supply of large wood is more dependent on flood events piegay 1993 pettit et al 2005 in this case a large water flow and sediment transport can cause the generation of several large woods such a large wood jam can generate another large wood jam due to wood collision the deposition patterns are also dependent on the density of large wood this study employed a smaller density for large wood than other experiments e g welber et al 2013 bertoldi et al 2014 davidson et al 2015 we employed the dimensional ratio between the wood profile and flume size together with the hydraulic conditions for generation in an alternating bar based on the size of the wood pieces we adjusted the density of the wood piece 620 kg m3 to allow both the generation of a wood jam and single deposition if the density of the wood piece increases deposition may become considerably stabilized ruiz villanueva et al 2016c costigan et al 2015 chen et al 2019 and the number of stored wood pieces can increase thus this can accelerate the formation of wood jams on the contrary a smaller density of wood pieces relatively increases single wood deposition compared to wood jams as the deposition of large wood becomes unstable in run3 the proportion 0 50 of rootwad was similar to the value 0 57 of exp2 on the contrary in run1 the final proportion of the wood pieces was larger 0 79 than that on exp1 0 57 because many wood pieces without rootwad flowed downstream due to a wood jam collapse which could have been caused by wood collision this result is likely more connected to wood collision rather than to the overestimated rootwad effect if the wood collision is not used in the simulation wood overlapping occurs fig 5 c which then unrealistically captures other wood pieces accordingly the rootwad effect cannot be indicated as shown in runs 3 and 6 the final proportions are 0 5 fig 8 in exp1 and 2 we considered the rootwad size based on kang and kimura kang and kimura 2018 and the rootwad effect had a smaller effect on large wood deposition than that in the experiments of kang and kimura kang and kimura 2018 this is because the rootwad size was insufficient to decrease the draft of a wood piece in the moveable channel bed for a fixed flatbed with a constant slope see kang and kimura kang and kimura 2018 pieces with rootwad are more practical for allowing the wood deposition than that in a moveable bed even if it has the same size and the water flow velocity is large in contrast on a moveable bed rootwad increases the flow velocity resulting in sediment transport around it such that the elevation of the contacted bed decreases and becomes separated from the channel bed thus a moveable bed decreases the rootwad effect more than a fixed bed 4 3 model limitation in terms of bed change piping and tunnel erosion by large wood for an elaborate discussion on the experimental data linked with modeling additional discussion about bed changes by wood deposition together with wood collision is described here piping and tunnel erosion by wood deposition with collision were observed in the experiments initially piping erosion around the trunk was generated as shown in fig 10 the laterally deposited wood pieces can be significantly subjected to water flow owing to the large projection area therefore large wood can significantly obstruct the water flow at this time most of the water flows occurred along the trunk wise direction certain water flows moved between the bed surface and wood piece fig 10 a 1 at the channel bed which consisted of the bedload and had a small gap due to its porosity on the contacted bed with the wood piece such water flow can cause small piping erosion similar to seepage flow fig 10 a 2 moreover this water flow can disturb the deposition of the wood pieces degrading the channel bed as well as yielding an increase in the water depth on the contacted bed abbe and montgomery 1996 cherry and beschta 1989 thus wood pieces can gradually move downstream the size of piping erosion can locally be developed because of increased shear stress jensen et al 1990 on the contacted bed due to the increased water flow fig 10 a 3 thus this process finally becomes tunnel erosion fig 10 a 4 herein the wood piece gradually moved downstream exhibiting a floating motion this was because the water depth and flow velocity were sufficient to move the wood piece moreover when the wood piece was contacting other deposited wood pieces or step shapes of the channel bed it consistently kept floating cherry and beschta cherry and beschta 1989 also observed this type of erosion they referred to it as scour erosion in their experiment showing that a laterally deposited wood piece against the streamwise direction tends to produce more localized scour than the streamwise deposition of wood pieces on the channel bed similarly our study shows that laterally deposited wood can cause more such erosion abbe and montgomery abbe and montgomery 1996 also interpreted that this erosion can lead to pool and bar formation this is because wood initiates the formation of a stable bar apex and meander jams that alter the local flow hydraulics and thereby the spatial characteristics of scour and deposition such erosion by large wood can create pools locally and this helps in developing diverse habitats inoue and nakano 1998 aumen et al 1990 and rehabilitating habitats aumen et al 1990 bilby and ward 1991 thus this phenomenon can be an important factor for ecosystem management however the present model could not reproduce such erosion piping and tunnel this is because these types of erosion are caused by a three dimensional flow structure while the present model was a two dimensional plane model if such erosions need to be simulated a three dimensional model is necessary moreover the minimum gap between the wood and channel bed should be parameterized in the simulation to trigger piping erosion as shown in fig 10 a 1 see jensen et al 1990 otherwise it is difficult for piping erosion to occur in the simulation because the seepage flow between the wood and channel bed does not indicate when the gap is too small 5 conclusions large wood behavior on a moveable bed was investigated through laboratory experiments and computational models the experiments showed diverse interactions between the channel bed and large wood in particular the simulations showed the effect of the time interval of the wood supply together with wood collisions the main findings of the study are summarized below 1 simulation reproducibility and wood collision in the simulations of runs 1 and 4 with collision the time changes in stored wood showed good agreement with the high correlations r2 0 7 particularly runs 1 and 4 indicated significantly lower rmse values than those without wood collision runs 3 and 6 in terms of the spatial pattern run 1 based on exp1 accurately reproduced large wood deposition however run 4 based on exp2 only partially reproduced the spatial deposition patterns this is because the water depth of run 4 was overestimated resulting in wood pieces more easily flowing downstream in the simulations additionally the motion of wood pieces is more sensitive to water depth than flow velocity more importantly runs 3 and 6 demonstrated that the absence of wood collision can allow wood overlapping which results in a significant number of stored wood pieces thus wood collision can be an important parameter in computational modeling to improve model reproducibility 1 deposition patterns of large wood by main factors the time interval of wood supply rootwad and wood collision the time interval of wood supply can affect the wood storage of the domain in all simulations except for run 3 and 6 when the time interval of wood supply decreased wood storage decreased because wood collision became more active by the supplied floating wood pieces which then collapsed wood jams or disturbed the depositions of other wood however the simulations of both runs 3 and 6 without collision showed unclear patterns due to wood overlapping wood collision can affect the rootwad proportion through wood collapse if the simulation does not reproduce wood collision the rootwad proportion may be unclearly indicated additionally the rootwad effect was not significant on the moveable bed this was because the rootwads increased the flow velocity resulting in sediment transport around it on the moveable bed such that the elevation of the contacted bed decreased and became separated from the channel bed thus it is necessary to increase the rootwad size when considering moveable beds in future work credit authorship contribution statement taeun kang data curtion writing original draft visualization investigation ichiro kimura conceptualization methodology software supervision shinichiro onda software validation writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we thank the japan society for the promotion of science jsps for providing funding for this study 18f18359 supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2021 103912 appendix supplementary materials image application 1 
303,hydraulic fracturing is a process aimed at improving the productivity of oil gas or geothermal reservoirs during hydrofracturing backflow follows injection and represents the second phase of the process when part of the fracturing fluid returns from fractures to well and from well to surface a conceptual model is presented to grasp the essential features of the phenomenon conceiving the draining subsurface domain as a planar and rigid fracture backflow against an outlet pressure in the injection well is induced by the relaxation of the fracture wall exerting a force on the fluid proportional to h λ with h the time variable aperture and λ a non negative exponent an overload on the fracture may contribute to slowing or accelerating the closure process the fluid rheology is described by the three parameter ellis constitutive equation well representing the shear thinning rheology typical of hydrofracturing fluids and coupling newtonian and power law behaviour the interplay between these tendencies is modulated by a dimensionless number n encapsulating most problem parameters the range of variation of n is discussed and found to vary around unity the time variable aperture and discharge rate the space time variable pressure field and the time to drain a specified fraction of the fracture volume are derived as functions of geometry length and initial aperture wall elastic parameters fluid properties outlet pressure p e and overload f 0 the late time behaviour of the system is practically independent from rheology as the newtonian nature of the fluid prevails at low shear stress in particular aperture and discharge scale asymptotically with time as t 1 λ 2 and t 1 λ 3 for p e f 0 0 else the aperture tends to a constant residual value proportional to p e f 0 λ a case study with equally spaced fractures adopting realistic geometric mechanical and rheological parameters is examined two fluids normally used in fracking technology show completely different behaviours with backflow dynamics and drainage times initially not dissimilar later varying by orders of magnitude keywords hydraulic fracturing non newtonian ellis rheology elastic wall backflow 1 introduction hydraulic fracturing is a process aimed at improving the productivity of oil gas or geothermal reservoirs analysis of the different phases of hydraulic fracturing is of particular modelling and experimental interest e g dutler et al 2020 wu et al 2020 an understanding of fractured media flow induced by the relaxation of elastic fracture walls is crucial in modelling fracturing fluid backflow a complicated phenomenon involving hydrodynamic mechanical and chemical processes backflow is typically the final phase of the hydraulic fracturing process in the first one fracturing fluid is injected at high pressure in a rock mass forming new fractures and enlarging existing ones in the second phase proppant is introduced in the subsurface environment to prop fractures open then when the injection ceases the pressure drops existing and new fractures tend to close and a portion of the injected fracturing fluid often mixed with proppant ezulike et al 2016 flows back towards the injection well and interact with the relaxing walls of the fractures as the retention of fracturing fluid in the fracture network impairs the fracture conductivity reducing the wellbore productivity balhoff and miller 2005 and favours migration in the subsurface environment along different pathways birdsell et al 2015 it is of utmost interest to optimize the amount of fluid recovered irrespective of the reservoir product be it oil zanganeh et al 2015 gas ghanbari and dehghanpour 2016 or heat mclennan et al 2015 the scientific literature offers two main approaches to modelling backflow i detailed numerical simulations involving single fractures zeng et al 2016 fracture networks makedonska et al 2016 or dual or triple porosity models wang et al 2018a or ii conceptual models capturing the main features of the interaction between fracture flow and wall relaxation huang et al 2019 including the effects of branching networks described at different degrees of complexity dana et al 2018 2019 a recent addition to the modelling effort is the influence of fluid rheology following the notion that the backflow fluid is non newtonian in the widest sense barbati et al 2016 as not only the relationship between shear stress and shear rate is nonlinear but also exhibits normal stress and temperature dependency as well as viscoelasticity thixotropy and nonzero yield stress hormozi and frigaard 2017 at the same time non newtonian fluids allow achieving several engineering objectives such as i minimize the pressure drop in the entire process ii carry suspended proppant iii minimize the leak off within the formation iv adapt their characteristics to different environments in terms of temperature and chemical composition and v flow back easily towards the wellbore given their versatility and economic value these fluids are typically treated for reuse once recovered removing contaminants they may have transported to the surface lester et al 2014 the recovery ratios of backflow fluid vary between 2 and 48 according to ipatova and chuprakov 2020 with considerable economic value modelling non newtonian backflow is in its early stage in variance with the injection and fracture formation stage for which several conceptualizations and models are available see detournay 2016 for a review and the recent work by wrobel 2020 comparing different rheological models for fracturing fluids to the best of our knowledge only chiapponi et al 2019 considered non newtonian fluids in the context of backflow modelling these authors examined flow of a power law fluid towards a wellbore in a single fracture of annular geometry supporting their theoretical findings with laboratory experiments the present paper develops the analysis of non newtonian backflow for a smooth fracture common in field applications osiptov 2017 and adds realism by employing a three parameter ellis model that well represents the rheology of hydrofracturing moukhtari and lecampion 2018 and drilling fluids shah et al 2010 the ellis model tends to newtonian for low shear rates to power law for high shear rates and allows avoiding the unphysical effect of infinite apparent viscosity at zero shear rate that is typical of the power law model for shear thinning fluids myers 2005 we note in passing that our results are of a general nature for newtonian pressurized flow in ducts of variable width and may be of interest for and be applied also to deformable microfluidic anand et al 2019 and biological ali et al 2019 systems the plan of the paper is as follows section 2 formulates the problem of relaxation induced backflow of an ellis fluid in a fracture with nonlinear wall reaction and subject to overload numerical results obtained are presented and discussed in section 3 as a function of dimensionless groups characterizing the system the indicial exponent α quantifying the degree of shear thinning behaviour of the ellis fluid the non negative exponent λ modulating the fracture wall reaction and a further group n encapsulating most problem parameters section 4 illustrates an hypothetical case study adopting realistic geometric and mechanical parameters and two real hydrofracturing fluids decribed by the ellis model section 5 reports the main conclusions and perspectives for future work in appendix a the special case of a newtonian fluid is examined obtaining results that generalize those of dana et al 2018 to a nonlinear wall reaction while appendix b presents an alternative expression for the dimensionless number n shown to be a combination of well known dimensionless groups in fluid mechanics 2 material and methods 2 1 problem statement a rock fracture produced by hydrofracturing though of irregular geometry is often conceptualized for modelling purposes as a 3 d space of length l width w and aperture h between two parallel walls balhoff and thompson 2006 the cartesian coordinate system x y z is illustrated in fig 1 and the fracture is subject to a pressure gradient p p x 0 0 in the x direction in horizontal fractures the additional gravity induced pressure gradient is perpendicular the flow plane and has no effect on the flow field if the x y plane is not horizontal the z direction perpendicular to the walls is not vertical and gravity effects can be included in a reduced pressure term p thus leading to a mathematical treatment with no gravity term to consider for instance for the fig 2 below representing multiple vertical fractures backflowing to an horizontal well the reduced pressure p is equal to p p ρ g y the walls are taken to be rigid so that the aperture h t is solely a function of time and the deformation is concentrated for mathematical convenience in the upper wall that behaves as a nonlinear elastic foundation exerting a reaction on the fluid at t 0 the relaxation of the wall induces a backflow in the negative x direction and the fracture begins to drain subject to a constant outlet pressure p e at x 0 and to a no flow boundary condition at the upstream end x l three further hypotheses are adopted i the flow is quasi steady allowing to neglect the time derivative of the velocity in the momentum equation ii the fracture aspect ratio is small h 0 l 1 warranting the lubrication approximation and iii the flow is essentially one dimensional along x l w the latter conceptualization is usually adopted in hydrogeology also when the two dimensions are comparable as it is often the case for rock fractures wang et al 2018a the flowback fluid is taken to be incompressible of density ρ non newtonian shear thinning barbati et al 2016 and described by the ellis three parameter model skelland 1967 under the above assumptions the fluid undergoes simple shear flow in the x direction and the ellis rheology is described by the following relationship between shear stress τ z x hereinafter τ and shear rate γ z x hereinafter simply γ 1 τ μ 0 1 τ τ 0 α 1 γ γ u z where u is the velocity in the x direction the rheological law 1 features a viscosity parameter μ 0 a constant τ 0 defined as the shear stress corresponding to apparent viscosity μ 0 2 and an indicial parameter α typically larger than one as the fluid is shear thinning for α 1 a pseudo newtonian behaviour with dynamic viscosity μ 0 2 is recovered see fig 3 showing the apparent viscosity μ a p p τ γ for the ellis model compared to newtonian and power law models newtonian behaviour in the form of a plateau for low shear rates is also observed for γ 0 for high shear rates the behaviour is power law and its two parameters can be determined from the ellis model parameters see appendix a in balhoff and thompson 2006 in particular the rheological index is n 1 α al behadili et al 2019 note that when curve fitting is performed on real data n and 1 α may significantly differ moukhtari and lecampion 2018 as two different models are fitted to the same data set it is also seen that the ellis model allows avoiding the unphysical effect of infinite apparent viscosity at zero shear rate that is typical of power law shear thinning fluids myers 2005 in the following we will consider α 1 dealing with the case α 1 in the appendix and the parameters μ 0 and τ 0 to be finite and positive couette poiseuille slit flow of an ellis fluid under a constant pressure gradient was studied extensively by steller 2001 listing all combinations of parameters leading to newtonian or pseudo newtonian behaviour in particular the negative velocity u z under a positive reduced pressure gradient p in the x direction is 2 u z t 1 8 μ 0 h 2 2 z h 2 p x 1 α 1 2 α 1 μ 0 τ 0 α 1 h α 1 2 z h α 1 p x p x α 1 the corresponding average velocity u and flow per unit width q x in the x direction are 3 u h 2 12 μ 0 p x h α 1 2 α 1 α 2 μ 0 τ 0 α 1 p x p x α 1 q x u h for the newtonian case α 1 the latter equation reduces to the classical cubic law zimmerman and bodvarsson 1996 written for a fluid with viscosity μ 0 2 the continuity equation reads dana et al 2018 4 d h d t h t u x 0 and substituting eqs 3 and 4 gives 5 d h d t h 3 12 μ 0 2 p x 2 α h α 2 2 α 1 α 2 μ 0 τ 0 α 1 p x α 1 2 p x 2 the problem formulation is completed by the force balance expressed per unit width of fracture among the fluid pressure and the elastic reaction of the upper wall taken to be proportional to aperture h an overload at the upper wall f 0 a force per unit width is included in the balance for generality chiapponi et al 2019 the overload represents an additional force exerted by the walls and usually opposing the fracture opening due e g to a residual stress state generated by the load history of the rocks it is assumed constant and independent from the fracture aperture the balance reads 6 0 l p x t d x e l h t f 0 where the constant of proportionality e has dimensions m l 2 t 2 for a linear elastic foundation called a winkler soil in geotechnical applications e is equal for a thin elastic layer of thickness l to the ratio between the young modulus of the layer s material e m l 1 t 2 and l e e l in the context of hydraulic fracturing l may be identified with the fracture spacing dana et al 2018 chiapponi et al 2019 a design parameter that depends among others on the type of rock in hydraulically fractured shales values of l l equal to 0 057 0 28 and 0 029 are reported respectively by ghanbari and dehghanpour 2016 wang et al 2018a and wang et al 2018b in the case of vertical sub vertical fractures perpendicular to a horizontal sub horizontal well or borehole the geometry of the idealized system is described by fig 2 showing the two wings of equally spaced planar fractures of half length l width w aperture h and spacing l albeit the flow very close to the well is radial the influence of the boundary condition at the well decreases rapidly with distance and flow in most of the fracture half length l is uniform consistently with the assumption l w hence as an approximation the boundary condition of assigned pressure p e at the well is extended to a segment of height w in the case of planar vertical fractures parallel to and propagating from a vertical well the geometry of the flow is plane without using this approximation a further issue deserving investigation is the linearity of the relationship between the wall reaction and the fracture aperture in fact a nonlinear elastic behaviour can be the result of the pervasive damage of rocks by microcracks and voids which determines nonlinearity even for infinitesimal strain also with an incremental jump in the elastic modulus from tension to compression budiansky and o connell 1976 lyakhovsky et al 1997 in this case the young modulus of the material is a function of the strain rate e e 0 h l and assuming that the latter dependence is expressed with a power law function one has 7 e e 0 h l λ 1 where λ is a non negative exponent modulating the nature of the reaction for λ 1 a constant young modulus is recovered while 0 λ 1 is associated to a softening behaviour and λ 1 to a stiffening one the assumption results in 8 e e 0 l h l λ 1 e h λ 1 and eq 6 is modified as 9 0 l p x t d x e l h λ t f 0 with e e 0 l λ of dimensions m l 1 λ t 2 eqs 5 and 9 are subject to the following initial and boundary conditions 10 h 0 h 0 p x t x l t 0 p 0 t p e h 0 being the initial fracture aperture and p e the exit pressure at the well the solution to the above problem yields two relevant quantities expressed per unit width the flowrate exiting the fracture at the well q t and the residual volume of the fracture at a given time v t these are easily derivable as 11 q t l d h t d t v t l h t 2 2 dimensionless form dimensionless quantities are defined as 12 x x l h h h 0 t t t c p p p e p c p e p e p c q q t c h 0 l q u 0 h 0 v v h 0 l where the scales for pressure and time are 13 p c e h 0 λ t c 2 α α 2 l h 0 1 α 1 h 0 α λ μ 0 τ 0 α 1 e α and u 0 l t c is a velocity scale this leads to the dimensionless counterpart of eq 5 14 d h d t n h 3 2 p x 2 h α 2 p x α 1 2 p x 2 where the pure number 15 n 2 α 3 α 2 τ 0 l e h 0 λ 1 α 1 2 α 3 α 2 τ 0 p c h 0 l α 1 modulates the relative importance of the newtonian behaviour of the ellis fluid at low shear rate expressed by the first term on the r h s of eq 14 with respect to the second term the power law behaviour at high shear rate for a newtonian fluid α 1 n reduces to unity for a shear thinning fluid α 1 n is zero for τ 0 0 and or a rigid wall e e 0 l λ but the latter case renders the scales 13 meaningless in eq 15 defining n the quantity within brackets represents the ratio between the characteristic shear stress τ 0 of the ellis fluid and the pressure scale p c e h 0 λ associated with the elastic reaction of the fracture wall the ratio is in turn corrected by the initial aspect ratio of the fracture h 0 l this formulation of n includes only parameters defined at the single fracture scale note that if the scheme of multiple fractures with spacing l depicted in fig 3 is considered eq 15 may be rewritten as 16 n 2 α 3 α 2 τ 0 e 0 l l h 0 l 2 l l λ 1 h 0 l λ 1 α 1 where τ 0 e 0 is the ratio between the representative shear stress of the fluid and the young modulus of the host rock and l l is the dimensionless fracture spacing the terms to the power λ 1 represent the contribution due to non linear elastic behaviour of the walls and disappear for λ 1 an alternative formulation of n as a function of cauchy reynolds and ellis dimensionless groups is reported in appendix b to grasp the order of magnitude of n we recall that l l may be taken to vary between 0 03 and 0 3 with l l 0 1 being appropriate for an order of magnitude analysis while the initial fracture aspect ratio h 0 l a number much smaller than 1 may be considered of order 10 3 10 5 ghanbari and dehghanpour 2016 wang et al 2018a 2018b the latter reference also reports e 0 2 5 10 10 pa for the rock elastic modulus in fractured shales quite close values e 0 3 10 10 pa and e 0 2 76 10 10 pa are reported in detournay 2016 and fisher and warpinski 2012 hence reference values e 0 2 5 3 0 10 10 pa are considered actual values of rheological parameters for ellis fluids are quite scarce in the literature a reference specific to fracking is moukhtari and lecampion 2018 where the ellis parameters are reported for two fracturing fluids hpg hydroxypropylguar and ves viscoelastic surfactant for the first μ 0 0 44 pa s τ 0 2 01 pa and α 1 22 for the second μ 0 49 pa s τ 0 8 836 pa and α 12 adopting as reference geometrical parameters l l 0 1 and h 0 l 10 4 and a young modulus of e 0 2 75 10 10 pa for the host rock one obtains n 0 209 for hpg and n 0 for ves indicating that for the latter fluid the newtonian component of rheological behaviour is negligible a further consideration is that ves is very strongly shear thinning α 1 therefore the value of n is extremely sensitive to variations in parameters adopting for example l l 0 125 h 0 l 10 5 and e 0 2 5 10 10 pa again realistic values one obtains n 0 100 for ves and n 0 618 for hpg this second set of parameters is adopted for later reference in section 4 describing a case study and is shown there in dimensional form see table 1 trying further combinations of realistic values for fluid and rock properties it is seen that n may take values smaller or larger than unity the former case being more frequent this indicates a certain prevalence of the power law component of rheology over the newtonian one although the asymptotic system behaviour is dominated by the latter as will be shown in the next section we bear in mind that a large variety of combinations is possible for the two parameters n and α depending on geometry and properties of fluid and rock but with the constraint from the definition 15 that for α 1 it must be n 1 the dynamic boundary condition 9 and the boundary conditions 10 transform as 17 0 1 p x t d x h λ p e f 0 18 h 0 1 p x 1 t 0 p 0 t 0 2 3 solution a solution to eq 14 is sought by integrating in two steps the pressure of the fluid and the fracture aperture posing 19 u x t p x h d h d t eq 14 can be written as 20 b 1 a u α 1 u x h where 21 a a t h α 1 n b b t n h 3 while the second boundary condition in eq 18 becomes 22 u 1 t 0 separating variables in eq 20 and integrating with the boundary condition 22 leads to 23 b u a u α 1 α α h 1 x eq 23 can be rewritten as 24 u α c u d 1 x 0 where 25 c c t α n h α 1 d d t α h h 2 α eq 25 is algebraic in u and admits an analytical solution for α 1 2 3 and for α 1 2 1 3 in the form of a combination of functions of h and h this solution can be integrated once in space with the boundary condition p 0 t 0 obtaining the pressure field the pressure field is finally integrated in x 0 1 and the integral in eq 17 is computed as a function of h and h then eq 17 is transformed in a nonlinear ode which is numerically integrated with the initial condition h 0 1 these solutions are analytical in the x coordinate and numerical in the time domain and seem quite cumbersome while their accuracy is comparable to that of a fully numerical solution in space and time the latter also has the advantage of a free selection of the indicial parameter α among the many possible numerical schemes we adopt a finite difference in time and an implicit resolver in space with a step size reduction to track solution accurately the code is written in mathematica introducing a parametric solver for the function u x t as a function of n α h i 1 h i δ t where h i 1 and h i are the values at time i 1 δ t and i δ t respectively the only free parameter is h i 1 all the other parameters are given each time iteration includes the following steps the function u x i 1 is estimated by solving eq 20 in parametric form with h h i 1 h i δ t with the term h taken to be the average between h i 1 and h i and with the b c u 1 i 1 0 where h i 1 is the free parameter h 0 1 is assumed at the first step the space values of u known in parametric form are used to solve the differential problem p x i 1 x u x i 1 with p 0 i 1 0 obtaining the pressure p x i 1 the pressure field is numerically integrated in parametric form in the domain 0 1 the parametric integral is inserted in eq 17 and the equality is forced with a newton method for finding the value of the parameter h i 1 the procedure is repeated for the next time step shifting the values h i 1 once the pressure p x t and aperture h t fields are known the dimensionless flowrate and fracture volume are given by 26 q t d h t d t h v t h t hence at late time the fracture volume and flowrate behave like the aperture and its time derivative respectively for zero borehole pressure and overload the corresponding time scalings are t 1 λ 2 and t 1 λ 3 3 results and discussion fig 4 shows the results of the numerical computation for the fracture aperture and different α values with the analytical solution h 1 9 t 1 3 valid for the newtonian case and a linearly elastic fracture dana et al 2018 corresponding to α 1 n 1 and λ 1 note that the values α 1 n 1 imply newtonian behaviour but with a viscosity equal to μ 0 2 thus halving the time scale t c in eq 13 this requires doubling the dimensionless time t in eq 12 to compare results of equations having a different time scale the time integration is performed with a time step δ t 0 01 since the results of the numerical integration using this fully explicit scheme fit exceedingly well the analytical solution it is not necessary to adopt higher order schemes even considering that the solution has no singularity and behaves rather smoothly the asymptotic behaviour of the solution h t is dictated by the interplay between the two terms on the r h s of eq 14 the second term scales with the gradient pressure decaying in time and with a power of h always larger than 3 since α 1 whereas the first term scales with the third power of h and has n as a coefficient since h 1 and the gradient pressure quickly decays to values less than unity the dominant term is the first one which entails the asymptotic behaviour h t 1 2 λ see fig 5 where different values of α for n 1 and p e 0 produce almost parallel curves for large t fig 5 also shows how variations in λ significantly affect the late time behaviour for fixed α a stiffening λ 1 softening λ 1 elastic reaction of the walls delays facilitates the drainage it is also seen that the parameter α mainly controls the early stage the parameter λ the late stage of the backflow process fig 6 shows results for a fixed α 2 λ 1 and different n values the asymptote is reached much faster for larger n in sum the early time behaviour for zero external pressure at the well is in general dominated by the second term in eq 14 unless the coefficient n 1 in the latter case both terms substantially contribute to the time evolution of h in presence of a non zero external pressure p e 0 or a negative overload f 0 an additional force per unit of wall surface acting in the same direction of the internal pressure the asymptotic residual aperture is equal to p e f 0 1 λ see fig 7 where both effects are included the curves coalesce to the asymptote faster for larger n values implying a dominance of the newtonian behaviour while for small n the power law behaviour prevails and the asymptote is reached for larger dimensionless times upon plotting results for α 3 not shown the main curves for λ 1 and the secondary curves for λ 1 are very similar to those for α 2 fig 8 shows the pressure distribution for two different combinations of the parameters and a shear thinning fluid with α 2 results for other combinations are similar and thus not shown with a pressure decay in space time quicker or slower depending on the parameter values at all times the residual pressure within the fracture increases with smaller n values implying a behaviour closer to newtonian and with smaller λ values i e a softening wall however when the fluid is closer to newtonian the effect of a λ variation is irrelevant an important quantity characterizing the performance of the backflow process is the time required to recover the fluid injected in the fracture network and not lost in the form of leakoff here the network is conceptualized as a single fracture and fluid losses are not explicitly represented they are assumed to take place in the upstream network however the time t y needed to recover y of the fracture volume provides an indication of how rapid the recovery is contour maps in the α n space of the dimensionless time t 90 needed to recover 90 of the fluid are depicted in fig 9 for a linear wall reaction λ 1 as the degree of shear thinning behaviour rises with α for constant n there is a sharp increase in dimensionless t y for n 0 5 while t y is almost independent on α for n 2 conversely t y for costant α decreases with larger n values i e as the fluid behaviour is closer to newtonian this effect is more evident for larger α highest values of t y are attained for large α and low n lowest values for small α and large n the two combinations farthest and closest to newtonian behaviour the effect of a sublinear wall reaction λ 0 5 is depicted in fig 10 that of a supralinear wall reaction in fig 11 the dimensionless time to recover the bulk of the stored fluid is decidedly faster or slower with a softening or stiffening wall demonstrating once again the decisive influence of the parameter λ modulating the wall reaction at late time a word of caution is needed when drawing comparisons between non newtonian fluids with different rheology as the models are semi empirical and the time scale used for the dimensionless formulation depends upon the rheological parameters of the ellis model and is particularly sensitive to the value of the indicial exponent α hence model outputs are best compared in dimensional coordinates when quantitative results are needed 4 a case study a case study is illustrated by comparing the performance of two real hydrofracturing fluids moukhtari and lecampion 2018 hpg hydroxypropylguar and ves viscoelastic surfactant in a realistic setting the rheological parameters according with the ellis model are reported for both fluids in table 1 together with realistic geometric and mechanical parameters within plausible ranges deduced from the literature see the earlier discussion in section 2 2 it is seen that hpg is relatively close to newtonian in behaviour while ves is extremely shear thinning with an equivalent rheological index n less than 0 1 when expressed according to the power law model fig 12 shows the relaxation of the fracture aperture for the two fluids the aperture for the hpg is only initially slightly larger than for the ves but then closes more rapidly reaching one tenth of the initial value at a time around 500 h the closure is much more gradual for the ves requiring about a year to reach the same stage the difference between corresponding pressure profiles illustrated in fig 13 shows a decidedly sharper pressure decrease for hpg than for ves in the initial stage fig 14 shows the time to recover the volume stored in the fracture for the two fluids following the same trend manifested for the evolution of fracture opening ves demonstrates a higher drainage capacity than hpg in the very early phase for y 15 subsequently it is much less efficient and requires an extra time at least three orders of magnitude larger to drain the same percentage of fluid than hpg overall the large difference in rheology mainly encapsulated in the α value translates into corresponding wide differences in terms of aperture pressure and drainage time this is so because the value of the dimensionless group n is very low for ves thus allowing the fluid to manifest its essentially power law nature we tried a number of other combinations of parameters and found that for very shear thinning fluids like ves the results are very sensitive to relatively small changes in parameters slightly increasing the modulus of elasticity e to 3 10 10 and increasing the spacing to 20 m leaving the other parameters in table 1 unchanged leads to n h p g 0 659 and n v e s 2 360 while the change in the n value associated to hpg is modest 6 6 and implies the system behaviour is essentially unchanged with respect to the reference case the increase in n for the ves is dramatic 2260 and entails a fluid behaviour closer to newtonian despite the exceedingly high value of α upon plotting the aperture variation over time for this case not shown the two fluids exhibit a similar behaviour with only modest differences less than 10 in the fracture aperture at early times and an almost identical behaviour later on the pressure profiles do not show any significant differences 5 conclusions a conceptual model for backflow of non newtonian fluid from a closing rock fracture was presented in this paper under the assumption of ellis rheology and elastic but non deformable wall the problem in plane geometry is tractable in semi analytical form to yield the time variable fracture aperture h t pressure field p x t and discharge rate q t as well as the drainage time t y for a specified recovery rate y outlet pressure p e and overload f 0 our results lead to the following specific conclusions the ellis model adopted herein to describe shear thinning rheology couples newtonian and power law behaviour when an ellis fluid backflows from a relaxating fracture the interplay between the two natures is modulated by a dimensionless group n encapsulating the main problem parameters n can be expressed in terms of i the indicial exponent α of the ellis rheology ii the parameter λ governing the wall relaxation process iii the ratio between the characteristic shear stress of the ellis fluid τ 0 and the rock modulus of elasticity e iv two geometric ratios the fracture initial aspect ratio h 0 l and dimensionless spacing l l an alternative format of n is a modified ratio between the cauchy number and the product of reynolds and ellis numbers the factors n and α mostly influence the early and intermediate time evolution of the system when n 1 the power law behaviour prevails for n 1 the pure newtonian case is recovered α 1 entails n 1 while for n 1 the behaviour is mixed for late time the system behaviour tends to newtonian is independent of n and is governed by the wall relaxation parameter λ aperture and discharge scale asymptotically with time as t 1 λ 2 and t 1 λ 3 for p e f 0 0 else the aperture tends asymptotically to a constant value proportional to p e f 0 1 λ very shear thinning fluids larger α and reactive walls larger λ are associated with a more gradual closure of the aperture the residual pressure within the fracture increases with smaller n values and with a softening wall λ 1 when the fluid is close to newtonian the effect of a λ variation is almost irrelevant the dimensionless drainage time t y attains the largest values for large α and low n the lowest values for small α and large n the two combinations farthest and closest to newtonian behaviour a non linear reaction of the walls result in a faster slower recovery for λ 1 softening and λ 1 stiffening for recovery values close to 100 t y is very sensitive to variations of model parameters results are discussed in dimensional form for a case study to reinforce the notion that dimensionless results need to be compared with caution as scales include fluid rheological parameters realistic geometric and mechanical parameters are adopted for a system of equally spaced fractures and results are compared for two fluids hpg and ves normally used in fracking technology the time evolution of the aperture and the dependence of the drainage time upon the recovery ratio are similar at early times then differ by orders of magnitude at intermediate and late times the developments presented together with earlier results dana et al 2018 chiapponi et al 2019 provide an overview of the backflow phenomenon in the two basic geometric configurations for a single fracture plane and radial and for three rheological models of increasing complexity newtonian power law and ellis further improvements of the model remain open in several directions e g i a more complex geometry considering nonplanar fractures with non negligible curvature ii the combination of non newtonian rheology with multiple fracture systems adopting the asymptotic viewpoint of dana et al 2019 iii the incorporation of particle transport to simulate the settling of solid proppant credit authorship contribution statement valentina ciriello methodology writing original draft alessandro lenci methodology visualization sandro longo methodology software writing review editing visualization vittorio di federico conceptualization writing review editing project administration funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported in part by università di bologna almaidea 2017 linea senior grant awarded to vittorio di federico the authors have no conflicts of interest to declare there are no data sharing issues since all of the numerical information is provided in the figures produced by solving the equations in the paper appendix a the newtonian case n 1 for α 1 and n 1 eq 25 reduces to a 1 c 1 d h h 3 and integrating eq 24 using these expressions yields a 2 p x t h 4 h 3 x 1 2 1 substituting in eq 17 and integrating p x t over x gives a 3 h 3 h 3 h λ p e f 0 generalizing eq 2 14 of dana et al 2018 where λ 1 and f 0 0 to nonlinear wall reaction and non zero overload now define an effective pressure p e p e f 0 at the fracture outflow this symbol will be used for brevity in the sequel consider first the case p e 0 integration of eq a 3 over time t yields with the first b c in eq 18 a 4 h t 1 3 2 λ t 1 2 λ that for λ 1 gives back eq 2 15 of dana et al 2018 consider now the case p e 0 integration with the help of mathematica and using transformation formulae for the analytic continuation of hypergeometric functions gradshteyn and ryzhik 2014 yields for generic λ the following implicit equation a 5 t 1 3 λ 2 1 h λ 2 2 f 1 1 λ 2 λ 2 λ 1 λ p e h λ 2 f 1 1 λ 2 λ 2 λ 1 λ p e where 2 f 1 α β γ z is the hypergeometric function of parameters α β γ and argument z specific results for λ 1 2 λ 1 λ 2 i e a sublinear linear or supralinear wall reaction can be obtained as a 6 t 1 18 p e 5 12 ln h 1 2 1 p e h p e 12 p e h 1 2 6 p e 2 h 4 p e 3 h 3 2 3 p e 4 h 2 12 p e 6 p e 2 4 p e 3 3 p e 4 a 7 t 1 6 p e 3 2 ln h 1 p e h p e 2 p e h p e 2 h 2 2 p e p e 2 a 8 t 1 6 p e 2 ln h 2 1 p e h 2 p e p e h 2 p e either by direct integration of eq a 5 or using transformations involving the hypergeometric functions gradshteyn and ryzhik 2014 eq a 7 valid for λ 1 is identical to eq 2 18 of dana et al 2018 other results in terms of trascendental and algebraic functions can be obtained for other special values of λ n or 1 λ n but are too cumbersome to report and or of little technical interest expressions a 5 a 8 when evaluated for given p e allow deriving h t and the drainage time t y needed to drain y of the fracture volume as the latter quantity is given in dimensionless form by h according to 26 to derive t y it is sufficient to evaluate a 5 and its special cases 15 and a 8 for h 100 y 100 finally it is wortwhile to derive the asymptotic behaviour of the general eq a 5 for the limit case λ 0 according to eq 9 λ 0 implies a wall reaction constant over time rather than dependent from the fracture aperture integrating a 3 for h λ 1 gives a 9 h 1 1 6 1 p e t 1 2 a result that can be simplified for large time to h 1 6 1 p e t 1 2 and further for p e 0 to h 1 6 t 1 2 eq a 9 can be also obtained directly from eq a 5 for λ 0 on the basis of eq 9 121 1 in gradshteyn and ryzhik 2014 the late time scaling for a newtonian fluid and a wall with constant reaction λ 0 is therefore h t 1 2 a result coinciding with the scaling h t 1 2 λ implied by fig 5 for a newtonian fluid with n 1 α 1 appendix b the dimensionless group n the pure number n may be expressed as a function of well known dimensionless groups in fluid mechanics see e g massey 1971 multiplying and dividing eq 16 by ρ μ 0 h 0 u 0 3 where u 0 is the reference velocity defined in 12 yields b 1 n k ca re el α 1 ca ρ u 0 2 e re 2 ρ u 0 h 0 μ 0 el μ 0 u 0 τ 0 h 0 b 2 k k α λ l l h 0 l 2 α 3 α 4 l l λ h 0 l λ 1 α 1 where ca re and el are the cauchy reynolds and ellis numbers and k a geometric factor correcting the ratio ca re el in turn ca is the ratio between inertial forces and elastic forces transmitted by solid walls re is the ratio between inertial and viscous forces while el is the ratio between the viscous stress associated with the low shear rate newtonian behaviour and the shear stress τ 0 associated with high shear rate non newtonian power law behaviour 
303,hydraulic fracturing is a process aimed at improving the productivity of oil gas or geothermal reservoirs during hydrofracturing backflow follows injection and represents the second phase of the process when part of the fracturing fluid returns from fractures to well and from well to surface a conceptual model is presented to grasp the essential features of the phenomenon conceiving the draining subsurface domain as a planar and rigid fracture backflow against an outlet pressure in the injection well is induced by the relaxation of the fracture wall exerting a force on the fluid proportional to h λ with h the time variable aperture and λ a non negative exponent an overload on the fracture may contribute to slowing or accelerating the closure process the fluid rheology is described by the three parameter ellis constitutive equation well representing the shear thinning rheology typical of hydrofracturing fluids and coupling newtonian and power law behaviour the interplay between these tendencies is modulated by a dimensionless number n encapsulating most problem parameters the range of variation of n is discussed and found to vary around unity the time variable aperture and discharge rate the space time variable pressure field and the time to drain a specified fraction of the fracture volume are derived as functions of geometry length and initial aperture wall elastic parameters fluid properties outlet pressure p e and overload f 0 the late time behaviour of the system is practically independent from rheology as the newtonian nature of the fluid prevails at low shear stress in particular aperture and discharge scale asymptotically with time as t 1 λ 2 and t 1 λ 3 for p e f 0 0 else the aperture tends to a constant residual value proportional to p e f 0 λ a case study with equally spaced fractures adopting realistic geometric mechanical and rheological parameters is examined two fluids normally used in fracking technology show completely different behaviours with backflow dynamics and drainage times initially not dissimilar later varying by orders of magnitude keywords hydraulic fracturing non newtonian ellis rheology elastic wall backflow 1 introduction hydraulic fracturing is a process aimed at improving the productivity of oil gas or geothermal reservoirs analysis of the different phases of hydraulic fracturing is of particular modelling and experimental interest e g dutler et al 2020 wu et al 2020 an understanding of fractured media flow induced by the relaxation of elastic fracture walls is crucial in modelling fracturing fluid backflow a complicated phenomenon involving hydrodynamic mechanical and chemical processes backflow is typically the final phase of the hydraulic fracturing process in the first one fracturing fluid is injected at high pressure in a rock mass forming new fractures and enlarging existing ones in the second phase proppant is introduced in the subsurface environment to prop fractures open then when the injection ceases the pressure drops existing and new fractures tend to close and a portion of the injected fracturing fluid often mixed with proppant ezulike et al 2016 flows back towards the injection well and interact with the relaxing walls of the fractures as the retention of fracturing fluid in the fracture network impairs the fracture conductivity reducing the wellbore productivity balhoff and miller 2005 and favours migration in the subsurface environment along different pathways birdsell et al 2015 it is of utmost interest to optimize the amount of fluid recovered irrespective of the reservoir product be it oil zanganeh et al 2015 gas ghanbari and dehghanpour 2016 or heat mclennan et al 2015 the scientific literature offers two main approaches to modelling backflow i detailed numerical simulations involving single fractures zeng et al 2016 fracture networks makedonska et al 2016 or dual or triple porosity models wang et al 2018a or ii conceptual models capturing the main features of the interaction between fracture flow and wall relaxation huang et al 2019 including the effects of branching networks described at different degrees of complexity dana et al 2018 2019 a recent addition to the modelling effort is the influence of fluid rheology following the notion that the backflow fluid is non newtonian in the widest sense barbati et al 2016 as not only the relationship between shear stress and shear rate is nonlinear but also exhibits normal stress and temperature dependency as well as viscoelasticity thixotropy and nonzero yield stress hormozi and frigaard 2017 at the same time non newtonian fluids allow achieving several engineering objectives such as i minimize the pressure drop in the entire process ii carry suspended proppant iii minimize the leak off within the formation iv adapt their characteristics to different environments in terms of temperature and chemical composition and v flow back easily towards the wellbore given their versatility and economic value these fluids are typically treated for reuse once recovered removing contaminants they may have transported to the surface lester et al 2014 the recovery ratios of backflow fluid vary between 2 and 48 according to ipatova and chuprakov 2020 with considerable economic value modelling non newtonian backflow is in its early stage in variance with the injection and fracture formation stage for which several conceptualizations and models are available see detournay 2016 for a review and the recent work by wrobel 2020 comparing different rheological models for fracturing fluids to the best of our knowledge only chiapponi et al 2019 considered non newtonian fluids in the context of backflow modelling these authors examined flow of a power law fluid towards a wellbore in a single fracture of annular geometry supporting their theoretical findings with laboratory experiments the present paper develops the analysis of non newtonian backflow for a smooth fracture common in field applications osiptov 2017 and adds realism by employing a three parameter ellis model that well represents the rheology of hydrofracturing moukhtari and lecampion 2018 and drilling fluids shah et al 2010 the ellis model tends to newtonian for low shear rates to power law for high shear rates and allows avoiding the unphysical effect of infinite apparent viscosity at zero shear rate that is typical of the power law model for shear thinning fluids myers 2005 we note in passing that our results are of a general nature for newtonian pressurized flow in ducts of variable width and may be of interest for and be applied also to deformable microfluidic anand et al 2019 and biological ali et al 2019 systems the plan of the paper is as follows section 2 formulates the problem of relaxation induced backflow of an ellis fluid in a fracture with nonlinear wall reaction and subject to overload numerical results obtained are presented and discussed in section 3 as a function of dimensionless groups characterizing the system the indicial exponent α quantifying the degree of shear thinning behaviour of the ellis fluid the non negative exponent λ modulating the fracture wall reaction and a further group n encapsulating most problem parameters section 4 illustrates an hypothetical case study adopting realistic geometric and mechanical parameters and two real hydrofracturing fluids decribed by the ellis model section 5 reports the main conclusions and perspectives for future work in appendix a the special case of a newtonian fluid is examined obtaining results that generalize those of dana et al 2018 to a nonlinear wall reaction while appendix b presents an alternative expression for the dimensionless number n shown to be a combination of well known dimensionless groups in fluid mechanics 2 material and methods 2 1 problem statement a rock fracture produced by hydrofracturing though of irregular geometry is often conceptualized for modelling purposes as a 3 d space of length l width w and aperture h between two parallel walls balhoff and thompson 2006 the cartesian coordinate system x y z is illustrated in fig 1 and the fracture is subject to a pressure gradient p p x 0 0 in the x direction in horizontal fractures the additional gravity induced pressure gradient is perpendicular the flow plane and has no effect on the flow field if the x y plane is not horizontal the z direction perpendicular to the walls is not vertical and gravity effects can be included in a reduced pressure term p thus leading to a mathematical treatment with no gravity term to consider for instance for the fig 2 below representing multiple vertical fractures backflowing to an horizontal well the reduced pressure p is equal to p p ρ g y the walls are taken to be rigid so that the aperture h t is solely a function of time and the deformation is concentrated for mathematical convenience in the upper wall that behaves as a nonlinear elastic foundation exerting a reaction on the fluid at t 0 the relaxation of the wall induces a backflow in the negative x direction and the fracture begins to drain subject to a constant outlet pressure p e at x 0 and to a no flow boundary condition at the upstream end x l three further hypotheses are adopted i the flow is quasi steady allowing to neglect the time derivative of the velocity in the momentum equation ii the fracture aspect ratio is small h 0 l 1 warranting the lubrication approximation and iii the flow is essentially one dimensional along x l w the latter conceptualization is usually adopted in hydrogeology also when the two dimensions are comparable as it is often the case for rock fractures wang et al 2018a the flowback fluid is taken to be incompressible of density ρ non newtonian shear thinning barbati et al 2016 and described by the ellis three parameter model skelland 1967 under the above assumptions the fluid undergoes simple shear flow in the x direction and the ellis rheology is described by the following relationship between shear stress τ z x hereinafter τ and shear rate γ z x hereinafter simply γ 1 τ μ 0 1 τ τ 0 α 1 γ γ u z where u is the velocity in the x direction the rheological law 1 features a viscosity parameter μ 0 a constant τ 0 defined as the shear stress corresponding to apparent viscosity μ 0 2 and an indicial parameter α typically larger than one as the fluid is shear thinning for α 1 a pseudo newtonian behaviour with dynamic viscosity μ 0 2 is recovered see fig 3 showing the apparent viscosity μ a p p τ γ for the ellis model compared to newtonian and power law models newtonian behaviour in the form of a plateau for low shear rates is also observed for γ 0 for high shear rates the behaviour is power law and its two parameters can be determined from the ellis model parameters see appendix a in balhoff and thompson 2006 in particular the rheological index is n 1 α al behadili et al 2019 note that when curve fitting is performed on real data n and 1 α may significantly differ moukhtari and lecampion 2018 as two different models are fitted to the same data set it is also seen that the ellis model allows avoiding the unphysical effect of infinite apparent viscosity at zero shear rate that is typical of power law shear thinning fluids myers 2005 in the following we will consider α 1 dealing with the case α 1 in the appendix and the parameters μ 0 and τ 0 to be finite and positive couette poiseuille slit flow of an ellis fluid under a constant pressure gradient was studied extensively by steller 2001 listing all combinations of parameters leading to newtonian or pseudo newtonian behaviour in particular the negative velocity u z under a positive reduced pressure gradient p in the x direction is 2 u z t 1 8 μ 0 h 2 2 z h 2 p x 1 α 1 2 α 1 μ 0 τ 0 α 1 h α 1 2 z h α 1 p x p x α 1 the corresponding average velocity u and flow per unit width q x in the x direction are 3 u h 2 12 μ 0 p x h α 1 2 α 1 α 2 μ 0 τ 0 α 1 p x p x α 1 q x u h for the newtonian case α 1 the latter equation reduces to the classical cubic law zimmerman and bodvarsson 1996 written for a fluid with viscosity μ 0 2 the continuity equation reads dana et al 2018 4 d h d t h t u x 0 and substituting eqs 3 and 4 gives 5 d h d t h 3 12 μ 0 2 p x 2 α h α 2 2 α 1 α 2 μ 0 τ 0 α 1 p x α 1 2 p x 2 the problem formulation is completed by the force balance expressed per unit width of fracture among the fluid pressure and the elastic reaction of the upper wall taken to be proportional to aperture h an overload at the upper wall f 0 a force per unit width is included in the balance for generality chiapponi et al 2019 the overload represents an additional force exerted by the walls and usually opposing the fracture opening due e g to a residual stress state generated by the load history of the rocks it is assumed constant and independent from the fracture aperture the balance reads 6 0 l p x t d x e l h t f 0 where the constant of proportionality e has dimensions m l 2 t 2 for a linear elastic foundation called a winkler soil in geotechnical applications e is equal for a thin elastic layer of thickness l to the ratio between the young modulus of the layer s material e m l 1 t 2 and l e e l in the context of hydraulic fracturing l may be identified with the fracture spacing dana et al 2018 chiapponi et al 2019 a design parameter that depends among others on the type of rock in hydraulically fractured shales values of l l equal to 0 057 0 28 and 0 029 are reported respectively by ghanbari and dehghanpour 2016 wang et al 2018a and wang et al 2018b in the case of vertical sub vertical fractures perpendicular to a horizontal sub horizontal well or borehole the geometry of the idealized system is described by fig 2 showing the two wings of equally spaced planar fractures of half length l width w aperture h and spacing l albeit the flow very close to the well is radial the influence of the boundary condition at the well decreases rapidly with distance and flow in most of the fracture half length l is uniform consistently with the assumption l w hence as an approximation the boundary condition of assigned pressure p e at the well is extended to a segment of height w in the case of planar vertical fractures parallel to and propagating from a vertical well the geometry of the flow is plane without using this approximation a further issue deserving investigation is the linearity of the relationship between the wall reaction and the fracture aperture in fact a nonlinear elastic behaviour can be the result of the pervasive damage of rocks by microcracks and voids which determines nonlinearity even for infinitesimal strain also with an incremental jump in the elastic modulus from tension to compression budiansky and o connell 1976 lyakhovsky et al 1997 in this case the young modulus of the material is a function of the strain rate e e 0 h l and assuming that the latter dependence is expressed with a power law function one has 7 e e 0 h l λ 1 where λ is a non negative exponent modulating the nature of the reaction for λ 1 a constant young modulus is recovered while 0 λ 1 is associated to a softening behaviour and λ 1 to a stiffening one the assumption results in 8 e e 0 l h l λ 1 e h λ 1 and eq 6 is modified as 9 0 l p x t d x e l h λ t f 0 with e e 0 l λ of dimensions m l 1 λ t 2 eqs 5 and 9 are subject to the following initial and boundary conditions 10 h 0 h 0 p x t x l t 0 p 0 t p e h 0 being the initial fracture aperture and p e the exit pressure at the well the solution to the above problem yields two relevant quantities expressed per unit width the flowrate exiting the fracture at the well q t and the residual volume of the fracture at a given time v t these are easily derivable as 11 q t l d h t d t v t l h t 2 2 dimensionless form dimensionless quantities are defined as 12 x x l h h h 0 t t t c p p p e p c p e p e p c q q t c h 0 l q u 0 h 0 v v h 0 l where the scales for pressure and time are 13 p c e h 0 λ t c 2 α α 2 l h 0 1 α 1 h 0 α λ μ 0 τ 0 α 1 e α and u 0 l t c is a velocity scale this leads to the dimensionless counterpart of eq 5 14 d h d t n h 3 2 p x 2 h α 2 p x α 1 2 p x 2 where the pure number 15 n 2 α 3 α 2 τ 0 l e h 0 λ 1 α 1 2 α 3 α 2 τ 0 p c h 0 l α 1 modulates the relative importance of the newtonian behaviour of the ellis fluid at low shear rate expressed by the first term on the r h s of eq 14 with respect to the second term the power law behaviour at high shear rate for a newtonian fluid α 1 n reduces to unity for a shear thinning fluid α 1 n is zero for τ 0 0 and or a rigid wall e e 0 l λ but the latter case renders the scales 13 meaningless in eq 15 defining n the quantity within brackets represents the ratio between the characteristic shear stress τ 0 of the ellis fluid and the pressure scale p c e h 0 λ associated with the elastic reaction of the fracture wall the ratio is in turn corrected by the initial aspect ratio of the fracture h 0 l this formulation of n includes only parameters defined at the single fracture scale note that if the scheme of multiple fractures with spacing l depicted in fig 3 is considered eq 15 may be rewritten as 16 n 2 α 3 α 2 τ 0 e 0 l l h 0 l 2 l l λ 1 h 0 l λ 1 α 1 where τ 0 e 0 is the ratio between the representative shear stress of the fluid and the young modulus of the host rock and l l is the dimensionless fracture spacing the terms to the power λ 1 represent the contribution due to non linear elastic behaviour of the walls and disappear for λ 1 an alternative formulation of n as a function of cauchy reynolds and ellis dimensionless groups is reported in appendix b to grasp the order of magnitude of n we recall that l l may be taken to vary between 0 03 and 0 3 with l l 0 1 being appropriate for an order of magnitude analysis while the initial fracture aspect ratio h 0 l a number much smaller than 1 may be considered of order 10 3 10 5 ghanbari and dehghanpour 2016 wang et al 2018a 2018b the latter reference also reports e 0 2 5 10 10 pa for the rock elastic modulus in fractured shales quite close values e 0 3 10 10 pa and e 0 2 76 10 10 pa are reported in detournay 2016 and fisher and warpinski 2012 hence reference values e 0 2 5 3 0 10 10 pa are considered actual values of rheological parameters for ellis fluids are quite scarce in the literature a reference specific to fracking is moukhtari and lecampion 2018 where the ellis parameters are reported for two fracturing fluids hpg hydroxypropylguar and ves viscoelastic surfactant for the first μ 0 0 44 pa s τ 0 2 01 pa and α 1 22 for the second μ 0 49 pa s τ 0 8 836 pa and α 12 adopting as reference geometrical parameters l l 0 1 and h 0 l 10 4 and a young modulus of e 0 2 75 10 10 pa for the host rock one obtains n 0 209 for hpg and n 0 for ves indicating that for the latter fluid the newtonian component of rheological behaviour is negligible a further consideration is that ves is very strongly shear thinning α 1 therefore the value of n is extremely sensitive to variations in parameters adopting for example l l 0 125 h 0 l 10 5 and e 0 2 5 10 10 pa again realistic values one obtains n 0 100 for ves and n 0 618 for hpg this second set of parameters is adopted for later reference in section 4 describing a case study and is shown there in dimensional form see table 1 trying further combinations of realistic values for fluid and rock properties it is seen that n may take values smaller or larger than unity the former case being more frequent this indicates a certain prevalence of the power law component of rheology over the newtonian one although the asymptotic system behaviour is dominated by the latter as will be shown in the next section we bear in mind that a large variety of combinations is possible for the two parameters n and α depending on geometry and properties of fluid and rock but with the constraint from the definition 15 that for α 1 it must be n 1 the dynamic boundary condition 9 and the boundary conditions 10 transform as 17 0 1 p x t d x h λ p e f 0 18 h 0 1 p x 1 t 0 p 0 t 0 2 3 solution a solution to eq 14 is sought by integrating in two steps the pressure of the fluid and the fracture aperture posing 19 u x t p x h d h d t eq 14 can be written as 20 b 1 a u α 1 u x h where 21 a a t h α 1 n b b t n h 3 while the second boundary condition in eq 18 becomes 22 u 1 t 0 separating variables in eq 20 and integrating with the boundary condition 22 leads to 23 b u a u α 1 α α h 1 x eq 23 can be rewritten as 24 u α c u d 1 x 0 where 25 c c t α n h α 1 d d t α h h 2 α eq 25 is algebraic in u and admits an analytical solution for α 1 2 3 and for α 1 2 1 3 in the form of a combination of functions of h and h this solution can be integrated once in space with the boundary condition p 0 t 0 obtaining the pressure field the pressure field is finally integrated in x 0 1 and the integral in eq 17 is computed as a function of h and h then eq 17 is transformed in a nonlinear ode which is numerically integrated with the initial condition h 0 1 these solutions are analytical in the x coordinate and numerical in the time domain and seem quite cumbersome while their accuracy is comparable to that of a fully numerical solution in space and time the latter also has the advantage of a free selection of the indicial parameter α among the many possible numerical schemes we adopt a finite difference in time and an implicit resolver in space with a step size reduction to track solution accurately the code is written in mathematica introducing a parametric solver for the function u x t as a function of n α h i 1 h i δ t where h i 1 and h i are the values at time i 1 δ t and i δ t respectively the only free parameter is h i 1 all the other parameters are given each time iteration includes the following steps the function u x i 1 is estimated by solving eq 20 in parametric form with h h i 1 h i δ t with the term h taken to be the average between h i 1 and h i and with the b c u 1 i 1 0 where h i 1 is the free parameter h 0 1 is assumed at the first step the space values of u known in parametric form are used to solve the differential problem p x i 1 x u x i 1 with p 0 i 1 0 obtaining the pressure p x i 1 the pressure field is numerically integrated in parametric form in the domain 0 1 the parametric integral is inserted in eq 17 and the equality is forced with a newton method for finding the value of the parameter h i 1 the procedure is repeated for the next time step shifting the values h i 1 once the pressure p x t and aperture h t fields are known the dimensionless flowrate and fracture volume are given by 26 q t d h t d t h v t h t hence at late time the fracture volume and flowrate behave like the aperture and its time derivative respectively for zero borehole pressure and overload the corresponding time scalings are t 1 λ 2 and t 1 λ 3 3 results and discussion fig 4 shows the results of the numerical computation for the fracture aperture and different α values with the analytical solution h 1 9 t 1 3 valid for the newtonian case and a linearly elastic fracture dana et al 2018 corresponding to α 1 n 1 and λ 1 note that the values α 1 n 1 imply newtonian behaviour but with a viscosity equal to μ 0 2 thus halving the time scale t c in eq 13 this requires doubling the dimensionless time t in eq 12 to compare results of equations having a different time scale the time integration is performed with a time step δ t 0 01 since the results of the numerical integration using this fully explicit scheme fit exceedingly well the analytical solution it is not necessary to adopt higher order schemes even considering that the solution has no singularity and behaves rather smoothly the asymptotic behaviour of the solution h t is dictated by the interplay between the two terms on the r h s of eq 14 the second term scales with the gradient pressure decaying in time and with a power of h always larger than 3 since α 1 whereas the first term scales with the third power of h and has n as a coefficient since h 1 and the gradient pressure quickly decays to values less than unity the dominant term is the first one which entails the asymptotic behaviour h t 1 2 λ see fig 5 where different values of α for n 1 and p e 0 produce almost parallel curves for large t fig 5 also shows how variations in λ significantly affect the late time behaviour for fixed α a stiffening λ 1 softening λ 1 elastic reaction of the walls delays facilitates the drainage it is also seen that the parameter α mainly controls the early stage the parameter λ the late stage of the backflow process fig 6 shows results for a fixed α 2 λ 1 and different n values the asymptote is reached much faster for larger n in sum the early time behaviour for zero external pressure at the well is in general dominated by the second term in eq 14 unless the coefficient n 1 in the latter case both terms substantially contribute to the time evolution of h in presence of a non zero external pressure p e 0 or a negative overload f 0 an additional force per unit of wall surface acting in the same direction of the internal pressure the asymptotic residual aperture is equal to p e f 0 1 λ see fig 7 where both effects are included the curves coalesce to the asymptote faster for larger n values implying a dominance of the newtonian behaviour while for small n the power law behaviour prevails and the asymptote is reached for larger dimensionless times upon plotting results for α 3 not shown the main curves for λ 1 and the secondary curves for λ 1 are very similar to those for α 2 fig 8 shows the pressure distribution for two different combinations of the parameters and a shear thinning fluid with α 2 results for other combinations are similar and thus not shown with a pressure decay in space time quicker or slower depending on the parameter values at all times the residual pressure within the fracture increases with smaller n values implying a behaviour closer to newtonian and with smaller λ values i e a softening wall however when the fluid is closer to newtonian the effect of a λ variation is irrelevant an important quantity characterizing the performance of the backflow process is the time required to recover the fluid injected in the fracture network and not lost in the form of leakoff here the network is conceptualized as a single fracture and fluid losses are not explicitly represented they are assumed to take place in the upstream network however the time t y needed to recover y of the fracture volume provides an indication of how rapid the recovery is contour maps in the α n space of the dimensionless time t 90 needed to recover 90 of the fluid are depicted in fig 9 for a linear wall reaction λ 1 as the degree of shear thinning behaviour rises with α for constant n there is a sharp increase in dimensionless t y for n 0 5 while t y is almost independent on α for n 2 conversely t y for costant α decreases with larger n values i e as the fluid behaviour is closer to newtonian this effect is more evident for larger α highest values of t y are attained for large α and low n lowest values for small α and large n the two combinations farthest and closest to newtonian behaviour the effect of a sublinear wall reaction λ 0 5 is depicted in fig 10 that of a supralinear wall reaction in fig 11 the dimensionless time to recover the bulk of the stored fluid is decidedly faster or slower with a softening or stiffening wall demonstrating once again the decisive influence of the parameter λ modulating the wall reaction at late time a word of caution is needed when drawing comparisons between non newtonian fluids with different rheology as the models are semi empirical and the time scale used for the dimensionless formulation depends upon the rheological parameters of the ellis model and is particularly sensitive to the value of the indicial exponent α hence model outputs are best compared in dimensional coordinates when quantitative results are needed 4 a case study a case study is illustrated by comparing the performance of two real hydrofracturing fluids moukhtari and lecampion 2018 hpg hydroxypropylguar and ves viscoelastic surfactant in a realistic setting the rheological parameters according with the ellis model are reported for both fluids in table 1 together with realistic geometric and mechanical parameters within plausible ranges deduced from the literature see the earlier discussion in section 2 2 it is seen that hpg is relatively close to newtonian in behaviour while ves is extremely shear thinning with an equivalent rheological index n less than 0 1 when expressed according to the power law model fig 12 shows the relaxation of the fracture aperture for the two fluids the aperture for the hpg is only initially slightly larger than for the ves but then closes more rapidly reaching one tenth of the initial value at a time around 500 h the closure is much more gradual for the ves requiring about a year to reach the same stage the difference between corresponding pressure profiles illustrated in fig 13 shows a decidedly sharper pressure decrease for hpg than for ves in the initial stage fig 14 shows the time to recover the volume stored in the fracture for the two fluids following the same trend manifested for the evolution of fracture opening ves demonstrates a higher drainage capacity than hpg in the very early phase for y 15 subsequently it is much less efficient and requires an extra time at least three orders of magnitude larger to drain the same percentage of fluid than hpg overall the large difference in rheology mainly encapsulated in the α value translates into corresponding wide differences in terms of aperture pressure and drainage time this is so because the value of the dimensionless group n is very low for ves thus allowing the fluid to manifest its essentially power law nature we tried a number of other combinations of parameters and found that for very shear thinning fluids like ves the results are very sensitive to relatively small changes in parameters slightly increasing the modulus of elasticity e to 3 10 10 and increasing the spacing to 20 m leaving the other parameters in table 1 unchanged leads to n h p g 0 659 and n v e s 2 360 while the change in the n value associated to hpg is modest 6 6 and implies the system behaviour is essentially unchanged with respect to the reference case the increase in n for the ves is dramatic 2260 and entails a fluid behaviour closer to newtonian despite the exceedingly high value of α upon plotting the aperture variation over time for this case not shown the two fluids exhibit a similar behaviour with only modest differences less than 10 in the fracture aperture at early times and an almost identical behaviour later on the pressure profiles do not show any significant differences 5 conclusions a conceptual model for backflow of non newtonian fluid from a closing rock fracture was presented in this paper under the assumption of ellis rheology and elastic but non deformable wall the problem in plane geometry is tractable in semi analytical form to yield the time variable fracture aperture h t pressure field p x t and discharge rate q t as well as the drainage time t y for a specified recovery rate y outlet pressure p e and overload f 0 our results lead to the following specific conclusions the ellis model adopted herein to describe shear thinning rheology couples newtonian and power law behaviour when an ellis fluid backflows from a relaxating fracture the interplay between the two natures is modulated by a dimensionless group n encapsulating the main problem parameters n can be expressed in terms of i the indicial exponent α of the ellis rheology ii the parameter λ governing the wall relaxation process iii the ratio between the characteristic shear stress of the ellis fluid τ 0 and the rock modulus of elasticity e iv two geometric ratios the fracture initial aspect ratio h 0 l and dimensionless spacing l l an alternative format of n is a modified ratio between the cauchy number and the product of reynolds and ellis numbers the factors n and α mostly influence the early and intermediate time evolution of the system when n 1 the power law behaviour prevails for n 1 the pure newtonian case is recovered α 1 entails n 1 while for n 1 the behaviour is mixed for late time the system behaviour tends to newtonian is independent of n and is governed by the wall relaxation parameter λ aperture and discharge scale asymptotically with time as t 1 λ 2 and t 1 λ 3 for p e f 0 0 else the aperture tends asymptotically to a constant value proportional to p e f 0 1 λ very shear thinning fluids larger α and reactive walls larger λ are associated with a more gradual closure of the aperture the residual pressure within the fracture increases with smaller n values and with a softening wall λ 1 when the fluid is close to newtonian the effect of a λ variation is almost irrelevant the dimensionless drainage time t y attains the largest values for large α and low n the lowest values for small α and large n the two combinations farthest and closest to newtonian behaviour a non linear reaction of the walls result in a faster slower recovery for λ 1 softening and λ 1 stiffening for recovery values close to 100 t y is very sensitive to variations of model parameters results are discussed in dimensional form for a case study to reinforce the notion that dimensionless results need to be compared with caution as scales include fluid rheological parameters realistic geometric and mechanical parameters are adopted for a system of equally spaced fractures and results are compared for two fluids hpg and ves normally used in fracking technology the time evolution of the aperture and the dependence of the drainage time upon the recovery ratio are similar at early times then differ by orders of magnitude at intermediate and late times the developments presented together with earlier results dana et al 2018 chiapponi et al 2019 provide an overview of the backflow phenomenon in the two basic geometric configurations for a single fracture plane and radial and for three rheological models of increasing complexity newtonian power law and ellis further improvements of the model remain open in several directions e g i a more complex geometry considering nonplanar fractures with non negligible curvature ii the combination of non newtonian rheology with multiple fracture systems adopting the asymptotic viewpoint of dana et al 2019 iii the incorporation of particle transport to simulate the settling of solid proppant credit authorship contribution statement valentina ciriello methodology writing original draft alessandro lenci methodology visualization sandro longo methodology software writing review editing visualization vittorio di federico conceptualization writing review editing project administration funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported in part by università di bologna almaidea 2017 linea senior grant awarded to vittorio di federico the authors have no conflicts of interest to declare there are no data sharing issues since all of the numerical information is provided in the figures produced by solving the equations in the paper appendix a the newtonian case n 1 for α 1 and n 1 eq 25 reduces to a 1 c 1 d h h 3 and integrating eq 24 using these expressions yields a 2 p x t h 4 h 3 x 1 2 1 substituting in eq 17 and integrating p x t over x gives a 3 h 3 h 3 h λ p e f 0 generalizing eq 2 14 of dana et al 2018 where λ 1 and f 0 0 to nonlinear wall reaction and non zero overload now define an effective pressure p e p e f 0 at the fracture outflow this symbol will be used for brevity in the sequel consider first the case p e 0 integration of eq a 3 over time t yields with the first b c in eq 18 a 4 h t 1 3 2 λ t 1 2 λ that for λ 1 gives back eq 2 15 of dana et al 2018 consider now the case p e 0 integration with the help of mathematica and using transformation formulae for the analytic continuation of hypergeometric functions gradshteyn and ryzhik 2014 yields for generic λ the following implicit equation a 5 t 1 3 λ 2 1 h λ 2 2 f 1 1 λ 2 λ 2 λ 1 λ p e h λ 2 f 1 1 λ 2 λ 2 λ 1 λ p e where 2 f 1 α β γ z is the hypergeometric function of parameters α β γ and argument z specific results for λ 1 2 λ 1 λ 2 i e a sublinear linear or supralinear wall reaction can be obtained as a 6 t 1 18 p e 5 12 ln h 1 2 1 p e h p e 12 p e h 1 2 6 p e 2 h 4 p e 3 h 3 2 3 p e 4 h 2 12 p e 6 p e 2 4 p e 3 3 p e 4 a 7 t 1 6 p e 3 2 ln h 1 p e h p e 2 p e h p e 2 h 2 2 p e p e 2 a 8 t 1 6 p e 2 ln h 2 1 p e h 2 p e p e h 2 p e either by direct integration of eq a 5 or using transformations involving the hypergeometric functions gradshteyn and ryzhik 2014 eq a 7 valid for λ 1 is identical to eq 2 18 of dana et al 2018 other results in terms of trascendental and algebraic functions can be obtained for other special values of λ n or 1 λ n but are too cumbersome to report and or of little technical interest expressions a 5 a 8 when evaluated for given p e allow deriving h t and the drainage time t y needed to drain y of the fracture volume as the latter quantity is given in dimensionless form by h according to 26 to derive t y it is sufficient to evaluate a 5 and its special cases 15 and a 8 for h 100 y 100 finally it is wortwhile to derive the asymptotic behaviour of the general eq a 5 for the limit case λ 0 according to eq 9 λ 0 implies a wall reaction constant over time rather than dependent from the fracture aperture integrating a 3 for h λ 1 gives a 9 h 1 1 6 1 p e t 1 2 a result that can be simplified for large time to h 1 6 1 p e t 1 2 and further for p e 0 to h 1 6 t 1 2 eq a 9 can be also obtained directly from eq a 5 for λ 0 on the basis of eq 9 121 1 in gradshteyn and ryzhik 2014 the late time scaling for a newtonian fluid and a wall with constant reaction λ 0 is therefore h t 1 2 a result coinciding with the scaling h t 1 2 λ implied by fig 5 for a newtonian fluid with n 1 α 1 appendix b the dimensionless group n the pure number n may be expressed as a function of well known dimensionless groups in fluid mechanics see e g massey 1971 multiplying and dividing eq 16 by ρ μ 0 h 0 u 0 3 where u 0 is the reference velocity defined in 12 yields b 1 n k ca re el α 1 ca ρ u 0 2 e re 2 ρ u 0 h 0 μ 0 el μ 0 u 0 τ 0 h 0 b 2 k k α λ l l h 0 l 2 α 3 α 4 l l λ h 0 l λ 1 α 1 where ca re and el are the cauchy reynolds and ellis numbers and k a geometric factor correcting the ratio ca re el in turn ca is the ratio between inertial forces and elastic forces transmitted by solid walls re is the ratio between inertial and viscous forces while el is the ratio between the viscous stress associated with the low shear rate newtonian behaviour and the shear stress τ 0 associated with high shear rate non newtonian power law behaviour 
304,about 40 of global crop production takes place on irrigated land which accounts for approximately 20 of the global farmland the great majority of freshwater consumption by human societies is associated with irrigation which contributes to a major modification of the global water cycle by enhancing evapotranspiration and reducing surface and groundwater runoff in many regions of the world irrigation contributes to streamflow and groundwater depletion soil salinization cooler microclimate conditions and altered land atmosphere interactions despite the important role played by irrigation in food security water cycle soil productivity and near surface atmospheric conditions its global extent remains poorly quantified to date global maps of irrigated land are often based on estimates from circa year 2000 here we apply artificial intelligence methods based on machine learning algorithms to satellite remote sensing and monthly climate data to map the spatial extent of irrigated areas between 2001 and 2015 we provide global annual maps of irrigated land at 9km resolution for the 2001 2015 and we make this dataset available online keywords irrigation global irrigated areas irrigation maps machine learning 1 introduction the global demand for agricultural products is increasing as a result of demographic growth shifts to resource intensive diets and increasing reliance on biofuels godfray et al 2010 foley et al 2011 cassidy et al 2013 to sustain these ongoing trends global crop production will have to more than double by 2100 beltran pena et al 2020 thereby dramatically increasing human pressure on the limited land and water resources of the planet e g falkenmark et al 2006 ramankutty et al 2008 rockstrom et al 2009 cassidy et al 2013 seekell 2017 despite the big push for food security pathways that rely on more efficient use of resources reduction of food waste and moderation of consumption kummu et al 2012 davis et al 2014 springmann et al 2018 the demand for increased agricultural production will be unavoidable it will require either the expansion of agriculture at the expenses of natural ecosystems such as forests savannas and grasslands or the increase in crop yields in the land that is currently cultivated foley et al 2011 known as agricultural intensification the latter approach would prevent additional losses of natural habitat and biodiversity and avoid the greenhouse gas emissions associated with land conversions runyan and d odorico 2016 at the same time intensification will require the provision of additional inputs that are needed to improve yields through an adequate supply of fertilizers and water erisman et al 2008 rosa et al 2018 in many regions of the world the closure of the gap between actual and maximum potential yields requires irrigation mueller et al 2012 previous studies have mapped the rainfed croplands where the local water resources are sufficient to sustainably meet the local irrigation water requirements jägermeyr et al 2017 rosa et al 2018 2020a a major limitation in this line of research is the lack of knowledge of the extent and distribution of irrigated land most studies rely on a reconstruction of the areas equipped for agriculture before or around year 2000 portmann et al 2010 thenkabail et al 2009 a recent extension of these analyses has reconstructed the global history of irrigated areas between 1900 and 2005 siebert et al 2015 while other studies meier et al 2018 have mapped irrigation for the period 1999 2012providing a new global irrigation map of all the areas irrigated in that time spanat the spatial resolution of 30 arcsec moreover irrigated areas have been mapped at 30m resolution for south asia and australia gfsad30 salmon et al 2015 similar estimates were obtained using moderate resolution imaging spectroradiometer modis data for the conterminous us at 250 1000m resolution brown et al 2019 with agriculture contributing to 90 of human water consumption lack of knowledge of global irrigated areas prevents an analysis of the extent to which water resources around the world are used sustainably i e without depleting local groundwater stocks and environmental flows it also limits our ability to investigate ongoing changes in agricultural practices around the world davis et al 2017 jägermeyr et al 2017 d odorico et al 2018 rosa et al 2019 satellite remote sensing combined with modern machine learning techniques provides unprecedented opportunities to identify areas equipped for irrigation where irrigation may therefore take place for at least part of the year irrigated areas tend to exhibit higher productivity and greenness than their rainfed counterparts in the same region as a result irrigation is expected to be detectable with greenness indices such as evi and ndvi e g kotsuki and tanaka 2015 moreover there is compelling evidence muller et al 2016 2017 thiery et al 2020 that irrigated areas tend to be cooler during the day than adjacent rainfed land as a result of the different partitioning of the incoming solar radiation into sensible and latent heat fluxes with irrigated soil exhibiting greater evapotranspiration and associated latent heat fluxes kueppers et al 2007 puma et al 2010 lobell et al 2008 2009 bonfils and lobell 2007 therefore surface temperature which can be detected from space is expected to be a good proxy for irrigation wei et al 2013 cook et al 2020 in this study we develop a global assessment of irrigation and provide global annual maps of irrigated land at 9km resolution for the 2001 2015 period to that end we apply advanced machine learning methods to data available from satellite remote sensing using as label to train the algorithm the known distributions of irrigated land from siebert et al 2015 we make this dataset available on the zenodo repository https zenodo org record 4659476 2 data and methods irrigation maps are produced by two machine learning models a a time series model and b a point in time time stationary model as explained below the results from these two models are then combined for better overall performance 2 1 data sets both models are trained on a global dataset of irrigation extent the time series model is trained only on areas identified as croplands originating from siebert et al 2015 this dataset is available at a spatial resolution of 5 arc minutes i e 1 pixel of the map corresponds to an area of 86 km2 or 8604 ha at the equator which corresponds to a pixel side of 9 276 km we use area equipped for irrigation data from the siebert et al 2015 dataset specifically the one listed as hyde final as labels to train the algorithm 2 1 1 time series model the four available hyde final datasets from siebert et al 2015 for 1985 2000 were used as labels because satellite data used in this study were not consistently available pre 1981 labels from previous years were discarded given the restraints on satellite and geographic data available from 1985 2015 we used as features the following datasets a ndvi data sourced from the avhrr 15 day data set and compiled in the global inventory monitoring and modeling system gimms ncar 2018 these data were aggregated biannually to take the mean variance and max of all inputs over a 6 month period b terraclimate data were taken from the university of idaho online archives and re projected to the same resolution as the ndvi data and labels abatzoglou et al 2018 for each year monthly values of climate variables were used to estimate annual mean maximum variance and minimum the long term averages aggregated by terraclimate for the years 1981 2010 were also fed to the model as a stand in for long term suitability for agriculture the datasets that proved most useful to the purpose of this study were maximum temperature minimum temperature vapor pressure downward surface shortwave radiation wind speed actual evapotranspiration climate water deficit i e the difference between potential and actual evapotranspiration and soil moisture these hydroclimatic variables are known to control evapotranspiration rates and crop water demand e g katul et al 2012 given the relatively high computing requirements to train the model globally over two decades a decision was made to concentrate on cultivated land by filtering the data based on the 2010 gfsad global food security support analysis data croplands extent including irrigated and rainfed croplands massey et al 2017 this mask was selected because it contained most of the irrigated land and was more extensive than the modis land cover data set applying this mask reduced the required computing time by a factor of ten and allowed for the inclusion of many more training features the time series model was performed in two stages firstly a binary test was used to determine whether at least 1 of land was irrigated secondly any land that was detected to be irrigated was then run through another binary test to detect whether the land was highly irrigated defined as a pixel with at least 20 of the area or 2000 hectares irrigated the result was a three class feature irrigated at 1 irrigated between 1 to 20 and irrigated 20 or more within the gfsad cropland area the non irrigated land is more than four times greater than the irrigated land i e about 20 of the cultivated land is irrigated e g molden 2010 decision tree models including random forests and extra trees tend to not perform well with such an imbalance between classes therefore we under sampled the non irrigated land at a rate of 60 this significantly improved the overall performance of the model the f1 and kappa metrics were used to evaluate the performance of the model table 2 given the class imbalance a model could deliver relatively high accuracy by only predicting non irrigated land the kappa metric represents how much better the model performs with respect to a baseline model that simply predicts based on class frequencies or expected accuracy thus kappa is expressed as model accuracy expected accuracy 1 expected accuracy kappa adjusts the accuracy score to reflect class imbalance and therefore kappa is a better metric when there is class imbalance i e there is no even split between classes as in our case as we have only 11 irrigated land the f1 score is a measure that incorporates both recall and precision the kappa metric of the first stage processing was 78 with an f1 score of 92 table 2 on the second layer of learning differentiating between highly irrigated and other irrigation classes kappa was 76 and f1 again was 92 this led to an overall model performance of kappa 0 76 and accuracy a measure of how often the model classifies correctly of 0 89 and f1 of 0 88 to interpret the kappa values we use the scale proposed by landis and koch 1977 specifically this scale classifies values of kappa in the 0 0 20 interval as slight 0 21 0 40 as fair 0 41 0 60 as moderate 0 61 0 80 as substantial and 0 81 1 as almost perfect based on this interpretation the above kappa values are substantial although somewhat arbitrary these intervals provide a benchmark for the interpretation of the kappa values table 2 2 1 2 time stationary model for the time stationary model we used labels from siebert et al 2015 for the year 2005 and the same spatial resolution as in the time series model i e with pixels of 8604 ha which corresponds to a pixel side of about 9 276 km we ignored pixels with less than 25 hectares of land equipped for irrigation and classified them as not irrigated pixels with 25 to 2000 hectares equipped for irrigation were classified as low or medium irrigation pixels with more than 2000 hectares of land equipped for irrigation were classified as high irrigation because the distribution is highly skewed it drops rapidly as we go to higher values of irrigated areas there is only a relatively limited amount of data at higher levels of irrigation we took a random sample of 20 000 points worldwide to develop a machine learning model to train the model we used features pertaining to climate soil vegetation and land cover from terraclimate abatzoglou et al 2018 the global land data assimilation system gldas rodell et al 2004 and modis datasets our final choice was a random forest model with 1000 trees a bagging fraction of 0 63 and 10 variables per split we also used a random forest algorithm to select features based on their importance our final model used 11 features including latitude and longitude x and y annual average maximum temperature tmmx potential evapotranspiration pet downward surface shortwave radiation srad and wind speed vs from terraclimate annual average albedo albedo inst direct evaporation from bare soil esoil tavg atmospheric pressure psurf f inst from gldas land cover type mcd12q1 for umd and annual maximum evi from modis the relative importance of these predictors is shown in fig 1 again the microclimate variables chosen as features are known for playing a role in determining the rates of evapotranspiration and the crop water requirements e g katul et al 2012 we used the r statistical environment for model development and tuning we then exported this model to google earth engine and ran it there for the years 2001 through 2015 this platform provided us with a repository to access many geospatial datasets as well as a distributed computing infrastructure that allowed us to process large amounts of feature data and classify the cultivated land based on the occurrence and intensity of irrigation the time stationary model has accuracy of 0 89 and a kappa value of 0 56 2 2 combining the two models we then combined predictions from the above two models first we partitioned the world s land area into croplands and non croplands using the gfsad crop dominance dataset to determine the cropland boundaries considering all crop classes in gfsad for croplands we predict irrigation using the time series model for non croplands we predict with the time stationary model our combined map has accuracy of 0 94 and a kappa value of 0 73 the confusion matrix and summary statistics are shown in tables 1 and 2 2 3 validation after running our model we validated the false positives and false negatives produced by the model by taking a random sample of 100 points in each class we downloaded landsat 7 toa top of the atmosphere imagery for these points for a buffer of 9 km and a buffer of further 9 km we then visually inspected each picture to validate the model classification 3 results we developed annual global maps of irrigation fig 3 encompassing all continents except antarctica and the pacific islands of oceania https zenodo org record 4659476 these maps are at a spatial resolution of 5 arc minutes 9276 m at the equator and are available for every year from 2001 through 2015 the maps show the extent of 3 classes of irrigation i no irrigation ii low concentration irrigation 1 to 20 irrigated and iii high concentration irrigation 20 irrigated the model predicts that as of 2005 11 2 of the world s land was used as irrigated cropland of this 80 5 is in the low irrigation class and 19 5 is in the high irrigation class the confusion matrix table 1 shows that 97 of the non irrigated areas were correctly predicted as non irrigated 73 of low concentration irrigated areas and 80 of high concentration irrigated areas were correctly predicted as irrigated at low or high concentrations respectively these discrepancies between predictions and actual irrigation are for most part due to 22 of low irrigation areas being mistakenly classified as non irrigated and 16 of high concentration irrigation areas being mistakenly classified as low concentration table 1 we use a validation process to evaluate to what extent these discrepancies are due to limitations in our predictions or are the result of inconsistencies in our label and do not correspond to actual misclassifications in our predictions the results of the validation process show that model performance is better than what is reported in sections 2 2 and 2 3 in fact for points that are false positives we found that 41 out of 100 points were croplands 27 or a mosaic of croplands and non croplands such as uncultivated hills water bodies or forests 14 this means that the model was correct in its classification for these points the remaining 59 points were non croplands for points that are false negatives we found that 47 out of 100 points were indeed non croplands this means the model was correct in its classification for these points 33 points were a mosaic and 20 points were croplands these numbers show some of the limitations in the use of siebert et al 2015 as training labels bring uncertainty to the model assessment metrics reported above based solely on the training labels these results show that our models fare poorly in areas which have a mosaic of croplands and non croplands this problem occurs in the brazilian highlands northern germany and southern china where the terrain is a mix of hilly areas and croplands our spatial resolution assigns 86 square km to each pixel and this can be too coarse to identify irrigation in such mosaics fig 2 additionally one of the feature datasets gldas is at even coarser resolution which can cause predictions to mis classify adjoining pixels moreover mosaics have the effect of confusing our model in the low class of irrigation fig 2 globally most irrigation occurs at low density i e with less than 20 of the area being irrigated particularly in sub saharan africa oceania and south america south asia and east asia however exhibit a relatively large fraction of their irrigated areas at high density irrigation 51 and 32 respectively pointing to regions of the world of particularly intensified crop production fig 3 our predictions of irrigated and non irrigated areas between 2001 and 2015 show some interesting trends fig 4 there is a slight decreasing trend in irrigation in high levels of irrigation but there is an increasing trend in irrigation in low levels of irrigation 1 20 irrigated land this may indicate increased irrigation because of drought or reduced rainfall in regions traditionally reliant on rainfall interestingly we see fig 5 that irrigation has decreased in central parts of eastern china thailand saudi arabia russia and the southeastern usa syria albania eastern germany northern italy and certain parts of brazilian highlands fig 6 we see an increase in irrigation in central parts of the united states fig 7 along the mid low course of the danube hungary serbia romania and bulgaria northern india and in drier regions of central asia links to these global maps and the validation areas are provided in the online materials 4 discussion this study differs from previous efforts based on statistical methods portmann et al 2010 in that it uses machine learning algorithms to map irrigated areas worldwide the dataset developed in this research provides a global scale mapping of low and high intensity density irrigated areas with about 9 km resolution this analysis allows us to investigate spatiotemporal patterns of irrigation worldwide specifically we find that high density irrigated areas are found mostly in south asia and east asia followed by north america europe and the middle east north africa mena region between 2001 and 2015 irrigated areas have increased across north america and south asia and the increase was contributed by an expansion of low density irrigation areas conversely south america and the mena region saw a decrease in irrigated areas as a result of a decrease in areas with low density irrigation the case of east asia is different because while low density irrigation areas have increased low density irrigated areas have shrunk leading to an overall decrease of irrigated areas these patterns are expected to change as an effect of climate change with an expansion of irrigated agriculture at the mid high latitude and a loss of irrigation suitability or the need for seasonal reservoirs in breadbasket regions of eurasia rosa et al 2020b a comparison between our results for 2015 and those obtained by meier et al 2018 for the same years table 3 shows an overall agreement in the classification or irrigated and non irrigated areas ranging between 81 96 depending on the region with an average of 91 when the comparison is limited to the areas classified as irrigated in our study the agreement ranges between 59 81 with an average of 73 alternatively we can measure the agreement as a percentage of the irrigated area in meier et al 2018 in that case the agreement drops to 24 77 with an average of 48 the differences are likely a consequence of the different methods used by the two studies we are unable to establish however which one of the two methods provides the correct classification most likely both methods produce correct or wrong results in different regions the validation results presented in the previous section for a sample of 100 pixels shows that for the year 2001 our classification tends to be in stronger agreement than the label siebert et al 2015 with the signs of irrigation detectable with google earth in areas classified in this study as irrigated of course a more extensive analysis would be needed to generalize these conclusions to the entire world this new dataset allows for the detection of areas around the world where irrigation has emerged or disappeared in recent years it also shows where irrigation has been consistent or intermittent during the study period 2001 2015 and allows for the detection of areas affected by unsustainable use of water resources for irrigation in fact the results from this study can be combined with hydrologic models to show to what extent irrigation is emerging or disappearing in areas where the local water resources are sufficient to allow for sustainable irrigation practices e g rosa et al 2018 with irrigation accounting for nearly 90 of global water consumption by human societies a better knowledge of interannual variability of irrigated areas around the world allows for a better quantitative understanding of the rates of human appropriation of water resources and their recent trends declaration of competing interest all authors have nothing to disclose acknowledgments the authors thank lorenzo rosa ethz for providing comments and suggestions author statement todeschini m c rulli p d odorico conceived the study d nagaraj e proust a todeschini designed the study e proust d nagaraj performed the research d nagaraj e proust m c rulli analyzed the results all authors wrote the manuscript appendix in the zenodo archive https zenodo org record 4659476 the reader can find prediction maps prediction maps for every year from 2001 to 2015 in geotiff format class 0 represents no irrigation class 1 is low to medium irrigation and class 2 is high irrigation assessment map model assessment map for the year 2005 difference map map showing cropland differences between 2001 and 2015 
304,about 40 of global crop production takes place on irrigated land which accounts for approximately 20 of the global farmland the great majority of freshwater consumption by human societies is associated with irrigation which contributes to a major modification of the global water cycle by enhancing evapotranspiration and reducing surface and groundwater runoff in many regions of the world irrigation contributes to streamflow and groundwater depletion soil salinization cooler microclimate conditions and altered land atmosphere interactions despite the important role played by irrigation in food security water cycle soil productivity and near surface atmospheric conditions its global extent remains poorly quantified to date global maps of irrigated land are often based on estimates from circa year 2000 here we apply artificial intelligence methods based on machine learning algorithms to satellite remote sensing and monthly climate data to map the spatial extent of irrigated areas between 2001 and 2015 we provide global annual maps of irrigated land at 9km resolution for the 2001 2015 and we make this dataset available online keywords irrigation global irrigated areas irrigation maps machine learning 1 introduction the global demand for agricultural products is increasing as a result of demographic growth shifts to resource intensive diets and increasing reliance on biofuels godfray et al 2010 foley et al 2011 cassidy et al 2013 to sustain these ongoing trends global crop production will have to more than double by 2100 beltran pena et al 2020 thereby dramatically increasing human pressure on the limited land and water resources of the planet e g falkenmark et al 2006 ramankutty et al 2008 rockstrom et al 2009 cassidy et al 2013 seekell 2017 despite the big push for food security pathways that rely on more efficient use of resources reduction of food waste and moderation of consumption kummu et al 2012 davis et al 2014 springmann et al 2018 the demand for increased agricultural production will be unavoidable it will require either the expansion of agriculture at the expenses of natural ecosystems such as forests savannas and grasslands or the increase in crop yields in the land that is currently cultivated foley et al 2011 known as agricultural intensification the latter approach would prevent additional losses of natural habitat and biodiversity and avoid the greenhouse gas emissions associated with land conversions runyan and d odorico 2016 at the same time intensification will require the provision of additional inputs that are needed to improve yields through an adequate supply of fertilizers and water erisman et al 2008 rosa et al 2018 in many regions of the world the closure of the gap between actual and maximum potential yields requires irrigation mueller et al 2012 previous studies have mapped the rainfed croplands where the local water resources are sufficient to sustainably meet the local irrigation water requirements jägermeyr et al 2017 rosa et al 2018 2020a a major limitation in this line of research is the lack of knowledge of the extent and distribution of irrigated land most studies rely on a reconstruction of the areas equipped for agriculture before or around year 2000 portmann et al 2010 thenkabail et al 2009 a recent extension of these analyses has reconstructed the global history of irrigated areas between 1900 and 2005 siebert et al 2015 while other studies meier et al 2018 have mapped irrigation for the period 1999 2012providing a new global irrigation map of all the areas irrigated in that time spanat the spatial resolution of 30 arcsec moreover irrigated areas have been mapped at 30m resolution for south asia and australia gfsad30 salmon et al 2015 similar estimates were obtained using moderate resolution imaging spectroradiometer modis data for the conterminous us at 250 1000m resolution brown et al 2019 with agriculture contributing to 90 of human water consumption lack of knowledge of global irrigated areas prevents an analysis of the extent to which water resources around the world are used sustainably i e without depleting local groundwater stocks and environmental flows it also limits our ability to investigate ongoing changes in agricultural practices around the world davis et al 2017 jägermeyr et al 2017 d odorico et al 2018 rosa et al 2019 satellite remote sensing combined with modern machine learning techniques provides unprecedented opportunities to identify areas equipped for irrigation where irrigation may therefore take place for at least part of the year irrigated areas tend to exhibit higher productivity and greenness than their rainfed counterparts in the same region as a result irrigation is expected to be detectable with greenness indices such as evi and ndvi e g kotsuki and tanaka 2015 moreover there is compelling evidence muller et al 2016 2017 thiery et al 2020 that irrigated areas tend to be cooler during the day than adjacent rainfed land as a result of the different partitioning of the incoming solar radiation into sensible and latent heat fluxes with irrigated soil exhibiting greater evapotranspiration and associated latent heat fluxes kueppers et al 2007 puma et al 2010 lobell et al 2008 2009 bonfils and lobell 2007 therefore surface temperature which can be detected from space is expected to be a good proxy for irrigation wei et al 2013 cook et al 2020 in this study we develop a global assessment of irrigation and provide global annual maps of irrigated land at 9km resolution for the 2001 2015 period to that end we apply advanced machine learning methods to data available from satellite remote sensing using as label to train the algorithm the known distributions of irrigated land from siebert et al 2015 we make this dataset available on the zenodo repository https zenodo org record 4659476 2 data and methods irrigation maps are produced by two machine learning models a a time series model and b a point in time time stationary model as explained below the results from these two models are then combined for better overall performance 2 1 data sets both models are trained on a global dataset of irrigation extent the time series model is trained only on areas identified as croplands originating from siebert et al 2015 this dataset is available at a spatial resolution of 5 arc minutes i e 1 pixel of the map corresponds to an area of 86 km2 or 8604 ha at the equator which corresponds to a pixel side of 9 276 km we use area equipped for irrigation data from the siebert et al 2015 dataset specifically the one listed as hyde final as labels to train the algorithm 2 1 1 time series model the four available hyde final datasets from siebert et al 2015 for 1985 2000 were used as labels because satellite data used in this study were not consistently available pre 1981 labels from previous years were discarded given the restraints on satellite and geographic data available from 1985 2015 we used as features the following datasets a ndvi data sourced from the avhrr 15 day data set and compiled in the global inventory monitoring and modeling system gimms ncar 2018 these data were aggregated biannually to take the mean variance and max of all inputs over a 6 month period b terraclimate data were taken from the university of idaho online archives and re projected to the same resolution as the ndvi data and labels abatzoglou et al 2018 for each year monthly values of climate variables were used to estimate annual mean maximum variance and minimum the long term averages aggregated by terraclimate for the years 1981 2010 were also fed to the model as a stand in for long term suitability for agriculture the datasets that proved most useful to the purpose of this study were maximum temperature minimum temperature vapor pressure downward surface shortwave radiation wind speed actual evapotranspiration climate water deficit i e the difference between potential and actual evapotranspiration and soil moisture these hydroclimatic variables are known to control evapotranspiration rates and crop water demand e g katul et al 2012 given the relatively high computing requirements to train the model globally over two decades a decision was made to concentrate on cultivated land by filtering the data based on the 2010 gfsad global food security support analysis data croplands extent including irrigated and rainfed croplands massey et al 2017 this mask was selected because it contained most of the irrigated land and was more extensive than the modis land cover data set applying this mask reduced the required computing time by a factor of ten and allowed for the inclusion of many more training features the time series model was performed in two stages firstly a binary test was used to determine whether at least 1 of land was irrigated secondly any land that was detected to be irrigated was then run through another binary test to detect whether the land was highly irrigated defined as a pixel with at least 20 of the area or 2000 hectares irrigated the result was a three class feature irrigated at 1 irrigated between 1 to 20 and irrigated 20 or more within the gfsad cropland area the non irrigated land is more than four times greater than the irrigated land i e about 20 of the cultivated land is irrigated e g molden 2010 decision tree models including random forests and extra trees tend to not perform well with such an imbalance between classes therefore we under sampled the non irrigated land at a rate of 60 this significantly improved the overall performance of the model the f1 and kappa metrics were used to evaluate the performance of the model table 2 given the class imbalance a model could deliver relatively high accuracy by only predicting non irrigated land the kappa metric represents how much better the model performs with respect to a baseline model that simply predicts based on class frequencies or expected accuracy thus kappa is expressed as model accuracy expected accuracy 1 expected accuracy kappa adjusts the accuracy score to reflect class imbalance and therefore kappa is a better metric when there is class imbalance i e there is no even split between classes as in our case as we have only 11 irrigated land the f1 score is a measure that incorporates both recall and precision the kappa metric of the first stage processing was 78 with an f1 score of 92 table 2 on the second layer of learning differentiating between highly irrigated and other irrigation classes kappa was 76 and f1 again was 92 this led to an overall model performance of kappa 0 76 and accuracy a measure of how often the model classifies correctly of 0 89 and f1 of 0 88 to interpret the kappa values we use the scale proposed by landis and koch 1977 specifically this scale classifies values of kappa in the 0 0 20 interval as slight 0 21 0 40 as fair 0 41 0 60 as moderate 0 61 0 80 as substantial and 0 81 1 as almost perfect based on this interpretation the above kappa values are substantial although somewhat arbitrary these intervals provide a benchmark for the interpretation of the kappa values table 2 2 1 2 time stationary model for the time stationary model we used labels from siebert et al 2015 for the year 2005 and the same spatial resolution as in the time series model i e with pixels of 8604 ha which corresponds to a pixel side of about 9 276 km we ignored pixels with less than 25 hectares of land equipped for irrigation and classified them as not irrigated pixels with 25 to 2000 hectares equipped for irrigation were classified as low or medium irrigation pixels with more than 2000 hectares of land equipped for irrigation were classified as high irrigation because the distribution is highly skewed it drops rapidly as we go to higher values of irrigated areas there is only a relatively limited amount of data at higher levels of irrigation we took a random sample of 20 000 points worldwide to develop a machine learning model to train the model we used features pertaining to climate soil vegetation and land cover from terraclimate abatzoglou et al 2018 the global land data assimilation system gldas rodell et al 2004 and modis datasets our final choice was a random forest model with 1000 trees a bagging fraction of 0 63 and 10 variables per split we also used a random forest algorithm to select features based on their importance our final model used 11 features including latitude and longitude x and y annual average maximum temperature tmmx potential evapotranspiration pet downward surface shortwave radiation srad and wind speed vs from terraclimate annual average albedo albedo inst direct evaporation from bare soil esoil tavg atmospheric pressure psurf f inst from gldas land cover type mcd12q1 for umd and annual maximum evi from modis the relative importance of these predictors is shown in fig 1 again the microclimate variables chosen as features are known for playing a role in determining the rates of evapotranspiration and the crop water requirements e g katul et al 2012 we used the r statistical environment for model development and tuning we then exported this model to google earth engine and ran it there for the years 2001 through 2015 this platform provided us with a repository to access many geospatial datasets as well as a distributed computing infrastructure that allowed us to process large amounts of feature data and classify the cultivated land based on the occurrence and intensity of irrigation the time stationary model has accuracy of 0 89 and a kappa value of 0 56 2 2 combining the two models we then combined predictions from the above two models first we partitioned the world s land area into croplands and non croplands using the gfsad crop dominance dataset to determine the cropland boundaries considering all crop classes in gfsad for croplands we predict irrigation using the time series model for non croplands we predict with the time stationary model our combined map has accuracy of 0 94 and a kappa value of 0 73 the confusion matrix and summary statistics are shown in tables 1 and 2 2 3 validation after running our model we validated the false positives and false negatives produced by the model by taking a random sample of 100 points in each class we downloaded landsat 7 toa top of the atmosphere imagery for these points for a buffer of 9 km and a buffer of further 9 km we then visually inspected each picture to validate the model classification 3 results we developed annual global maps of irrigation fig 3 encompassing all continents except antarctica and the pacific islands of oceania https zenodo org record 4659476 these maps are at a spatial resolution of 5 arc minutes 9276 m at the equator and are available for every year from 2001 through 2015 the maps show the extent of 3 classes of irrigation i no irrigation ii low concentration irrigation 1 to 20 irrigated and iii high concentration irrigation 20 irrigated the model predicts that as of 2005 11 2 of the world s land was used as irrigated cropland of this 80 5 is in the low irrigation class and 19 5 is in the high irrigation class the confusion matrix table 1 shows that 97 of the non irrigated areas were correctly predicted as non irrigated 73 of low concentration irrigated areas and 80 of high concentration irrigated areas were correctly predicted as irrigated at low or high concentrations respectively these discrepancies between predictions and actual irrigation are for most part due to 22 of low irrigation areas being mistakenly classified as non irrigated and 16 of high concentration irrigation areas being mistakenly classified as low concentration table 1 we use a validation process to evaluate to what extent these discrepancies are due to limitations in our predictions or are the result of inconsistencies in our label and do not correspond to actual misclassifications in our predictions the results of the validation process show that model performance is better than what is reported in sections 2 2 and 2 3 in fact for points that are false positives we found that 41 out of 100 points were croplands 27 or a mosaic of croplands and non croplands such as uncultivated hills water bodies or forests 14 this means that the model was correct in its classification for these points the remaining 59 points were non croplands for points that are false negatives we found that 47 out of 100 points were indeed non croplands this means the model was correct in its classification for these points 33 points were a mosaic and 20 points were croplands these numbers show some of the limitations in the use of siebert et al 2015 as training labels bring uncertainty to the model assessment metrics reported above based solely on the training labels these results show that our models fare poorly in areas which have a mosaic of croplands and non croplands this problem occurs in the brazilian highlands northern germany and southern china where the terrain is a mix of hilly areas and croplands our spatial resolution assigns 86 square km to each pixel and this can be too coarse to identify irrigation in such mosaics fig 2 additionally one of the feature datasets gldas is at even coarser resolution which can cause predictions to mis classify adjoining pixels moreover mosaics have the effect of confusing our model in the low class of irrigation fig 2 globally most irrigation occurs at low density i e with less than 20 of the area being irrigated particularly in sub saharan africa oceania and south america south asia and east asia however exhibit a relatively large fraction of their irrigated areas at high density irrigation 51 and 32 respectively pointing to regions of the world of particularly intensified crop production fig 3 our predictions of irrigated and non irrigated areas between 2001 and 2015 show some interesting trends fig 4 there is a slight decreasing trend in irrigation in high levels of irrigation but there is an increasing trend in irrigation in low levels of irrigation 1 20 irrigated land this may indicate increased irrigation because of drought or reduced rainfall in regions traditionally reliant on rainfall interestingly we see fig 5 that irrigation has decreased in central parts of eastern china thailand saudi arabia russia and the southeastern usa syria albania eastern germany northern italy and certain parts of brazilian highlands fig 6 we see an increase in irrigation in central parts of the united states fig 7 along the mid low course of the danube hungary serbia romania and bulgaria northern india and in drier regions of central asia links to these global maps and the validation areas are provided in the online materials 4 discussion this study differs from previous efforts based on statistical methods portmann et al 2010 in that it uses machine learning algorithms to map irrigated areas worldwide the dataset developed in this research provides a global scale mapping of low and high intensity density irrigated areas with about 9 km resolution this analysis allows us to investigate spatiotemporal patterns of irrigation worldwide specifically we find that high density irrigated areas are found mostly in south asia and east asia followed by north america europe and the middle east north africa mena region between 2001 and 2015 irrigated areas have increased across north america and south asia and the increase was contributed by an expansion of low density irrigation areas conversely south america and the mena region saw a decrease in irrigated areas as a result of a decrease in areas with low density irrigation the case of east asia is different because while low density irrigation areas have increased low density irrigated areas have shrunk leading to an overall decrease of irrigated areas these patterns are expected to change as an effect of climate change with an expansion of irrigated agriculture at the mid high latitude and a loss of irrigation suitability or the need for seasonal reservoirs in breadbasket regions of eurasia rosa et al 2020b a comparison between our results for 2015 and those obtained by meier et al 2018 for the same years table 3 shows an overall agreement in the classification or irrigated and non irrigated areas ranging between 81 96 depending on the region with an average of 91 when the comparison is limited to the areas classified as irrigated in our study the agreement ranges between 59 81 with an average of 73 alternatively we can measure the agreement as a percentage of the irrigated area in meier et al 2018 in that case the agreement drops to 24 77 with an average of 48 the differences are likely a consequence of the different methods used by the two studies we are unable to establish however which one of the two methods provides the correct classification most likely both methods produce correct or wrong results in different regions the validation results presented in the previous section for a sample of 100 pixels shows that for the year 2001 our classification tends to be in stronger agreement than the label siebert et al 2015 with the signs of irrigation detectable with google earth in areas classified in this study as irrigated of course a more extensive analysis would be needed to generalize these conclusions to the entire world this new dataset allows for the detection of areas around the world where irrigation has emerged or disappeared in recent years it also shows where irrigation has been consistent or intermittent during the study period 2001 2015 and allows for the detection of areas affected by unsustainable use of water resources for irrigation in fact the results from this study can be combined with hydrologic models to show to what extent irrigation is emerging or disappearing in areas where the local water resources are sufficient to allow for sustainable irrigation practices e g rosa et al 2018 with irrigation accounting for nearly 90 of global water consumption by human societies a better knowledge of interannual variability of irrigated areas around the world allows for a better quantitative understanding of the rates of human appropriation of water resources and their recent trends declaration of competing interest all authors have nothing to disclose acknowledgments the authors thank lorenzo rosa ethz for providing comments and suggestions author statement todeschini m c rulli p d odorico conceived the study d nagaraj e proust a todeschini designed the study e proust d nagaraj performed the research d nagaraj e proust m c rulli analyzed the results all authors wrote the manuscript appendix in the zenodo archive https zenodo org record 4659476 the reader can find prediction maps prediction maps for every year from 2001 to 2015 in geotiff format class 0 represents no irrigation class 1 is low to medium irrigation and class 2 is high irrigation assessment map model assessment map for the year 2005 difference map map showing cropland differences between 2001 and 2015 
