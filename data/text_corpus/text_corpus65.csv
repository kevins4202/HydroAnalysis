index,text
325,many present day statistical schemes for postprocessing weather forecasts in particular precipitation forecasts rely on calibration using prescribed statistical models to relate forecast statistics to distributional parameters the efficacy of such schemes is often constrained not only by prescribed predictor predictand relation but also by arbitrary choices of temporal window and lead time range for training to address this limitation we propose an end to end computationally efficient hybrid postprocessing scheme capable of producing full predictive distributions of precipitation accumulation without explicit stratification of forecast observation pairs by forecast lead time and season the proposed framework uses the censored shifted gamma distribution csgd as the predictive distribution but uses an artificial neural network ann to estimate the distributional parameters of csgd through a unified approach this approach referred to as ann csgd allows for simultaneous estimation of distributional parameters over multiple lead times and seasons in a single model by incorporating the latter variables as predictors to the ann we test our proposed ann csgd model for postprocessing of ensemble mean forecasts of 24 h precipitation totals over selected river basins in california at one to seven day lead times from the global ensemble forecast system gefs the probabilistic quantitative precipitation forecasts pqpfs from the ann csgd are more skillful overall than those from the benchmark csgd and the mixed type meta gaussian distribution mmgd models the ann csgd pqpfs highly improve the performance of those from csgd in predicting the probability of precipitation pop and are also much sharper and reliable at higher precipitation thresholds we demonstrate how the hybrid approach by using the entire available training data and its modified formulation efficiently represents interactions between gefs forecasts and season lead times thus leading to enhanced predictive performance keywords statistical postprocessing artificial neural networks probabilistic quantitative precipitation forecast predictive distribution 1 introduction statistical postprocessing techniques are increasingly used to improve the reliability and skill of real time probabilistic quantitative precipitation forecasts pqpfs produced by numerical weather prediction nwp models broadly speaking these techniques can be categorized as nonparametric and parametric ones a prominent example of the former is the analog approach hamill and whitaker 2006 hamill et al 2015 the parametric techniques rely on prescribed parametric forms of conditional predictive joint and marginal distributions and employ various techniques ranging from regression to the method of moments and their variants for estimating distributional parameters many of the modern parametric approaches fall under the broad umbrella of ensemble model output statistics emos gneiting et al 2005 also known as nonhomogeneous regression as the name implies the emos approaches use prescribed predictive distributions and relate distributional parameters to ensemble statistics through a set of regression equations scheuerer and hamill 2015 zhang et al 2017 stauffer et al 2017 the extent to which postprocessing techniques have improved forecast skill has varied in practice li et al 2017 wilks 2018 vannitsem et al 2020 there are several common limitations in postprocessing methods adopted to date among the frequently cited are the inflexible and subjective way of selecting predictors structural rigidity that makes it difficult to integrate ancillary predictors and the ad hoc way of determining spatial temporal training domains see related discussions in rasp and lerch 2018 the advent of machine learning techniques offers many new opportunities to address these limitations relative to the parametric approaches emos techniques included some of the recent machine learning techniques offer flexibility in identifying predictors in integrating ancillary information and in capturing complex nonlinear predictor predictand relationships that are difficult to characterize parametrically see e g taillardat et al 2019 particularly promising are the various artificial neural networks anns which have been known for their ability to model nonlinear dependencies recent years have seen an explosion of ann based prediction paradigms liu et al 2016 brenowitz and bretherton 2018 gentine et al 2018 rasp et al 2018 chapman et al 2019 cloud et al 2019 gagne et al 2019 lagerquist et al 2019 yet the use of these techniques in the context of postprocessing remains relatively limited rasp and lerch 2018 is perhaps the first attempt of this nature the authors explored a hybrid scheme that retains a parametric form of the predictive distribution of 2 m temperature but relies on anns to estimate the distribution parameters from the ensemble statistics of 2 m temperature as well as ancillary variables scheuerer et al 2020 in a similar vein developed an ann based scheme for producing 7 day accumulated pqpfs at subseasonal range 2 4 weeks from nwp ensemble forecasts and showed that the pqpfs thus generated broadly outperforms climatology other studies of note include bremnes 2020 wherein ann was used for postprocessing wind speed forecasts collectively these studies indicate that embedding local information and incorporating ancillary forecast variables can lead to clear discernible improvements in forecast skills they further suggest that ann models contrary to the common perception of being black boxes can help uncover and offer physical insights to the meteorological processes that underpin the links between predictors and predictands inspired by the successes of recent ann based postprocessing approaches and motivated by the broader need for improving the skill of pqpf while circumventing limitations inherent in existing emos schemes we propose a hybrid ann nonhomogeneous regression based scheme capable of postprocessing precipitation forecasts at multiple lead times and seasons in a unified way the proposed scheme retains the parametric form of the predictive distribution of precipitation proposed by scheuerer and hamill 2015 and baran and nemoda 2016 but departs from the conventional emos by using anns to relate nwp forecasts to the distributional parameters the potential advantages of the proposed scheme which we will henceforth refer to as ann csgd are three fold first this scheme does not require an explicit prescription of predictor predictand relationships as is currently done in emos models it can discover and integrate arbitrary nonlinear relationships through training second the training of the model can be done using the entire data archive and thereby obviate the need for explicit treatment of lead time based and seasonally varying nwp forecast errors third it can account for seasonal variations in the interaction between nwp forecasts and temporal predictors in this paper we describe and evaluate the proposed scheme which relies only on the ensemble mean of nwp forecasts as the major predictor the evaluation is conducted for sub basins within three selected river basins in california the proposed scheme is applied to postprocess global ensemble forecast system gefs hamill et al 2013 precipitation reforecasts along with two benchmark schemes the first is the single predictor version of the censored shifted gamma distribution csgd scheuerer and hamill 2015 the second is the mixed type mata gaussian distribution mmgd wu et al 2011 which has been the standard method in the u s national weather service nws hydrologic ensemble forecast service hefs demargne et al 2014 our overarching hypothesis is that the flexibility accorded by the ann based model in establishing complex predictor distributional parameter relationships in determining temporal training windows and in lumping forecasts for different lead times will help the proposed scheme attain superior predictive performance relative to the benchmarks the reminder of this paper is organized as follows section 2 describes the proposed ann csgd scheme as well as the benchmark methods data and experimental setup section 3 presents the outcomes of the experiments and section 4 summarizes the findings and discusses future possible extensions 2 materials and methods 2 1 proposed model the censored shifted gamma distribution csgd introduced by scheuerer and hamill 2015 has been a popular choice to represent the right skewed mixed type dichotomous continuous nature of the predictive distribution of precipitation scheuerer and hamill 2015 baran and nemoda 2016 zhang et al 2017 scheuerer et al 2020 let f k θ denote the cumulative distribution function cdf of the gamma distribution with shape parameter k 0 and scale parameter θ 0 the cdf at realized precipitation value y and quantile functions of csgd for any 0 p 1 are defined by scheuerer and hamill 2015 baran and nemoda 2016 1 f 0 k θ δ y f k θ y δ y 0 0 y 0 2 q p m a x 0 δ f k θ 1 p where the additional parameter δ 0 shifts the gamma distribution to the negative values to form the csgd the shifted gamma distribution is left censored at zero by assigning the mass probability f k θ δ to the origin to account for non negativity of precipitation amounts to relate the mean μ kθ standard deviation σ k θ and shift parameter δ of predictive csgds to the predictors we propose a fully connected dense feed forward neural network where each node receives a linear combination of weighted outputs from nodes in the previous layer adjusts it by adding a bias quantity and applies an activation function to the result our proposed ann csgd structure fig 1 consists of the following elements input layer where covariates are introduced to the network one hidden layer we use the exponential linear unit elu with α 1 as the activation function to introduce nonlinearity to the network 3 f x x x 0 α exp x 1 x 0 elus are known to provide more precise and faster learning compared to the other activation functions in deep learning experiments see clevert et al 2015 layer normalization ba et al 2016 which normalizes each sample output from hidden nodes to maintain the mean and standard deviation of node outputs within each example close to 0 and 1 respectively recent studies see e g xu et al 2019 show that layer normalization helps stabilize the training process by enabling smoother gradients and yields faster training convergence output layer with a linear activation function we set three csgd parameters as functions of the network outputs o i to constrain the values of these parameters to reasonable ranges i e μ σ 0 and δ 0 therefore we set δ sqrt o 1 2 μ exp o 2 and σ exp o 3 these additional functions can be interpreted as inverse link functions used in conventional distributional regression or generalized additive models for location scale and shape gamlss rigby and stasinopoulos 2005 see also cannon 2012 rasp and lerch 2018 we incorporate the ensemble mean forecast forecast lead time 1 to 7 days and month of the year of the verifying observations 1 to 12 as predictors to the ann using the latter two predictors enables us to train a single model to postprocess forecasts from multiple lead times and months lead time values are normalized by dividing each quantity by the maximum value i e day 7 to account for seasonal cycle we use the cosine term cos 2π month 1 12 to both introduce the cyclical nature of the month of the year to the network and to enforce the network to encode the annual cycle of precipitation over the study area see liu et al 2018 scheuerer et al 2017 we retain the average value of continuous ranked probability score crps of predictive csgds as the loss function for training the weights and biases of the ann csgd the ann is trained by minimizing the crps computed using collocated and coincidental forecast observation pairs over training data see the appendix b for the mathematical definition of crps 4 crps 1 n i 1 n crps f k i θ i δ i y i the analytical expression of crps for a paired csgd predictive distribution and verifying observation was proposed by scheuerer and hamill 2015 similarly we implement 5 crps f k i θ i δ i y i y i δ i 2 f k i θ i y i δ i 1 θ i k i π b 1 2 k i 1 2 1 f 2 k i θ i 2 δ i θ i k i 1 2 f k i θ i δ f k i 1 θ i δ i f k i θ i δ i 2 2 f k i 1 θ i y i δ i δ f k θ δ 2 where b 0 0 is the beta function and ki θ i δ i are three parameters of ith predictive csgd with yi being the corresponding verifying observation to minimize the loss function we use the adam stochastic gradient descent based optimization algorithm kingma and ba 2014 and update model parameters based on small batches randomly sampled from the training dataset one major challenge in applying anns is to constrain the complexity of the model while attaining optimal predictions overfitting can occur if a very complex structure is used several regularization techniques to reduce generalization errors in anns are available as reviewed by goodfellow et al 2016 among them we use early stopping which is one of the most popular and widely used regularization techniques in anns in our work we leave 20 of the available training data as the validation set and do not include them in training process this practice enables us to reduce overfitting by monitoring the average loss value over the validation set while we train the model and return the best possible training parameters weights and biases at the time when the lowest crps for the validation set is achieved we terminate training when no further decrease in validation set loss is seen after 15 iterations through all training batches or the entire training data epochs with up to 1000 epochs we train anns using the previously described process with all possible combinations of different settings using the early stopping technique for the following hyperparameters number of nodes in the hidden layer 5 10 15 batch size 2048 4096 8192 learning rate of the adam optimization algorithm 0 01 0 005 all networks are trained with the same random number generator seed and are evaluated based on the average loss value in the validation set the ann configuration with the lowest validation loss is chosen for out of sample predictions individual tested anns have o 7n 3 trainable parameters where n refers to the number of nodes in the hidden layer we used a simple non trained layer as the normalization layer our assessments showed that training layer normalization parameters beta and gamma does not yield significant improvement over the non trained one and possibly increases the risk of overfitting due to the increased number of overall network parameters 2 2 benchmark models 2 2 1 csgd to generate postprocessed precipitation forecasts at a given location for each forecast lead time and month of the year scheuerer and hamill 2015 first fit three climatological csgd parameters μ cl σ cl and δ cl to locally observed training precipitation data using a 91 day temporal window centered around the 15th of each month in the second step these parameters are included in nonlinear nonhomogeneous regression equations to relate monthly parameters of predictive csgds to statistics of spatially smoothed ensemble of forecasts in this study we use the regression equations that incorporate only the ensemble mean 6 μ μ c l a 1 log 1 exp a 1 1 a 2 a 3 f f c l 7 σ a 4 σ c l μ μ c l 8 δ δ c l where f and f c l correspond to the raw ensemble mean forecasts and their climatological mean in training data respectively in the version of csgd described in scheuerer and hamill 2015 the predictive shift parameter δ is kept identical to the climatological shift to ensure that the predictive csgd reverts to climatology as a limiting case when the forecast becomes less skillful e g at longer lead times see related discussion in scheuerer and hamill 2015 the four regression coefficients a 1 a 2 a 3 a 4 are estimated by minimizing the crps using the closed form expression proposed by scheuerer and hamill 2015 see sec 2 1 as a function of csgd parameters over training data past studies scheuerer and hamill 2015 baran and nemoda 2016 zhang et al 2017 baran and lerch 2018 taillardat et al 2019 show that csgd method and its variants perform well in comparison with other modern postprocessing techniques recent exploratory analyses see ghazvinian et al 2020 fig 1 showed that the climatological csgd shift parameter derived by crps minimization approach tends to be inflated and this leads to an underestimation of a probability of precipitation pop this bias directly affected the performance of predictive csgd primarily in predicting pop and to a degree the predicted magnitude of precipitation this was particularly evident at shorter lead times and in rainy seasons where the predictive distribution of precipitation deviates widely from climatology 2 2 2 mmgd the mmgd herr and krzysztofowicz 2005 wu et al 2011 was developed by the u s nws as a component of the meteorological ensemble forecast processor mefp of the operational hefs demargne et al 2014 this mechanism is routinely used to generate calibrated pqpf from single valued precipitation forecasts ensemble mean at river basin scales and at temporal aggregation scales ranging from 6 h to 3 months and for lead times up to 9 months wu et al 2018 demargne et al 2014 in contrast to the csgd where pop and the probability of magnitude of precipitation are estimated using the same predictive distribution mmgd uses a bayesian approach to break down the predictive distribution to explicitly account for the dichotomous continuous nature of precipitation let x and y denote the random variables of a single valued quantitative precipitation forecast and the observed precipitation amount respectively the conditional distributions of observed precipitation given a current forecast of no precipitation and positive precipitation are given as follows details of this derivation can be found in wu et al 2011 and ghazvinian et al 2020 9 f y x y x x 0 p y 0 x 0 p 0 y y x 0 a 1 a g y y 10 f y x y x x 0 p y y x x x 0 c x 1 c x d y x y x where a and c x represent mass probabilities of observed precipitation being equal to zero and are combined with the continuous conditional distributions gy y p y y x 0 y 0 and d y x y x p y y x x x 0 y 0 to construct the predictive distributions to estimate d y x y x its marginal continuous variates x x 0 y 0 and y x 0 y 0 undergo normal quantile transformation nqt yielding standard normal variates u φ 1 dx x and v φ 1 dy y following the meta gaussian distribution theorem of kelly and krzysztofowicz 1997 d y x y x assumes the following form 11 d y x y x φ φ 1 d y y ρ φ 1 d x x 1 ρ 2 where φ and φ 1 denote the standard normal cdf and quantile function of standard normal distribution respectively and ρ is the pearson s product correlation coefficient between uand v the performance of mmgd has been evaluated in a number of studies see e g wu et al 2011 brown et al 2014a demargne et al 2014 kim et al 2018 seo et al 2015 ghazvinian et al 2019 while conclusions indicate that overall mmgd produces reliable pqpfs and is capable of preserving the skill in the raw forecast its pqpfs underestimate heavy to extreme precipitation amounts low reliability for higher thresholds the latter finding was also corroborated by zhang et al 2017 where the authors compared the performances of mmgd and csgd over the mid atlantic region in u s their results pointed to the superior performance of csgd in that study csgd s ability to ingest additional ensemble statistics as predictors was shown to play a key role in its outperformance further performance comparisons by ghazvinian et al 2020 which relied on only the ensemble mean predictor and were conducted over the american river basin in california pointed to the clear outperformance of mmgd particularly in predicting pop the authors confirmed that the use of a two part scheme helped improve the representation of the predictive distribution we select mmgd as the second reference model to further address these discrepancies in the findings of previous studies this enables us to determine whether our unified ann csgd model improves upon the operational paradigm mmgd especially in situations where csgd underperforms the latter and helps us identify possible factors that contribute to the differential performance of the three schemes 2 3 data and experimental setup the experiments focus on 24 h mean areal precipitation map totals over sub basins of three major river basins in the service area of the nws california nevada river forecast center cnrfc https www cnrfc noaa gov we use ensemble mean precipitation forecasts from january 1985 through december 2016 32 years for lead times 1 to 7 days these data were obtained from the global ensemble forecast system gefs version 10 reforecast dataset hamill et al 2013 and were processed by the cnrfc at 1 degree spatial resolution and 6 h accumulation intervals issued daily at 00 universal time utc as ground truth we use the basin map data generated by the cnrfc the map data were created using the so called mountain mapper tool which relies on the parameter elevation regressions on independent slopes model prism daly et al 2008 to group gauges and interpolate gauge reports onto the domain of each watershed the cnrfc map series are at 6 h increments and are available for the period between october 1948 and september 2017 the map data were temporally aggregated to 24 h accumulation and paired with coincidental reforecasts postprocessing experiments are performed over sub basins in the american river basin nfdc1 folc1 the russian river basin wsdc1 guec1 and the eel river basin dosc1 ftsc1 fig 2 and separately for upper lower elevation zones when applicable sub basin names and corresponding nws ids are presented in table 1 the cnrfc runs hefs routinely to produce postprocessed pqpfs and ensemble streamflow forecasts for many of the sub basins for each river basin we selected one headwater and one downstream sub basin for the hindcast experiment to examine the potential elevation dependence in forecast skills the selected basins have been recognized for their importance in water resources management and flood control as noted in past hydrometeorological forecast postprocessing verification studies see e g wu et al 2011 brown et al 2012 seo et al 2015 he 2016 scheuerer et al 2017 ghazvinian et al 2020 the climate of the region is characterized by very dry summers with most of its annual precipitation falling during the cool season october april and the highest monthly averaged precipitation typically recorded in january the american river originates from the tahoe and el dorado national forests of the sierra nevada and is one of the major water supply sources for california streamflow in the american river is mainly 2 3 supplied from wintertime rainfall and snowmelt runoff with a small portion 1 3 from spring to early summer snowmelt runoff dettinger et al 2014 on the other hand the russian and eel river basins are coastal basins where snowmelt runoff is much less important scheuerer et al 2017 to be consistent with the cnrfc operations we use the nearest neighbor interpolation brown et al 2014a seo et al 2015 ghazvinian et al 2020 to pair forecasts observations for generating pqpfs and evaluating the performances of ann csgd relative to the two benchmark models we adopt an 8 fold cross validation approach in this approach for a given basin we divide the data to 8 consecutive 4 year length folds predictions for each fold are produced using each postprocessing mechanism trained with the data of remaining 7 folds 28 years postprocessed out of sample forecasts from all models are verified against observations in individual months of the year in verification years and separately for each sub basin and forecast lead time this leads to 32 years of verified forecasts for each sub basin and lead time while the ann csgd uses the entire available training data i e covering all lead times and seasons for training and hyperparameter tuning the benchmark models are trained using subsamples representing each forecast lead time and a month season of the year to gain insights on how increasing the length of training record and using different seasonal windows for training can affect the predictions of benchmark models we train each model with different training window sizes and regulations a summary of training schemes for ann csgd and benchmark models is provided as follows unified approach ann csgd uses forecast observation pairs of all months and lead times of training years for training and hyperparameter tuning resulting in a training sample size of up to 7 lead times 28 years 365 days 71540 20 of which is dedicated for hyperparameter tuning and not used in training mmgd and csgd with 61 days and 91 days training windows mmgd 61 csgd 61 and mmgd 91 csgd 91 use 61 and 91 training days around the 15th of each month across training years for generating pqpf for out of sample data of that month yielding training sample size up to 28 years 61 days 1708 and 28 years 91 days 2548 for each lead time and month respectively 61 days and 91 days training windows have been used in several past studies e g hamill et al 2015 scheuerer and hamill 2015 scheuerer and hamill 2018 scheuerer et al 2017 wu et al 2018 mmgd seasonal training scheme mmgd seasonal where forecasts in out of sample data from the cool october april and dry may september seasons are postprocessed by a model trained using the data in each season thus a single model is trained for each season and each lead time csgd seasonal training scheme csgd seasonal scheuerer et al 2020 where the climatological csgd parameters μ cl σ cl and δ cl as well as the climatological mean forecast f c l are derived using a 61 day window around the 15th of each month but the same regression coefficients are used across cool and dry seasons to increase the training sample size the latter two training schemes yield a sample size of up to 5942 and 4284 for the cool and dry seasons respectively 3 results in this section we present verification results using different metrics see appendix b for mathematical definitions and details we first use the continuous ranked probability skill score crpss to assess the overall predictive performance of pqpfs from ann csgd relative to those from the benchmark models with different training scenarios subsequently we analyze ann csgd s performance relative to the benchmark models with a 61 day training window using brier skill score bss reliability diagrams and mean squared error skill score msess 3 1 overall predictive performance of pqpfs fig 3 compares crpss of pqpfs from ann csgd and those from the benchmark models with different training scenarios and for the three river basins the results are computed using cross validated forecasts from all months and are aggregated over sub basins of each river basin with mmgd 61 as the reference forecast to assess whether differences in predictive performances shown are statistically significant we perform one sided diebold mariano test diebold and mariano 1995 for all possible pairs of model comparisons see appendix b for details these results are provided in tables s1 s3 in the supplemental material to this article overall ann csgd generates the most skillful pqpfs across lead times in the american river fig 3a ann csgd outperforms its baseline csgd with different training scenarios by a wide margin the improvement upon each csgd scheme is statistically significant at all lead times nevertheless performance differences between ann csgd and each of mmgds are not statistically significant in the russian river basin fig 3b ann csgd significantly outperforms each of benchmark models in a large number of cases in the eel river basin fig 3c ann csgd outperforms both mmgds and csgds though its difference with mmgd 61 is not statistically significant it is apparent that the relative performance of mmgd and csgd varies by river basin and at different lead times except for the american river basin where most differences are not statistically significant the seasonal version of mmgd trails behind those calibrated with 61 and 91 day moving windows for all three river basins the performance differences of csgd 61 and csgd 91 are not statistically significant across the lead times interestingly unlike mmgd seasonal csgd seasonal tends to considerably improve its performance at longer lead times and for all river basins the training strategy used in csgd seasonal was recently introduced by scheuerer et al 2020 in their subseasonal forecast scheme 2 week ahead this scheme presumes that nwp forecast error characteristics change on a season scale when the forecast has very limited skill our result confirms the hypothesis that performance is enhanced through the use of wider seasonal windows expanding the seasonal window potentially reduces the risk of overfitting of nonlinear csgd regression model coefficients at longer lead times when the signal to noise ratio is rather poor the results corroborate our postulation that different temporal data pooling methods for training statistical postprocessing models exert influences on the accuracy of postprocessed pqpfs the use of mmgd as an alternative scheme serves to further illustrate the significance of ann csgd model emos methods such as csgd are deemed inflexible in that the response variable in these models is assumed to follow a single unimodal parametric distribution see e g taillardat et al 2016 wu et al 2019 baran and lerch 2018 which potentially limits their performance as such why does ann csgd retain its superior performance relative to csgd across lead times and study basins while both use the same predictive distribution this is most likely due to the fact that ann csgd uses the entire training dataset and encodes nonlinear lead time and seasonal error dependencies in forecasts in an adaptable manner thus it can preserve the skill of raw forecast particularly at longer lead times where postprocessing via csgd seasonal offers marginal benefit or even degrades forecast skill another advantage of the proposed scheme is that it reduces the risk of overfitting due to the early stopping algorithm implemented as a part of its training 3 2 brier skill score and reliability fig 4 shows the results of bss for three thresholds 0 25 30 and 60 mm 24 h and for the three river basins while both ann csgd and csgd underperform mmgd in predicting events 0 25 mm i e pop ann csgd interestingly conspicuously outperforms csgd fig 4a c as pointed out by ghazvinian et al 2020 csgd performs poorly in predicting the pop due to its reliance on the climatological shift parameter see also sec 2 2 1 for further details when the forecast is very skillful the predictive csgd departs from climatology so does the optimal shift parameter at longer forecast lead times the forecast skill declines and the predictive csgd tends to approach the unconditional climatological one this feature is reflected in the improvement in csgd s performance across the lead times ann csgd on the other hand directly estimates the shift parameter of the predictive csgd as an arbitrary function of predictors thus eliminating the need for a climatological shift parameter this results in large and statistically significant improvements relative to the csgd in predicting the pop as for the outperformance of mmgd relative to the ann csgd we hypothesize that the flexible two part structure of mmgd is likely a major contributor a detailed discussion on this matter can be found in ghazvinian et al 2020 at the middle threshold of 30 mm day ann csgd outperforms both schemes in the american river basin fig 4d in the russian river basin and the eel river basin fig 4e and f the relative performance of ann csgd and csgd is mixed but both manage to outperform mmgd except at day 7 in the russian river basin where csgd slightly underperforms though it is not statistically significant not shown here at the highest threshold namely 60 mm day fig 4g i ann csgd outperforms all other schemes csgd mostly outperforms mmgd in the russian river basin fig 4h but underperforms the latter in american river and eel river basins fig 4g i to compare the calibration of pqpfs produced through each scheme we plot reliability diagrams for the same events and evaluate the contribution of reliability and resolution to the brier score figs 5 7 to attain a large enough sample size to better study larger thresholds we lump cross validate forecasts at all lead times and divide forecast probabilities 0 1 into 15 evenly distributed probability categories to discern the differential performance of schemes under higher probability categories the major findings for each river basin are summarized as follows american river basin in predicting positive precipitation events 0 25 mm day fig 5a c ann csgd s outperformance relative to csgd is attributed to improvements in both reliability lower rel and resolution higher res ann csgd mitigates to a great extent the underforecast issue of csgd ann csgd generates pqpfs that are more reliable than mmgd but are characterized with lower resolution yielding an overall inferior predictive performance at higher thresholds fig 5d i ann csgd clearly outperforms both csgd and mmgd in terms of both reliability and resolution as shown in the histograms embedded in each subplot ann csgd generates pqpfs that are able to issue high probabilities in predicting mid to heavy precipitation with higher frequencies and this points to improved sharpness fig 5f and i russian river basin similar to the american river basin at the lowest threshold fig 6a c ann csgd produces forecasts with higher reliability lower rel than mmgd but with lower resolution and overall lower predictive skill higher bs in 30 mm day ann csgd performs better than csgd in terms of both reliability and resolution fig 6e f at the highest threshold fig 6h i the lack of reliability in ann csgd pqpfs relative to those from csgd is compensated by the higher resolution and this leads to a superior predictive performance of the former as evidenced by the lower bs mmgd at both thresholds fig 6d g produces less reliable pqpfs with lowest sharpness at the 30 mm day threshold fig 6d mmgd pqpfs resolution is somewhat higher but is compensated by lower reliability eel river basin at the lowest threshold 0 25mm day fig 7a c the relative performance of schemes is quite similar to that for the other two river basins with ann csgd outperforming mmgd in terms of reliability but not resolution at higher thresholds fig 7d i pqpfs from ann csgd are more reliable and sharper and overall more skillful lowest bs though at the highest threshold i e 60 mm day the former exhibit slightly lower resolution than those from mmgd but this is compensated by superior reliability 3 3 evaluation of deterministic forecasts finally we compute mean squared error skill score msess to evaluate the performance of the distribution mean of pqpf produced using each scheme relative to the gefs ensemble mean forecast fig 8 these results are accompanied by the results of the diebold mariano test based on the squared error of mean pqpfs see tables s4 s6 in the supplemental material the relative performance varies among the river basins for the american river basin fig 8a all postprocessed pqpfs outperform the gefs ensemble mean in terms of msess ann csgd pqpfs perform favorably against mmgd pqpfs for all three river basins the performance differences are not statistically significant for both the russian and eel river basins fig 8b and c msess values are generally lower relative to those for the american river basin this as we posit is attributable to location dependent biases in the gefs ensemble mean forecast for example gefs is more skillful in the russian and eel river basins according to the msess results relative to climatological forecasts the results are shown in fig s1 of supplemental materials for the russian river basin fig 8b underperformance of postprocessed pqpf relative to the gefs ensemble mean is seen however the performance differences are not statistically significant unlike the benchmarks mean pqpf from ann csgd for russian river basin significantly outperforms gefs ensemble mean forecast in all lead times for both the russian and eel river basins fig 8b c ann csgd tends to outperform the other two schemes though the performance differentials are not statistically significant when comparing with mmgd 4 discussion and conclusions we propose a unified univariate hybrid neural network parametric pqpf postprocessing scheme capable of producing postprocessed forecasts for lead times at least up to 7 days medium range this scheme retains the use of parametric predictive distribution but employs ann to estimate distribution parameters from forecast observation pairs the predictors explored in this study include ensemble mean forecast forecast lead time and month of the year whereas the predictands are three parameters of the predictive censored shifted gamma distribution csgd the ann csgd model parameters were obtained by minimizing a loss function that is the closed form expression of crps for csgd scheuerer and hamill 2015 with the adam stochastic gradient descent algorithm kingma and ba 2014 as the optimization approach to test the performance of our model we conducted cross validation experiments to generate medium range lead times 1 7 days daily accumulated pqpfs over selected river basins in the service area of the cnrfc we used two benchmarking postprocessing schemes in this study namely the csgd emos scheuerer and hamill 2015 with a single predictor formulation and the nws operational postprocessor mixed type meta gaussian distribution mmgd these benchmark models were calibrated based on different seasonal data pooling scenarios to investigate the possible impacts of training window size and strategies on the performance of postprocessed pqpfs verification results showed that ann csgd in general outperform the baseline csgd and mmgd in terms of overall calibration and significantly so in some cases interestingly ann csgd mainly impacts improves bss of pqpf from csgd at the lowest threshold which has disproportionate impacts on crpss ann csgd manages to address the csgd s poor performance in predicting pop as noted in ghazvinian et al 2020 while the ann csgd performance comparison results are mixed in predicting 30 mm day thresholds it outperforms both benchmark models in predicting large extreme events 60 mm day on average the proposed method generates high probability forecasts for heavy precipitation more frequently than benchmarks as assessed by sharpness histograms higher sharpness this is particularly useful to cnrfc s operational precipitation and flood forecasting practice and thus could benefit real time reservoir operations e g determining reservoir release schedules in california in its current practice cnrfc relies on hefs to produce pqpfs from nwp precipitation forecasts and then generates ensemble streamflow forecasts which are used to guide real time flood management and control practices the mmgd model embedded in hefs has shown to systematically underestimate heavy precipitation amounts leading to negative biases in subsequent flood forecasts demargne et al 2014 brown et al 2014b the superior performance of the proposed ann csgd on heavy precipitation estimation makes it a viable tool to address limitations in the forecast skills for extreme precipitation and floods these improvements in forecasts will in turn serve to aid real time reservoir operations and flood risk management in contrast to the csgd version of scheuerer and hamill 2015 the proposed method directly estimates predictive csgd s shift parameter given each set of predictors in doing so it circumvents the need of invoking climatology and thereby alleviates the bias issue in estimating the pop in the existing csgd scheme furthermore the use of ann allows for representations of complex interactions between three predictive csgd parameters together these new features help the scheme produce sharper narrower predictive distributions than the benchmark csgd moreover ann csgd is able to use much larger training data with extra high forecast observation values and efficiently translate this to predictive skill at the highest threshold the new scheme also has a distinct practical advantage in that it eliminates the need for more computationally expensive and operationally labor intensive approach used in most contemporary statistical postprocessing schemes whereas the benchmark models need to be re trained for every forecasting lead time and month season ann csgd does not and it can simultaneously utilize forecast observation pairs across all lead times months and seasons our results support our hypothesis that the fixed size seasonal window training schemes for current postprocessing methods may not be sufficient for generating consistently skillful pqpfs across all lead times in other words the performance of existing schemes may be improved by identifying an optimal seasonal training window specific for each lead time depending on the study area and the statistical model at hand for example it was shown that a seasonal csgd tended to improve the performance benchmark 61 day and 91 day csgds at longer lead times but not in shorter lead times ann csgd on the other hand automatically adapts to the changes in raw forecasts observations errors along with all lead times and seasons and hence is capable of producing pqpfs with consistently higher skills a major limitation of nonhomogeneous regression or gamlss techniques is that their performance is dependent on the robustness of user prescribed regression relationships moreover they are typically limited in digesting ordinal temporal covariates such as those used in the ann csgd model the proposed model by contrast can freely learn to characterize arbitrary nonlinear predictor distribution parameters relationships and among predictors interactions efficiently a well known challenge in training ann models is model configuration hyperparameter tuning to achieve the best validation score generally it is very difficult to find the best possible ann configuration in a very large parameter space as pointed out by scher 2018 there is a trade off between robustness which depends on the depth and thoroughness of grid search and computational expenses for example our initial assessment showed that maintaining the architecture but expanding the number of layers does not significantly improve the model performance other regularization techniques such as l1 could be used in combination with early stopping to further reduce generalization errors however these techniques could require deeper search for hyperparameters and therefore increase computational complexity we also experimented with training embedding layers with different sizes 2 3 4 5 6 7 to project discrete lead times onto a larger vector of inputs but only found very marginal improvements in the validation score therefore we decided not to include embedding layers in our final model in future work we aim to extend the current approach to create a spatially adaptable scheme for postprocessing medium range ensemble precipitation forecasts on a gridded basis we expect to achieve this by incorporating geographical information into the network as shown by scheuerer et al 2020 in their subseasonal forecasting approach for example entire ensemble members or their statistics at a grid point in addition to those from a specific radius of surrounding grid points can be direct inputs to the model as the predictors such a model potentially eliminates the need for generating a local superensemble to address the issue of displacement errors in gridded precipitation forecasts additionally the current study focuses on 24 h accumulated precipitation in operations cnrfc produces 6 hourly pqpfs and updates their forecasts every 6 h during major storm events to align with cnrfc operations we also plan to explore the performance of the proposed ann csgd in generating 6 hourly pqpfs in our future work finally stacked convolution or long short term memory lstm layers applied on top of embedding vectors appear to be very effective in object detection krizhevsky et al 2012 in computer vision and in natural language processing collobert et al 2011 including machine translation and question answering devlin et al 2018 we envision investigating similar techniques to possibly improve the skill of postprocessed forecast at longer lead times credit authorship contribution statement mohammadvaghef ghazvinian conceptualization data curation formal analysis methodology software validation visualization writing original draft yu zhang conceptualization funding acquisition investigation methodology project administration resources software supervision writing review editing dong jun seo conceptualization methodology funding acquisition writing review editing minxue he conceptualization methodology writing review editing nelun fernando conceptualization methodology resources writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors thank the editor and reviewers for their valuable comments that helped improve the article the first author was financially supported by the faculty startup fund for dr yu zhang provided by ut arlington noaa grant na18oar4590370 01 texas water development board contract no 1800012276 and nsf grant 1909367 these supports are duly acknowledged here the authors would also like to thank michael scheuerer at norwegian computing center nr whose comments and suggestions led to the development of the scheme and cnrfc staff for providing the forecast and analysis archive supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2021 103907 appendix c supplementary materials image application 1 appendix a implementation details we implemented our ann codes in python python software foundation 2018 using google s deep learning platform tensorflow abadi et al 2016 and keras api chollet et al 2015 for fitting csgd climatological and predictive distributions r core team 2018 scripts provided by dr michael scheuerer were used to calibrate nws postprocessor mixed type meta gaussian distribution mmgd a research version very similar to the operational one was implemented in r appendix b verification metrics used in this study a mean squared error skill score msess the mean squared error skill score msess jolliffe and stephenson 2003 measures the reduction in mean squared error mse of deterministic forecast mean pqpf ensemble mean and verifying observations relative to the reference forecast a1 m s e s s 1 1 n i 1 n x i y i 2 1 n i 1 n x i r e f y i 2 positive values of msess indicates improvement in skill of deterministic forecast relative to the reference forecast b brier skill score bss the brier score bs brier 1950 is equivalent to mean squared error of probabilistic forecast exceeding a given threshold over n pairs of forecast and observations a2 b s τ 1 n i 1 n f i τ i y i τ 2 where fi τ is the probability of probabilistic forecast exceeding the threshold value τ and i is the indicator step function that takes the value 1 if the ith verifying observation exceeds the threshold value and 0 otherwise bs is negatively oriented and ranges from zero to one to assess the improvement in bs relative a reference forecast we compute brier skill score a3 b s s 1 b s b s r e f positive values of bss indicate improvement of bs over that of reference forecast brier score can be decomposed to three terms reliability or type i conditional bias resolution and uncertainty murphy 1973 wilks 2011 a4 b s τ r e l i a b i l i t y τ r e s o l u t i o n τ u n c e r t a i n i t y τ 1 n i 1 k n i f i τ o i τ 2 1 n i 1 k n i o i τ o τ 2 o τ 1 o τ where k indicates the number of categories forecast are aggregated to n is the number of cases in each category o i τ is the average climatological probability acp exceeding the threshold τ in that category and o τ is the overall acp it should be noted that uncertainty term as seen is independent of the forecast source probabilistic forecasts with lower higher reliability resolution values are desirable c continuous ranked probability score crps the continuous ranked probability score crps matheson and winkler 1976 measures the integral of squared differences between the cumulative distribution function cdf of probabilistic forecast and verifying observation it is a popular metric to assess the overall predictive performance of probabilistic forecasts sharpness and reliability see gneiting et al 2007 for further details crps averaged over the sample of forecast observations with size of n is given by a5 c r p s 1 n i 1 n f i x i y i x 2 d x where fi denotes the cdf of pqpf at the ith forecast instance and yi is the verifying observation i is the indicator step function which takes the value of 1 if x yi and 0 elsewhere continuous ranked probability skill score crpss is routinely used to assess the performance of probabilistic forecast relative to a reference forecast a6 c r p s s 1 c r p s c r p s r e f d reliability diagrams and sharpness histograms the reliability and resolution of a probabilistic forecast for exceeding some specific thresholds τ can be assessed graphically using reliability diagrams the reliability diagram consists of a plot of the average values of forecast probabilities exceeding τ against that of observed relative frequencies over each defined probability category in a reliable probabilistic forecast the reliability diagram should be close to 1 1 line interested readers are referred to brocker and smith 2007 and wilks 2011 for details on how to interpret the deficiencies in probabilistic forecasts using reliability diagrams to assess the sharpness of pqpf for specific thresholds we use sharpness histograms to investigate the frequency of forecast probabilities for different probability bins note a sharp forecast is characterized by higher frequencies for the forecast probabilities close to either 0 or 1 e the diebold mariano test to assess statistical significance of verification score differences between two forecast methods we use the diebold mariano statistical test of the null hypothesis of equal predictive performance diebold and mariano 1995 let δ s f1 s f2 denote the vector of verification score s differences from two competing forecast methods f 1 and f 2 over verification sample with length n δ 1 n i 1 n δ i and σ δ a suitable estimator of asymptotic standard deviation of δ under standard regularity conditions the test statistic t n n δ σ δ asymptotically follows a standard gaussian distribution under the null hypothesis of no difference in predictive performances of two competing forecast methods following the past studies baran and lerch 2016 2018 rasp and lerch 2018 σ δ can be estimated by square root of sample autocovariance up to lag k 1 for the k step ahead forecasts to account for temporal dependencies in forecast errors we use one sided diebold mariano tests the alternative hypothesis is that forecast method f 2 underperforms forecast method f 1 and the statistical significancy of the test s statistic can be assessed by obtaining corresponding p value we perform the tests based on both crps and squared error of mean pqpf on a limited basis and for each lead time and separately for each river basin to address spatial dependence of forecast errors scores are averaged across sub basins in each river basins m scheuerer 2021 personal communication further we adjust the test results by accounting for test multiplicity i e simultaneously analyzing test results of multiple lead times using false discovery rate fdr method benjamini and hochberg 1995 by controlling the fdr at the level α fdr 0 05 note that this procedure was discussed by wilks 2016 in spatial context where test results are interpreted simultaneously across multiple grid points but also was suggested to be applied whenever the results of simultaneous several hypothesis tests are reported or interpreted 
325,many present day statistical schemes for postprocessing weather forecasts in particular precipitation forecasts rely on calibration using prescribed statistical models to relate forecast statistics to distributional parameters the efficacy of such schemes is often constrained not only by prescribed predictor predictand relation but also by arbitrary choices of temporal window and lead time range for training to address this limitation we propose an end to end computationally efficient hybrid postprocessing scheme capable of producing full predictive distributions of precipitation accumulation without explicit stratification of forecast observation pairs by forecast lead time and season the proposed framework uses the censored shifted gamma distribution csgd as the predictive distribution but uses an artificial neural network ann to estimate the distributional parameters of csgd through a unified approach this approach referred to as ann csgd allows for simultaneous estimation of distributional parameters over multiple lead times and seasons in a single model by incorporating the latter variables as predictors to the ann we test our proposed ann csgd model for postprocessing of ensemble mean forecasts of 24 h precipitation totals over selected river basins in california at one to seven day lead times from the global ensemble forecast system gefs the probabilistic quantitative precipitation forecasts pqpfs from the ann csgd are more skillful overall than those from the benchmark csgd and the mixed type meta gaussian distribution mmgd models the ann csgd pqpfs highly improve the performance of those from csgd in predicting the probability of precipitation pop and are also much sharper and reliable at higher precipitation thresholds we demonstrate how the hybrid approach by using the entire available training data and its modified formulation efficiently represents interactions between gefs forecasts and season lead times thus leading to enhanced predictive performance keywords statistical postprocessing artificial neural networks probabilistic quantitative precipitation forecast predictive distribution 1 introduction statistical postprocessing techniques are increasingly used to improve the reliability and skill of real time probabilistic quantitative precipitation forecasts pqpfs produced by numerical weather prediction nwp models broadly speaking these techniques can be categorized as nonparametric and parametric ones a prominent example of the former is the analog approach hamill and whitaker 2006 hamill et al 2015 the parametric techniques rely on prescribed parametric forms of conditional predictive joint and marginal distributions and employ various techniques ranging from regression to the method of moments and their variants for estimating distributional parameters many of the modern parametric approaches fall under the broad umbrella of ensemble model output statistics emos gneiting et al 2005 also known as nonhomogeneous regression as the name implies the emos approaches use prescribed predictive distributions and relate distributional parameters to ensemble statistics through a set of regression equations scheuerer and hamill 2015 zhang et al 2017 stauffer et al 2017 the extent to which postprocessing techniques have improved forecast skill has varied in practice li et al 2017 wilks 2018 vannitsem et al 2020 there are several common limitations in postprocessing methods adopted to date among the frequently cited are the inflexible and subjective way of selecting predictors structural rigidity that makes it difficult to integrate ancillary predictors and the ad hoc way of determining spatial temporal training domains see related discussions in rasp and lerch 2018 the advent of machine learning techniques offers many new opportunities to address these limitations relative to the parametric approaches emos techniques included some of the recent machine learning techniques offer flexibility in identifying predictors in integrating ancillary information and in capturing complex nonlinear predictor predictand relationships that are difficult to characterize parametrically see e g taillardat et al 2019 particularly promising are the various artificial neural networks anns which have been known for their ability to model nonlinear dependencies recent years have seen an explosion of ann based prediction paradigms liu et al 2016 brenowitz and bretherton 2018 gentine et al 2018 rasp et al 2018 chapman et al 2019 cloud et al 2019 gagne et al 2019 lagerquist et al 2019 yet the use of these techniques in the context of postprocessing remains relatively limited rasp and lerch 2018 is perhaps the first attempt of this nature the authors explored a hybrid scheme that retains a parametric form of the predictive distribution of 2 m temperature but relies on anns to estimate the distribution parameters from the ensemble statistics of 2 m temperature as well as ancillary variables scheuerer et al 2020 in a similar vein developed an ann based scheme for producing 7 day accumulated pqpfs at subseasonal range 2 4 weeks from nwp ensemble forecasts and showed that the pqpfs thus generated broadly outperforms climatology other studies of note include bremnes 2020 wherein ann was used for postprocessing wind speed forecasts collectively these studies indicate that embedding local information and incorporating ancillary forecast variables can lead to clear discernible improvements in forecast skills they further suggest that ann models contrary to the common perception of being black boxes can help uncover and offer physical insights to the meteorological processes that underpin the links between predictors and predictands inspired by the successes of recent ann based postprocessing approaches and motivated by the broader need for improving the skill of pqpf while circumventing limitations inherent in existing emos schemes we propose a hybrid ann nonhomogeneous regression based scheme capable of postprocessing precipitation forecasts at multiple lead times and seasons in a unified way the proposed scheme retains the parametric form of the predictive distribution of precipitation proposed by scheuerer and hamill 2015 and baran and nemoda 2016 but departs from the conventional emos by using anns to relate nwp forecasts to the distributional parameters the potential advantages of the proposed scheme which we will henceforth refer to as ann csgd are three fold first this scheme does not require an explicit prescription of predictor predictand relationships as is currently done in emos models it can discover and integrate arbitrary nonlinear relationships through training second the training of the model can be done using the entire data archive and thereby obviate the need for explicit treatment of lead time based and seasonally varying nwp forecast errors third it can account for seasonal variations in the interaction between nwp forecasts and temporal predictors in this paper we describe and evaluate the proposed scheme which relies only on the ensemble mean of nwp forecasts as the major predictor the evaluation is conducted for sub basins within three selected river basins in california the proposed scheme is applied to postprocess global ensemble forecast system gefs hamill et al 2013 precipitation reforecasts along with two benchmark schemes the first is the single predictor version of the censored shifted gamma distribution csgd scheuerer and hamill 2015 the second is the mixed type mata gaussian distribution mmgd wu et al 2011 which has been the standard method in the u s national weather service nws hydrologic ensemble forecast service hefs demargne et al 2014 our overarching hypothesis is that the flexibility accorded by the ann based model in establishing complex predictor distributional parameter relationships in determining temporal training windows and in lumping forecasts for different lead times will help the proposed scheme attain superior predictive performance relative to the benchmarks the reminder of this paper is organized as follows section 2 describes the proposed ann csgd scheme as well as the benchmark methods data and experimental setup section 3 presents the outcomes of the experiments and section 4 summarizes the findings and discusses future possible extensions 2 materials and methods 2 1 proposed model the censored shifted gamma distribution csgd introduced by scheuerer and hamill 2015 has been a popular choice to represent the right skewed mixed type dichotomous continuous nature of the predictive distribution of precipitation scheuerer and hamill 2015 baran and nemoda 2016 zhang et al 2017 scheuerer et al 2020 let f k θ denote the cumulative distribution function cdf of the gamma distribution with shape parameter k 0 and scale parameter θ 0 the cdf at realized precipitation value y and quantile functions of csgd for any 0 p 1 are defined by scheuerer and hamill 2015 baran and nemoda 2016 1 f 0 k θ δ y f k θ y δ y 0 0 y 0 2 q p m a x 0 δ f k θ 1 p where the additional parameter δ 0 shifts the gamma distribution to the negative values to form the csgd the shifted gamma distribution is left censored at zero by assigning the mass probability f k θ δ to the origin to account for non negativity of precipitation amounts to relate the mean μ kθ standard deviation σ k θ and shift parameter δ of predictive csgds to the predictors we propose a fully connected dense feed forward neural network where each node receives a linear combination of weighted outputs from nodes in the previous layer adjusts it by adding a bias quantity and applies an activation function to the result our proposed ann csgd structure fig 1 consists of the following elements input layer where covariates are introduced to the network one hidden layer we use the exponential linear unit elu with α 1 as the activation function to introduce nonlinearity to the network 3 f x x x 0 α exp x 1 x 0 elus are known to provide more precise and faster learning compared to the other activation functions in deep learning experiments see clevert et al 2015 layer normalization ba et al 2016 which normalizes each sample output from hidden nodes to maintain the mean and standard deviation of node outputs within each example close to 0 and 1 respectively recent studies see e g xu et al 2019 show that layer normalization helps stabilize the training process by enabling smoother gradients and yields faster training convergence output layer with a linear activation function we set three csgd parameters as functions of the network outputs o i to constrain the values of these parameters to reasonable ranges i e μ σ 0 and δ 0 therefore we set δ sqrt o 1 2 μ exp o 2 and σ exp o 3 these additional functions can be interpreted as inverse link functions used in conventional distributional regression or generalized additive models for location scale and shape gamlss rigby and stasinopoulos 2005 see also cannon 2012 rasp and lerch 2018 we incorporate the ensemble mean forecast forecast lead time 1 to 7 days and month of the year of the verifying observations 1 to 12 as predictors to the ann using the latter two predictors enables us to train a single model to postprocess forecasts from multiple lead times and months lead time values are normalized by dividing each quantity by the maximum value i e day 7 to account for seasonal cycle we use the cosine term cos 2π month 1 12 to both introduce the cyclical nature of the month of the year to the network and to enforce the network to encode the annual cycle of precipitation over the study area see liu et al 2018 scheuerer et al 2017 we retain the average value of continuous ranked probability score crps of predictive csgds as the loss function for training the weights and biases of the ann csgd the ann is trained by minimizing the crps computed using collocated and coincidental forecast observation pairs over training data see the appendix b for the mathematical definition of crps 4 crps 1 n i 1 n crps f k i θ i δ i y i the analytical expression of crps for a paired csgd predictive distribution and verifying observation was proposed by scheuerer and hamill 2015 similarly we implement 5 crps f k i θ i δ i y i y i δ i 2 f k i θ i y i δ i 1 θ i k i π b 1 2 k i 1 2 1 f 2 k i θ i 2 δ i θ i k i 1 2 f k i θ i δ f k i 1 θ i δ i f k i θ i δ i 2 2 f k i 1 θ i y i δ i δ f k θ δ 2 where b 0 0 is the beta function and ki θ i δ i are three parameters of ith predictive csgd with yi being the corresponding verifying observation to minimize the loss function we use the adam stochastic gradient descent based optimization algorithm kingma and ba 2014 and update model parameters based on small batches randomly sampled from the training dataset one major challenge in applying anns is to constrain the complexity of the model while attaining optimal predictions overfitting can occur if a very complex structure is used several regularization techniques to reduce generalization errors in anns are available as reviewed by goodfellow et al 2016 among them we use early stopping which is one of the most popular and widely used regularization techniques in anns in our work we leave 20 of the available training data as the validation set and do not include them in training process this practice enables us to reduce overfitting by monitoring the average loss value over the validation set while we train the model and return the best possible training parameters weights and biases at the time when the lowest crps for the validation set is achieved we terminate training when no further decrease in validation set loss is seen after 15 iterations through all training batches or the entire training data epochs with up to 1000 epochs we train anns using the previously described process with all possible combinations of different settings using the early stopping technique for the following hyperparameters number of nodes in the hidden layer 5 10 15 batch size 2048 4096 8192 learning rate of the adam optimization algorithm 0 01 0 005 all networks are trained with the same random number generator seed and are evaluated based on the average loss value in the validation set the ann configuration with the lowest validation loss is chosen for out of sample predictions individual tested anns have o 7n 3 trainable parameters where n refers to the number of nodes in the hidden layer we used a simple non trained layer as the normalization layer our assessments showed that training layer normalization parameters beta and gamma does not yield significant improvement over the non trained one and possibly increases the risk of overfitting due to the increased number of overall network parameters 2 2 benchmark models 2 2 1 csgd to generate postprocessed precipitation forecasts at a given location for each forecast lead time and month of the year scheuerer and hamill 2015 first fit three climatological csgd parameters μ cl σ cl and δ cl to locally observed training precipitation data using a 91 day temporal window centered around the 15th of each month in the second step these parameters are included in nonlinear nonhomogeneous regression equations to relate monthly parameters of predictive csgds to statistics of spatially smoothed ensemble of forecasts in this study we use the regression equations that incorporate only the ensemble mean 6 μ μ c l a 1 log 1 exp a 1 1 a 2 a 3 f f c l 7 σ a 4 σ c l μ μ c l 8 δ δ c l where f and f c l correspond to the raw ensemble mean forecasts and their climatological mean in training data respectively in the version of csgd described in scheuerer and hamill 2015 the predictive shift parameter δ is kept identical to the climatological shift to ensure that the predictive csgd reverts to climatology as a limiting case when the forecast becomes less skillful e g at longer lead times see related discussion in scheuerer and hamill 2015 the four regression coefficients a 1 a 2 a 3 a 4 are estimated by minimizing the crps using the closed form expression proposed by scheuerer and hamill 2015 see sec 2 1 as a function of csgd parameters over training data past studies scheuerer and hamill 2015 baran and nemoda 2016 zhang et al 2017 baran and lerch 2018 taillardat et al 2019 show that csgd method and its variants perform well in comparison with other modern postprocessing techniques recent exploratory analyses see ghazvinian et al 2020 fig 1 showed that the climatological csgd shift parameter derived by crps minimization approach tends to be inflated and this leads to an underestimation of a probability of precipitation pop this bias directly affected the performance of predictive csgd primarily in predicting pop and to a degree the predicted magnitude of precipitation this was particularly evident at shorter lead times and in rainy seasons where the predictive distribution of precipitation deviates widely from climatology 2 2 2 mmgd the mmgd herr and krzysztofowicz 2005 wu et al 2011 was developed by the u s nws as a component of the meteorological ensemble forecast processor mefp of the operational hefs demargne et al 2014 this mechanism is routinely used to generate calibrated pqpf from single valued precipitation forecasts ensemble mean at river basin scales and at temporal aggregation scales ranging from 6 h to 3 months and for lead times up to 9 months wu et al 2018 demargne et al 2014 in contrast to the csgd where pop and the probability of magnitude of precipitation are estimated using the same predictive distribution mmgd uses a bayesian approach to break down the predictive distribution to explicitly account for the dichotomous continuous nature of precipitation let x and y denote the random variables of a single valued quantitative precipitation forecast and the observed precipitation amount respectively the conditional distributions of observed precipitation given a current forecast of no precipitation and positive precipitation are given as follows details of this derivation can be found in wu et al 2011 and ghazvinian et al 2020 9 f y x y x x 0 p y 0 x 0 p 0 y y x 0 a 1 a g y y 10 f y x y x x 0 p y y x x x 0 c x 1 c x d y x y x where a and c x represent mass probabilities of observed precipitation being equal to zero and are combined with the continuous conditional distributions gy y p y y x 0 y 0 and d y x y x p y y x x x 0 y 0 to construct the predictive distributions to estimate d y x y x its marginal continuous variates x x 0 y 0 and y x 0 y 0 undergo normal quantile transformation nqt yielding standard normal variates u φ 1 dx x and v φ 1 dy y following the meta gaussian distribution theorem of kelly and krzysztofowicz 1997 d y x y x assumes the following form 11 d y x y x φ φ 1 d y y ρ φ 1 d x x 1 ρ 2 where φ and φ 1 denote the standard normal cdf and quantile function of standard normal distribution respectively and ρ is the pearson s product correlation coefficient between uand v the performance of mmgd has been evaluated in a number of studies see e g wu et al 2011 brown et al 2014a demargne et al 2014 kim et al 2018 seo et al 2015 ghazvinian et al 2019 while conclusions indicate that overall mmgd produces reliable pqpfs and is capable of preserving the skill in the raw forecast its pqpfs underestimate heavy to extreme precipitation amounts low reliability for higher thresholds the latter finding was also corroborated by zhang et al 2017 where the authors compared the performances of mmgd and csgd over the mid atlantic region in u s their results pointed to the superior performance of csgd in that study csgd s ability to ingest additional ensemble statistics as predictors was shown to play a key role in its outperformance further performance comparisons by ghazvinian et al 2020 which relied on only the ensemble mean predictor and were conducted over the american river basin in california pointed to the clear outperformance of mmgd particularly in predicting pop the authors confirmed that the use of a two part scheme helped improve the representation of the predictive distribution we select mmgd as the second reference model to further address these discrepancies in the findings of previous studies this enables us to determine whether our unified ann csgd model improves upon the operational paradigm mmgd especially in situations where csgd underperforms the latter and helps us identify possible factors that contribute to the differential performance of the three schemes 2 3 data and experimental setup the experiments focus on 24 h mean areal precipitation map totals over sub basins of three major river basins in the service area of the nws california nevada river forecast center cnrfc https www cnrfc noaa gov we use ensemble mean precipitation forecasts from january 1985 through december 2016 32 years for lead times 1 to 7 days these data were obtained from the global ensemble forecast system gefs version 10 reforecast dataset hamill et al 2013 and were processed by the cnrfc at 1 degree spatial resolution and 6 h accumulation intervals issued daily at 00 universal time utc as ground truth we use the basin map data generated by the cnrfc the map data were created using the so called mountain mapper tool which relies on the parameter elevation regressions on independent slopes model prism daly et al 2008 to group gauges and interpolate gauge reports onto the domain of each watershed the cnrfc map series are at 6 h increments and are available for the period between october 1948 and september 2017 the map data were temporally aggregated to 24 h accumulation and paired with coincidental reforecasts postprocessing experiments are performed over sub basins in the american river basin nfdc1 folc1 the russian river basin wsdc1 guec1 and the eel river basin dosc1 ftsc1 fig 2 and separately for upper lower elevation zones when applicable sub basin names and corresponding nws ids are presented in table 1 the cnrfc runs hefs routinely to produce postprocessed pqpfs and ensemble streamflow forecasts for many of the sub basins for each river basin we selected one headwater and one downstream sub basin for the hindcast experiment to examine the potential elevation dependence in forecast skills the selected basins have been recognized for their importance in water resources management and flood control as noted in past hydrometeorological forecast postprocessing verification studies see e g wu et al 2011 brown et al 2012 seo et al 2015 he 2016 scheuerer et al 2017 ghazvinian et al 2020 the climate of the region is characterized by very dry summers with most of its annual precipitation falling during the cool season october april and the highest monthly averaged precipitation typically recorded in january the american river originates from the tahoe and el dorado national forests of the sierra nevada and is one of the major water supply sources for california streamflow in the american river is mainly 2 3 supplied from wintertime rainfall and snowmelt runoff with a small portion 1 3 from spring to early summer snowmelt runoff dettinger et al 2014 on the other hand the russian and eel river basins are coastal basins where snowmelt runoff is much less important scheuerer et al 2017 to be consistent with the cnrfc operations we use the nearest neighbor interpolation brown et al 2014a seo et al 2015 ghazvinian et al 2020 to pair forecasts observations for generating pqpfs and evaluating the performances of ann csgd relative to the two benchmark models we adopt an 8 fold cross validation approach in this approach for a given basin we divide the data to 8 consecutive 4 year length folds predictions for each fold are produced using each postprocessing mechanism trained with the data of remaining 7 folds 28 years postprocessed out of sample forecasts from all models are verified against observations in individual months of the year in verification years and separately for each sub basin and forecast lead time this leads to 32 years of verified forecasts for each sub basin and lead time while the ann csgd uses the entire available training data i e covering all lead times and seasons for training and hyperparameter tuning the benchmark models are trained using subsamples representing each forecast lead time and a month season of the year to gain insights on how increasing the length of training record and using different seasonal windows for training can affect the predictions of benchmark models we train each model with different training window sizes and regulations a summary of training schemes for ann csgd and benchmark models is provided as follows unified approach ann csgd uses forecast observation pairs of all months and lead times of training years for training and hyperparameter tuning resulting in a training sample size of up to 7 lead times 28 years 365 days 71540 20 of which is dedicated for hyperparameter tuning and not used in training mmgd and csgd with 61 days and 91 days training windows mmgd 61 csgd 61 and mmgd 91 csgd 91 use 61 and 91 training days around the 15th of each month across training years for generating pqpf for out of sample data of that month yielding training sample size up to 28 years 61 days 1708 and 28 years 91 days 2548 for each lead time and month respectively 61 days and 91 days training windows have been used in several past studies e g hamill et al 2015 scheuerer and hamill 2015 scheuerer and hamill 2018 scheuerer et al 2017 wu et al 2018 mmgd seasonal training scheme mmgd seasonal where forecasts in out of sample data from the cool october april and dry may september seasons are postprocessed by a model trained using the data in each season thus a single model is trained for each season and each lead time csgd seasonal training scheme csgd seasonal scheuerer et al 2020 where the climatological csgd parameters μ cl σ cl and δ cl as well as the climatological mean forecast f c l are derived using a 61 day window around the 15th of each month but the same regression coefficients are used across cool and dry seasons to increase the training sample size the latter two training schemes yield a sample size of up to 5942 and 4284 for the cool and dry seasons respectively 3 results in this section we present verification results using different metrics see appendix b for mathematical definitions and details we first use the continuous ranked probability skill score crpss to assess the overall predictive performance of pqpfs from ann csgd relative to those from the benchmark models with different training scenarios subsequently we analyze ann csgd s performance relative to the benchmark models with a 61 day training window using brier skill score bss reliability diagrams and mean squared error skill score msess 3 1 overall predictive performance of pqpfs fig 3 compares crpss of pqpfs from ann csgd and those from the benchmark models with different training scenarios and for the three river basins the results are computed using cross validated forecasts from all months and are aggregated over sub basins of each river basin with mmgd 61 as the reference forecast to assess whether differences in predictive performances shown are statistically significant we perform one sided diebold mariano test diebold and mariano 1995 for all possible pairs of model comparisons see appendix b for details these results are provided in tables s1 s3 in the supplemental material to this article overall ann csgd generates the most skillful pqpfs across lead times in the american river fig 3a ann csgd outperforms its baseline csgd with different training scenarios by a wide margin the improvement upon each csgd scheme is statistically significant at all lead times nevertheless performance differences between ann csgd and each of mmgds are not statistically significant in the russian river basin fig 3b ann csgd significantly outperforms each of benchmark models in a large number of cases in the eel river basin fig 3c ann csgd outperforms both mmgds and csgds though its difference with mmgd 61 is not statistically significant it is apparent that the relative performance of mmgd and csgd varies by river basin and at different lead times except for the american river basin where most differences are not statistically significant the seasonal version of mmgd trails behind those calibrated with 61 and 91 day moving windows for all three river basins the performance differences of csgd 61 and csgd 91 are not statistically significant across the lead times interestingly unlike mmgd seasonal csgd seasonal tends to considerably improve its performance at longer lead times and for all river basins the training strategy used in csgd seasonal was recently introduced by scheuerer et al 2020 in their subseasonal forecast scheme 2 week ahead this scheme presumes that nwp forecast error characteristics change on a season scale when the forecast has very limited skill our result confirms the hypothesis that performance is enhanced through the use of wider seasonal windows expanding the seasonal window potentially reduces the risk of overfitting of nonlinear csgd regression model coefficients at longer lead times when the signal to noise ratio is rather poor the results corroborate our postulation that different temporal data pooling methods for training statistical postprocessing models exert influences on the accuracy of postprocessed pqpfs the use of mmgd as an alternative scheme serves to further illustrate the significance of ann csgd model emos methods such as csgd are deemed inflexible in that the response variable in these models is assumed to follow a single unimodal parametric distribution see e g taillardat et al 2016 wu et al 2019 baran and lerch 2018 which potentially limits their performance as such why does ann csgd retain its superior performance relative to csgd across lead times and study basins while both use the same predictive distribution this is most likely due to the fact that ann csgd uses the entire training dataset and encodes nonlinear lead time and seasonal error dependencies in forecasts in an adaptable manner thus it can preserve the skill of raw forecast particularly at longer lead times where postprocessing via csgd seasonal offers marginal benefit or even degrades forecast skill another advantage of the proposed scheme is that it reduces the risk of overfitting due to the early stopping algorithm implemented as a part of its training 3 2 brier skill score and reliability fig 4 shows the results of bss for three thresholds 0 25 30 and 60 mm 24 h and for the three river basins while both ann csgd and csgd underperform mmgd in predicting events 0 25 mm i e pop ann csgd interestingly conspicuously outperforms csgd fig 4a c as pointed out by ghazvinian et al 2020 csgd performs poorly in predicting the pop due to its reliance on the climatological shift parameter see also sec 2 2 1 for further details when the forecast is very skillful the predictive csgd departs from climatology so does the optimal shift parameter at longer forecast lead times the forecast skill declines and the predictive csgd tends to approach the unconditional climatological one this feature is reflected in the improvement in csgd s performance across the lead times ann csgd on the other hand directly estimates the shift parameter of the predictive csgd as an arbitrary function of predictors thus eliminating the need for a climatological shift parameter this results in large and statistically significant improvements relative to the csgd in predicting the pop as for the outperformance of mmgd relative to the ann csgd we hypothesize that the flexible two part structure of mmgd is likely a major contributor a detailed discussion on this matter can be found in ghazvinian et al 2020 at the middle threshold of 30 mm day ann csgd outperforms both schemes in the american river basin fig 4d in the russian river basin and the eel river basin fig 4e and f the relative performance of ann csgd and csgd is mixed but both manage to outperform mmgd except at day 7 in the russian river basin where csgd slightly underperforms though it is not statistically significant not shown here at the highest threshold namely 60 mm day fig 4g i ann csgd outperforms all other schemes csgd mostly outperforms mmgd in the russian river basin fig 4h but underperforms the latter in american river and eel river basins fig 4g i to compare the calibration of pqpfs produced through each scheme we plot reliability diagrams for the same events and evaluate the contribution of reliability and resolution to the brier score figs 5 7 to attain a large enough sample size to better study larger thresholds we lump cross validate forecasts at all lead times and divide forecast probabilities 0 1 into 15 evenly distributed probability categories to discern the differential performance of schemes under higher probability categories the major findings for each river basin are summarized as follows american river basin in predicting positive precipitation events 0 25 mm day fig 5a c ann csgd s outperformance relative to csgd is attributed to improvements in both reliability lower rel and resolution higher res ann csgd mitigates to a great extent the underforecast issue of csgd ann csgd generates pqpfs that are more reliable than mmgd but are characterized with lower resolution yielding an overall inferior predictive performance at higher thresholds fig 5d i ann csgd clearly outperforms both csgd and mmgd in terms of both reliability and resolution as shown in the histograms embedded in each subplot ann csgd generates pqpfs that are able to issue high probabilities in predicting mid to heavy precipitation with higher frequencies and this points to improved sharpness fig 5f and i russian river basin similar to the american river basin at the lowest threshold fig 6a c ann csgd produces forecasts with higher reliability lower rel than mmgd but with lower resolution and overall lower predictive skill higher bs in 30 mm day ann csgd performs better than csgd in terms of both reliability and resolution fig 6e f at the highest threshold fig 6h i the lack of reliability in ann csgd pqpfs relative to those from csgd is compensated by the higher resolution and this leads to a superior predictive performance of the former as evidenced by the lower bs mmgd at both thresholds fig 6d g produces less reliable pqpfs with lowest sharpness at the 30 mm day threshold fig 6d mmgd pqpfs resolution is somewhat higher but is compensated by lower reliability eel river basin at the lowest threshold 0 25mm day fig 7a c the relative performance of schemes is quite similar to that for the other two river basins with ann csgd outperforming mmgd in terms of reliability but not resolution at higher thresholds fig 7d i pqpfs from ann csgd are more reliable and sharper and overall more skillful lowest bs though at the highest threshold i e 60 mm day the former exhibit slightly lower resolution than those from mmgd but this is compensated by superior reliability 3 3 evaluation of deterministic forecasts finally we compute mean squared error skill score msess to evaluate the performance of the distribution mean of pqpf produced using each scheme relative to the gefs ensemble mean forecast fig 8 these results are accompanied by the results of the diebold mariano test based on the squared error of mean pqpfs see tables s4 s6 in the supplemental material the relative performance varies among the river basins for the american river basin fig 8a all postprocessed pqpfs outperform the gefs ensemble mean in terms of msess ann csgd pqpfs perform favorably against mmgd pqpfs for all three river basins the performance differences are not statistically significant for both the russian and eel river basins fig 8b and c msess values are generally lower relative to those for the american river basin this as we posit is attributable to location dependent biases in the gefs ensemble mean forecast for example gefs is more skillful in the russian and eel river basins according to the msess results relative to climatological forecasts the results are shown in fig s1 of supplemental materials for the russian river basin fig 8b underperformance of postprocessed pqpf relative to the gefs ensemble mean is seen however the performance differences are not statistically significant unlike the benchmarks mean pqpf from ann csgd for russian river basin significantly outperforms gefs ensemble mean forecast in all lead times for both the russian and eel river basins fig 8b c ann csgd tends to outperform the other two schemes though the performance differentials are not statistically significant when comparing with mmgd 4 discussion and conclusions we propose a unified univariate hybrid neural network parametric pqpf postprocessing scheme capable of producing postprocessed forecasts for lead times at least up to 7 days medium range this scheme retains the use of parametric predictive distribution but employs ann to estimate distribution parameters from forecast observation pairs the predictors explored in this study include ensemble mean forecast forecast lead time and month of the year whereas the predictands are three parameters of the predictive censored shifted gamma distribution csgd the ann csgd model parameters were obtained by minimizing a loss function that is the closed form expression of crps for csgd scheuerer and hamill 2015 with the adam stochastic gradient descent algorithm kingma and ba 2014 as the optimization approach to test the performance of our model we conducted cross validation experiments to generate medium range lead times 1 7 days daily accumulated pqpfs over selected river basins in the service area of the cnrfc we used two benchmarking postprocessing schemes in this study namely the csgd emos scheuerer and hamill 2015 with a single predictor formulation and the nws operational postprocessor mixed type meta gaussian distribution mmgd these benchmark models were calibrated based on different seasonal data pooling scenarios to investigate the possible impacts of training window size and strategies on the performance of postprocessed pqpfs verification results showed that ann csgd in general outperform the baseline csgd and mmgd in terms of overall calibration and significantly so in some cases interestingly ann csgd mainly impacts improves bss of pqpf from csgd at the lowest threshold which has disproportionate impacts on crpss ann csgd manages to address the csgd s poor performance in predicting pop as noted in ghazvinian et al 2020 while the ann csgd performance comparison results are mixed in predicting 30 mm day thresholds it outperforms both benchmark models in predicting large extreme events 60 mm day on average the proposed method generates high probability forecasts for heavy precipitation more frequently than benchmarks as assessed by sharpness histograms higher sharpness this is particularly useful to cnrfc s operational precipitation and flood forecasting practice and thus could benefit real time reservoir operations e g determining reservoir release schedules in california in its current practice cnrfc relies on hefs to produce pqpfs from nwp precipitation forecasts and then generates ensemble streamflow forecasts which are used to guide real time flood management and control practices the mmgd model embedded in hefs has shown to systematically underestimate heavy precipitation amounts leading to negative biases in subsequent flood forecasts demargne et al 2014 brown et al 2014b the superior performance of the proposed ann csgd on heavy precipitation estimation makes it a viable tool to address limitations in the forecast skills for extreme precipitation and floods these improvements in forecasts will in turn serve to aid real time reservoir operations and flood risk management in contrast to the csgd version of scheuerer and hamill 2015 the proposed method directly estimates predictive csgd s shift parameter given each set of predictors in doing so it circumvents the need of invoking climatology and thereby alleviates the bias issue in estimating the pop in the existing csgd scheme furthermore the use of ann allows for representations of complex interactions between three predictive csgd parameters together these new features help the scheme produce sharper narrower predictive distributions than the benchmark csgd moreover ann csgd is able to use much larger training data with extra high forecast observation values and efficiently translate this to predictive skill at the highest threshold the new scheme also has a distinct practical advantage in that it eliminates the need for more computationally expensive and operationally labor intensive approach used in most contemporary statistical postprocessing schemes whereas the benchmark models need to be re trained for every forecasting lead time and month season ann csgd does not and it can simultaneously utilize forecast observation pairs across all lead times months and seasons our results support our hypothesis that the fixed size seasonal window training schemes for current postprocessing methods may not be sufficient for generating consistently skillful pqpfs across all lead times in other words the performance of existing schemes may be improved by identifying an optimal seasonal training window specific for each lead time depending on the study area and the statistical model at hand for example it was shown that a seasonal csgd tended to improve the performance benchmark 61 day and 91 day csgds at longer lead times but not in shorter lead times ann csgd on the other hand automatically adapts to the changes in raw forecasts observations errors along with all lead times and seasons and hence is capable of producing pqpfs with consistently higher skills a major limitation of nonhomogeneous regression or gamlss techniques is that their performance is dependent on the robustness of user prescribed regression relationships moreover they are typically limited in digesting ordinal temporal covariates such as those used in the ann csgd model the proposed model by contrast can freely learn to characterize arbitrary nonlinear predictor distribution parameters relationships and among predictors interactions efficiently a well known challenge in training ann models is model configuration hyperparameter tuning to achieve the best validation score generally it is very difficult to find the best possible ann configuration in a very large parameter space as pointed out by scher 2018 there is a trade off between robustness which depends on the depth and thoroughness of grid search and computational expenses for example our initial assessment showed that maintaining the architecture but expanding the number of layers does not significantly improve the model performance other regularization techniques such as l1 could be used in combination with early stopping to further reduce generalization errors however these techniques could require deeper search for hyperparameters and therefore increase computational complexity we also experimented with training embedding layers with different sizes 2 3 4 5 6 7 to project discrete lead times onto a larger vector of inputs but only found very marginal improvements in the validation score therefore we decided not to include embedding layers in our final model in future work we aim to extend the current approach to create a spatially adaptable scheme for postprocessing medium range ensemble precipitation forecasts on a gridded basis we expect to achieve this by incorporating geographical information into the network as shown by scheuerer et al 2020 in their subseasonal forecasting approach for example entire ensemble members or their statistics at a grid point in addition to those from a specific radius of surrounding grid points can be direct inputs to the model as the predictors such a model potentially eliminates the need for generating a local superensemble to address the issue of displacement errors in gridded precipitation forecasts additionally the current study focuses on 24 h accumulated precipitation in operations cnrfc produces 6 hourly pqpfs and updates their forecasts every 6 h during major storm events to align with cnrfc operations we also plan to explore the performance of the proposed ann csgd in generating 6 hourly pqpfs in our future work finally stacked convolution or long short term memory lstm layers applied on top of embedding vectors appear to be very effective in object detection krizhevsky et al 2012 in computer vision and in natural language processing collobert et al 2011 including machine translation and question answering devlin et al 2018 we envision investigating similar techniques to possibly improve the skill of postprocessed forecast at longer lead times credit authorship contribution statement mohammadvaghef ghazvinian conceptualization data curation formal analysis methodology software validation visualization writing original draft yu zhang conceptualization funding acquisition investigation methodology project administration resources software supervision writing review editing dong jun seo conceptualization methodology funding acquisition writing review editing minxue he conceptualization methodology writing review editing nelun fernando conceptualization methodology resources writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors thank the editor and reviewers for their valuable comments that helped improve the article the first author was financially supported by the faculty startup fund for dr yu zhang provided by ut arlington noaa grant na18oar4590370 01 texas water development board contract no 1800012276 and nsf grant 1909367 these supports are duly acknowledged here the authors would also like to thank michael scheuerer at norwegian computing center nr whose comments and suggestions led to the development of the scheme and cnrfc staff for providing the forecast and analysis archive supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2021 103907 appendix c supplementary materials image application 1 appendix a implementation details we implemented our ann codes in python python software foundation 2018 using google s deep learning platform tensorflow abadi et al 2016 and keras api chollet et al 2015 for fitting csgd climatological and predictive distributions r core team 2018 scripts provided by dr michael scheuerer were used to calibrate nws postprocessor mixed type meta gaussian distribution mmgd a research version very similar to the operational one was implemented in r appendix b verification metrics used in this study a mean squared error skill score msess the mean squared error skill score msess jolliffe and stephenson 2003 measures the reduction in mean squared error mse of deterministic forecast mean pqpf ensemble mean and verifying observations relative to the reference forecast a1 m s e s s 1 1 n i 1 n x i y i 2 1 n i 1 n x i r e f y i 2 positive values of msess indicates improvement in skill of deterministic forecast relative to the reference forecast b brier skill score bss the brier score bs brier 1950 is equivalent to mean squared error of probabilistic forecast exceeding a given threshold over n pairs of forecast and observations a2 b s τ 1 n i 1 n f i τ i y i τ 2 where fi τ is the probability of probabilistic forecast exceeding the threshold value τ and i is the indicator step function that takes the value 1 if the ith verifying observation exceeds the threshold value and 0 otherwise bs is negatively oriented and ranges from zero to one to assess the improvement in bs relative a reference forecast we compute brier skill score a3 b s s 1 b s b s r e f positive values of bss indicate improvement of bs over that of reference forecast brier score can be decomposed to three terms reliability or type i conditional bias resolution and uncertainty murphy 1973 wilks 2011 a4 b s τ r e l i a b i l i t y τ r e s o l u t i o n τ u n c e r t a i n i t y τ 1 n i 1 k n i f i τ o i τ 2 1 n i 1 k n i o i τ o τ 2 o τ 1 o τ where k indicates the number of categories forecast are aggregated to n is the number of cases in each category o i τ is the average climatological probability acp exceeding the threshold τ in that category and o τ is the overall acp it should be noted that uncertainty term as seen is independent of the forecast source probabilistic forecasts with lower higher reliability resolution values are desirable c continuous ranked probability score crps the continuous ranked probability score crps matheson and winkler 1976 measures the integral of squared differences between the cumulative distribution function cdf of probabilistic forecast and verifying observation it is a popular metric to assess the overall predictive performance of probabilistic forecasts sharpness and reliability see gneiting et al 2007 for further details crps averaged over the sample of forecast observations with size of n is given by a5 c r p s 1 n i 1 n f i x i y i x 2 d x where fi denotes the cdf of pqpf at the ith forecast instance and yi is the verifying observation i is the indicator step function which takes the value of 1 if x yi and 0 elsewhere continuous ranked probability skill score crpss is routinely used to assess the performance of probabilistic forecast relative to a reference forecast a6 c r p s s 1 c r p s c r p s r e f d reliability diagrams and sharpness histograms the reliability and resolution of a probabilistic forecast for exceeding some specific thresholds τ can be assessed graphically using reliability diagrams the reliability diagram consists of a plot of the average values of forecast probabilities exceeding τ against that of observed relative frequencies over each defined probability category in a reliable probabilistic forecast the reliability diagram should be close to 1 1 line interested readers are referred to brocker and smith 2007 and wilks 2011 for details on how to interpret the deficiencies in probabilistic forecasts using reliability diagrams to assess the sharpness of pqpf for specific thresholds we use sharpness histograms to investigate the frequency of forecast probabilities for different probability bins note a sharp forecast is characterized by higher frequencies for the forecast probabilities close to either 0 or 1 e the diebold mariano test to assess statistical significance of verification score differences between two forecast methods we use the diebold mariano statistical test of the null hypothesis of equal predictive performance diebold and mariano 1995 let δ s f1 s f2 denote the vector of verification score s differences from two competing forecast methods f 1 and f 2 over verification sample with length n δ 1 n i 1 n δ i and σ δ a suitable estimator of asymptotic standard deviation of δ under standard regularity conditions the test statistic t n n δ σ δ asymptotically follows a standard gaussian distribution under the null hypothesis of no difference in predictive performances of two competing forecast methods following the past studies baran and lerch 2016 2018 rasp and lerch 2018 σ δ can be estimated by square root of sample autocovariance up to lag k 1 for the k step ahead forecasts to account for temporal dependencies in forecast errors we use one sided diebold mariano tests the alternative hypothesis is that forecast method f 2 underperforms forecast method f 1 and the statistical significancy of the test s statistic can be assessed by obtaining corresponding p value we perform the tests based on both crps and squared error of mean pqpf on a limited basis and for each lead time and separately for each river basin to address spatial dependence of forecast errors scores are averaged across sub basins in each river basins m scheuerer 2021 personal communication further we adjust the test results by accounting for test multiplicity i e simultaneously analyzing test results of multiple lead times using false discovery rate fdr method benjamini and hochberg 1995 by controlling the fdr at the level α fdr 0 05 note that this procedure was discussed by wilks 2016 in spatial context where test results are interpreted simultaneously across multiple grid points but also was suggested to be applied whenever the results of simultaneous several hypothesis tests are reported or interpreted 
326,inverse modeling of large scale spatially variable parameters fields at fine resolution can be reduced to estimating the projections on dominant principal components of the underlying parameter fields based on principal component analysis of the spatial covariance for unknown or biased prior structural parameters of spatial covariance models an iterative procedure consisting of two successive steps is usually implemented i e estimation of spatial covariance followed by estimation of the spatially variable parameter fields conditional on the spatial covariance and observations in this study we develop an iterative computationally efficient method to update dominant principal components for nonlinear inverse problems of large scale spatial fields that adaptively corrects the bias from the initially defined prior spatial covariance our algorithm involves two layer iterations the inner iteration is to obtain the best estimates of projections on given retained principal components and the outer iteration implements an efficient rank one updating to correct the retained principal components using the posterior covariance associated with the best estimates of the projections numerical experiments show that inversion results can be significantly improved for large scale inverse problems with biased structural parameters for spatial covariance the experiment results show that the iterative correction is essentially to match the distribution patterns of the spatially correlated parameter field with its most dominant principal components we also investigate the performance of the developed method under different biased covariance model initialization including model type bias variance bias and correlation length bias the correction cannot fundamentally change the smoothness defined by the covariance model type but can still describe major distribution patterns including anisotropy biased variance can be corrected and yields similar best estimates and variance maps and biased correlation length can be corrected within an applicable range keywords principal component spatial covariance inverse modeling 1 introduction spatially distributed parameters are often required as inputs for mathematical models to simulate spatiotemporal processes in a variety of scientific disciplines including hydrology agriculture natural resources environmental sciences ecology and health studies inverse problems are most likely to arise whenever such spatially variable model parameters are difficult or simply cost prohibitive to measure thus in many cases we have to rely on indirect sparse or coarse spatial resolution measurements i e dependent variables to inversely estimate finer spatial resolution parameters i e target variable e g fienen et al 2006 cirpka et al 2007 cardiff et al 2009 liao and cirpka 2011 zhang et al 2014 zhao et al 2018 estimating large scale spatially variable model parameters on a fine resolution based on a limited number of measurements is the most onerous spatial inverse problem underdetermined often ill posed and computationally challenging in which the observation data alone are not sufficient to determine a unique solution kitanidis 2015 for underdetermined inverse problems the gesostatistical approach ga provides a rigorous successive linear approach to estimate spatially correlated parameter fields such as log hydraulic conductivity given indirect measurements like hydraulic heads and tracer concentrations kitanidis and vomvoris 1983 hoeksema and kitanidis 1984 kitanidis 1986 kitanidis 1995 yeh et al 1996 yeh and liu 2000 zhu and yeh 2005 nowak and cirpka 2006 cardiff and barrash 2011 liao and cirpka 2011 tiedeman and barrash 2019 for high dimensional inverse problems with millions of unknowns major computational costs include storage and computations of covariance matrices and iterative implementation of forward models to determine the jacobian matrix for nonlinear problems in the past decades many computational advances have been proposed to facilitate the storage and computation of covariance matrices and to reduce the number of forward model computations within the classic bayesian geostatistical framework for solving large dimensional inverse problems nowak et al 2003 nowak and cirpka 2004 liu and kitanidis 2011 saibaba et al 2012 ambikasaran et al 2013 liu et al 2013 kitanidis and lee 2014 lee and kitanidis 2014 lee et al 2016 lin et al 2017 klein et al 2017 recently we developed a reformulated framework of bayesian inverse modeling for large scale spatial fields on reduced dimensions comprised by principal components zhao and luo 2020 in the new framework the inverse problem is transformed from direct estimation of the underlying parameter fields to estimating the coefficients or projections on the dominant principal components thus the dimension reduction is realized at the stage of problem setup transforming an underdetermined inverse problem to an overdetermined inverse problem as a result computational techniques for evaluating large dimensional covariance and jacobian matrices are naturally built in the reformulated framework the number of normal equations to be solved is reduced to the number of retained principal components which is independent of the number of observations thus the new approach is scalable and particularly efficient for high dimensional inverse problems also with a large volume of observations the computational efficiency of this approach has been further improved by incorporating quasi newton methods to approximate the jacobian matrices zhao and luo 2021 however dominant principal components in zhao and luo 2020 were assumed constant throughout the inverse process implying that the prior covariance represents the true spatial covariance and there is no need to correct unknown or biased structural parameters of spatial covariance actually this issue has been mostly ignored in many recent studies using geostatistical approach nowak et al 2003 nowak and cirpka 2004 liu and kitanidis 2011 saibaba et al 2012 ambikasaran et al 2013 liu et al 2013 kitanidis and lee 2014 lee and kitanidis 2014 lee et al 2016 lin et al 2017 tiedeman and barrash 2019 though the original quasilinear theory of geostatisical approach proposed to use the method of restricted maximum likelihood to estimate the structural parameters kitanidis 1995 we should be aware that it is very common to have biased prior estimation of spatial covariance models in practice because an accurate estimation requires a large number of direct measurements of the target parameter fields which are typically unavailable at field sites nowak et al 2010 thus it may be necessary to update dominant principal components during inversion which is analogy to the correction of the initial guess of the spatial covariance fienen et al 2008 in ensemble kalman filter methods algorithms have also been proposed to reparametrize the mean size and spatial distribution of facies chang and zhang 2014 2015 the present study has two specific objectives firstly we aim to complete the reformulated framework proposed by zhao and luo 2020 by developing a practical method to iteratively correct the retained dominant principal components unlike the classic quasilinear theory to estimate parametric spatial covariance by solving another optimization problem i e the method of restricted maximum likelihood kitanidis 1995 our method is an efficient rank one update of dominant principal components using the posterior covariance of the projections on principal components the idea combines the iteration structure from quasilinear geostatistical approach kitanidis 1995 and the updating based on the posterior covariance from the successive linear estimator yeh et al 1996 zha et al 2018 secondly we will use the developed approach and numerical experiments to investigate how the biased prior information can be corrected through the iterative algorithm of updating the dominant principal components the prior information to be explored include the covariance type mean of the parameter field variance and correlation length of the covariance model this question has not been addressed in previous studies of geostatistical approach 2 methodology 2 1 reformulated geostatistical approach rga for the sake of completeness we first briefly review the reformulated geostatistical approach rga proposed by zhao and luo 2020 including its formulation and solution method the general observation equation to relate the data and the unknown spatial process is described by 1 y h s v where y r n 1 represents the observation data vector s r m 1 is the unknown spatially variable parameter h represents the forward model and v r n 1 is the epistemic error vector usually gaussian with mean 0 and covariance r r n n typically proportional to the identity matrix i e r σ r 2 i the spatially variable parameter field is typically modeled as a sum of deterministic trend functions with unknown coefficients and random variability matheron 1971 2 s x β ɛ where x r m p represents the known drift function β r p 1 represents the unknown drift coefficient and ɛ r m 1 is the variability vector with zero mean two point spatial covariance models as a function of distance are usually defined to describe the random variability which essentially regulates the smoothness of the underlying spatially distributed parameter fields 3 e s x β s x β t q ss θ where q ss r m m is the covariance matrix which is a function of structural parameters θ in addition the epistemic error variance σ r 2 may also be considered as a structural parameter in general inverse problems of spatially variable parameters can be formulated as the optimization problem 4 min s f s 1 2 y h s t r 1 y h s 1 2 s x β t q ss 1 s x β eq 4 can be conveniently interpreted in a bayesian framework where the first item represents the likelihood for data fitting and the second the prior of the unknown field for smoothness regularization the random process s conceptualized by a typical geostatistical spatial covariance structure can be described by a latent variable model 5 s x β z α where z r m k is the matrix with k scaled most dominant eigenvectors from the covariance matrix of the random process q ss r m m satisfying q ss zz t the latent variable α r k 1 is the coefficient vector representing the projections of the parameter fluctuations on the retained principal components the number of retained principal components k can be determined by how much percentage of variance they may contain from the prior covariance matrix kitanidis and lee 2014 one practical technique is to retain the first k principal components that explains 90 of the total variance of the original random field a more likelihood informed or data driven approach is to pick a k value based on rayleigh ritz ratio cui et al 2014 lee et al 2016 eq 5 represents a significant dimension reduction by transforming the estimation of s to the estimation of the latent variables α or the projection of s on k retained principal components where k is typically less than 100 the reformulated approach is to solve the following optimization problem zhao and luo 2020 6 min α β f α β 1 2 y h α β t r 1 y h α β 1 2 α t α eq 6 assumes known spatial covariance models and structural parameters thus the moment of the projections on the principal components satisfies e αα t i 2 2 solution of rga given prior spatial covariance an iterative procedure of the gauss newton method is applied to linearize h about the most recent estimates to solve the nonlinear optimization problem starting with the initial guess α 0 and β 0 we solve the system of k p equations 7 h α t r 1 h α i h α t r 1 h β h β t r 1 h α h β t r 1 h β α l 1 β l 1 h α t r 1 y h α l β l h α α l h β β l h β t r 1 y h α l β l h α α l h β β l where the subscription l represents the number of iteration h α and h β are the jacobian matrices 8 h α h α α α l h β h β h β which can be determined with k p forward model evaluations zhao and luo 2020 a more efficient jacobian matrix computation can be achieved by broyden s method which approximates the jacobian between each iteration with only one forward model evaluation zhao and luo 2021 the number of normal equations to be solved in 7 is reduced to the number of retained principal components which is independent of the number of observations thus the new approach is particularly efficient and scalable for large dimensional inverse problems equipped with a large size of observations after the iterative procedure converges the best estimates α and β can be substituted back into eq 5 to construct the best estimate of the spatially variable parameter the posterior covariance of α is 9 q α α h α t r 1 h α i h α t r 1 h β h β t r 1 h β 1 h β t r 1 h α 1 to generate conditional realizations of the spatial field kitanidis 1995 suggested a procedure to make corrections on unconditional realizations so that the observations can be reproduced within the variance of measurement error we shall notice that the mean and covariance of the conditional realization ensemble are a gaussian approximation to the true posterior mean and covariance for computational efficiency instead of direct simulation of conditional realizations of the spatially variable parameter s c conditional realization of α c can be generated to approximate s c as 10 s c x β z α c x β z α g t r where g r k k is the cholesky factor of the covariance matrix satisfying g t g q α α and r r k 1 is a sample from n 0 i we notice that the rga framework shares some similarities with null space monte carlo however there are two major differences 1 though both approaches calibrate super parameters that in a reduced space of the original model parameters rga derives the super parameters namely the principal component coefficients merely relying on the information from prior covariance while null space monte carlo derives its super parameters based on jacobians of the forward model with respect to its original parameters 2 null space monte carlo adopts an extra modification step to adjust its original parameters even after the optimal super parameters are computed which does not exist in the current rga framework 2 3 iterative correction of spatial covariance for the purpose of comparison we first outline how to update structural parameters in a covariance model in the textbook ga following the successive process of estimating spatially variable parameter fields and spatial covariance the linearization of the observation function after the iterations in the previous step converge to the value α can be written as 11 y 0 y h α β h α α h β β h α α h β β v h z α h x β v where h is the jacobian with respect to s at s in ga the marginal posterior distribution p θy 0 or restricted maximum likelihood p y 0 θ can be constructed accordingly to estimate the structural parameters for the predefined covariance model type these estimated parameters will then be defined as the updated prior information for the next iteration in the framework based on principal components the structural parameters only determine the initial principal components z during the inversion the iteration of estimating structural parameters or spatial covariance is equivalent to updating the principal components zha et al 2018 starting with an initial guess of structural parameters the posterior mean and variance define the statistical moments for the conditional spatial process eq 9 we can generate conditional realizations of α c and then conduct singular value decomposition for zα c to evaluate the corrected principal components for the next iteration this method is consistent with the scheme we used for decomposing the initial spatial covariance with guessed structural parameters the advantage of this method is that the number of retained principal components k can be re determined for each iteration if we keep a constant k the corrected principal components can be derived from a rank one updating formula for the posterior covariance matrix from eq 10 the posterior covariance for the conditional realizations is given by 12 e z α c α c t z t z α α t q α α z t v λ 1 2 α α t q α α λ 1 2 v t v z α v z α t z z t where z α r k k is the matrix containing scaled eigenvectors of λ 1 2 α α t q α α λ 1 2 satisfying z α z α t λ 1 2 α α t q α α λ 1 2 and z is the updated principal components used for the next iteration we shall notice that the term α α t cannot be removed as the mean because it cannot be projected on the drift for the given prior covariance the convergence of the principal components is defined for α α t q α α to approach an identity matrix so that the update of principal components is complete this is a typical rank one update to correct the prior covariance so that the covariance matrix of principal component coefficients approaches an identity matrix the iteration procedure in our approach is the same as that in the geostatistical approach kitanidis 1995 i e two layer iterations however the posterior mean and covariance of the estimated projections are directly used to correct the principal components for the next iteration round thus unlike the geostatistical approach parameterizing the covariance model in each iteration only the initial covariance is parameterized and the corrected principal components become nonparametric 2 4 algorithmic implementation fig 1 summarizes the proposed inverse approach in a flow chart the dashed lines represent the approach by generating conditional realizations to re evaluate the number of retained principal components we directly use the corrected principal components for the next iteration the initial guess of structural parameters is typically based on data analysis of field measurements such as experimental semivariogram analysis kitanidis 1997 the most important part of this step is to select the model type which essentially defines the smoothness of the underlying parameter field later we will show that corrected principal components cannot change the smoothness fundamentally a small number can be arbitrarily chosen for the initial guess of σ r 2 if there is no prior information about the errors from measurement and modeling σ r 2 can be updated with the principal components for the next iteration a simple method is to use the mean squared error between the measurements and the best model fitting of the previous iteration however one should be cautious about updating σ r 2 because it quickly decays and is likely to cause the data fitting term dominate in the objective function leading to the problem of overfitting a practical method is to define a minimum threshold value to control the updating of σ r 2 cross validation can also be used to examine and reduce the risk of overfitting 3 numerical experiments two numerical experiments of two dimensional steady state hydraulic tomography yeh and liu 2000 to estimate heterogeneous transmissivity fields are presented to test the applications of the proposed approach and to investigate the correction of biased prior information the numerical experiments are numerically simulated to compute steady stead hydraulic heads in a saturated confined aquifer table 1 summarizes the geostatistical and geometric parameters to set up these experiments and the initial geostatistical parameters applied to invert the transmissivity fields both cases have the same domain size and resolution of 1024 1024 the two heterogeneous fields of logarithmic transmissivity are gaussian random fields with an anisotropic gaussian covariance model and an isotropic exponential covariance model respectively sequential pumping tests are simulated in a well network consisting of 25 pumping wells uniformly distributed in the domain fig 2 during the simulated pumping testes these wells alternatively act as the pumping well and monitoring wells to record the steady state hydraulic heads the inversion routines for these cases start from biased structural parameters as the initial guess we consider the correct model types i e gaussian and exponential models but with biased parameter values including the mean variance and correlation lengths we also vary specific parameter values to examine the inversion performance regarding different extent of biasedness for the anisotropic gaussian model we start from an isotropic model during the inversion the hydraulic head data is normalized by the norm of the data vector to prevent data overfitting we define minimum threshold values for the data fitting errors based on the magnitude range of the normalized measurements for the gaussian model case we use 4 of the squared magnitude range and for the exponential model case 1 is used the number of retained principal components is k 50 for both cases 4 results and discussion 4 1 best inverse estimates fig 2 demonstrates the inversion results of the proposed approach both cases start from the uniform field with the biased initial mean values listed in table 1 for simplicity only inversion results from the first the second and the last iteration are presented the inversion results from the first iteration are completely based on the initial guess of the structural parameters because of the biased initialization the inversion results only roughly capture the main features of the true fields which are the vague shapes of high low conductivity regions in the second iteration benefitting from the corrected principal components and the updated error variance the inversion results contain more details of the true fields after the convergence of the principal components the inversion results delineate the features of the studied fields with much better accuracy and details including the anisotropy in the gaussian case the improvement of inversion accuracy is quantified by the map accuracy shown in fig 3 a map accuracy is defined as how much percentage of the entire field is correctly inverted if the difference between the local estimate and the corresponding true parameter value is smaller than a predefined threshold then the estimate at this location is recognized as correctly inverted the deviation threshold to determine the map accuracy is typically set as 10 15 of the magnitude range of the original field kang et al 2017 fig 3b shows the improvement of data fitting by a decreasing normalized root mean squared error nrmse which is defined by 15 nrmse 1 n y y t y y y max y min where y is the modeling results both cases yield high map accuracy and low nrmse i e good data fitting after several iterations the high quality results imply that for the test cases the biased initial mean and structural parameters of the covariance model listed in table 1 can be corrected during the iterative inversion process 4 2 iteratively corrected principal components to understand how the iterative approach affects the principal components the iterative correction of the most dominant i e the first principal component is demonstrated in fig 4 during the iterative process the first principal component axis converges to possess the same spatial pattern as the best estimate for the exponential case the high and low value zones are reversed compared with fig 3 they are considered as the same distribution pattern because the projection or coefficient on the principal components can be negative fig 4 demonstrates that the major distribution pattern of the best estimate is dominated by the distribution pattern of the first principal component and the iterative correction of the principal components is essentially to match the distribution patterns of the underlying parameter field by the most dominant principal component as a result the principal component coefficient or projection evolves to be more concentrated on the first principal axis during iterations in addition fig 4 indicates that there is no need to keep a large number of retained principal components or the number can be gradually lowered as the iterative method proceeds 4 3 covariance model type and parameters to further examine the potential of the method we use different initial covariance model types and model parameters to investigate the inversion performance by the iteratively corrected principal components fig 5 shows the inverse results based on wrong selection of initial covariance model type we keep the same initial parameter values but swap the covariance model type that is we use an exponential covariance model as the initial guess to simulate the smooth differentiable gaussian field and assume a gaussian model for the continuous but non differentiable exponential model in fact gaussian covariance models were used in previous studies to characterize major distribution patterns in the absence of the prior information of the model type liu and kitanidis 2011 fig 5 shows that the method of iteratively corrected principal components cannot fundamentally change the smoothness of the estimated parameter field however both cases provide satisfactory results describing the major distribution patterns in fact their map accuracy and nrmse are very close to the cases shown in fig 2 with the correct initial model type the initial mean values for the transmissivity field listed in table 1 represent biased initialization with orders of magnitude difference from the true values we also tried values with higher orders of magnitude difference and the inversion results were similar to fig 2 indicating that the bias incurred by false mean values can be successfully corrected through the iteration procedures and imposes little impact on the final inversion estimates it is worth noting that the biased mean values are corrected within the inner iteration by rga fig 6 shows the inversion results using different correlation lengths for the gaussian model the slightly anisotropic field can be well estimated by an initially isotropic model it is clear that there is an applicable range for the initial correlation length for short correlation lengths such as lx ly 5m the high conductivity zones are scattered and the correction of principal components fails to generate the large scale zonal patterns for long correlation lengths such as lx ly 40m it is difficult to distinguish different zones compared with the true parameter values there is a quite wide applicable range for the biased initial correlation length that can be corrected fig 7 shows the inverse results for different initial variance values for the gaussian model case all other parameters are the same as those listed in table 1 the true field has a moderate variance of 1 it is clear that the method is not sensitive to the initial variance value from true to highly heterogeneous setup i e variance of 4 the method yields similar best estimates and associated variance maps indicating that a biased initial variance can be sufficiently corrected during the iterations 5 conclusion in this study we propose an iterative approach to correct the principal components for spatial inverse problems with unknown or biased prior information regarding the spatial covariance and structural parameters two iterative routines are involved the inner one is to solve the nonlinear inverse problem given all structural parameters or principal components of the covariance matrix and the outer one is to correct the principal components and error variance the iterative correction of the principal components is achieved by decomposing a rank one update of the posterior covariance matrix of a group of latent variables this group of latent variables represents the projections of the original variable parameter on the retained principal components the proposed approach maintains scalability within each inner iteration and mitigates the potential deterioration of results incurred by the biased initialization of structural parameters two numerical experiments with different spatial correlation patterns are implemented to investigate the validity of the proposed iterative approach the results indicate that the proposed iterative approach produces accurate estimates under biased assumptions of the structural parameters the iterative correction of the principal components is essentially to match the distribution patterns of the underlying parameter field with the most dominant principal component the developed approach is used to investigate to what extent of biasness of the prior information can be corrected specifically we show that the smoothness defined by the initial model type is difficult to be fundamentally changed but the major distribution patterns can be precisely characterized the variance in the covariance model can be well corrected even for very biased initial values while the correlation length has an applicable range in the numerical cases we use constant number of retained principal components to simplify the updating it is possible to reduce the number during the inversion to further reduce the computational costs because the parameter distribution pattern is described by the updated dominant principal components the algorithm includes a method by generating conditional realizations to realize a variable number of retained principal components a rigorous study is underway in addition we can combine the proposed approach with quasi newton methods to reduce the forward model runs to approximate the jacobian matrix zhao and luo 2021 credit authorship contribution statement yue zhao conceptualization methodology writing original draft jian luo conceptualization methodology supervision writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we thank dr wolfgang nowak and two other anonymous reviewers for their constructive comments that helped improve the manuscript we thank dr olaf cirpka for providing the computer codes for conducting the forward numerical simulations 
326,inverse modeling of large scale spatially variable parameters fields at fine resolution can be reduced to estimating the projections on dominant principal components of the underlying parameter fields based on principal component analysis of the spatial covariance for unknown or biased prior structural parameters of spatial covariance models an iterative procedure consisting of two successive steps is usually implemented i e estimation of spatial covariance followed by estimation of the spatially variable parameter fields conditional on the spatial covariance and observations in this study we develop an iterative computationally efficient method to update dominant principal components for nonlinear inverse problems of large scale spatial fields that adaptively corrects the bias from the initially defined prior spatial covariance our algorithm involves two layer iterations the inner iteration is to obtain the best estimates of projections on given retained principal components and the outer iteration implements an efficient rank one updating to correct the retained principal components using the posterior covariance associated with the best estimates of the projections numerical experiments show that inversion results can be significantly improved for large scale inverse problems with biased structural parameters for spatial covariance the experiment results show that the iterative correction is essentially to match the distribution patterns of the spatially correlated parameter field with its most dominant principal components we also investigate the performance of the developed method under different biased covariance model initialization including model type bias variance bias and correlation length bias the correction cannot fundamentally change the smoothness defined by the covariance model type but can still describe major distribution patterns including anisotropy biased variance can be corrected and yields similar best estimates and variance maps and biased correlation length can be corrected within an applicable range keywords principal component spatial covariance inverse modeling 1 introduction spatially distributed parameters are often required as inputs for mathematical models to simulate spatiotemporal processes in a variety of scientific disciplines including hydrology agriculture natural resources environmental sciences ecology and health studies inverse problems are most likely to arise whenever such spatially variable model parameters are difficult or simply cost prohibitive to measure thus in many cases we have to rely on indirect sparse or coarse spatial resolution measurements i e dependent variables to inversely estimate finer spatial resolution parameters i e target variable e g fienen et al 2006 cirpka et al 2007 cardiff et al 2009 liao and cirpka 2011 zhang et al 2014 zhao et al 2018 estimating large scale spatially variable model parameters on a fine resolution based on a limited number of measurements is the most onerous spatial inverse problem underdetermined often ill posed and computationally challenging in which the observation data alone are not sufficient to determine a unique solution kitanidis 2015 for underdetermined inverse problems the gesostatistical approach ga provides a rigorous successive linear approach to estimate spatially correlated parameter fields such as log hydraulic conductivity given indirect measurements like hydraulic heads and tracer concentrations kitanidis and vomvoris 1983 hoeksema and kitanidis 1984 kitanidis 1986 kitanidis 1995 yeh et al 1996 yeh and liu 2000 zhu and yeh 2005 nowak and cirpka 2006 cardiff and barrash 2011 liao and cirpka 2011 tiedeman and barrash 2019 for high dimensional inverse problems with millions of unknowns major computational costs include storage and computations of covariance matrices and iterative implementation of forward models to determine the jacobian matrix for nonlinear problems in the past decades many computational advances have been proposed to facilitate the storage and computation of covariance matrices and to reduce the number of forward model computations within the classic bayesian geostatistical framework for solving large dimensional inverse problems nowak et al 2003 nowak and cirpka 2004 liu and kitanidis 2011 saibaba et al 2012 ambikasaran et al 2013 liu et al 2013 kitanidis and lee 2014 lee and kitanidis 2014 lee et al 2016 lin et al 2017 klein et al 2017 recently we developed a reformulated framework of bayesian inverse modeling for large scale spatial fields on reduced dimensions comprised by principal components zhao and luo 2020 in the new framework the inverse problem is transformed from direct estimation of the underlying parameter fields to estimating the coefficients or projections on the dominant principal components thus the dimension reduction is realized at the stage of problem setup transforming an underdetermined inverse problem to an overdetermined inverse problem as a result computational techniques for evaluating large dimensional covariance and jacobian matrices are naturally built in the reformulated framework the number of normal equations to be solved is reduced to the number of retained principal components which is independent of the number of observations thus the new approach is scalable and particularly efficient for high dimensional inverse problems also with a large volume of observations the computational efficiency of this approach has been further improved by incorporating quasi newton methods to approximate the jacobian matrices zhao and luo 2021 however dominant principal components in zhao and luo 2020 were assumed constant throughout the inverse process implying that the prior covariance represents the true spatial covariance and there is no need to correct unknown or biased structural parameters of spatial covariance actually this issue has been mostly ignored in many recent studies using geostatistical approach nowak et al 2003 nowak and cirpka 2004 liu and kitanidis 2011 saibaba et al 2012 ambikasaran et al 2013 liu et al 2013 kitanidis and lee 2014 lee and kitanidis 2014 lee et al 2016 lin et al 2017 tiedeman and barrash 2019 though the original quasilinear theory of geostatisical approach proposed to use the method of restricted maximum likelihood to estimate the structural parameters kitanidis 1995 we should be aware that it is very common to have biased prior estimation of spatial covariance models in practice because an accurate estimation requires a large number of direct measurements of the target parameter fields which are typically unavailable at field sites nowak et al 2010 thus it may be necessary to update dominant principal components during inversion which is analogy to the correction of the initial guess of the spatial covariance fienen et al 2008 in ensemble kalman filter methods algorithms have also been proposed to reparametrize the mean size and spatial distribution of facies chang and zhang 2014 2015 the present study has two specific objectives firstly we aim to complete the reformulated framework proposed by zhao and luo 2020 by developing a practical method to iteratively correct the retained dominant principal components unlike the classic quasilinear theory to estimate parametric spatial covariance by solving another optimization problem i e the method of restricted maximum likelihood kitanidis 1995 our method is an efficient rank one update of dominant principal components using the posterior covariance of the projections on principal components the idea combines the iteration structure from quasilinear geostatistical approach kitanidis 1995 and the updating based on the posterior covariance from the successive linear estimator yeh et al 1996 zha et al 2018 secondly we will use the developed approach and numerical experiments to investigate how the biased prior information can be corrected through the iterative algorithm of updating the dominant principal components the prior information to be explored include the covariance type mean of the parameter field variance and correlation length of the covariance model this question has not been addressed in previous studies of geostatistical approach 2 methodology 2 1 reformulated geostatistical approach rga for the sake of completeness we first briefly review the reformulated geostatistical approach rga proposed by zhao and luo 2020 including its formulation and solution method the general observation equation to relate the data and the unknown spatial process is described by 1 y h s v where y r n 1 represents the observation data vector s r m 1 is the unknown spatially variable parameter h represents the forward model and v r n 1 is the epistemic error vector usually gaussian with mean 0 and covariance r r n n typically proportional to the identity matrix i e r σ r 2 i the spatially variable parameter field is typically modeled as a sum of deterministic trend functions with unknown coefficients and random variability matheron 1971 2 s x β ɛ where x r m p represents the known drift function β r p 1 represents the unknown drift coefficient and ɛ r m 1 is the variability vector with zero mean two point spatial covariance models as a function of distance are usually defined to describe the random variability which essentially regulates the smoothness of the underlying spatially distributed parameter fields 3 e s x β s x β t q ss θ where q ss r m m is the covariance matrix which is a function of structural parameters θ in addition the epistemic error variance σ r 2 may also be considered as a structural parameter in general inverse problems of spatially variable parameters can be formulated as the optimization problem 4 min s f s 1 2 y h s t r 1 y h s 1 2 s x β t q ss 1 s x β eq 4 can be conveniently interpreted in a bayesian framework where the first item represents the likelihood for data fitting and the second the prior of the unknown field for smoothness regularization the random process s conceptualized by a typical geostatistical spatial covariance structure can be described by a latent variable model 5 s x β z α where z r m k is the matrix with k scaled most dominant eigenvectors from the covariance matrix of the random process q ss r m m satisfying q ss zz t the latent variable α r k 1 is the coefficient vector representing the projections of the parameter fluctuations on the retained principal components the number of retained principal components k can be determined by how much percentage of variance they may contain from the prior covariance matrix kitanidis and lee 2014 one practical technique is to retain the first k principal components that explains 90 of the total variance of the original random field a more likelihood informed or data driven approach is to pick a k value based on rayleigh ritz ratio cui et al 2014 lee et al 2016 eq 5 represents a significant dimension reduction by transforming the estimation of s to the estimation of the latent variables α or the projection of s on k retained principal components where k is typically less than 100 the reformulated approach is to solve the following optimization problem zhao and luo 2020 6 min α β f α β 1 2 y h α β t r 1 y h α β 1 2 α t α eq 6 assumes known spatial covariance models and structural parameters thus the moment of the projections on the principal components satisfies e αα t i 2 2 solution of rga given prior spatial covariance an iterative procedure of the gauss newton method is applied to linearize h about the most recent estimates to solve the nonlinear optimization problem starting with the initial guess α 0 and β 0 we solve the system of k p equations 7 h α t r 1 h α i h α t r 1 h β h β t r 1 h α h β t r 1 h β α l 1 β l 1 h α t r 1 y h α l β l h α α l h β β l h β t r 1 y h α l β l h α α l h β β l where the subscription l represents the number of iteration h α and h β are the jacobian matrices 8 h α h α α α l h β h β h β which can be determined with k p forward model evaluations zhao and luo 2020 a more efficient jacobian matrix computation can be achieved by broyden s method which approximates the jacobian between each iteration with only one forward model evaluation zhao and luo 2021 the number of normal equations to be solved in 7 is reduced to the number of retained principal components which is independent of the number of observations thus the new approach is particularly efficient and scalable for large dimensional inverse problems equipped with a large size of observations after the iterative procedure converges the best estimates α and β can be substituted back into eq 5 to construct the best estimate of the spatially variable parameter the posterior covariance of α is 9 q α α h α t r 1 h α i h α t r 1 h β h β t r 1 h β 1 h β t r 1 h α 1 to generate conditional realizations of the spatial field kitanidis 1995 suggested a procedure to make corrections on unconditional realizations so that the observations can be reproduced within the variance of measurement error we shall notice that the mean and covariance of the conditional realization ensemble are a gaussian approximation to the true posterior mean and covariance for computational efficiency instead of direct simulation of conditional realizations of the spatially variable parameter s c conditional realization of α c can be generated to approximate s c as 10 s c x β z α c x β z α g t r where g r k k is the cholesky factor of the covariance matrix satisfying g t g q α α and r r k 1 is a sample from n 0 i we notice that the rga framework shares some similarities with null space monte carlo however there are two major differences 1 though both approaches calibrate super parameters that in a reduced space of the original model parameters rga derives the super parameters namely the principal component coefficients merely relying on the information from prior covariance while null space monte carlo derives its super parameters based on jacobians of the forward model with respect to its original parameters 2 null space monte carlo adopts an extra modification step to adjust its original parameters even after the optimal super parameters are computed which does not exist in the current rga framework 2 3 iterative correction of spatial covariance for the purpose of comparison we first outline how to update structural parameters in a covariance model in the textbook ga following the successive process of estimating spatially variable parameter fields and spatial covariance the linearization of the observation function after the iterations in the previous step converge to the value α can be written as 11 y 0 y h α β h α α h β β h α α h β β v h z α h x β v where h is the jacobian with respect to s at s in ga the marginal posterior distribution p θy 0 or restricted maximum likelihood p y 0 θ can be constructed accordingly to estimate the structural parameters for the predefined covariance model type these estimated parameters will then be defined as the updated prior information for the next iteration in the framework based on principal components the structural parameters only determine the initial principal components z during the inversion the iteration of estimating structural parameters or spatial covariance is equivalent to updating the principal components zha et al 2018 starting with an initial guess of structural parameters the posterior mean and variance define the statistical moments for the conditional spatial process eq 9 we can generate conditional realizations of α c and then conduct singular value decomposition for zα c to evaluate the corrected principal components for the next iteration this method is consistent with the scheme we used for decomposing the initial spatial covariance with guessed structural parameters the advantage of this method is that the number of retained principal components k can be re determined for each iteration if we keep a constant k the corrected principal components can be derived from a rank one updating formula for the posterior covariance matrix from eq 10 the posterior covariance for the conditional realizations is given by 12 e z α c α c t z t z α α t q α α z t v λ 1 2 α α t q α α λ 1 2 v t v z α v z α t z z t where z α r k k is the matrix containing scaled eigenvectors of λ 1 2 α α t q α α λ 1 2 satisfying z α z α t λ 1 2 α α t q α α λ 1 2 and z is the updated principal components used for the next iteration we shall notice that the term α α t cannot be removed as the mean because it cannot be projected on the drift for the given prior covariance the convergence of the principal components is defined for α α t q α α to approach an identity matrix so that the update of principal components is complete this is a typical rank one update to correct the prior covariance so that the covariance matrix of principal component coefficients approaches an identity matrix the iteration procedure in our approach is the same as that in the geostatistical approach kitanidis 1995 i e two layer iterations however the posterior mean and covariance of the estimated projections are directly used to correct the principal components for the next iteration round thus unlike the geostatistical approach parameterizing the covariance model in each iteration only the initial covariance is parameterized and the corrected principal components become nonparametric 2 4 algorithmic implementation fig 1 summarizes the proposed inverse approach in a flow chart the dashed lines represent the approach by generating conditional realizations to re evaluate the number of retained principal components we directly use the corrected principal components for the next iteration the initial guess of structural parameters is typically based on data analysis of field measurements such as experimental semivariogram analysis kitanidis 1997 the most important part of this step is to select the model type which essentially defines the smoothness of the underlying parameter field later we will show that corrected principal components cannot change the smoothness fundamentally a small number can be arbitrarily chosen for the initial guess of σ r 2 if there is no prior information about the errors from measurement and modeling σ r 2 can be updated with the principal components for the next iteration a simple method is to use the mean squared error between the measurements and the best model fitting of the previous iteration however one should be cautious about updating σ r 2 because it quickly decays and is likely to cause the data fitting term dominate in the objective function leading to the problem of overfitting a practical method is to define a minimum threshold value to control the updating of σ r 2 cross validation can also be used to examine and reduce the risk of overfitting 3 numerical experiments two numerical experiments of two dimensional steady state hydraulic tomography yeh and liu 2000 to estimate heterogeneous transmissivity fields are presented to test the applications of the proposed approach and to investigate the correction of biased prior information the numerical experiments are numerically simulated to compute steady stead hydraulic heads in a saturated confined aquifer table 1 summarizes the geostatistical and geometric parameters to set up these experiments and the initial geostatistical parameters applied to invert the transmissivity fields both cases have the same domain size and resolution of 1024 1024 the two heterogeneous fields of logarithmic transmissivity are gaussian random fields with an anisotropic gaussian covariance model and an isotropic exponential covariance model respectively sequential pumping tests are simulated in a well network consisting of 25 pumping wells uniformly distributed in the domain fig 2 during the simulated pumping testes these wells alternatively act as the pumping well and monitoring wells to record the steady state hydraulic heads the inversion routines for these cases start from biased structural parameters as the initial guess we consider the correct model types i e gaussian and exponential models but with biased parameter values including the mean variance and correlation lengths we also vary specific parameter values to examine the inversion performance regarding different extent of biasedness for the anisotropic gaussian model we start from an isotropic model during the inversion the hydraulic head data is normalized by the norm of the data vector to prevent data overfitting we define minimum threshold values for the data fitting errors based on the magnitude range of the normalized measurements for the gaussian model case we use 4 of the squared magnitude range and for the exponential model case 1 is used the number of retained principal components is k 50 for both cases 4 results and discussion 4 1 best inverse estimates fig 2 demonstrates the inversion results of the proposed approach both cases start from the uniform field with the biased initial mean values listed in table 1 for simplicity only inversion results from the first the second and the last iteration are presented the inversion results from the first iteration are completely based on the initial guess of the structural parameters because of the biased initialization the inversion results only roughly capture the main features of the true fields which are the vague shapes of high low conductivity regions in the second iteration benefitting from the corrected principal components and the updated error variance the inversion results contain more details of the true fields after the convergence of the principal components the inversion results delineate the features of the studied fields with much better accuracy and details including the anisotropy in the gaussian case the improvement of inversion accuracy is quantified by the map accuracy shown in fig 3 a map accuracy is defined as how much percentage of the entire field is correctly inverted if the difference between the local estimate and the corresponding true parameter value is smaller than a predefined threshold then the estimate at this location is recognized as correctly inverted the deviation threshold to determine the map accuracy is typically set as 10 15 of the magnitude range of the original field kang et al 2017 fig 3b shows the improvement of data fitting by a decreasing normalized root mean squared error nrmse which is defined by 15 nrmse 1 n y y t y y y max y min where y is the modeling results both cases yield high map accuracy and low nrmse i e good data fitting after several iterations the high quality results imply that for the test cases the biased initial mean and structural parameters of the covariance model listed in table 1 can be corrected during the iterative inversion process 4 2 iteratively corrected principal components to understand how the iterative approach affects the principal components the iterative correction of the most dominant i e the first principal component is demonstrated in fig 4 during the iterative process the first principal component axis converges to possess the same spatial pattern as the best estimate for the exponential case the high and low value zones are reversed compared with fig 3 they are considered as the same distribution pattern because the projection or coefficient on the principal components can be negative fig 4 demonstrates that the major distribution pattern of the best estimate is dominated by the distribution pattern of the first principal component and the iterative correction of the principal components is essentially to match the distribution patterns of the underlying parameter field by the most dominant principal component as a result the principal component coefficient or projection evolves to be more concentrated on the first principal axis during iterations in addition fig 4 indicates that there is no need to keep a large number of retained principal components or the number can be gradually lowered as the iterative method proceeds 4 3 covariance model type and parameters to further examine the potential of the method we use different initial covariance model types and model parameters to investigate the inversion performance by the iteratively corrected principal components fig 5 shows the inverse results based on wrong selection of initial covariance model type we keep the same initial parameter values but swap the covariance model type that is we use an exponential covariance model as the initial guess to simulate the smooth differentiable gaussian field and assume a gaussian model for the continuous but non differentiable exponential model in fact gaussian covariance models were used in previous studies to characterize major distribution patterns in the absence of the prior information of the model type liu and kitanidis 2011 fig 5 shows that the method of iteratively corrected principal components cannot fundamentally change the smoothness of the estimated parameter field however both cases provide satisfactory results describing the major distribution patterns in fact their map accuracy and nrmse are very close to the cases shown in fig 2 with the correct initial model type the initial mean values for the transmissivity field listed in table 1 represent biased initialization with orders of magnitude difference from the true values we also tried values with higher orders of magnitude difference and the inversion results were similar to fig 2 indicating that the bias incurred by false mean values can be successfully corrected through the iteration procedures and imposes little impact on the final inversion estimates it is worth noting that the biased mean values are corrected within the inner iteration by rga fig 6 shows the inversion results using different correlation lengths for the gaussian model the slightly anisotropic field can be well estimated by an initially isotropic model it is clear that there is an applicable range for the initial correlation length for short correlation lengths such as lx ly 5m the high conductivity zones are scattered and the correction of principal components fails to generate the large scale zonal patterns for long correlation lengths such as lx ly 40m it is difficult to distinguish different zones compared with the true parameter values there is a quite wide applicable range for the biased initial correlation length that can be corrected fig 7 shows the inverse results for different initial variance values for the gaussian model case all other parameters are the same as those listed in table 1 the true field has a moderate variance of 1 it is clear that the method is not sensitive to the initial variance value from true to highly heterogeneous setup i e variance of 4 the method yields similar best estimates and associated variance maps indicating that a biased initial variance can be sufficiently corrected during the iterations 5 conclusion in this study we propose an iterative approach to correct the principal components for spatial inverse problems with unknown or biased prior information regarding the spatial covariance and structural parameters two iterative routines are involved the inner one is to solve the nonlinear inverse problem given all structural parameters or principal components of the covariance matrix and the outer one is to correct the principal components and error variance the iterative correction of the principal components is achieved by decomposing a rank one update of the posterior covariance matrix of a group of latent variables this group of latent variables represents the projections of the original variable parameter on the retained principal components the proposed approach maintains scalability within each inner iteration and mitigates the potential deterioration of results incurred by the biased initialization of structural parameters two numerical experiments with different spatial correlation patterns are implemented to investigate the validity of the proposed iterative approach the results indicate that the proposed iterative approach produces accurate estimates under biased assumptions of the structural parameters the iterative correction of the principal components is essentially to match the distribution patterns of the underlying parameter field with the most dominant principal component the developed approach is used to investigate to what extent of biasness of the prior information can be corrected specifically we show that the smoothness defined by the initial model type is difficult to be fundamentally changed but the major distribution patterns can be precisely characterized the variance in the covariance model can be well corrected even for very biased initial values while the correlation length has an applicable range in the numerical cases we use constant number of retained principal components to simplify the updating it is possible to reduce the number during the inversion to further reduce the computational costs because the parameter distribution pattern is described by the updated dominant principal components the algorithm includes a method by generating conditional realizations to realize a variable number of retained principal components a rigorous study is underway in addition we can combine the proposed approach with quasi newton methods to reduce the forward model runs to approximate the jacobian matrix zhao and luo 2021 credit authorship contribution statement yue zhao conceptualization methodology writing original draft jian luo conceptualization methodology supervision writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we thank dr wolfgang nowak and two other anonymous reviewers for their constructive comments that helped improve the manuscript we thank dr olaf cirpka for providing the computer codes for conducting the forward numerical simulations 
327,surface depressions are important topographic characteristics for surface runoff initiation and the spatial distribution of depressions may further affect the timing and quantity of surface runoff reaching channels and outlets however many hydrologic models simulate the fill spill processes for depression dominated areas in a lumped manner and release outflows from depressions to channels or outlets directly as a result the progressive formation of contributing area ca and the dynamic runoff contribution process are not well characterized the objective of this study is to improve depression oriented hydrologic modeling by incorporating the influence of depression storages and their spatial distribution into the simulation of surface runoff generation and flow routing to achieve this objective a modified depression oriented variable contributing area md vca model is developed to simulate the depression dominated catchment response during rainfall events which employs a new depressional time area zone scheme to deal with the spatially distributed depression storages tracks the intrinsic changing patterns of the connected areas and depression storage simulates the connected area based surface runoff generation dynamics implements a new ca based surface runoff routing technique and quantifies the likelihood of occurrence of outlet ca and runoff contributions using the joint probability distribution associated with depression storages and their spatial distribution the performance of the md vca model was evaluated through the application to a depression dominated watershed in the prairie pothole region of north dakota simulation results demonstrated that the md vca model was able to simulate the threshold controlled overland flow dynamics under different rainfall conditions and it effectively revealed the influences of depression storages and their spatial distribution on surface runoff generation and propagation processes keywords hydrologic modeling depressions hydrologic connectivity surface runoff joint probability distribution 1 introduction depressions which serve as surface impoundments play important roles in hydrologic processes blanchette et al 2019 chu 2017 darboux and huang 2005 kamphorst et al 2000 for example depressions undergo filling spilling and merging processes during rainfall events resulting in threshold controlled discontinuous overland flow chu et al 2013 yang and chu 2015 as such the water yields in terms of spatial distribution and magnitudes are affected huang and bradford 1990 rajib et al 2020 and the outlet contributing area ca expands gradually as rainfall continues chu 2017 grimm and chu 2018 peñuela et al 2016 shaw et al 2013 in addition the spatial arrangement of depression storages further affects the quantity and timing of runoff reaching the basin outlet shook et al 2021 demonstrated the influences of spatial and frequency distribution of depression storages on the relationships of the connected fraction depression storage volume however in many hydrologic models the influence of depressions within a subbasin is often simulated using a lumped depression which has a constant ca and releases outflow to the subbasin outlet directly in this case the outlet ca remains constant at the beginning of a rainfall event and then suddenly jumps to the entire subbasin area when the lumped depression starts to spill water antoine et al 2009 wang et al 2019 resulting in a failure in mimicking the real variations in the outlet ca and the surface runoff generation and routing processes thus there is a significant research need for developing methods to quantify the hydrologic effects of spatially distributed depression storages and simulate the threshold controlled rainfall runoff processes to deal with the complicated influences of depressions on catchment response new modeling approaches have been developed for instance chu et al 2013 and yang and chu 2015 simulated the overland flow on depression dominated surfaces by proposing the concepts of puddle to puddle p2p and cell to cell c2c and developing a physically based distributed model the model captured the specific hydrologic connections of depressions and the detailed threshold behaviors of overland flow fortin et al 2001 developed a distributed hydrologic model hydrotel to simulate the spatiotemporal variations of hydrologic processes and fossey et al 2015 further incorporated the wetland modules based on the concept of hydrologic equivalent wetland hew wang et al 2008 into the hydrotel to simulate the hydrologic effects of wetlands fossey et al 2016 and wu et al 2020 analyzed the influences of spatially distributed wetlands on streamflow using the hydrotel model antoine et al 2011 simulated the influence of depression storage and surface detention on surface runoff triggering and propagation at a grid inter rill scale by weighting the effective water input or ca using a relative surface connection function evenson et al 2016 modified the soil and water assessment tool swat to create two additional types of hydrologic response units hrus that respectively represented individual depressions and their catchments the interactions of depressions and surface runoff routing were also simulated in the modified swat model by constructing the upstream downstream networks of depressions to facilitate watershed scale hydrologic modeling in depression dominated regions some studies have been conducted to simulate the depression induced surface runoff generation processes for example wang et al 2019 developed a depression oriented hydrologic hydrol d model for the simulation of threshold controlled dynamic overland flow in hydrol d all depressions within a subbasin were aggregated together and hierarchical control thresholds were applied to determine the dynamic water release from the lumped depression of the subbasin grimm and chu 2020 improved the hec hms model for depression dominated areas by incorporating a newly developed depression threshold control proxy dtcp specifically all depressions and their cas within a subbasin were lumped together and the dtcp specified the relationship between depression storage and outflow for the depressional area to simulate the filling spilling overland flow dynamics in these models the influences of spatially distributed depressions are simplified in a lumped manner at a subbasin level and the outflow from depressions are released to the subbasin outlet directly mekonnen et al 2016 incorporated a probability distribution approach into the swat to improve hydrologic modeling for depression dominated areas in their model the threshold depression storage capacity associated with the filling spilling conditions of depressions was determined first and a probability density function was used to estimate the depression storage and outflow from fully filled depressions similarly the surface runoff generated from the spatially distributed depressions was delivered to the subbasin main channel directly to simulate the influence of the spatially distributed depressions abedini 1998 applied a probability density function of travel times to translate the direct runoff generated from depressions to the basin outlet the travel time function was equivalent to an instantaneous unit hydrograph and simulated the catchment response when all depressions were fully filled the objective of this study is to improve hydrologic modeling for depression dominated areas by incorporating both depression storages and their spatial distribution in surface runoff routing to quantify the variations in outlet ca and the threshold controlled overland flow dynamics and propagation to address this objective a modified depression oriented variable contributing area md vca model is developed in this study by introducing a new depressional time area zone scheme to account for the spatially distributed depression storages and developing a variable ca based surface runoff routing technique to simulate the movement of overland flow and runoff detention and retention in addition the joint probability distributions associated with both depression storages and their spatial distribution are established to depict the likelihood of occurrences of outlet cas and runoff contributions the performance of the md vca model is evaluated through the application to a depression dominated watershed in north dakota the modeling results reveal the influences of both depression storages and their spatial distribution and highlight the improvements of the md vca model 2 materials and methods 2 1 surface topographic characteristics and digitalization depressions have specific shapes storages and contributing areas moreover if they share common thresholds they may merge with others during rainfall events forming higher level depressions the highest level depressions which embed all depressions that have the potential to merge together with their contributing areas are termed as puddle based units pbus chu et al 2010 similarly the channel segments and their contributing areas are defined as channel based units cbus tahmasebi nasab et al 2017 in this study pbus and cbus are taken as basic hydrotopographic units for depressional areas and non depressional areas respectively to identify such hydrotopographic units and their spatial distributions an arcgis based surface delineation algorithm hud dc wang and chu 2020 was used based on the original digital elevation model dem of a topographic surface hud dc identifies all pbus and cbus including the highest level depressions referred to as depressions hereafter and channel segments as well as their contributing areas in addition the maximum depression storage of depressions and the surface areas of the identified pbus and cbus are calculated which are further used for the md vca modeling 2 2 md vca modeling framework in this study two impact factors of catchment response are considered depression storages and their spatial distribution note that the spatial distribution of cbus are also included since they are hydrologic units with zero depression storage depression storages affect the initiation of surface runoff while their spatial distribution may further influence the timing and quantity of surface runoff reaching outlets when surface runoff is generated from hydrologic units i e pbus or cbus under a certain rainfall condition the areas of these units are considered to be connected areas moreover when generated surface from a hydrologic unit reaches the outlet the area of this unit is identified as outlet ca fig 1 illustrates the md vca modeling framework based on the detailed topographic parameters e g depression storages and cas of depressions provided by the hud dc algorithm a surface topographic analysis procedure is performed for each subbasin to assess the intrinsic influences of depressions on runoff contribution which is then implemented in the simulation of variable ca and threshold controlled overland flow to account for the aforementioned two impact factors new analysis and simulation methods are developed in the md vca model fig 1 specifically in the surface topographic analysis procedure a subbasin is divided into a number of depressional time area zones to deal with the spatially distributed depression storages and a joint probability distribution associated with depression storages and their spatial distribution that depicts the likelihood of the occurrence of the outlet discharge is identified the intrinsic changing patterns of depression storage and connected areas for each depressional time area zone are determined then a variable connected area based surface runoff generation algorithm is applied to track the filling spilling of depressions and the formation of connected areas for each time step in the rainfall event and a new variable ca based surface runoff routing technique is developed in this study to mimic the propagation of generated surface runoff and the expansion of ca 2 3 intrinsic characteristics dominated by spatially distributed depression storages 2 3 1 delineation of depressional time area zones the depressional time area zones are established to consider the influence of spatially distributed hydrologic units on surface runoff routing fig 1 specifically the spatial distribution of hydrologic units i e pbus and cbus in a subbasin is represented by their runoff travel time to the subbasin outlet and depressional time area zones of the subbasin are specified by the runoff travel time isochrones fig 2 a each depressional time area zone contains all hydrologic units that have the possibility to contribute surface runoff to subbasin outlet within the same time interval then as detailed in the following subsections the connected areas of each depressional time area zone are controlled by the other impact factor i e depression storages and the surface runoff generated from the connected areas is further routed to the subbasin outlet to calculate the runoff travel time for a hydrologic unit the flow length from its depression threshold or channel ending point to the subbasin outlet is determined by using the arcgis flow direction function and flow length function the runoff travel time from the depression threshold or channel ending point to the subbasin outlet is calculated by using the scs lag equation 1 t i j 0 000227 l i j 0 8 1000 c n j 9 0 7 s j 0 5 where t i j runoff travel time from depression threshold or channel ending point of unit i in subbasin j to the subbasin outlet h l i j flow length from the depression threshold or channel ending point of unit i in subbasin j to the subbasin outlet m cnj curve number of subbasin j sj average slope of subbasin j m m to create the depressional time area zone diagram fig 2a and b the interval of the time isochrones is the same as that in the simulation procedure i e 1 h thus the hydrologic units with a runoff travel time less than or equal to 1 h are included in depressional time area zone 1 the hydrologic units with a runoff travel time between 1 and 2 h are included in depressional time area zone 2 and all other higher depressional time area zones are defined in the same fashion 2 3 2 identification of joint probability distribution the surface topographic analysis also explores the probability distribution of runoff contributions from the subbasin area which is further used by the simulation procedure to determine the likelihood of occurrence of outlet discharge fig 1 to obtain the subbasin level probability distribution the runoff contributions of hydrologic units are first analyzed by using the depression storage capacities and runoff travel times calculated by eq 1 of hydrologic units note that the depressional time area zone scheme is not used in this analysis specifically the hydrologic units with a smaller depression storage capacity have a higher probability to generate surface runoff and the hydrologic units that have a shorter runoff travel time have a higher probability to contribute surface runoff to the subbasin outlet when surface runoff is generated thus a joint probability distribution associated with depression storage and runoff travel time is developed in this study to depict runoff contributions of hydrologic units by analyzing the impacts of depression storage and runoff travel time separately to examine the probability of a hydrologic unit that generates surface runoff all hydrologic units are organized in an ascending order based on their maximum depression storages and assigned unique ranking numbers starting from 0 note that the hydrologic units that have the same maximum depression storage have the same ranking number the probability of a hydrologic unit that generates surface runoff is calculated by 2 p i j d s 1 m i j d s m j where p i j ds probability of unit i in subbasin j that generates surface runoff m i j ds ranking number of unit i in subbasin j based on its maximum depression storage and mj total number of units in subbasin j to determine the probability of a hydrologic unit that contributes surface runoff to the subbasin outlet when surface runoff is generated all hydrologic units are rearranged in an ascending order based on their runoff travel times and reassigned ranking numbers starting from 0 similarly the hydrologic units that have the same runoff travel time have the same ranking number the probability that a hydrologic unit contributes surface runoff to the subbasin outlet when surface runoff is generated is calculated by 3 p i j t 1 m i j t m j where p i j t probability that unit i in subbasin j contributes surface runoff to the subbasin outlet when surface runoff is generated and m i j t ranking number of unit i in subbasin j based on its runoff travel time then the joint probability of the hydrologic units whose generated surface runoff reaches the subbasin outlet is calculated by 4 p i j d s t p i j d s p i j t where p i j ds t joint probability of unit i in subbasin j that contributes surface runoff to the subbasin outlet the joint probability distribution of runoff contributions of hydrologic units is illustrated in fig 2c without considering the spatial distribution of depressions obtained by eq 2 squares in fig 2c the probability of a hydrologic unit contributing surface runoff to the subbasin outlet gradually decreases with the increase of the maximum depression storage due to the impact of the spatial distribution of hydrologic units the joint probabilities of runoff contributions of hydrologic units fluctuate at different maximum depression storages even at the same maximum depression storage and are less than or equal to the probability that only considers depression storage once the joint probabilities of runoff contributions of hydrologic units are obtained the subbasin level runoff contributions can be described that is when the surface runoff generated from any particular hydrologic unit reaches the subbasin outlet the outlet ca consists of all hydrologic units that have a joint probability greater than or equal to that of this particular unit thus the ca is coupled with the joint probability distribution to describe the probability of subbasin level runoff contributions fig 2d the joint probability decreases as the ca increases and reaches its minimum when the entire subbasin contributes surface runoff to the outlet 2 3 3 determination of intrinsic property curves to facilitate the simulation of the depression storage controlled surface runoff generation fig 1 the surface topographic analysis tracks the intrinsic changing patterns of fully filled depression storage and connected areas during depression filling for each depressional time area zone specifically a constant net water input is uniformly applied to a subbasin and the depression storage of a hydrotopographic unit is updated after each incremental application by 5 d s i z j k min d s i z j k 1 a i z j i j k m d s i z j where ds i z j k depression storage of unit i in depressional time area zone z of subbasin j at time step k l3 a i z j area of unit i in depressional time area zone z of subbasin j l2 i j k depth of net water input of subbasin j at time step k l and mds i z j maximum depression storage of unit i in depressional time area zone z of subbasin j l3 when the depression storage of a unit reaches its maximum value this unit is then recognized as a part of the connected areas of the depressional time area zone as discussed previously a subbasin is divided into depressional time area zones for transferring surface runoff to the subbasin outlet thus the connected area and the fully filled depression storage fds for each depressional time area zone are calculated respectively after each incremental application of the net water input by 6 c n a z j k i 0 m j a i z j μ i k 7 f s z j k i 0 m j m d s i z j μ i k where cna z j k connected area of depressional time area zone z in subbasin j at time step k l3 μ i k filling spilling index of unit i at time step k if unit i is fully filled at time step k μ i k 1 otherwise μ i k 0 and fs z j k fully filled depression storage of depressional time area zone z in subbasin j at time step k l3 fig 2e shows the connected area versus cumulative net water input and the fds versus cumulative net water input for depressional time area zone 1 of the subbasin in which the connected area and fds are normalized by the area and total depression storage of the zone respectively 2 4 modeling of threshold controlled overland flow dynamics 2 4 1 variable connected area based surface runoff generation once the intrinsic characteristics dominated by the spatially distributed depression storages are detected by the surface topographic analysis procedure the md vca model uses a variable connected area based surface runoff generation approach to determine the connected area depression storage and generated surface runoff of the subbasin through a time loop for a rainfall event figs 1 and 2 at each time step the scs curve number method is used to simulate the subbasin level rainfall excess which then fills depressions and or becomes surface runoff to model surface runoff generation the connected areas of all depressional time area zones fig 2f are first simulated based on the cumulative rainfall excess and the corresponding intrinsic changing pattern of the connected areas fig 2e for each depressional time area zone during a rainfall event the connected area at a time step is kept the same as that in the previous time step if rainfall excess is zero otherwise the connected area expands until reaching the entire zone area the surface runoff generated from the connected area of a depressional time area zone fig 2g can be calculated by 8 r z j k z o n e p j k a z j k c n a ds z j k c n a ds 0 z j k c n a where r z j k z o n e surface runoff generated from depressional time area zone z in subbasin j at time step k l3 p j k depth of rainfall excess of subbasin j at time step k l a z j k c n a connected area of zone z in subbasin j at time step k l2 and ds 0 z j k c n a and ds z j k c n a depression storages of the contributing area of depressional time area zone z in subbasin j at the beginning and end of time step k respectively l3 if a partially filled area of a time area zone exists i e the connected area is less than the entire area of the depressional time area zone all rainfall excess of this area since the beginning of the rainfall event is trapped in depressions and the depression storage of this area is given by 9 ds z j k p f a c p j k a z j z o n e a z j k c n a where ds j k p f a depression storage of the partially filled area of depressional time area zone z in subbasin j at time step k l3 cp j k cumulative depth of rainfall excess of subbasin j at time step k l a z j z o n e area of depressional time area zone z in subbasin j l2 then the total connected area and depression storage of the subbasin can be respectively expressed as 10 a j k c n a z 1 n a z j k c n a 11 ds z j k s u b z 1 n ds z j k c n a ds z j k p f a where a j k c n a connected area of subbasin j at time step k l2 and ds z j k s u b depression storage of subbasin j at time step k l3 2 4 2 variable contributing area based surface runoff routing once surface runoff is generated from the connected areas of a subbasin it is then transferred to the subbasin outlet within some time scales thus based on the depressional time area zone scheme a new variable ca based surface runoff routing technique is developed in the md vca model to transfer the surface runoff generated from each depressional time area zone to the subbasin outlet figs 1 and 2 since the time interval of the time area isochrones is the same as that of simulation the outlet ca at each time step fig 2h is calculated by using the linear superposition of the connected areas of depressional time area zones for example the ca at time step 1 is equal to the connected areas of depressional time area zone 1 at time step 1 and the ca at time step 2 consists of the connected areas of depressional time area zone 1 at time step 2 and the connected areas of depressional time area zone 2 at time step 1 thus the outlet ca can be expressed as 12 a j k c a z 1 t a z j k z 1 c n a where a j k c a ca of subbasin j at time step k l2 then the surface runoff from the outlet ca fig 2i is given by 13 r j k c a z 1 t r z j k z 1 z o n e where r j k c a surface runoff generated from the ca of subbasin j at time step k l3 the surface runoff generated from the ca is subject to the detention due to depression filling and the attenuation is simulated by using a linear reservoir equation in this study fig 2 thus the direct runoff hydrograph at the subbasin outlet fig 2j can be determined by 14 q j k 2 δ t 2 k δ t r j k c a 2 k δ t 2 k δ t q j k 1 where q j k surface runoff reaching the subbasin outlet at time step k l3 k storage coefficient and r j k c a average surface runoff from the ca of subbasin j at time step k l3 in addition based on the relationship between the joint probability distribution of runoff contribution and the subbasin ca fig 2d the probability of occurrence of the ca or outlet discharge is calculated fig 2k 2 5 study area and input data the md vca model was tested through an application to the upper portion of the upper sheyenne river watershed a depression dominated watershed within the prairie pothole region ppr of north dakota fig 3 shows the geographical location the final outlet i e a usgs gaging station 05055300 channel network and land use of the selected watershed according to the national land cover database nlcd 2011 the watershed area 4 619 7 km2 is mostly covered by cultivated crops 49 5 and hay 20 and 13 8 of watershed area is dominated by open water and wetlands in addition to the land use and land cover data the dem soil type meteorological and hydrological data are also required for the md vca model in this study the 10 m resolution dem downloaded from the usgs national map tnm was used to divide the watershed into subbasins and determine surface topographic parameters e g maximum depression storages of for each subbasin the state soil geographic statsgo2 dataset provided the soil data for the selected watershed and the composite curve numbers for all subbasins were then calculated based on the land use and soil data precipitation data at an hourly interval were obtained from a precipitation station latitude 48 167 n longitude 99 648 w fig 3 of the north dakota agriculture weather network ndawn the observed discharge data at the watershed outlet which were used to validate the md vca model were downloaded from the usgs national water information system 2 6 evaluation of model performance the performance of the md vca model was evaluated by using three rainfall events event 1 6 26 2009 18 00 7 2 2009 8 00 event 2 9 20 2019 18 00 9 24 2019 0 00 and event 3 6 13 2017 4 00 6 18 2017 10 00 which have different features such as magnitudes and durations specifically rainfall event 1 was used for calibration and the two other events were used for model verification with the presence of long antecedent dry periods the initial baseflow values of these three events were set to the observed discharges at the beginning of the corresponding rainfall events a set of parameters were selected for the model calibration and their initial values were estimated for example the surface storage coefficient k was determined by the ratio of the discharge at the inflection point on the falling limb of the observed hydrograph to the derivative of discharge with respect to time clark 1945 usace hec 2000 for the purpose of sensitivity analysis a dimensionless sensitivity index ficklin et al 2012 lenhart et al 2002 was used in this study 15 i o 2 o 1 o 0 2 δ p p 0 where i sensitivity index δp percentage adjustment of the parameter and o 0 o 1 and o 2 model output values with parameter values at p 0 p 0 δp and p 0 δp respectively this sensitivity index varies between and and a larger absolute value of the index indicates that the parameter has a stronger impact on the model output in this sensitivity analysis the model output was specified as the mean of the simulated outlet discharges during a rainfall event and the percentage for the parameter adjustment was set to 10 after the sensitivity analysis manual trial and error calibration was performed eventually the simulated and observed discharges at the watershed outlet were compared and four statistical parameters nash sutcliffe efficiency nse percent bias pbias ratio of the root mean square error to the standard deviation of the observed data rsr and coefficient of determination r 2 were used to quantitatively evaluate the performance of the md vca model they are mathematically expressed as 16 n s e 1 k 1 n q k o b s q k s i m 2 k 1 n q k o b s q o b s 2 17 p b i a s k 1 n q k s i m q k o b s k 1 n q o b s k 100 18 r s r k 1 n q k o b s q k s i m 2 k 1 n q k o b s q o b s 2 19 r 2 k 1 n q k o b s q o b s q k s i m q s i m k 1 n q k o b s q o b s 2 k 1 n q k s i m q s i m 2 2 where n total number of discharge observations q k o b s observed outlet discharge at time step k l3 t q k s i m simulated outlet discharge at time step k l3 t q o b s mean of the observed outlet discharges l3 t and q s i m mean of the simulated outlet discharges l3 t in addition to account for the model uncertainty the reliability of the modeling of connected areas and cas was evaluated by using a statistical method specifically the rainfall excess at the end of each rainfall event was calculated and used to analyze the filling spilling conditions of depressions based on which the connected areas and cas at the end of each rainfall event were determined then according to the central limit theorem clt the normal statistic was used to calculate the limits of the proportions of the connected and contributing areas at a level of 95 confidence which were referred to as the 95 confidence intervals of the connected areas and cas afterwards the connected areas and cas simulated by the md vca model were compared with the corresponding 95 confidence intervals to demonstrate the accuracy of the simulation results 2 7 modeling scenarios the influence of spatially distributed depressions was discussed through three modeling scenarios the first scenario s1 only accounted for the influence of depressions on surface runoff generation and assumed that the generated surface runoff contributed to the corresponding subbasin outlet when it was generated thus this scenario was performed by setting only one depressional time area zone per subbasin in the md vca model then the surface runoff generation of a subbasin was simulated by tracking the connected areas and the depression storage of the depressional time area zone i e the subbasin and the generated surface runoff was routed to the subbasin outlet by using a linear reservoir function the second scenario s2 was set up to simulate the influence of spatial distribution of generated surface runoff on surface runoff routing without considering depressions in this scenario storage capacities of depressions were set to zero and thus each subbasin consisted of a number of cbus and special pbus to deal with the spatial distribution of generated surface runoff each subbasin was divided into many depressionless time area zones with a 1 h time interval of isochrones the depressionless time area zones have the same ranges and areas as the corresponding depressional time area zones when depressions are taken into consideration which is because a depressional time area zone is designed to contain all pbus and cbus that have the potential to contribute surface runoff to the subbasin outlet during the same time interval then the generated surface runoff of each depressionless time area zone was routed to subbasin outlet based on the time area zone scheme and a linear reservoir function in the md vca model in the third scenario s3 the md vca model was used to simulate the influence of spatially distributed depressions on surface runoff generation and routing processes in this scenario a subbasin was divided into many depressional time area zones with a 1 h time interval of isochrones connected areas and surface runoff during a rainfall were tracked for each depressional time area zone and the generated surface runoff was routed to the subbasin outlet based on the depressional time area zone scheme and a linear reservoir function 3 results and analyses 3 1 topographic characteristics depression storages and their spatial distribution are the two impact factors of catchment response considered in this study fig 3 shows the delineated 12 subbasins of the watershed and table 1 lists the areas of pbus and cbus and the total maximum depression storages for all subbasins for a better understanding of the topographic characteristics of all subbasins fig 4 displays the distributions of maximum depression storages and runoff travel times from hydrologic units channel ending points of cbus or depression thresholds of pbus to the corresponding subbasin outlets for all subbasins specifically the maximum depression storages of all subbasins are right skewed distributed indicating that subbasins contain numerous depressions with smaller depression storages that can be quickly fully filled to generate surface runoff during the early stage of a rainfall event the runoff travel times from hydrologic units to the corresponding subbasin outlets are approximately normally distributed and the median travel time varies among subbasins which can be attributed to their distinct characteristics e g sizes and shapes the hydrologic units with a shorter travel time to the corresponding subbasin outlet have a higher probability to make runoff contributions when surface runoff is generated fig 5 shows the joint probability of the normalized ca for all subbasins under the influence of both depression storages and their spatial distribution the hydrologic units with smaller depression storages and runoff travel time to the corresponding subbasin outlet have higher probabilities to become part of the outlet ca and the joint probability of outlet ca decreases as the expansion of outlet ca and reaches its minimum when all hydrologic units contribute surface runoff to the subbasin outlet to account for the impact of the spatial distribution of depression storages on subbasin runoff contribution each subbasin was also divided into a number of depressional time area zones the time interval of the isochrones is one hour table 1 lists the number of depressional time area zones for all subbasins and fig 6 shows the spatial distribution of depressional time area zones of subbasin 1 as an example fig 7 a illustrates the percentages of non depressional areas and depressional areas for all depressional time area zones of subbasin 1 as shown in fig 7a the non depressional areas only dominate a small portion of the area of the corresponding zones and they generate surface runoff and are connected once rainfall satisfies the initial abstraction i e canopy interception and infiltration before surface runoff initiates as rainfall continues more depressions are filled and start to generate surface runoff and thus the connected areas of each zone expand to further investigate the generation of surface runoff fig 7b and 7c show the formation of connected areas and the increasing patterns of fully filled depression storages for two representative depressional time area zones zone 4 and zone 8 respectively the percentages of connected areas of both zones are larger than zero before water input i e rainfall is applied representing the ratio of non depressional area of each zone at the beginning of depression filling the connected areas and fully filled depression storage of both zones increase rapidly since many smaller depressions are quickly filled and generate surface runoff as the net water input increases the connected areas and fully filled depression storage of zone 4 exhibit a stepwise increasing pattern while the connected areas and fully filled depression storage of zone 8 only show slight stepwise changes which can be attributed to the properties of pbus i e surface areas and depression storages of pbus of both zones for example there are several pbus with large depression storages in zone 4 which take a longer time to be fully filled and become a part of connected areas of the zone resulting in stepwise changes since there are a large number of depressions with different maximum depression storages in the subbasins and each depressional zone only dominates a small portion of the subbasin area fig 7a the stepwise changes in the connected areas and fully filled depression storage of the subbasin fig 7d are not as obvious as those in fig 7b and c the ratios of connected areas and the fully filled depression storages of both zones eventually reach 1 0 when all depressions are fully filled the intrinsic changing patterns of the connected areas and fully filled depression storage facilitate the simulation of ca formation and surface runoff generation for real rainfall events 3 2 evaluation of the md vca model table 2 lists the calibrated parameters and their sensitivity index values according to ficklin et al 2012 the parameter is considered as extremely high sensitivity when i 1 0 and high sensitivity when 0 2 i 1 0 and a negative value of the index indicates an inverse impact of the parameter on the outlet discharge thus the five selected parameters including curve number cn baseflow recession constant r initial abstraction coefficient ia surface storage coefficient k and baseflow threshold value bt were adjusted in the calibration process in addition the lag time used for channel routing which was not significantly sensitive was also taken into consideration in the calibration fig 8 shows the comparisons between the observed and simulated hydrographs at the watershed outlet for the calibration event event 1 and two validation events event 2 and event 3 table 3 shows the percent deviations of the simulated peak flows and peak times comparing to the observed data i e observed value simulated value 100 observed value calculated for the three rainfall events as shown in fig 8 and table 3 the peak flows and peak times of the hydrographs for events 1 and 2 reasonably match the observed data and the general shapes of the hydrographs for the three events are well characterized a slight underestimate at the beginning of the hydrograph for event 1 fig 8a and a slightly early peak flow for event 3 fig 8c can be observed which may be due to the spatial and temporal variations of the rainfall in addition the nse rsr pbias and r 2 were used to quantitatively describe the agreement between the simulated and observed hydrographs table 3 also lists the values of nse rsr pbias and r2 for the calibration and validation events according to moriasi et al 2007 2015 simulations are considered as very good if the nse is greater than 0 75 the rsr is less than 0 5 and pbias is less than 10 in this study the calculated nse rsr and pbias fall into the recommended ranges indicating a very good agreement between the simulated and observed discharges in addition the r 2 values 0 95 for the calibration and validation events also indicated a good performance of the md vca model in addition to the comparisons of simulated and observed hydrographs at the watershed outlet the simulated connected areas cas depression storage and surface runoff of the calibration and validation events were analyzed for all subbasins fig 9 a c show the simulated connected areas and cas as well as their corresponding 95 uncertainty bands i e the 95 confidence intervals obtained from the statistical analysis for all subbasins at the end of the calibration and validation events respectively the connected areas and cas simulated by the md vca model fall into their 95 uncertainty bands demonstrating the reliability of the simulation results of the md vca model this also demonstrates the capability of the md vca model in mimicking the threshold controlled runoff generation and propagation processes under the influence of model uncertainty fig 9d f display the simulated depression storage and the total amount of generated surface runoff of all subbasins i e water depth over the corresponding subbasin area at the end of the calibration and validation events respectively for the three storm events more rainfall excess water was trapped by depressions and less water became surface runoff as shown in fig 9 the simulated connected areas depression storage and surface runoff vary among all subbasins which can be attributed to the differences in the land use soil type and surface topographic characteristics e g areas and depression storages of hydrologic units of the subbasins the cas of all subbasins also differ due to the dissimilar spatial distributions of hydrologic units for example at the end of the calibration event 18 6 of the connected areas of subbasin 5 became the subbasin ca while only 4 of the connected areas of subbasin 1 became the subbasin ca this is because subbasin 5 has more hydrologic units with smaller depression storages which are located close to the subbasin outlet 3 3 hydrologic effects of depression storages and their spatial distribution to demonstrate the role of the spatially distributed depressions in surface runoff generation and routing simulation results from the three modeling scenarios were compared fig 10 shows the connected areas cas and surface runoff simulated for the three modeling scenarios for subbasin 1 during the storm event 2 that occurred in september 2019 as an example specifically figs 10a and 10c demonstrate the influence of depressions on the surface runoff generation by illustrating the simulated connected areas normalized by the subbasin area and the total amount of surface runoff generated from the subbasin connected areas in the three modeling scenarios during the storm event as aforementioned s1 and s3 considered the influence of the real delineated depression storages on surface runoff generation while s2 ignored the influence of depression storages thus in s2 all rainfall excess became surface runoff and the entire subbasin was connected in s1 and s3 however all rainfall excess of cbus became surface runoff and the rainfall excess of pbus flowed to depressions and surface runoff initiated only when there were fully filled depressions in this way a part of rainfall excess of this subbasin was trapped in depressions and the remaining water became surface runoff whereas the subbasin connected areas consisted of the non depressional area i e areas of cbus as well as the cas and ponding areas of the fully filled depressions thus as shown in fig 10a the normalized connected areas of the subbasin in s2 at each time step reached 1 0 while the normalized connected areas of the subbasin in s1 and s3 at each time step were equal and less than 1 0 similarly s1 and s3 had the same amount of total surface runoff at each time step and the values of generated surface runoff in s1 and s3 were less than that simulated in s2 fig 10c which was the total amount of rainfall excess at this time step fig 10b and d respectively show the subbasin ca normalized by the subbasin area and the surface runoff generated on the ca for the three modeling scenarios during the storm event revealing the influence of the spatial distribution of depression storages on surface runoff routing as discussed previously in s1 all surface runoff contributed to the subbasin outlet when it was generated and the subbasin connected areas became the outlet ca of the subbasin while s2 and s3 accounted for the spatial distributions of generated surface runoff thus the simulated outlet ca at each time step in s1 was equal to the corresponding subbasin connected areas fig 10a and b and the runoff contribution at each time step also equaled the total amount of surface runoff generated at the same time step fig 10c and d in this case the runoff contributions in the subbasin varied with the rainfall intensities and there was no runoff contribution to the subbasin outlet if there was no rainfall excess fig 10d in s2 and s3 however the spatial distribution of surface runoff was considered thus the outlet cas and the runoff contributions of the subbasin varied with both rainfall intensity which affected the connected areas or the generated surface runoff of each time area zone and runoff travel time for example at 9 20 2019 23 00 in s2 and s3 i e the first time step with rainfall excess only the connected areas of time area zone 1 became the outlet cas very small values fig 10b the surface runoff generated from time area zone 1 made runoff contributions to the subbasin outlet at this time step also very small values fig 10d and the surface runoff generated from other time area zones contributed to the subbasin outlet at the following time steps depending on the runoff travel time of each time area zone therefore after 9 21 2019 7 00 i e the end of the storm the outlet ca and runoff contribution in s2 and s3 were still larger than zero fig 10b and d in addition the outlet cas and runoff contributions simulated in s2 were greater than those in s3 for the same time steps fig 10b and d which can be attributed to the fact that s2 did not consider depression storage the time when the outlet ca and runoff contributions reached their peak values were the same as that in s2 and s3 which can be attributed to the same scheme of time area zones i e the depressionless and depressional time area zones in s2 and s3 had the same spatial range from the comparisons of s1 s3 fig 10 it can be found that there are significant differences in the subbasin connected areas and the total amounts of surface runoff simulated with and without considering depressions demonstrating the importance of considering depressions in the modeling of surface runoff generation in addition there are notable discrepancies in the formation of the outlet ca and the timing and quantity of runoff contributions with and without considering the spatial distribution of depressions even though the subbasin connected areas and the total amount of surface runoff are the same in both scenarios i e s1 and s3 thus it is essential to incorporate both depression storages and their spatial distribution in the simulation of rainfall runoff dominated by surface depressions the significance of considering spatially distributed depression storages was also emphasized by fossey et al 2016 and wu et al 2020 who explored the influence of the geographic location of wetlands on streamflow for different watersheds 4 discussion in this study the md vca model was developed to incorporate the influences of both depression storages and their spatial distribution into the simulation of depression oriented rainfall runoff processes in recent years some hydrologic models have been developed in which the depression filling spilling processes are simulated and the influences of the spatial distribution of depressions and generated surface runoff are simplified for example zeng et al 2020 simulated the overland flow filling spilling dynamics for depression dominated areas using a series of probability distribution functions of depression storages arnold et al 1998 used the surface runoff lag method to route the generated overland flow to the subbasin main channel in the lag method an empirical equation is used to control the amount of surface runoff reaching channels for each time step however these models cannot provide the details on the progressive formation of the outlet ca and the propagation processes of surface runoff the md vca model developed in this study simulates the influence of spatially distributed depressions on surface runoff propagation by introducing the depressional time area zone scheme and developing the ca based surface runoff routing algorithm resultantly in addition to mimicking the generation of surface runoff the md vca model also is able track the formation of ca and runoff contributions in the transfer of generated surface runoff to the subbasin outlet fig 9 to demonstrate the performance of the md vca the simulation results were analyzed in the modeling for real rainfall events the simulated outlet hydrographs showed a good agreement with the observed data fig 8 indicating the ability of the md vca model in mimicking the threshold controlled overland flow dynamics under different rainfall conditions in addition to the outlet discharges the simulated connected areas and cas for all subbasins for the calibration and validation events were verified by comparing to their 95 confidence intervals fig 9 the comparisons demonstrated the reasonability and accuracy of the simulation results in addition during the calibration and validation events occurred in summer and fall months a large portion of rainfall excess was trapped by depressions and the simulated outlet cas of all subbasins only reached 20 39 of their areas during those rainfall events fig 9 which are consistent with the findings in other studies for depression dominated areas zeng et al 2020 evenson et al 2015 incorporating the spatial distribution of depression storages is one of the unique features of the md vca model the joint probability distribution associated with depression storages and their spatial distributions is determined in the md vca model to quantify the likelihood of occurrences of the outlet ca the md vca model implements simulations of connected areas and surface runoff for each depressional time area zone of subbasins particularly a new depressional time area zone scheme and a unique variable ca based surface runoff routing technique are utilized for surface runoff routing in the md vca model the depressional time area zones of each subbasin are determined based on the flow length and flow travel time from pbus and cbus to the corresponding subbasin outlet note that the md vca model is able to provide the progressive formation of the outlet ca and the propagation of surface runoff which can be attributed to the proposed depressional time area zone scheme however if the spatial distribution of depression storages is not considered the connected areas at each time step have to be lumped together and viewed as the outlet ca in this case the spatial and temporal variations of the outlet ca cannot be well characterized and the movement of surface runoff to the subbasin outlet cannot be properly simulated since the spatial distribution of the generated surface runoff is not considered 5 summary and conclusions the md vca model was developed in this study to simulate the influences of spatially distributed depression storages on catchment responses during rainfall events and the associated threshold behavior in the md vca model a depression dominated subbasin was divided into a number of depressional time area zones based on the spatial distribution of depression storages each of which contained all hydrologic units that had the possibilities to contribute surface runoff to the corresponding subbasin outlet during the same time interval then the intrinsic changing patterns of connected areas and fully filled depression storage were detected for each depressional time area zone which was used to determine the connected areas and the total amount of surface runoff of the subbasin during the surface runoff generation processes under rainfall events the formation of the outlet ca and the contribution of surface runoff to the subbasin outlet were tracked by a newly developed ca based surface runoff routing technique in addition the joint probability distributions associated with both depression storages and their spatial distribution were created to describe the likelihood of occurrences of the outlet cas and runoff contributions the performance of the md vca model was evaluated through the application to a depression dominated watershed in the prairie pothole region of north dakota the simulated hydrographs at the watershed outlet reasonably matched the observed ones and the four statistical parameters rsr nse pbias and r 2 also demonstrated a good agreement between the simulated and observed hydrographs and the ability of the md vca model in simulating the surface runoff initiation and propagation under the influence of spatially distributed depression storages in addition to the discharges at the watershed outlet the simulated connected areas and cas for all subbasins were also verified by their 95 confidence ranges the influences of depression storages and their spatial distribution on surface runoff generation and routing were revealed in this study through three modeling scenarios s1 considering the influence of depression storages only s2 ignoring depression storages and only simulating the influence of spatially distributed surface runoff and s3 taking both depression storages and their spatial distributions into account without considering depression storages the connected areas and the total amounts of generated surface runoff were overestimated leading to overestimations of the outlet cas and runoff contributions to outlets without considering the spatial distribution of depression storages the connected areas and total amounts of generated surface runoff were captured however the model failed to track the outlet cas and characterize the timing and quantity of runoff contributions this modeling study helps bridge the gap in simulating the formation of ca and the contribution of surface runoff under the influence of spatially distributed depression storages credit authorship contribution statement lan zeng conceptualization methodology software validation formal analysis investigation writing original draft writing review editing visualization xuefeng chu conceptualization methodology formal analysis investigation writing review editing data curtion supervision project administration funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this material is based upon work supported by the national science foundation under grant no nsf epscor award iia 1355466 the north dakota water resources research institute also provided partial financial support in the form of a graduate fellowship for the first author 
327,surface depressions are important topographic characteristics for surface runoff initiation and the spatial distribution of depressions may further affect the timing and quantity of surface runoff reaching channels and outlets however many hydrologic models simulate the fill spill processes for depression dominated areas in a lumped manner and release outflows from depressions to channels or outlets directly as a result the progressive formation of contributing area ca and the dynamic runoff contribution process are not well characterized the objective of this study is to improve depression oriented hydrologic modeling by incorporating the influence of depression storages and their spatial distribution into the simulation of surface runoff generation and flow routing to achieve this objective a modified depression oriented variable contributing area md vca model is developed to simulate the depression dominated catchment response during rainfall events which employs a new depressional time area zone scheme to deal with the spatially distributed depression storages tracks the intrinsic changing patterns of the connected areas and depression storage simulates the connected area based surface runoff generation dynamics implements a new ca based surface runoff routing technique and quantifies the likelihood of occurrence of outlet ca and runoff contributions using the joint probability distribution associated with depression storages and their spatial distribution the performance of the md vca model was evaluated through the application to a depression dominated watershed in the prairie pothole region of north dakota simulation results demonstrated that the md vca model was able to simulate the threshold controlled overland flow dynamics under different rainfall conditions and it effectively revealed the influences of depression storages and their spatial distribution on surface runoff generation and propagation processes keywords hydrologic modeling depressions hydrologic connectivity surface runoff joint probability distribution 1 introduction depressions which serve as surface impoundments play important roles in hydrologic processes blanchette et al 2019 chu 2017 darboux and huang 2005 kamphorst et al 2000 for example depressions undergo filling spilling and merging processes during rainfall events resulting in threshold controlled discontinuous overland flow chu et al 2013 yang and chu 2015 as such the water yields in terms of spatial distribution and magnitudes are affected huang and bradford 1990 rajib et al 2020 and the outlet contributing area ca expands gradually as rainfall continues chu 2017 grimm and chu 2018 peñuela et al 2016 shaw et al 2013 in addition the spatial arrangement of depression storages further affects the quantity and timing of runoff reaching the basin outlet shook et al 2021 demonstrated the influences of spatial and frequency distribution of depression storages on the relationships of the connected fraction depression storage volume however in many hydrologic models the influence of depressions within a subbasin is often simulated using a lumped depression which has a constant ca and releases outflow to the subbasin outlet directly in this case the outlet ca remains constant at the beginning of a rainfall event and then suddenly jumps to the entire subbasin area when the lumped depression starts to spill water antoine et al 2009 wang et al 2019 resulting in a failure in mimicking the real variations in the outlet ca and the surface runoff generation and routing processes thus there is a significant research need for developing methods to quantify the hydrologic effects of spatially distributed depression storages and simulate the threshold controlled rainfall runoff processes to deal with the complicated influences of depressions on catchment response new modeling approaches have been developed for instance chu et al 2013 and yang and chu 2015 simulated the overland flow on depression dominated surfaces by proposing the concepts of puddle to puddle p2p and cell to cell c2c and developing a physically based distributed model the model captured the specific hydrologic connections of depressions and the detailed threshold behaviors of overland flow fortin et al 2001 developed a distributed hydrologic model hydrotel to simulate the spatiotemporal variations of hydrologic processes and fossey et al 2015 further incorporated the wetland modules based on the concept of hydrologic equivalent wetland hew wang et al 2008 into the hydrotel to simulate the hydrologic effects of wetlands fossey et al 2016 and wu et al 2020 analyzed the influences of spatially distributed wetlands on streamflow using the hydrotel model antoine et al 2011 simulated the influence of depression storage and surface detention on surface runoff triggering and propagation at a grid inter rill scale by weighting the effective water input or ca using a relative surface connection function evenson et al 2016 modified the soil and water assessment tool swat to create two additional types of hydrologic response units hrus that respectively represented individual depressions and their catchments the interactions of depressions and surface runoff routing were also simulated in the modified swat model by constructing the upstream downstream networks of depressions to facilitate watershed scale hydrologic modeling in depression dominated regions some studies have been conducted to simulate the depression induced surface runoff generation processes for example wang et al 2019 developed a depression oriented hydrologic hydrol d model for the simulation of threshold controlled dynamic overland flow in hydrol d all depressions within a subbasin were aggregated together and hierarchical control thresholds were applied to determine the dynamic water release from the lumped depression of the subbasin grimm and chu 2020 improved the hec hms model for depression dominated areas by incorporating a newly developed depression threshold control proxy dtcp specifically all depressions and their cas within a subbasin were lumped together and the dtcp specified the relationship between depression storage and outflow for the depressional area to simulate the filling spilling overland flow dynamics in these models the influences of spatially distributed depressions are simplified in a lumped manner at a subbasin level and the outflow from depressions are released to the subbasin outlet directly mekonnen et al 2016 incorporated a probability distribution approach into the swat to improve hydrologic modeling for depression dominated areas in their model the threshold depression storage capacity associated with the filling spilling conditions of depressions was determined first and a probability density function was used to estimate the depression storage and outflow from fully filled depressions similarly the surface runoff generated from the spatially distributed depressions was delivered to the subbasin main channel directly to simulate the influence of the spatially distributed depressions abedini 1998 applied a probability density function of travel times to translate the direct runoff generated from depressions to the basin outlet the travel time function was equivalent to an instantaneous unit hydrograph and simulated the catchment response when all depressions were fully filled the objective of this study is to improve hydrologic modeling for depression dominated areas by incorporating both depression storages and their spatial distribution in surface runoff routing to quantify the variations in outlet ca and the threshold controlled overland flow dynamics and propagation to address this objective a modified depression oriented variable contributing area md vca model is developed in this study by introducing a new depressional time area zone scheme to account for the spatially distributed depression storages and developing a variable ca based surface runoff routing technique to simulate the movement of overland flow and runoff detention and retention in addition the joint probability distributions associated with both depression storages and their spatial distribution are established to depict the likelihood of occurrences of outlet cas and runoff contributions the performance of the md vca model is evaluated through the application to a depression dominated watershed in north dakota the modeling results reveal the influences of both depression storages and their spatial distribution and highlight the improvements of the md vca model 2 materials and methods 2 1 surface topographic characteristics and digitalization depressions have specific shapes storages and contributing areas moreover if they share common thresholds they may merge with others during rainfall events forming higher level depressions the highest level depressions which embed all depressions that have the potential to merge together with their contributing areas are termed as puddle based units pbus chu et al 2010 similarly the channel segments and their contributing areas are defined as channel based units cbus tahmasebi nasab et al 2017 in this study pbus and cbus are taken as basic hydrotopographic units for depressional areas and non depressional areas respectively to identify such hydrotopographic units and their spatial distributions an arcgis based surface delineation algorithm hud dc wang and chu 2020 was used based on the original digital elevation model dem of a topographic surface hud dc identifies all pbus and cbus including the highest level depressions referred to as depressions hereafter and channel segments as well as their contributing areas in addition the maximum depression storage of depressions and the surface areas of the identified pbus and cbus are calculated which are further used for the md vca modeling 2 2 md vca modeling framework in this study two impact factors of catchment response are considered depression storages and their spatial distribution note that the spatial distribution of cbus are also included since they are hydrologic units with zero depression storage depression storages affect the initiation of surface runoff while their spatial distribution may further influence the timing and quantity of surface runoff reaching outlets when surface runoff is generated from hydrologic units i e pbus or cbus under a certain rainfall condition the areas of these units are considered to be connected areas moreover when generated surface from a hydrologic unit reaches the outlet the area of this unit is identified as outlet ca fig 1 illustrates the md vca modeling framework based on the detailed topographic parameters e g depression storages and cas of depressions provided by the hud dc algorithm a surface topographic analysis procedure is performed for each subbasin to assess the intrinsic influences of depressions on runoff contribution which is then implemented in the simulation of variable ca and threshold controlled overland flow to account for the aforementioned two impact factors new analysis and simulation methods are developed in the md vca model fig 1 specifically in the surface topographic analysis procedure a subbasin is divided into a number of depressional time area zones to deal with the spatially distributed depression storages and a joint probability distribution associated with depression storages and their spatial distribution that depicts the likelihood of the occurrence of the outlet discharge is identified the intrinsic changing patterns of depression storage and connected areas for each depressional time area zone are determined then a variable connected area based surface runoff generation algorithm is applied to track the filling spilling of depressions and the formation of connected areas for each time step in the rainfall event and a new variable ca based surface runoff routing technique is developed in this study to mimic the propagation of generated surface runoff and the expansion of ca 2 3 intrinsic characteristics dominated by spatially distributed depression storages 2 3 1 delineation of depressional time area zones the depressional time area zones are established to consider the influence of spatially distributed hydrologic units on surface runoff routing fig 1 specifically the spatial distribution of hydrologic units i e pbus and cbus in a subbasin is represented by their runoff travel time to the subbasin outlet and depressional time area zones of the subbasin are specified by the runoff travel time isochrones fig 2 a each depressional time area zone contains all hydrologic units that have the possibility to contribute surface runoff to subbasin outlet within the same time interval then as detailed in the following subsections the connected areas of each depressional time area zone are controlled by the other impact factor i e depression storages and the surface runoff generated from the connected areas is further routed to the subbasin outlet to calculate the runoff travel time for a hydrologic unit the flow length from its depression threshold or channel ending point to the subbasin outlet is determined by using the arcgis flow direction function and flow length function the runoff travel time from the depression threshold or channel ending point to the subbasin outlet is calculated by using the scs lag equation 1 t i j 0 000227 l i j 0 8 1000 c n j 9 0 7 s j 0 5 where t i j runoff travel time from depression threshold or channel ending point of unit i in subbasin j to the subbasin outlet h l i j flow length from the depression threshold or channel ending point of unit i in subbasin j to the subbasin outlet m cnj curve number of subbasin j sj average slope of subbasin j m m to create the depressional time area zone diagram fig 2a and b the interval of the time isochrones is the same as that in the simulation procedure i e 1 h thus the hydrologic units with a runoff travel time less than or equal to 1 h are included in depressional time area zone 1 the hydrologic units with a runoff travel time between 1 and 2 h are included in depressional time area zone 2 and all other higher depressional time area zones are defined in the same fashion 2 3 2 identification of joint probability distribution the surface topographic analysis also explores the probability distribution of runoff contributions from the subbasin area which is further used by the simulation procedure to determine the likelihood of occurrence of outlet discharge fig 1 to obtain the subbasin level probability distribution the runoff contributions of hydrologic units are first analyzed by using the depression storage capacities and runoff travel times calculated by eq 1 of hydrologic units note that the depressional time area zone scheme is not used in this analysis specifically the hydrologic units with a smaller depression storage capacity have a higher probability to generate surface runoff and the hydrologic units that have a shorter runoff travel time have a higher probability to contribute surface runoff to the subbasin outlet when surface runoff is generated thus a joint probability distribution associated with depression storage and runoff travel time is developed in this study to depict runoff contributions of hydrologic units by analyzing the impacts of depression storage and runoff travel time separately to examine the probability of a hydrologic unit that generates surface runoff all hydrologic units are organized in an ascending order based on their maximum depression storages and assigned unique ranking numbers starting from 0 note that the hydrologic units that have the same maximum depression storage have the same ranking number the probability of a hydrologic unit that generates surface runoff is calculated by 2 p i j d s 1 m i j d s m j where p i j ds probability of unit i in subbasin j that generates surface runoff m i j ds ranking number of unit i in subbasin j based on its maximum depression storage and mj total number of units in subbasin j to determine the probability of a hydrologic unit that contributes surface runoff to the subbasin outlet when surface runoff is generated all hydrologic units are rearranged in an ascending order based on their runoff travel times and reassigned ranking numbers starting from 0 similarly the hydrologic units that have the same runoff travel time have the same ranking number the probability that a hydrologic unit contributes surface runoff to the subbasin outlet when surface runoff is generated is calculated by 3 p i j t 1 m i j t m j where p i j t probability that unit i in subbasin j contributes surface runoff to the subbasin outlet when surface runoff is generated and m i j t ranking number of unit i in subbasin j based on its runoff travel time then the joint probability of the hydrologic units whose generated surface runoff reaches the subbasin outlet is calculated by 4 p i j d s t p i j d s p i j t where p i j ds t joint probability of unit i in subbasin j that contributes surface runoff to the subbasin outlet the joint probability distribution of runoff contributions of hydrologic units is illustrated in fig 2c without considering the spatial distribution of depressions obtained by eq 2 squares in fig 2c the probability of a hydrologic unit contributing surface runoff to the subbasin outlet gradually decreases with the increase of the maximum depression storage due to the impact of the spatial distribution of hydrologic units the joint probabilities of runoff contributions of hydrologic units fluctuate at different maximum depression storages even at the same maximum depression storage and are less than or equal to the probability that only considers depression storage once the joint probabilities of runoff contributions of hydrologic units are obtained the subbasin level runoff contributions can be described that is when the surface runoff generated from any particular hydrologic unit reaches the subbasin outlet the outlet ca consists of all hydrologic units that have a joint probability greater than or equal to that of this particular unit thus the ca is coupled with the joint probability distribution to describe the probability of subbasin level runoff contributions fig 2d the joint probability decreases as the ca increases and reaches its minimum when the entire subbasin contributes surface runoff to the outlet 2 3 3 determination of intrinsic property curves to facilitate the simulation of the depression storage controlled surface runoff generation fig 1 the surface topographic analysis tracks the intrinsic changing patterns of fully filled depression storage and connected areas during depression filling for each depressional time area zone specifically a constant net water input is uniformly applied to a subbasin and the depression storage of a hydrotopographic unit is updated after each incremental application by 5 d s i z j k min d s i z j k 1 a i z j i j k m d s i z j where ds i z j k depression storage of unit i in depressional time area zone z of subbasin j at time step k l3 a i z j area of unit i in depressional time area zone z of subbasin j l2 i j k depth of net water input of subbasin j at time step k l and mds i z j maximum depression storage of unit i in depressional time area zone z of subbasin j l3 when the depression storage of a unit reaches its maximum value this unit is then recognized as a part of the connected areas of the depressional time area zone as discussed previously a subbasin is divided into depressional time area zones for transferring surface runoff to the subbasin outlet thus the connected area and the fully filled depression storage fds for each depressional time area zone are calculated respectively after each incremental application of the net water input by 6 c n a z j k i 0 m j a i z j μ i k 7 f s z j k i 0 m j m d s i z j μ i k where cna z j k connected area of depressional time area zone z in subbasin j at time step k l3 μ i k filling spilling index of unit i at time step k if unit i is fully filled at time step k μ i k 1 otherwise μ i k 0 and fs z j k fully filled depression storage of depressional time area zone z in subbasin j at time step k l3 fig 2e shows the connected area versus cumulative net water input and the fds versus cumulative net water input for depressional time area zone 1 of the subbasin in which the connected area and fds are normalized by the area and total depression storage of the zone respectively 2 4 modeling of threshold controlled overland flow dynamics 2 4 1 variable connected area based surface runoff generation once the intrinsic characteristics dominated by the spatially distributed depression storages are detected by the surface topographic analysis procedure the md vca model uses a variable connected area based surface runoff generation approach to determine the connected area depression storage and generated surface runoff of the subbasin through a time loop for a rainfall event figs 1 and 2 at each time step the scs curve number method is used to simulate the subbasin level rainfall excess which then fills depressions and or becomes surface runoff to model surface runoff generation the connected areas of all depressional time area zones fig 2f are first simulated based on the cumulative rainfall excess and the corresponding intrinsic changing pattern of the connected areas fig 2e for each depressional time area zone during a rainfall event the connected area at a time step is kept the same as that in the previous time step if rainfall excess is zero otherwise the connected area expands until reaching the entire zone area the surface runoff generated from the connected area of a depressional time area zone fig 2g can be calculated by 8 r z j k z o n e p j k a z j k c n a ds z j k c n a ds 0 z j k c n a where r z j k z o n e surface runoff generated from depressional time area zone z in subbasin j at time step k l3 p j k depth of rainfall excess of subbasin j at time step k l a z j k c n a connected area of zone z in subbasin j at time step k l2 and ds 0 z j k c n a and ds z j k c n a depression storages of the contributing area of depressional time area zone z in subbasin j at the beginning and end of time step k respectively l3 if a partially filled area of a time area zone exists i e the connected area is less than the entire area of the depressional time area zone all rainfall excess of this area since the beginning of the rainfall event is trapped in depressions and the depression storage of this area is given by 9 ds z j k p f a c p j k a z j z o n e a z j k c n a where ds j k p f a depression storage of the partially filled area of depressional time area zone z in subbasin j at time step k l3 cp j k cumulative depth of rainfall excess of subbasin j at time step k l a z j z o n e area of depressional time area zone z in subbasin j l2 then the total connected area and depression storage of the subbasin can be respectively expressed as 10 a j k c n a z 1 n a z j k c n a 11 ds z j k s u b z 1 n ds z j k c n a ds z j k p f a where a j k c n a connected area of subbasin j at time step k l2 and ds z j k s u b depression storage of subbasin j at time step k l3 2 4 2 variable contributing area based surface runoff routing once surface runoff is generated from the connected areas of a subbasin it is then transferred to the subbasin outlet within some time scales thus based on the depressional time area zone scheme a new variable ca based surface runoff routing technique is developed in the md vca model to transfer the surface runoff generated from each depressional time area zone to the subbasin outlet figs 1 and 2 since the time interval of the time area isochrones is the same as that of simulation the outlet ca at each time step fig 2h is calculated by using the linear superposition of the connected areas of depressional time area zones for example the ca at time step 1 is equal to the connected areas of depressional time area zone 1 at time step 1 and the ca at time step 2 consists of the connected areas of depressional time area zone 1 at time step 2 and the connected areas of depressional time area zone 2 at time step 1 thus the outlet ca can be expressed as 12 a j k c a z 1 t a z j k z 1 c n a where a j k c a ca of subbasin j at time step k l2 then the surface runoff from the outlet ca fig 2i is given by 13 r j k c a z 1 t r z j k z 1 z o n e where r j k c a surface runoff generated from the ca of subbasin j at time step k l3 the surface runoff generated from the ca is subject to the detention due to depression filling and the attenuation is simulated by using a linear reservoir equation in this study fig 2 thus the direct runoff hydrograph at the subbasin outlet fig 2j can be determined by 14 q j k 2 δ t 2 k δ t r j k c a 2 k δ t 2 k δ t q j k 1 where q j k surface runoff reaching the subbasin outlet at time step k l3 k storage coefficient and r j k c a average surface runoff from the ca of subbasin j at time step k l3 in addition based on the relationship between the joint probability distribution of runoff contribution and the subbasin ca fig 2d the probability of occurrence of the ca or outlet discharge is calculated fig 2k 2 5 study area and input data the md vca model was tested through an application to the upper portion of the upper sheyenne river watershed a depression dominated watershed within the prairie pothole region ppr of north dakota fig 3 shows the geographical location the final outlet i e a usgs gaging station 05055300 channel network and land use of the selected watershed according to the national land cover database nlcd 2011 the watershed area 4 619 7 km2 is mostly covered by cultivated crops 49 5 and hay 20 and 13 8 of watershed area is dominated by open water and wetlands in addition to the land use and land cover data the dem soil type meteorological and hydrological data are also required for the md vca model in this study the 10 m resolution dem downloaded from the usgs national map tnm was used to divide the watershed into subbasins and determine surface topographic parameters e g maximum depression storages of for each subbasin the state soil geographic statsgo2 dataset provided the soil data for the selected watershed and the composite curve numbers for all subbasins were then calculated based on the land use and soil data precipitation data at an hourly interval were obtained from a precipitation station latitude 48 167 n longitude 99 648 w fig 3 of the north dakota agriculture weather network ndawn the observed discharge data at the watershed outlet which were used to validate the md vca model were downloaded from the usgs national water information system 2 6 evaluation of model performance the performance of the md vca model was evaluated by using three rainfall events event 1 6 26 2009 18 00 7 2 2009 8 00 event 2 9 20 2019 18 00 9 24 2019 0 00 and event 3 6 13 2017 4 00 6 18 2017 10 00 which have different features such as magnitudes and durations specifically rainfall event 1 was used for calibration and the two other events were used for model verification with the presence of long antecedent dry periods the initial baseflow values of these three events were set to the observed discharges at the beginning of the corresponding rainfall events a set of parameters were selected for the model calibration and their initial values were estimated for example the surface storage coefficient k was determined by the ratio of the discharge at the inflection point on the falling limb of the observed hydrograph to the derivative of discharge with respect to time clark 1945 usace hec 2000 for the purpose of sensitivity analysis a dimensionless sensitivity index ficklin et al 2012 lenhart et al 2002 was used in this study 15 i o 2 o 1 o 0 2 δ p p 0 where i sensitivity index δp percentage adjustment of the parameter and o 0 o 1 and o 2 model output values with parameter values at p 0 p 0 δp and p 0 δp respectively this sensitivity index varies between and and a larger absolute value of the index indicates that the parameter has a stronger impact on the model output in this sensitivity analysis the model output was specified as the mean of the simulated outlet discharges during a rainfall event and the percentage for the parameter adjustment was set to 10 after the sensitivity analysis manual trial and error calibration was performed eventually the simulated and observed discharges at the watershed outlet were compared and four statistical parameters nash sutcliffe efficiency nse percent bias pbias ratio of the root mean square error to the standard deviation of the observed data rsr and coefficient of determination r 2 were used to quantitatively evaluate the performance of the md vca model they are mathematically expressed as 16 n s e 1 k 1 n q k o b s q k s i m 2 k 1 n q k o b s q o b s 2 17 p b i a s k 1 n q k s i m q k o b s k 1 n q o b s k 100 18 r s r k 1 n q k o b s q k s i m 2 k 1 n q k o b s q o b s 2 19 r 2 k 1 n q k o b s q o b s q k s i m q s i m k 1 n q k o b s q o b s 2 k 1 n q k s i m q s i m 2 2 where n total number of discharge observations q k o b s observed outlet discharge at time step k l3 t q k s i m simulated outlet discharge at time step k l3 t q o b s mean of the observed outlet discharges l3 t and q s i m mean of the simulated outlet discharges l3 t in addition to account for the model uncertainty the reliability of the modeling of connected areas and cas was evaluated by using a statistical method specifically the rainfall excess at the end of each rainfall event was calculated and used to analyze the filling spilling conditions of depressions based on which the connected areas and cas at the end of each rainfall event were determined then according to the central limit theorem clt the normal statistic was used to calculate the limits of the proportions of the connected and contributing areas at a level of 95 confidence which were referred to as the 95 confidence intervals of the connected areas and cas afterwards the connected areas and cas simulated by the md vca model were compared with the corresponding 95 confidence intervals to demonstrate the accuracy of the simulation results 2 7 modeling scenarios the influence of spatially distributed depressions was discussed through three modeling scenarios the first scenario s1 only accounted for the influence of depressions on surface runoff generation and assumed that the generated surface runoff contributed to the corresponding subbasin outlet when it was generated thus this scenario was performed by setting only one depressional time area zone per subbasin in the md vca model then the surface runoff generation of a subbasin was simulated by tracking the connected areas and the depression storage of the depressional time area zone i e the subbasin and the generated surface runoff was routed to the subbasin outlet by using a linear reservoir function the second scenario s2 was set up to simulate the influence of spatial distribution of generated surface runoff on surface runoff routing without considering depressions in this scenario storage capacities of depressions were set to zero and thus each subbasin consisted of a number of cbus and special pbus to deal with the spatial distribution of generated surface runoff each subbasin was divided into many depressionless time area zones with a 1 h time interval of isochrones the depressionless time area zones have the same ranges and areas as the corresponding depressional time area zones when depressions are taken into consideration which is because a depressional time area zone is designed to contain all pbus and cbus that have the potential to contribute surface runoff to the subbasin outlet during the same time interval then the generated surface runoff of each depressionless time area zone was routed to subbasin outlet based on the time area zone scheme and a linear reservoir function in the md vca model in the third scenario s3 the md vca model was used to simulate the influence of spatially distributed depressions on surface runoff generation and routing processes in this scenario a subbasin was divided into many depressional time area zones with a 1 h time interval of isochrones connected areas and surface runoff during a rainfall were tracked for each depressional time area zone and the generated surface runoff was routed to the subbasin outlet based on the depressional time area zone scheme and a linear reservoir function 3 results and analyses 3 1 topographic characteristics depression storages and their spatial distribution are the two impact factors of catchment response considered in this study fig 3 shows the delineated 12 subbasins of the watershed and table 1 lists the areas of pbus and cbus and the total maximum depression storages for all subbasins for a better understanding of the topographic characteristics of all subbasins fig 4 displays the distributions of maximum depression storages and runoff travel times from hydrologic units channel ending points of cbus or depression thresholds of pbus to the corresponding subbasin outlets for all subbasins specifically the maximum depression storages of all subbasins are right skewed distributed indicating that subbasins contain numerous depressions with smaller depression storages that can be quickly fully filled to generate surface runoff during the early stage of a rainfall event the runoff travel times from hydrologic units to the corresponding subbasin outlets are approximately normally distributed and the median travel time varies among subbasins which can be attributed to their distinct characteristics e g sizes and shapes the hydrologic units with a shorter travel time to the corresponding subbasin outlet have a higher probability to make runoff contributions when surface runoff is generated fig 5 shows the joint probability of the normalized ca for all subbasins under the influence of both depression storages and their spatial distribution the hydrologic units with smaller depression storages and runoff travel time to the corresponding subbasin outlet have higher probabilities to become part of the outlet ca and the joint probability of outlet ca decreases as the expansion of outlet ca and reaches its minimum when all hydrologic units contribute surface runoff to the subbasin outlet to account for the impact of the spatial distribution of depression storages on subbasin runoff contribution each subbasin was also divided into a number of depressional time area zones the time interval of the isochrones is one hour table 1 lists the number of depressional time area zones for all subbasins and fig 6 shows the spatial distribution of depressional time area zones of subbasin 1 as an example fig 7 a illustrates the percentages of non depressional areas and depressional areas for all depressional time area zones of subbasin 1 as shown in fig 7a the non depressional areas only dominate a small portion of the area of the corresponding zones and they generate surface runoff and are connected once rainfall satisfies the initial abstraction i e canopy interception and infiltration before surface runoff initiates as rainfall continues more depressions are filled and start to generate surface runoff and thus the connected areas of each zone expand to further investigate the generation of surface runoff fig 7b and 7c show the formation of connected areas and the increasing patterns of fully filled depression storages for two representative depressional time area zones zone 4 and zone 8 respectively the percentages of connected areas of both zones are larger than zero before water input i e rainfall is applied representing the ratio of non depressional area of each zone at the beginning of depression filling the connected areas and fully filled depression storage of both zones increase rapidly since many smaller depressions are quickly filled and generate surface runoff as the net water input increases the connected areas and fully filled depression storage of zone 4 exhibit a stepwise increasing pattern while the connected areas and fully filled depression storage of zone 8 only show slight stepwise changes which can be attributed to the properties of pbus i e surface areas and depression storages of pbus of both zones for example there are several pbus with large depression storages in zone 4 which take a longer time to be fully filled and become a part of connected areas of the zone resulting in stepwise changes since there are a large number of depressions with different maximum depression storages in the subbasins and each depressional zone only dominates a small portion of the subbasin area fig 7a the stepwise changes in the connected areas and fully filled depression storage of the subbasin fig 7d are not as obvious as those in fig 7b and c the ratios of connected areas and the fully filled depression storages of both zones eventually reach 1 0 when all depressions are fully filled the intrinsic changing patterns of the connected areas and fully filled depression storage facilitate the simulation of ca formation and surface runoff generation for real rainfall events 3 2 evaluation of the md vca model table 2 lists the calibrated parameters and their sensitivity index values according to ficklin et al 2012 the parameter is considered as extremely high sensitivity when i 1 0 and high sensitivity when 0 2 i 1 0 and a negative value of the index indicates an inverse impact of the parameter on the outlet discharge thus the five selected parameters including curve number cn baseflow recession constant r initial abstraction coefficient ia surface storage coefficient k and baseflow threshold value bt were adjusted in the calibration process in addition the lag time used for channel routing which was not significantly sensitive was also taken into consideration in the calibration fig 8 shows the comparisons between the observed and simulated hydrographs at the watershed outlet for the calibration event event 1 and two validation events event 2 and event 3 table 3 shows the percent deviations of the simulated peak flows and peak times comparing to the observed data i e observed value simulated value 100 observed value calculated for the three rainfall events as shown in fig 8 and table 3 the peak flows and peak times of the hydrographs for events 1 and 2 reasonably match the observed data and the general shapes of the hydrographs for the three events are well characterized a slight underestimate at the beginning of the hydrograph for event 1 fig 8a and a slightly early peak flow for event 3 fig 8c can be observed which may be due to the spatial and temporal variations of the rainfall in addition the nse rsr pbias and r 2 were used to quantitatively describe the agreement between the simulated and observed hydrographs table 3 also lists the values of nse rsr pbias and r2 for the calibration and validation events according to moriasi et al 2007 2015 simulations are considered as very good if the nse is greater than 0 75 the rsr is less than 0 5 and pbias is less than 10 in this study the calculated nse rsr and pbias fall into the recommended ranges indicating a very good agreement between the simulated and observed discharges in addition the r 2 values 0 95 for the calibration and validation events also indicated a good performance of the md vca model in addition to the comparisons of simulated and observed hydrographs at the watershed outlet the simulated connected areas cas depression storage and surface runoff of the calibration and validation events were analyzed for all subbasins fig 9 a c show the simulated connected areas and cas as well as their corresponding 95 uncertainty bands i e the 95 confidence intervals obtained from the statistical analysis for all subbasins at the end of the calibration and validation events respectively the connected areas and cas simulated by the md vca model fall into their 95 uncertainty bands demonstrating the reliability of the simulation results of the md vca model this also demonstrates the capability of the md vca model in mimicking the threshold controlled runoff generation and propagation processes under the influence of model uncertainty fig 9d f display the simulated depression storage and the total amount of generated surface runoff of all subbasins i e water depth over the corresponding subbasin area at the end of the calibration and validation events respectively for the three storm events more rainfall excess water was trapped by depressions and less water became surface runoff as shown in fig 9 the simulated connected areas depression storage and surface runoff vary among all subbasins which can be attributed to the differences in the land use soil type and surface topographic characteristics e g areas and depression storages of hydrologic units of the subbasins the cas of all subbasins also differ due to the dissimilar spatial distributions of hydrologic units for example at the end of the calibration event 18 6 of the connected areas of subbasin 5 became the subbasin ca while only 4 of the connected areas of subbasin 1 became the subbasin ca this is because subbasin 5 has more hydrologic units with smaller depression storages which are located close to the subbasin outlet 3 3 hydrologic effects of depression storages and their spatial distribution to demonstrate the role of the spatially distributed depressions in surface runoff generation and routing simulation results from the three modeling scenarios were compared fig 10 shows the connected areas cas and surface runoff simulated for the three modeling scenarios for subbasin 1 during the storm event 2 that occurred in september 2019 as an example specifically figs 10a and 10c demonstrate the influence of depressions on the surface runoff generation by illustrating the simulated connected areas normalized by the subbasin area and the total amount of surface runoff generated from the subbasin connected areas in the three modeling scenarios during the storm event as aforementioned s1 and s3 considered the influence of the real delineated depression storages on surface runoff generation while s2 ignored the influence of depression storages thus in s2 all rainfall excess became surface runoff and the entire subbasin was connected in s1 and s3 however all rainfall excess of cbus became surface runoff and the rainfall excess of pbus flowed to depressions and surface runoff initiated only when there were fully filled depressions in this way a part of rainfall excess of this subbasin was trapped in depressions and the remaining water became surface runoff whereas the subbasin connected areas consisted of the non depressional area i e areas of cbus as well as the cas and ponding areas of the fully filled depressions thus as shown in fig 10a the normalized connected areas of the subbasin in s2 at each time step reached 1 0 while the normalized connected areas of the subbasin in s1 and s3 at each time step were equal and less than 1 0 similarly s1 and s3 had the same amount of total surface runoff at each time step and the values of generated surface runoff in s1 and s3 were less than that simulated in s2 fig 10c which was the total amount of rainfall excess at this time step fig 10b and d respectively show the subbasin ca normalized by the subbasin area and the surface runoff generated on the ca for the three modeling scenarios during the storm event revealing the influence of the spatial distribution of depression storages on surface runoff routing as discussed previously in s1 all surface runoff contributed to the subbasin outlet when it was generated and the subbasin connected areas became the outlet ca of the subbasin while s2 and s3 accounted for the spatial distributions of generated surface runoff thus the simulated outlet ca at each time step in s1 was equal to the corresponding subbasin connected areas fig 10a and b and the runoff contribution at each time step also equaled the total amount of surface runoff generated at the same time step fig 10c and d in this case the runoff contributions in the subbasin varied with the rainfall intensities and there was no runoff contribution to the subbasin outlet if there was no rainfall excess fig 10d in s2 and s3 however the spatial distribution of surface runoff was considered thus the outlet cas and the runoff contributions of the subbasin varied with both rainfall intensity which affected the connected areas or the generated surface runoff of each time area zone and runoff travel time for example at 9 20 2019 23 00 in s2 and s3 i e the first time step with rainfall excess only the connected areas of time area zone 1 became the outlet cas very small values fig 10b the surface runoff generated from time area zone 1 made runoff contributions to the subbasin outlet at this time step also very small values fig 10d and the surface runoff generated from other time area zones contributed to the subbasin outlet at the following time steps depending on the runoff travel time of each time area zone therefore after 9 21 2019 7 00 i e the end of the storm the outlet ca and runoff contribution in s2 and s3 were still larger than zero fig 10b and d in addition the outlet cas and runoff contributions simulated in s2 were greater than those in s3 for the same time steps fig 10b and d which can be attributed to the fact that s2 did not consider depression storage the time when the outlet ca and runoff contributions reached their peak values were the same as that in s2 and s3 which can be attributed to the same scheme of time area zones i e the depressionless and depressional time area zones in s2 and s3 had the same spatial range from the comparisons of s1 s3 fig 10 it can be found that there are significant differences in the subbasin connected areas and the total amounts of surface runoff simulated with and without considering depressions demonstrating the importance of considering depressions in the modeling of surface runoff generation in addition there are notable discrepancies in the formation of the outlet ca and the timing and quantity of runoff contributions with and without considering the spatial distribution of depressions even though the subbasin connected areas and the total amount of surface runoff are the same in both scenarios i e s1 and s3 thus it is essential to incorporate both depression storages and their spatial distribution in the simulation of rainfall runoff dominated by surface depressions the significance of considering spatially distributed depression storages was also emphasized by fossey et al 2016 and wu et al 2020 who explored the influence of the geographic location of wetlands on streamflow for different watersheds 4 discussion in this study the md vca model was developed to incorporate the influences of both depression storages and their spatial distribution into the simulation of depression oriented rainfall runoff processes in recent years some hydrologic models have been developed in which the depression filling spilling processes are simulated and the influences of the spatial distribution of depressions and generated surface runoff are simplified for example zeng et al 2020 simulated the overland flow filling spilling dynamics for depression dominated areas using a series of probability distribution functions of depression storages arnold et al 1998 used the surface runoff lag method to route the generated overland flow to the subbasin main channel in the lag method an empirical equation is used to control the amount of surface runoff reaching channels for each time step however these models cannot provide the details on the progressive formation of the outlet ca and the propagation processes of surface runoff the md vca model developed in this study simulates the influence of spatially distributed depressions on surface runoff propagation by introducing the depressional time area zone scheme and developing the ca based surface runoff routing algorithm resultantly in addition to mimicking the generation of surface runoff the md vca model also is able track the formation of ca and runoff contributions in the transfer of generated surface runoff to the subbasin outlet fig 9 to demonstrate the performance of the md vca the simulation results were analyzed in the modeling for real rainfall events the simulated outlet hydrographs showed a good agreement with the observed data fig 8 indicating the ability of the md vca model in mimicking the threshold controlled overland flow dynamics under different rainfall conditions in addition to the outlet discharges the simulated connected areas and cas for all subbasins for the calibration and validation events were verified by comparing to their 95 confidence intervals fig 9 the comparisons demonstrated the reasonability and accuracy of the simulation results in addition during the calibration and validation events occurred in summer and fall months a large portion of rainfall excess was trapped by depressions and the simulated outlet cas of all subbasins only reached 20 39 of their areas during those rainfall events fig 9 which are consistent with the findings in other studies for depression dominated areas zeng et al 2020 evenson et al 2015 incorporating the spatial distribution of depression storages is one of the unique features of the md vca model the joint probability distribution associated with depression storages and their spatial distributions is determined in the md vca model to quantify the likelihood of occurrences of the outlet ca the md vca model implements simulations of connected areas and surface runoff for each depressional time area zone of subbasins particularly a new depressional time area zone scheme and a unique variable ca based surface runoff routing technique are utilized for surface runoff routing in the md vca model the depressional time area zones of each subbasin are determined based on the flow length and flow travel time from pbus and cbus to the corresponding subbasin outlet note that the md vca model is able to provide the progressive formation of the outlet ca and the propagation of surface runoff which can be attributed to the proposed depressional time area zone scheme however if the spatial distribution of depression storages is not considered the connected areas at each time step have to be lumped together and viewed as the outlet ca in this case the spatial and temporal variations of the outlet ca cannot be well characterized and the movement of surface runoff to the subbasin outlet cannot be properly simulated since the spatial distribution of the generated surface runoff is not considered 5 summary and conclusions the md vca model was developed in this study to simulate the influences of spatially distributed depression storages on catchment responses during rainfall events and the associated threshold behavior in the md vca model a depression dominated subbasin was divided into a number of depressional time area zones based on the spatial distribution of depression storages each of which contained all hydrologic units that had the possibilities to contribute surface runoff to the corresponding subbasin outlet during the same time interval then the intrinsic changing patterns of connected areas and fully filled depression storage were detected for each depressional time area zone which was used to determine the connected areas and the total amount of surface runoff of the subbasin during the surface runoff generation processes under rainfall events the formation of the outlet ca and the contribution of surface runoff to the subbasin outlet were tracked by a newly developed ca based surface runoff routing technique in addition the joint probability distributions associated with both depression storages and their spatial distribution were created to describe the likelihood of occurrences of the outlet cas and runoff contributions the performance of the md vca model was evaluated through the application to a depression dominated watershed in the prairie pothole region of north dakota the simulated hydrographs at the watershed outlet reasonably matched the observed ones and the four statistical parameters rsr nse pbias and r 2 also demonstrated a good agreement between the simulated and observed hydrographs and the ability of the md vca model in simulating the surface runoff initiation and propagation under the influence of spatially distributed depression storages in addition to the discharges at the watershed outlet the simulated connected areas and cas for all subbasins were also verified by their 95 confidence ranges the influences of depression storages and their spatial distribution on surface runoff generation and routing were revealed in this study through three modeling scenarios s1 considering the influence of depression storages only s2 ignoring depression storages and only simulating the influence of spatially distributed surface runoff and s3 taking both depression storages and their spatial distributions into account without considering depression storages the connected areas and the total amounts of generated surface runoff were overestimated leading to overestimations of the outlet cas and runoff contributions to outlets without considering the spatial distribution of depression storages the connected areas and total amounts of generated surface runoff were captured however the model failed to track the outlet cas and characterize the timing and quantity of runoff contributions this modeling study helps bridge the gap in simulating the formation of ca and the contribution of surface runoff under the influence of spatially distributed depression storages credit authorship contribution statement lan zeng conceptualization methodology software validation formal analysis investigation writing original draft writing review editing visualization xuefeng chu conceptualization methodology formal analysis investigation writing review editing data curtion supervision project administration funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this material is based upon work supported by the national science foundation under grant no nsf epscor award iia 1355466 the north dakota water resources research institute also provided partial financial support in the form of a graduate fellowship for the first author 
328,foam injection is an effective method for modifying gas mobility in subsurface flow applications making it ideal for environmental remediation applications remediation of contaminated soils aquifers of nonaqueous phase liquids using foamed surfactant solutions is a viable option but a predictive foam model is needed that is flexible to the addition of more accurate physical descriptions such a model is essential to ensure successful operations in soil remediation applications the objective of this paper is to develop a full physics mechanistic transient foam flow model and integrate it into the multiphysics modular ad gprs framework automatic differentiation general purpose research simulator we chose ad gprs because it allows rapid prototyping and addition of complex physics and modeling strategies we develop the model ground up from pore scale observations and implement a new flowing foam fraction constitutive relation that depends on the local pressure gradient local permeability and flowing bubble density our model predicts the two flow regimes commonly observed in steady state pressure gradient measurements the low quality regime and the high quality regime additionally the model is used to match transient experimental results of homogeneous and slightly heterogeneous cores with a wide range of permeability values the implementation of this model within ad gprs allows testing of ideas and modeling strategies as well as inclusion of more complex physics or foam generation kinetics keywords foam mechanistic population balance foam with napl foam heterogeneous 1 introduction subsurface aquifer soil contaminant remediation is a complicated process due to complexities introduced by fluid fluid interactions fluid rock interactions or rock heterogeneities the contaminants could exist as free phase in the pore space of soils and or bind tightly on the soil rock surfaces wang and mulligan 2004 the density of some nonaqueous phase liquids napls is larger than the resident water hence they migrate through the vadose zone to the bottom of the aquifer where permeability is small and often become immobilized hirasaki et al 1997a additionally the insolubility and or immiscibility of these napls in water and the large interfacial tension between them and water makes traditional remediation efforts such as pump and treat difficult oolman et al 1995 pope and wade 1995 hirasaki et al 1997a wang and mulligan 2004 an effective method that has the potential to alleviate some of the challenges faced during remediation processes is to foam an injection gas foams also are relevant to the storage of carbon dioxide in saline formations eide et al 2020 the discontinuity of the gaseous phase in foams gives them the advantage to make remediation efforts more effective because bubbles are separated by thin liquid films foams develop a much larger apparent viscosity when compared to gas alone hirasaki and lawson 1985 moreover the discontinuity of the gaseous phase helps foams to immobilize or trap more gas in the high permeability layers tang and kovscek 2006 that ultimately diverts some of the gas to the lower permeability layers therefore foams can enhance the sweep of aqueous surfactant solutions by diverting injectants to low permeability zones during aquifer soil remediation jobs diversion ultimately aids greater drainage of contaminants out of the affected zones hirasaki et al 1997b hirasaki et al 1997b demonstrated that foam is successful for environmental aquifer restoration purposes an aquifer that had been contaminated with dense napl was chosen to examine the effectiveness of using foamed surfactant solution to clean the napl from the aquifer the chosen aquifer was characterized as heterogeneous because it contained thief zones that had permeability upwards of 100 darcy and relatively tighter zones that had permeabilities that were 40 darcy or smaller the napl was trapped primarily in the tighter zones towards the bottom of the aquifer and therefore presented an additional challenge for other remediation processes such as pump and treat or surfactant based remediation processes that are incapable of diverting liquids to the lower permeability zones hirasaki et al 1997a foam removed essentially all of the napl from the swept pore volume as evidenced by tracer tests that measured a reduced average napl saturation of only 0 03 hirasaki et al 1997a szafranski et al 1998 there were key observations and important lessons learned from the demonstration project first air injection pressure to produce foam was low and the pressure gradient necessary for foam propagation was kept small second foam diverted the surfactant solution from high permeability zones to the low permeability zones this implies that trapped foam behaved differently in thief zones than in tight zones moreover it seems that as foam flowed initially into the thief zones it built a relatively larger pressure gradient and hence the trapped foam fraction was large the trapped foam fraction is the fraction of the gas saturation that is immobile in a time averaged sense on the other hand lower permeability zones received diverted foam that was described as frothy indicating that the trapped foam fraction was less when pressure gradients were small mamun et al 2002 confirmed that the two flow regimes commonly observed in petroleum applications i e low quality and high quality regimes also apply in this low pressure gradient that is applicable to environmental remediation in related work tang and kovscek 2006 performed an extensive study of the variables influencing the trapped foam fraction in porous media and reported that the portion of the foam that flows differs with pressure gradient permeability and bubble density moreover pore network analysis shows that trapped foam varies with bubble density and pressure gradient kharabaf and yortsos 1998 almajid and kovscek 2019 we anticipate that in low pressure gradient environments such as those encountered during environmental remediation that the trapped foam fraction is sensitive to the evolved pressure gradient due to foam therefore an improved foam flow model that takes this effect into account is needed for more accurate predictive modeling of foam remediation processes additionally a framework that allows easy and rapid prototyping of improved physical descriptions that is capable of interweaving upscaled pore scale phenomena is needed the framework should be extensible to multidimensions such that predictive models are readily extended to aquifer scale 10 s of meter scale because it is a significant undertaking to move to aquifer scale directly this paper focuses attention on the meter scale core scale where we develop and verify a mechanistic population balance model that is built on physical pore scale observations while the destabilizing mechanisms of foam in the presence of napl are important this paper takes a first step and develops a model that works in the absence of napl or at residual saturation of napls accordingly the objective of this paper is to develop and implement a transient foam model that is based on physical observations and is capable of predicting foam transport behavior in porous media there are several approaches to model foam flow empirical local equilibrium or full physics models have been used in the past ma et al 2015 we seek a framework that is easy to manipulate to include more complex physics to describe foam flow to that end we use the modular multiphysics framework of the automatic differentiation general purpose research simulator ad gprs and add a foam simulation option the mechanistic population balance model developed is compared to data from new transient foam flooding experiments homogeneous foam flow experimental data in the case of constant and transient surfactant were used to validate the simulator predictions kovscek et al 1995 importantly heterogeneous low and high quality transient foam flow experiments were conducted and matched to a reasonable degree of accuracy the model developed differs from current foam models and especially from that of chen et al 2010 in three ways first our numerical implementation uses the automatic differentiation capability of ad gprs to compute the jacobian thereby permitting more rapid prototyping of equations second the foam model used in chen et al 2010 is a local equilibrium foam model their model assumes instantaneous equilibrium between generation and coalescence as soon as gas and enough surfactant are present in the same grid block based on this instantaneous equilibrium the bubble density is computed algebraically therefore the bubble density is not a primary variable that is solved for numerically our current work does not assume local equilibrium hence we solve the full physics model and include the bubble density as a primary variable along with pressure and water saturation as outlined in later sections third the flowing foam fraction x f used in the work of chen et al 2010 is based only on the bubble density in the current work however x f depends on the pressure gradient permeability and bubble density this paper proceeds by presenting a pore scale depiction of how foam resides in the pore space a quick overview of the experimental set up and procedures is outlined next then the theoretical foundation of the mechanistic population balance model is presented model predictions of experimental cases are presented next finally we lay out a concise summary of this paper s findings 2 foam in porous media foam in porous media is defined as a dispersion of gas in a continuous liquid phase fig 1 illustrates a pore level description of foam ettinger and radke 1992 gillis and radke 1990 chambers and radke 1991 kovscek and radke 1994 in the figure the grains are indicated by the beige colored circles flowing gas by clear white shading trapped gas by the hatched white shading and the wetting aqueous phase by blue coloring for better illustration of the distribution of fluids the highly schematic figure has three pore spaces of increasing dimension the pore space increases in size from the lower part of the figure moving upwards in accordance with capillarity the wetting aqueous phase fills the smallest pore space and attaches to the grains by thin liquid films note the liquid phase is continuous therefore the liquid relative permeability in the presence of foam remains unchanged because only small amounts of liquid travel in the thin surfactant stabilized films called lamellae that subdivide the gas phase during transient foam flow lamellae resist flow and hence bubble trains favor flowing in the least resistive pathways almajid and kovscek 2019 the flowing bubbles convect convec through the largest sized pore space while trapped bubbles block gas flow through the intermediate sized pore space in the presence of strong foam gas mobility is modified by a combination of an increase in the gas apparent viscosity and an increase in the trapped gas fraction because of the drag that flowing bubbles experience due to the pore walls and pore constrictions falls et al 1989 and the constant rearrangement of the gas liquid interfacial area of flowing bubbles due to viscous and capillary forces hirasaki and lawson 1985 the viscosity of the foamed gas appears much larger than that of unfoamed gas significantly gas tracer experiments gillis and radke 1990 friedmann et al 1991 tang and kovscek 2006 infer fractions of trapped gas from effluent data that range from 85 to 99 depending on the flow conditions moreover direct x ray computed tomography ct estimates in situ trapped gas fractions between 36 and 70 and shows how the tracer concentration is not uniform along the length of the core nguyen et al 2009 lamellae that separate the flowing and trapped gas bubbles evolve from a balance between generation and coalescence mechanisms that depend on the flow conditions as well as the initial and boundary conditions of the porous medium additionally generation and coalescence mechanisms set the final distribution of the bubbles in the porous medium ettinger and radke 1992 almajid et al 2019 several microvisual studies confirmed that lamellae are generated by one of three generation mechanisms snap off lamellae division or leave behind chambers and radke 1991 almajid and kovscek 2016 in the absence of oil coalescence is mainly dominated by the capillary suction mechanism gas diffusion between neighboring bubbles could also destroy the generated lamellae but its effect is minimal in comparison to the capillary suction mechanism the details of these generation and coalescence mechanisms are laid out elsewhere almajid and kovscek 2016 3 experimental set up and procedures foam flow experiments were performed using a heterogeneous sandstone core the core was placed inside an aluminum core holder that allows the acquisition of images necessary to track the water saturation during the experiments fig 2 shows a schematic of the experimental set up the core was chosen such that its length 60 cm is much greater than the entry length for net foam generation 12 cm chen 2009 fig 3 a shows a reconstructed porosity image of the core we used the average porosity of the core was about 25 fig 3b but there were tighter laminations cutting through it at an angle that made it slightly heterogeneous fig 3a pressure and water saturation profiles were tracked during the experiments aqueous phase saturation was obtained using an x ray ct scanner the experimental set up and procedures are identical to those outlined elsewhere and the reader is encouraged to consult these citations for more details about the experiments almajid 2019 almajid et al 2019 during a transient foam flow experiment the vacuumed clean core is initially flushed with co 2 and then fully saturated with 0 5 wt nacl brine backpressure is released and reapplied periodically to assist in removing all gas from the system the brine is then replaced with a foamer solution the foamer solution contains 0 5 wt active stepan bioterg as40 sodium c14 16 olefin sulfonate in 0 5 wt nacl brine the gas and the foamer solution are coinjected into the core that is at a prespecified backpressure 100 psi the injected density of bubbles is therefore zero because no foam pregenerator is used gas injection is achieved with a mass flow controller brooks mass flow controller 5850tr while liquid injection is controlled by a dual piston quizix pump quizix qx5000 gas darcy velocities range from 0 14 m day to 1 90 m day at the prescribed backpressure while liquid darcy velocities range from 0 14 m day to 0 21 m day 4 model development 4 1 governing equations mass conservation equations for the gaseous and aqueous phases are the basis of the model aziz and settari 1979 for a general phase k we write 1 t ϕ ρ k s k ρ k u k q k where the subscript k is interchanged by g or w to denote the gaseous or the aqueous phases respectively t is time ϕ is the porosity of the porous medium ρ k is the mass density of the phase s k is the saturation of the phase u k is the superficial or darcy velocity and q k is the source sink term that is used to apply boundary conditions in the case where the porous medium is not initially saturated with surfactant solution an additional mass conservation equation on the surfactant is necessary the surfactant transports as an adsorbing tracer with the water phase and therefore its conservation equation is written as 2 t ϕ c s s w γ u w c s q s where c s is the number or molar surfactant concentration in the aqueous phase γ is the amount of surfactant adsorption on the rock in units of moles per void volume q s is the source sink term for surfactant in units of moles volume time almajid and kovscek 2019 use a pore network model to show the importance of bubble texture to the foam displacement process in their work as the probability that a pore throat contains a lamella resulting from snap off f s o increases so does the average bubble texture density accordingly the displacement becomes more uniform and travels like a shock through the network other foam models also emphasize the importance of texture to the accurate prediction of foam flow through the porous medium hirasaki and lawson 1985 friedmann and jensen 1986 falls et al 1988 1989 ettinger and radke 1992 ransohoff and radke 1988 chambers and radke 1991 kovscek et al 1995 chen et al 2010 for these reasons and to model foam flow mechanistically it is necessary to add one more conservation equation describing the flowing bubble density n f written as patzek 1988 3 t ϕ s g f n f s g t n t u g n f ϕ s g r g r c q b where subscripts f and t denote the flowing and trapped gas respectively n f is the flowing bubble density per unit volume of flowing gas and n t is the trapped bubble density per unit volume of the trapped gas the first term on the left hand side of eq 3 is the accumulation of the bubbles the second term represents the convection of foam bubbles that travel with the gas phase on the right hand side of eq 3 the first term represents the net generation of foam the rate of generation r g and the rate of coalescence r c are represented on a per volume of gas basis lastly q b is a source sink term of bubbles in the case where no pregenerated bubbles are injected into the porous medium this source term is set to zero in its presence or absence the kinetic expressions of r g and r c determine the evolved bubble texture in the porous medium the importance of these kinetic expressions is appreciated by recognizing that at steady state away from any sources or sinks the evolved bubble texture is determined by equating the rate of generation to the rate of coalescence r g r c ettinger and radke 1992 furthermore the assumption of bubble texture being set entirely by the instantaneous balance between generation and coalescence was used to model transient foam flow successfully chen et al 2010 in this paper we use the full physics model without any relaxation of the problem we present a relaxed form of the full physics model elsewhere and show that it is possible to obtain meaningful results with less computational cost almajid 2019 the final residual form of the conservation equations described in this section that were incorporated into ad gprs are shown in the supporting information 4 2 rates of generation and coalescence there are several mechanisms by which foam is generated in a porous medium ransohoff and radke 1988 observed snap off lamella division and leave behind liontas et al 2013 add two more neighbor induced pinch off mechanisms all of these mechanisms lead to stronger foam generation with some dominating over others almajid and kovscek 2019 compute the likelihood of snap off in a cubic pore network to be larger than the likelihood of lamella division for this particular reason we chose snap off to be the mechanism responsible for foam generation in our model consonant with the above arguments the rate of generation formulation we chose is similar to that used by chen et al 2010 and is applicable for the low and high quality regimes 4 r g k 1 0 1 n f n ω v w v g 1 3 where k 1 0 is taken to be a constant ω is a constant determining the shape of inverse proportionality of foam germination sites to preexisting gas bubbles throughout this paper we use a value of 3 for ω based on the sensitivity study of chen 2009 as opposed to germination sites where lamellae are created termination sites are where lamellae break and thus bubbles coalesce the stability of a lamella depends on the surfactant properties surfactant concentration liquid saturation rock type and how fast the lamella traverses the pore khatib et al 1988 jimenez and radke 1989 kahrobaei and farajzadeh 2019 surfactant rock interactions dictate how much surfactant is lost from the lamella to the rock surface additionally the balance between the lamella s disjoining pressure and the capillary pressure of the surrounding region determines the rate by which coalescence happens rapidly moving lamellae do not have enough time to heal as they stretch and rupture is inevitable jimenez and radke 1989 based on these arguments we choose the rate of coalescence to be 5 r c k 1 0 p c p c c s p c 2 v g n f where k 1 0 is taken to be a constant moreover the work of aronson et al 1994 on various aqueous surfactants suggests the following functional form for p c as a function of c s 6 p c c s p c m a x tanh c s c s where p c m a x is a maximum value for p c and c s is a reference surfactant concentration for strong net foam generation the capillary pressure of the porous medium is approximated using the leverett j function kovscek et al 1995 the exact formula is defined in the supporting information 4 3 phase mobilities in addition to the governing equations and the foam kinetic expressions for generation and coalescence additional flow rate relationships for the wetting and the gas phases are necessary to complete the model we use the multiphase extension of darcy s law 7 u k k k r k μ k p k ρ k g z where k is the absolute permeability k r k is the relative permeability of phase k μ k is its viscosity p k is its pressure ρ k is its density g the gravitational constant and z is the depth accordingly with the pore scale depiction of foam shown in fig 1 the wetting liquid mobility is unaffected by the presence of the bubbles as the wetting phase maintains continuity in the pore space thus the viscosity of the wetting liquid is set to be a constant and a modified corey type is used to describe its relative permeability as described by kovscek et al 1995 the discontinuity of the gas phase suggests that its mobility is affected by the presence of foam thus the gas mobility is adjusted accordingly the gas viscosity is hence replaced with an apparent gas viscosity in the presence of foam flowing bubbles do not exhibit a newtonian viscosity because they lay down thin lubracting films on the pore walls as they move in the pore space hirasaki and lawson 1985 therefore the apparent viscosity of gas in the presence of foam μ f is written as 8 μ f μ g α n f v g 1 3 where μ g is the gas viscosity in the absence of foam α is a constant of proportionality that is dependent on the surfactant system n f is the bubble density and v g is the local interstitial gas velocity the apparent viscosity increases with bubble density but it is shear thinning at constant bubble densities because not all of the gas phase is mobile during foam flow gillis and radke 1990 cohen et al 1997 kharabaf and yortsos 1998 tang and kovscek 2006 almajid and kovscek 2019 the gas phase relative permeability in the presence of foam must be adjusted similar to the wetting liquid phase we adopt a modified corey type relative permeability description but adjust the gas phase relative permeability to account for the fraction of the gas phase actually flowing the flowing foam fraction x f s g f s g is introduced as done by previous studies kovscek et al 1995 the expression of x f used is that proposed by tang and kovscek 2006 that is written as 9 x f ψ p g n f k 1 2 g where ψ is a constant of proportionality and g is a percolation exponent that is taken to be equal to 0 4 because our core represents a large 3d pore network the expression captures the fact that the flowing foam fraction increases when the applied pressure gradient increases the bubble density decreases or the permeability decreases kharabaf and yortsos 1997 chen et al 2006 almajid and kovscek 2019 for a more detailed descriptions about the specific forms used for the relative permeability of the gaseous and aqueous phase as well as the origin of the form that the flowing foam fraction takes please refer to the supporting information 5 numerical implementation 5 1 ad gprs the mechanistic foam model developed above was implemented in the automatic differentiation general purpose research simulator ad gprs framework voskov and tchelepi 2012 zhou et al 2011 zhou 2012 the governing equations in residual form that were included in ad gprs are included in the supporting information ad gprs provides flexibility to solve multiphysics problems as it has a general implicit coupling framework rin et al 2017 the framework is modular where a multiphysics problem can be split into subproblems each subproblem represents particular physics for this paper we add a foam subproblem and use the already implemented flow problem adding a foam subproblem to such a modular and flexible framework is a first step to modeling more complex physical processes that include foam physics such as steam foams that might employ the flow foam and thermal subproblems we use the fully implicit method fim for time approximation because we found the foam problem to be very stiff we use a sequential structure as a type of nonlinear preconditioner to compute a good initial guess for the fully coupled fully implicit multiphysics problem this approach is similar to the work done by wong et al 2018 2019 for a geothermal energy problem our general scheme is to first solve for flow alone while holding the bubble density constant then use that solution as a constant and solve for bubble density alone and finally combine the two problems to find the final solution in other words we solved for each individual physics subproblem separately first to provide a good initial guess to the fully coupled problem a key challenge to the implementation of the mechanistic foam model is the computation of the apparent gas phase viscosity when foam is present hirasaki and lawson 1985 the apparent gas viscosity when foam is present depends on the flowing bubble density as well as the interstitial velocity of the gas both of these variables change with space and time as the pressure as well as the saturation of each block evolves the following section describes in detail our implementation of this important variable in our model 5 2 computing apparent gas viscosity we use the bisection method to compute the effective viscosity as a function of the primary variables pressure water saturation and bubble density so a bisection method is applied to solve f p s w n f μ f 0 for a given p s w n f 10 f p s w n f μ f m f p n f s w μ f μ f 0 where m f p n f s w μ f is the procedure to compute the effective gas phase viscosity as a function of primary variables and can be obtained by rearranging eq 8 to get 11 m f p n f s w μ f μ g α n f μ g 1 3 v g 1 3 μ f 1 3 where μ g is the unfoamed gas viscosity and v g is the gas velocity that is still not yet affected by the bubbles at the current timestep thus the function to compute the gas phase effective viscosity as a function of our primary variables is written as 12 μ f value p s w n f bisection f p s w n f μ f m f the procedure to compute the effective gas viscosity as a function of primary variables once we have the viscosity the next step is to ensure that the derivatives are properly computed this ensures that the derivatives of all the other properties dependent on the foam viscosity are correctly computed the derivative computation is based on the inverse function theorem which states that 13 d y d x d f d y 1 d f d x where 14 f x y 0 because we are interested in computing the effective gas viscosity derivative with respect to the primary variables we apply the inverse function theorem for y μ f x p s w n f and 15 f x y f p s w n f m p s w n f μ f 0 once the adscalar effective gas viscosity value and derivatives are computed for each cell all other properties are evaluated the derivatives all have the correct derivatives because they are all a function of our primary variables p n f s w 5 3 numerical issues with rate of coalescence one issue that could hinder the efficiency of the numerical implementation is the quadratic form of the rate of coalescence especially during high gas fractional flow runs as the gas saturation of a grid block increases so does its p c if p c is the same as or greater than p c we run into numerical problems in the case that they are equal there exists a discontinuity we resolve this by adding a small value ϵ 1 10 8 to ensure that the difference is always positive on the other hand when p c is greater than p c then the rate of coalescence decreases because of the quadratic form used which is incorrect this same numerical issue makes optimizing foam simulators to deduce parameters difficult ma et al 2019 to resolve this issue one could multiply the constant of rate of coalescence by a very large number to indicate chaotic coalescence that is typically reported in this regime another method to overcome this issue is to linearly extrapolate the value that one would get if the difference between p c and p c was ϵ based on their actual difference we opted to use the first method because it simplifies the implementation more work needs to be done to study as well as to resolve this issue in the future 6 model predictions of foam flow foam flow in porous media is extremely nonlinear and has many interesting behaviors in this section we test the validity of the full physics mechanistic model developed here to predict both the steady state and transient behaviors we qualitatively show that the model is able to predict the two flow regimes that are usually observed in steady state foam flow experiments osterloh and jante 1992 then we use several transient experimental data to compare and validate our model results for transient foam flow we compare our quantitative predictions to four experimental datasets two of the data sets are homogeneous cases from the literature and are included in the supporting information the experimental data in the supporting information validate the model results against cases when the initial saturation condition is altered on the other hand the experimental data in the main paper compares the model s predictions versus results from the heterogeneous sandstone core sample fig 3 6 1 steady state foam flow steady state pressure gradient measurements during foam flow yield interesting behavior osterloh and jante 1992 measure the steady state pressure gradient along a sandpack and report that foam exhibits two regimes depending on the injection conditions they report the steady state pressure gradient to be independent of gas velocities and dependent on liquid velocities when the gas fractional flow is large they coin this regime as the high quality regime on the other hand their data show that the steady state pressure gradient is independent of the imposed liquid velocity but dependent on the gas velocity when the gas fractional flow is low they coin this regime as the low quality regime the transition between the two regimes occurs in their experiments at a foam quality gas fractional flow of 94 alvarez et al 2001 further confirm these results by measuring steady state pressure gradient along cores and sandpacks with various surfactant formulations and gases they conclude that the transition foam quality depends on the permeability of the porous medium cores exhibit smaller transition foam qualities than sandpacks this is attributed to differences in the details of capillary pressure between consolidated and unconsolidated rocks fig 4 presents our model predictions of the steady state pressure gradients as a function of the gas and the liquid darcy velocities using the parameters in table s1 in the supporting information when the liquid darcy velocity is small less than 1 0 m day the pressure gradient is insensitive to the gas velocity but sensitive to the liquid velocity conversely when the liquid velocity is large the pressure gradient increases with the gas velocity but remains constant with respect to the liquid velocity this seems to be in accordance with the experimental observations in the literature osterloh and jante 1992 alvarez et al 2001 the steady state average bubble density predictions are plotted as a function of the gas and liquid darcy velocities in fig 5 the average bubble density depends on the liquid velocity when the liquid velocity is small the average bubble density appears insensitive to the gas velocity as the liquid velocity increases the average bubble density is a function of both gas and liquid velocities larger liquid velocities make the foam bubbles finer but larger gas velocities coarsen the foam bubbles this seems to be in agreement with theory as the generation rate varies linearly with the liquid velocity persoff et al 1989 kovscek and radke 1996 almajid 2019 while the coalescence rate increases as the gas velocity increases jimenez and radke 1989 6 2 transient foam flow heterogeneous f g 50 in this example we model a low quality regime experiment in the slightly heterogeneous core sample gas and foamer solution are injected into a core that is completely saturated with foamer solution gas is injected at a darcy velocity of 0 14 m day relative to the applied backpressure of 0 7 mpa while the foamer solution is injected at a darcy velocity of 0 14 m day the resultant foam quality is 50 at the core exit because ct images of the core show two lower permeability layers sandwiching a higher permeability layer we expect the behavior of the pressure saturation and bubble density to be different than the validation cases in the supplementary information table 1 lists the model parameters for the heterogeneous cases measured aqueous phase saturation profiles as well as measured pressure profiles indicate that the foam generated in the experiment was relatively weak until a dimensionless distance of 0 3 having observed this we alter the flowing foam fraction in our model and make it discontinuous along the core more precisely we assign each grid a value of the constant of proportionality ψ in eq 9 in this example the assigned ψ value is larger in the first 0 3 dimensionless distance of the core having a larger constant of proportionality indicates that foam flows more easily and hence weak foam is formed even after foam generation fig 6 plots the aqueous phase saturation profiles at varying dimensionless times at the first three dimensionless times i e 0 11 pvi 0 33 pvi and 0 66 pvi the gas moves into the core quickly draining a considerable amount of liquid 0 4 units of saturation measured data show that strong foam starts to generate somewhere around dimensionless distance of 0 3 predicted aqueous phase saturation tracks the advancement of the two dimensionless times well the model predicts slower front advancement at 0 66 pvi and 1 10 pvi this is attributed to the higher perm layer that the free gas preferentially flows through if it is not foamed even considering this heterogeneity in the core the model tracks roughly the position of the measured front additionally steady state is reached after 2 20 pvi that is matched to a good agreement by the numerical model a foam s strength is not only gauged by the resultant aqueous phase saturation but also by the resultant pressure gradient it builds fig 7 shows the predicted and measured pressure profiles at four dimensionless times consonant with the saturation measurement the model predicts early times and steady state well and mismatches the times in between again the mismatch between the model and experiments at intermediate times is attributed to the heterogeneity of the core that affects the amount of gas in the core and the manner by which it resides inside the core from a closer examination of figs 6 and 7 at a time of 1 10 pvi we observe that the amount of gas estimated by the model is larger than the actual experiment the discontinuous drop in s w occurs at a greater distance from the inlet experimentally than numerically second after the discontinuous drop in s w the gas travels in a piston like fashion meaning that it develops a strong foam but that is not necessarily the case experimentally the experiment shows a smeared front rather than a piston like displacement consistent with the subcore scale heterogeneity in fig 3 a a more piston like displacement translates into a larger pressure drop developed hence there is difference in intermediate pressure drop estimations by the model the larger aqueous phase saturation and the shallow pressure gradient in the weak foam region i e dimensionless distance less than 0 3 are achieved by using a discontinuous value of the constant of proportionality in the expression for flowing foam fraction as discussed above the flowing foam fraction that results from the model is shown in fig 8 larger values of x f lead to larger gas relative permeability values and hence larger aqueous phase saturation for instance fig 6 shows a larger aqueous phase saturation near the inlet of 0 8 that corresponds to a flowing foam fraction of 1 0 when the foam is weak we expect easier mobilization and vice versa the local minima in the x f profile at the displacement front coincide with the regions of large net foam where the bubble density is large fig 9 one interesting feature about fig 9 is the average bubble density upstream and downstream of the x f discontinuity almajid 2019 uses the pore network of almajid and kovscek 2019 to capture qualitatively the aqueous phase saturation behavior observed in these experiments using two snap off probabilities f s o upstream and downstream of the discontinuity a larger f s o downstream of the discontinuity represented stronger foam and finer texture the reported mean bubble densities in fig 9 upstream and downstream of the discontinuity n f and n f seem to be in agreement with our qualitative results the average bubble density is larger where foam is stronger 6 3 transient foam flow heterogeneous f g 90 in this example we model a high quality regime experiment in the same slightly heterogeneous core sample gas and foamer solution are injected into a core that is completely saturated with foamer solution gas is injected at a darcy velocity of 1 90 m day relative to the applied backpressure of 0 7 mpa while the foamer solution is injected at a darcy velocity of 0 21 m day the resultant foam quality is 90 at the core exit the main difference between this experiment and the previous one is only the gas fractional flow applied fig 10 plots the aqueous phase saturation against dimensionless distance for four dimensionless times the model tracks the front well in the early times i e less than 1 28 pvi the wetting liquid saturation is underestimated by the model in the weak foam region i e below dimensionless distance of 0 5 interestingly the front is less smeared after passing the discontinuity at dimensionless distance of 0 5 the pressure profiles of three dimensionless times are shown in fig 11 similar to the 50 quality foam the model approximates the pressure profile well in the very early times and at steady state but mismatches the experimental results in intermediate times after the foam front passes the discontinuity in the core the mismatch in this case is less severe because the drier foam presents less smearing in the foam front in intermediate times again we use a discontinuous flowing foam fraction to capture the large aqueous phase saturation and shallow pressure gradients upstream of the discontinuity in dimensionless distance 0 5 the resulting flowing foam fraction is shown in fig 12 the local minima in x f at the displacement front are due to the region of large net foam generation near the foam front as explained in the previous case we also report the average bubble density in this case before and after the discontinuity in fig 13 the general magnitude of predicted foam texture 200 to 300 mm 3 agrees with the measurements of chen et al 2010 under similar surfactant and flow rate conditions in a permeable sandstone observations made with respect to fig 9 also generally apply here the more interesting observation is that mean values of texture are smaller than the low quality foam case for example the texture at a position of x d 0 8 is roughly 260 mm 3 at a time of 2 2 pvi in fig 8 whereas the texture is 180 mm 3 at the same position at 2 1 pvi in fig 12 this makes sense because wetter foams have larger relative liquid velocities as compared to the gas liquid velocities and hence snap off could occur more frequently there 7 discussion the gas flux is not necessarily a monotone function during these simulations this discussion explores monotonicity the prospects for scale up of the simulation approach to larger domains and the extension of the model to include napl destablizating effects for a discussion of 2d simulations and the role of heterogeneity the reader is encouraged to check the s4 in the supporting information that justifies the use of 1d simulations as an approximation of the 2d results 7 1 monotonicity in the gas flux function to examine the presence of or the absence of monotonicity in the gas flux function during foam flow we plot the gas flux function given different mobility ratios the gas fractional flow is defined as 16 f g 1 1 λ w λ g where f g is the gas fractional flow λ w k r w μ w is the aqueous phase mobility in units of c p 1 and λ g k r g μ g is the gaseous phase mobility in units of c p 1 we alter the gaseous phase mobility by increasing the gas effective viscosity μ g f in the presence of foam for example a strong foam exhibits an effective viscosity that is 100 times larger than the gas viscosity and follows the gas fractional flow curve indicated by μ g f μ g 10 2 in fig 14 thus foam strength increases going from the right most curve to the left most curve in that figure we first use the parameters and the simulation results of the transient foam flow experiment in heterogeneous media presaturated with surfactant solution in the supporting information to examine the monotonicity of the gas flux function in a porous medium that is initially fully saturated with foamer solution we use a 1d model that has 240 grid blocks and refine timesteps to be able to see the saturation changes with time fig 15 shows the gas fractional flow that is exhibited by two grid blocks grid block 5 is close to the injection point while grid block 20 is away from the injection point and represents how gas fractional flow behaves in the rest of the grid blocks away from the injection point fig 15 a and b show that the gas flux function is nonmonotonic when the porous medium is initially fully saturated with foamer solution initially the gas flux function increases monotonically as the bubble density increases and as the water saturation decreases because of the relative increase in the bubble density caused by net foam generation at the foam front the gas flux function is nonmonotonic as evidenced by the zoomed in figures the water saturation as well as the gas fractional flow decrease as the bubble density profiles equilibrate the next case we examine is transient foam flow where the homogeneous porous medium is initially fully saturated with brine similarly we show two grid blocks one close to the injection side fig 16 a and another one that is farther from the injection point and represents how the rest of the grid blocks behave fig 16b the model in this case is much finer than the previous case and has 1920 grid blocks the gas flux function in this case increases monotonically examining the flowing bubble density profiles of this case shows no region of net foam generation and a relative increase of foam texture similar to the previous case this observations seems to explain why the gas flux function is monotonic in this case as a point of clarification there is no region of net foam generation because surfactant concentration is also transient in this case hence the surfactant concentration is relatively small at the foam front and conditions are not favorable for net foam generation when comparing the previous two cases we observe another interesting behavior when the porous medium is initially fully saturated with foamer solution the gas mobility is modified such that the gas flux function follows only one curve out of all possible curves except at late times where nonmonotonic behavior is witnessed the situation is distinctly different and more interesting in the case where the porous medium is initially fully saturated with brine i e contains no surfactant in that case the initial free gas ineffectively drains the medium to a large water saturation while the gas mobility follows that of an unfoamed gas however as soon as the gas is foamed the gas flux function jumps from one gas fractional flow curve to the other as the gas mobility is further modified the zoomed in figures of both fig 16a and b show the intersection points between the respective grid block gas fractional flow curve and the theoretical curves 7 2 scale up to larger domains the mechanistic model developed and tested above performs well against experimental data especially in homogeneous rock samples at the meter scale there are however some challenges that need to be circumvented before we take this further to 10 s of meter scale we believe that heterogeneity plays a vital and pivotal role in any foam flow process and is inevitable to be encountered in nature therefore a key step that needs further investigation is the integration of the heterogeneity found in rock samples this may be accomplished by introducing rock classes in which each class has differing multiphase properties i e relative permeability curves capillary pressure curves apparent viscosity behavior a version of this idea for fractured media was implemented by pancharoen et al 2012 their work needs to be extended to include capillary pressure and relative permeability differences in each rock type and then be implemented within our model to move to larger length scales the other issue is the implementation of wells in larger scale domains this is due to the fact that foam viscosity exhibits a shear thinning behavior a 10 s of meter scale simulation could have grid blocks that are much larger than the wellbore hence a large difference between the bottom hole flowing pressure and a grid block s pressure is expected a significant difference in pressure would lead to shear thinning behavior between the wellbore and the grid block li et al 2006 circumvent this issue through adding a negative viscosity dependent apparent viscosity skin factor in the well model to accommodate the non newtonian effects between the wellbore and the grid block 7 3 extension to include napl destabilizing effects the model in this paper captures foam generation and coalescence mechanisms in the absence of mobile napls hence the model is suitable for applications in the absence of napls or in the presence of residual napls it is however well documented that napls have a destabilizing effect on foam farajzadeh et al 2012 almajid and kovscek 2016 schramm and novosad 1990 manlowe and radke 1990 myers 1999 myers and radke 2000 bergeron et al 1993 because the population balance model is implemented in the ad gprs framework it allows a straightforward extension to include such effects for instance following the work of myers and radke 2000 a third reaction term that accounts for coalescence of foam due to the presence of napls r c o needs to be included the term that myers and radke 2000 included in their work was a function of the bubble flux v g n f the napl saturation s o the imposed capillary pressure on a pseudoemulsion film p c p f and the rupture capillary pressure of a pseudoemulsion film p c p f additionally p c p f depended on the napl water and gas water capillary pressures and the surface tensions of the system all of these parameters once defined can be implemented to compute a third reaction term that adds the destabilizing effect of napls on foam 8 concluding remarks this paper illustrates the development and implementation of a mechanistic full physics population balance transient foam flow model into the multiphysics modular ad gprs simulator the automatic differentiation capabilities significantly aid efforts to prototype expressions of physical phenomena we propose a new flowing foam fraction function and extend an existing population balance model to predict experimental data in homogeneous and slightly heterogeneous porous media in the homogeneous cases we find that the model predicts well the measured data the model predictions were compared to constant surfactant concentration as well as transient surfactant flow cases qualitatively our model predicts the two foam flow regimes usually reported in steady state foam flow literature low quality foam and high quality foam in the slightly heterogeneous cases our model tracks well the location of the saturation front and the steady state and the early time pressure and aqueous phase saturation profiles the intermediate time pressure profiles are overestimated due to the difference of the amount of gas in the system between the model and experiments the heterogeneous cases were predicted with a discontinuous flowing foam fraction function that is consonant with the steady state results of almajid et al 2019 the model also predicts larger averaged bubble density when the gas has passed the high low permeability barrier confirming results of previous pore network studies kharabaf and yortsos 1998 almajid and kovscek 2019 furthermore we have shown that foam flow in porous media is nonmonotonic when the porous medium is presaturated with surfactant solution and that the nonmonotonicity disappears when it is initially brine saturated implementation of the foam population balance model in the modular multiphysics ad gprs simulator provides a first step into modeling more complex and larger scale processes in the future for instance mechanistic foam flow simulation at aquifer scale may be conducted additionally steamfoam physics could be captured theoretically if the flow foam and thermal subproblems are coupled in ad gprs credit authorship contribution statement muhammad m almajid methodology investigation software writing original draft zhi yang wong methodology investigation software anthony r kovscek conceptualization methodology supervision funding acquisition writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements m m almajid acknowledges a graduate fellowship from saudi aramco additional financial support was provided by the supri a industrial affiliates supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2021 103877 appendix a supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 supplementary data s2 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s2 
328,foam injection is an effective method for modifying gas mobility in subsurface flow applications making it ideal for environmental remediation applications remediation of contaminated soils aquifers of nonaqueous phase liquids using foamed surfactant solutions is a viable option but a predictive foam model is needed that is flexible to the addition of more accurate physical descriptions such a model is essential to ensure successful operations in soil remediation applications the objective of this paper is to develop a full physics mechanistic transient foam flow model and integrate it into the multiphysics modular ad gprs framework automatic differentiation general purpose research simulator we chose ad gprs because it allows rapid prototyping and addition of complex physics and modeling strategies we develop the model ground up from pore scale observations and implement a new flowing foam fraction constitutive relation that depends on the local pressure gradient local permeability and flowing bubble density our model predicts the two flow regimes commonly observed in steady state pressure gradient measurements the low quality regime and the high quality regime additionally the model is used to match transient experimental results of homogeneous and slightly heterogeneous cores with a wide range of permeability values the implementation of this model within ad gprs allows testing of ideas and modeling strategies as well as inclusion of more complex physics or foam generation kinetics keywords foam mechanistic population balance foam with napl foam heterogeneous 1 introduction subsurface aquifer soil contaminant remediation is a complicated process due to complexities introduced by fluid fluid interactions fluid rock interactions or rock heterogeneities the contaminants could exist as free phase in the pore space of soils and or bind tightly on the soil rock surfaces wang and mulligan 2004 the density of some nonaqueous phase liquids napls is larger than the resident water hence they migrate through the vadose zone to the bottom of the aquifer where permeability is small and often become immobilized hirasaki et al 1997a additionally the insolubility and or immiscibility of these napls in water and the large interfacial tension between them and water makes traditional remediation efforts such as pump and treat difficult oolman et al 1995 pope and wade 1995 hirasaki et al 1997a wang and mulligan 2004 an effective method that has the potential to alleviate some of the challenges faced during remediation processes is to foam an injection gas foams also are relevant to the storage of carbon dioxide in saline formations eide et al 2020 the discontinuity of the gaseous phase in foams gives them the advantage to make remediation efforts more effective because bubbles are separated by thin liquid films foams develop a much larger apparent viscosity when compared to gas alone hirasaki and lawson 1985 moreover the discontinuity of the gaseous phase helps foams to immobilize or trap more gas in the high permeability layers tang and kovscek 2006 that ultimately diverts some of the gas to the lower permeability layers therefore foams can enhance the sweep of aqueous surfactant solutions by diverting injectants to low permeability zones during aquifer soil remediation jobs diversion ultimately aids greater drainage of contaminants out of the affected zones hirasaki et al 1997b hirasaki et al 1997b demonstrated that foam is successful for environmental aquifer restoration purposes an aquifer that had been contaminated with dense napl was chosen to examine the effectiveness of using foamed surfactant solution to clean the napl from the aquifer the chosen aquifer was characterized as heterogeneous because it contained thief zones that had permeability upwards of 100 darcy and relatively tighter zones that had permeabilities that were 40 darcy or smaller the napl was trapped primarily in the tighter zones towards the bottom of the aquifer and therefore presented an additional challenge for other remediation processes such as pump and treat or surfactant based remediation processes that are incapable of diverting liquids to the lower permeability zones hirasaki et al 1997a foam removed essentially all of the napl from the swept pore volume as evidenced by tracer tests that measured a reduced average napl saturation of only 0 03 hirasaki et al 1997a szafranski et al 1998 there were key observations and important lessons learned from the demonstration project first air injection pressure to produce foam was low and the pressure gradient necessary for foam propagation was kept small second foam diverted the surfactant solution from high permeability zones to the low permeability zones this implies that trapped foam behaved differently in thief zones than in tight zones moreover it seems that as foam flowed initially into the thief zones it built a relatively larger pressure gradient and hence the trapped foam fraction was large the trapped foam fraction is the fraction of the gas saturation that is immobile in a time averaged sense on the other hand lower permeability zones received diverted foam that was described as frothy indicating that the trapped foam fraction was less when pressure gradients were small mamun et al 2002 confirmed that the two flow regimes commonly observed in petroleum applications i e low quality and high quality regimes also apply in this low pressure gradient that is applicable to environmental remediation in related work tang and kovscek 2006 performed an extensive study of the variables influencing the trapped foam fraction in porous media and reported that the portion of the foam that flows differs with pressure gradient permeability and bubble density moreover pore network analysis shows that trapped foam varies with bubble density and pressure gradient kharabaf and yortsos 1998 almajid and kovscek 2019 we anticipate that in low pressure gradient environments such as those encountered during environmental remediation that the trapped foam fraction is sensitive to the evolved pressure gradient due to foam therefore an improved foam flow model that takes this effect into account is needed for more accurate predictive modeling of foam remediation processes additionally a framework that allows easy and rapid prototyping of improved physical descriptions that is capable of interweaving upscaled pore scale phenomena is needed the framework should be extensible to multidimensions such that predictive models are readily extended to aquifer scale 10 s of meter scale because it is a significant undertaking to move to aquifer scale directly this paper focuses attention on the meter scale core scale where we develop and verify a mechanistic population balance model that is built on physical pore scale observations while the destabilizing mechanisms of foam in the presence of napl are important this paper takes a first step and develops a model that works in the absence of napl or at residual saturation of napls accordingly the objective of this paper is to develop and implement a transient foam model that is based on physical observations and is capable of predicting foam transport behavior in porous media there are several approaches to model foam flow empirical local equilibrium or full physics models have been used in the past ma et al 2015 we seek a framework that is easy to manipulate to include more complex physics to describe foam flow to that end we use the modular multiphysics framework of the automatic differentiation general purpose research simulator ad gprs and add a foam simulation option the mechanistic population balance model developed is compared to data from new transient foam flooding experiments homogeneous foam flow experimental data in the case of constant and transient surfactant were used to validate the simulator predictions kovscek et al 1995 importantly heterogeneous low and high quality transient foam flow experiments were conducted and matched to a reasonable degree of accuracy the model developed differs from current foam models and especially from that of chen et al 2010 in three ways first our numerical implementation uses the automatic differentiation capability of ad gprs to compute the jacobian thereby permitting more rapid prototyping of equations second the foam model used in chen et al 2010 is a local equilibrium foam model their model assumes instantaneous equilibrium between generation and coalescence as soon as gas and enough surfactant are present in the same grid block based on this instantaneous equilibrium the bubble density is computed algebraically therefore the bubble density is not a primary variable that is solved for numerically our current work does not assume local equilibrium hence we solve the full physics model and include the bubble density as a primary variable along with pressure and water saturation as outlined in later sections third the flowing foam fraction x f used in the work of chen et al 2010 is based only on the bubble density in the current work however x f depends on the pressure gradient permeability and bubble density this paper proceeds by presenting a pore scale depiction of how foam resides in the pore space a quick overview of the experimental set up and procedures is outlined next then the theoretical foundation of the mechanistic population balance model is presented model predictions of experimental cases are presented next finally we lay out a concise summary of this paper s findings 2 foam in porous media foam in porous media is defined as a dispersion of gas in a continuous liquid phase fig 1 illustrates a pore level description of foam ettinger and radke 1992 gillis and radke 1990 chambers and radke 1991 kovscek and radke 1994 in the figure the grains are indicated by the beige colored circles flowing gas by clear white shading trapped gas by the hatched white shading and the wetting aqueous phase by blue coloring for better illustration of the distribution of fluids the highly schematic figure has three pore spaces of increasing dimension the pore space increases in size from the lower part of the figure moving upwards in accordance with capillarity the wetting aqueous phase fills the smallest pore space and attaches to the grains by thin liquid films note the liquid phase is continuous therefore the liquid relative permeability in the presence of foam remains unchanged because only small amounts of liquid travel in the thin surfactant stabilized films called lamellae that subdivide the gas phase during transient foam flow lamellae resist flow and hence bubble trains favor flowing in the least resistive pathways almajid and kovscek 2019 the flowing bubbles convect convec through the largest sized pore space while trapped bubbles block gas flow through the intermediate sized pore space in the presence of strong foam gas mobility is modified by a combination of an increase in the gas apparent viscosity and an increase in the trapped gas fraction because of the drag that flowing bubbles experience due to the pore walls and pore constrictions falls et al 1989 and the constant rearrangement of the gas liquid interfacial area of flowing bubbles due to viscous and capillary forces hirasaki and lawson 1985 the viscosity of the foamed gas appears much larger than that of unfoamed gas significantly gas tracer experiments gillis and radke 1990 friedmann et al 1991 tang and kovscek 2006 infer fractions of trapped gas from effluent data that range from 85 to 99 depending on the flow conditions moreover direct x ray computed tomography ct estimates in situ trapped gas fractions between 36 and 70 and shows how the tracer concentration is not uniform along the length of the core nguyen et al 2009 lamellae that separate the flowing and trapped gas bubbles evolve from a balance between generation and coalescence mechanisms that depend on the flow conditions as well as the initial and boundary conditions of the porous medium additionally generation and coalescence mechanisms set the final distribution of the bubbles in the porous medium ettinger and radke 1992 almajid et al 2019 several microvisual studies confirmed that lamellae are generated by one of three generation mechanisms snap off lamellae division or leave behind chambers and radke 1991 almajid and kovscek 2016 in the absence of oil coalescence is mainly dominated by the capillary suction mechanism gas diffusion between neighboring bubbles could also destroy the generated lamellae but its effect is minimal in comparison to the capillary suction mechanism the details of these generation and coalescence mechanisms are laid out elsewhere almajid and kovscek 2016 3 experimental set up and procedures foam flow experiments were performed using a heterogeneous sandstone core the core was placed inside an aluminum core holder that allows the acquisition of images necessary to track the water saturation during the experiments fig 2 shows a schematic of the experimental set up the core was chosen such that its length 60 cm is much greater than the entry length for net foam generation 12 cm chen 2009 fig 3 a shows a reconstructed porosity image of the core we used the average porosity of the core was about 25 fig 3b but there were tighter laminations cutting through it at an angle that made it slightly heterogeneous fig 3a pressure and water saturation profiles were tracked during the experiments aqueous phase saturation was obtained using an x ray ct scanner the experimental set up and procedures are identical to those outlined elsewhere and the reader is encouraged to consult these citations for more details about the experiments almajid 2019 almajid et al 2019 during a transient foam flow experiment the vacuumed clean core is initially flushed with co 2 and then fully saturated with 0 5 wt nacl brine backpressure is released and reapplied periodically to assist in removing all gas from the system the brine is then replaced with a foamer solution the foamer solution contains 0 5 wt active stepan bioterg as40 sodium c14 16 olefin sulfonate in 0 5 wt nacl brine the gas and the foamer solution are coinjected into the core that is at a prespecified backpressure 100 psi the injected density of bubbles is therefore zero because no foam pregenerator is used gas injection is achieved with a mass flow controller brooks mass flow controller 5850tr while liquid injection is controlled by a dual piston quizix pump quizix qx5000 gas darcy velocities range from 0 14 m day to 1 90 m day at the prescribed backpressure while liquid darcy velocities range from 0 14 m day to 0 21 m day 4 model development 4 1 governing equations mass conservation equations for the gaseous and aqueous phases are the basis of the model aziz and settari 1979 for a general phase k we write 1 t ϕ ρ k s k ρ k u k q k where the subscript k is interchanged by g or w to denote the gaseous or the aqueous phases respectively t is time ϕ is the porosity of the porous medium ρ k is the mass density of the phase s k is the saturation of the phase u k is the superficial or darcy velocity and q k is the source sink term that is used to apply boundary conditions in the case where the porous medium is not initially saturated with surfactant solution an additional mass conservation equation on the surfactant is necessary the surfactant transports as an adsorbing tracer with the water phase and therefore its conservation equation is written as 2 t ϕ c s s w γ u w c s q s where c s is the number or molar surfactant concentration in the aqueous phase γ is the amount of surfactant adsorption on the rock in units of moles per void volume q s is the source sink term for surfactant in units of moles volume time almajid and kovscek 2019 use a pore network model to show the importance of bubble texture to the foam displacement process in their work as the probability that a pore throat contains a lamella resulting from snap off f s o increases so does the average bubble texture density accordingly the displacement becomes more uniform and travels like a shock through the network other foam models also emphasize the importance of texture to the accurate prediction of foam flow through the porous medium hirasaki and lawson 1985 friedmann and jensen 1986 falls et al 1988 1989 ettinger and radke 1992 ransohoff and radke 1988 chambers and radke 1991 kovscek et al 1995 chen et al 2010 for these reasons and to model foam flow mechanistically it is necessary to add one more conservation equation describing the flowing bubble density n f written as patzek 1988 3 t ϕ s g f n f s g t n t u g n f ϕ s g r g r c q b where subscripts f and t denote the flowing and trapped gas respectively n f is the flowing bubble density per unit volume of flowing gas and n t is the trapped bubble density per unit volume of the trapped gas the first term on the left hand side of eq 3 is the accumulation of the bubbles the second term represents the convection of foam bubbles that travel with the gas phase on the right hand side of eq 3 the first term represents the net generation of foam the rate of generation r g and the rate of coalescence r c are represented on a per volume of gas basis lastly q b is a source sink term of bubbles in the case where no pregenerated bubbles are injected into the porous medium this source term is set to zero in its presence or absence the kinetic expressions of r g and r c determine the evolved bubble texture in the porous medium the importance of these kinetic expressions is appreciated by recognizing that at steady state away from any sources or sinks the evolved bubble texture is determined by equating the rate of generation to the rate of coalescence r g r c ettinger and radke 1992 furthermore the assumption of bubble texture being set entirely by the instantaneous balance between generation and coalescence was used to model transient foam flow successfully chen et al 2010 in this paper we use the full physics model without any relaxation of the problem we present a relaxed form of the full physics model elsewhere and show that it is possible to obtain meaningful results with less computational cost almajid 2019 the final residual form of the conservation equations described in this section that were incorporated into ad gprs are shown in the supporting information 4 2 rates of generation and coalescence there are several mechanisms by which foam is generated in a porous medium ransohoff and radke 1988 observed snap off lamella division and leave behind liontas et al 2013 add two more neighbor induced pinch off mechanisms all of these mechanisms lead to stronger foam generation with some dominating over others almajid and kovscek 2019 compute the likelihood of snap off in a cubic pore network to be larger than the likelihood of lamella division for this particular reason we chose snap off to be the mechanism responsible for foam generation in our model consonant with the above arguments the rate of generation formulation we chose is similar to that used by chen et al 2010 and is applicable for the low and high quality regimes 4 r g k 1 0 1 n f n ω v w v g 1 3 where k 1 0 is taken to be a constant ω is a constant determining the shape of inverse proportionality of foam germination sites to preexisting gas bubbles throughout this paper we use a value of 3 for ω based on the sensitivity study of chen 2009 as opposed to germination sites where lamellae are created termination sites are where lamellae break and thus bubbles coalesce the stability of a lamella depends on the surfactant properties surfactant concentration liquid saturation rock type and how fast the lamella traverses the pore khatib et al 1988 jimenez and radke 1989 kahrobaei and farajzadeh 2019 surfactant rock interactions dictate how much surfactant is lost from the lamella to the rock surface additionally the balance between the lamella s disjoining pressure and the capillary pressure of the surrounding region determines the rate by which coalescence happens rapidly moving lamellae do not have enough time to heal as they stretch and rupture is inevitable jimenez and radke 1989 based on these arguments we choose the rate of coalescence to be 5 r c k 1 0 p c p c c s p c 2 v g n f where k 1 0 is taken to be a constant moreover the work of aronson et al 1994 on various aqueous surfactants suggests the following functional form for p c as a function of c s 6 p c c s p c m a x tanh c s c s where p c m a x is a maximum value for p c and c s is a reference surfactant concentration for strong net foam generation the capillary pressure of the porous medium is approximated using the leverett j function kovscek et al 1995 the exact formula is defined in the supporting information 4 3 phase mobilities in addition to the governing equations and the foam kinetic expressions for generation and coalescence additional flow rate relationships for the wetting and the gas phases are necessary to complete the model we use the multiphase extension of darcy s law 7 u k k k r k μ k p k ρ k g z where k is the absolute permeability k r k is the relative permeability of phase k μ k is its viscosity p k is its pressure ρ k is its density g the gravitational constant and z is the depth accordingly with the pore scale depiction of foam shown in fig 1 the wetting liquid mobility is unaffected by the presence of the bubbles as the wetting phase maintains continuity in the pore space thus the viscosity of the wetting liquid is set to be a constant and a modified corey type is used to describe its relative permeability as described by kovscek et al 1995 the discontinuity of the gas phase suggests that its mobility is affected by the presence of foam thus the gas mobility is adjusted accordingly the gas viscosity is hence replaced with an apparent gas viscosity in the presence of foam flowing bubbles do not exhibit a newtonian viscosity because they lay down thin lubracting films on the pore walls as they move in the pore space hirasaki and lawson 1985 therefore the apparent viscosity of gas in the presence of foam μ f is written as 8 μ f μ g α n f v g 1 3 where μ g is the gas viscosity in the absence of foam α is a constant of proportionality that is dependent on the surfactant system n f is the bubble density and v g is the local interstitial gas velocity the apparent viscosity increases with bubble density but it is shear thinning at constant bubble densities because not all of the gas phase is mobile during foam flow gillis and radke 1990 cohen et al 1997 kharabaf and yortsos 1998 tang and kovscek 2006 almajid and kovscek 2019 the gas phase relative permeability in the presence of foam must be adjusted similar to the wetting liquid phase we adopt a modified corey type relative permeability description but adjust the gas phase relative permeability to account for the fraction of the gas phase actually flowing the flowing foam fraction x f s g f s g is introduced as done by previous studies kovscek et al 1995 the expression of x f used is that proposed by tang and kovscek 2006 that is written as 9 x f ψ p g n f k 1 2 g where ψ is a constant of proportionality and g is a percolation exponent that is taken to be equal to 0 4 because our core represents a large 3d pore network the expression captures the fact that the flowing foam fraction increases when the applied pressure gradient increases the bubble density decreases or the permeability decreases kharabaf and yortsos 1997 chen et al 2006 almajid and kovscek 2019 for a more detailed descriptions about the specific forms used for the relative permeability of the gaseous and aqueous phase as well as the origin of the form that the flowing foam fraction takes please refer to the supporting information 5 numerical implementation 5 1 ad gprs the mechanistic foam model developed above was implemented in the automatic differentiation general purpose research simulator ad gprs framework voskov and tchelepi 2012 zhou et al 2011 zhou 2012 the governing equations in residual form that were included in ad gprs are included in the supporting information ad gprs provides flexibility to solve multiphysics problems as it has a general implicit coupling framework rin et al 2017 the framework is modular where a multiphysics problem can be split into subproblems each subproblem represents particular physics for this paper we add a foam subproblem and use the already implemented flow problem adding a foam subproblem to such a modular and flexible framework is a first step to modeling more complex physical processes that include foam physics such as steam foams that might employ the flow foam and thermal subproblems we use the fully implicit method fim for time approximation because we found the foam problem to be very stiff we use a sequential structure as a type of nonlinear preconditioner to compute a good initial guess for the fully coupled fully implicit multiphysics problem this approach is similar to the work done by wong et al 2018 2019 for a geothermal energy problem our general scheme is to first solve for flow alone while holding the bubble density constant then use that solution as a constant and solve for bubble density alone and finally combine the two problems to find the final solution in other words we solved for each individual physics subproblem separately first to provide a good initial guess to the fully coupled problem a key challenge to the implementation of the mechanistic foam model is the computation of the apparent gas phase viscosity when foam is present hirasaki and lawson 1985 the apparent gas viscosity when foam is present depends on the flowing bubble density as well as the interstitial velocity of the gas both of these variables change with space and time as the pressure as well as the saturation of each block evolves the following section describes in detail our implementation of this important variable in our model 5 2 computing apparent gas viscosity we use the bisection method to compute the effective viscosity as a function of the primary variables pressure water saturation and bubble density so a bisection method is applied to solve f p s w n f μ f 0 for a given p s w n f 10 f p s w n f μ f m f p n f s w μ f μ f 0 where m f p n f s w μ f is the procedure to compute the effective gas phase viscosity as a function of primary variables and can be obtained by rearranging eq 8 to get 11 m f p n f s w μ f μ g α n f μ g 1 3 v g 1 3 μ f 1 3 where μ g is the unfoamed gas viscosity and v g is the gas velocity that is still not yet affected by the bubbles at the current timestep thus the function to compute the gas phase effective viscosity as a function of our primary variables is written as 12 μ f value p s w n f bisection f p s w n f μ f m f the procedure to compute the effective gas viscosity as a function of primary variables once we have the viscosity the next step is to ensure that the derivatives are properly computed this ensures that the derivatives of all the other properties dependent on the foam viscosity are correctly computed the derivative computation is based on the inverse function theorem which states that 13 d y d x d f d y 1 d f d x where 14 f x y 0 because we are interested in computing the effective gas viscosity derivative with respect to the primary variables we apply the inverse function theorem for y μ f x p s w n f and 15 f x y f p s w n f m p s w n f μ f 0 once the adscalar effective gas viscosity value and derivatives are computed for each cell all other properties are evaluated the derivatives all have the correct derivatives because they are all a function of our primary variables p n f s w 5 3 numerical issues with rate of coalescence one issue that could hinder the efficiency of the numerical implementation is the quadratic form of the rate of coalescence especially during high gas fractional flow runs as the gas saturation of a grid block increases so does its p c if p c is the same as or greater than p c we run into numerical problems in the case that they are equal there exists a discontinuity we resolve this by adding a small value ϵ 1 10 8 to ensure that the difference is always positive on the other hand when p c is greater than p c then the rate of coalescence decreases because of the quadratic form used which is incorrect this same numerical issue makes optimizing foam simulators to deduce parameters difficult ma et al 2019 to resolve this issue one could multiply the constant of rate of coalescence by a very large number to indicate chaotic coalescence that is typically reported in this regime another method to overcome this issue is to linearly extrapolate the value that one would get if the difference between p c and p c was ϵ based on their actual difference we opted to use the first method because it simplifies the implementation more work needs to be done to study as well as to resolve this issue in the future 6 model predictions of foam flow foam flow in porous media is extremely nonlinear and has many interesting behaviors in this section we test the validity of the full physics mechanistic model developed here to predict both the steady state and transient behaviors we qualitatively show that the model is able to predict the two flow regimes that are usually observed in steady state foam flow experiments osterloh and jante 1992 then we use several transient experimental data to compare and validate our model results for transient foam flow we compare our quantitative predictions to four experimental datasets two of the data sets are homogeneous cases from the literature and are included in the supporting information the experimental data in the supporting information validate the model results against cases when the initial saturation condition is altered on the other hand the experimental data in the main paper compares the model s predictions versus results from the heterogeneous sandstone core sample fig 3 6 1 steady state foam flow steady state pressure gradient measurements during foam flow yield interesting behavior osterloh and jante 1992 measure the steady state pressure gradient along a sandpack and report that foam exhibits two regimes depending on the injection conditions they report the steady state pressure gradient to be independent of gas velocities and dependent on liquid velocities when the gas fractional flow is large they coin this regime as the high quality regime on the other hand their data show that the steady state pressure gradient is independent of the imposed liquid velocity but dependent on the gas velocity when the gas fractional flow is low they coin this regime as the low quality regime the transition between the two regimes occurs in their experiments at a foam quality gas fractional flow of 94 alvarez et al 2001 further confirm these results by measuring steady state pressure gradient along cores and sandpacks with various surfactant formulations and gases they conclude that the transition foam quality depends on the permeability of the porous medium cores exhibit smaller transition foam qualities than sandpacks this is attributed to differences in the details of capillary pressure between consolidated and unconsolidated rocks fig 4 presents our model predictions of the steady state pressure gradients as a function of the gas and the liquid darcy velocities using the parameters in table s1 in the supporting information when the liquid darcy velocity is small less than 1 0 m day the pressure gradient is insensitive to the gas velocity but sensitive to the liquid velocity conversely when the liquid velocity is large the pressure gradient increases with the gas velocity but remains constant with respect to the liquid velocity this seems to be in accordance with the experimental observations in the literature osterloh and jante 1992 alvarez et al 2001 the steady state average bubble density predictions are plotted as a function of the gas and liquid darcy velocities in fig 5 the average bubble density depends on the liquid velocity when the liquid velocity is small the average bubble density appears insensitive to the gas velocity as the liquid velocity increases the average bubble density is a function of both gas and liquid velocities larger liquid velocities make the foam bubbles finer but larger gas velocities coarsen the foam bubbles this seems to be in agreement with theory as the generation rate varies linearly with the liquid velocity persoff et al 1989 kovscek and radke 1996 almajid 2019 while the coalescence rate increases as the gas velocity increases jimenez and radke 1989 6 2 transient foam flow heterogeneous f g 50 in this example we model a low quality regime experiment in the slightly heterogeneous core sample gas and foamer solution are injected into a core that is completely saturated with foamer solution gas is injected at a darcy velocity of 0 14 m day relative to the applied backpressure of 0 7 mpa while the foamer solution is injected at a darcy velocity of 0 14 m day the resultant foam quality is 50 at the core exit because ct images of the core show two lower permeability layers sandwiching a higher permeability layer we expect the behavior of the pressure saturation and bubble density to be different than the validation cases in the supplementary information table 1 lists the model parameters for the heterogeneous cases measured aqueous phase saturation profiles as well as measured pressure profiles indicate that the foam generated in the experiment was relatively weak until a dimensionless distance of 0 3 having observed this we alter the flowing foam fraction in our model and make it discontinuous along the core more precisely we assign each grid a value of the constant of proportionality ψ in eq 9 in this example the assigned ψ value is larger in the first 0 3 dimensionless distance of the core having a larger constant of proportionality indicates that foam flows more easily and hence weak foam is formed even after foam generation fig 6 plots the aqueous phase saturation profiles at varying dimensionless times at the first three dimensionless times i e 0 11 pvi 0 33 pvi and 0 66 pvi the gas moves into the core quickly draining a considerable amount of liquid 0 4 units of saturation measured data show that strong foam starts to generate somewhere around dimensionless distance of 0 3 predicted aqueous phase saturation tracks the advancement of the two dimensionless times well the model predicts slower front advancement at 0 66 pvi and 1 10 pvi this is attributed to the higher perm layer that the free gas preferentially flows through if it is not foamed even considering this heterogeneity in the core the model tracks roughly the position of the measured front additionally steady state is reached after 2 20 pvi that is matched to a good agreement by the numerical model a foam s strength is not only gauged by the resultant aqueous phase saturation but also by the resultant pressure gradient it builds fig 7 shows the predicted and measured pressure profiles at four dimensionless times consonant with the saturation measurement the model predicts early times and steady state well and mismatches the times in between again the mismatch between the model and experiments at intermediate times is attributed to the heterogeneity of the core that affects the amount of gas in the core and the manner by which it resides inside the core from a closer examination of figs 6 and 7 at a time of 1 10 pvi we observe that the amount of gas estimated by the model is larger than the actual experiment the discontinuous drop in s w occurs at a greater distance from the inlet experimentally than numerically second after the discontinuous drop in s w the gas travels in a piston like fashion meaning that it develops a strong foam but that is not necessarily the case experimentally the experiment shows a smeared front rather than a piston like displacement consistent with the subcore scale heterogeneity in fig 3 a a more piston like displacement translates into a larger pressure drop developed hence there is difference in intermediate pressure drop estimations by the model the larger aqueous phase saturation and the shallow pressure gradient in the weak foam region i e dimensionless distance less than 0 3 are achieved by using a discontinuous value of the constant of proportionality in the expression for flowing foam fraction as discussed above the flowing foam fraction that results from the model is shown in fig 8 larger values of x f lead to larger gas relative permeability values and hence larger aqueous phase saturation for instance fig 6 shows a larger aqueous phase saturation near the inlet of 0 8 that corresponds to a flowing foam fraction of 1 0 when the foam is weak we expect easier mobilization and vice versa the local minima in the x f profile at the displacement front coincide with the regions of large net foam where the bubble density is large fig 9 one interesting feature about fig 9 is the average bubble density upstream and downstream of the x f discontinuity almajid 2019 uses the pore network of almajid and kovscek 2019 to capture qualitatively the aqueous phase saturation behavior observed in these experiments using two snap off probabilities f s o upstream and downstream of the discontinuity a larger f s o downstream of the discontinuity represented stronger foam and finer texture the reported mean bubble densities in fig 9 upstream and downstream of the discontinuity n f and n f seem to be in agreement with our qualitative results the average bubble density is larger where foam is stronger 6 3 transient foam flow heterogeneous f g 90 in this example we model a high quality regime experiment in the same slightly heterogeneous core sample gas and foamer solution are injected into a core that is completely saturated with foamer solution gas is injected at a darcy velocity of 1 90 m day relative to the applied backpressure of 0 7 mpa while the foamer solution is injected at a darcy velocity of 0 21 m day the resultant foam quality is 90 at the core exit the main difference between this experiment and the previous one is only the gas fractional flow applied fig 10 plots the aqueous phase saturation against dimensionless distance for four dimensionless times the model tracks the front well in the early times i e less than 1 28 pvi the wetting liquid saturation is underestimated by the model in the weak foam region i e below dimensionless distance of 0 5 interestingly the front is less smeared after passing the discontinuity at dimensionless distance of 0 5 the pressure profiles of three dimensionless times are shown in fig 11 similar to the 50 quality foam the model approximates the pressure profile well in the very early times and at steady state but mismatches the experimental results in intermediate times after the foam front passes the discontinuity in the core the mismatch in this case is less severe because the drier foam presents less smearing in the foam front in intermediate times again we use a discontinuous flowing foam fraction to capture the large aqueous phase saturation and shallow pressure gradients upstream of the discontinuity in dimensionless distance 0 5 the resulting flowing foam fraction is shown in fig 12 the local minima in x f at the displacement front are due to the region of large net foam generation near the foam front as explained in the previous case we also report the average bubble density in this case before and after the discontinuity in fig 13 the general magnitude of predicted foam texture 200 to 300 mm 3 agrees with the measurements of chen et al 2010 under similar surfactant and flow rate conditions in a permeable sandstone observations made with respect to fig 9 also generally apply here the more interesting observation is that mean values of texture are smaller than the low quality foam case for example the texture at a position of x d 0 8 is roughly 260 mm 3 at a time of 2 2 pvi in fig 8 whereas the texture is 180 mm 3 at the same position at 2 1 pvi in fig 12 this makes sense because wetter foams have larger relative liquid velocities as compared to the gas liquid velocities and hence snap off could occur more frequently there 7 discussion the gas flux is not necessarily a monotone function during these simulations this discussion explores monotonicity the prospects for scale up of the simulation approach to larger domains and the extension of the model to include napl destablizating effects for a discussion of 2d simulations and the role of heterogeneity the reader is encouraged to check the s4 in the supporting information that justifies the use of 1d simulations as an approximation of the 2d results 7 1 monotonicity in the gas flux function to examine the presence of or the absence of monotonicity in the gas flux function during foam flow we plot the gas flux function given different mobility ratios the gas fractional flow is defined as 16 f g 1 1 λ w λ g where f g is the gas fractional flow λ w k r w μ w is the aqueous phase mobility in units of c p 1 and λ g k r g μ g is the gaseous phase mobility in units of c p 1 we alter the gaseous phase mobility by increasing the gas effective viscosity μ g f in the presence of foam for example a strong foam exhibits an effective viscosity that is 100 times larger than the gas viscosity and follows the gas fractional flow curve indicated by μ g f μ g 10 2 in fig 14 thus foam strength increases going from the right most curve to the left most curve in that figure we first use the parameters and the simulation results of the transient foam flow experiment in heterogeneous media presaturated with surfactant solution in the supporting information to examine the monotonicity of the gas flux function in a porous medium that is initially fully saturated with foamer solution we use a 1d model that has 240 grid blocks and refine timesteps to be able to see the saturation changes with time fig 15 shows the gas fractional flow that is exhibited by two grid blocks grid block 5 is close to the injection point while grid block 20 is away from the injection point and represents how gas fractional flow behaves in the rest of the grid blocks away from the injection point fig 15 a and b show that the gas flux function is nonmonotonic when the porous medium is initially fully saturated with foamer solution initially the gas flux function increases monotonically as the bubble density increases and as the water saturation decreases because of the relative increase in the bubble density caused by net foam generation at the foam front the gas flux function is nonmonotonic as evidenced by the zoomed in figures the water saturation as well as the gas fractional flow decrease as the bubble density profiles equilibrate the next case we examine is transient foam flow where the homogeneous porous medium is initially fully saturated with brine similarly we show two grid blocks one close to the injection side fig 16 a and another one that is farther from the injection point and represents how the rest of the grid blocks behave fig 16b the model in this case is much finer than the previous case and has 1920 grid blocks the gas flux function in this case increases monotonically examining the flowing bubble density profiles of this case shows no region of net foam generation and a relative increase of foam texture similar to the previous case this observations seems to explain why the gas flux function is monotonic in this case as a point of clarification there is no region of net foam generation because surfactant concentration is also transient in this case hence the surfactant concentration is relatively small at the foam front and conditions are not favorable for net foam generation when comparing the previous two cases we observe another interesting behavior when the porous medium is initially fully saturated with foamer solution the gas mobility is modified such that the gas flux function follows only one curve out of all possible curves except at late times where nonmonotonic behavior is witnessed the situation is distinctly different and more interesting in the case where the porous medium is initially fully saturated with brine i e contains no surfactant in that case the initial free gas ineffectively drains the medium to a large water saturation while the gas mobility follows that of an unfoamed gas however as soon as the gas is foamed the gas flux function jumps from one gas fractional flow curve to the other as the gas mobility is further modified the zoomed in figures of both fig 16a and b show the intersection points between the respective grid block gas fractional flow curve and the theoretical curves 7 2 scale up to larger domains the mechanistic model developed and tested above performs well against experimental data especially in homogeneous rock samples at the meter scale there are however some challenges that need to be circumvented before we take this further to 10 s of meter scale we believe that heterogeneity plays a vital and pivotal role in any foam flow process and is inevitable to be encountered in nature therefore a key step that needs further investigation is the integration of the heterogeneity found in rock samples this may be accomplished by introducing rock classes in which each class has differing multiphase properties i e relative permeability curves capillary pressure curves apparent viscosity behavior a version of this idea for fractured media was implemented by pancharoen et al 2012 their work needs to be extended to include capillary pressure and relative permeability differences in each rock type and then be implemented within our model to move to larger length scales the other issue is the implementation of wells in larger scale domains this is due to the fact that foam viscosity exhibits a shear thinning behavior a 10 s of meter scale simulation could have grid blocks that are much larger than the wellbore hence a large difference between the bottom hole flowing pressure and a grid block s pressure is expected a significant difference in pressure would lead to shear thinning behavior between the wellbore and the grid block li et al 2006 circumvent this issue through adding a negative viscosity dependent apparent viscosity skin factor in the well model to accommodate the non newtonian effects between the wellbore and the grid block 7 3 extension to include napl destabilizing effects the model in this paper captures foam generation and coalescence mechanisms in the absence of mobile napls hence the model is suitable for applications in the absence of napls or in the presence of residual napls it is however well documented that napls have a destabilizing effect on foam farajzadeh et al 2012 almajid and kovscek 2016 schramm and novosad 1990 manlowe and radke 1990 myers 1999 myers and radke 2000 bergeron et al 1993 because the population balance model is implemented in the ad gprs framework it allows a straightforward extension to include such effects for instance following the work of myers and radke 2000 a third reaction term that accounts for coalescence of foam due to the presence of napls r c o needs to be included the term that myers and radke 2000 included in their work was a function of the bubble flux v g n f the napl saturation s o the imposed capillary pressure on a pseudoemulsion film p c p f and the rupture capillary pressure of a pseudoemulsion film p c p f additionally p c p f depended on the napl water and gas water capillary pressures and the surface tensions of the system all of these parameters once defined can be implemented to compute a third reaction term that adds the destabilizing effect of napls on foam 8 concluding remarks this paper illustrates the development and implementation of a mechanistic full physics population balance transient foam flow model into the multiphysics modular ad gprs simulator the automatic differentiation capabilities significantly aid efforts to prototype expressions of physical phenomena we propose a new flowing foam fraction function and extend an existing population balance model to predict experimental data in homogeneous and slightly heterogeneous porous media in the homogeneous cases we find that the model predicts well the measured data the model predictions were compared to constant surfactant concentration as well as transient surfactant flow cases qualitatively our model predicts the two foam flow regimes usually reported in steady state foam flow literature low quality foam and high quality foam in the slightly heterogeneous cases our model tracks well the location of the saturation front and the steady state and the early time pressure and aqueous phase saturation profiles the intermediate time pressure profiles are overestimated due to the difference of the amount of gas in the system between the model and experiments the heterogeneous cases were predicted with a discontinuous flowing foam fraction function that is consonant with the steady state results of almajid et al 2019 the model also predicts larger averaged bubble density when the gas has passed the high low permeability barrier confirming results of previous pore network studies kharabaf and yortsos 1998 almajid and kovscek 2019 furthermore we have shown that foam flow in porous media is nonmonotonic when the porous medium is presaturated with surfactant solution and that the nonmonotonicity disappears when it is initially brine saturated implementation of the foam population balance model in the modular multiphysics ad gprs simulator provides a first step into modeling more complex and larger scale processes in the future for instance mechanistic foam flow simulation at aquifer scale may be conducted additionally steamfoam physics could be captured theoretically if the flow foam and thermal subproblems are coupled in ad gprs credit authorship contribution statement muhammad m almajid methodology investigation software writing original draft zhi yang wong methodology investigation software anthony r kovscek conceptualization methodology supervision funding acquisition writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements m m almajid acknowledges a graduate fellowship from saudi aramco additional financial support was provided by the supri a industrial affiliates supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2021 103877 appendix a supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 supplementary data s2 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s2 
329,the inherent complexity of the fluid flow in subsurface systems brings potential inevitable uncertainty in their characterization computationally intensive high dimensional inversion problems often emerge in solving the fluid flow problems of various scenarios which required to be probed to improve the efficiency of solving such problems surrogate strategies are widely used to quantify the uncertainty of underground multiphase flow models in this paper a deep learning surrogate model is developed for predicting the time dependent dynamic multiphase flow in a two dimensional 2d channelized geological system the surrogate model is combined with a residual u net and an autoregressive strategy which considers the output at the previous time step as input and predict the output at the current time step the residual u net has a symmetric network structure similar to u net and contains extra residual units the rich skip connections in the network can promote information dissemination and achieve better prediction performance with fewer parameters we demonstrated the performance of the autoregressive residual u net ar runet for predicting the migration of solute transport in heterogeneous 2d binary model the result shows the ar runet surrogate model can provide an accurate approximation of saturation and pressure fields at different times we also have demonstrated that with the autoregressive strategy this network can achieve similar predict results with relatively less training data the performance of the ar runet network is also compared with the autoregressive dense net ar dense the findings indicate that the ar runet can provide effective measures for developing surrogate model and uncertainty analysis in dynamic multiphase flow predictions of subsurface systems keywords multiphase flow subsurface systems deep learning flow modeling 1 introduction fluid flow is an important process in describing a wide range of engineering problems including identification of underground contaminant geological carbon storage energy resources and natural hazards involved with geo materials however the heterogeneity in subsurface systems bring great challenges to evaluate the subsurface flow and solute transport processes to date in the field of subsurface physical simulations the flow and transport of heterogeneous formations have been extensively studied including the identification of contaminant source mo et al 2019a zhang et al 2015 data assimilation chang et al 2017 mahjour et al 2020 model inversion kang et al 2018 reuschen et al 2020 and uncertainty quantification de barros 2018 sureshjani et al 2020 due to the complexity and heterogeneity of such systems along with our incomplete understanding of their properties it often requires a relatively large number of stochastic degrees of freedom to characterize the heterogeneity accurately tahmasebi 2018 therefore a large number of surrogate models chen et al 2020 kwidzinski 2019 ni and benson 2020 wang 2019 zhang et al 2020 have been developed to reduce the computational cost and avoid the curse of dimensionality in solving high dimensional inversion problems recently the surrogate methods have become increasingly popular in underground multiphase flow research due to their considerable computational efficiency in general establishing an efficient surrogate model not only can accurately approximate the high dimensional input output mapping relationship but also reduce the computational burden these surrogate models for solving high dimensional subsurface flow problems can be divided into physics based and data driven by whether there is actual physical meaning in the calculation process and some models will also include both of them asher et al 2015 razavi et al 2012 tang et al 2020 physics based surrogate methods usually adopt the strategy of ignoring or simplifying the physical model parameters establishing a simplified physical model and performing the forward calculation on such model thereby reducing the calculation cost the second type of physics based surrogate methods typically include projecting the governing equations into the low dimensional problem agarwal et al 2014 he and durlofsky 2014 jin and durlofsky 2018 simplifying the underlying physics or reducing numerical resolution he et al 2013a mo et al 2017 karhunen loe ve expansion laloy et al 2013 zhang and lu 2004 orthogonal decomposition gosses et al 2018 li et al 2013 multigrid method arrarás et al 2019 li et al 2020 tahmasebi and kamrava 2018 and multiscale finite element method he et al 2013b xie et al 2017 have been used to build efficient surrogate models these surrogate models compared with traditional models perform better in terms of computational efficiency but sometimes are limited in dealing with strongly nonlinear and high dimensional problems elsheikh et al 2013 li et al 2012 on the other hand data driven surrogate methods for groundwater and solute transport models have been extensively studied the most common of them include monte carlo method lu et al 2018 shi et al 2012 polynomial chaos expansion elsheikh et al 2014 gaussian processes ju et al 2018 nottingham and user 2011 bayesian method ait el fquih et al 2020 giordano et al 2013 and deep neural networks dnns jin et al 2020 mo et al 2019b tang et al 2021 zhong et al 2019 for example xing et al 2019 proposed ensemble surrogate model to solve the inverse contaminant source identification problems the surrogate model consists of kriging radial basis functions and least squares support vector machines while the adaptive metropolis markov chain monte carlo method is adopted to assign weights to the designated models xiao and tian 2020 introduced a surrogate model intending to estimate subsurface geological parameters of high dimensional inversion problems a set of subdomain linear models containing a few high fidelity model simulations were used to replace the original high fidelity model however the strong heterogeneity of natural formations requires a large number of stochastic degrees of freedom to represent which always results in high dimensional problems with strong nonlinear output most of the existing surrogate methods are severely affected by the input dimension as the input dimension increases the computational cost of the proxy method increases exponentially besides the strongly nonlinear output often requires more training sets which will further increase the computational cost with the rapid development in artificial intelligence techniques and graphics computing hardware in recent years more and more dnns methods are applied to construct surrogate models and solve the problem of high dimensional inversion in subsurface systems mo et al 2019b 2019a tang et al 2020 zhong et al 2019 characterization of small scale porous media kamrava et al 2021 2020 2019b 2019a and large scale subsurface modeling and predictions bai and tahmasebi 2021 2020a 2020b dnns methods have demonstrated good robustness and generalization properties in image like data prediction and segmentation the task with high dimensional input and output fields have been transformed into an image to image regression problem in deep learning based surrogate models compared to the traditional surrogate method the large deep complex network architectures in the dnns surrogate method increases the ability to characterize high dimensional and strong nonlinear relationships between inputs and outputs dnns method achieves accurate approximation of complex output through deep network architectures and a certain number of iterations mo et al 2019a b proposed to establish a dnns surrogate model using a combination of autoregressive strategy and dense convolutional encoder decoder network architecture which provides an accurate approximation for a 3321 dimensional hydraulic head and log gaussian distributed conductivity field trained with a limited amount of data to achieve the purpose of working with fewer training images and more precise segmentation u net architecture was developed by navab et al 2015 many studies have shown that deeper network structures generally have better prediction performance simonyan and zisserman 2014 zeng et al 2016 but at the same time it also brings the issue of vanishing gradients which will make the deeper neural networks harder to train glorot and bengio 2010 he et al 2016 proposed a deep residual learning architecture to deal with this issue later wen et al 2019 adopted a residual u net deep to achieve an accurate prediction of co2 saturation and pressure with layered heterogeneity and non gaussian distributed permeability more recently mo et al 2020 developed a deep residual dense convolutional neural network in which the ability of deep neural networks method in capturing input output relationships in subsurface systems with non gaussian log conductivity fields is demonstrated in this study we have developed an autoregressive residual u net ar runet surrogate model to predict the time dependent dynamic subsurface flow for a complex 2d channelized geological system in this research model the injected water flooded the square binary channelized formation from its left boundary the outputs of the flooding task are the saturation and pressure maps at four different times after water flooding here the autoregressive strategy refers to using the output of the previous time as the input of the neural network to predict the output of the current time rather than changing the network architecture the residual learning strategy has been proved to be an effective solution to ease the training of networks with quite deep architecture he et al 2016 zhang et al 2018 to do so a five layer deep residual unit has been added to traditional u net architecture developing a new residual u net architecture runet with a significantly improved performance for surrogate modeling of highly complex mappings the results show that after the autoregressive strategy was processed to the input and time dependent output data the prediction accuracy of the dnns method has increased significantly which outperforms other available methods for the problem of this paper the results of this paper therefore can help to enhance our understanding from the hydrological processes by considering various scenarios of the subsurface systems without the necessity of conducting the expensive methods for computational fluid dynamics thus one can assess the uncertainty in much more accurate and informed fashion when a variety of possibilities are considered using hydrologically realistic models e g heterogeneity geology the rest of the paper is organized as follows in section 2 we introduce the governing equations of the dynamic subsurface multiphase flow in channelized geological models in section 3 the deep residual u net with an autoregressive strategy is proposed in section 4 the ar runet method is applied for dynamic subsurface multiphase flow involving multiple realizations of a 2d binary channelized system in section 5 we discuss the advantage and limitations of our method by comparing it with other dnns methods finally in section 6 we summarize this work and provide suggestions for future investigations 2 governing equations in this research the surrogate model of dynamic subsurface multiphase flow in the channelized geological model is considered through which the relationship between the uncertainty of the subsurface fluid propagation and the properties of formation heterogeneity e g permeability and porosity was studied thus the governing equations for two immiscible fluids flow of mass conservation for each component are considered and given pruess et al 1999 1 t ϕ s j ρ j ρ j v j ρ j q j where subscript j denotes component with j w for wetting fluid phase and j n denotes non wetting fluid phase sj is phase saturation t is the time φ is the rock porosity ρ j is phase density qj is the source term combining mass conservation and darcy s law the pore space flow velocity vj can be defined 2 v j k ϕ k r j μ j p j where k is the absolute permeability tensor determined by the channelized geological model μ j is the phase viscosity krj is the relative permeability pj is the phase pressure the governing equation is also controlled by the sum of the saturation of the wetting phase and the non wetting phase equal to 1 the pressure of each phase is related to the capillary pressure pc 3 p n p w p c s w where pn and pw are the pressure of the non wetting phase and the wetting phase respectively as a function of saturation in this work the computational domain is a 2d geological model without considering the influence of gravity acceleration besides molecular diffusion and hydrodynamic dispersion are also not included in our model 3 methodology 3 1 deep convolutional neural networks dnns method refers to a network with multiple hidden layers generally each layer is composed of linear neurons and nonlinear activation functions the output of the lth layer of an n layer network can be expressed as 4 z l η z l 1 σ w l z l 1 b l where the superscript l denotes the layer index z l 1 and z l are the input and output of the lth layer l 0 1 2 3 n respectively w l and b l denote the linear weight matrix and bias vector the process of training with dnns is the process of finding the appropriate linear weight matrix w l and bias vector b l corresponding to all hidden layers and output layers letting all the calculated outputs from the input training samples by dnns method be as close as possible to the real outputs σ is the nonlinear activation function the sigmoid function the rectified linear unit relu the hyperbolic function and the softplus function are the common activation functions which need to be selected according to the characteristics of the training set baldi 2014 in this research the bias term b l has not been taken into consideration thus the relationship could also be written as 5 z l η z l 1 σ w l z l 1 a deep and complex network structure coupled with the fully connected characteristics often leads to a large number of network parameters the fully connectedness of dnns methods indicates that each node is connected to all nodes of the previous layer the fully connected characteristics always make them prone to overfitting therefore the deep convolutional network architecture is used to reduce the network parameters because the convolutional neural network explicitly considers the local spatial dependence of the input data it is particularly suitable for processing image data the convolutional neural network uses a specific filter also called the kernel function to establish a sparse local connectivity model between neurons in adjacent layers and achieve local spatial correlation each neuron is only connected to only a small region of the input area organized into feature maps specifically when the input x is a two dimensional image with width w and height h x r h w a convolutional layer η is obtained by employing a series of q 1 nf kernel function k f r m n the feature value η i j q x i j at the location i j after the convolution transformation can be expressed as 6 η i j q x i j p 1 m q 1 n x m n k f i m i n after the convolution calculation input x r h w of each layer will generate the nf layer feature maps the relationship of input size sinput h w of output size soutput h w is given by dumoulin and visin 2016 7 s o u t p u t s i n p u t 2 p k f s 1 where the two important parameters determined the size of the output is the stride s and the zero padding p stride s is the distance between two successive moves of the kernel function and the zero padding p indicates filling the boundary of the input image with zeros to ensure that the information on the boundary is saved in the feature maps is the floor function fig 1 is an example of feature map calculation of two dimensional image with size of 7 7 first the way the convolution function performs is shown by the calculations conducted for one pixel cell then the corresponding relations and the calculation process are demonstrated including a convolution layer a batch normalization layer bn and a rectified linear unit relu bn is to standardize the input data so that the output data has a mean of 0 and a variance of 1 the logic behind relu is to change all the negative values to zero max pooling with the kernel size of 2 2 and the stride s of 2 is used as a downsampling operator which reduces the size by the order of 1 2 the process of training with dnns is the process of finding the appropriate linear weight matrix w l namely the convolution kernel 3 2 autoregressive residual u net ar runet in this study an autoregressive data processing strategy is adopted before dnns training to improve the performance of the dnns method in predicting time varying saturation and pressure in a groundwater problem the residual u net architecture was selected in this surrogate model it transforms the surrogate modeling task into an image to image regression problem to take advantage of the great characteristics of convolutional networks in image processing in the following subsections the image to image regression strategy the residual u net architecture and the autoregressive data process have been briefly introduced 3 2 1 modeling as image to image regression in the traditional surrogate models the goal of the model is to find the solution of the governing equation within the rectangular range of size h w where hand wdenote the grid numbers along the two different axes the speed of this solution will be greatly reduced when solving high dimensional problems in the surrogate model based on dnns the input data and output data are regarded as images then through multiple iterations of the complex neural network the error is reduced and one can achieve accurate prediction of output images in the image to image regression model the input output relationship of the 2d regression task can be described as the following mapping 8 λ r d i n h w r d o u t h w where λ denotes the mapping relationship from input to output din and dout are the dimension of the input and output images in recent years with the development of gpu and other computing equipment it is easy and fast to calculate many large matrices and images at one time which is also an important factor for the explosive development and application of deep convolution neural network technology it has been proved in many studies that the use of deep convolution network for predictions in subsurface systems has excellent performance mo et al 2019a tahmasebi et al 2020 tang et al 2020 xiao and tian 2020 when the input and output are 3d data one can consider the 3d input as a series of images of size h w at different depths to train 3 2 2 residual u net architecture in this study inspired by the deep residual learning and u net a residual u net method was proposed for conducting a surrogate modeling the u net architecture has a successive contracting path to capture feature maps and a sequential symmetric expanding path to increase the resolution of the output in the contracting path there are a total of four downsampling processes each time downsampling the number of feature channels will be doubled and the length and width of feature maps are reduced to half before each downsampling there will be two consecutive convolution blocks to capture feature values each convolution block contains a convolution unpadded convolutions a batch normalization layer bn ioffe and szegedy 2015 and a rectified linear unit relu nair and hinton 2010 fig 2 c in the subsequent symmetric expanding path the feature maps will be restored to the original input images size after four times of upsampling processes that will halve the feature channels while double the size of feature maps in order to achieve precise localization high resolution features from the contracting path on the left side will be conserved and combined with the upsampled output on the right side at the final layer a 1 1 convolution is used to convert the feature maps into output images of different channels in u net architecture downsampling is realized by a 2 2 max pooling operation and upsampling is formed by a 2 2 upsampling and 3 3 convolution up conv in order to stitch the output segmentation map seamlessly the deconvolution is not adopted to replace the upsampling process which helps to avoid the influence of the grid effect this runet architecture has the same symmetric contracting and expanding path networks as the u net architecture with an additional residual network the detailed runet architecture is shown in fig 2 a the yellow arrow is a convolution block composed of a 3 3 convolutional layer a bn layer and a relu layer which can extract and output the feature map of the same size as the input image the green arrow is a 2 2 max pooling operation to extract the downsamped feature map whose length and width are half of the input image the blue arrow is a convolution block consisting of a 3 3 up conv layer a bn layer and a relu layer which can get an upsampled feature map the grey arrow represents a copy and concatenate operation using which the feature maps white box from the left contracting path are concatenated with the right feature maps blue box the residual network purple arrow consists of five residual convolutional blocks see fig 2 b each residual convolutional block consists a 3 3 convolutional layer a bn layer a relu layer another convolutional layer another bn layer an identity shortcut and another relu layer after the sum process as shown in fig 2 d 3 2 3 autoregressive strategy for time dependent process since the emergence of autoregressive for fitting the prediction model five decades ago akaike 1969 it has been widely used in various fields the autoregressive model is used for predicting when there is some correlation between values in a time series forecasting behavior is generally based on past behavior the relationship between the input and output in autoregressive can be expressed as 9 x t i 1 t φ i x t i ε t c where x t and ε t denotes the variable observed and white noise at time t respectively c is a constant item ϕ is the autocorrelation coefficient it can be written into 10 φ i c o v x t x t i v a r x t which is the ratio of covariance to variance the autocorrelation coefficient of the data is the key to determining the predictive effect of the autoregressive model when using the autoregressive model the autocorrelation coefficient is generally required to be greater than 0 5 in this work the output saturation or pressure maps are regarded as images at different time steps nt 4 thus it can be written as 11 y y 1 y 2 y 3 y 4 the relationship between the model input x and output ycan be described as a function y j j 1 n t f x j which is shown in fig 3 a this problem can be considered as a network system consisting of one input channel x and nt time varying output channels y j j 1 n t in fact in the series of time dependent output y j j 1 n t the output at the previous time step is the intermediate state of the output at the next time and there is a good autocorrelation between the output s elements y j j 1 n t taking n forward models into consideration the training data can be expressed as 12 x i y j i j 1 n t i 1 n putting the training data into the neural network η the relationship will be written into 13 y j i j 1 n t i 1 n η x i i 1 n where y j i j 1 n t is the output of the ith forward model time step j 1 nt forward model number i 1 n xi is the input of the ith forward model once the autoregressive strategy is adopted the previous output will be used as the next input the autoregressive time varying model is proposed as following 14 y j f x y j 1 in this model the current output yj j 1 nt is a function of the time independent input x and the output at the last time step y j 1 where y 0 is the initial state therefore the relationship map between the autoregressive model input x and output y i i 1 n t can be depicted as shown in fig 3 b in the autoregressive network the training samples of n forward model can be organized as 15 x y j 1 y j j 1 n t m 1 n x i y j 1 i y j i i 1 j 1 n n t when the autoregressive method is considered in the dnns network η it is a way of data processing before dnns network training thus the network relationship can be expressed as 16 y j i i 1 j 1 n n t η x i y j 1 i i 1 j 1 n n t y m m 1 n n t η x m m 1 n n t where y j i and y j 1 i represent the output at the jth and j 1 th time step of the ith forward model respectively time stepj 1 nt forward model number i 1 n xi is the input of the ith forward model the train number and size of training data of the ar dnns model is nt times of common dnns model which leads to its higher accuracy and longer training time the calculation cost in the whole process is still relatively cheap compared to the forward model this aspect will be discussed in detail in section 5 3 2 4 networks training in the network training process y η x θ a series of the network parameters θ is determined to achieve an accurate approximation of the trained output y m m n n t the regularized l 1 norm loss function is selected during the training process combined with the cost function representing by 17 j θ 1 n n t m 1 n n t η x m θ y m α 2 θ t θ where α is a regularization coefficient also called weight decay the stochastic gradient descent sgd is used to solve the cost function in eq 17 as an optimizer expressing as 18 θ j θ 1 n n t m 1 n n t θ η x m θ y m the cost of this calculation is n nt as the training set n nt of data increases each step of the calculation will consume a relatively long time therefore the minibatch strategy is adopted each time only a small part of the sample b x m y m m 1 m b is drawn from the training set mb is the number of each minibatch process generally speaking mb is less than 100 which is determined by the size of the training image and the computing device the gradient estimate of each minibatch can be expressed as 19 g 1 m b θ m 1 m b η x m θ y m when using the minibatch strategy the gradient descent in the sgb algorithm is as follows 20 θ θ ε g where ε is the learning rate is a key parameter of the sgd algorithm adam kingma and ba 2014 a commonly used optimization algorithm with an adaptive learning rate and a kind of sgd algorithm was selected in this study 3 3 performance metrics two metrics are used to quantify the performance of the dnns surrogate model namely the coefficient of determination r 2 and the root mean square error rmse 21 r 2 1 m 1 n c y m y m 2 2 m 1 n c y m y 2 2 and 22 r m s e 1 n c m 1 n c y m y m 2 2 where ym is the value of the forward model y m is the value of network predictions y is the average value of ym expressed as y 1 n c m 1 n c y m r 2 is a measure that reflects the difference between the predicted value and the model distribution the closer it is to 1 the better the prediction effect rmse is a metric that measures the deviation between the predicted value and the model value in a convergent algorithm as the number of measurements nc increases the rmse will decrease when rmse is closer to 0 it indicates that the prediction effect is better and the algorithm is more robust without the autoregressive method nt saturation or pressure maps at different times will be treated as a whole output ym is y j i i 1 n t i 1 n y r h w the number of dnns network nc called during the entire training process will equal to train number n after the autoregressive strategy is adopted saturation or pressure maps at different times will be used as output one by one ym is y j i i 1 j 1 n n t y r h w the number of ar dnns network will equal to n nt where n is train number nt is the time steps number in this study we consider the uncertainty associated with the input binary channeled geological map and quantify its effects on the output saturation and pressure fields the maximum value of the absolute approximation errors has been introduced to characterize the predicted result of the different methods and it can be expressed as 23 e r r o r max max y m y m it is a parameter that reflects the maximum error predicted by the algorithm in each output in this study there are a total of nt error max the two common statistical parameters mean and median are used to quantify this uncertainty without loss of generality they can be expressed as the mean 24 μ κ η x p x d x and the median 25 m e d i a n 1 2 η o x n n t 1 2 η o x n n t 1 2 where κ is the sample set p x is the probability distribution of input x in the sample set η is the relationship from input to output it represents error max in this research η o x is an ordered output list of n nt numbers and denote the floor and ceiling functions respectively a maximum error is not always a relatively good indicator of the prediction performance of an algorithm at a certain point thus the probability density function pdf of the surrogate model output value at constant observation location has been used to compare with that of the forward output the differences between them have been quantified by the coefficient of determination r 2 4 application in this section we introduce a 2d binary facies channelized model created by the cross correlation simulation ccsim method and conducted a multiphase flow for obtaining the saturation and pressure at different times the different predict performance of ar runet surrogate method has been compared with those for existing runet wen et al 2019 dense networks mo et al 2019a and ar dense mo et al 2019b deep neutral network surrogate methods 4 1 experiment setting 4 1 1 multiphase flow modeling in this research a subsurface multiphase flooding problem was studied as shown in fig 4 the binary facies channelized geological map lies in a horizontal plane is defined on a 128 128 grid with a grid size of 1m 1m and a thickness of 1m in each grid block the yellow cells indicate the advantageous flow channel sandstone with a porosity equal to 30 and a permeability of 2000 md while the blue block is the shale and its porosity and permeability are 2 and 0 01 md respectively there are 64 injected wells on the left boundary where the flooding fluid was injected at a rate of 0 2 m3 day the initial formation pressure is 250 kpa while the flooding pressure is 500 kpa the initial fluid saturation and the residual fluid saturation is 20 and 80 respectively the detailed simulation parameters are listed in table 1 in every forward model the saturation and pressure maps at four different times 10 days 40 days 100 days and 180 days were calculated which takes about 2 minutes on a xeon e3 1270 3 5 ghz cpu with 16gb ram 4 1 2 geomodel generation the dnns surrogate model requires a relatively large number of training multiphase flow simulations to achieve the accurate dynamic output states from the binary facies channelized geological maps the geomodel was expanded to four different training sample sets and a test sample set by the cross correlation simulation ccsim algorithm tahmasebi et al 2012 ccsim method can generate the realizations of a subsurface system with accurate conditioning and continuity it has been proven to be superior to the commonly used multiple point geostatistics method in terms of computational efficiency and quality in ccsim a higher order cross correlations function based on the correlations between the various parts was developed and used along a raster path to generate a new model that matches a given training image ti which has been detailed in tahmasebi et al 2012 2014 the cross correlations function was used to measure the similarity between the ti and overlap in the simulation grid and is expressed as 26 i j x 0 ℓ x 1 x 0 ℓ y 1 t i x i y i d e v t x y where ti x y denotes the value at point x y of the given image ti of size lx ly devt x y is the data event at point x y using template t i and j represent the shift steps in the x and y directions respectively fig 5 displays six random binary channelized models generated by ccsim with the size of 128 128 in the binary channelized geological maps the yellow dominant flow channels sandstones are randomly distributed in the simulated geological body and have good connectivity vividly simulating the uncertainty and connectivity of the underground geological body given these stochastic models each grid is assigned a specific porosity and permeability as mentioned above and dynamic output results are obtained through the forward model introduced in session 2 4 2 network design and data sets to illustrate the advantage and effectiveness of the autoregressive strategy in improving the prediction accuracy of networks for the time varying outputs of the subsurface multiphase flow models four different types of deep learning networks are trained and compared runet residual u net architecture see fig 2 a each time of training a series of different time dependent output are predicted without using the autoregression method to reorganize the data as eq 16 dense deep dense convolutional encoder decoder networks mo et al 2019 it shows a good performance in predicting the output concentration fields at different time steps from a time varying source and a log conductivity field ar runet autoregressive residual u net architecture the autoregressive strategy is used to process the input and output as in eq 16 ar dense autoregressive deep dense convolutional encoder decoder networks the input and output are reorganized by autoregressive strategy before training the l 1 loss function defined in eq 17 and 200 epochs are used in the above dnns networks to illustrate the convergence of the approximation error in the dnns networks four training sample sets obtained from n 200 350 750 and 1 000 model evaluations and a test sample set with 500 model evaluations are generated the output at t 10 40 100 180 days has been observed in the inverse problem which indicates that the number of observation points is nt 4 in runet and dense networks there are one input channel x r 1 128 128 the binary facies channelized geomodel and four output channels y j j 1 4 r 4 128 128 the dynamic saturation and pressure maps at the observed time in ar runet and ar dense networks in which the training data have been reorganized by eq 16 there are two input channels x i y j 1 i r 2 128 128 and one output channel y j i r 1 128 128 at time step j 1 2 3 4 indicated by forward model number i 1 n where y 0 is the given initial state the differences between the two networks without the autoregressive strategy i e runet and dense and the two autoregressive networks i e ar runet and ar dense are summarized in table 2 after adopting the autoregressive strategy the training data is split and the neural network are called more times with the same training set the detailed input output and the used times of the networks in training with n training sample sets are listed in the runet and ar runet network there a 4 layer successive contracting path to capture feature maps a 4 layer successive symmetric expanding path to increase the resolution of the output feature maps a five block residual network was added to u net architecture to capture the accuracy feature maps in the size of 8 8 finally there is an output convolutional layer that adjusts the output channel the detailed network architectures of runet and ar runet are shown in table 3 there are 29 convolution layers in runet and ar runet network architecture the number of network parameters is 128 918 404 in runet and 128 918 785 in ar runet in table 3 the output shape includes output channel number nc the height of the feature map h and the width of the feature map the size of output feature maps can be given by the values of input feature maps size convolution kernel size stride and zero padding the relationship defined in eq 7 the network structure of runet and ar runet is the same except that the convolutional layer in the input layer and the output layer has made some changes to deal with different training data in our proposed architecture the convolution kernel size stride and zero padding are the same in all convolutional layers conv2d except that in the output layers where a convolution layer with kernel size kf 3 stride s 1 and zero padding p 1 is used it is commonly used convolutional layers that capture feature maps without changing the output size bn denotes a batch normalization layer relu is a rectified linear layer maxpool2d denotes a downsample layer with kernel size kf 2 stride s 2 which reduced the size of the feature map by half upsample is a upsample layer with scale factor fs 2 after this process the width and height of the feature maps will be doubled the last output layer is the convolution layer with kernel size kf 1 stride s 1 and zero padding p 0 this convolution layer can also keep the output image and input image the same size and a smaller kernel size helps improve the output resolution 5 result in this section we assess the performances of runet dense ar runet and ar dense networks in approximating the time dependent dynamic subsurface multiphase flow predictions of the 2d channelized geological system different performance metrics are used to evaluate the dnns surrogate method 5 1 approximation accuracy assessment of surrogate model the proposed four different dnns architecture and loss function discussed in section 4 were conducted sample sets with 200 350 750 and 1 000 realizations were trained and tested in a sample set with 500 realizations the training processes cost range 11 230 minutes using a nvidia tesla p100 gpu after reorganizing the data with the autoregressive strategy the number of training samples will be increased to nt times the number of the forward model runs it increases the training time at the same time note that we are only able to access a portion of the gpu memory since it is operated in a shared high performance computing hpc system after training of 200 epochs the prediction results of different surrogate models show very clear differences taking the output of dynamic saturation distributions as an example the r 2 score and rmse for the test set for each surrogate with different training data set sizes are shown in table 4 and fig 7 the result of dynamic pressure prediction is basically the same as this the autoregressive strategy will greatly improve the prediction results of the underground time dependent multiphase flow of the dnns surrogate model in this research the autoregressive dnns surrogate method ar runet and ar dense can achieve r 2 score higher than 0 980 and rmse lower than 0 020 with only 200 train samples in fig 6 shows the ar runet surrogate method with only 200 training samples achieve the highest r 2 value of 0 986 in predicting dynamic saturation distributions with a 128 128 dimensional stochastic input with more training samples put into the dnns network all these surrogate methods have achieved better predictive performance and the difference between the predictive performance of these methods will gradually decrease when increasing the train samples size to 1 000 the ar runet model achieves a r 2 value of 0 998 the results show that the use of autoregressive strategies when training samples are limited can improve the performance of dnns alternative models in underground multiphase flow prediction the performance of the networks is further compared in figs 6 9 which respectively show the predictions of dense runet ar dense and ar runet using n 400 training samples for the dynamic saturation pressure maps at t 10 40 100 and 180 days part a reflects the prediction performance of saturation at different times and part b shows the prediction of formation pressure at different times for comparison the forward model predictions are shown in the first row of each plot marked as y the second row of each plot is the result of dnns surrogate model predictions denoted by y y y denotes the difference between the predictions of the forward model and the dnns method each plot has four columns which represent the saturation pressure outputs at 10 40 100 and 180 days fig 7 shows that with the process of flooding the distribution of output becomes more and more complex and the accuracy of dense network prediction gradually decreases there are relatively large approximation errors between the output of the forward model and dense network output at time equals to 180 days especially near the dominant channel of the geological body mo et al 2019 used an additional weight loss to improve the approximation performance around the only contaminant source location however in the training of the binary geological channel map in this study each point at the boundary of the dominant channel is equivalent to a source and there will be countless randomly distributed sources in each training image therefore the additional weight loss method will be efficient here fig 8 shows the predictions of the residual u net network with 200 model evaluations runet demonstrated a good ability to capture the value of the boundary with a high gradient and its prediction performance is significantly better than the dense network in order to improve the dnns surrogate model s approximation performance in addition to optimizing the dnns network we can increase the number of training samples or use an autoregressive strategy when the training data is limited the autoregressive strategy can have the same effect on improving the predicted results after the autoregressive method is adopted the training data are reorganized as eq 16 and both the ar dense and ar runet methods performances better than before as figs 9 and 10 show fig 9 indicates there are significantly reduced approximation errors in the predicted saturation result using the ar dense method while the pressure predictions with a higher gradient at the boundary still have the relatively larger approximation errors than other regions at the boundary fig 10 shows the predicted result by the ar runet method compared with the results of the forward model the errors of the ar runet predictions are relatively small and evenly distributed and there is no clear high error area in the predictions in this research the dynamics of subsurface multiphase flow problem has been transferred into a task of image to image regression by the dnns surrogate method first the results in this session indicate that dnns alternative models have strong flexibility and robustness when solving high dimensional input output problems with strongly nonlinear characteristics second different dnns have distinct ways of capturing features in the training of input output relationships and choosing appropriate dnn based surrogate models for different problems can achieve better prediction performance in this research there are many step change boundaries in binary geological maps compared with the dense network the residual u net network runet has a better ability to capture boundary features so the runet method has a better performance in handling the same high dimensional time dependent prediction problem in addition such surrogate dnns models are much faster than the forward model each prediction of an 128 128 image will take less than a second on a recent gpu device finally the autoregressive strategy has advantages in solving time varying output it uses the correlation between different time outputs to reorganize and expand the training data and achieve higher training performance with relatively limited train samples through nonlinear projections of the input into high level coarse feature maps in an image to image regression process the dnns surrogate method handles the problem of the high dimensionality of input implicitly while providing an excellent capability to deal with complex output responses these developed dnns surrogate methods in this research were able to effectively provide relatively good approximation with limited training samples 5 2 uncertainty modeling in this section the effectiveness and efficiency of the ar runet and ar dense surrogate models are analyzed and compared the maximum value of the absolute approximation errors error max defined as eq 23 of the ar runet and ar dense method have been calculated and compared in fig 11 the probability density function pdf distributions of error max in the predicted saturation maps with different training samples n 200 350 750 1 000 of ar runet and ar dense method are compared it should be noted that in each subplot 2 000 predictions of t 10 40 100 180 days are included i e the output saturation maps at nt 4 different time steps in ntest 500 test samples the maximum absolute error error max of the prediction is a useful parameter that reflects the accuracy of the prediction the smaller the value of error max the higher the approximate accuracy as can be seen in fig 11 the values of maximum absolute error error max are significantly reduced when the number of training samples increases from 200 350 750 to 1 000 indicating that the prediction performance has become better besides the ar runet method has always had smaller error max values than ar dense with different train samples in table 5 the median and mean of pdf distributions of error max drafted in fig 11 are calculated to make the difference of prediction performance between ar runet and ar dense clearer table 5 shows the ar runet method has smaller median and mean values of the error max than the ar dense method as the number of training increases the median and mean of error max decreases as for the distribution of error max in pressure prediction there is a similar result as in fig 11 the assumed formation condition in this study are good enough and the dominant channel has porosity and permeability of 30 and 2000 md respectively after flooding the pressure in the dominant channels reached a balance soon which results in a pressure sharp change region at the boundary between the two geological media with different physical properties the pressure maps with a sharp change random boundary between different geological media make the prediction complicated in fig 12 the probability density function pdf distributions of error max in the predicted pressure maps with different training samples n 200 350 750 1 000 of ar runet and ar dense methods are compared the values of ar runet s maximum absolute error error max of predicted pressure maps distributes within 0 2 while the error max of the ar dense method always has a relatively wide distribution range 0 0 5 since it has a relatively poor prediction performance when dealing with step change boundaries as shown in fig 9 b in table 6 the median and mean of pdf distributions in pressure prediction of ar runet and ar dense method are shown the result of ar runet also has clear advantages in predicting the dynamic saturation distribution after flooding compared to ar dense in this study the test set consists of 500 random 2d binary geological models generated by the ccsim method accompanied by the saturation and pressure distributions at different observation times as the number of samples in the training set increases the prediction performance in the test set becomes significantly better in fig 13 the output saturation maps of a ar dense and b ar runet surrogate models predicted with 1 000 training samples are compared both dnns surrogate methods have good predictions with small approximation errors to make the comparison more specific an observation point with location of 64 64 is used to observe the prediction performance of ar runet and ar dense on saturation values in 500 test samples fig 14 shows the pdfs of saturation at this location at 10 40 100 and 180 days comparing the pdfs at different times with the flooding process the saturation value at the observation point becomes higher reflecting the diffusion of different fluids after flooding the saturation value around 0 20 reflects the initial saturation when the observation point is mudstone these mudstone observation points have no significant increase in saturation after flooding in contrast at time t 180 days the saturation output of the forward model becomes more complicated and the ar dense method represents lager approximation errors of the complex saturation distribution fig 15 shows the output pressure maps of a ar dense and b ar runet surrogate models predicted with 1 000 train samples are compared with the forward model output in the initial conditions of the control equation of formation pressure a constant pressure control point is set at the location 128 64 the ar dense method does not perform well for the prediction near the constant pressure control point while ar runet can handle every prediction more perfectly during this process in fig 16 the pdfs of pressure at the location 64 64 at 10 40 100 and 180 days indicate the same problem presents in fig 15 in order to highlight the difference between the ar runet and ar dense methods the consistency coefficient r 2 calculated based on the prediction performance of figs 14 and 16 is listed in table 7 in table 7 as the time after flooding gradually increases from 10 40 100 and 180 days the facies become more complex and the predicted performance is on a downward trend for the output of multiple flows at the observation point in 500 test samples including saturation and pressure ar runet has always performed better than ar dense the r 2 value of pressure pdfs is higher than that in saturation prediction in fact the formation pressure quickly reaches equilibrium after the flooding and the values of pressure will be more concentrated 6 conclusions surrogate methods are used widely to alleviate the large computational burden associated with the subsurface multiphase flow including in forward and inversion models however the high dimensionality and strong nonlinearity always bring difficulties to the application of surrogate models in this study a new dnn based surrogate method was proposed for efficiently solving the high dimensional subsurface multiphase flow problem the autoregressive residual u net ar runet architecture is adopted to transform the task into an image to image regression problem reduce the computational cost and avoid the curse of dimensionality the autoregressive was particularly used to reconstruct the training data before putting them into the deep learning network architecture in which the time dependent output at the previous time step is regarded as the input to predict the current output with the autoregressive process the original dynamic multiphase flow prediction results output at one time are split into nt separate ones nt is the time step number of time dependent output the autoregressive strategy enables the network to efficiently and accurately capture the complex relationship between the inputs and time varying outputs with limited train samples to further address the dnns with intensive data a residual u net architecture with a 4 layer successive contracting path to capture feature maps and a 4 layer successive symmetric expanding path to increase the resolution of the output feature maps between them a 5 layer residual network is added the performance of the proposed method is demonstrated using a complex subsurface multiphase flow prediction problem with a 128 128 the results indicate that the surrogate models based on dnns use the image to image regression method to solve the surrogate task and have strong applicability and robustness with only 1 000 train samples the four mentioned dnn methods could all achieve correlation coefficients higher than 0 9 the ar runet method demonstrated the best performance in terms of providing the accurate approximations of time varying outputs whether it is a more discrete saturation output or pressure output with step changes the application of the method in the addressing subsurface multiphase flow task showed that the deep network based surrogate model can achieve comparably accurate results to those obtained with the traditional methods but with less computational cost thus our proposed method can be used as an alternative wherein the hydrological processes along with geology and heterogeneity are all considered in the same framework this technique provides a basis through which one can examine various hydro geological models without the necessity of conducting the full physics modeling which allows to quantify the uncertainty and make the future plans in a more comprehensive and realistic fashion although the proposed surrogate method based on dnn has shown a great potential when solving the task of subsurface flow problems the assumed heterogeneity of high dimensional systems in the current models is still relatively simple most of them are gaussian distribution or bimodal distribution the input based on the heterogeneous distribution of the actual geological model will put forward higher requirements for the dnn method credit authorship contribution statement zhihao jiang methodology data curtion writing original draft visualization investigation validation pejman tahmasebi conceptualization methodology validation writing review editing supervision zhiqiang mao validation writing review editing supervision funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this work was supported by hpc of the university of wyoming the first and third authors also appreciate of china scholarship council supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2021 103878 appendix supplementary materials image application 1 
329,the inherent complexity of the fluid flow in subsurface systems brings potential inevitable uncertainty in their characterization computationally intensive high dimensional inversion problems often emerge in solving the fluid flow problems of various scenarios which required to be probed to improve the efficiency of solving such problems surrogate strategies are widely used to quantify the uncertainty of underground multiphase flow models in this paper a deep learning surrogate model is developed for predicting the time dependent dynamic multiphase flow in a two dimensional 2d channelized geological system the surrogate model is combined with a residual u net and an autoregressive strategy which considers the output at the previous time step as input and predict the output at the current time step the residual u net has a symmetric network structure similar to u net and contains extra residual units the rich skip connections in the network can promote information dissemination and achieve better prediction performance with fewer parameters we demonstrated the performance of the autoregressive residual u net ar runet for predicting the migration of solute transport in heterogeneous 2d binary model the result shows the ar runet surrogate model can provide an accurate approximation of saturation and pressure fields at different times we also have demonstrated that with the autoregressive strategy this network can achieve similar predict results with relatively less training data the performance of the ar runet network is also compared with the autoregressive dense net ar dense the findings indicate that the ar runet can provide effective measures for developing surrogate model and uncertainty analysis in dynamic multiphase flow predictions of subsurface systems keywords multiphase flow subsurface systems deep learning flow modeling 1 introduction fluid flow is an important process in describing a wide range of engineering problems including identification of underground contaminant geological carbon storage energy resources and natural hazards involved with geo materials however the heterogeneity in subsurface systems bring great challenges to evaluate the subsurface flow and solute transport processes to date in the field of subsurface physical simulations the flow and transport of heterogeneous formations have been extensively studied including the identification of contaminant source mo et al 2019a zhang et al 2015 data assimilation chang et al 2017 mahjour et al 2020 model inversion kang et al 2018 reuschen et al 2020 and uncertainty quantification de barros 2018 sureshjani et al 2020 due to the complexity and heterogeneity of such systems along with our incomplete understanding of their properties it often requires a relatively large number of stochastic degrees of freedom to characterize the heterogeneity accurately tahmasebi 2018 therefore a large number of surrogate models chen et al 2020 kwidzinski 2019 ni and benson 2020 wang 2019 zhang et al 2020 have been developed to reduce the computational cost and avoid the curse of dimensionality in solving high dimensional inversion problems recently the surrogate methods have become increasingly popular in underground multiphase flow research due to their considerable computational efficiency in general establishing an efficient surrogate model not only can accurately approximate the high dimensional input output mapping relationship but also reduce the computational burden these surrogate models for solving high dimensional subsurface flow problems can be divided into physics based and data driven by whether there is actual physical meaning in the calculation process and some models will also include both of them asher et al 2015 razavi et al 2012 tang et al 2020 physics based surrogate methods usually adopt the strategy of ignoring or simplifying the physical model parameters establishing a simplified physical model and performing the forward calculation on such model thereby reducing the calculation cost the second type of physics based surrogate methods typically include projecting the governing equations into the low dimensional problem agarwal et al 2014 he and durlofsky 2014 jin and durlofsky 2018 simplifying the underlying physics or reducing numerical resolution he et al 2013a mo et al 2017 karhunen loe ve expansion laloy et al 2013 zhang and lu 2004 orthogonal decomposition gosses et al 2018 li et al 2013 multigrid method arrarás et al 2019 li et al 2020 tahmasebi and kamrava 2018 and multiscale finite element method he et al 2013b xie et al 2017 have been used to build efficient surrogate models these surrogate models compared with traditional models perform better in terms of computational efficiency but sometimes are limited in dealing with strongly nonlinear and high dimensional problems elsheikh et al 2013 li et al 2012 on the other hand data driven surrogate methods for groundwater and solute transport models have been extensively studied the most common of them include monte carlo method lu et al 2018 shi et al 2012 polynomial chaos expansion elsheikh et al 2014 gaussian processes ju et al 2018 nottingham and user 2011 bayesian method ait el fquih et al 2020 giordano et al 2013 and deep neural networks dnns jin et al 2020 mo et al 2019b tang et al 2021 zhong et al 2019 for example xing et al 2019 proposed ensemble surrogate model to solve the inverse contaminant source identification problems the surrogate model consists of kriging radial basis functions and least squares support vector machines while the adaptive metropolis markov chain monte carlo method is adopted to assign weights to the designated models xiao and tian 2020 introduced a surrogate model intending to estimate subsurface geological parameters of high dimensional inversion problems a set of subdomain linear models containing a few high fidelity model simulations were used to replace the original high fidelity model however the strong heterogeneity of natural formations requires a large number of stochastic degrees of freedom to represent which always results in high dimensional problems with strong nonlinear output most of the existing surrogate methods are severely affected by the input dimension as the input dimension increases the computational cost of the proxy method increases exponentially besides the strongly nonlinear output often requires more training sets which will further increase the computational cost with the rapid development in artificial intelligence techniques and graphics computing hardware in recent years more and more dnns methods are applied to construct surrogate models and solve the problem of high dimensional inversion in subsurface systems mo et al 2019b 2019a tang et al 2020 zhong et al 2019 characterization of small scale porous media kamrava et al 2021 2020 2019b 2019a and large scale subsurface modeling and predictions bai and tahmasebi 2021 2020a 2020b dnns methods have demonstrated good robustness and generalization properties in image like data prediction and segmentation the task with high dimensional input and output fields have been transformed into an image to image regression problem in deep learning based surrogate models compared to the traditional surrogate method the large deep complex network architectures in the dnns surrogate method increases the ability to characterize high dimensional and strong nonlinear relationships between inputs and outputs dnns method achieves accurate approximation of complex output through deep network architectures and a certain number of iterations mo et al 2019a b proposed to establish a dnns surrogate model using a combination of autoregressive strategy and dense convolutional encoder decoder network architecture which provides an accurate approximation for a 3321 dimensional hydraulic head and log gaussian distributed conductivity field trained with a limited amount of data to achieve the purpose of working with fewer training images and more precise segmentation u net architecture was developed by navab et al 2015 many studies have shown that deeper network structures generally have better prediction performance simonyan and zisserman 2014 zeng et al 2016 but at the same time it also brings the issue of vanishing gradients which will make the deeper neural networks harder to train glorot and bengio 2010 he et al 2016 proposed a deep residual learning architecture to deal with this issue later wen et al 2019 adopted a residual u net deep to achieve an accurate prediction of co2 saturation and pressure with layered heterogeneity and non gaussian distributed permeability more recently mo et al 2020 developed a deep residual dense convolutional neural network in which the ability of deep neural networks method in capturing input output relationships in subsurface systems with non gaussian log conductivity fields is demonstrated in this study we have developed an autoregressive residual u net ar runet surrogate model to predict the time dependent dynamic subsurface flow for a complex 2d channelized geological system in this research model the injected water flooded the square binary channelized formation from its left boundary the outputs of the flooding task are the saturation and pressure maps at four different times after water flooding here the autoregressive strategy refers to using the output of the previous time as the input of the neural network to predict the output of the current time rather than changing the network architecture the residual learning strategy has been proved to be an effective solution to ease the training of networks with quite deep architecture he et al 2016 zhang et al 2018 to do so a five layer deep residual unit has been added to traditional u net architecture developing a new residual u net architecture runet with a significantly improved performance for surrogate modeling of highly complex mappings the results show that after the autoregressive strategy was processed to the input and time dependent output data the prediction accuracy of the dnns method has increased significantly which outperforms other available methods for the problem of this paper the results of this paper therefore can help to enhance our understanding from the hydrological processes by considering various scenarios of the subsurface systems without the necessity of conducting the expensive methods for computational fluid dynamics thus one can assess the uncertainty in much more accurate and informed fashion when a variety of possibilities are considered using hydrologically realistic models e g heterogeneity geology the rest of the paper is organized as follows in section 2 we introduce the governing equations of the dynamic subsurface multiphase flow in channelized geological models in section 3 the deep residual u net with an autoregressive strategy is proposed in section 4 the ar runet method is applied for dynamic subsurface multiphase flow involving multiple realizations of a 2d binary channelized system in section 5 we discuss the advantage and limitations of our method by comparing it with other dnns methods finally in section 6 we summarize this work and provide suggestions for future investigations 2 governing equations in this research the surrogate model of dynamic subsurface multiphase flow in the channelized geological model is considered through which the relationship between the uncertainty of the subsurface fluid propagation and the properties of formation heterogeneity e g permeability and porosity was studied thus the governing equations for two immiscible fluids flow of mass conservation for each component are considered and given pruess et al 1999 1 t ϕ s j ρ j ρ j v j ρ j q j where subscript j denotes component with j w for wetting fluid phase and j n denotes non wetting fluid phase sj is phase saturation t is the time φ is the rock porosity ρ j is phase density qj is the source term combining mass conservation and darcy s law the pore space flow velocity vj can be defined 2 v j k ϕ k r j μ j p j where k is the absolute permeability tensor determined by the channelized geological model μ j is the phase viscosity krj is the relative permeability pj is the phase pressure the governing equation is also controlled by the sum of the saturation of the wetting phase and the non wetting phase equal to 1 the pressure of each phase is related to the capillary pressure pc 3 p n p w p c s w where pn and pw are the pressure of the non wetting phase and the wetting phase respectively as a function of saturation in this work the computational domain is a 2d geological model without considering the influence of gravity acceleration besides molecular diffusion and hydrodynamic dispersion are also not included in our model 3 methodology 3 1 deep convolutional neural networks dnns method refers to a network with multiple hidden layers generally each layer is composed of linear neurons and nonlinear activation functions the output of the lth layer of an n layer network can be expressed as 4 z l η z l 1 σ w l z l 1 b l where the superscript l denotes the layer index z l 1 and z l are the input and output of the lth layer l 0 1 2 3 n respectively w l and b l denote the linear weight matrix and bias vector the process of training with dnns is the process of finding the appropriate linear weight matrix w l and bias vector b l corresponding to all hidden layers and output layers letting all the calculated outputs from the input training samples by dnns method be as close as possible to the real outputs σ is the nonlinear activation function the sigmoid function the rectified linear unit relu the hyperbolic function and the softplus function are the common activation functions which need to be selected according to the characteristics of the training set baldi 2014 in this research the bias term b l has not been taken into consideration thus the relationship could also be written as 5 z l η z l 1 σ w l z l 1 a deep and complex network structure coupled with the fully connected characteristics often leads to a large number of network parameters the fully connectedness of dnns methods indicates that each node is connected to all nodes of the previous layer the fully connected characteristics always make them prone to overfitting therefore the deep convolutional network architecture is used to reduce the network parameters because the convolutional neural network explicitly considers the local spatial dependence of the input data it is particularly suitable for processing image data the convolutional neural network uses a specific filter also called the kernel function to establish a sparse local connectivity model between neurons in adjacent layers and achieve local spatial correlation each neuron is only connected to only a small region of the input area organized into feature maps specifically when the input x is a two dimensional image with width w and height h x r h w a convolutional layer η is obtained by employing a series of q 1 nf kernel function k f r m n the feature value η i j q x i j at the location i j after the convolution transformation can be expressed as 6 η i j q x i j p 1 m q 1 n x m n k f i m i n after the convolution calculation input x r h w of each layer will generate the nf layer feature maps the relationship of input size sinput h w of output size soutput h w is given by dumoulin and visin 2016 7 s o u t p u t s i n p u t 2 p k f s 1 where the two important parameters determined the size of the output is the stride s and the zero padding p stride s is the distance between two successive moves of the kernel function and the zero padding p indicates filling the boundary of the input image with zeros to ensure that the information on the boundary is saved in the feature maps is the floor function fig 1 is an example of feature map calculation of two dimensional image with size of 7 7 first the way the convolution function performs is shown by the calculations conducted for one pixel cell then the corresponding relations and the calculation process are demonstrated including a convolution layer a batch normalization layer bn and a rectified linear unit relu bn is to standardize the input data so that the output data has a mean of 0 and a variance of 1 the logic behind relu is to change all the negative values to zero max pooling with the kernel size of 2 2 and the stride s of 2 is used as a downsampling operator which reduces the size by the order of 1 2 the process of training with dnns is the process of finding the appropriate linear weight matrix w l namely the convolution kernel 3 2 autoregressive residual u net ar runet in this study an autoregressive data processing strategy is adopted before dnns training to improve the performance of the dnns method in predicting time varying saturation and pressure in a groundwater problem the residual u net architecture was selected in this surrogate model it transforms the surrogate modeling task into an image to image regression problem to take advantage of the great characteristics of convolutional networks in image processing in the following subsections the image to image regression strategy the residual u net architecture and the autoregressive data process have been briefly introduced 3 2 1 modeling as image to image regression in the traditional surrogate models the goal of the model is to find the solution of the governing equation within the rectangular range of size h w where hand wdenote the grid numbers along the two different axes the speed of this solution will be greatly reduced when solving high dimensional problems in the surrogate model based on dnns the input data and output data are regarded as images then through multiple iterations of the complex neural network the error is reduced and one can achieve accurate prediction of output images in the image to image regression model the input output relationship of the 2d regression task can be described as the following mapping 8 λ r d i n h w r d o u t h w where λ denotes the mapping relationship from input to output din and dout are the dimension of the input and output images in recent years with the development of gpu and other computing equipment it is easy and fast to calculate many large matrices and images at one time which is also an important factor for the explosive development and application of deep convolution neural network technology it has been proved in many studies that the use of deep convolution network for predictions in subsurface systems has excellent performance mo et al 2019a tahmasebi et al 2020 tang et al 2020 xiao and tian 2020 when the input and output are 3d data one can consider the 3d input as a series of images of size h w at different depths to train 3 2 2 residual u net architecture in this study inspired by the deep residual learning and u net a residual u net method was proposed for conducting a surrogate modeling the u net architecture has a successive contracting path to capture feature maps and a sequential symmetric expanding path to increase the resolution of the output in the contracting path there are a total of four downsampling processes each time downsampling the number of feature channels will be doubled and the length and width of feature maps are reduced to half before each downsampling there will be two consecutive convolution blocks to capture feature values each convolution block contains a convolution unpadded convolutions a batch normalization layer bn ioffe and szegedy 2015 and a rectified linear unit relu nair and hinton 2010 fig 2 c in the subsequent symmetric expanding path the feature maps will be restored to the original input images size after four times of upsampling processes that will halve the feature channels while double the size of feature maps in order to achieve precise localization high resolution features from the contracting path on the left side will be conserved and combined with the upsampled output on the right side at the final layer a 1 1 convolution is used to convert the feature maps into output images of different channels in u net architecture downsampling is realized by a 2 2 max pooling operation and upsampling is formed by a 2 2 upsampling and 3 3 convolution up conv in order to stitch the output segmentation map seamlessly the deconvolution is not adopted to replace the upsampling process which helps to avoid the influence of the grid effect this runet architecture has the same symmetric contracting and expanding path networks as the u net architecture with an additional residual network the detailed runet architecture is shown in fig 2 a the yellow arrow is a convolution block composed of a 3 3 convolutional layer a bn layer and a relu layer which can extract and output the feature map of the same size as the input image the green arrow is a 2 2 max pooling operation to extract the downsamped feature map whose length and width are half of the input image the blue arrow is a convolution block consisting of a 3 3 up conv layer a bn layer and a relu layer which can get an upsampled feature map the grey arrow represents a copy and concatenate operation using which the feature maps white box from the left contracting path are concatenated with the right feature maps blue box the residual network purple arrow consists of five residual convolutional blocks see fig 2 b each residual convolutional block consists a 3 3 convolutional layer a bn layer a relu layer another convolutional layer another bn layer an identity shortcut and another relu layer after the sum process as shown in fig 2 d 3 2 3 autoregressive strategy for time dependent process since the emergence of autoregressive for fitting the prediction model five decades ago akaike 1969 it has been widely used in various fields the autoregressive model is used for predicting when there is some correlation between values in a time series forecasting behavior is generally based on past behavior the relationship between the input and output in autoregressive can be expressed as 9 x t i 1 t φ i x t i ε t c where x t and ε t denotes the variable observed and white noise at time t respectively c is a constant item ϕ is the autocorrelation coefficient it can be written into 10 φ i c o v x t x t i v a r x t which is the ratio of covariance to variance the autocorrelation coefficient of the data is the key to determining the predictive effect of the autoregressive model when using the autoregressive model the autocorrelation coefficient is generally required to be greater than 0 5 in this work the output saturation or pressure maps are regarded as images at different time steps nt 4 thus it can be written as 11 y y 1 y 2 y 3 y 4 the relationship between the model input x and output ycan be described as a function y j j 1 n t f x j which is shown in fig 3 a this problem can be considered as a network system consisting of one input channel x and nt time varying output channels y j j 1 n t in fact in the series of time dependent output y j j 1 n t the output at the previous time step is the intermediate state of the output at the next time and there is a good autocorrelation between the output s elements y j j 1 n t taking n forward models into consideration the training data can be expressed as 12 x i y j i j 1 n t i 1 n putting the training data into the neural network η the relationship will be written into 13 y j i j 1 n t i 1 n η x i i 1 n where y j i j 1 n t is the output of the ith forward model time step j 1 nt forward model number i 1 n xi is the input of the ith forward model once the autoregressive strategy is adopted the previous output will be used as the next input the autoregressive time varying model is proposed as following 14 y j f x y j 1 in this model the current output yj j 1 nt is a function of the time independent input x and the output at the last time step y j 1 where y 0 is the initial state therefore the relationship map between the autoregressive model input x and output y i i 1 n t can be depicted as shown in fig 3 b in the autoregressive network the training samples of n forward model can be organized as 15 x y j 1 y j j 1 n t m 1 n x i y j 1 i y j i i 1 j 1 n n t when the autoregressive method is considered in the dnns network η it is a way of data processing before dnns network training thus the network relationship can be expressed as 16 y j i i 1 j 1 n n t η x i y j 1 i i 1 j 1 n n t y m m 1 n n t η x m m 1 n n t where y j i and y j 1 i represent the output at the jth and j 1 th time step of the ith forward model respectively time stepj 1 nt forward model number i 1 n xi is the input of the ith forward model the train number and size of training data of the ar dnns model is nt times of common dnns model which leads to its higher accuracy and longer training time the calculation cost in the whole process is still relatively cheap compared to the forward model this aspect will be discussed in detail in section 5 3 2 4 networks training in the network training process y η x θ a series of the network parameters θ is determined to achieve an accurate approximation of the trained output y m m n n t the regularized l 1 norm loss function is selected during the training process combined with the cost function representing by 17 j θ 1 n n t m 1 n n t η x m θ y m α 2 θ t θ where α is a regularization coefficient also called weight decay the stochastic gradient descent sgd is used to solve the cost function in eq 17 as an optimizer expressing as 18 θ j θ 1 n n t m 1 n n t θ η x m θ y m the cost of this calculation is n nt as the training set n nt of data increases each step of the calculation will consume a relatively long time therefore the minibatch strategy is adopted each time only a small part of the sample b x m y m m 1 m b is drawn from the training set mb is the number of each minibatch process generally speaking mb is less than 100 which is determined by the size of the training image and the computing device the gradient estimate of each minibatch can be expressed as 19 g 1 m b θ m 1 m b η x m θ y m when using the minibatch strategy the gradient descent in the sgb algorithm is as follows 20 θ θ ε g where ε is the learning rate is a key parameter of the sgd algorithm adam kingma and ba 2014 a commonly used optimization algorithm with an adaptive learning rate and a kind of sgd algorithm was selected in this study 3 3 performance metrics two metrics are used to quantify the performance of the dnns surrogate model namely the coefficient of determination r 2 and the root mean square error rmse 21 r 2 1 m 1 n c y m y m 2 2 m 1 n c y m y 2 2 and 22 r m s e 1 n c m 1 n c y m y m 2 2 where ym is the value of the forward model y m is the value of network predictions y is the average value of ym expressed as y 1 n c m 1 n c y m r 2 is a measure that reflects the difference between the predicted value and the model distribution the closer it is to 1 the better the prediction effect rmse is a metric that measures the deviation between the predicted value and the model value in a convergent algorithm as the number of measurements nc increases the rmse will decrease when rmse is closer to 0 it indicates that the prediction effect is better and the algorithm is more robust without the autoregressive method nt saturation or pressure maps at different times will be treated as a whole output ym is y j i i 1 n t i 1 n y r h w the number of dnns network nc called during the entire training process will equal to train number n after the autoregressive strategy is adopted saturation or pressure maps at different times will be used as output one by one ym is y j i i 1 j 1 n n t y r h w the number of ar dnns network will equal to n nt where n is train number nt is the time steps number in this study we consider the uncertainty associated with the input binary channeled geological map and quantify its effects on the output saturation and pressure fields the maximum value of the absolute approximation errors has been introduced to characterize the predicted result of the different methods and it can be expressed as 23 e r r o r max max y m y m it is a parameter that reflects the maximum error predicted by the algorithm in each output in this study there are a total of nt error max the two common statistical parameters mean and median are used to quantify this uncertainty without loss of generality they can be expressed as the mean 24 μ κ η x p x d x and the median 25 m e d i a n 1 2 η o x n n t 1 2 η o x n n t 1 2 where κ is the sample set p x is the probability distribution of input x in the sample set η is the relationship from input to output it represents error max in this research η o x is an ordered output list of n nt numbers and denote the floor and ceiling functions respectively a maximum error is not always a relatively good indicator of the prediction performance of an algorithm at a certain point thus the probability density function pdf of the surrogate model output value at constant observation location has been used to compare with that of the forward output the differences between them have been quantified by the coefficient of determination r 2 4 application in this section we introduce a 2d binary facies channelized model created by the cross correlation simulation ccsim method and conducted a multiphase flow for obtaining the saturation and pressure at different times the different predict performance of ar runet surrogate method has been compared with those for existing runet wen et al 2019 dense networks mo et al 2019a and ar dense mo et al 2019b deep neutral network surrogate methods 4 1 experiment setting 4 1 1 multiphase flow modeling in this research a subsurface multiphase flooding problem was studied as shown in fig 4 the binary facies channelized geological map lies in a horizontal plane is defined on a 128 128 grid with a grid size of 1m 1m and a thickness of 1m in each grid block the yellow cells indicate the advantageous flow channel sandstone with a porosity equal to 30 and a permeability of 2000 md while the blue block is the shale and its porosity and permeability are 2 and 0 01 md respectively there are 64 injected wells on the left boundary where the flooding fluid was injected at a rate of 0 2 m3 day the initial formation pressure is 250 kpa while the flooding pressure is 500 kpa the initial fluid saturation and the residual fluid saturation is 20 and 80 respectively the detailed simulation parameters are listed in table 1 in every forward model the saturation and pressure maps at four different times 10 days 40 days 100 days and 180 days were calculated which takes about 2 minutes on a xeon e3 1270 3 5 ghz cpu with 16gb ram 4 1 2 geomodel generation the dnns surrogate model requires a relatively large number of training multiphase flow simulations to achieve the accurate dynamic output states from the binary facies channelized geological maps the geomodel was expanded to four different training sample sets and a test sample set by the cross correlation simulation ccsim algorithm tahmasebi et al 2012 ccsim method can generate the realizations of a subsurface system with accurate conditioning and continuity it has been proven to be superior to the commonly used multiple point geostatistics method in terms of computational efficiency and quality in ccsim a higher order cross correlations function based on the correlations between the various parts was developed and used along a raster path to generate a new model that matches a given training image ti which has been detailed in tahmasebi et al 2012 2014 the cross correlations function was used to measure the similarity between the ti and overlap in the simulation grid and is expressed as 26 i j x 0 ℓ x 1 x 0 ℓ y 1 t i x i y i d e v t x y where ti x y denotes the value at point x y of the given image ti of size lx ly devt x y is the data event at point x y using template t i and j represent the shift steps in the x and y directions respectively fig 5 displays six random binary channelized models generated by ccsim with the size of 128 128 in the binary channelized geological maps the yellow dominant flow channels sandstones are randomly distributed in the simulated geological body and have good connectivity vividly simulating the uncertainty and connectivity of the underground geological body given these stochastic models each grid is assigned a specific porosity and permeability as mentioned above and dynamic output results are obtained through the forward model introduced in session 2 4 2 network design and data sets to illustrate the advantage and effectiveness of the autoregressive strategy in improving the prediction accuracy of networks for the time varying outputs of the subsurface multiphase flow models four different types of deep learning networks are trained and compared runet residual u net architecture see fig 2 a each time of training a series of different time dependent output are predicted without using the autoregression method to reorganize the data as eq 16 dense deep dense convolutional encoder decoder networks mo et al 2019 it shows a good performance in predicting the output concentration fields at different time steps from a time varying source and a log conductivity field ar runet autoregressive residual u net architecture the autoregressive strategy is used to process the input and output as in eq 16 ar dense autoregressive deep dense convolutional encoder decoder networks the input and output are reorganized by autoregressive strategy before training the l 1 loss function defined in eq 17 and 200 epochs are used in the above dnns networks to illustrate the convergence of the approximation error in the dnns networks four training sample sets obtained from n 200 350 750 and 1 000 model evaluations and a test sample set with 500 model evaluations are generated the output at t 10 40 100 180 days has been observed in the inverse problem which indicates that the number of observation points is nt 4 in runet and dense networks there are one input channel x r 1 128 128 the binary facies channelized geomodel and four output channels y j j 1 4 r 4 128 128 the dynamic saturation and pressure maps at the observed time in ar runet and ar dense networks in which the training data have been reorganized by eq 16 there are two input channels x i y j 1 i r 2 128 128 and one output channel y j i r 1 128 128 at time step j 1 2 3 4 indicated by forward model number i 1 n where y 0 is the given initial state the differences between the two networks without the autoregressive strategy i e runet and dense and the two autoregressive networks i e ar runet and ar dense are summarized in table 2 after adopting the autoregressive strategy the training data is split and the neural network are called more times with the same training set the detailed input output and the used times of the networks in training with n training sample sets are listed in the runet and ar runet network there a 4 layer successive contracting path to capture feature maps a 4 layer successive symmetric expanding path to increase the resolution of the output feature maps a five block residual network was added to u net architecture to capture the accuracy feature maps in the size of 8 8 finally there is an output convolutional layer that adjusts the output channel the detailed network architectures of runet and ar runet are shown in table 3 there are 29 convolution layers in runet and ar runet network architecture the number of network parameters is 128 918 404 in runet and 128 918 785 in ar runet in table 3 the output shape includes output channel number nc the height of the feature map h and the width of the feature map the size of output feature maps can be given by the values of input feature maps size convolution kernel size stride and zero padding the relationship defined in eq 7 the network structure of runet and ar runet is the same except that the convolutional layer in the input layer and the output layer has made some changes to deal with different training data in our proposed architecture the convolution kernel size stride and zero padding are the same in all convolutional layers conv2d except that in the output layers where a convolution layer with kernel size kf 3 stride s 1 and zero padding p 1 is used it is commonly used convolutional layers that capture feature maps without changing the output size bn denotes a batch normalization layer relu is a rectified linear layer maxpool2d denotes a downsample layer with kernel size kf 2 stride s 2 which reduced the size of the feature map by half upsample is a upsample layer with scale factor fs 2 after this process the width and height of the feature maps will be doubled the last output layer is the convolution layer with kernel size kf 1 stride s 1 and zero padding p 0 this convolution layer can also keep the output image and input image the same size and a smaller kernel size helps improve the output resolution 5 result in this section we assess the performances of runet dense ar runet and ar dense networks in approximating the time dependent dynamic subsurface multiphase flow predictions of the 2d channelized geological system different performance metrics are used to evaluate the dnns surrogate method 5 1 approximation accuracy assessment of surrogate model the proposed four different dnns architecture and loss function discussed in section 4 were conducted sample sets with 200 350 750 and 1 000 realizations were trained and tested in a sample set with 500 realizations the training processes cost range 11 230 minutes using a nvidia tesla p100 gpu after reorganizing the data with the autoregressive strategy the number of training samples will be increased to nt times the number of the forward model runs it increases the training time at the same time note that we are only able to access a portion of the gpu memory since it is operated in a shared high performance computing hpc system after training of 200 epochs the prediction results of different surrogate models show very clear differences taking the output of dynamic saturation distributions as an example the r 2 score and rmse for the test set for each surrogate with different training data set sizes are shown in table 4 and fig 7 the result of dynamic pressure prediction is basically the same as this the autoregressive strategy will greatly improve the prediction results of the underground time dependent multiphase flow of the dnns surrogate model in this research the autoregressive dnns surrogate method ar runet and ar dense can achieve r 2 score higher than 0 980 and rmse lower than 0 020 with only 200 train samples in fig 6 shows the ar runet surrogate method with only 200 training samples achieve the highest r 2 value of 0 986 in predicting dynamic saturation distributions with a 128 128 dimensional stochastic input with more training samples put into the dnns network all these surrogate methods have achieved better predictive performance and the difference between the predictive performance of these methods will gradually decrease when increasing the train samples size to 1 000 the ar runet model achieves a r 2 value of 0 998 the results show that the use of autoregressive strategies when training samples are limited can improve the performance of dnns alternative models in underground multiphase flow prediction the performance of the networks is further compared in figs 6 9 which respectively show the predictions of dense runet ar dense and ar runet using n 400 training samples for the dynamic saturation pressure maps at t 10 40 100 and 180 days part a reflects the prediction performance of saturation at different times and part b shows the prediction of formation pressure at different times for comparison the forward model predictions are shown in the first row of each plot marked as y the second row of each plot is the result of dnns surrogate model predictions denoted by y y y denotes the difference between the predictions of the forward model and the dnns method each plot has four columns which represent the saturation pressure outputs at 10 40 100 and 180 days fig 7 shows that with the process of flooding the distribution of output becomes more and more complex and the accuracy of dense network prediction gradually decreases there are relatively large approximation errors between the output of the forward model and dense network output at time equals to 180 days especially near the dominant channel of the geological body mo et al 2019 used an additional weight loss to improve the approximation performance around the only contaminant source location however in the training of the binary geological channel map in this study each point at the boundary of the dominant channel is equivalent to a source and there will be countless randomly distributed sources in each training image therefore the additional weight loss method will be efficient here fig 8 shows the predictions of the residual u net network with 200 model evaluations runet demonstrated a good ability to capture the value of the boundary with a high gradient and its prediction performance is significantly better than the dense network in order to improve the dnns surrogate model s approximation performance in addition to optimizing the dnns network we can increase the number of training samples or use an autoregressive strategy when the training data is limited the autoregressive strategy can have the same effect on improving the predicted results after the autoregressive method is adopted the training data are reorganized as eq 16 and both the ar dense and ar runet methods performances better than before as figs 9 and 10 show fig 9 indicates there are significantly reduced approximation errors in the predicted saturation result using the ar dense method while the pressure predictions with a higher gradient at the boundary still have the relatively larger approximation errors than other regions at the boundary fig 10 shows the predicted result by the ar runet method compared with the results of the forward model the errors of the ar runet predictions are relatively small and evenly distributed and there is no clear high error area in the predictions in this research the dynamics of subsurface multiphase flow problem has been transferred into a task of image to image regression by the dnns surrogate method first the results in this session indicate that dnns alternative models have strong flexibility and robustness when solving high dimensional input output problems with strongly nonlinear characteristics second different dnns have distinct ways of capturing features in the training of input output relationships and choosing appropriate dnn based surrogate models for different problems can achieve better prediction performance in this research there are many step change boundaries in binary geological maps compared with the dense network the residual u net network runet has a better ability to capture boundary features so the runet method has a better performance in handling the same high dimensional time dependent prediction problem in addition such surrogate dnns models are much faster than the forward model each prediction of an 128 128 image will take less than a second on a recent gpu device finally the autoregressive strategy has advantages in solving time varying output it uses the correlation between different time outputs to reorganize and expand the training data and achieve higher training performance with relatively limited train samples through nonlinear projections of the input into high level coarse feature maps in an image to image regression process the dnns surrogate method handles the problem of the high dimensionality of input implicitly while providing an excellent capability to deal with complex output responses these developed dnns surrogate methods in this research were able to effectively provide relatively good approximation with limited training samples 5 2 uncertainty modeling in this section the effectiveness and efficiency of the ar runet and ar dense surrogate models are analyzed and compared the maximum value of the absolute approximation errors error max defined as eq 23 of the ar runet and ar dense method have been calculated and compared in fig 11 the probability density function pdf distributions of error max in the predicted saturation maps with different training samples n 200 350 750 1 000 of ar runet and ar dense method are compared it should be noted that in each subplot 2 000 predictions of t 10 40 100 180 days are included i e the output saturation maps at nt 4 different time steps in ntest 500 test samples the maximum absolute error error max of the prediction is a useful parameter that reflects the accuracy of the prediction the smaller the value of error max the higher the approximate accuracy as can be seen in fig 11 the values of maximum absolute error error max are significantly reduced when the number of training samples increases from 200 350 750 to 1 000 indicating that the prediction performance has become better besides the ar runet method has always had smaller error max values than ar dense with different train samples in table 5 the median and mean of pdf distributions of error max drafted in fig 11 are calculated to make the difference of prediction performance between ar runet and ar dense clearer table 5 shows the ar runet method has smaller median and mean values of the error max than the ar dense method as the number of training increases the median and mean of error max decreases as for the distribution of error max in pressure prediction there is a similar result as in fig 11 the assumed formation condition in this study are good enough and the dominant channel has porosity and permeability of 30 and 2000 md respectively after flooding the pressure in the dominant channels reached a balance soon which results in a pressure sharp change region at the boundary between the two geological media with different physical properties the pressure maps with a sharp change random boundary between different geological media make the prediction complicated in fig 12 the probability density function pdf distributions of error max in the predicted pressure maps with different training samples n 200 350 750 1 000 of ar runet and ar dense methods are compared the values of ar runet s maximum absolute error error max of predicted pressure maps distributes within 0 2 while the error max of the ar dense method always has a relatively wide distribution range 0 0 5 since it has a relatively poor prediction performance when dealing with step change boundaries as shown in fig 9 b in table 6 the median and mean of pdf distributions in pressure prediction of ar runet and ar dense method are shown the result of ar runet also has clear advantages in predicting the dynamic saturation distribution after flooding compared to ar dense in this study the test set consists of 500 random 2d binary geological models generated by the ccsim method accompanied by the saturation and pressure distributions at different observation times as the number of samples in the training set increases the prediction performance in the test set becomes significantly better in fig 13 the output saturation maps of a ar dense and b ar runet surrogate models predicted with 1 000 training samples are compared both dnns surrogate methods have good predictions with small approximation errors to make the comparison more specific an observation point with location of 64 64 is used to observe the prediction performance of ar runet and ar dense on saturation values in 500 test samples fig 14 shows the pdfs of saturation at this location at 10 40 100 and 180 days comparing the pdfs at different times with the flooding process the saturation value at the observation point becomes higher reflecting the diffusion of different fluids after flooding the saturation value around 0 20 reflects the initial saturation when the observation point is mudstone these mudstone observation points have no significant increase in saturation after flooding in contrast at time t 180 days the saturation output of the forward model becomes more complicated and the ar dense method represents lager approximation errors of the complex saturation distribution fig 15 shows the output pressure maps of a ar dense and b ar runet surrogate models predicted with 1 000 train samples are compared with the forward model output in the initial conditions of the control equation of formation pressure a constant pressure control point is set at the location 128 64 the ar dense method does not perform well for the prediction near the constant pressure control point while ar runet can handle every prediction more perfectly during this process in fig 16 the pdfs of pressure at the location 64 64 at 10 40 100 and 180 days indicate the same problem presents in fig 15 in order to highlight the difference between the ar runet and ar dense methods the consistency coefficient r 2 calculated based on the prediction performance of figs 14 and 16 is listed in table 7 in table 7 as the time after flooding gradually increases from 10 40 100 and 180 days the facies become more complex and the predicted performance is on a downward trend for the output of multiple flows at the observation point in 500 test samples including saturation and pressure ar runet has always performed better than ar dense the r 2 value of pressure pdfs is higher than that in saturation prediction in fact the formation pressure quickly reaches equilibrium after the flooding and the values of pressure will be more concentrated 6 conclusions surrogate methods are used widely to alleviate the large computational burden associated with the subsurface multiphase flow including in forward and inversion models however the high dimensionality and strong nonlinearity always bring difficulties to the application of surrogate models in this study a new dnn based surrogate method was proposed for efficiently solving the high dimensional subsurface multiphase flow problem the autoregressive residual u net ar runet architecture is adopted to transform the task into an image to image regression problem reduce the computational cost and avoid the curse of dimensionality the autoregressive was particularly used to reconstruct the training data before putting them into the deep learning network architecture in which the time dependent output at the previous time step is regarded as the input to predict the current output with the autoregressive process the original dynamic multiphase flow prediction results output at one time are split into nt separate ones nt is the time step number of time dependent output the autoregressive strategy enables the network to efficiently and accurately capture the complex relationship between the inputs and time varying outputs with limited train samples to further address the dnns with intensive data a residual u net architecture with a 4 layer successive contracting path to capture feature maps and a 4 layer successive symmetric expanding path to increase the resolution of the output feature maps between them a 5 layer residual network is added the performance of the proposed method is demonstrated using a complex subsurface multiphase flow prediction problem with a 128 128 the results indicate that the surrogate models based on dnns use the image to image regression method to solve the surrogate task and have strong applicability and robustness with only 1 000 train samples the four mentioned dnn methods could all achieve correlation coefficients higher than 0 9 the ar runet method demonstrated the best performance in terms of providing the accurate approximations of time varying outputs whether it is a more discrete saturation output or pressure output with step changes the application of the method in the addressing subsurface multiphase flow task showed that the deep network based surrogate model can achieve comparably accurate results to those obtained with the traditional methods but with less computational cost thus our proposed method can be used as an alternative wherein the hydrological processes along with geology and heterogeneity are all considered in the same framework this technique provides a basis through which one can examine various hydro geological models without the necessity of conducting the full physics modeling which allows to quantify the uncertainty and make the future plans in a more comprehensive and realistic fashion although the proposed surrogate method based on dnn has shown a great potential when solving the task of subsurface flow problems the assumed heterogeneity of high dimensional systems in the current models is still relatively simple most of them are gaussian distribution or bimodal distribution the input based on the heterogeneous distribution of the actual geological model will put forward higher requirements for the dnn method credit authorship contribution statement zhihao jiang methodology data curtion writing original draft visualization investigation validation pejman tahmasebi conceptualization methodology validation writing review editing supervision zhiqiang mao validation writing review editing supervision funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this work was supported by hpc of the university of wyoming the first and third authors also appreciate of china scholarship council supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2021 103878 appendix supplementary materials image application 1 
