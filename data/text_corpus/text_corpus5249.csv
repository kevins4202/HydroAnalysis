index,text
26245,the development process for an environmental model involves multiple iterations of a planning implementation assessment cycle probabilistic programming languages ppls are designed to expedite this process with general purpose methods for implementing models efficiently inferring their parameters and generating probabilistic predictions probabilistic programming exists at the intersection of bayesian statistics machine learning and process based modelling and therefore can be of value to the environmental modelling community in this review article we explain how it can be used to accelerate model development and allow for statistical inference using more complicated models and larger data sets than previously possible specific challenges and limitations to employing such frameworks are also raised we provide guidance to help modellers decide whether incorporating probabilistic programming in their work may improve the efficiency and quality of their analyses keywords bayesian statistics parameter estimation uncertainty quantification probabilistic programming 1 introduction the widespread usage of mathematical models to represent and forecast environmental processes builds upon a rich scientific history of iteration between theory and practice box 1976 this procedure alternates between the generation of testable hypotheses about the natural world and the investigation of their validity via statistical analysis of data which may be incomplete or prone to random variation the environmental sciences present special challenges due to the complexity of the systems studied and hypotheses posed clark 2004 as well as the interactions between anthropogenic and natural causes modelling environmental dynamics consequently requires techniques from both physical and statistical sciences national research council 2001 while theory based models usually integrate detailed knowledge regarding the key processes involved empirical models are more appropriate for scenarios involving substantial uncertainty about either the structure or governing parameters of the system studied though scientific inquiry often involves alternation between both approaches box and youle 1955 regardless of the choice of model used model assumptions must be checked to ensure that the end product is an appropriate representation the specific logic encoded in the data generating process will invariably differ from case to case but much of the labor involved in estimation and validation can be spared by using general frameworks for model construction and assessment probabilistic programming languages ppls intend to unify a scattered and mostly disconnected landscape of models and algorithms by basing model structure and estimation on generic reusable components that can be applied in a wide variety of contexts ghahramani 2015 parameter estimation is automatic in a ppl in the sense that it is performed with general purpose algorithms designed to work on very broad classes of models with little tuning a central philosophy of probabilistic programming is that the act of writing down a model should be wholly divorced from its estimation and inference in a ppl assumptions about the world are encoded in a graphical model which depicts the relationships between variables in terms of a network within this network nodes correspond to model variables and edges correspond to dependencies which may be either stochastic or deterministic this formalism provides a unifying view of models with many interacting random variables wainwright and jordan 2007 in terms of modular substructures within the underlying graph with a well designed ppl the modeller is able to rapidly prototype and test new ideas by modifying and combining existing models this enables a more efficient way to explore multiple alternative model forms lahtinen et al 2017 there already exist some environmental modelling frameworks which share similar objectives such as code reuse and modularity e g oms3 david et al 2013 but are less focused on inference and optimization in many situations researchers stand to benefit from using a general purpose inference algorithm as compared to a case by case piecemeal application of one or several estimation algorithms written expressly for a single purpose optimization frameworks such as pest doherty et al 2010 provide sophisticated algorithms for model fitting but offer little facility for specifying model structure or performing a statistical critique of the parameter estimates probabilistic programming instead offers a unified modelling framework integrating model definition estimation and criticism for conventional statistical analyses process based modelling and deep neural networks among other modelling learning approaches despite their name ppls are embedded in a high level programming language some of the more recent frameworks are pymc3 python stan c with interfaces for several other languages and edward python every ppl includes functionality for declaring and using stochastic variables in arbitrary model structures goodman 2013 these are defined in terms of probability distributions and do not refer to known numerical quantities stored within computer memory but rather are used to build a probabilistic model of a system this model can then be used to generate simulated outcomes as well as to assess the likelihood of observed outcomes in aid of parameter inference optimization and inference algorithms are frequently the most time consuming and difficult part of the modelling process fischer and schumann 2003 and ppls are designed to streamline this process by including user friendly community standard implementations of these algorithms a ppl may include additional utilities for scoring and ranking models or saving and loading previously computed parameter estimates some readers may be familiar with older statistical modelling frameworks such as bugs lunn et al 2000 or jags plummer 2003 to this audience we use this review to communicate how more recent ppls can enable inference for a larger class of models and facilitate big data analyses for those who have no familiarity with this subject we hope to show why implementation of environmental models in a ppl may be advantageous of course not every ppl is suited for every task in this review we focus on those which have a particular ability to represent mechanistic environmental processes and statistical representations we begin with an overview of the probabilistic programming concept followed by a brief history we then present a typical workflow of probabilistic programming next we discuss the algorithms typically used for parameter estimation and uncertainty quantification as well as some algorithmic developments that have dramatically enhanced the functionality of modern probabilistic programming frameworks we then cover the benefits of employing ppls in an environmental modelling project and present some relevant examples of their application within environmental modelling we conclude by discussing remaining challenges and future directions of the field as well as potential avenues of research 2 overview of probabilistic programming the term probabilistic programming is expansive and includes software intended for diverse aims and with varying limitations probabilistic programming can be loosely defined as the application of deterministic computer programs tightly integrated with stochastic elements or constructs for random choice goodman 2013 with the ability to be conditioned on observed data however a key feature of probabilistic programming in all cases is the separation of model structure and estimation in virtually all probabilistic programming frameworks a user defines the model structure by calling functions that add nodes to the underlying graphical model which represents a joint probability distribution for the problem at hand these variables can be scalar valued matrix valued or representative of richer stochastic processes such as gaussian random walks or autoregressions deterministic transformations can then be applied to these random variables to create derived quantities these variables and transformations must be specified in sufficient detail that the model s likelihood function can be evaluated and employed in either monte carlo based techniques or faster optimization centric approaches used to estimate model parameters while early research in probabilistic programming initially grew out of theoretical work in computer science kozen 1981 solomonoff 1978 efforts to develop practical general purpose inference engines in tandem with reusable model components took place towards the late 1980s and early 1990s the resulting combination of software packages and frameworks appears to have emerged from the confluence of research communities with two distinct backgrounds and objectives the first koller et al 1997 pearl 1987 was centered around researchers concerned with artificial intelligence and probabilistic reasoning while the second was largely composed of statisticians who desired flexible and widely applicable inference packages the latter group was aided immensely by the translation of markov chain monte carlo mcmc from the physics literature into mainstream statistical practice gelfand and smith 1990 3 workflow using a probabilistic programming framework for parameter estimation or uncertainty quantification involves three main steps fig 1 first the structure of the model is defined by specifying a network of random variables and their interconnections these connections may be deterministic or stochastic relations the model is declared in a bottom up fashion prior distributions or fixed values are designated for basic model parameters and derived quantities are then declared as functions of these initial parameters second an inference engine is specified and either a sampling or optimization based approach is applied with little or no tuning of the inference algorithm this is similar in spirit to other community modelling efforts such as summa clark et al 2015 in which the model logic reflecting the structure of the data generating process is cleanly separated from the numerical solvers used for simulation finally summaries of the posterior distribution are generated and diagnostic statistics of the estimation or optimization process are assessed jakeman et al 2006 once a generative process is specified and the posterior distribution is estimated it is always possible to then draw simulations of imaginary data which can be invaluable in assessing whether the model generates realistic or plausible outcomes this workflow places a strong emphasis on description and understanding of the data generating process fig 2 by specifying the form of this process in a graphical model it is hoped that the modeller is made aware of issues that might not have been identified with a purely test based approach furthermore it reduces the tendency to fall back to specific model forms that are familiar but unsuited to the problem at hand 4 algorithms while the methodology of estimation in probabilistic graphical models is nearly always described as bayesian in some way using a probabilistic programming language does not necessarily require agreement with a bayesian interpretation of probability as subjective degrees of belief clark 2004 however ppls readily exploit the algorithms developed for bayesian inference imcluding markov chain monte carlo mcmc an in depth review and explanation of mcmc is beyond the scope of this work and excellent tutorials can be found elsewhere andrieu et al 2003 brooks et al 2011 robert and casella 2011 it is relevant to this work insofar as it is a method for estimating the parameters of an arbitrary computational model provided that the model can be run for hundreds to thousands of iterations within a reasonable amount of time the core idea behind mcmc is that the model s likelihood function p x θ and prior distribution p θ can be used to construct a trajectory through the space of all possible parameter settings θ we note that the definition of parameter as it pertains to this discussion is quite broad for example linear regression coefficients are parameters as are the site specific soil porosity or hydraulic conductivity values in a distributed hydrology model in the case of inverse modelling the unknown quantities to be estimated are the initial conditions or forcing leading to the observed data and these unobserved quantities are also be described as parameters in a statistical sense the likelihood function expresses the conditional probability of the observed data x given the parameters θ using bayes rule we can express the posterior probability of the parameters given the data as p θ x p x θ p θ p x where p θ denotes a prior distribution over the variables however the evidence term p x is difficult to compute as p x θ p x θ p θ and the space of possible parameter sets is very large we can therefore only evaluate the posterior density π θ x p x θ p θ while this density is not normalized and therefore not useful for directly obtaining the posterior probability without normalizing by p x it can be used to guide a markov chain s θ 1 θ 2 through the parameter space provided that this chain satisfies certain conditions brooks et al 2011 it is also an approximation converging to the desired posterior after many samples have been drawn many different varieties of mcmc algorithms exist due to the wide range of possible proposal mechanisms used to suggest new candidate parameter sets for parameter settings with high likelihood the model is a good match for the data this allows us to find optimal parameter settings using mcmc computationally intensive models may not be good candidates for this procedure because mcmc usually requires upwards of 103 model simulations to adequately explore the posterior distribution over the parameters in some applications i e vrugt et al 2008 it is not unusual to run 105 106 simulations to ensure that a sufficient number of samples is drawn gibbs sampling a form of mcmc which iteratively updates a single parameter at a time is the centerpiece of the bugs bayesian analysis using gibbs sampling software this package along with its microsoft windows specific variant winbugs lunn et al 2000 and open source counterpart jags plummer 2003 has found widespread usage indeed this series of software represents one of the earliest examples of a bayesian programming environment that was widely used by researchers outside the fields of statistics and artificial intelligence one of the key features of this software is its object oriented approach which allows for extensions such as a graphical user interface geospatial modelling methods and a simple model comparison and selection scheme lunn et al 2009 numerous other probabilistic programming languages incorporating mcmc were developed between 1990 and 2010 some of the more notable ones include prism sato and kameya 1997 church goodman et al 2008 and infer net while the proliferation of mcmc based methods in applied statistics enabled bayesian parameter estimation for a broad class of models each algorithm had its disadvantages gibbs sampling requires the ability to compute conditional distributions that express the probability of a single parameter given all other parameters this may not be possible for complicated models the gibbs sampler is actually a special case of the metropolis hastings algorithm hastings 1970 but the latter is frequently used to refer specifically to a procedure in which many parameters are updated simultaneously and the conditional distribution is not evaluated this variant of the metropolis hastings algorithm only requires the calculation of the model s posterior density but it also generates proposals according to a random walk typically converging to the posterior in a number of iterations proportional to the number of model parameters roberts et al 1997 this can lead to unreasonably long convergence times with increasing numbers of model parameters though the metropolis algorithm can be an effective tool for the calibration of simple environmental models kuczera and parent 1998 several other bayesian estimation methods have been included in probabilistic programming languages including other varieties of mcmc such as sequential monte carlo doucet et al 1999 there are also alternate estimation frameworks such as approximate bayesian computation marin et al 2012 the suitability of these methods for inference tend to be problem and model specific and the models tend not to be used as the main workhorse of inference in any existing probabilistic programming languages fortunately the kernels from different mcmc methods can be chained or applied to smaller subcomponents of the model while still satisfying the requirements of a valid estimation procedure johnson et al 2013 evolutionary monte carlo and sequential monte carlo smc have found abundant use in the hydrology and earth sciences community jeremiah et al 2012 pathiraja et al 2018 but are not incorporated as the primary inference engine in any pp framework pymc3 and webppl both include an evolutionary algorithmic sampler such as differential evolution braak 2006 but more sophisticated variants such as dream vrugt 2016 are not yet incorporated for models wholly or partially defined on continuous random variables it is possible to calculate the gradient of the posterior density θ π θ x with regard to some or all of the model parameters this can be very helpful because ascending the gradient corresponds to moving towards higher values of π θ x and a better model fit this can also lead to more rapid convergence as well the metropolis adjusted langevin algorithm a version of metropolis hastings incorporating an up gradient bias term in its kernel converges much more rapidly than the ordinary metropolis algorithm for high dimensional models pillai et al 2012 hamiltonian monte carlo neal 2012 is available in several ppls and similarly is highly effective due to its use of the posterior gradient unfortunately it is not feasible to calculate the gradient by hand for many complicated model structures symbolic differentiation is not an ideal alternative as the number of terms in the derivative will quickly grow under repeated application of the chain rule a numerical analysis of the gradient using finite differences is likely to be unstable and inefficient in when applied to models with many parameters fournier et al 2012 a fourth option is to use automatic differentiation this is an algorithmic approach to evaluating derivatives of arbitrary programs that borrows elements from both symbolic and numerical differentiation automatic differentiation involves recursively applying the chain rule and only storing intermediate numerical values rather than a large number of symbolic terms griewank and walther 2003 while not all ppls table 1 make use of automatic differentiation those that do use it incorporate separate and distinct software packages developed specifically for this task edward for example relies on tensorflow a library developed by google for machine learning while pymc3 is built on top of theano in both of these cases the actual model specification in a probabilistic programming framework consists of adding variables to a computation graph which can then be traversed either forward for normal simulation or backwards for automatic differentiation a secondary benefit of using an external computational library for calculations is that it can optimize the computation graph to eliminate redundant function evaluations fig 3 and automatically compile the designated model in a slower dynamically typed programming language such as r or python into a static typed language such as c c we view the incorporation of automatic differentiation as a major step in the evolution of probabilistic programming for two primary reasons first as mentioned in the previous paragraph it allows the implementation of more efficient mcmc samplers that can use this gradient information to navigate a high dimensional parameter space second automatic differentiation is a staple of modern deep learning methods lecun et al 2015 involving neural networks and consequently the insights and computational techniques designed to improve that class of methods can be brought to bear for difficult estimation problems several ppls introduced since the late 2000s such as stan carpenter et al 2017 pymc3 salvatier et al 2016 and edward tran et al 2016 incorporate automatic differentiation each of these ppls incorporates at least one efficient gradient based mcmc algorithm such as hamiltonian monte carlo or its extension the nou turn sampler hoffman and gelman 2014 and one gradient based non mcmc approach such as variational inference kingma 2017 kucukelbir et al 2015 intended for probabilistic models conditioned on large amounts of data while mcmc approximates the integral p x θ p θ p x for the desired probabilistic model an alternative approach is to identify a simpler model with fewer cross parameter correlations such that the new model is easily optimized to approximate the desired posterior of the original model this strategy describes a wide range of algorithms that fall under the umbrella of variational inference beal and ghahramani 2003 blei et al 2017 a key advantage of variational inference is that by avoiding repeated sampling it is both deterministic and relatively fast although it does not share mcmc s asymptotic posterior convergence guarantees furthermore vi tends to underestimate the variance of model parameter estimates blei et al 2017 which can be a major deficiency when performing uncertainty quantification variational methods compare most favorably with mcmc in situations requiring large amounts of data in which vi can be orders of magnitude faster blei et al 2017 automatic differentiation again plays a key role here by providing a model specific formula for optimizing the variational objective without expert knowledge on the part of the user kucukelbir et al 2015 in theory this simplifies parameter estimation for big data problems in models from multiple disciplines including statistics the earth sciences and machine learning by providing a shared interface table 1 lists commonly used probabilistic programming languages along with several attributes describing their usability in the next section we review recent work that employs probabilistic programming in environmental modelling for environmental modellers we recommend using stan or pymc3 as these are both relatively mature with a straightforward model syntax but incorporate modern algorithms for bayesian inference such as hmc or nuts 5 benefits to environmental modelling the positive impacts of utilizing a probabilistic programming language can largely be broken down into two categories 1 effects that enhance the quality reproducibility and generality of scientific analysis and 2 features of ppls that reduce the effort required to analyze and extract knowledge from data here we will discuss provide a brief overview of these impacts which are addressed more extensively in bishop 2012 tran et al 2016 and carpenter et al 2017 5 1 integrated uncertainty new challenges posed by complex environmental systems necessitate analyses integrating heterogeneous sources of data and models from different disciplines laniak et al 2013 integrated environmental modelling is becoming an increasingly important part of the environmental sciences since the ongoing confluence of ecological economic and environmental processes demands a holistic treatment jakeman and letcher 2003 univariate or low dimensional uncertainty analyses of model components are acceptable when the coupling between subsystems is weak or nonexistent but such an approach can obscure key system dynamics or misrepresent the probabilities of joint outcomes unless a comprehensive integrated uncertainty analysis is performed bayesian models naturally address these issues and while a ppl is not strictly necessary to perform these analyses the process of joint uncertainty quantification is greatly expedited by their usage furthermore coupled systems will invariably have more parameters than either of the independent subsystems alone and this can preclude application of many mcmc methods which are ineffective at high dimensional parameter estimation newer gradient based mcmc algorithms are more suitable for addressing these estimation problems and therefore are especially well suited for large integrated models with numerous components in cases where there is substantial model uncertainty nonparametric specifications of a model component such as a gaussian process can be highly useful uusitalo et al 2015 gaussian processes and other related nonparametric model components are available in newer ppls such as stan pymc3 and edward probabilistic programming makes it especially easy to specify and estimate hierarchical bayesian models of environmental processes e g borsuk et al 2001 which allow for pooling of information across different groups of observations in order to constrain models that might otherwise be too flexible and overparameterized qian et al 2010 5 2 statistical rigor a full bayesian analysis of an environmental model employing mcmc may be more complicated and difficult to understand than the commonly employed generalized likelihood uncertainty estimation glue framework beven and binley 1992 or even a highly subjective visual assessment of goodness of fit krause et al 2005 however less statistically principled approaches may lead to underestimating uncertainty montanari 2005 or failing to find good parameter settings vrugt et al 2009 it is certainly possible to assume an ad hoc set of reasonable parameter values and apply a sophisticated search heuristic however this disregards the substantial body of knowledge on convergence of estimates attribution of uncertainty and numerous other statistical considerations some powerful search heuristics such as evolutionary algorithms can be made into a valid mcmc sampler with modification vrugt 2016 these points address the issue of parameter estimation but ppls also enhance the rigor of the modelling process by more easily freeing the users from unrealistic distributional constraints such as homoscedastic and uncorrelated errors a prerequisite for useful reproducible modelling work is the ability to examine all of the assumptions encoded in the model structure de vos et al 2011 being able to check model assumptions takes an important role during implementation because ppls typically require the user to specify each component of the generative model however this does not guarantee that correct or defensible choices will always be made for example deciding on an error model can be difficult and involve many choices regarding autocorrelation heteroscedasticity bias and distributional form smith et al 2015 5 3 reduced effort model optimization requires search algorithms to suggest parameter settings that provide a good fit to the data it is time intensive and tedious to implement every possible algorithm ppls are specifically designed to allow users with little or no experience with sophisticated bayesian estimation algorithms to use them with minimal tuning salvatier et al 2016 this allows researchers to devote time to core scientific analyses that would otherwise be spent implementing and testing optimization or estimation routines this approach modelling estimation and analysis within a unified framework allows the user to focus on tailoring a model to the specific problem at hand rather than worry about learning another class of methods to estimate parameters bishop 2012 a sample workflow using pymc3 to refine and develop a regression model is shown in fig 2 it highlights the relative ease with which different model structures are accommodated the roadmap for at least one ppl edward tran et al 2016 includes a community repository of models with a common metadata and storage format in addition several ppls including pymc3 and stan incorporate plotting and diagnostic functions for assessing model fit one desirable property of a mcmc sampler used for parameter estimation is that its samples are approximately uncorrelated i e the n th sample drawn is mostly independent from the n 1st and the n 1st sample this is important because poor sampling often leads to high autocorrelations that can be diagnosed from sampler statistics brooks and gelman 1998 and autocorrelation plots functions for evaluating and plotting these diagnostics are included within most ppls 6 example applications environmental models frequently employ both data driven and theory driven approaches in this section we provide a summary of selected recent papers in the environmental sciences that make use of probabilistic programming towards either of those ends a key commonality between all of these studies is that they employ a ppl s automatic inference algorithm for parameter estimation statistics minded researchers in ecology have long embraced bayesian methods and their accompanying software packages for hierarchical models clark 2004 they have also advocated for the adoption monnahan et al 2017 of gradient based estimation methods that are present in recent generation ppls such as stan the concerns of developers who create environmental software for water quality forecasting or assessing ecosystem services may differ from those of ecologists who study those subjects however this latter group frequently works with data from disparate sources with substantial uncertainty cressie et al 2009 and consequently lessons learned in that discipline may be broadly applicable across the environmental sciences perhaps one of the most direct and appropriate usages of a ppl to improve an environmental model can be found in a study by appling et al 2018 after noting that an existing stream metabolism model suffered from parameter identifiability equifinality issues appling et al implemented this metabolic model in stan introducing new stochastic relations between processes to reflect prior knowledge as well as a more sophisticated error model this is an example of the standard method for addressing identifiability equifinality in a bayesian setting use of expert knowledge in the form of focused priors or additional model structure that reflect knowledge of the real world processes a more sophisticated example of modelling a complicated error process is given by rath et al 2017 in this work an empirical model of salinity for a california estuary is integrated with a bayesian neural network model of the resulting errors in order to enhance its performance however instead of performing a joint estimation of all model parameters at once rath et al applied a two step fitting procedure estimating each model s parameters separately in another water resources application cooper and krueger 2017 compared the effectiveness of jags and stan as applied to the task of sediment fingerprinting the identification of the proportion of sediment sources contributing to observed stream sediment loads in climatology bracken et al 2016 leveraged the flexibility of stan to build a hierarchical spatial model of extreme precipitation events across the united states this model included multiple components such as a copula describing the probabilistic dependence across 2600 observation sites as well as an autoregressive component to describe spatial variations in parameters for models conditioned on large quantities of data variational inference vi offers the potential for substantially faster estimation as compared to sampling based approaches such as mcmc automatic variational inference utilizing automatic differentiation are available in pymc3 stan and edward however due in part to its novelty variational inference has been used sparingly within the environmental and earth sciences wingate et al 2016 employed vi to estimate parameters for a generative model of geological formations they conditioned this model on observations of borehole well logs to identify the possible orientation and location of rock formations which would be consistent with the observed data within the energy sector carstens et al 2018 used variational inference as implemented in pymc3 to efficiently compute parameter estimates for several simple energy use models we anticipate that mounting interest in environmental big data fleming et al 2017 and sophisticated model structures will lead to increased interest in efficient estimation methods furthermore variational algorithms are known to be highly effective for training bayesian deep neural networks gal and ghahramani 2015 researchers in both machine learning neal 1996 and ecological modelling reichert and omlin 1997 have noted the usefulness of bayesian models for constraining unidentifiable models through the intelligent use of prior distributions 6 1 integration of exotic inference methods and model forms computational models of the environment may concern phenomena especially amenable to mechanistic representations while also including dynamics requiring a more empirical black box type model beck 1987 some of the most pressing environmental challenges include climate change urban design and sustainability which currently exist at the intersection of empirical and mechanistic representations millett and estrin 2012 attempts to integrate the two approaches must find answers to several questions first how can uncertainty from one model component be propagated to another second are the assumptions that each model makes still independently consistent when considered in union third which processes demand a mechanistic representation and which can be represented with an empirical model once these issues have been resolved hybrid models that are built to incorporate both aspects may become highly useful in situations where there is insufficient knowledge to fully specify a mechanistic model of all relevant dynamics and yet there is insufficient data to perform inference with a complex empirical model turnhoff et al 2016 the substantial uncertainties associated with environmental datasets combined with the inherently physical dynamics of the systems studied imply that the environmental modelling community stands to uniquely benefit from adopting ppls for model development and implementation bayesian models of text image and time series data are ubiquitous in the statistics and machine learning literature consequently models of joint distributions of texts or images with environmental processes could potentially be produced using probabilistic programming techniques as noted in the 2009 report on grand challenges of the future for environmental modelling beck 2009 the environmental modelling community stands to benefit greatly if it can make use of the innovations and novel developments from the artificial intelligence and machine learning communities as ppls provide community standard implementations for algorithms and model components they serve as a rapid pipeline for the dissemination of powerful optimization algorithms from mathematical and statistical sciences into applied practice ppls are a natural tool choice to aid in the development of hybrid models since they provide expressions for constructing both empirical and mechanistic models including inference methods which are appropriate for both the class of empirical models that can be estimated with recent generation ppls includes standard statistical models such as generalized linear models principal components analysis and gaussian mixture models modifications to these models including non gaussian distributions are trivial within a probabilistic programming framework larger more complex forms are also feasible pymc3 and edward are both built on computational libraries abadi et al 2015 bergstra et al 2010 originally designed for deep neural networks and therefore naturally support multilayered models with large numbers of connections this union between statistical modelling frameworks and deep neural networks has led to the formation of the nascent field of deep probabilistic programming coined by tran et al 2017 7 remaining challenges while probabilistic programming can improve the quality and efficiency of research work there are significant drawbacks and caveats that render this approach unwise or infeasible in some situations in this section we cover those issues 7 1 ppl specific syntax probabilistic programming s design philosophy is that the modeled process should be written in a generative fashion i e expressing each modeled quantity as a stochastic or deterministic function of relevant variables the software s internal bookkeeping must tabulate the number and type of connections between variables necessitating a ppl specific syntax to keep track of these dependencies in an older ppl such as bugs the types of processes represented are fairly limited and thus the ppl specific syntax is simple and straightforward in a more sophisticated platform such pymc3 or edward the types of functions and computing constructs available are more numerous and consequently there are more syntactical and procedural points which the user must learn to produce working code this is one of the greatest obstacles to adoption each ppl has its own way of expressing random variables dependencies and other details additionally ppls designed for statistical work such as pymc3 and stan require that high level simulation settings be known prior to execution models with variable timesteps are difficult to accommodate within this framework continuous time models utilizing an adaptive timestep i e kumar et al 2009 to resolve system dynamics over short timescales are therefore not good candidates currently for implementation in a ppl 7 2 learning bayesian methods to specify a model using probabilistic programming the user must possess a degree of fluency in statistical analysis and in bayesian methods in particular this can be a hindrance as it is relatively difficult to teach bayesian methodology due to its heavy use of conditional probabilities and other concepts that may be relatively unfamiliar to users with a traditional statistical education moore 1997 markov chain monte carlo is not part of the statistical training that most scientists receive and it can be difficult to transition from a test oriented statistics workflow to one that is centered on simulation and model building modellers familiar with metrics of model fit such as nash sutcliffe efficiency or reliability may find that model likelihoods or information criteria are more difficult to work with as they are typically not comparable across different model structures 7 3 opaque estimation the inclusion of a general purpose estimation inference algorithm within most ppls simplifies and streamlines the procedure of estimating model parameters and quantifying uncertainty however this means that methods that require extensive tuning or adjustment with expert knowledge are unsuitable for this purpose to this end algorithms that replace user defined tuning parameters with additional sampling mechanisms are favored for example the nou turn sampler included in stan and pymc3 removes a user specified parameter for the sampling trajectory step size by including a recursive subroutine that iteratively checks backwards and forwards along possible paths a necessary side effect of this modification is additional complexity in the inference algorithm and added difficulty in troubleshooting poor performance as more sophisticated inference methods are included as part of ppl core functionality it is likely that the gap between the complexity of these algorithms and the average user s understanding will only grow 8 future directions as is often the case with modelling methods developed across several disciplines the class of problems that can be addressed using techniques from probabilistic programming is not yet well characterized ppls have been used to study dynamical systems such as recurrent neural networks from a machine learning centric point of view tran et al 2017 but it is unclear whether their inference engines can be further optimized estimation in environmental systems additionally these algorithms require a formal likelihood function that maps model parameters and observed data to probabilities for situations in which the likelihood is either not tractable or too time consuming for mcmc approximate bayesian computation abc methods marjoram et al 2003 tavare et al 1997 using quasi likelihoods or summary statistics for calibrating environmental models nott et al 2012 sadegh and vrugt 2013 can be employed unfortunately abc is not yet incorporated into the functionality of most ppls although abc based modelling frameworks are available lintusaari et al 2017 finally the future of probabilistic programming across the computational sciences is unclear as of 2018 new ppls with additional features are still being released often with private sponsorship as in the case of pyro released by uber and tensorflow probability google while these software products are open source and in the public domain their functionality and development is undoubtedly guided by the needs of corporate researchers whose aims may differ from those of academic researchers 9 conclusion the needs of environmental modellers differ from those of researchers in many other scientific disciplines because of the common use of both empirical or black box and mechanistic models for environmental applications consequently developers and researchers must carefully balance appropriate representations of uncertainty in all its forms with detailed process equations as the sophistication and computational demands of models continues to grow issues such as identifiability reproducibility and efficient inference will only become more pressing probabilistic programming languages provide community standard implementations of modelling components and estimation algorithms that can greatly expedite the model development process and facilitate exciting interdisciplinary research with this software researchers are able to define and estimate a wide variety of models ranging from basic regressions to deep neural networks as well as process based models all of which reside within the same bayesian framework we note that this comes with an added degree of complexity due to the additional software and libraries needed for this approach furthermore highly computationally intensive models with simulations requiring 102 s are poor candidates for mcmc methods because these methods are the inference engines of most ppls we hope that this review informs readers of new methodologies that can accelerate their work and allow them to focus on those parts of the scientific modelling process which are most informative acknowledgements this research was funded by nasa via the earth and space sciences fellowship program and the national science foundation via an igert traineeship through duke wisenet additional financial support was provided by the duke university wetland center 
26245,the development process for an environmental model involves multiple iterations of a planning implementation assessment cycle probabilistic programming languages ppls are designed to expedite this process with general purpose methods for implementing models efficiently inferring their parameters and generating probabilistic predictions probabilistic programming exists at the intersection of bayesian statistics machine learning and process based modelling and therefore can be of value to the environmental modelling community in this review article we explain how it can be used to accelerate model development and allow for statistical inference using more complicated models and larger data sets than previously possible specific challenges and limitations to employing such frameworks are also raised we provide guidance to help modellers decide whether incorporating probabilistic programming in their work may improve the efficiency and quality of their analyses keywords bayesian statistics parameter estimation uncertainty quantification probabilistic programming 1 introduction the widespread usage of mathematical models to represent and forecast environmental processes builds upon a rich scientific history of iteration between theory and practice box 1976 this procedure alternates between the generation of testable hypotheses about the natural world and the investigation of their validity via statistical analysis of data which may be incomplete or prone to random variation the environmental sciences present special challenges due to the complexity of the systems studied and hypotheses posed clark 2004 as well as the interactions between anthropogenic and natural causes modelling environmental dynamics consequently requires techniques from both physical and statistical sciences national research council 2001 while theory based models usually integrate detailed knowledge regarding the key processes involved empirical models are more appropriate for scenarios involving substantial uncertainty about either the structure or governing parameters of the system studied though scientific inquiry often involves alternation between both approaches box and youle 1955 regardless of the choice of model used model assumptions must be checked to ensure that the end product is an appropriate representation the specific logic encoded in the data generating process will invariably differ from case to case but much of the labor involved in estimation and validation can be spared by using general frameworks for model construction and assessment probabilistic programming languages ppls intend to unify a scattered and mostly disconnected landscape of models and algorithms by basing model structure and estimation on generic reusable components that can be applied in a wide variety of contexts ghahramani 2015 parameter estimation is automatic in a ppl in the sense that it is performed with general purpose algorithms designed to work on very broad classes of models with little tuning a central philosophy of probabilistic programming is that the act of writing down a model should be wholly divorced from its estimation and inference in a ppl assumptions about the world are encoded in a graphical model which depicts the relationships between variables in terms of a network within this network nodes correspond to model variables and edges correspond to dependencies which may be either stochastic or deterministic this formalism provides a unifying view of models with many interacting random variables wainwright and jordan 2007 in terms of modular substructures within the underlying graph with a well designed ppl the modeller is able to rapidly prototype and test new ideas by modifying and combining existing models this enables a more efficient way to explore multiple alternative model forms lahtinen et al 2017 there already exist some environmental modelling frameworks which share similar objectives such as code reuse and modularity e g oms3 david et al 2013 but are less focused on inference and optimization in many situations researchers stand to benefit from using a general purpose inference algorithm as compared to a case by case piecemeal application of one or several estimation algorithms written expressly for a single purpose optimization frameworks such as pest doherty et al 2010 provide sophisticated algorithms for model fitting but offer little facility for specifying model structure or performing a statistical critique of the parameter estimates probabilistic programming instead offers a unified modelling framework integrating model definition estimation and criticism for conventional statistical analyses process based modelling and deep neural networks among other modelling learning approaches despite their name ppls are embedded in a high level programming language some of the more recent frameworks are pymc3 python stan c with interfaces for several other languages and edward python every ppl includes functionality for declaring and using stochastic variables in arbitrary model structures goodman 2013 these are defined in terms of probability distributions and do not refer to known numerical quantities stored within computer memory but rather are used to build a probabilistic model of a system this model can then be used to generate simulated outcomes as well as to assess the likelihood of observed outcomes in aid of parameter inference optimization and inference algorithms are frequently the most time consuming and difficult part of the modelling process fischer and schumann 2003 and ppls are designed to streamline this process by including user friendly community standard implementations of these algorithms a ppl may include additional utilities for scoring and ranking models or saving and loading previously computed parameter estimates some readers may be familiar with older statistical modelling frameworks such as bugs lunn et al 2000 or jags plummer 2003 to this audience we use this review to communicate how more recent ppls can enable inference for a larger class of models and facilitate big data analyses for those who have no familiarity with this subject we hope to show why implementation of environmental models in a ppl may be advantageous of course not every ppl is suited for every task in this review we focus on those which have a particular ability to represent mechanistic environmental processes and statistical representations we begin with an overview of the probabilistic programming concept followed by a brief history we then present a typical workflow of probabilistic programming next we discuss the algorithms typically used for parameter estimation and uncertainty quantification as well as some algorithmic developments that have dramatically enhanced the functionality of modern probabilistic programming frameworks we then cover the benefits of employing ppls in an environmental modelling project and present some relevant examples of their application within environmental modelling we conclude by discussing remaining challenges and future directions of the field as well as potential avenues of research 2 overview of probabilistic programming the term probabilistic programming is expansive and includes software intended for diverse aims and with varying limitations probabilistic programming can be loosely defined as the application of deterministic computer programs tightly integrated with stochastic elements or constructs for random choice goodman 2013 with the ability to be conditioned on observed data however a key feature of probabilistic programming in all cases is the separation of model structure and estimation in virtually all probabilistic programming frameworks a user defines the model structure by calling functions that add nodes to the underlying graphical model which represents a joint probability distribution for the problem at hand these variables can be scalar valued matrix valued or representative of richer stochastic processes such as gaussian random walks or autoregressions deterministic transformations can then be applied to these random variables to create derived quantities these variables and transformations must be specified in sufficient detail that the model s likelihood function can be evaluated and employed in either monte carlo based techniques or faster optimization centric approaches used to estimate model parameters while early research in probabilistic programming initially grew out of theoretical work in computer science kozen 1981 solomonoff 1978 efforts to develop practical general purpose inference engines in tandem with reusable model components took place towards the late 1980s and early 1990s the resulting combination of software packages and frameworks appears to have emerged from the confluence of research communities with two distinct backgrounds and objectives the first koller et al 1997 pearl 1987 was centered around researchers concerned with artificial intelligence and probabilistic reasoning while the second was largely composed of statisticians who desired flexible and widely applicable inference packages the latter group was aided immensely by the translation of markov chain monte carlo mcmc from the physics literature into mainstream statistical practice gelfand and smith 1990 3 workflow using a probabilistic programming framework for parameter estimation or uncertainty quantification involves three main steps fig 1 first the structure of the model is defined by specifying a network of random variables and their interconnections these connections may be deterministic or stochastic relations the model is declared in a bottom up fashion prior distributions or fixed values are designated for basic model parameters and derived quantities are then declared as functions of these initial parameters second an inference engine is specified and either a sampling or optimization based approach is applied with little or no tuning of the inference algorithm this is similar in spirit to other community modelling efforts such as summa clark et al 2015 in which the model logic reflecting the structure of the data generating process is cleanly separated from the numerical solvers used for simulation finally summaries of the posterior distribution are generated and diagnostic statistics of the estimation or optimization process are assessed jakeman et al 2006 once a generative process is specified and the posterior distribution is estimated it is always possible to then draw simulations of imaginary data which can be invaluable in assessing whether the model generates realistic or plausible outcomes this workflow places a strong emphasis on description and understanding of the data generating process fig 2 by specifying the form of this process in a graphical model it is hoped that the modeller is made aware of issues that might not have been identified with a purely test based approach furthermore it reduces the tendency to fall back to specific model forms that are familiar but unsuited to the problem at hand 4 algorithms while the methodology of estimation in probabilistic graphical models is nearly always described as bayesian in some way using a probabilistic programming language does not necessarily require agreement with a bayesian interpretation of probability as subjective degrees of belief clark 2004 however ppls readily exploit the algorithms developed for bayesian inference imcluding markov chain monte carlo mcmc an in depth review and explanation of mcmc is beyond the scope of this work and excellent tutorials can be found elsewhere andrieu et al 2003 brooks et al 2011 robert and casella 2011 it is relevant to this work insofar as it is a method for estimating the parameters of an arbitrary computational model provided that the model can be run for hundreds to thousands of iterations within a reasonable amount of time the core idea behind mcmc is that the model s likelihood function p x θ and prior distribution p θ can be used to construct a trajectory through the space of all possible parameter settings θ we note that the definition of parameter as it pertains to this discussion is quite broad for example linear regression coefficients are parameters as are the site specific soil porosity or hydraulic conductivity values in a distributed hydrology model in the case of inverse modelling the unknown quantities to be estimated are the initial conditions or forcing leading to the observed data and these unobserved quantities are also be described as parameters in a statistical sense the likelihood function expresses the conditional probability of the observed data x given the parameters θ using bayes rule we can express the posterior probability of the parameters given the data as p θ x p x θ p θ p x where p θ denotes a prior distribution over the variables however the evidence term p x is difficult to compute as p x θ p x θ p θ and the space of possible parameter sets is very large we can therefore only evaluate the posterior density π θ x p x θ p θ while this density is not normalized and therefore not useful for directly obtaining the posterior probability without normalizing by p x it can be used to guide a markov chain s θ 1 θ 2 through the parameter space provided that this chain satisfies certain conditions brooks et al 2011 it is also an approximation converging to the desired posterior after many samples have been drawn many different varieties of mcmc algorithms exist due to the wide range of possible proposal mechanisms used to suggest new candidate parameter sets for parameter settings with high likelihood the model is a good match for the data this allows us to find optimal parameter settings using mcmc computationally intensive models may not be good candidates for this procedure because mcmc usually requires upwards of 103 model simulations to adequately explore the posterior distribution over the parameters in some applications i e vrugt et al 2008 it is not unusual to run 105 106 simulations to ensure that a sufficient number of samples is drawn gibbs sampling a form of mcmc which iteratively updates a single parameter at a time is the centerpiece of the bugs bayesian analysis using gibbs sampling software this package along with its microsoft windows specific variant winbugs lunn et al 2000 and open source counterpart jags plummer 2003 has found widespread usage indeed this series of software represents one of the earliest examples of a bayesian programming environment that was widely used by researchers outside the fields of statistics and artificial intelligence one of the key features of this software is its object oriented approach which allows for extensions such as a graphical user interface geospatial modelling methods and a simple model comparison and selection scheme lunn et al 2009 numerous other probabilistic programming languages incorporating mcmc were developed between 1990 and 2010 some of the more notable ones include prism sato and kameya 1997 church goodman et al 2008 and infer net while the proliferation of mcmc based methods in applied statistics enabled bayesian parameter estimation for a broad class of models each algorithm had its disadvantages gibbs sampling requires the ability to compute conditional distributions that express the probability of a single parameter given all other parameters this may not be possible for complicated models the gibbs sampler is actually a special case of the metropolis hastings algorithm hastings 1970 but the latter is frequently used to refer specifically to a procedure in which many parameters are updated simultaneously and the conditional distribution is not evaluated this variant of the metropolis hastings algorithm only requires the calculation of the model s posterior density but it also generates proposals according to a random walk typically converging to the posterior in a number of iterations proportional to the number of model parameters roberts et al 1997 this can lead to unreasonably long convergence times with increasing numbers of model parameters though the metropolis algorithm can be an effective tool for the calibration of simple environmental models kuczera and parent 1998 several other bayesian estimation methods have been included in probabilistic programming languages including other varieties of mcmc such as sequential monte carlo doucet et al 1999 there are also alternate estimation frameworks such as approximate bayesian computation marin et al 2012 the suitability of these methods for inference tend to be problem and model specific and the models tend not to be used as the main workhorse of inference in any existing probabilistic programming languages fortunately the kernels from different mcmc methods can be chained or applied to smaller subcomponents of the model while still satisfying the requirements of a valid estimation procedure johnson et al 2013 evolutionary monte carlo and sequential monte carlo smc have found abundant use in the hydrology and earth sciences community jeremiah et al 2012 pathiraja et al 2018 but are not incorporated as the primary inference engine in any pp framework pymc3 and webppl both include an evolutionary algorithmic sampler such as differential evolution braak 2006 but more sophisticated variants such as dream vrugt 2016 are not yet incorporated for models wholly or partially defined on continuous random variables it is possible to calculate the gradient of the posterior density θ π θ x with regard to some or all of the model parameters this can be very helpful because ascending the gradient corresponds to moving towards higher values of π θ x and a better model fit this can also lead to more rapid convergence as well the metropolis adjusted langevin algorithm a version of metropolis hastings incorporating an up gradient bias term in its kernel converges much more rapidly than the ordinary metropolis algorithm for high dimensional models pillai et al 2012 hamiltonian monte carlo neal 2012 is available in several ppls and similarly is highly effective due to its use of the posterior gradient unfortunately it is not feasible to calculate the gradient by hand for many complicated model structures symbolic differentiation is not an ideal alternative as the number of terms in the derivative will quickly grow under repeated application of the chain rule a numerical analysis of the gradient using finite differences is likely to be unstable and inefficient in when applied to models with many parameters fournier et al 2012 a fourth option is to use automatic differentiation this is an algorithmic approach to evaluating derivatives of arbitrary programs that borrows elements from both symbolic and numerical differentiation automatic differentiation involves recursively applying the chain rule and only storing intermediate numerical values rather than a large number of symbolic terms griewank and walther 2003 while not all ppls table 1 make use of automatic differentiation those that do use it incorporate separate and distinct software packages developed specifically for this task edward for example relies on tensorflow a library developed by google for machine learning while pymc3 is built on top of theano in both of these cases the actual model specification in a probabilistic programming framework consists of adding variables to a computation graph which can then be traversed either forward for normal simulation or backwards for automatic differentiation a secondary benefit of using an external computational library for calculations is that it can optimize the computation graph to eliminate redundant function evaluations fig 3 and automatically compile the designated model in a slower dynamically typed programming language such as r or python into a static typed language such as c c we view the incorporation of automatic differentiation as a major step in the evolution of probabilistic programming for two primary reasons first as mentioned in the previous paragraph it allows the implementation of more efficient mcmc samplers that can use this gradient information to navigate a high dimensional parameter space second automatic differentiation is a staple of modern deep learning methods lecun et al 2015 involving neural networks and consequently the insights and computational techniques designed to improve that class of methods can be brought to bear for difficult estimation problems several ppls introduced since the late 2000s such as stan carpenter et al 2017 pymc3 salvatier et al 2016 and edward tran et al 2016 incorporate automatic differentiation each of these ppls incorporates at least one efficient gradient based mcmc algorithm such as hamiltonian monte carlo or its extension the nou turn sampler hoffman and gelman 2014 and one gradient based non mcmc approach such as variational inference kingma 2017 kucukelbir et al 2015 intended for probabilistic models conditioned on large amounts of data while mcmc approximates the integral p x θ p θ p x for the desired probabilistic model an alternative approach is to identify a simpler model with fewer cross parameter correlations such that the new model is easily optimized to approximate the desired posterior of the original model this strategy describes a wide range of algorithms that fall under the umbrella of variational inference beal and ghahramani 2003 blei et al 2017 a key advantage of variational inference is that by avoiding repeated sampling it is both deterministic and relatively fast although it does not share mcmc s asymptotic posterior convergence guarantees furthermore vi tends to underestimate the variance of model parameter estimates blei et al 2017 which can be a major deficiency when performing uncertainty quantification variational methods compare most favorably with mcmc in situations requiring large amounts of data in which vi can be orders of magnitude faster blei et al 2017 automatic differentiation again plays a key role here by providing a model specific formula for optimizing the variational objective without expert knowledge on the part of the user kucukelbir et al 2015 in theory this simplifies parameter estimation for big data problems in models from multiple disciplines including statistics the earth sciences and machine learning by providing a shared interface table 1 lists commonly used probabilistic programming languages along with several attributes describing their usability in the next section we review recent work that employs probabilistic programming in environmental modelling for environmental modellers we recommend using stan or pymc3 as these are both relatively mature with a straightforward model syntax but incorporate modern algorithms for bayesian inference such as hmc or nuts 5 benefits to environmental modelling the positive impacts of utilizing a probabilistic programming language can largely be broken down into two categories 1 effects that enhance the quality reproducibility and generality of scientific analysis and 2 features of ppls that reduce the effort required to analyze and extract knowledge from data here we will discuss provide a brief overview of these impacts which are addressed more extensively in bishop 2012 tran et al 2016 and carpenter et al 2017 5 1 integrated uncertainty new challenges posed by complex environmental systems necessitate analyses integrating heterogeneous sources of data and models from different disciplines laniak et al 2013 integrated environmental modelling is becoming an increasingly important part of the environmental sciences since the ongoing confluence of ecological economic and environmental processes demands a holistic treatment jakeman and letcher 2003 univariate or low dimensional uncertainty analyses of model components are acceptable when the coupling between subsystems is weak or nonexistent but such an approach can obscure key system dynamics or misrepresent the probabilities of joint outcomes unless a comprehensive integrated uncertainty analysis is performed bayesian models naturally address these issues and while a ppl is not strictly necessary to perform these analyses the process of joint uncertainty quantification is greatly expedited by their usage furthermore coupled systems will invariably have more parameters than either of the independent subsystems alone and this can preclude application of many mcmc methods which are ineffective at high dimensional parameter estimation newer gradient based mcmc algorithms are more suitable for addressing these estimation problems and therefore are especially well suited for large integrated models with numerous components in cases where there is substantial model uncertainty nonparametric specifications of a model component such as a gaussian process can be highly useful uusitalo et al 2015 gaussian processes and other related nonparametric model components are available in newer ppls such as stan pymc3 and edward probabilistic programming makes it especially easy to specify and estimate hierarchical bayesian models of environmental processes e g borsuk et al 2001 which allow for pooling of information across different groups of observations in order to constrain models that might otherwise be too flexible and overparameterized qian et al 2010 5 2 statistical rigor a full bayesian analysis of an environmental model employing mcmc may be more complicated and difficult to understand than the commonly employed generalized likelihood uncertainty estimation glue framework beven and binley 1992 or even a highly subjective visual assessment of goodness of fit krause et al 2005 however less statistically principled approaches may lead to underestimating uncertainty montanari 2005 or failing to find good parameter settings vrugt et al 2009 it is certainly possible to assume an ad hoc set of reasonable parameter values and apply a sophisticated search heuristic however this disregards the substantial body of knowledge on convergence of estimates attribution of uncertainty and numerous other statistical considerations some powerful search heuristics such as evolutionary algorithms can be made into a valid mcmc sampler with modification vrugt 2016 these points address the issue of parameter estimation but ppls also enhance the rigor of the modelling process by more easily freeing the users from unrealistic distributional constraints such as homoscedastic and uncorrelated errors a prerequisite for useful reproducible modelling work is the ability to examine all of the assumptions encoded in the model structure de vos et al 2011 being able to check model assumptions takes an important role during implementation because ppls typically require the user to specify each component of the generative model however this does not guarantee that correct or defensible choices will always be made for example deciding on an error model can be difficult and involve many choices regarding autocorrelation heteroscedasticity bias and distributional form smith et al 2015 5 3 reduced effort model optimization requires search algorithms to suggest parameter settings that provide a good fit to the data it is time intensive and tedious to implement every possible algorithm ppls are specifically designed to allow users with little or no experience with sophisticated bayesian estimation algorithms to use them with minimal tuning salvatier et al 2016 this allows researchers to devote time to core scientific analyses that would otherwise be spent implementing and testing optimization or estimation routines this approach modelling estimation and analysis within a unified framework allows the user to focus on tailoring a model to the specific problem at hand rather than worry about learning another class of methods to estimate parameters bishop 2012 a sample workflow using pymc3 to refine and develop a regression model is shown in fig 2 it highlights the relative ease with which different model structures are accommodated the roadmap for at least one ppl edward tran et al 2016 includes a community repository of models with a common metadata and storage format in addition several ppls including pymc3 and stan incorporate plotting and diagnostic functions for assessing model fit one desirable property of a mcmc sampler used for parameter estimation is that its samples are approximately uncorrelated i e the n th sample drawn is mostly independent from the n 1st and the n 1st sample this is important because poor sampling often leads to high autocorrelations that can be diagnosed from sampler statistics brooks and gelman 1998 and autocorrelation plots functions for evaluating and plotting these diagnostics are included within most ppls 6 example applications environmental models frequently employ both data driven and theory driven approaches in this section we provide a summary of selected recent papers in the environmental sciences that make use of probabilistic programming towards either of those ends a key commonality between all of these studies is that they employ a ppl s automatic inference algorithm for parameter estimation statistics minded researchers in ecology have long embraced bayesian methods and their accompanying software packages for hierarchical models clark 2004 they have also advocated for the adoption monnahan et al 2017 of gradient based estimation methods that are present in recent generation ppls such as stan the concerns of developers who create environmental software for water quality forecasting or assessing ecosystem services may differ from those of ecologists who study those subjects however this latter group frequently works with data from disparate sources with substantial uncertainty cressie et al 2009 and consequently lessons learned in that discipline may be broadly applicable across the environmental sciences perhaps one of the most direct and appropriate usages of a ppl to improve an environmental model can be found in a study by appling et al 2018 after noting that an existing stream metabolism model suffered from parameter identifiability equifinality issues appling et al implemented this metabolic model in stan introducing new stochastic relations between processes to reflect prior knowledge as well as a more sophisticated error model this is an example of the standard method for addressing identifiability equifinality in a bayesian setting use of expert knowledge in the form of focused priors or additional model structure that reflect knowledge of the real world processes a more sophisticated example of modelling a complicated error process is given by rath et al 2017 in this work an empirical model of salinity for a california estuary is integrated with a bayesian neural network model of the resulting errors in order to enhance its performance however instead of performing a joint estimation of all model parameters at once rath et al applied a two step fitting procedure estimating each model s parameters separately in another water resources application cooper and krueger 2017 compared the effectiveness of jags and stan as applied to the task of sediment fingerprinting the identification of the proportion of sediment sources contributing to observed stream sediment loads in climatology bracken et al 2016 leveraged the flexibility of stan to build a hierarchical spatial model of extreme precipitation events across the united states this model included multiple components such as a copula describing the probabilistic dependence across 2600 observation sites as well as an autoregressive component to describe spatial variations in parameters for models conditioned on large quantities of data variational inference vi offers the potential for substantially faster estimation as compared to sampling based approaches such as mcmc automatic variational inference utilizing automatic differentiation are available in pymc3 stan and edward however due in part to its novelty variational inference has been used sparingly within the environmental and earth sciences wingate et al 2016 employed vi to estimate parameters for a generative model of geological formations they conditioned this model on observations of borehole well logs to identify the possible orientation and location of rock formations which would be consistent with the observed data within the energy sector carstens et al 2018 used variational inference as implemented in pymc3 to efficiently compute parameter estimates for several simple energy use models we anticipate that mounting interest in environmental big data fleming et al 2017 and sophisticated model structures will lead to increased interest in efficient estimation methods furthermore variational algorithms are known to be highly effective for training bayesian deep neural networks gal and ghahramani 2015 researchers in both machine learning neal 1996 and ecological modelling reichert and omlin 1997 have noted the usefulness of bayesian models for constraining unidentifiable models through the intelligent use of prior distributions 6 1 integration of exotic inference methods and model forms computational models of the environment may concern phenomena especially amenable to mechanistic representations while also including dynamics requiring a more empirical black box type model beck 1987 some of the most pressing environmental challenges include climate change urban design and sustainability which currently exist at the intersection of empirical and mechanistic representations millett and estrin 2012 attempts to integrate the two approaches must find answers to several questions first how can uncertainty from one model component be propagated to another second are the assumptions that each model makes still independently consistent when considered in union third which processes demand a mechanistic representation and which can be represented with an empirical model once these issues have been resolved hybrid models that are built to incorporate both aspects may become highly useful in situations where there is insufficient knowledge to fully specify a mechanistic model of all relevant dynamics and yet there is insufficient data to perform inference with a complex empirical model turnhoff et al 2016 the substantial uncertainties associated with environmental datasets combined with the inherently physical dynamics of the systems studied imply that the environmental modelling community stands to uniquely benefit from adopting ppls for model development and implementation bayesian models of text image and time series data are ubiquitous in the statistics and machine learning literature consequently models of joint distributions of texts or images with environmental processes could potentially be produced using probabilistic programming techniques as noted in the 2009 report on grand challenges of the future for environmental modelling beck 2009 the environmental modelling community stands to benefit greatly if it can make use of the innovations and novel developments from the artificial intelligence and machine learning communities as ppls provide community standard implementations for algorithms and model components they serve as a rapid pipeline for the dissemination of powerful optimization algorithms from mathematical and statistical sciences into applied practice ppls are a natural tool choice to aid in the development of hybrid models since they provide expressions for constructing both empirical and mechanistic models including inference methods which are appropriate for both the class of empirical models that can be estimated with recent generation ppls includes standard statistical models such as generalized linear models principal components analysis and gaussian mixture models modifications to these models including non gaussian distributions are trivial within a probabilistic programming framework larger more complex forms are also feasible pymc3 and edward are both built on computational libraries abadi et al 2015 bergstra et al 2010 originally designed for deep neural networks and therefore naturally support multilayered models with large numbers of connections this union between statistical modelling frameworks and deep neural networks has led to the formation of the nascent field of deep probabilistic programming coined by tran et al 2017 7 remaining challenges while probabilistic programming can improve the quality and efficiency of research work there are significant drawbacks and caveats that render this approach unwise or infeasible in some situations in this section we cover those issues 7 1 ppl specific syntax probabilistic programming s design philosophy is that the modeled process should be written in a generative fashion i e expressing each modeled quantity as a stochastic or deterministic function of relevant variables the software s internal bookkeeping must tabulate the number and type of connections between variables necessitating a ppl specific syntax to keep track of these dependencies in an older ppl such as bugs the types of processes represented are fairly limited and thus the ppl specific syntax is simple and straightforward in a more sophisticated platform such pymc3 or edward the types of functions and computing constructs available are more numerous and consequently there are more syntactical and procedural points which the user must learn to produce working code this is one of the greatest obstacles to adoption each ppl has its own way of expressing random variables dependencies and other details additionally ppls designed for statistical work such as pymc3 and stan require that high level simulation settings be known prior to execution models with variable timesteps are difficult to accommodate within this framework continuous time models utilizing an adaptive timestep i e kumar et al 2009 to resolve system dynamics over short timescales are therefore not good candidates currently for implementation in a ppl 7 2 learning bayesian methods to specify a model using probabilistic programming the user must possess a degree of fluency in statistical analysis and in bayesian methods in particular this can be a hindrance as it is relatively difficult to teach bayesian methodology due to its heavy use of conditional probabilities and other concepts that may be relatively unfamiliar to users with a traditional statistical education moore 1997 markov chain monte carlo is not part of the statistical training that most scientists receive and it can be difficult to transition from a test oriented statistics workflow to one that is centered on simulation and model building modellers familiar with metrics of model fit such as nash sutcliffe efficiency or reliability may find that model likelihoods or information criteria are more difficult to work with as they are typically not comparable across different model structures 7 3 opaque estimation the inclusion of a general purpose estimation inference algorithm within most ppls simplifies and streamlines the procedure of estimating model parameters and quantifying uncertainty however this means that methods that require extensive tuning or adjustment with expert knowledge are unsuitable for this purpose to this end algorithms that replace user defined tuning parameters with additional sampling mechanisms are favored for example the nou turn sampler included in stan and pymc3 removes a user specified parameter for the sampling trajectory step size by including a recursive subroutine that iteratively checks backwards and forwards along possible paths a necessary side effect of this modification is additional complexity in the inference algorithm and added difficulty in troubleshooting poor performance as more sophisticated inference methods are included as part of ppl core functionality it is likely that the gap between the complexity of these algorithms and the average user s understanding will only grow 8 future directions as is often the case with modelling methods developed across several disciplines the class of problems that can be addressed using techniques from probabilistic programming is not yet well characterized ppls have been used to study dynamical systems such as recurrent neural networks from a machine learning centric point of view tran et al 2017 but it is unclear whether their inference engines can be further optimized estimation in environmental systems additionally these algorithms require a formal likelihood function that maps model parameters and observed data to probabilities for situations in which the likelihood is either not tractable or too time consuming for mcmc approximate bayesian computation abc methods marjoram et al 2003 tavare et al 1997 using quasi likelihoods or summary statistics for calibrating environmental models nott et al 2012 sadegh and vrugt 2013 can be employed unfortunately abc is not yet incorporated into the functionality of most ppls although abc based modelling frameworks are available lintusaari et al 2017 finally the future of probabilistic programming across the computational sciences is unclear as of 2018 new ppls with additional features are still being released often with private sponsorship as in the case of pyro released by uber and tensorflow probability google while these software products are open source and in the public domain their functionality and development is undoubtedly guided by the needs of corporate researchers whose aims may differ from those of academic researchers 9 conclusion the needs of environmental modellers differ from those of researchers in many other scientific disciplines because of the common use of both empirical or black box and mechanistic models for environmental applications consequently developers and researchers must carefully balance appropriate representations of uncertainty in all its forms with detailed process equations as the sophistication and computational demands of models continues to grow issues such as identifiability reproducibility and efficient inference will only become more pressing probabilistic programming languages provide community standard implementations of modelling components and estimation algorithms that can greatly expedite the model development process and facilitate exciting interdisciplinary research with this software researchers are able to define and estimate a wide variety of models ranging from basic regressions to deep neural networks as well as process based models all of which reside within the same bayesian framework we note that this comes with an added degree of complexity due to the additional software and libraries needed for this approach furthermore highly computationally intensive models with simulations requiring 102 s are poor candidates for mcmc methods because these methods are the inference engines of most ppls we hope that this review informs readers of new methodologies that can accelerate their work and allow them to focus on those parts of the scientific modelling process which are most informative acknowledgements this research was funded by nasa via the earth and space sciences fellowship program and the national science foundation via an igert traineeship through duke wisenet additional financial support was provided by the duke university wetland center 
26246,catchment scale water quality models have become important tools for water quality management planning and reporting worldwide in this review we synthesise recent developments in water quality modelling focusing on catchment scale models of freshwater non urban systems and their ability to support catchment management we explore 10 key attributes in selected existing water quality models these attributes can be characterised as model use model purposes representation of constituents scenario analysis and documentation model development process representation spatial heterogeneities temporal dynamics and data requirements and model performance calibration validation and uncertainty tools we deliberate on 11 key challenges and or emerging topics in catchment water quality modelling large scale applications model integration model usability and communication preliminary data analysis modelling management practices technology advancement incorporating soft data model identifiability uncertainty analysis good modelling practices and capacity building and differentiating the effects of climate impacts from those associated with land use and management practices keywords water quality catchment models catchment management sediments nutrients uncertainty 1 introduction water resource management often involves the monitoring and modelling of water quality and quantity many erosion and water quality models have been developed to determine the source transformation transport and delivery of constituents in catchments and into waterbodies usually in terms of concentrations and or loads in practice these models can be used to predict water quality in areas where monitoring is not feasible or to predict water quality conditions resulting from different management strategies and under specified climate regimes research into the development of erosion and water quality models has mostly focussed on better representation of the biophysical processes e g formulae to represent the generation filtration transformation and transport processes of different constituents spatial and temporal dynamics of the constituents this focus has been accompanied by improved software platforms and on tools for measuring and attempting to increase the predictive skill of the models e g calibration sensitivity and uncertainty assessment tools here we use the term water quality models to include models directly related to in stream water quality as well as models indirectly related to in stream water quality such as erosion models we use the term catchment models to encompass models that predict sediment and or constituents in streams or rivers at a catchment scale which generally include generation land to water delivery and in stream processes we reserve the term integrated models for models that incorporate systems or processes beyond water quality such as a broader social economic ecological system in which the objectives of the water quality modelling sub problem may be embedded in this paper we mostly focus on catchment models other models such as erosion or in stream models are generally invoked due to their usages in catchment models or modelling platforms several reviews on water quality modelling related topics have been published in the scientific literature and most have necessarily had specific scope some review articles focus on particular geographic locations constituents or types of systems e g coastal or urban systems whilst others concentrate on a specific aspect of water quality modelling table 1 many of the commonly used catchment models have been previously compared in the published literature aksoy and kavvas 2005 borah and bera 2003 deliman et al 1999 despite the aforementioned literature there has been relatively little focus on catchment scale river systems modelling and in particular how it can support catchment management there has been no comprehensive update on the water quality modelling advances in the last decade the purpose of this review is to compare key attributes of commonly used catchment scale water quality models characterised by their use development and performance and to critique their ability to support catchment management from this comparison we identify and deliberate upon unresolved or emerging topics and challenges in the development and use of catchment scale water quality models we focus on models of moderate to large catchment scale freshwater non urban systems although our focus is on sediments and nutrients the range of constituents included in the commonly used models are briefly summarised and most of our discussions apply to the modelling of other constituents the review does not consider the modelling of urban catchments estuarine or ocean systems models developed and applied solely to plot or field scales nor does it discuss in detail the hydrologic processes of soil and water readers are referred to table 1 for literature on these topics we start with a brief overview of water quality models in the literature in section 2 in sections 3 to 5 we explore and discuss key attributes of existing water quality models citing relevant models as appropriate these attributes are model use e g model purposes constituents modelled scenario analysis and management practices and software availability and documentation model development e g process representation spatial heterogeneities and scales temporal dynamics and data requirements and model performance e g calibration validation and uncertainty tools drawing on the preceding three sections we discuss some of the unresolved or emerging topics and challenges in the development and use of water quality models section 6 2 overview of water quality models in the literature to understand the general publication trends in water quality modelling we undertook a database search from scopus it being one of the largest online databases in the world covering over 22 000 peer reviewed journals it also provides simple tools that facilitate basic visualization and statistical analysis of papers in order to obtain the most relevant publications the option with titles abstracts and keywords was selected with the keywords listed in table 2 the search was limited to the subject areas of environmental science earth and planetary sciences agricultural and biological sciences chemistry computer science mathematics engineering social science and decision science the search found 50 530 papers between 1935 and 2018 note we may not have the complete issues for 2018 in some journals as the search was undertaken on 4 dec 2018 fig 1 the rate of publications per year increased from 1970 particularly so over the last 15 years in fact 76 38 542 of the papers were published between 2003 and 2018 most of these papers were predominately published from authors with addresses in the usa followed by china uk germany canada and australia we also explored the publication records of 42 water quality models or modelling platforms in the environmental field these models and or platforms were selected because either they were reviewed in other literature e g merritt et al 2003 tsakiris and alexakis 2012 or they have been used by government agencies to support river system planning e g the integrated water quantity and quality simulation model iqqm in australia simons et al 1996 note that applications of some river system planning tools have historically focused on water quantity modelling but the intent of many agencies was that water quality capabilities would be expanded over time invoking the scopus database again the model names were added to the search terms in table 2 individually and the resulting numbers of papers between 2003 and 2018 are illustrated in fig 2 a total of 3282 papers were found with the soil and water assessment tool swat gassman et al 2014 neitsch et al 2011 overwhelmingly dominating the publication records accounting for 44 of the papers other highly published catchment models include hydrologic simulation program fortran hspf bicknell et al 2001 integrated catchment model inca whitehead et al 1998 and spatially referenced regressions on watershed attributes sparrow schwarz et al 2006 erosion models such as universal soil loss equation usle and variations renard et al 1991 and water erosion prediction project wepp flanagan et al 2007 are also highly cited the qual series brown and barnwell 1987 are the most cited in stream models together the above models account for a further 29 of the papers we identified note that this finding only applies to literature sources included in the scopus database e g journal articles conference papers book chapters grey literature such as government reports are not included in the database full names model types and key references of these models are provided in the supplementary table 3 current models and software model use the main catchment scale water quality models and platforms considered in this review include swat hspf inca and ewater source carr and podger 2012 although other models are also mentioned when comparison is felt warranted hspf and swat have been widely applied in the united states and worldwide inca was developed as part of european union funded projects and has been applied to a wide range of key european ecosystems the ewater source platform is being adopted in government agencies as australia s national framework for catchment models and is being used in international partnership projects across asia 3 1 model purposes broadly two types of model purpose are identified scientific exploration e g improving understanding of the sources fate of constituents in a water body through modelling and decision support e g assessing climate and or management impacts more specifically water quality models have been used for e g schwarz et al 2006 erosion water quality description or risk assessment describing past or present water quality conditions for an area on the basis of monitoring data extrapolation and or modelling can help extend point scale monitoring information to understand water quality status for a wider region containing the monitoring sites contaminant source analysis can help directly or through surrogate indicators in identifying and quantifying the sources of constituents and their strengths using spatially distributed or semi distributed models water quality simulation or scenario analysis using a calibrated model to predict constituent loads concentrations on the basis of a set of altered inputs such as climate hydrology and land use management scenarios examining the importance of explanatory factors and processes focusing on the calibration process and its results directly and or undertaking dedicated sensitivity and uncertainty analyses can help explore the predictive value of a set of potential explanatory variables and may also compare alternative mathematical forms design of monitoring networks water quality modelling and subsequent uncertainty analysis for informing the design of monitoring networks so that the utility of monitoring data can be optimised to meet objectives of monitoring modelling development and planning and balancing the need for obtaining a representative picture of regional water quality conditions and more detailed information on specific locations or problems early warning and forecasting providing time sensitive information to support decision making for drinking water supply environmental protection and emergency response e g maier and dandy 2000 water quality models vary depending on process representations and complexity the selection of appropriate models and the process and scale detail in them depends on the modelling purpose as well as the nature of the environmental system constituents of concern and the prior knowledge and data available for example models of different scales and complexity are needed to address management questions that focus on different aspects of spatial and temporal dynamics such as what are the annual average constituent loads at catchment outlets under different climate management scenarios what are the spatial and temporal distributions of target constituents under different climate management scenarios what are the relative contributions of various sources of constituents over time what are the forecasts of water quality to meet catchment water quality objectives or standards e g total maximum daily loads in the u s water quality reduction targets for the great barrier reef what are the likely pollutant attenuation trajectories and times to recovery under various management scenarios what are the effects of transient or extreme events how to handle model uncertainty using a risk based approach how much information is contained in the available data and how does this define the appropriate model complexity overly complex models may introduce uncertainty add unnecessary data collection and computational burden and shift focus away from problem solutions to endless analysis overly simple models may miss or inadequately represent key processes or may not address relevant management questions at appropriate scales at either extreme models may not be defensible in adversarial review 3 2 representation of constituents the representation of constituents in the models is shaped by the purpose and focus of the models noting that both may have changed throughout the history of development or use of particular models the purpose and focus define what constituents are modelled and how they are modelled and are tied in with choices around process representation as discussed in section 4 1 the water quality constituents most consistently represented in the commonly used catchment scale models are sediments nitrogen n and phosphorus p species table 3 these constituents are the focus of many catchment water quality modelling applications reflecting both the importance of these macronutrients to agricultural production and the potential ecological impacts of their exports from catchments into rivers and receiving waterbodies the forms of n and p considered are fairly standard across the models listed in table 3 nitrate nitrite ammonia and total n for nitrogen and dissolved and or particulate forms of organic or inorganic p whilst some models focus at least initially on a limited set of constituents others have been developed with a view to providing broader support to water quality planning by allowing assessment of a more comprehensive suite of constituents for example qual2e an instream water quality model developed for application to well mixed dendritic streams is capable of modelling the concentrations of 15 constituents for each defined reach conservative minerals coliform bacteria and non conservative constituents however it does not model generation and export of constituents from land and thus is sometimes linked to the swat catchment model hspf and swat draw upon decades of water quality modelling by incorporating existing models e g the qual series as component modules with modifications and development of additional modules as required with respect to modelling the fate and transport of pathogens many models use faecal coliforms as an indicator of pathogen concentrations e g qual2e but the use of indicator organisms like coliforms has been questioned with pachepsky et al 2016 concluding that coliform indicators are insufficient for providing information about the degree of contamination from particular pathogens there are however models that consider particular pathogens directly rather than through indicators e g ferguson et al 2007 giving a more complete picture of the potential sources of those pathogens and management options that can be used for reducing contamination risks most of the models produce spatially varied e g by subcatchments or hydrogeomorphic units daily time series outputs of constituent concentrations and or loads these outputs may be reported by process or land use for example inca n could output daily or annual land use specific n fluxes for all transformations processes at the land phase as well as time series of flow and n constituents discharged into a receiving water or stream the ewater source and sparrow platforms employ a similar approach in that they do not contain explicit models for any constituents instead they have inbuilt functions that users must parameterise in order to represent a particular constituent in ewater source simple linear and power functions based on flows are available in sparrow non linear regression equations based on landscape and channel characteristics are provided it is also possible to write customised functions or plug ins in ewater source to model specific constituents for example the dynamic sednet model has been developed as a plug in to ewater source for simulating suspended sediment loads in the great barrier reef catchments ellis and searle 2013 similarly the mike eco lab module in mike she graham and butts 2005 can be employed as a generic equation solver for simulating in stream processes of constituents and requires users to parameterise the generic functions to predict the transport of a specified constituent in soil and water all constituent outputs whether as loads or concentrations have certain spatial and temporal properties details on spatial and temporal scales of the models are described in sections 4 2 and 4 3 spatially and or temporally varied constituent outputs can be aggregated in various ways to produce summarised results for comparison norton et al 2003 these include instantaneous values of variables e g sediment concentrations or loads at catchment outlet integrals of variables e g annual sediment loads variation of variables e g standard deviation of sediment loads over space or time distribution of variables e g medium or 90th percentile of sediment loads over space or time proportion of sediment above a certain threshold location of hotspots e g the subcatchment contributing the highest loading of a particular constituent less frequently used aggregations may include frequency of events e g frequency of significant sediment output events size of events e g peak duration of significant sediment output events interval between events e g interval between significant sediment output events where short interval may indicate little ecosystem recovery time cumulative frequency distributions e g flow and dissolved constituents 3 3 scenario analysis and management practices scenario analysis is one of the main uses of catchment water quality models scenarios are typically constructed to explore the potential changes in constituent outputs under alternate drivers like climate conditions and or management practices to achieve this it is important to explicitly incorporate intended drivers and management practices in the model at the appropriate scales so that their effects can be modelled at regional scales climate land use change and flow management are important drivers locally four groups of management practices can be identified in water quality models point source management input managerial practices cultivation practices and structural practices hashemi et al 2016 thus point source management can include local nutrient retention such as wastewater treatment and installation of filter systems input managerial practices focus on control of water and constituent inputs to agricultural land and the reduction of constituent surplus and resultant transport to surface and groundwater systems examples include fertiliser applications livestock stocking rates irrigation management for water inputs cultivation practices and erosion control relate to reduction of constituent outputs to waterbodies by land management such as enhancing retention or promoting transformation to non reactive forms tillage methods contour strip cropping catch or cover crops and harvest operations are examples of cultivation practices gully and streambank stabilisation are examples of erosion control structural practices refer to regulation of water and constituent flows in the landscape such as buffer strips and wetlands most catchment water quality models have the capacity to model scenarios such as climate inputs change land use changes flow scenarios and point source control table 4 shows the capacity for four commonly used daily scale catchment models the models that have detailed representation of nitrogen and phosphorus cycles such as hspf and swat generally have a greater capacity to specify detailed management practices standard for these two models in the literature is the specification of the input managerial and cultivation practices particularly fertiliser and manure management application and tillage practices inca allows further discretisation of fertiliser scenarios by allowing the definition of fertiliser rates based on crop type similarly hspf allows the definition of monthly application rates to represent starter side dress and other fertiliser applications for each specific crop whilst assuming that fertiliser is applied in a manner that will not harm the crop swat has also had much development effort to provide the capacity to represent structural conservation practices there is some capability for modelling management impacts in ewater source such as riparian buffer and point source management note that the level of management representation is related to the spatial heterogeneity and scale of water quality models the large scale catchment model lascam for example operates at the subcatchment scale and thus is unable to distinguish between planting in the recharge areas of each subcatchment versus planting in the discharge zones viney and sivapalan 2001 even the more detailed and spatially disaggregated models require simplifications or assumptions to simulate particular interventions and the approach taken to spatially distribute the model will affect the modelling of management practices for example arabi et al 2008 highlighted that the way in which catchments are disaggregated prior to parameterising distributed models may affect the evaluation of management practices and interventions a previous study arabi et al 2006 examined how the effectiveness of practices simulated using swat changed with the number and locations of subcatchments defined in the model finding that sediment and nutrient outputs of the model were highly sensitive to subcatchment size compared to agricultural management and streambank management the literature on using water quality models to explore impacts on gully density exports and or stabilisation is relatively limited freebairn et al 2015 outlined how dsednet could be used to simulate the impact of scenarios of changes in rainfall and runoff and or land use change on the activity or density of gullies and the effectiveness of management interventions to stabilise gullies bastola et al 2018 incorporated dominant gullying processes into the child landscape evolution model and once calibrated for a small 4 km2 watershed used it to assess the effectiveness of gully backfilling and revegetation otherwise scenario modelling of ephemeral gullies has received some attention using annagnps in very small catchments e g taguas et al 2012 much of the water quality literature takes a standard approach to scenario analysis comparing scenarios of some reference e g current natural condition against a suite of scenarios representing combinations of alternate climate land use or land management practices for example water quality models have been used to predict the relative effectiveness of land use change versus changes in agricultural practices farkas et al 2013 and where and when best management practice bmp for different industries are most effective at reducing instream nutrients concentrations in order to meet a total maximum daily load tmdl usually determined at a compliance monitoring location santhi et al 2001 vigiak et al 2016 used scenario modelling to understand the impact of current riparian lands in reducing sediment fluxes in freshwater systems the authors calibrated and validated the swat model under the current conditions and then formulated three scenarios one that removed the riparian filtering service one that set the streambank vegetation cover to its minimum value and lastly one with both of the previous changes implemented analysis of trade offs among different objectives has also been applied examples include lautenbach et al 2013 and bostian et al 2015 where the non dominated sorting algorithm nsga ii was employed to analyse trade offs among crop production water quality and quantity exploratory approaches to scenario analysis that address uncertainty such as scenario discovery bryant and lempert 2010 robust decision making lempert and collins 2007 or dynamic adaptive policy pathways haasnoot et al 2013 have not been widely applied in the field of catchment water quality modelling 3 4 software availability and documentation unsurprisingly open source or public domain models with an extensive development history such as swat and hspf alone or within the basins package set the standard in terms of availability and comprehensiveness of supporting tools and documentation as well as access to the source code and executables table 5 within the united states swat alone has established a wide user base including the nrcss epa texas river authorities noaa universities and environmental consulting firms there is also a substantial worldwide swat community there has been much investment in educational resources and updated swat literature databases and the development of supporting tools to aid the set up evaluation and assessment in swat these include gis interfacing tools for qgis and arcgis a parameter estimation tool a water ecosystem modelling tool an output viewer tool an input check tool a climate model data tool and a calibration uncertainty and sensitivity tool see http swat tamu edu for further detail accessed 16 01 2018 in australia a similar philosophy of access and provision of support was generally evident with e2 and watercast predecessors of ewater source albeit with an understandably smaller resourcing base than the us agency supported models e g hspf sparrow swat however since 2007 ewater source has become an industry standard for water quantity quality modelling within australia it is a semi commercial product with restrictions for the free public version other approaches to software distribution include the provision of access to closed source executables e g inca https www niva no en projectweb inca accessed 15 01 2018 with or without costs mike she is a commercial product and while software and associated user manuals can be downloaded from the dhi website a licence is required for access to fully featured software 4 current models and software model development 4 1 process representation appropriately representing important processes in a model is a fundamental aspect of water quality modelling especially at the catchment scale where complexity of the system and lack of observations bring extra challenges arnold et al 2015 presented case studies in which a model showed excellent statistical agreement with measured stream gauge data but misrepresented processes water balance nutrient balance sediment source sinks within a catchment this resulted in errors when exploring the effects of management scenarios for example underestimation of upstream sources may be negated by overestimation of downstream sources a good fit at a downstream monitoring site may still be achieved and the implication is that the effects of management occurring in the upstream source areas will be underestimated in general there are three main processes to be considered when modelling the fate of constituents in a catchment i generation ii filtration delivery from sources to channels and iii in stream transport which may include deposition remobilisation and transformation most of the commonly used models attempt to represent one or more of these main processes with different conceptualisations a comparison of process representation in four established commonly used catchment models is listed in table 6 in terms of the hydrological process all models account for both surface water and shallow groundwater dynamics but deep percolation is assumed lost from the system recent development in catchment models has seen increasing research into the integration of catchment models e g swat with more sophisticated groundwater models e g modflow to better represent feedback fluxes between surface water and groundwater guzman et al 2015a wei et al in press in terms of the constituent generation filtration and delivery and in stream processes some of the most common conceptualisations are discussed below 4 1 1 constituent generation conceptualisations of the constituent generation process range from using static export rates e g mass ha yr from a land use to capturing event based processes through event mean concentration emc and dry weather concentration dwc to representing constituent generation processes in more detail this applies to both erosion modelling and simulating nutrient processes in the land phase the event mean concentration dry weather concentration emc dwc model was originally used for urban stormwater systems but has been applied in catchment water quality modelling when observational data are insufficient to inform the development of more detailed generation models the model considers two constant concentrations of constituents i e emc and dwc combined with flows emc estimates sediment or nutrient loads from events and dwc estimates loads from base flow different emc and dwc values can be generated to represent different land uses approaches to calculate emc values vary and have been reviewed by bartley et al 2012 the authors reported that emc values can be highly uncertain due to approaches to calculating emc water quality sampling methods definitions of events and measurements of concentrations from different catchment sizes and or land use compositions the emc dwc model by using constant emc and dwc values over time has been criticised for not being able to capture changes in management practices other than land use change for example the effects of changes in fertiliser application on nitrogen emc and dwc in addition a constant emc value e g derived from mean event concentration measurements may not capture the dynamics of relationships between concentrations and flow to address these limitations the rating curve method has been used to capture the non linear relationship between concentrations and flows horowitz 2003 usle based approaches including rusle renard et al 1997 and musle williams and berndt 1977 are widely used in simulating catchment scale sediment generation lascam uses usle for hillslope erosion swat invokes musle which focuses on storm based sediment generation using runoff as the driver of sediment generation combined with sediment lag to represent sediment delivery to channel sednet wilkinson et al 2009 uses rusle which focuses on rainfall erosivity as the main driver coupled with sediment delivery ratio sdr to account for sediment delivery to channel erskine et al 2002 reported that musle performed marginally better than rusle for estimating sediment yield in small basins near sydney sadeghi et al 2014 reviewed the use of musle worldwide and reported that it provides good predictions if applied in similar conditions on which it was developed that is for storm based sediment yields at subcatchment scales 15 1500 ha slopes from 0 9 to 5 9 and slope lengths of 79 174 m in forest and grassland areas with appropriate calibration applications of musle under other conditions could produce good results sadeghi et al 2014 the inca sed model is driven by actual precipitation hydrologically effective rainfall her the portion of the actual precipitation that generates an in stream runoff response or a mixture of the two lazar et al 2010 the runoff oriented conceptualisation is helpful when accounting for 1 protection of soil by snow cover or high erosion rates caused by snow melt 2 development of crust caused by drying of clay or organic soils which protects the soil against splash erosion and flow erosion and 3 filling of soil water storage after a dry period where erosion only occurs when soil is saturated in contrast hspf is a process based model that simulates detailed processes of erosion such as sediment detachment re attachment of detached sediment wash off of detached sediment and scouring of the soil surface bicknell et al 2001 compared to sheet and rill erosion the gully erosion process is more complex and difficult to predict and gully erosion modelling is still under represented in the literature valentin et al 2005 three types of gullies have been identified ephemeral gullies small channels eroded by concentrated overland flow that can be easily filled by normal tillage only to reform again in the same location after additional runoff events permanent or classical gullies channels too deep to ameliorate easily with ordinary farm tillage equipment and which typically range from 0 5 to 30 m in depth and bank or edge of field gullies develop wherever concentrated runoff crosses an earth bank poesen et al 2010 only a few water quality models include the prediction of soil loss from ephemeral gully erosion examples include field scale models such as creams knisel 1980 and wepp hspf scouring process and ephemeral gully erosion model egem woodward 1999 these models estimate concentrated flow detachment based on flow shear stress exerted on the bed material the critical shear stress and the transport capacity of the flow and the sediment load poesen et al 2003 egem was improved and later implemented in the annagnps model gordon et al 2007 whilst creams together with gleams and epic were the base for swat krysanova and arnold 2008 sednet simulates long term average sediment yields from permanent or bank gullies wilkinson et al 2009 the model relies on inputs from gully mapping to provide information on gully length and cross sectional area and assumptions on the age of the gullies poesen et al 2003 noted that compared to prediction of soil loss from gully erosion the prediction of gully location including the initiation and end of gully erosion was less studied this remains the case and there are few models that are capable of predicting gully initiation and progression similar to sediment modelling the conceptualisation of nutrient generation in the land phase may range from simpler regression methods to representing nutrient cycling for example ewater source implements simple approaches such as emc dwc and rating curve methods as mentioned earlier in contrast swat and hspf simulate detailed nitrogen and phosphorus cycles bicknell et al 2001 neitsch et al 2011 for the nitrogen cycle swat monitors five pools of nitrogen in soil two mineral nitrogen pools including ammonium and nitrate and three organic nitrogen pools including active humus stable humus and fresh plant residue nitrogen is added to the soil through fertiliser manure or residue application and is removed from soil through plant uptake leaching volatilisation denitrification and erosion within the soil processes modelled include mineralisation decomposition immobilisation and nitrification six pools of phosphorus are modelled in swat including three organic phosphorus active stable and fresh and three inorganic phosphorus stable active and solution pools processes modelled for phosphorus cycling include application of fertiliser manure or residue plant uptake erosion leaching mineralisation decomposition immobilisation and sorption hspf simulates similar processes for nitrogen and phosphorus cycles but different algorithms are employed for some processes inca n also considers transformations of mineral nitrogen and includes processes such as mineralisation immobilisation and nitrification however it assumes an unlimited organic nitrogen pool and thus transformations within the organic n pool are not simulated wade et al 2002 4 1 2 filtration and delivery in terms of the landscape filtration delivery process from the landscape to stream networks model conceptualisations range from using i a constant e g delivery ratio ii a function of attribute s such as vegetation or terrain to iii more detailed pathways of movement and transformation in this paper we adopt the term proposed by hoos and mcmahon 2009 i e landscape delivery ratio to refer to the capacity of a landscape to deliver constituent loads to a channel which is different from the in stream delivery ratio many factors may influence the landscape delivery ratio such as rainfall overland flows topography vegetation and soil properties and their complex interactions beven et al 2005 fully accounting for these processes and pathways can be difficult at medium to large catchment scales for large scale catchment models where data are limited a fixed landscape delivery ratio is often used for example in the ewater source platform at each time step a fixed percentage of the constituent mass load associated with the quick flow and a fixed percentage of the constituent associated with the baseflow are removed ewater 2017 other modelling frameworks such as inca sed lazar et al 2010 and rusle2 foster 2013 adopt a concept of sediment transport capacity sediment transport capacity is an estimate of the maximum amount of material that is transported from the landscape to the stream not in stream transport capacity in inca sed this capacity is calculated based on subcatchment area and reach length lazar et al 2010 when the pool of readily transportable sediment in the landscape sstore and mass mobilised through splash erosion ssp exceeds the sediment carrying capacity of overland flow stc the mass of sediment transported to the stream qsed equals stc otherwise qsed equals the sum of ssp and flow erosion sfl similarly rusle2 uses the transport capacity concept to compute deposition of sediment before reaching the stream foster 2013 in contrast to hillslope transport capacity swat uses surface runoff lag to simulate the delivery of sediment nutrients from landscape to channel neitsch et al 2011 this applies to situations when the time of concentration i e the amount of time for water to flow from the remotest point in the subcatchment to the subcatchment outlet is greater than 1 day e g in large subcatchments and thus only a proportion of the surface runoff and subsequent sediment and nutrients reaches the channel on the day it is generated this proportion is calculated based on time of concentration t conc and a surface runoff lag coefficient surlag i e 1 exp surlag t conc the rest of the sediment or nutrients is stored and added to the supply for the next day in terms of nutrients nitrate in swat is delivered from landscape to channel through surface runoff lateral flow or percolation neitsch et al 2011 delivery of organic nitrogen and phosphorus through surface runoff is accounted for through use of the nutrient enrichment ratio a process where as sediment is transported from landscape to channel sediment becomes enriched with clay particles and consequently greater concentrations of organic nutrients attached primarily to clay particles are found for those delivered to channel than those on the landscape sparrow uses a land to water delivery factor to account for the delivery of constituent loads generated from a subcatchment to channel schwarz et al 2006 it is expressed as an exponential functional form determined by a vector of delivery variables and associated coefficients the delivery variables which are user specified identify factors affecting delivery of constituent loads to channels examples include soil permeability land surface slope and wetland area the process of riparian filtration is sometimes included in catchment model platforms such as ewater source and swat to account for effectiveness of riparian management in reducing constituent loading to rivers and streams in ewater source ewater 2017 the load based sediment or nutrient delivery ratio method uses simple rule based relationships between sediment nutrient delivery ratio their loading rate and loading rate thresholds the amount of trapping that occurs in the riparian zone is given by a sediment delivery ratio that increases linearly from 0 to 1 depending on loading per unit vegetation filter length sediment delivery ratio remains zero when sediment loading rate is below a sediment loading rate threshold slrt and remains 1 when the loading rate is above a sediment loading rate at sill slrs threshold ewater source also includes a riparian particulate model rpm newham et al 2005 for riparian buffers which accounts for trapping of fine particulate through infiltration and adhesion at a daily scale using as many as 28 parameters that can vary spatially it empirically simulates the loss of particulate constituents using characteristics of the buffer soil vegetation and flow in comparison swat uses an empirical model derived from simulations from the vegetative filter strip model vfsmod munoz carpena et al 1999 to calculate sediment reduction sediment reduction is linearly and negatively related to sediment loading and runoff volume per unit vegetation filter strip area and positively related to saturated hydraulic conductivity of the soil neitsch et al 2011 the latter two variables explain runoff reduction reduction rates of total nitrogen and phosphorus are related to sediment reduction whereas reduction rates of nitrate and soluble phosphorus are related to runoff reduction 4 1 3 in stream processes in stream transport processes for constituent loading can be represented either through empirical routing functions or more detailed algorithms that attempt to represent complex interactions of constituents for in stream transport of constituents a closed system constituent load balance approach is often used this assumption may be violated for some situations for example when 1 sources of atmospheric dust and chemicals from other catchments and or in rainfall or groundwater flow become important 2 a constituent is non conservative in stream or 3 there is activation of pathogens although these sources and processes may be considered in some landscape component of the catchment models e g nitrogen cycle in swat they are generally not considered in the in stream transport component sediment transport capacity is used in swat and dynamic sednet an ewater source plugin ellis 2017 to capture deposition of fine sediment in channels although the algorithms to calculate the transport capacity are different for these two models swat has four stream power algorithms to estimate the transport capacity of a channel based on non linear expressions that involve peak velocity the default stream power model is the bagnold stream power equation bagnold 1977 in dynamic sednet remobilisation of fine sediment is also simulated in addition to deposition this model requires the daily calculation of sediment transport capacities for deposition and remobilisation of fine sediment inca sed applies stokes law to compute deposition of sediment and the mass of suspended sediment deposition is calculated as the product of settling velocity and sediment mass in suspension the latter is also calculated from effective entrainment based on the bagnold stream power equation bagnold 1977 in terms of the modelling of bank erosion rates different conceptualisations have been adopted in catchment models at the simpler end of the spectrum inca sed estimates bank erosion rates based on a simple power function of discharge lazar et al 2010 in dynamic sednet the rate of bank erosion is simulated based on stream power which is linearly related to bankfull discharge and river bed slope and an erodability factor which represents the combined effects of riparian vegetation and bank erodability wilkinson et al 2009 in contrast the potential rate of bank erosion in swat is predicted based on a threshold response through the excess shear stress equation which assumes bank erosion occurs when bank effective shear stress is greater than the bank critical shear stress neitsch et al 2011 the effect of bank vegetation is accounted for through the calculation of critical shear stress there are several models that specifically simulate in stream water quality some of which have been incorporated into catchment models to represent in stream processes examples include the qual series with the latest versions being qual2e brown and barnwell 1987 and qual2k which is implemented in excel chapra and pelletier 2003 qual2e was modified and implemented in iqqm as its water quality module simons et al 1996 qual2e has also been loosely coupled with swat with swat producing catchment inputs of water and constituents and qual2e simulating instream processes migliaccio et al 2007 qual2k includes the modelling of in stream carbon nutrient cycles dissolved oxygen and anoxia simulation sediment water interactions algae light extinction ph and pathogens wasp is a general dynamic mass balance framework for modelling fate and transport of contaminants in surface waters wool et al 2006 which has been included in the basins framework it can be applied in one two or three dimensions the latest version wasp7 includes the simulation of heat eutrophication toxicants and mercury 4 2 spatial heterogeneities and scales spatial discretisation is a key consideration in conceptualising catchment scale water quality models it can assist in identifying the sources of constituents that contribute the higher loading rates and therefore in targeting investment in catchment monitoring and management sparrow is reported to provide consistent results that allows users to identify major sources and environmental factors affecting nutrient fate and transport at the regional to subregional scale preston et al 2011 many other catchment scale models use either a fully distributed approach or use hydrological connectivity to break the catchment into subcatchments from this structure a node link network is formed where nodes represent subcatchments and links represent river reaches within a subcatchment four major types of spatial discretisation have been characterised by arnold et al 2010 as 1 no discretisation or lumped spatial approaches where the subcatchments are represented by a fixed set of properties such as dominant soil land use and slope with this type management changes in a specific subarea for example cannot be represented to have a differential effect than say changes elsewhere in an equivalently sized area fig 3 lascam sivapalan et al 2002 is such an example 2 semi distributed spatial approaches based on properties of land use soil type and topography like slope examples of such models are the original formulation of swat which uses hydrologic response units hrus moriasi et al 2012 the functional units fus of ewater source wilkinson et al 2009 and the landscape units in inca sed lazar et al 2010 as interactions between spatial units are not considered the effect of catchment management practices at upslope spatial units is not simulated differently to that at downstream spatial units 3 semi distributed approaches based on topographic positions in this type a representative hillslope is used to represent the landscape in a subcatchment bonumá et al 2014 for example a subcatchment may be separated into three spatially connected landscape units being upslope midslope and valley bottom areas uniform properties within a landscape unit are assumed and thus the unit does not distinguish between different climate regions nor covers variable slope conditions examples include the land segment in hspf bicknell et al 2001 and overland flow elements ofes in wepp flanagan et al 2012 4 fully distributed spatial approaches where a subcatchment is divided into hydraulically connected elements such as grids or triangular elements each element has its own combination of properties such as slope land cover soil etc this type demands substantial amounts of data and typically has much larger computational requirements than lumped or semi distributed approaches and thereby is not considered feasible for large scale catchments pignotti et al 2017 the grid approach is implemented in wam bottcher et al 2012 annagnps bosch et al 1998 and answers beasley et al 1980 the property based semi distributed approach is the one most commonly used for catchment models each spatial unit e g hru fu represents similar hydrological behaviour land use tends to be assigned simply such as forest grazing cropping and urban areas the approach does not capture spatial interactions between the spatial units with consequences for the modelling of management practices nor does it capture the effect of distance of a spatial unit to streams and it cannot account for potential throughflow entering a spatial unit from neighbouring upland spatial units knowledge of drainage or channelization structures within a spatial unit e g a horticulture drainage system while important in managing constituent transport often cannot be captured adequately in a subcatchment scale stream network investigations are ongoing to develop a hybrid spatial discretisation approach between property based and position based semi distributed approaches earlier works include j2000 s fink et al 2007 and ages w ascough et al 2015 which both employ a topological routing scheme to account for spatial connections between hrus a combination of landscape units and hrus was developed by volk et al 2007 and implemented in the new version of swat swat bieger et al 2017 to address the limitations of typical hrus which have no spatial interconnectivity ning et al 2015 propose another approach involving the generation of spatially continuous hrus in a subcatchment with explicit hydrological properties land cover soil type and slope and a specific location in theory spatial units can be defined in such detail as to capture the factors deemed important relating to spatial positions of the property based spatial unit however the level of detail needs to be consistent with the information available so that model uncertainty and runtimes are not increased unnecessarily 4 3 temporal dynamics the representation of temporal heterogeneity of constituent fluxes is another important consideration in the conceptualisation of water quality models ideally model conceptualisation should include the selection of suitable temporal resolutions and incorporation of other temporal dynamic processes in the models that are scientifically valid useful for management and feasible in terms of resource and data availability a typical phenomenon termed hot moments by mcclain et al 2003 is that constituents can display disproportionally high export rates over a short period of time such as when fertilisers and pesticides are applied thus daily or coarser representation of flow may not adequately capture the hot moments of constituent generation and transport yang et al 2016 other factors can also suggest a requirement for sub daily models one example being the coupling of outputs of a water quality model to a receiving water model where traditional hydrodynamic and biogeochemical process models of fluid dynamics operate on hourly or lower time steps set against this is the fact that most water quality models operate on a daily or larger time step however in a more recent development of swat sub daily processes such as weather and infiltration processes and channel routing have received more attention but this functionality is generally invoked for modelling urban stormwater management or small areas of agricultural land e g maharjan et al 2013 on the other hand sometimes the management questions being asked of water quality modelling can be simplified to facilitate the use of daily or larger time steps if the modelling objectives are related for example to long term loads of a conservative constituent then a daily time step or even monthly may suffice alternatively it might be convenient to formulate a management question in terms of risk wherein one asks what conditions and modelling assumptions might cause a problem these sorts of simplifications become desirable when sub daily simulations of water quality for catchments overwhelm existing model support infrastructure e g input data processing runtimes data storage and post processing calibration data calibration timeframe perhaps pervasively problematic with sub daily simulations is that they require long term temporal resolution in rainfall records and flow gauge data which are often not available in most areas sub daily simulations also pose challenges in data quality assurance analysis and requirements as software to perform these checks is not readily available and existing capabilities are not well utilised therefore the requirement to increase temporal scale of constituent generation should be assessed on a case by case basis and reflection be given to what management questions can be answered with the resources available 4 4 data requirements and parameters 4 4 1 input data requirements the input data needed to satisfy the data requirements of the common water quality models and software platforms vary from moderate e g ewater source to high e g hspf reflecting their different process representations and temporal and spatial resolutions table 7 most of the catchment models require basic inputs such as climate typically rainfall and potential evapotranspiration for rainfall runoff models dem for generation of stream networks and spatial segregation of a catchment and mapping and classifications of land use the main difference in terms of input data requirements between catchment water quality models lies in the number of inputs for each of the data categories meteorological and or hydrological data catchment stream geometry and use land cover land management soil data and constituent data more complex models such as hspf and swat include the simulation of snow melt which adds additional data requirements swat also implements qual2e as its in stream model which has 100 individual inputs across three categories stream or river system inputs defined for each reach or smaller element global variables including water quality constituents and some physical characteristics and forcing functions such as flow water quality characteristics and local climatology inputs complex models may be difficult to parameterise or apply well to data sparse environments to address the challenges and risks of applying models in such a situation swat and hspf modelling communities have invested in tools and data sets to support their application to data sparse environments for example swat includes a number of weather generators which can downscale monthly climate data to daily data required for the models for hspf the minimum data required to model water quality is a dem land use data and some weather data additional data may be needed to run hspf such as soil data for erosion and sediment transport modelling applications stream network data to improve watershed delineation and watershed analysis and flow data for calibration purposes these data can potentially be sourced from global databases in the absence of higher resolution local data crossette et al 2015 there are implications for the interpretation of model results and analysis of model performance when there is little local data to provide input to or test any model see sections 5 and 6 3 however this is balanced by the value gained when model developers provide initial default input and output data sets this can encourage model users to source reasonable values as errors in input values and or units can severely compromise model results many developers provide user manuals that contain this information 4 4 2 model parameters the number of parameters required to simulate water quality depends on the model structure the constituent selected and the number of spatial units in general the more empirically based models e g sparrow require less parameters than those with more process based models e g swat table 8 model parameters can be measured e g soil bulk density estimated through literature or databases e g curve number or calibrated e g delivery ratio it is suggested that we should use measurements as much as possible and minimise the number of parameters needed for calibration malone et al 2015 refsgaard 1997 over parameterisation tends to be a pervasive feature of distributed models this is sometimes addressed by parameter reduction such as by fixing spatial patterns of a parameter but allowing its absolute value to be calibrated refsgaard 1997 alternatively the size and complexity of a model are addressed by providing constraints that regularise the calibration problem when many parameters are used in a calibration whittaker et al 2010 the simplest constraints are where some parameters are fixed at particular values but constraints can also be bounds or probability distributions model calibration is discussed further in the next section 5 current models and software model performance 5 1 calibration and validation calibration and validation are major challenges for catchment scale models compared to point scale or field scale models catchment scale models often involve quantity and constituent fluxes with respect to multiple spatial units draining to connected channels spatial variations in these multiple spatial units and additional processes and their interactions such as for in channel floodplain reservoir and wetland representations arnold et al 2015 some review papers have been published recently which give a good overview of the current practices and recommendations for model calibration arnold et al 2015 moriasi et al 2012 2015 single site calibration validation is generally recommended for areas with uniform characteristics e g soil slope vegetation meteorology while multi site calibration validation is recommended for large areas with more varied complex physical characteristics and or when observed data for a given process are available at multiple locations within the study area moriasi et al 2015 evidence for the benefits of the multi site approach for large catchments is mixed for example shrestha et al 2016 reported that multi site calibration did not improve simulations of flow and sediments compared to single site calibration however simulation results for tn and tp loads improved in terms of calibration applying an appropriate and systematic strategy is essential the appropriate approach however depends on the complexity of the application and ranges from a single stage for a simple model looking at a single process to a stepwise iterative approach for complex models with multiple processes and parameter interactions daggupati et al 2015 a stepwise iterative approach applies to the use of multiple parameters to calibrate single or multiple output variables each parameter is optimised in sequence prior parameters are readjusted after optimising successive parameters so that changes in successive parameters have not shifted the optimal value daggupati et al 2015 currently the general sequence of calibration in catchment models is 1 calibration of hydrology parameters at multiple time scales focusing on baseflow and storm runoff 2 calibration of sediment focusing on the ratio of upland and channel sources and 3 calibration of nutrient and pesticide parameters arnold et al 2015 calibration can be undertaken manually and or using automated calibration software coupling of manual and automated calibration is often recommended for catchment models van liew et al 2005 reported that manual adjustments may be necessary following auto calibration to maintain mass balance and adequately represent the range in magnitude of output variables developing and testing of auto calibration techniques has been a rapidly developing field although most applications are related to the hydrology component of catchment models confesor and whittaker 2007 seong et al 2015 van griensven and meixner 2007 van liew et al 2005 for example hspf is typically calibrated manually assisted by the expert system for the calibration of hspf hspexp hydrology only duda et al 2012 there have been attempts to calibrate hspf models using the parameter estimation software pest doherty and johnston 2003 but the gauss levenberg marquardt glm search algorithm employed in pest can have difficulty in finding a global optimum solution and its performance can depend on initial parameter sets defined by users seong et al 2015 more recently an automated calibration tool for hspf hspf sce was developed using r to link hspf and the shuffled complex evolution optimisation algorithm sce ua seong et al 2015 ewater source has a calibration wizard built in to calibrate the hydrology component of the model including rainfall runoff and routing primarily for unregulated systems ewater 2017 the calibration wizard allows the selection of different optimisation functions including sce ua uniform random sampling and rosenbrock a local optimiser ewater 2017 swat cup swat calibration uncertainty procedures current version 5 is freely available software which links swat models with a range of tools to support calibration sensitivity and uncertainty analyses abbaspour 2015 mike she has autocal as a built in sensitivity analysis and calibration tool a review of calibration methods of 22 models including nine models that can be applied at catchment scales is provided in moriasi et al 2012 calibration and validation of water quality models require the consideration of the allocation of spatio temporally distributed data daggupati et al 2015 summarised three different approaches to data splitting temporal split sample applied when catchment conditions are stationary and sufficient temporal data are available for the studied site differential split sample applied when the model used to predict scenarios is greatly different from existing conditions such as climate change or land use change and the proxy basin approach applied when there is insufficient data for temporal split sample approach but data available for a similar site the temporal split sample approach remains the most commonly used method in which the observation data are split into two or more parts one for calibration and one for validation daggupati et al 2015 a range of statistics and qualitative methods can be used as model performance measures and criteria for calibration and validation bennett et al 2013 sometimes different statistics may be necessary to address different processes or requirements for stream flows base flows and or constituent loads concentrations moriasi et al 2007 moriasi et al 2007 recommended that three quantitative statistics be used in model evaluation these are the nash sutcliffe efficiency nse the percent bias pbias and the ratio of the root mean square error to the standard deviation of measured data rsr in addition to graphical techniques in addition to selection of statistical measures daggupati et al 2015 and bennett et al 2013 point out the importance of linking calibration measures with goals of model use for example applications where there is greater interest in simulating relative results e g comparing scenarios may focus on model performance using differences in observed data relative to a baseline rather than on the absolute results the latter may be useful in comparing model outputs to given criteria or thresholds such as those specified in water quality guidelines in practice most models are calibrated or validated to the flow and load or concentration at one or more stream gauges where data are most likely to be available however arnold et al 2015 suggested it is also important to calibrate validate for sediment nutrient balance and the calibration of sources and sinks this is because errors in model conceptualisation or development may lead to overestimation of one process and compensating underestimation of the other leading to overall good fit at stream gauges when using such a model for scenario analysis for example testing management options that focus on a particular process the model may give incorrect results because the process is not well represented in the model although hard data i e long term measured time series data typically at a point within a catchment are often not available for the calibration of sediment nutrient balance or sources and sinks soft data i e information that may not be directly and or routinely measured in the study area may be useful arnold et al 2015 more detailed discussion on soft data is provided in section 6 3 1 with this concept swat check was developed to create process based figures for visualisation of the appropriateness of output values in constituent balance e g sediments from different sources and sinks and alert model developers to output values outside the typical range white et al 2014 5 2 uncertainty tools the importance of addressing uncertainty in model predictions is widely recognised in water quality modelling especially if the models are used for risk assessment and or decision making harmel and smith 2007 reckhow 1994 abbaspour 2015 argued that uncertainty and calibration are intimately linked model calibration is conditional e g type and length of data used objective function definition optimisation routine and thus the results of calibrated models along with their uncertainties are conditioned on the assumptions explicit or implicit in the model and calibration although many model platforms provide auto calibration tools provision of uncertainty tools is not common examples of inbuilt uncertainty tools with the model platform include swat cup abbaspour 2015 and qual2eu which allow the user to assess the effect of model sensitivities and of uncertain input data on model forecasts using first order error analysis and monte carlo simulation brown and barnwell 1987 most uncertainty studies of water quality models are employed using model independent tools matott et al 2009 identified 65 evaluation tools for environmental models including 26 uncertainty analysis tools such as simlab glue pest and mcat although tools are abundant these tools are developed using different programming languages input output file formats compilers and development platforms making it awkward to readily integrate them into existing models matott et al 2009 commonly used uncertainty quantification methods for hydrology and water quality models include monte carlo simulation first order analysis foa or gaussian approximation kalman filtering generalised likelihood uncertainty estimation glue bayesian analysis and mean value first order reliability analysis method mform each method has its own advantages and requirements guzman et al 2015b more detailed discussion on uncertainty analysis methods is provided in section 6 3 3 6 discussion of challenges and emerging topics the previous three sections explored the attributes of water quality models based on some of the key catchment water quality models or platforms in the international scientific literature drawing on these we identified key challenges or emerging topics in the development or use of water quality models from the model use perspective key challenges or emerging topics include large scale applications model integration improving model usability and communication from a model development point of view we would like to draw attention to preliminary data analysis modelling management practices and technology advancement in terms of model performance three topics stand out for more attention incorporating soft data model identifiability and restructuring and advances in analysis of uncertainty and its management overarching challenges relate to adherence to best modelling practices and capacity building of modellers and model users and crucially the inherent challenge in separating the effect of climate from the influences of land cover or practice on water quality some of these discussions are also supported by other authors pechlivanidis et al 2011 rode et al 2010 volk et al 2009 6 1 model use 6 1 1 large scale applications around the world water quality models are increasingly being applied to large river basins typical applications have drainage areas between hundreds to thousands of km2 borah and bera 2004 wade et al 2002 more recently water quality models have also been used for mega catchments for example sparrow has been used to assess tn and tp sources in the mississippi and atchafalaya river basins 3 million km2 alexander et al 2008 ewater source with customised dynamic sednet plugins is being used for predicting sediment yields in the great barrier reef catchments in australia 420 000 km2 ellis and searle 2013 these models are intended to help the water quality community make large scale management decisions that will have significant costs and implications for funding arrangements in addition environmental forecasting programs are being actively developed around the world pelletier and tyedmers 2010 tilman et al 2001 the ability to forecast water quality changes at large scales especially under potential extreme event pressures flood and drought may become increasingly desirable by government agencies there are several challenges associated with large scale applications of water quality models natural complexity spatial heterogeneity and sparse measurements are always an issue but are exacerbated for large scale applications jakeman et al 1998 we are tasking the models to address more complex systems and pollutant types ambrose et al 2009 and more challenging management questions including balancing complex management objectives across different scales davis et al 2017 barriers in data availability and model calibration can be significant for large scale applications abbaspour et al 2015 to address these challenges we need to be more careful to ensure the models we develop or use have the right complexity reaching a balance between developing an overly aggregated model versus one with so much detail that model set up and application is prohibitive is one of the top priorities for large scale applications if the research community were to actively reflect and publish on the complexity of their catchment models this would provide a valuable resource for readers in terms of making choices relating to model complexity for their own case studies 6 1 2 model integration catchment water quality models are increasingly being used in conjunction with receiving water models debele et al 2008 another type of integration that has received less attention is between water quality models and socio economic models at a local level such integration allows us to explore trade offs such as between water quality protection and farm production kaim et al 2018 at broader scales water quality models can be a component of modelling for assessing water energy food nexus issues in this context integration of water quality models and economic production models can help us answer questions about the impact of broad policies such as the promotion of biofuels ambrose et al 2009 however the complexity of water quality models may hinder both their integration with other models and the computational efficiency of the integrated models in addition as models are linked to others to create integrated models some uncertainties in each model component can accumulate through the integrated system while others may average out to address these challenges we need to investigate and develop more capable frameworks that allow efficient transfer of outputs including feedbacks between different types of models 6 1 3 model usability and communication catchment water quality models can produce a large amount of output data over space and time for different constituents with outputs presented in different ways e g trends statistical summaries when models are used for exploratory scenario analysis a large number of scenarios can be produced and there is a need to communicate consistency relevance and usefulness of the scenarios communicating such complex multi dimensional results is not trivial in addition recipients of model based knowledge may need to know the assumptions uncertainties and limitations of the models and how these affect the interpretation of model results for model users in particular and also understanding the transferability and applicability of the models for other case studies e g for modellers however interactions between developers of model frameworks model users and those with long term catchment understanding e g landholders are often limited by timeframes and resources for model development to address these issues we need to improve visualisation tools to allow modellers and decision makers to better understand insights generated from the modelling results ambrose et al 2009 develop methods for greater inclusion of stakeholder knowledge and interaction to improve understanding of catchment dynamics invest in more than just the models but also user interfaces user groups manuals training and model needs analysis establish better communication between model developers and model users and those impacted by model outputs government agencies landholders community improve model documentation including model performance and mapping of what we need from the system against what the model can do these activities should be driven both by funders of water quality model development and the research and model development community it requires attitudinal change in prioritising model usability and communication reflection from the modelling community including users and developers in terms of what works and does not work in improving model usability and communication and explicit resourcing of such activities there is a general trend across the modelling community to incorporate models and model results within decision support frameworks that may allow for greater interaction with model outputs mcintosh et al 2011 these frameworks help improve the communication of model results but also allow for the assessment of a number of scenarios or different model parameterisations in similar formats the intent is to reduce the complexity of model outputs into simpler representations of the information although this can sometimes lead to loss of understanding in the complexity of simulated natural systems in addition decision support systems can be useful repositories of the model development process kelly 2015 for instance the watershed analysis risk management framework warmf includes a consensus module which takes the form of a road map to guide stakeholders through the consensus building process chen et al 2004 an iterative process is proposed to enable social learning of the stakeholders so that a clear vision of what they need and want from a decision support system can be articulated and implemented volk et al 2010 6 2 model development 6 2 1 preliminary data analysis water quality modelling can benefit from preliminary data analysis to understand system response and deficiencies in the data high frequency observations for example can permit the use of sophisticated tools such as spectral analysis e g neal et al 2013 to gain understanding of the dynamics driving variations in water quality constituents unfortunately some necessary observations e g nutrient concentrations are not available at most locations limiting the use of such tools in many places we tend to be limited to sparsely sampled water quality information although information such as flow ec temperature and turbidity are increasingly being collected routinely if a small amount of high frequency event based data is available then tools such as correlation analysis between constituent concentrations and continuously monitored quantities including potential drivers like streamflow and rainfall as well as other relevant quantities such as ec or turbidity can be useful in gaining understanding of the dynamics drewry et al 2009 estimation of the groundwater contribution baseflow to total streamflow e g croke 2010 can also be useful in understanding the variations in observed concentrations particularly for constituents entering the stream from aquifers regression is a commonly used approach to convert patchy observed water quality data into information that can be used in a water quality model letcher et al 2002 this can include relating the concentration of a constituent e g tss to flow or a less sampled constituent e g particulate p to a more frequently measured quantity e g turbidity regression methods have been used extensively in constituent load estimation several tools have been developed and compared lee et al 2016 ullrich and volk 2010 such as the usgs loadest runkel 2013 runkel et al 2004 fluxmaster saad et al 2011 developed for use in sparrow and weighted regression on time season and discharge wrtds hirsch et al 2010 the regression methods employed in these tools were based on or improved upon the model described in cohen 2005 and cohen et al 1989 which consists of inputs in flow decimal time and seasonal and or period factors the result of such regressions needs to be adequately tested including reporting the uncertainty in the regressed values care also needs to be taken in the choice of function being fitted the tendency is to use linear regression which can include using transformations to make the relationship linear e g fitting a power law relationship however sometimes a non linear regression is better even though this requires an iterative approach note that the regression technique works well provided the dynamics of the system being modelled do not change changes in the system would require the regression relationship to be re established e g land use and climate change temporary storage of drainage return flows and recirculation of canal deliveries 6 2 2 modelling management practices informing management practices is promoted as one of the main uses of water quality models this invokes the need for water quality models to accurately represent the effects of management practices information on management practices is often lacking or is aggregated e g at the administrative district levels thus modellers need to decide how to spatially and temporally distribute practices in their model such as generating typical schedules for crop schönhart et al 2011 or fertiliser applications in addition knowledge about the sensitivity of the catchment models to parameters relating to management practices is still lacking e g ullrich and volk 2009 a key limitation of most if not all catchment water quality models is the representation of lag times this has implications for modelling the impacts of management practices meals et al 2010 argue that simulation models are not yet able to represent landscape processes realistically enough to support their use for program planning forecasting and evaluation of long term restoration efforts in complex systems the inability of water quality models to effectively capture lags can stem from the processes that are represented how practices can be represented using model parameters the spatial and temporal resolution of the model and the relevant data needed to calibrate the lag processes in situations where there has been a build up of a constituent in a water body e g salt phosphorus heavy metals there will be a lag time in the system response to changes in management practices which address generation of and delivery of the constituent to the water body meals et al 2010 decompose lag time into three components the amount of time taken for practices to produce the required effect it can take time for a treatment measure such as a revegetation project to reach full effectiveness equally the effectiveness of some treatments could decline over time or vary under different weather conditions the capacity of models that take either an export rate or emc dwc approach in modelling constituent generation to represent the change in treatment effectiveness over time is limited the amount of time taken for effects to be delivered to the water body this component relates to the travel time from the point of the intervention e g the riparian buffer to the water body where the response is desired groundwater systems can have a long lag time due to the amount of a constituent stored in an aquifer and the input and output flux rates this can include saline aquifers as well as contaminated aquifers for example due to use of firefighting foams that contain perfluorooctane sulfonate pfos and perfluorooctanoic acid pfoa the amount of time it takes for the water body to respond the lag time will also depend on the change in the load delivered to the water body relative to the store of the constituent in the water body and bed sediments for example poorly flushed water bodies may have a large store of sediment and nutrients in the water column and bed and hence any improvements in water quality and ecology in response to catchment treatments will take longer to discern compared with well flushed receiving bodies over the longer time horizon incorporating lag time associated with the legacy sediment can also be important in some systems legacy sediment broadly applies to anthropogenic sediment produced episodically over a period of decades or centuries james 2013 legacy sediment was deposited to streams and is still being transported understanding the lagged response and prolonged recruitment of legacy sediment and adequately representing these processes in catchment models are still challenging walter et al 2007 another challenge in modelling management practices is the incorporation of adoption and implementation of interventions across a catchment adoption of an intervention can take considerable time to reach peak levels reflecting that people may decide to adopt at different rates but also the time needed to implement programs and similarly the level of compliance on an aspect of water quality practice can vary in time and space not often considered in the conceptualisation of water quality models or the definition of scenarios this contributes to the inability stated by meals et al 2010 of most models to realistically simulate the effectiveness of many management practices many models apply static management practices where the area or effectiveness of a treatment e g sediment trapping efficiency are temporally static this can be problematic if the models are used for evaluating change in management practices over time 6 2 3 technology advancement as has been suggested by ambrose et al 2009 improved computer technology and widespread use of the internet in the early 2000s enabled substantial improvements in water quality models and their applications developments have been centred around the handling of detailed environmental analysis both spatially and temporally improved user interfaces and linkages with gis software enhancing accessibility to environmental data and the construction of robust modelling frameworks linking hydrology and water quality as technology continues to advance at a rapid pace it is apt that we identify some of the main challenges of linking technology advancement to the continued development of water quality models and platforms from the monitoring technology perspective traditional and emerging remote sensing has become a valuable and important data source to improve water quality models new satellites and sensors can provide higher resolution spectral e g to differentiate between algal groups and spatial e g smaller grid cell size data needed to monitor water quality ritchie et al 2003 however there are several challenges associated with remote sensing data including costs especially for new and or high resolution data tools and expertise required to analyse including uncertainty and ground truth validate and interpret the remote sensing data turner et al 2003 for example fisher et al 2018 evaluated the value of information in a higher resolution satellite image 1 m from digital globe vs 30 m from landsat on land use classification and total suspended solids load estimates from swat they concluded that higher resolution data may be preferable to lower resolution data under certain conditions such as 1 a landscape has small features and or fine scale variation in land cover land use 2 a large portion of land cover land use change patches are smaller than the pixel size at lower resolutions and 3 high accuracy is necessary to inform decisions the use of unmanned aerial vehicle uav technology may offer high resolution data for water resource management while being flexible in terms of costs and the location and timing of data acquisition debell et al 2015 pajares 2015 computer technology for water quality modelling has an increasing role to play with the growing demand for model performance testing uncertainty and scenario analysis parallel and distributed processing allow us to better utilise personal computers with multiple processers or to take advantage of the internet and supercomputer technology ambrose et al 2009 buyya et al 2009 humphrey et al 2012 advances in data management and data mining technologies enable management of large amounts of data and interpretation of these data development in information technologies has seen the use of water quality models for real time forecasting bedri et al 2014 and web based applications booth et al 2011 to aid scenario analysis and decision making enhancing modelling frameworks to automatically connect models to external data such as real time online data bases for meteorology flows and point source discharges can improve the utility of the models for such purposes ambrose et al 2009a however more data are not always better and the modelling community must improve its understanding of the value of information for the modelling and or model users and whether this justifies the investment in new technology this necessitates better understanding of user needs with respect to technology and water quality assessments in addition value of information statistical concepts and model based methods can be effectively utilised in the design of monitoring networks both with respect to on ground information such as land cover and practices and in stream measurements the aim would be to use the model to identify monitoring information that reduces critical uncertainties in model predictions at reasonable cost 6 3 model performance 6 3 1 incorporating soft data soft data can provide a means to formally assess the reasonableness and consistency of model structures and outputs arnold et al 2015 defines soft data as information on individual processes that may not be directly measured in the study area may be an average annual estimate and may entail considerable uncertainty seibert and mcdonnell 2002 advocated the use of soft data to augment hard data in the model calibration process stating that it should be actively identified and used where available they identified two ways in which soft data can be used to constrain model calibration 1 to evaluate aspects of the model simulations for which there is no hard data available e g ungauged systems and 2 to assess how reasonable the parameter values are based on field experience qualitative information can also be used in parameter inference mode by superimposing a constraint on the model output arnold et al 2015 posited that the process of developing soft data sets increases the modellers understanding of water quality budgets for the study area thus enhancing their ability to constrain the calibration process this is particularly relevant for catchment and basin scale modelling more so for ungauged systems where formal methods based solely on hard data may not be applicable winsemius et al 2009 the other use of soft data is to inform the selection and conceptualisation of models although not the focus of the seibert and mcdonnell 2002 paper the authors noted that the discussions with the experimentalist informed the conceptualisation of the model they developed discussions with other stakeholders e g catchment managers could provide a similar service a general process from arnold et al 2015 for incorporating soft data into the calibration process is given in fig 4 we identify several key challenges in incorporating soft data into water quality models and analyses these are centred around data collection approaches to map soft data onto numerical models mis trust in soft data and the transferability of soft data the challenge around collection of soft data revolves around what to collect and how to collect and use the data a broad list of examples of soft data were identified by arnold et al 2015 namely regional estimates of baseflow ratios or et average depths of groundwater tables average annual runoff coefficients for various land uses annual rates of denitrification from research plots found in the literature event mean concentrations nutrient sediment export coefficients sediment deposition from reservoir sedimentation studies average crop vegetation lai and county crop yields these data can be absolute values such as nitrogen uptake rates or relative comparisons such as baseflow ratio yen et al 2016 the sources of soft data include refereed literature grey literature and field surveys arnold et al 2015 expert opinion from experimentalists seibert and mcdonnell 2002 or others and model outputs or analyses e g transfer functions winsemius et al 2009 there are substantial transaction costs in the collection of soft data not only in terms of the time and financial resources needed to collect collate and analyse the data but perhaps also investment in building the capacity of modellers to appreciate understand and effectively utilise soft data mapping soft data into numerical models is not a trivial process soft data are inherently patchy and discontinuous in nature and therefore highly uncertain seibert and mcdonnell 2002 and so may not be readily converted into numerical values methods for the identification and transformation of valuable soft data for the task in question are required to reduce the transaction costs in soft data collection and uncertainties associated with the soft data an additional challenge for the more complex models is how to concurrently consider numerous soft data constraints yen et al 2016 has addressed this for swat auto calibration applications by coupling swat check with the integrated parameter estimation and uncertainty analysis tool soft hard data evaluation ipeat sd tool to constrain the bounds of soft data with this tool 59 constraints can be considered although the authors caution that modellers should be aware of which constraints are needed as it is possible that no feasible solution may be identiﬁed if too many constraints are included simultaneously thus the authors advised the purpose of using soft data constraints to be clearly deﬁned before conducting a model calibration study yen et al 2016 related to the aforementioned challenge of using soft data in numerical models is the mismatch in scales of the data and models for example while data may be instantaneous model results may reflect daily or coarser averages as with trust in models trust in the soft data can also be an issue that may hinder their adoption in modelling practices part of building capacity and trust in the use of soft data is challenging traditional practice around the assessment and calibration of models and fostering awareness of the potential benefits this was neatly demonstrated in the seminal paper of seibert and mcdonnell 2002 where the authors demonstrated that the standard metric reff obtained using soft data multicriteria calibration was lower than that obtained without it but this was negated by improved overall performance and much reduced parameter uncertainty soft data ideally should be local to the study catchment or area however it is possible to use soft data for other areas as long as the transferability of the soft data can be justified little thought has been given to the transferability of soft data in the literature to progress the research and practice around the collection and use of soft data in water quality modelling there is a need for more published examples in the literature that demonstrates what has or has not worked under particular conditions and for documentation of tools to support modellers utilising soft data in their work for example a flow chart would help illustrate how to apply soft data locally and in areas with similar characteristics for model based simulation to date most literature considering soft data has focused on hydrological components despite evidence that watershed modelling results can be compromised by failing to consider soft data for both flow and nutrient processes yen et al 2016 however this literature base has started to expand in recent years due to the stated priority of this area of research by the swat research community arnold et al 2015 bieger et al 2017 yen et al 2016 6 3 2 model identifiability and restructuring structural non identifiability koopmans and reiersol 1950 occurs if a model is found to have non unique parameters due to model structure input and outputs even when exact model inputs and output data are used in conjunction with a given objective function and constraints shin et al 2015 identifiability of a water quality model or indeed an environmental model can be considered as the extent to which one can capture its parameter values from observational data and other prior knowledge of the system marsili libelli et al 2014 an ideal water quality model needs to strike a balance between representing processes on the one hand and retaining model identifiability on the other identifiability to a general extent means keeping model complexity under control while it may seem important to represent more complexity in a model increases in parameterisation should be justified by being able to identify the additional parameterisation either from the data available or from other knowledge of information striking the balance between process representation and identifiability may require the restructuring of an existing model this might involve simplifying an overly complex model to make it more identifiable conversely models that focus on mathematical solutions over process representation and are therefore often considered too simplistic by developers of process based models may require modification to make the relationship between the model and the process more intuitive while many modellers focus on better process representations less attention is given to model identifiability which contributes to uncertainty for example schwarz et al 2006 reported that non unique models those for which nearly identical model predictions result from the use of different parameter sets and values may have large uncertainties in the interpretability of the parameters and their characterisation of the effects of specific processes there is growing recognition by the hydrology e g shin et al 2015 and water quality e g schwarz et al 2006 modelling communities that a large fraction of the total variability in the observations can frequently be explained by a relatively small number of model parameters and that increases in the number of parameters beyond these limits are likely to have at best only marginal increases in explanatory value to move forward we need to invest more in identifiability analysis which will help expose inadequacies in the data or suggest improvements in the model structure matott et al 2009 marsili libelli et al 2014 and guillaume et al under review provide an overview of practical techniques that can be used to assess identifiability for environmental models including the use of sensitivity analysis quadratic and higher degree response surface methods dynamic identifiability analysis and pseudo monte carlo methods bayesian and non bayesian these types of analysis may lead to restructuring of the models so that they can become more identifiable or we can accept the model as is but provide support for uncertainty analysis both from the viewpoint of model and inputs for models that are too simple we can investigate alternative hypotheses on different processes or undertake more studies to improve understanding of the processes 6 3 3 uncertainty analysis data model structure and parameters are typically considered when we analyse sources of uncertainties in model outputs however a more comprehensive list is warranted if one wants to truly capture and address sources of uncertainty additional sources that may need to be considered include conceptualisation of the model future forcing conditions initial boundary conditions prior knowledge model family e g empirical physical hybrid model purpose and or objectives verification validation process and uncertainty assessment including qualitative aspects model code and numerical implementation and communication process jakeman et al 2018a the aforementioned authors address this broader issue of uncertainty in the modelling process and model outputs and stress the need to employ good modelling practices see section 6 4 in a report to the queensland government on good modelling practice jakeman et al 2018b the authors made the following recommendations with regard to managing uncertainty in water quality modelling list and characterise sources and try to rank the criticality of uncertainties arising in the whole modelling process through such means as expert elicitation stakeholder engagement sensitivity and more quantitative uncertainty analyses carefully consider appropriate model complexity taking into account uncertainty data support and system behaviours this is likely to include effective simplification of the model with good documentation of the assumptions made and their implications factor in the appropriate costs of holistic uncertainty assessment i e taking into account all sources in project budgeting it will be worth it in the longer term place due emphasis on communicating uncertainty an area of emerging attention that could be advanced through focusing on meeting its challenges in the water sector visualization of indicators of concern are an aspect in such an endeavour their design should pay special attention to possible interpretation biases and ways to control them pay explicit attention to the way model results and uncertainty are communicated in written reports and publications in terms of addressing uncertainty in a water quality model several methods are available jakeman et al 2018a at a simple level qualitative approaches can be used to evaluate the prediction and the limitations involved this can be achieved through quality assurance of the modelling process refsgaard et al 2007 or through qualitative judgements about the information and how it is produced van der sluijs et al 2005 sensitivity assessment can be a useful procedure to explore the determinants of model outputs literature on sensitivity analysis techniques are abundant norton 2015 pianosi et al 2016 sarrazin et al 2016 and readers are directed to that literature for detailed descriptions and discussions on sensitivity analysis these techniques involve many trial runs of the model and are often limited by computational capacity including input output data overheads especially for large catchment models on the other hand most of the water quality models have very simple mathematical equations in that these models often consist of variables and parameters combined through only a few elementary operations such as products sums ratios powers and exponential the sensitivity of the results of these operations to input parameter variation can be analysed algebraically thus bypassing some of the need for computer simulation norton 2008 it also allows one to calculate the local derivatives at any point in the parameter range thereby obviating the problem posed by shin et al 2013 of having sensitivity being related to the parameter range selected there are several methods for estimating and or reducing the uncertainty in model outputs this include first order analysis which focuses on affects from individual inputs ignoring interactions between inputs c f screening in sensitivity analysis the kalman filter kalman 1960 is a method of assimilating new data to update parameter values and estimate their uncertainties as well as the uncertainty in the model output the assimilation of new data leads to a reduction in the uncertainties with the magnitude of the reduction depending on the information contained in the data and the noise present from a different perspective the first order reliability method form is a tool that can be useful in evaluating the impact of parameter uncertainty in terms of reliability vulnerability and resilience maier et al 2001 monte carlo simulation is a general method for analysing parameter uncertainties of which bayesian methods are a pertinent example bayesian inference methods produce a probabilistic distribution posterior density of parameter values that are consistent with observations kaipio and somersalo 2005 generalized likelihood uncertainty estimation glue is a popular bayesian like method for appreciating the uncertainties in a model beven and binley 1992 sensitivity analysis is often used to screen out insensitive parameters before applying uncertainty analysis shen et al 2012 for example used the morris qualitative screening method morris 1991 to select the 20 most sensitive parameters affecting stream flow and sediment yields in a swat model about 4500 km2 then used glue to investigate uncertainty of model outputs and parameters they identified several sensitive but non identifiable parameters given the available data which contributed to model uncertainty due to the non identifiable nature of these parameters calibration of these parameters will not be feasible and more detailed measurement data relevant for these parameters will be required shen et al 2012 surrogate models or model emulation can be attractive when a model is so computationally expensive that adequately understanding its behaviour and quantifying uncertainty is essentially intractable if using sampled runs of the model jakeman et al 2018a some of the most popular surrogate approaches include polynomial chaos expansions sudret 2008 gaussian processes rasmussen 2004 and sparse grids bungartz and griebel 2004 razavi et al 2012 provided a good review of surrogate modelling in water resources including water quality modelling yang et al 2018 use gaussian process emulation of a swat model to characterise the sobol sensitivities of parameters so that insensitive ones can be fixed and the range of sensitive ones can be narrowed thereby producing glue uncertainty estimates more efficiently when the uncertainties in model inputs and parameters are high other simulation methods such as exploratory modelling bankes 1993 and crash stress testing coron et al 2012 can be useful exploratory modelling and analysis involves exploring scenarios about future conditions model structure and parameter values it can be used to search for scenarios that lead to good or poor outcomes or specified objectives such as robustness metrics mcphail et al 2018 crash testing is a related less formal technique that attempts to identify through simulation experiments what parameter sets observation periods and other conditions establish limitations or invalidate the model this can involve examining the performance of the model through time and space e g for models with a node link structure the performance can be evaluated at each node most uncertainty analyses are framed around reporting of model behaviours other topics related to uncertainty analysis in water quality models that are less studied or reported upon include can we discern the signal of changes in management practices taking into account climate variations and model uncertainties how can the understanding of uncertainties be fed back into catchment monitoring practices and model improvement how can we best investigate the cascading of uncertainty this is especially an issue for catchments with many links and or component models how can we effectively interpret and communicate uncertainty considering the intended model uses e g harmel et al 2014 we need more discussion of and investigations into these questions to enhance the usability of catchment water quality modelling for decision making the benefits of uncertainty analysis go beyond its characterisation of model predictions uncertainty analysis can also assist in improving the design of water quality monitoring programs often the collection of water quality data is expensive and only a limited amount of observational data can be obtained but not all data provide the same amount of information about the processes they are helping inform and thus from the point of view of improving modelling uncertainty analysis can help us move toward a more optimal design of monitoring programs such that monitoring can be focused on reducing the critical sources of uncertainty jakeman and jakeman 2016 6 4 overarching challenges 6 4 1 good modelling practices and capacity building as water quality models are increasingly being promoted in assisting catchment management and policy we increasingly demand that the model development evaluation and application process conforms to standards that ensure the usability soundness and defensibility of their outputs for decision making ambrose et al 2009 our expectation with regards to models can be roughly grouped into three quality criteria van voorn et al 2016 credibility the scientific logic of the model and soundness of the knowledge salience the societal and political relevance of the use of the model legitimacy a fair representation of the views values and concerns of involved stakeholders in the model good practices for water quality models have been recommended for model calibration and validation daggupati et al 2015 documentation and reporting saraswat et al 2015 spatial and temporal considerations baffaut et al 2015 parameterisation malone et al 2015 sensitivity analysis yuan et al 2015 and uncertainty considerations guzman et al 2015b most of these recommendations relate to model development and performance testing techniques as covered in principle for environmental models in general by jakeman et al 2006 less attention has been given to other aspects of modelling relating to model use such as scoping problem framing and formulation and communicating of findings though these have been reported and discussed in jakeman et al 2018b some key points are the purpose and objectives of a model should include a clearly articulated set of user data requirements processes to be represented questions functionalities system boundaries and predictive quantities of interest when possible we should employ multiple lines of evidence or multiple methods for making water quality predictions from a communication point of view we should employ user centred design for visualization development early in the modelling process and leverage different visualization tools to engage different audiences e g academics policy makers stakeholders we should educate consumers of model results about the dangers of being provided only a single number upon which to base decisions but also address their needs by providing uncertainty information in a format that fits within their workflows and which is intuitive and readily communicable to non technical audiences developing good practices tailored to catchment water quality modelling is an on going task and requires us to instil good practices by undertaking and sharing cases as a community that involves both modellers and model users this should preferably be achieved through collaboration on an issue that involves the major water modelling domains part of this process is building up the capability and capacity of the developers and users of catchment water quality models in this regard open source software and code can support a continuous development by a user community as occurs with the swat platforms modelling is as much an art as a science and the performance and utility of a model relies on the knowledge and skills of the model developers likewise a better model and more accurate and reliable model results do not translate to better decisions as they also rely on both the ability of the model users to interpret the values and uncertainties associated with model results as well as the trust that model users place on the models and their developers decision making being a socio political process may influence when and what model based knowledge is requested and how this knowledge is used we modellers and users of models or their results need to reflect on the linkage between water quality models and decision making such reflection and ensuing dialogue will further advance the development of water quality models and their roles in supporting decision making 6 4 2 differentiating climate impacts from land use and management the capability to differentiate the impacts of land use change and or practices from climate influences is critical if we are to use models to practicably inform management however as argued by croke and jakeman 2001 from several australian studies the ability to predict the effects on flows and water quality of anything but major changes in climate and land use is limited especially if one is only using precipitation flow water quality time series data to inform the modelling the errors in predictions from climate tend to swamp modest changes in land cover and or land use consider for example a model that may give a good fit to observations on constituent concentrations at water quality monitoring sites the effects of management on improving water quality will not be represented adequately during scenario analysis if this good fit is due to an overestimation say of climate impacts on water quality being compensated by an underestimation of the influence of management practices hence developing and demonstrating a model s ability to distinguish climate impacts from the impacts of land use and management is not trivial renner et al 2014 firstly changes in land use and management practices often intertwine with climate impacts for example zhang et al 2001 demonstrated that the relationship between annual rainfall and annual evapotranspiration et varies greatly depending on land uses therefore as land use shifts over time and space the relationship between et and rainfall may change which has implications for the modelling of flows and subsequently constituents loads a second issue is that the calibration period with its specific climate pattern selected for a water quantity and or quality model may have a large impact on the parameter values daggupati et al 2015 and subsequently model predictions therefore we need as a first step to be more rigorous in understanding and representing how climate in the calibration period affects the model parameters and response in theory the water quality modelling community could advance its ability to discriminate between climate and land use or management signals at the catchment scale continuing to collect and analyse small scale monitoring data would improve process understanding whilst the development of robust methods to upscale learning from these small scale experiments to the catchment scale may advance catchment scale predictions and a model s ability to differentiate climates impacts from land use and management on the other hand it is important to recognise that this issue is somewhat intractable because modellers rarely if ever have enough data at catchment scales to test upscaling of model parameterisations developed from the small scale experiments perhaps the most immediate gain is to use models to indicate the value of data including those from on ground and in stream monitoring remote sensing experts and other sources this exercise would investigate what type when where and at what frequency of data provides the most leverage for increasing predictive accuracy and distinguishing between climate and land effects on water quantity and quality 7 conclusions despite a significant increase in published papers in catchment scale water quality modelling and an inherent wealth of new ideas methods and knowledge there has been no comprehensive review on the state of modelling advances since the early 2000s and even less so in the practices of modelling in supporting catchment management in this paper we have concentrated our review of literature mainly on the last 15 years so as to synthesise recent developments in water quality modelling the review focuses mainly on catchment scale models of freshwater non urban systems with particular emphasis on sediments and nutrients though many of the considerations extend to other constituents we explored 10 key attributes in selected existing water quality models grouped into three categories model use model development and model performance from this we deliberated on 11 key challenges and or emerging topics in catchment water quality modelling large scale applications model integration model usability and communication preliminary data analysis modelling management practices technology advancement incorporating soft data model identifiability and restructuring uncertainty analysis good modelling practices and capacity building and lastly differentiating climate impacts from other influences some of these challenges can be addressed relatively easily such as preliminary data analysis and ongoing attention to good modelling practices other challenges may be less tractable but are essential if we are to use catchment scale water quality models to inform management such as modelling management practices and differentiating the effects of climate impacts on water quality from those associated with land use and management developing and applying exploratory modelling and risk based approaches will help generate valuable information to support management decisions under uncertainties most water quality models possess complex parameterisations improving model identifiability by using analysis of it to devise a more balanced representation of the processes especially in the case of large scale applications using complex models can reduce computational demands and facilitate uncertainty analysis soft data can be employed to constrain parameters while model emulation can identify insensitive ones and facilitate uncertainty analysis especially when model runtimes are long advancing these challenges will help modellers to better support those responsible for management of water quality at the catchment level acknowledgement this work is funded by the australian and queensland governments queensland department of natural resources mines and energy and department of environment and science through the reef plan and the queensland water modelling network as well as the western australian government department of water and environmental regulation and the new south wales department of industry lands and water division we would like to thank nigel quinn martin volk one anonymous reviewer and the manuscript editor for their constructive comments on the manuscript appendix a supplementary data the following are the supplementary data to this article supplementary supplementary appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 12 008 
26246,catchment scale water quality models have become important tools for water quality management planning and reporting worldwide in this review we synthesise recent developments in water quality modelling focusing on catchment scale models of freshwater non urban systems and their ability to support catchment management we explore 10 key attributes in selected existing water quality models these attributes can be characterised as model use model purposes representation of constituents scenario analysis and documentation model development process representation spatial heterogeneities temporal dynamics and data requirements and model performance calibration validation and uncertainty tools we deliberate on 11 key challenges and or emerging topics in catchment water quality modelling large scale applications model integration model usability and communication preliminary data analysis modelling management practices technology advancement incorporating soft data model identifiability uncertainty analysis good modelling practices and capacity building and differentiating the effects of climate impacts from those associated with land use and management practices keywords water quality catchment models catchment management sediments nutrients uncertainty 1 introduction water resource management often involves the monitoring and modelling of water quality and quantity many erosion and water quality models have been developed to determine the source transformation transport and delivery of constituents in catchments and into waterbodies usually in terms of concentrations and or loads in practice these models can be used to predict water quality in areas where monitoring is not feasible or to predict water quality conditions resulting from different management strategies and under specified climate regimes research into the development of erosion and water quality models has mostly focussed on better representation of the biophysical processes e g formulae to represent the generation filtration transformation and transport processes of different constituents spatial and temporal dynamics of the constituents this focus has been accompanied by improved software platforms and on tools for measuring and attempting to increase the predictive skill of the models e g calibration sensitivity and uncertainty assessment tools here we use the term water quality models to include models directly related to in stream water quality as well as models indirectly related to in stream water quality such as erosion models we use the term catchment models to encompass models that predict sediment and or constituents in streams or rivers at a catchment scale which generally include generation land to water delivery and in stream processes we reserve the term integrated models for models that incorporate systems or processes beyond water quality such as a broader social economic ecological system in which the objectives of the water quality modelling sub problem may be embedded in this paper we mostly focus on catchment models other models such as erosion or in stream models are generally invoked due to their usages in catchment models or modelling platforms several reviews on water quality modelling related topics have been published in the scientific literature and most have necessarily had specific scope some review articles focus on particular geographic locations constituents or types of systems e g coastal or urban systems whilst others concentrate on a specific aspect of water quality modelling table 1 many of the commonly used catchment models have been previously compared in the published literature aksoy and kavvas 2005 borah and bera 2003 deliman et al 1999 despite the aforementioned literature there has been relatively little focus on catchment scale river systems modelling and in particular how it can support catchment management there has been no comprehensive update on the water quality modelling advances in the last decade the purpose of this review is to compare key attributes of commonly used catchment scale water quality models characterised by their use development and performance and to critique their ability to support catchment management from this comparison we identify and deliberate upon unresolved or emerging topics and challenges in the development and use of catchment scale water quality models we focus on models of moderate to large catchment scale freshwater non urban systems although our focus is on sediments and nutrients the range of constituents included in the commonly used models are briefly summarised and most of our discussions apply to the modelling of other constituents the review does not consider the modelling of urban catchments estuarine or ocean systems models developed and applied solely to plot or field scales nor does it discuss in detail the hydrologic processes of soil and water readers are referred to table 1 for literature on these topics we start with a brief overview of water quality models in the literature in section 2 in sections 3 to 5 we explore and discuss key attributes of existing water quality models citing relevant models as appropriate these attributes are model use e g model purposes constituents modelled scenario analysis and management practices and software availability and documentation model development e g process representation spatial heterogeneities and scales temporal dynamics and data requirements and model performance e g calibration validation and uncertainty tools drawing on the preceding three sections we discuss some of the unresolved or emerging topics and challenges in the development and use of water quality models section 6 2 overview of water quality models in the literature to understand the general publication trends in water quality modelling we undertook a database search from scopus it being one of the largest online databases in the world covering over 22 000 peer reviewed journals it also provides simple tools that facilitate basic visualization and statistical analysis of papers in order to obtain the most relevant publications the option with titles abstracts and keywords was selected with the keywords listed in table 2 the search was limited to the subject areas of environmental science earth and planetary sciences agricultural and biological sciences chemistry computer science mathematics engineering social science and decision science the search found 50 530 papers between 1935 and 2018 note we may not have the complete issues for 2018 in some journals as the search was undertaken on 4 dec 2018 fig 1 the rate of publications per year increased from 1970 particularly so over the last 15 years in fact 76 38 542 of the papers were published between 2003 and 2018 most of these papers were predominately published from authors with addresses in the usa followed by china uk germany canada and australia we also explored the publication records of 42 water quality models or modelling platforms in the environmental field these models and or platforms were selected because either they were reviewed in other literature e g merritt et al 2003 tsakiris and alexakis 2012 or they have been used by government agencies to support river system planning e g the integrated water quantity and quality simulation model iqqm in australia simons et al 1996 note that applications of some river system planning tools have historically focused on water quantity modelling but the intent of many agencies was that water quality capabilities would be expanded over time invoking the scopus database again the model names were added to the search terms in table 2 individually and the resulting numbers of papers between 2003 and 2018 are illustrated in fig 2 a total of 3282 papers were found with the soil and water assessment tool swat gassman et al 2014 neitsch et al 2011 overwhelmingly dominating the publication records accounting for 44 of the papers other highly published catchment models include hydrologic simulation program fortran hspf bicknell et al 2001 integrated catchment model inca whitehead et al 1998 and spatially referenced regressions on watershed attributes sparrow schwarz et al 2006 erosion models such as universal soil loss equation usle and variations renard et al 1991 and water erosion prediction project wepp flanagan et al 2007 are also highly cited the qual series brown and barnwell 1987 are the most cited in stream models together the above models account for a further 29 of the papers we identified note that this finding only applies to literature sources included in the scopus database e g journal articles conference papers book chapters grey literature such as government reports are not included in the database full names model types and key references of these models are provided in the supplementary table 3 current models and software model use the main catchment scale water quality models and platforms considered in this review include swat hspf inca and ewater source carr and podger 2012 although other models are also mentioned when comparison is felt warranted hspf and swat have been widely applied in the united states and worldwide inca was developed as part of european union funded projects and has been applied to a wide range of key european ecosystems the ewater source platform is being adopted in government agencies as australia s national framework for catchment models and is being used in international partnership projects across asia 3 1 model purposes broadly two types of model purpose are identified scientific exploration e g improving understanding of the sources fate of constituents in a water body through modelling and decision support e g assessing climate and or management impacts more specifically water quality models have been used for e g schwarz et al 2006 erosion water quality description or risk assessment describing past or present water quality conditions for an area on the basis of monitoring data extrapolation and or modelling can help extend point scale monitoring information to understand water quality status for a wider region containing the monitoring sites contaminant source analysis can help directly or through surrogate indicators in identifying and quantifying the sources of constituents and their strengths using spatially distributed or semi distributed models water quality simulation or scenario analysis using a calibrated model to predict constituent loads concentrations on the basis of a set of altered inputs such as climate hydrology and land use management scenarios examining the importance of explanatory factors and processes focusing on the calibration process and its results directly and or undertaking dedicated sensitivity and uncertainty analyses can help explore the predictive value of a set of potential explanatory variables and may also compare alternative mathematical forms design of monitoring networks water quality modelling and subsequent uncertainty analysis for informing the design of monitoring networks so that the utility of monitoring data can be optimised to meet objectives of monitoring modelling development and planning and balancing the need for obtaining a representative picture of regional water quality conditions and more detailed information on specific locations or problems early warning and forecasting providing time sensitive information to support decision making for drinking water supply environmental protection and emergency response e g maier and dandy 2000 water quality models vary depending on process representations and complexity the selection of appropriate models and the process and scale detail in them depends on the modelling purpose as well as the nature of the environmental system constituents of concern and the prior knowledge and data available for example models of different scales and complexity are needed to address management questions that focus on different aspects of spatial and temporal dynamics such as what are the annual average constituent loads at catchment outlets under different climate management scenarios what are the spatial and temporal distributions of target constituents under different climate management scenarios what are the relative contributions of various sources of constituents over time what are the forecasts of water quality to meet catchment water quality objectives or standards e g total maximum daily loads in the u s water quality reduction targets for the great barrier reef what are the likely pollutant attenuation trajectories and times to recovery under various management scenarios what are the effects of transient or extreme events how to handle model uncertainty using a risk based approach how much information is contained in the available data and how does this define the appropriate model complexity overly complex models may introduce uncertainty add unnecessary data collection and computational burden and shift focus away from problem solutions to endless analysis overly simple models may miss or inadequately represent key processes or may not address relevant management questions at appropriate scales at either extreme models may not be defensible in adversarial review 3 2 representation of constituents the representation of constituents in the models is shaped by the purpose and focus of the models noting that both may have changed throughout the history of development or use of particular models the purpose and focus define what constituents are modelled and how they are modelled and are tied in with choices around process representation as discussed in section 4 1 the water quality constituents most consistently represented in the commonly used catchment scale models are sediments nitrogen n and phosphorus p species table 3 these constituents are the focus of many catchment water quality modelling applications reflecting both the importance of these macronutrients to agricultural production and the potential ecological impacts of their exports from catchments into rivers and receiving waterbodies the forms of n and p considered are fairly standard across the models listed in table 3 nitrate nitrite ammonia and total n for nitrogen and dissolved and or particulate forms of organic or inorganic p whilst some models focus at least initially on a limited set of constituents others have been developed with a view to providing broader support to water quality planning by allowing assessment of a more comprehensive suite of constituents for example qual2e an instream water quality model developed for application to well mixed dendritic streams is capable of modelling the concentrations of 15 constituents for each defined reach conservative minerals coliform bacteria and non conservative constituents however it does not model generation and export of constituents from land and thus is sometimes linked to the swat catchment model hspf and swat draw upon decades of water quality modelling by incorporating existing models e g the qual series as component modules with modifications and development of additional modules as required with respect to modelling the fate and transport of pathogens many models use faecal coliforms as an indicator of pathogen concentrations e g qual2e but the use of indicator organisms like coliforms has been questioned with pachepsky et al 2016 concluding that coliform indicators are insufficient for providing information about the degree of contamination from particular pathogens there are however models that consider particular pathogens directly rather than through indicators e g ferguson et al 2007 giving a more complete picture of the potential sources of those pathogens and management options that can be used for reducing contamination risks most of the models produce spatially varied e g by subcatchments or hydrogeomorphic units daily time series outputs of constituent concentrations and or loads these outputs may be reported by process or land use for example inca n could output daily or annual land use specific n fluxes for all transformations processes at the land phase as well as time series of flow and n constituents discharged into a receiving water or stream the ewater source and sparrow platforms employ a similar approach in that they do not contain explicit models for any constituents instead they have inbuilt functions that users must parameterise in order to represent a particular constituent in ewater source simple linear and power functions based on flows are available in sparrow non linear regression equations based on landscape and channel characteristics are provided it is also possible to write customised functions or plug ins in ewater source to model specific constituents for example the dynamic sednet model has been developed as a plug in to ewater source for simulating suspended sediment loads in the great barrier reef catchments ellis and searle 2013 similarly the mike eco lab module in mike she graham and butts 2005 can be employed as a generic equation solver for simulating in stream processes of constituents and requires users to parameterise the generic functions to predict the transport of a specified constituent in soil and water all constituent outputs whether as loads or concentrations have certain spatial and temporal properties details on spatial and temporal scales of the models are described in sections 4 2 and 4 3 spatially and or temporally varied constituent outputs can be aggregated in various ways to produce summarised results for comparison norton et al 2003 these include instantaneous values of variables e g sediment concentrations or loads at catchment outlet integrals of variables e g annual sediment loads variation of variables e g standard deviation of sediment loads over space or time distribution of variables e g medium or 90th percentile of sediment loads over space or time proportion of sediment above a certain threshold location of hotspots e g the subcatchment contributing the highest loading of a particular constituent less frequently used aggregations may include frequency of events e g frequency of significant sediment output events size of events e g peak duration of significant sediment output events interval between events e g interval between significant sediment output events where short interval may indicate little ecosystem recovery time cumulative frequency distributions e g flow and dissolved constituents 3 3 scenario analysis and management practices scenario analysis is one of the main uses of catchment water quality models scenarios are typically constructed to explore the potential changes in constituent outputs under alternate drivers like climate conditions and or management practices to achieve this it is important to explicitly incorporate intended drivers and management practices in the model at the appropriate scales so that their effects can be modelled at regional scales climate land use change and flow management are important drivers locally four groups of management practices can be identified in water quality models point source management input managerial practices cultivation practices and structural practices hashemi et al 2016 thus point source management can include local nutrient retention such as wastewater treatment and installation of filter systems input managerial practices focus on control of water and constituent inputs to agricultural land and the reduction of constituent surplus and resultant transport to surface and groundwater systems examples include fertiliser applications livestock stocking rates irrigation management for water inputs cultivation practices and erosion control relate to reduction of constituent outputs to waterbodies by land management such as enhancing retention or promoting transformation to non reactive forms tillage methods contour strip cropping catch or cover crops and harvest operations are examples of cultivation practices gully and streambank stabilisation are examples of erosion control structural practices refer to regulation of water and constituent flows in the landscape such as buffer strips and wetlands most catchment water quality models have the capacity to model scenarios such as climate inputs change land use changes flow scenarios and point source control table 4 shows the capacity for four commonly used daily scale catchment models the models that have detailed representation of nitrogen and phosphorus cycles such as hspf and swat generally have a greater capacity to specify detailed management practices standard for these two models in the literature is the specification of the input managerial and cultivation practices particularly fertiliser and manure management application and tillage practices inca allows further discretisation of fertiliser scenarios by allowing the definition of fertiliser rates based on crop type similarly hspf allows the definition of monthly application rates to represent starter side dress and other fertiliser applications for each specific crop whilst assuming that fertiliser is applied in a manner that will not harm the crop swat has also had much development effort to provide the capacity to represent structural conservation practices there is some capability for modelling management impacts in ewater source such as riparian buffer and point source management note that the level of management representation is related to the spatial heterogeneity and scale of water quality models the large scale catchment model lascam for example operates at the subcatchment scale and thus is unable to distinguish between planting in the recharge areas of each subcatchment versus planting in the discharge zones viney and sivapalan 2001 even the more detailed and spatially disaggregated models require simplifications or assumptions to simulate particular interventions and the approach taken to spatially distribute the model will affect the modelling of management practices for example arabi et al 2008 highlighted that the way in which catchments are disaggregated prior to parameterising distributed models may affect the evaluation of management practices and interventions a previous study arabi et al 2006 examined how the effectiveness of practices simulated using swat changed with the number and locations of subcatchments defined in the model finding that sediment and nutrient outputs of the model were highly sensitive to subcatchment size compared to agricultural management and streambank management the literature on using water quality models to explore impacts on gully density exports and or stabilisation is relatively limited freebairn et al 2015 outlined how dsednet could be used to simulate the impact of scenarios of changes in rainfall and runoff and or land use change on the activity or density of gullies and the effectiveness of management interventions to stabilise gullies bastola et al 2018 incorporated dominant gullying processes into the child landscape evolution model and once calibrated for a small 4 km2 watershed used it to assess the effectiveness of gully backfilling and revegetation otherwise scenario modelling of ephemeral gullies has received some attention using annagnps in very small catchments e g taguas et al 2012 much of the water quality literature takes a standard approach to scenario analysis comparing scenarios of some reference e g current natural condition against a suite of scenarios representing combinations of alternate climate land use or land management practices for example water quality models have been used to predict the relative effectiveness of land use change versus changes in agricultural practices farkas et al 2013 and where and when best management practice bmp for different industries are most effective at reducing instream nutrients concentrations in order to meet a total maximum daily load tmdl usually determined at a compliance monitoring location santhi et al 2001 vigiak et al 2016 used scenario modelling to understand the impact of current riparian lands in reducing sediment fluxes in freshwater systems the authors calibrated and validated the swat model under the current conditions and then formulated three scenarios one that removed the riparian filtering service one that set the streambank vegetation cover to its minimum value and lastly one with both of the previous changes implemented analysis of trade offs among different objectives has also been applied examples include lautenbach et al 2013 and bostian et al 2015 where the non dominated sorting algorithm nsga ii was employed to analyse trade offs among crop production water quality and quantity exploratory approaches to scenario analysis that address uncertainty such as scenario discovery bryant and lempert 2010 robust decision making lempert and collins 2007 or dynamic adaptive policy pathways haasnoot et al 2013 have not been widely applied in the field of catchment water quality modelling 3 4 software availability and documentation unsurprisingly open source or public domain models with an extensive development history such as swat and hspf alone or within the basins package set the standard in terms of availability and comprehensiveness of supporting tools and documentation as well as access to the source code and executables table 5 within the united states swat alone has established a wide user base including the nrcss epa texas river authorities noaa universities and environmental consulting firms there is also a substantial worldwide swat community there has been much investment in educational resources and updated swat literature databases and the development of supporting tools to aid the set up evaluation and assessment in swat these include gis interfacing tools for qgis and arcgis a parameter estimation tool a water ecosystem modelling tool an output viewer tool an input check tool a climate model data tool and a calibration uncertainty and sensitivity tool see http swat tamu edu for further detail accessed 16 01 2018 in australia a similar philosophy of access and provision of support was generally evident with e2 and watercast predecessors of ewater source albeit with an understandably smaller resourcing base than the us agency supported models e g hspf sparrow swat however since 2007 ewater source has become an industry standard for water quantity quality modelling within australia it is a semi commercial product with restrictions for the free public version other approaches to software distribution include the provision of access to closed source executables e g inca https www niva no en projectweb inca accessed 15 01 2018 with or without costs mike she is a commercial product and while software and associated user manuals can be downloaded from the dhi website a licence is required for access to fully featured software 4 current models and software model development 4 1 process representation appropriately representing important processes in a model is a fundamental aspect of water quality modelling especially at the catchment scale where complexity of the system and lack of observations bring extra challenges arnold et al 2015 presented case studies in which a model showed excellent statistical agreement with measured stream gauge data but misrepresented processes water balance nutrient balance sediment source sinks within a catchment this resulted in errors when exploring the effects of management scenarios for example underestimation of upstream sources may be negated by overestimation of downstream sources a good fit at a downstream monitoring site may still be achieved and the implication is that the effects of management occurring in the upstream source areas will be underestimated in general there are three main processes to be considered when modelling the fate of constituents in a catchment i generation ii filtration delivery from sources to channels and iii in stream transport which may include deposition remobilisation and transformation most of the commonly used models attempt to represent one or more of these main processes with different conceptualisations a comparison of process representation in four established commonly used catchment models is listed in table 6 in terms of the hydrological process all models account for both surface water and shallow groundwater dynamics but deep percolation is assumed lost from the system recent development in catchment models has seen increasing research into the integration of catchment models e g swat with more sophisticated groundwater models e g modflow to better represent feedback fluxes between surface water and groundwater guzman et al 2015a wei et al in press in terms of the constituent generation filtration and delivery and in stream processes some of the most common conceptualisations are discussed below 4 1 1 constituent generation conceptualisations of the constituent generation process range from using static export rates e g mass ha yr from a land use to capturing event based processes through event mean concentration emc and dry weather concentration dwc to representing constituent generation processes in more detail this applies to both erosion modelling and simulating nutrient processes in the land phase the event mean concentration dry weather concentration emc dwc model was originally used for urban stormwater systems but has been applied in catchment water quality modelling when observational data are insufficient to inform the development of more detailed generation models the model considers two constant concentrations of constituents i e emc and dwc combined with flows emc estimates sediment or nutrient loads from events and dwc estimates loads from base flow different emc and dwc values can be generated to represent different land uses approaches to calculate emc values vary and have been reviewed by bartley et al 2012 the authors reported that emc values can be highly uncertain due to approaches to calculating emc water quality sampling methods definitions of events and measurements of concentrations from different catchment sizes and or land use compositions the emc dwc model by using constant emc and dwc values over time has been criticised for not being able to capture changes in management practices other than land use change for example the effects of changes in fertiliser application on nitrogen emc and dwc in addition a constant emc value e g derived from mean event concentration measurements may not capture the dynamics of relationships between concentrations and flow to address these limitations the rating curve method has been used to capture the non linear relationship between concentrations and flows horowitz 2003 usle based approaches including rusle renard et al 1997 and musle williams and berndt 1977 are widely used in simulating catchment scale sediment generation lascam uses usle for hillslope erosion swat invokes musle which focuses on storm based sediment generation using runoff as the driver of sediment generation combined with sediment lag to represent sediment delivery to channel sednet wilkinson et al 2009 uses rusle which focuses on rainfall erosivity as the main driver coupled with sediment delivery ratio sdr to account for sediment delivery to channel erskine et al 2002 reported that musle performed marginally better than rusle for estimating sediment yield in small basins near sydney sadeghi et al 2014 reviewed the use of musle worldwide and reported that it provides good predictions if applied in similar conditions on which it was developed that is for storm based sediment yields at subcatchment scales 15 1500 ha slopes from 0 9 to 5 9 and slope lengths of 79 174 m in forest and grassland areas with appropriate calibration applications of musle under other conditions could produce good results sadeghi et al 2014 the inca sed model is driven by actual precipitation hydrologically effective rainfall her the portion of the actual precipitation that generates an in stream runoff response or a mixture of the two lazar et al 2010 the runoff oriented conceptualisation is helpful when accounting for 1 protection of soil by snow cover or high erosion rates caused by snow melt 2 development of crust caused by drying of clay or organic soils which protects the soil against splash erosion and flow erosion and 3 filling of soil water storage after a dry period where erosion only occurs when soil is saturated in contrast hspf is a process based model that simulates detailed processes of erosion such as sediment detachment re attachment of detached sediment wash off of detached sediment and scouring of the soil surface bicknell et al 2001 compared to sheet and rill erosion the gully erosion process is more complex and difficult to predict and gully erosion modelling is still under represented in the literature valentin et al 2005 three types of gullies have been identified ephemeral gullies small channels eroded by concentrated overland flow that can be easily filled by normal tillage only to reform again in the same location after additional runoff events permanent or classical gullies channels too deep to ameliorate easily with ordinary farm tillage equipment and which typically range from 0 5 to 30 m in depth and bank or edge of field gullies develop wherever concentrated runoff crosses an earth bank poesen et al 2010 only a few water quality models include the prediction of soil loss from ephemeral gully erosion examples include field scale models such as creams knisel 1980 and wepp hspf scouring process and ephemeral gully erosion model egem woodward 1999 these models estimate concentrated flow detachment based on flow shear stress exerted on the bed material the critical shear stress and the transport capacity of the flow and the sediment load poesen et al 2003 egem was improved and later implemented in the annagnps model gordon et al 2007 whilst creams together with gleams and epic were the base for swat krysanova and arnold 2008 sednet simulates long term average sediment yields from permanent or bank gullies wilkinson et al 2009 the model relies on inputs from gully mapping to provide information on gully length and cross sectional area and assumptions on the age of the gullies poesen et al 2003 noted that compared to prediction of soil loss from gully erosion the prediction of gully location including the initiation and end of gully erosion was less studied this remains the case and there are few models that are capable of predicting gully initiation and progression similar to sediment modelling the conceptualisation of nutrient generation in the land phase may range from simpler regression methods to representing nutrient cycling for example ewater source implements simple approaches such as emc dwc and rating curve methods as mentioned earlier in contrast swat and hspf simulate detailed nitrogen and phosphorus cycles bicknell et al 2001 neitsch et al 2011 for the nitrogen cycle swat monitors five pools of nitrogen in soil two mineral nitrogen pools including ammonium and nitrate and three organic nitrogen pools including active humus stable humus and fresh plant residue nitrogen is added to the soil through fertiliser manure or residue application and is removed from soil through plant uptake leaching volatilisation denitrification and erosion within the soil processes modelled include mineralisation decomposition immobilisation and nitrification six pools of phosphorus are modelled in swat including three organic phosphorus active stable and fresh and three inorganic phosphorus stable active and solution pools processes modelled for phosphorus cycling include application of fertiliser manure or residue plant uptake erosion leaching mineralisation decomposition immobilisation and sorption hspf simulates similar processes for nitrogen and phosphorus cycles but different algorithms are employed for some processes inca n also considers transformations of mineral nitrogen and includes processes such as mineralisation immobilisation and nitrification however it assumes an unlimited organic nitrogen pool and thus transformations within the organic n pool are not simulated wade et al 2002 4 1 2 filtration and delivery in terms of the landscape filtration delivery process from the landscape to stream networks model conceptualisations range from using i a constant e g delivery ratio ii a function of attribute s such as vegetation or terrain to iii more detailed pathways of movement and transformation in this paper we adopt the term proposed by hoos and mcmahon 2009 i e landscape delivery ratio to refer to the capacity of a landscape to deliver constituent loads to a channel which is different from the in stream delivery ratio many factors may influence the landscape delivery ratio such as rainfall overland flows topography vegetation and soil properties and their complex interactions beven et al 2005 fully accounting for these processes and pathways can be difficult at medium to large catchment scales for large scale catchment models where data are limited a fixed landscape delivery ratio is often used for example in the ewater source platform at each time step a fixed percentage of the constituent mass load associated with the quick flow and a fixed percentage of the constituent associated with the baseflow are removed ewater 2017 other modelling frameworks such as inca sed lazar et al 2010 and rusle2 foster 2013 adopt a concept of sediment transport capacity sediment transport capacity is an estimate of the maximum amount of material that is transported from the landscape to the stream not in stream transport capacity in inca sed this capacity is calculated based on subcatchment area and reach length lazar et al 2010 when the pool of readily transportable sediment in the landscape sstore and mass mobilised through splash erosion ssp exceeds the sediment carrying capacity of overland flow stc the mass of sediment transported to the stream qsed equals stc otherwise qsed equals the sum of ssp and flow erosion sfl similarly rusle2 uses the transport capacity concept to compute deposition of sediment before reaching the stream foster 2013 in contrast to hillslope transport capacity swat uses surface runoff lag to simulate the delivery of sediment nutrients from landscape to channel neitsch et al 2011 this applies to situations when the time of concentration i e the amount of time for water to flow from the remotest point in the subcatchment to the subcatchment outlet is greater than 1 day e g in large subcatchments and thus only a proportion of the surface runoff and subsequent sediment and nutrients reaches the channel on the day it is generated this proportion is calculated based on time of concentration t conc and a surface runoff lag coefficient surlag i e 1 exp surlag t conc the rest of the sediment or nutrients is stored and added to the supply for the next day in terms of nutrients nitrate in swat is delivered from landscape to channel through surface runoff lateral flow or percolation neitsch et al 2011 delivery of organic nitrogen and phosphorus through surface runoff is accounted for through use of the nutrient enrichment ratio a process where as sediment is transported from landscape to channel sediment becomes enriched with clay particles and consequently greater concentrations of organic nutrients attached primarily to clay particles are found for those delivered to channel than those on the landscape sparrow uses a land to water delivery factor to account for the delivery of constituent loads generated from a subcatchment to channel schwarz et al 2006 it is expressed as an exponential functional form determined by a vector of delivery variables and associated coefficients the delivery variables which are user specified identify factors affecting delivery of constituent loads to channels examples include soil permeability land surface slope and wetland area the process of riparian filtration is sometimes included in catchment model platforms such as ewater source and swat to account for effectiveness of riparian management in reducing constituent loading to rivers and streams in ewater source ewater 2017 the load based sediment or nutrient delivery ratio method uses simple rule based relationships between sediment nutrient delivery ratio their loading rate and loading rate thresholds the amount of trapping that occurs in the riparian zone is given by a sediment delivery ratio that increases linearly from 0 to 1 depending on loading per unit vegetation filter length sediment delivery ratio remains zero when sediment loading rate is below a sediment loading rate threshold slrt and remains 1 when the loading rate is above a sediment loading rate at sill slrs threshold ewater source also includes a riparian particulate model rpm newham et al 2005 for riparian buffers which accounts for trapping of fine particulate through infiltration and adhesion at a daily scale using as many as 28 parameters that can vary spatially it empirically simulates the loss of particulate constituents using characteristics of the buffer soil vegetation and flow in comparison swat uses an empirical model derived from simulations from the vegetative filter strip model vfsmod munoz carpena et al 1999 to calculate sediment reduction sediment reduction is linearly and negatively related to sediment loading and runoff volume per unit vegetation filter strip area and positively related to saturated hydraulic conductivity of the soil neitsch et al 2011 the latter two variables explain runoff reduction reduction rates of total nitrogen and phosphorus are related to sediment reduction whereas reduction rates of nitrate and soluble phosphorus are related to runoff reduction 4 1 3 in stream processes in stream transport processes for constituent loading can be represented either through empirical routing functions or more detailed algorithms that attempt to represent complex interactions of constituents for in stream transport of constituents a closed system constituent load balance approach is often used this assumption may be violated for some situations for example when 1 sources of atmospheric dust and chemicals from other catchments and or in rainfall or groundwater flow become important 2 a constituent is non conservative in stream or 3 there is activation of pathogens although these sources and processes may be considered in some landscape component of the catchment models e g nitrogen cycle in swat they are generally not considered in the in stream transport component sediment transport capacity is used in swat and dynamic sednet an ewater source plugin ellis 2017 to capture deposition of fine sediment in channels although the algorithms to calculate the transport capacity are different for these two models swat has four stream power algorithms to estimate the transport capacity of a channel based on non linear expressions that involve peak velocity the default stream power model is the bagnold stream power equation bagnold 1977 in dynamic sednet remobilisation of fine sediment is also simulated in addition to deposition this model requires the daily calculation of sediment transport capacities for deposition and remobilisation of fine sediment inca sed applies stokes law to compute deposition of sediment and the mass of suspended sediment deposition is calculated as the product of settling velocity and sediment mass in suspension the latter is also calculated from effective entrainment based on the bagnold stream power equation bagnold 1977 in terms of the modelling of bank erosion rates different conceptualisations have been adopted in catchment models at the simpler end of the spectrum inca sed estimates bank erosion rates based on a simple power function of discharge lazar et al 2010 in dynamic sednet the rate of bank erosion is simulated based on stream power which is linearly related to bankfull discharge and river bed slope and an erodability factor which represents the combined effects of riparian vegetation and bank erodability wilkinson et al 2009 in contrast the potential rate of bank erosion in swat is predicted based on a threshold response through the excess shear stress equation which assumes bank erosion occurs when bank effective shear stress is greater than the bank critical shear stress neitsch et al 2011 the effect of bank vegetation is accounted for through the calculation of critical shear stress there are several models that specifically simulate in stream water quality some of which have been incorporated into catchment models to represent in stream processes examples include the qual series with the latest versions being qual2e brown and barnwell 1987 and qual2k which is implemented in excel chapra and pelletier 2003 qual2e was modified and implemented in iqqm as its water quality module simons et al 1996 qual2e has also been loosely coupled with swat with swat producing catchment inputs of water and constituents and qual2e simulating instream processes migliaccio et al 2007 qual2k includes the modelling of in stream carbon nutrient cycles dissolved oxygen and anoxia simulation sediment water interactions algae light extinction ph and pathogens wasp is a general dynamic mass balance framework for modelling fate and transport of contaminants in surface waters wool et al 2006 which has been included in the basins framework it can be applied in one two or three dimensions the latest version wasp7 includes the simulation of heat eutrophication toxicants and mercury 4 2 spatial heterogeneities and scales spatial discretisation is a key consideration in conceptualising catchment scale water quality models it can assist in identifying the sources of constituents that contribute the higher loading rates and therefore in targeting investment in catchment monitoring and management sparrow is reported to provide consistent results that allows users to identify major sources and environmental factors affecting nutrient fate and transport at the regional to subregional scale preston et al 2011 many other catchment scale models use either a fully distributed approach or use hydrological connectivity to break the catchment into subcatchments from this structure a node link network is formed where nodes represent subcatchments and links represent river reaches within a subcatchment four major types of spatial discretisation have been characterised by arnold et al 2010 as 1 no discretisation or lumped spatial approaches where the subcatchments are represented by a fixed set of properties such as dominant soil land use and slope with this type management changes in a specific subarea for example cannot be represented to have a differential effect than say changes elsewhere in an equivalently sized area fig 3 lascam sivapalan et al 2002 is such an example 2 semi distributed spatial approaches based on properties of land use soil type and topography like slope examples of such models are the original formulation of swat which uses hydrologic response units hrus moriasi et al 2012 the functional units fus of ewater source wilkinson et al 2009 and the landscape units in inca sed lazar et al 2010 as interactions between spatial units are not considered the effect of catchment management practices at upslope spatial units is not simulated differently to that at downstream spatial units 3 semi distributed approaches based on topographic positions in this type a representative hillslope is used to represent the landscape in a subcatchment bonumá et al 2014 for example a subcatchment may be separated into three spatially connected landscape units being upslope midslope and valley bottom areas uniform properties within a landscape unit are assumed and thus the unit does not distinguish between different climate regions nor covers variable slope conditions examples include the land segment in hspf bicknell et al 2001 and overland flow elements ofes in wepp flanagan et al 2012 4 fully distributed spatial approaches where a subcatchment is divided into hydraulically connected elements such as grids or triangular elements each element has its own combination of properties such as slope land cover soil etc this type demands substantial amounts of data and typically has much larger computational requirements than lumped or semi distributed approaches and thereby is not considered feasible for large scale catchments pignotti et al 2017 the grid approach is implemented in wam bottcher et al 2012 annagnps bosch et al 1998 and answers beasley et al 1980 the property based semi distributed approach is the one most commonly used for catchment models each spatial unit e g hru fu represents similar hydrological behaviour land use tends to be assigned simply such as forest grazing cropping and urban areas the approach does not capture spatial interactions between the spatial units with consequences for the modelling of management practices nor does it capture the effect of distance of a spatial unit to streams and it cannot account for potential throughflow entering a spatial unit from neighbouring upland spatial units knowledge of drainage or channelization structures within a spatial unit e g a horticulture drainage system while important in managing constituent transport often cannot be captured adequately in a subcatchment scale stream network investigations are ongoing to develop a hybrid spatial discretisation approach between property based and position based semi distributed approaches earlier works include j2000 s fink et al 2007 and ages w ascough et al 2015 which both employ a topological routing scheme to account for spatial connections between hrus a combination of landscape units and hrus was developed by volk et al 2007 and implemented in the new version of swat swat bieger et al 2017 to address the limitations of typical hrus which have no spatial interconnectivity ning et al 2015 propose another approach involving the generation of spatially continuous hrus in a subcatchment with explicit hydrological properties land cover soil type and slope and a specific location in theory spatial units can be defined in such detail as to capture the factors deemed important relating to spatial positions of the property based spatial unit however the level of detail needs to be consistent with the information available so that model uncertainty and runtimes are not increased unnecessarily 4 3 temporal dynamics the representation of temporal heterogeneity of constituent fluxes is another important consideration in the conceptualisation of water quality models ideally model conceptualisation should include the selection of suitable temporal resolutions and incorporation of other temporal dynamic processes in the models that are scientifically valid useful for management and feasible in terms of resource and data availability a typical phenomenon termed hot moments by mcclain et al 2003 is that constituents can display disproportionally high export rates over a short period of time such as when fertilisers and pesticides are applied thus daily or coarser representation of flow may not adequately capture the hot moments of constituent generation and transport yang et al 2016 other factors can also suggest a requirement for sub daily models one example being the coupling of outputs of a water quality model to a receiving water model where traditional hydrodynamic and biogeochemical process models of fluid dynamics operate on hourly or lower time steps set against this is the fact that most water quality models operate on a daily or larger time step however in a more recent development of swat sub daily processes such as weather and infiltration processes and channel routing have received more attention but this functionality is generally invoked for modelling urban stormwater management or small areas of agricultural land e g maharjan et al 2013 on the other hand sometimes the management questions being asked of water quality modelling can be simplified to facilitate the use of daily or larger time steps if the modelling objectives are related for example to long term loads of a conservative constituent then a daily time step or even monthly may suffice alternatively it might be convenient to formulate a management question in terms of risk wherein one asks what conditions and modelling assumptions might cause a problem these sorts of simplifications become desirable when sub daily simulations of water quality for catchments overwhelm existing model support infrastructure e g input data processing runtimes data storage and post processing calibration data calibration timeframe perhaps pervasively problematic with sub daily simulations is that they require long term temporal resolution in rainfall records and flow gauge data which are often not available in most areas sub daily simulations also pose challenges in data quality assurance analysis and requirements as software to perform these checks is not readily available and existing capabilities are not well utilised therefore the requirement to increase temporal scale of constituent generation should be assessed on a case by case basis and reflection be given to what management questions can be answered with the resources available 4 4 data requirements and parameters 4 4 1 input data requirements the input data needed to satisfy the data requirements of the common water quality models and software platforms vary from moderate e g ewater source to high e g hspf reflecting their different process representations and temporal and spatial resolutions table 7 most of the catchment models require basic inputs such as climate typically rainfall and potential evapotranspiration for rainfall runoff models dem for generation of stream networks and spatial segregation of a catchment and mapping and classifications of land use the main difference in terms of input data requirements between catchment water quality models lies in the number of inputs for each of the data categories meteorological and or hydrological data catchment stream geometry and use land cover land management soil data and constituent data more complex models such as hspf and swat include the simulation of snow melt which adds additional data requirements swat also implements qual2e as its in stream model which has 100 individual inputs across three categories stream or river system inputs defined for each reach or smaller element global variables including water quality constituents and some physical characteristics and forcing functions such as flow water quality characteristics and local climatology inputs complex models may be difficult to parameterise or apply well to data sparse environments to address the challenges and risks of applying models in such a situation swat and hspf modelling communities have invested in tools and data sets to support their application to data sparse environments for example swat includes a number of weather generators which can downscale monthly climate data to daily data required for the models for hspf the minimum data required to model water quality is a dem land use data and some weather data additional data may be needed to run hspf such as soil data for erosion and sediment transport modelling applications stream network data to improve watershed delineation and watershed analysis and flow data for calibration purposes these data can potentially be sourced from global databases in the absence of higher resolution local data crossette et al 2015 there are implications for the interpretation of model results and analysis of model performance when there is little local data to provide input to or test any model see sections 5 and 6 3 however this is balanced by the value gained when model developers provide initial default input and output data sets this can encourage model users to source reasonable values as errors in input values and or units can severely compromise model results many developers provide user manuals that contain this information 4 4 2 model parameters the number of parameters required to simulate water quality depends on the model structure the constituent selected and the number of spatial units in general the more empirically based models e g sparrow require less parameters than those with more process based models e g swat table 8 model parameters can be measured e g soil bulk density estimated through literature or databases e g curve number or calibrated e g delivery ratio it is suggested that we should use measurements as much as possible and minimise the number of parameters needed for calibration malone et al 2015 refsgaard 1997 over parameterisation tends to be a pervasive feature of distributed models this is sometimes addressed by parameter reduction such as by fixing spatial patterns of a parameter but allowing its absolute value to be calibrated refsgaard 1997 alternatively the size and complexity of a model are addressed by providing constraints that regularise the calibration problem when many parameters are used in a calibration whittaker et al 2010 the simplest constraints are where some parameters are fixed at particular values but constraints can also be bounds or probability distributions model calibration is discussed further in the next section 5 current models and software model performance 5 1 calibration and validation calibration and validation are major challenges for catchment scale models compared to point scale or field scale models catchment scale models often involve quantity and constituent fluxes with respect to multiple spatial units draining to connected channels spatial variations in these multiple spatial units and additional processes and their interactions such as for in channel floodplain reservoir and wetland representations arnold et al 2015 some review papers have been published recently which give a good overview of the current practices and recommendations for model calibration arnold et al 2015 moriasi et al 2012 2015 single site calibration validation is generally recommended for areas with uniform characteristics e g soil slope vegetation meteorology while multi site calibration validation is recommended for large areas with more varied complex physical characteristics and or when observed data for a given process are available at multiple locations within the study area moriasi et al 2015 evidence for the benefits of the multi site approach for large catchments is mixed for example shrestha et al 2016 reported that multi site calibration did not improve simulations of flow and sediments compared to single site calibration however simulation results for tn and tp loads improved in terms of calibration applying an appropriate and systematic strategy is essential the appropriate approach however depends on the complexity of the application and ranges from a single stage for a simple model looking at a single process to a stepwise iterative approach for complex models with multiple processes and parameter interactions daggupati et al 2015 a stepwise iterative approach applies to the use of multiple parameters to calibrate single or multiple output variables each parameter is optimised in sequence prior parameters are readjusted after optimising successive parameters so that changes in successive parameters have not shifted the optimal value daggupati et al 2015 currently the general sequence of calibration in catchment models is 1 calibration of hydrology parameters at multiple time scales focusing on baseflow and storm runoff 2 calibration of sediment focusing on the ratio of upland and channel sources and 3 calibration of nutrient and pesticide parameters arnold et al 2015 calibration can be undertaken manually and or using automated calibration software coupling of manual and automated calibration is often recommended for catchment models van liew et al 2005 reported that manual adjustments may be necessary following auto calibration to maintain mass balance and adequately represent the range in magnitude of output variables developing and testing of auto calibration techniques has been a rapidly developing field although most applications are related to the hydrology component of catchment models confesor and whittaker 2007 seong et al 2015 van griensven and meixner 2007 van liew et al 2005 for example hspf is typically calibrated manually assisted by the expert system for the calibration of hspf hspexp hydrology only duda et al 2012 there have been attempts to calibrate hspf models using the parameter estimation software pest doherty and johnston 2003 but the gauss levenberg marquardt glm search algorithm employed in pest can have difficulty in finding a global optimum solution and its performance can depend on initial parameter sets defined by users seong et al 2015 more recently an automated calibration tool for hspf hspf sce was developed using r to link hspf and the shuffled complex evolution optimisation algorithm sce ua seong et al 2015 ewater source has a calibration wizard built in to calibrate the hydrology component of the model including rainfall runoff and routing primarily for unregulated systems ewater 2017 the calibration wizard allows the selection of different optimisation functions including sce ua uniform random sampling and rosenbrock a local optimiser ewater 2017 swat cup swat calibration uncertainty procedures current version 5 is freely available software which links swat models with a range of tools to support calibration sensitivity and uncertainty analyses abbaspour 2015 mike she has autocal as a built in sensitivity analysis and calibration tool a review of calibration methods of 22 models including nine models that can be applied at catchment scales is provided in moriasi et al 2012 calibration and validation of water quality models require the consideration of the allocation of spatio temporally distributed data daggupati et al 2015 summarised three different approaches to data splitting temporal split sample applied when catchment conditions are stationary and sufficient temporal data are available for the studied site differential split sample applied when the model used to predict scenarios is greatly different from existing conditions such as climate change or land use change and the proxy basin approach applied when there is insufficient data for temporal split sample approach but data available for a similar site the temporal split sample approach remains the most commonly used method in which the observation data are split into two or more parts one for calibration and one for validation daggupati et al 2015 a range of statistics and qualitative methods can be used as model performance measures and criteria for calibration and validation bennett et al 2013 sometimes different statistics may be necessary to address different processes or requirements for stream flows base flows and or constituent loads concentrations moriasi et al 2007 moriasi et al 2007 recommended that three quantitative statistics be used in model evaluation these are the nash sutcliffe efficiency nse the percent bias pbias and the ratio of the root mean square error to the standard deviation of measured data rsr in addition to graphical techniques in addition to selection of statistical measures daggupati et al 2015 and bennett et al 2013 point out the importance of linking calibration measures with goals of model use for example applications where there is greater interest in simulating relative results e g comparing scenarios may focus on model performance using differences in observed data relative to a baseline rather than on the absolute results the latter may be useful in comparing model outputs to given criteria or thresholds such as those specified in water quality guidelines in practice most models are calibrated or validated to the flow and load or concentration at one or more stream gauges where data are most likely to be available however arnold et al 2015 suggested it is also important to calibrate validate for sediment nutrient balance and the calibration of sources and sinks this is because errors in model conceptualisation or development may lead to overestimation of one process and compensating underestimation of the other leading to overall good fit at stream gauges when using such a model for scenario analysis for example testing management options that focus on a particular process the model may give incorrect results because the process is not well represented in the model although hard data i e long term measured time series data typically at a point within a catchment are often not available for the calibration of sediment nutrient balance or sources and sinks soft data i e information that may not be directly and or routinely measured in the study area may be useful arnold et al 2015 more detailed discussion on soft data is provided in section 6 3 1 with this concept swat check was developed to create process based figures for visualisation of the appropriateness of output values in constituent balance e g sediments from different sources and sinks and alert model developers to output values outside the typical range white et al 2014 5 2 uncertainty tools the importance of addressing uncertainty in model predictions is widely recognised in water quality modelling especially if the models are used for risk assessment and or decision making harmel and smith 2007 reckhow 1994 abbaspour 2015 argued that uncertainty and calibration are intimately linked model calibration is conditional e g type and length of data used objective function definition optimisation routine and thus the results of calibrated models along with their uncertainties are conditioned on the assumptions explicit or implicit in the model and calibration although many model platforms provide auto calibration tools provision of uncertainty tools is not common examples of inbuilt uncertainty tools with the model platform include swat cup abbaspour 2015 and qual2eu which allow the user to assess the effect of model sensitivities and of uncertain input data on model forecasts using first order error analysis and monte carlo simulation brown and barnwell 1987 most uncertainty studies of water quality models are employed using model independent tools matott et al 2009 identified 65 evaluation tools for environmental models including 26 uncertainty analysis tools such as simlab glue pest and mcat although tools are abundant these tools are developed using different programming languages input output file formats compilers and development platforms making it awkward to readily integrate them into existing models matott et al 2009 commonly used uncertainty quantification methods for hydrology and water quality models include monte carlo simulation first order analysis foa or gaussian approximation kalman filtering generalised likelihood uncertainty estimation glue bayesian analysis and mean value first order reliability analysis method mform each method has its own advantages and requirements guzman et al 2015b more detailed discussion on uncertainty analysis methods is provided in section 6 3 3 6 discussion of challenges and emerging topics the previous three sections explored the attributes of water quality models based on some of the key catchment water quality models or platforms in the international scientific literature drawing on these we identified key challenges or emerging topics in the development or use of water quality models from the model use perspective key challenges or emerging topics include large scale applications model integration improving model usability and communication from a model development point of view we would like to draw attention to preliminary data analysis modelling management practices and technology advancement in terms of model performance three topics stand out for more attention incorporating soft data model identifiability and restructuring and advances in analysis of uncertainty and its management overarching challenges relate to adherence to best modelling practices and capacity building of modellers and model users and crucially the inherent challenge in separating the effect of climate from the influences of land cover or practice on water quality some of these discussions are also supported by other authors pechlivanidis et al 2011 rode et al 2010 volk et al 2009 6 1 model use 6 1 1 large scale applications around the world water quality models are increasingly being applied to large river basins typical applications have drainage areas between hundreds to thousands of km2 borah and bera 2004 wade et al 2002 more recently water quality models have also been used for mega catchments for example sparrow has been used to assess tn and tp sources in the mississippi and atchafalaya river basins 3 million km2 alexander et al 2008 ewater source with customised dynamic sednet plugins is being used for predicting sediment yields in the great barrier reef catchments in australia 420 000 km2 ellis and searle 2013 these models are intended to help the water quality community make large scale management decisions that will have significant costs and implications for funding arrangements in addition environmental forecasting programs are being actively developed around the world pelletier and tyedmers 2010 tilman et al 2001 the ability to forecast water quality changes at large scales especially under potential extreme event pressures flood and drought may become increasingly desirable by government agencies there are several challenges associated with large scale applications of water quality models natural complexity spatial heterogeneity and sparse measurements are always an issue but are exacerbated for large scale applications jakeman et al 1998 we are tasking the models to address more complex systems and pollutant types ambrose et al 2009 and more challenging management questions including balancing complex management objectives across different scales davis et al 2017 barriers in data availability and model calibration can be significant for large scale applications abbaspour et al 2015 to address these challenges we need to be more careful to ensure the models we develop or use have the right complexity reaching a balance between developing an overly aggregated model versus one with so much detail that model set up and application is prohibitive is one of the top priorities for large scale applications if the research community were to actively reflect and publish on the complexity of their catchment models this would provide a valuable resource for readers in terms of making choices relating to model complexity for their own case studies 6 1 2 model integration catchment water quality models are increasingly being used in conjunction with receiving water models debele et al 2008 another type of integration that has received less attention is between water quality models and socio economic models at a local level such integration allows us to explore trade offs such as between water quality protection and farm production kaim et al 2018 at broader scales water quality models can be a component of modelling for assessing water energy food nexus issues in this context integration of water quality models and economic production models can help us answer questions about the impact of broad policies such as the promotion of biofuels ambrose et al 2009 however the complexity of water quality models may hinder both their integration with other models and the computational efficiency of the integrated models in addition as models are linked to others to create integrated models some uncertainties in each model component can accumulate through the integrated system while others may average out to address these challenges we need to investigate and develop more capable frameworks that allow efficient transfer of outputs including feedbacks between different types of models 6 1 3 model usability and communication catchment water quality models can produce a large amount of output data over space and time for different constituents with outputs presented in different ways e g trends statistical summaries when models are used for exploratory scenario analysis a large number of scenarios can be produced and there is a need to communicate consistency relevance and usefulness of the scenarios communicating such complex multi dimensional results is not trivial in addition recipients of model based knowledge may need to know the assumptions uncertainties and limitations of the models and how these affect the interpretation of model results for model users in particular and also understanding the transferability and applicability of the models for other case studies e g for modellers however interactions between developers of model frameworks model users and those with long term catchment understanding e g landholders are often limited by timeframes and resources for model development to address these issues we need to improve visualisation tools to allow modellers and decision makers to better understand insights generated from the modelling results ambrose et al 2009 develop methods for greater inclusion of stakeholder knowledge and interaction to improve understanding of catchment dynamics invest in more than just the models but also user interfaces user groups manuals training and model needs analysis establish better communication between model developers and model users and those impacted by model outputs government agencies landholders community improve model documentation including model performance and mapping of what we need from the system against what the model can do these activities should be driven both by funders of water quality model development and the research and model development community it requires attitudinal change in prioritising model usability and communication reflection from the modelling community including users and developers in terms of what works and does not work in improving model usability and communication and explicit resourcing of such activities there is a general trend across the modelling community to incorporate models and model results within decision support frameworks that may allow for greater interaction with model outputs mcintosh et al 2011 these frameworks help improve the communication of model results but also allow for the assessment of a number of scenarios or different model parameterisations in similar formats the intent is to reduce the complexity of model outputs into simpler representations of the information although this can sometimes lead to loss of understanding in the complexity of simulated natural systems in addition decision support systems can be useful repositories of the model development process kelly 2015 for instance the watershed analysis risk management framework warmf includes a consensus module which takes the form of a road map to guide stakeholders through the consensus building process chen et al 2004 an iterative process is proposed to enable social learning of the stakeholders so that a clear vision of what they need and want from a decision support system can be articulated and implemented volk et al 2010 6 2 model development 6 2 1 preliminary data analysis water quality modelling can benefit from preliminary data analysis to understand system response and deficiencies in the data high frequency observations for example can permit the use of sophisticated tools such as spectral analysis e g neal et al 2013 to gain understanding of the dynamics driving variations in water quality constituents unfortunately some necessary observations e g nutrient concentrations are not available at most locations limiting the use of such tools in many places we tend to be limited to sparsely sampled water quality information although information such as flow ec temperature and turbidity are increasingly being collected routinely if a small amount of high frequency event based data is available then tools such as correlation analysis between constituent concentrations and continuously monitored quantities including potential drivers like streamflow and rainfall as well as other relevant quantities such as ec or turbidity can be useful in gaining understanding of the dynamics drewry et al 2009 estimation of the groundwater contribution baseflow to total streamflow e g croke 2010 can also be useful in understanding the variations in observed concentrations particularly for constituents entering the stream from aquifers regression is a commonly used approach to convert patchy observed water quality data into information that can be used in a water quality model letcher et al 2002 this can include relating the concentration of a constituent e g tss to flow or a less sampled constituent e g particulate p to a more frequently measured quantity e g turbidity regression methods have been used extensively in constituent load estimation several tools have been developed and compared lee et al 2016 ullrich and volk 2010 such as the usgs loadest runkel 2013 runkel et al 2004 fluxmaster saad et al 2011 developed for use in sparrow and weighted regression on time season and discharge wrtds hirsch et al 2010 the regression methods employed in these tools were based on or improved upon the model described in cohen 2005 and cohen et al 1989 which consists of inputs in flow decimal time and seasonal and or period factors the result of such regressions needs to be adequately tested including reporting the uncertainty in the regressed values care also needs to be taken in the choice of function being fitted the tendency is to use linear regression which can include using transformations to make the relationship linear e g fitting a power law relationship however sometimes a non linear regression is better even though this requires an iterative approach note that the regression technique works well provided the dynamics of the system being modelled do not change changes in the system would require the regression relationship to be re established e g land use and climate change temporary storage of drainage return flows and recirculation of canal deliveries 6 2 2 modelling management practices informing management practices is promoted as one of the main uses of water quality models this invokes the need for water quality models to accurately represent the effects of management practices information on management practices is often lacking or is aggregated e g at the administrative district levels thus modellers need to decide how to spatially and temporally distribute practices in their model such as generating typical schedules for crop schönhart et al 2011 or fertiliser applications in addition knowledge about the sensitivity of the catchment models to parameters relating to management practices is still lacking e g ullrich and volk 2009 a key limitation of most if not all catchment water quality models is the representation of lag times this has implications for modelling the impacts of management practices meals et al 2010 argue that simulation models are not yet able to represent landscape processes realistically enough to support their use for program planning forecasting and evaluation of long term restoration efforts in complex systems the inability of water quality models to effectively capture lags can stem from the processes that are represented how practices can be represented using model parameters the spatial and temporal resolution of the model and the relevant data needed to calibrate the lag processes in situations where there has been a build up of a constituent in a water body e g salt phosphorus heavy metals there will be a lag time in the system response to changes in management practices which address generation of and delivery of the constituent to the water body meals et al 2010 decompose lag time into three components the amount of time taken for practices to produce the required effect it can take time for a treatment measure such as a revegetation project to reach full effectiveness equally the effectiveness of some treatments could decline over time or vary under different weather conditions the capacity of models that take either an export rate or emc dwc approach in modelling constituent generation to represent the change in treatment effectiveness over time is limited the amount of time taken for effects to be delivered to the water body this component relates to the travel time from the point of the intervention e g the riparian buffer to the water body where the response is desired groundwater systems can have a long lag time due to the amount of a constituent stored in an aquifer and the input and output flux rates this can include saline aquifers as well as contaminated aquifers for example due to use of firefighting foams that contain perfluorooctane sulfonate pfos and perfluorooctanoic acid pfoa the amount of time it takes for the water body to respond the lag time will also depend on the change in the load delivered to the water body relative to the store of the constituent in the water body and bed sediments for example poorly flushed water bodies may have a large store of sediment and nutrients in the water column and bed and hence any improvements in water quality and ecology in response to catchment treatments will take longer to discern compared with well flushed receiving bodies over the longer time horizon incorporating lag time associated with the legacy sediment can also be important in some systems legacy sediment broadly applies to anthropogenic sediment produced episodically over a period of decades or centuries james 2013 legacy sediment was deposited to streams and is still being transported understanding the lagged response and prolonged recruitment of legacy sediment and adequately representing these processes in catchment models are still challenging walter et al 2007 another challenge in modelling management practices is the incorporation of adoption and implementation of interventions across a catchment adoption of an intervention can take considerable time to reach peak levels reflecting that people may decide to adopt at different rates but also the time needed to implement programs and similarly the level of compliance on an aspect of water quality practice can vary in time and space not often considered in the conceptualisation of water quality models or the definition of scenarios this contributes to the inability stated by meals et al 2010 of most models to realistically simulate the effectiveness of many management practices many models apply static management practices where the area or effectiveness of a treatment e g sediment trapping efficiency are temporally static this can be problematic if the models are used for evaluating change in management practices over time 6 2 3 technology advancement as has been suggested by ambrose et al 2009 improved computer technology and widespread use of the internet in the early 2000s enabled substantial improvements in water quality models and their applications developments have been centred around the handling of detailed environmental analysis both spatially and temporally improved user interfaces and linkages with gis software enhancing accessibility to environmental data and the construction of robust modelling frameworks linking hydrology and water quality as technology continues to advance at a rapid pace it is apt that we identify some of the main challenges of linking technology advancement to the continued development of water quality models and platforms from the monitoring technology perspective traditional and emerging remote sensing has become a valuable and important data source to improve water quality models new satellites and sensors can provide higher resolution spectral e g to differentiate between algal groups and spatial e g smaller grid cell size data needed to monitor water quality ritchie et al 2003 however there are several challenges associated with remote sensing data including costs especially for new and or high resolution data tools and expertise required to analyse including uncertainty and ground truth validate and interpret the remote sensing data turner et al 2003 for example fisher et al 2018 evaluated the value of information in a higher resolution satellite image 1 m from digital globe vs 30 m from landsat on land use classification and total suspended solids load estimates from swat they concluded that higher resolution data may be preferable to lower resolution data under certain conditions such as 1 a landscape has small features and or fine scale variation in land cover land use 2 a large portion of land cover land use change patches are smaller than the pixel size at lower resolutions and 3 high accuracy is necessary to inform decisions the use of unmanned aerial vehicle uav technology may offer high resolution data for water resource management while being flexible in terms of costs and the location and timing of data acquisition debell et al 2015 pajares 2015 computer technology for water quality modelling has an increasing role to play with the growing demand for model performance testing uncertainty and scenario analysis parallel and distributed processing allow us to better utilise personal computers with multiple processers or to take advantage of the internet and supercomputer technology ambrose et al 2009 buyya et al 2009 humphrey et al 2012 advances in data management and data mining technologies enable management of large amounts of data and interpretation of these data development in information technologies has seen the use of water quality models for real time forecasting bedri et al 2014 and web based applications booth et al 2011 to aid scenario analysis and decision making enhancing modelling frameworks to automatically connect models to external data such as real time online data bases for meteorology flows and point source discharges can improve the utility of the models for such purposes ambrose et al 2009a however more data are not always better and the modelling community must improve its understanding of the value of information for the modelling and or model users and whether this justifies the investment in new technology this necessitates better understanding of user needs with respect to technology and water quality assessments in addition value of information statistical concepts and model based methods can be effectively utilised in the design of monitoring networks both with respect to on ground information such as land cover and practices and in stream measurements the aim would be to use the model to identify monitoring information that reduces critical uncertainties in model predictions at reasonable cost 6 3 model performance 6 3 1 incorporating soft data soft data can provide a means to formally assess the reasonableness and consistency of model structures and outputs arnold et al 2015 defines soft data as information on individual processes that may not be directly measured in the study area may be an average annual estimate and may entail considerable uncertainty seibert and mcdonnell 2002 advocated the use of soft data to augment hard data in the model calibration process stating that it should be actively identified and used where available they identified two ways in which soft data can be used to constrain model calibration 1 to evaluate aspects of the model simulations for which there is no hard data available e g ungauged systems and 2 to assess how reasonable the parameter values are based on field experience qualitative information can also be used in parameter inference mode by superimposing a constraint on the model output arnold et al 2015 posited that the process of developing soft data sets increases the modellers understanding of water quality budgets for the study area thus enhancing their ability to constrain the calibration process this is particularly relevant for catchment and basin scale modelling more so for ungauged systems where formal methods based solely on hard data may not be applicable winsemius et al 2009 the other use of soft data is to inform the selection and conceptualisation of models although not the focus of the seibert and mcdonnell 2002 paper the authors noted that the discussions with the experimentalist informed the conceptualisation of the model they developed discussions with other stakeholders e g catchment managers could provide a similar service a general process from arnold et al 2015 for incorporating soft data into the calibration process is given in fig 4 we identify several key challenges in incorporating soft data into water quality models and analyses these are centred around data collection approaches to map soft data onto numerical models mis trust in soft data and the transferability of soft data the challenge around collection of soft data revolves around what to collect and how to collect and use the data a broad list of examples of soft data were identified by arnold et al 2015 namely regional estimates of baseflow ratios or et average depths of groundwater tables average annual runoff coefficients for various land uses annual rates of denitrification from research plots found in the literature event mean concentrations nutrient sediment export coefficients sediment deposition from reservoir sedimentation studies average crop vegetation lai and county crop yields these data can be absolute values such as nitrogen uptake rates or relative comparisons such as baseflow ratio yen et al 2016 the sources of soft data include refereed literature grey literature and field surveys arnold et al 2015 expert opinion from experimentalists seibert and mcdonnell 2002 or others and model outputs or analyses e g transfer functions winsemius et al 2009 there are substantial transaction costs in the collection of soft data not only in terms of the time and financial resources needed to collect collate and analyse the data but perhaps also investment in building the capacity of modellers to appreciate understand and effectively utilise soft data mapping soft data into numerical models is not a trivial process soft data are inherently patchy and discontinuous in nature and therefore highly uncertain seibert and mcdonnell 2002 and so may not be readily converted into numerical values methods for the identification and transformation of valuable soft data for the task in question are required to reduce the transaction costs in soft data collection and uncertainties associated with the soft data an additional challenge for the more complex models is how to concurrently consider numerous soft data constraints yen et al 2016 has addressed this for swat auto calibration applications by coupling swat check with the integrated parameter estimation and uncertainty analysis tool soft hard data evaluation ipeat sd tool to constrain the bounds of soft data with this tool 59 constraints can be considered although the authors caution that modellers should be aware of which constraints are needed as it is possible that no feasible solution may be identiﬁed if too many constraints are included simultaneously thus the authors advised the purpose of using soft data constraints to be clearly deﬁned before conducting a model calibration study yen et al 2016 related to the aforementioned challenge of using soft data in numerical models is the mismatch in scales of the data and models for example while data may be instantaneous model results may reflect daily or coarser averages as with trust in models trust in the soft data can also be an issue that may hinder their adoption in modelling practices part of building capacity and trust in the use of soft data is challenging traditional practice around the assessment and calibration of models and fostering awareness of the potential benefits this was neatly demonstrated in the seminal paper of seibert and mcdonnell 2002 where the authors demonstrated that the standard metric reff obtained using soft data multicriteria calibration was lower than that obtained without it but this was negated by improved overall performance and much reduced parameter uncertainty soft data ideally should be local to the study catchment or area however it is possible to use soft data for other areas as long as the transferability of the soft data can be justified little thought has been given to the transferability of soft data in the literature to progress the research and practice around the collection and use of soft data in water quality modelling there is a need for more published examples in the literature that demonstrates what has or has not worked under particular conditions and for documentation of tools to support modellers utilising soft data in their work for example a flow chart would help illustrate how to apply soft data locally and in areas with similar characteristics for model based simulation to date most literature considering soft data has focused on hydrological components despite evidence that watershed modelling results can be compromised by failing to consider soft data for both flow and nutrient processes yen et al 2016 however this literature base has started to expand in recent years due to the stated priority of this area of research by the swat research community arnold et al 2015 bieger et al 2017 yen et al 2016 6 3 2 model identifiability and restructuring structural non identifiability koopmans and reiersol 1950 occurs if a model is found to have non unique parameters due to model structure input and outputs even when exact model inputs and output data are used in conjunction with a given objective function and constraints shin et al 2015 identifiability of a water quality model or indeed an environmental model can be considered as the extent to which one can capture its parameter values from observational data and other prior knowledge of the system marsili libelli et al 2014 an ideal water quality model needs to strike a balance between representing processes on the one hand and retaining model identifiability on the other identifiability to a general extent means keeping model complexity under control while it may seem important to represent more complexity in a model increases in parameterisation should be justified by being able to identify the additional parameterisation either from the data available or from other knowledge of information striking the balance between process representation and identifiability may require the restructuring of an existing model this might involve simplifying an overly complex model to make it more identifiable conversely models that focus on mathematical solutions over process representation and are therefore often considered too simplistic by developers of process based models may require modification to make the relationship between the model and the process more intuitive while many modellers focus on better process representations less attention is given to model identifiability which contributes to uncertainty for example schwarz et al 2006 reported that non unique models those for which nearly identical model predictions result from the use of different parameter sets and values may have large uncertainties in the interpretability of the parameters and their characterisation of the effects of specific processes there is growing recognition by the hydrology e g shin et al 2015 and water quality e g schwarz et al 2006 modelling communities that a large fraction of the total variability in the observations can frequently be explained by a relatively small number of model parameters and that increases in the number of parameters beyond these limits are likely to have at best only marginal increases in explanatory value to move forward we need to invest more in identifiability analysis which will help expose inadequacies in the data or suggest improvements in the model structure matott et al 2009 marsili libelli et al 2014 and guillaume et al under review provide an overview of practical techniques that can be used to assess identifiability for environmental models including the use of sensitivity analysis quadratic and higher degree response surface methods dynamic identifiability analysis and pseudo monte carlo methods bayesian and non bayesian these types of analysis may lead to restructuring of the models so that they can become more identifiable or we can accept the model as is but provide support for uncertainty analysis both from the viewpoint of model and inputs for models that are too simple we can investigate alternative hypotheses on different processes or undertake more studies to improve understanding of the processes 6 3 3 uncertainty analysis data model structure and parameters are typically considered when we analyse sources of uncertainties in model outputs however a more comprehensive list is warranted if one wants to truly capture and address sources of uncertainty additional sources that may need to be considered include conceptualisation of the model future forcing conditions initial boundary conditions prior knowledge model family e g empirical physical hybrid model purpose and or objectives verification validation process and uncertainty assessment including qualitative aspects model code and numerical implementation and communication process jakeman et al 2018a the aforementioned authors address this broader issue of uncertainty in the modelling process and model outputs and stress the need to employ good modelling practices see section 6 4 in a report to the queensland government on good modelling practice jakeman et al 2018b the authors made the following recommendations with regard to managing uncertainty in water quality modelling list and characterise sources and try to rank the criticality of uncertainties arising in the whole modelling process through such means as expert elicitation stakeholder engagement sensitivity and more quantitative uncertainty analyses carefully consider appropriate model complexity taking into account uncertainty data support and system behaviours this is likely to include effective simplification of the model with good documentation of the assumptions made and their implications factor in the appropriate costs of holistic uncertainty assessment i e taking into account all sources in project budgeting it will be worth it in the longer term place due emphasis on communicating uncertainty an area of emerging attention that could be advanced through focusing on meeting its challenges in the water sector visualization of indicators of concern are an aspect in such an endeavour their design should pay special attention to possible interpretation biases and ways to control them pay explicit attention to the way model results and uncertainty are communicated in written reports and publications in terms of addressing uncertainty in a water quality model several methods are available jakeman et al 2018a at a simple level qualitative approaches can be used to evaluate the prediction and the limitations involved this can be achieved through quality assurance of the modelling process refsgaard et al 2007 or through qualitative judgements about the information and how it is produced van der sluijs et al 2005 sensitivity assessment can be a useful procedure to explore the determinants of model outputs literature on sensitivity analysis techniques are abundant norton 2015 pianosi et al 2016 sarrazin et al 2016 and readers are directed to that literature for detailed descriptions and discussions on sensitivity analysis these techniques involve many trial runs of the model and are often limited by computational capacity including input output data overheads especially for large catchment models on the other hand most of the water quality models have very simple mathematical equations in that these models often consist of variables and parameters combined through only a few elementary operations such as products sums ratios powers and exponential the sensitivity of the results of these operations to input parameter variation can be analysed algebraically thus bypassing some of the need for computer simulation norton 2008 it also allows one to calculate the local derivatives at any point in the parameter range thereby obviating the problem posed by shin et al 2013 of having sensitivity being related to the parameter range selected there are several methods for estimating and or reducing the uncertainty in model outputs this include first order analysis which focuses on affects from individual inputs ignoring interactions between inputs c f screening in sensitivity analysis the kalman filter kalman 1960 is a method of assimilating new data to update parameter values and estimate their uncertainties as well as the uncertainty in the model output the assimilation of new data leads to a reduction in the uncertainties with the magnitude of the reduction depending on the information contained in the data and the noise present from a different perspective the first order reliability method form is a tool that can be useful in evaluating the impact of parameter uncertainty in terms of reliability vulnerability and resilience maier et al 2001 monte carlo simulation is a general method for analysing parameter uncertainties of which bayesian methods are a pertinent example bayesian inference methods produce a probabilistic distribution posterior density of parameter values that are consistent with observations kaipio and somersalo 2005 generalized likelihood uncertainty estimation glue is a popular bayesian like method for appreciating the uncertainties in a model beven and binley 1992 sensitivity analysis is often used to screen out insensitive parameters before applying uncertainty analysis shen et al 2012 for example used the morris qualitative screening method morris 1991 to select the 20 most sensitive parameters affecting stream flow and sediment yields in a swat model about 4500 km2 then used glue to investigate uncertainty of model outputs and parameters they identified several sensitive but non identifiable parameters given the available data which contributed to model uncertainty due to the non identifiable nature of these parameters calibration of these parameters will not be feasible and more detailed measurement data relevant for these parameters will be required shen et al 2012 surrogate models or model emulation can be attractive when a model is so computationally expensive that adequately understanding its behaviour and quantifying uncertainty is essentially intractable if using sampled runs of the model jakeman et al 2018a some of the most popular surrogate approaches include polynomial chaos expansions sudret 2008 gaussian processes rasmussen 2004 and sparse grids bungartz and griebel 2004 razavi et al 2012 provided a good review of surrogate modelling in water resources including water quality modelling yang et al 2018 use gaussian process emulation of a swat model to characterise the sobol sensitivities of parameters so that insensitive ones can be fixed and the range of sensitive ones can be narrowed thereby producing glue uncertainty estimates more efficiently when the uncertainties in model inputs and parameters are high other simulation methods such as exploratory modelling bankes 1993 and crash stress testing coron et al 2012 can be useful exploratory modelling and analysis involves exploring scenarios about future conditions model structure and parameter values it can be used to search for scenarios that lead to good or poor outcomes or specified objectives such as robustness metrics mcphail et al 2018 crash testing is a related less formal technique that attempts to identify through simulation experiments what parameter sets observation periods and other conditions establish limitations or invalidate the model this can involve examining the performance of the model through time and space e g for models with a node link structure the performance can be evaluated at each node most uncertainty analyses are framed around reporting of model behaviours other topics related to uncertainty analysis in water quality models that are less studied or reported upon include can we discern the signal of changes in management practices taking into account climate variations and model uncertainties how can the understanding of uncertainties be fed back into catchment monitoring practices and model improvement how can we best investigate the cascading of uncertainty this is especially an issue for catchments with many links and or component models how can we effectively interpret and communicate uncertainty considering the intended model uses e g harmel et al 2014 we need more discussion of and investigations into these questions to enhance the usability of catchment water quality modelling for decision making the benefits of uncertainty analysis go beyond its characterisation of model predictions uncertainty analysis can also assist in improving the design of water quality monitoring programs often the collection of water quality data is expensive and only a limited amount of observational data can be obtained but not all data provide the same amount of information about the processes they are helping inform and thus from the point of view of improving modelling uncertainty analysis can help us move toward a more optimal design of monitoring programs such that monitoring can be focused on reducing the critical sources of uncertainty jakeman and jakeman 2016 6 4 overarching challenges 6 4 1 good modelling practices and capacity building as water quality models are increasingly being promoted in assisting catchment management and policy we increasingly demand that the model development evaluation and application process conforms to standards that ensure the usability soundness and defensibility of their outputs for decision making ambrose et al 2009 our expectation with regards to models can be roughly grouped into three quality criteria van voorn et al 2016 credibility the scientific logic of the model and soundness of the knowledge salience the societal and political relevance of the use of the model legitimacy a fair representation of the views values and concerns of involved stakeholders in the model good practices for water quality models have been recommended for model calibration and validation daggupati et al 2015 documentation and reporting saraswat et al 2015 spatial and temporal considerations baffaut et al 2015 parameterisation malone et al 2015 sensitivity analysis yuan et al 2015 and uncertainty considerations guzman et al 2015b most of these recommendations relate to model development and performance testing techniques as covered in principle for environmental models in general by jakeman et al 2006 less attention has been given to other aspects of modelling relating to model use such as scoping problem framing and formulation and communicating of findings though these have been reported and discussed in jakeman et al 2018b some key points are the purpose and objectives of a model should include a clearly articulated set of user data requirements processes to be represented questions functionalities system boundaries and predictive quantities of interest when possible we should employ multiple lines of evidence or multiple methods for making water quality predictions from a communication point of view we should employ user centred design for visualization development early in the modelling process and leverage different visualization tools to engage different audiences e g academics policy makers stakeholders we should educate consumers of model results about the dangers of being provided only a single number upon which to base decisions but also address their needs by providing uncertainty information in a format that fits within their workflows and which is intuitive and readily communicable to non technical audiences developing good practices tailored to catchment water quality modelling is an on going task and requires us to instil good practices by undertaking and sharing cases as a community that involves both modellers and model users this should preferably be achieved through collaboration on an issue that involves the major water modelling domains part of this process is building up the capability and capacity of the developers and users of catchment water quality models in this regard open source software and code can support a continuous development by a user community as occurs with the swat platforms modelling is as much an art as a science and the performance and utility of a model relies on the knowledge and skills of the model developers likewise a better model and more accurate and reliable model results do not translate to better decisions as they also rely on both the ability of the model users to interpret the values and uncertainties associated with model results as well as the trust that model users place on the models and their developers decision making being a socio political process may influence when and what model based knowledge is requested and how this knowledge is used we modellers and users of models or their results need to reflect on the linkage between water quality models and decision making such reflection and ensuing dialogue will further advance the development of water quality models and their roles in supporting decision making 6 4 2 differentiating climate impacts from land use and management the capability to differentiate the impacts of land use change and or practices from climate influences is critical if we are to use models to practicably inform management however as argued by croke and jakeman 2001 from several australian studies the ability to predict the effects on flows and water quality of anything but major changes in climate and land use is limited especially if one is only using precipitation flow water quality time series data to inform the modelling the errors in predictions from climate tend to swamp modest changes in land cover and or land use consider for example a model that may give a good fit to observations on constituent concentrations at water quality monitoring sites the effects of management on improving water quality will not be represented adequately during scenario analysis if this good fit is due to an overestimation say of climate impacts on water quality being compensated by an underestimation of the influence of management practices hence developing and demonstrating a model s ability to distinguish climate impacts from the impacts of land use and management is not trivial renner et al 2014 firstly changes in land use and management practices often intertwine with climate impacts for example zhang et al 2001 demonstrated that the relationship between annual rainfall and annual evapotranspiration et varies greatly depending on land uses therefore as land use shifts over time and space the relationship between et and rainfall may change which has implications for the modelling of flows and subsequently constituents loads a second issue is that the calibration period with its specific climate pattern selected for a water quantity and or quality model may have a large impact on the parameter values daggupati et al 2015 and subsequently model predictions therefore we need as a first step to be more rigorous in understanding and representing how climate in the calibration period affects the model parameters and response in theory the water quality modelling community could advance its ability to discriminate between climate and land use or management signals at the catchment scale continuing to collect and analyse small scale monitoring data would improve process understanding whilst the development of robust methods to upscale learning from these small scale experiments to the catchment scale may advance catchment scale predictions and a model s ability to differentiate climates impacts from land use and management on the other hand it is important to recognise that this issue is somewhat intractable because modellers rarely if ever have enough data at catchment scales to test upscaling of model parameterisations developed from the small scale experiments perhaps the most immediate gain is to use models to indicate the value of data including those from on ground and in stream monitoring remote sensing experts and other sources this exercise would investigate what type when where and at what frequency of data provides the most leverage for increasing predictive accuracy and distinguishing between climate and land effects on water quantity and quality 7 conclusions despite a significant increase in published papers in catchment scale water quality modelling and an inherent wealth of new ideas methods and knowledge there has been no comprehensive review on the state of modelling advances since the early 2000s and even less so in the practices of modelling in supporting catchment management in this paper we have concentrated our review of literature mainly on the last 15 years so as to synthesise recent developments in water quality modelling the review focuses mainly on catchment scale models of freshwater non urban systems with particular emphasis on sediments and nutrients though many of the considerations extend to other constituents we explored 10 key attributes in selected existing water quality models grouped into three categories model use model development and model performance from this we deliberated on 11 key challenges and or emerging topics in catchment water quality modelling large scale applications model integration model usability and communication preliminary data analysis modelling management practices technology advancement incorporating soft data model identifiability and restructuring uncertainty analysis good modelling practices and capacity building and lastly differentiating climate impacts from other influences some of these challenges can be addressed relatively easily such as preliminary data analysis and ongoing attention to good modelling practices other challenges may be less tractable but are essential if we are to use catchment scale water quality models to inform management such as modelling management practices and differentiating the effects of climate impacts on water quality from those associated with land use and management developing and applying exploratory modelling and risk based approaches will help generate valuable information to support management decisions under uncertainties most water quality models possess complex parameterisations improving model identifiability by using analysis of it to devise a more balanced representation of the processes especially in the case of large scale applications using complex models can reduce computational demands and facilitate uncertainty analysis soft data can be employed to constrain parameters while model emulation can identify insensitive ones and facilitate uncertainty analysis especially when model runtimes are long advancing these challenges will help modellers to better support those responsible for management of water quality at the catchment level acknowledgement this work is funded by the australian and queensland governments queensland department of natural resources mines and energy and department of environment and science through the reef plan and the queensland water modelling network as well as the western australian government department of water and environmental regulation and the new south wales department of industry lands and water division we would like to thank nigel quinn martin volk one anonymous reviewer and the manuscript editor for their constructive comments on the manuscript appendix a supplementary data the following are the supplementary data to this article supplementary supplementary appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 12 008 
26247,many applications of global sensitivity analysis gsa do not adequately account for the dynamical nature of earth and environmental systems models gupta and razavi 2018 highlight this fact and develop a sensitivity analysis framework from first principles based on the sensitivity information contained in trajectories of partial derivatives of the dynamical model responses with respect to controlling factors here we extend and generalize that framework to accommodate any gsa philosophy including derivative based approaches such as morris and delsa direct response based approaches such as the variance based sobol distribution based pawn and higher moment based methods and unifying variogram based approaches such as vars the framework is implemented within the vars tool software toolbox and demonstrated using the hbv sask model applied to the oldman watershed canada this enables a comprehensive multi variate investigation of the influence of parameters and forcings on different modeled state variables and responses without the need for observational data regarding those responses keywords global sensitivity analysis time varying sensitivity analysis parameter importance uncertainty variogram analysis of response surfaces vars sobol morris progressive latin hypercube sampling plhs dynamical systems models performance metrics sensitivity indices software availability the ggsm method and case study presented in this paper are embedded in the vars tool software package available at www vars tool com 1 introduction and background advanced dynamical earth systems models desms are commonly used to simulate the spatio temporal behaviors of natural and engineered processes over large domains bennett et al 2013 haghnegahdar et al 2017 their value derives from their capacity to help us understand how the behaviors and functioning of such systems vary dynamically with time in response to the interactions between complex internal dynamics and external forcings oreskes 2003 yassin et al 2017 to be able to diagnostically improve the predictive abilities of such models it is important to understand the dynamics underlying such behaviors and the extent to which different factors e g model parameters forcings and boundary and initial conditions exert controls on those behaviors over time razavi and gupta 2015 in recognition of this importance a variety of approaches and tools for so called global sensitivity analysis gsa have been developed most available gsa approaches fall under the two general categories of derivative based and direct response based e g variance based higher moment based or distribution based the former globalizes the concept of local sensitivity based on partial derivatives of some selected model response campolongo et al 2007 morris 1991 sobol and kucherenko 2009 whereas the latter measures the component contribution of different factors to the overall global frequency distribution of the response itself the most well known of these being the variance based sobol method saltelli et al 2008 sobol 2001 recently razavi and gupta 2016a 2016b developed a more general variogram based approach that bridges across and unifies the derivative based and variance based approaches by exploiting variograms cressie 1993 to account for the covariance in the factor space structure of the response and to study variability as a function of perturbation scale haghnegahdar and razavi 2017 importantly razavi and gupta 2016a 2016b showed that the variogram analysis of response surfaces vars approach is computationally more efficient than the derivative based and variance based methods while providing deeper insight into the global sensitivity of model response to uncertainties in and perturbations of various controlling factors in general gsa is a systems theoretic approach to characterizing the sensitivity of one or more model responses to different controlling but uncertain factors e g model parameters forcings and boundary and initial conditions saltelli et al 2008 razavi and gupta 2015 as discussed by gupta and razavi 2018 depending on the application model responses are typically selected to be either a one or more model performance metrics that quantify the closeness of the desm dynamic state flux responses to observed data b a specific targeted aspect of those state flux responses c a compressed set of properties e g signatures that characterize those state flux responses or d the spatio temporally varying state flux responses themselves use of the a type responses performance metrics is informative mainly in a model calibration context where typically the metrics provide spatio temporally aggregate measures of the model s ability to reproduce observations during some calibration period e g borgonovo et al 2017 haghnegahdar et al 2017 rosolem et al 2012 van werkhoven et al 2008 some studies have extended this aggregate metric approach to account for systems dynamics and time varying model performance by applying a moving window approach to the computation of the metric e g cibin et al 2010 herman et al 2013 pianosi and wagener 2016 unlike the metric based approaches methods that use other types of responses b d do not require the availability of corresponding data targeted b type responses are often used in a decision context for example flow peaks may be targeted when concerned with flood protection gsa studies using b type responses include van griensven et al 2006 who studied average catchment outflow over a period of time and savage et al 2016 who studied spatio temporally averaged maximum water depth compressed c type responses extend this targeted approach to investigate characteristic signature properties of the overall dynamical spatio temporal state flux response in this context a variety of methods both data driven campbell et al 2006 lamboni et al 2009 2011 marrel et al 2011 and physics driven gupta et al 2008 yilmaz et al 2008 have been used to compress the spatio temporal state flux responses of a model into sets of characteristic signature properties that are of a diagnostic contextually meaningful nature clearly the d type responses provide the most detailed spatio temporally varying information about the state flux behaviour of the model for example assessment of the time dependent sensitivity of model simulated streamflow can enable study of how model component process dominance varies over time and under different conditions guse et al 2014 reusser et al 2011 similarly one can study the sensitivity of spatial flood maps to different sources of uncertainty abily et al 2016 gsa based on the d type model responses is in principle particularly useful when we wish to better understand the dynamical behaviour of a desm further discussion of the use of these four different types of model responses in gsa appears in gupta and razavi 2018 in gupta and razavi 2018 we revisited the fundamental basis of gsa for dynamical systems models considering the four different model response types described above in particular we raised serious concerns about the use of performance metrics a type responses for gsa of desms we showed that an approach based on performance metrics is fundamentally inconsistent and incomplete is more correctly viewed as a form of model identifiability analysis rather than an analysis of sensitivity and that use of a performance metric to assess sensitivity unavoidably distorts the information provided by the model about relative parameter importance consequently we concluded that 1 it is a serious conceptual flaw to interpret the results of performance metric based analyses as being consistent and accurate indications of the importance of model parameters and 2 given that such approaches depend on availability of system state flux output observational data they are unable to provide an assessment of the sensitivity of responses for which such data are not available and so the analysis they provide is necessarily incomplete to address these issues and in keeping with our position that the most important goal of gsa is to gain better understanding of a model and the underlying system in gupta and razavi 2018 we framed the gsa problem from first principles starting from the theoretical basis for sensitivity the magnitudes and signs of the partial derivatives of model output trajectories with respect to their controlling factors based on those principles we then developed a global sensitivity matrix gsm approach that generates globalized derivative based parameter importance indices that account for the dynamical time varying nature of sensitivity of desms here we build upon gupta and razavi 2018 and previous work on gsa with d type responses and formalize a generalized global sensitivity matrix ggsm approach that can a be used with any gsa approach including derivative based direct response based such as variance based or more generally moment based and distribution based and variogram based methods b be applied to model responses of any type including a type if desired and justified in a calibration context and c be used to provide a multi method assessment of both time aggregate and time varying parameter importance within a single gsa experiment using the same set of parameter samples when coupled with the star vars algorithm razavi and gupta 2016b further we discuss the need for time normalization of sensitivity assessment of desms to better account for the dynamics of models and importantly their forcings we show herein how ggsm can be designed and how it can be used to assess the sensitivity of different model responses i e internal fluxes output fluxes state variables to various controlling factors including model parameters and forcings in the application example we demonstrate the capabilities of ggsm by evaluating morris elementary effects sobol total order effects and vars total variogram effects for the parameters of the hbv sask hydrologic model applied to the oldman watershed in canada this case study illustrates how the star vars algorithm coupled with the ggsm approach provides a computationally efficient vehicle for comparing gsa results provided by different methods and how it enables learning about the dynamically varying parameter and process importance of a dynamical system model our implementation of the ggsm approach within the vars tool software toolbox razavi et al 2019 provides the user with a comprehensive efficient and robust tool for global sensitivity analysis and provides a unified framework for ongoing research thereof both the model and the data set used in this paper are embedded within the vars tool software toolbox www vars tool com 2 generalized global sensitivity matrix approach to global sensitivity analysis 2 1 local and global sensitivity matrices in brief the gsa method developed by gupta and razavi 2018 proceeds as follows consider a desm driven by input sequence u t u t 1 u t d u and controlled by parameter set θ θ 1 θ n θ from initial state x 0 x 0 1 x 0 d x that gives rise to sequences of states x t x t 1 x t d x and output fluxes y t y t 1 y t d y from simulation time step t 1 to t here d u n θ d x and d y are the dimensions of the input parameter state and flux vectors respectively of interest is to analyze the sensitivity of states and or fluxes to parameters over time with no loss of generality the following mathematical development will be restricted to that of a specific model simulated flux y k y 1 k y t k and to its sensitivity to the model parameters θ this development is readily extended to any model generated response including but not limited to the modeled state variables x or to targeted or compressed quantities derived from x and or y and to other controlling factors forcings boundary and initial conditions etc note that these mathematical developments are also directly applicable to models without state variables that directly map inputs u t onto outputs y t the most basic way to characterize the local at a specific parameter location θ j sensitivity of y k θ to changes in θ is via the local sensitivity matrix θ j y k composed of the mathematical derivatives 1 θ j y k d y 1 k d θ 1 j d y t k d θ 1 j d y 1 k d θ n θ j d y t k d θ n θ j having n θ rows one for each parameter and t columns one for each time step where the index j indicates a specific location set of values for the parameters θ anywhere within the feasible space φ θ note that the values of the derivatives will in general vary with the location of θ j θ 1 j θ n θ j and so this matrix can be extended to a three dimensional global sensitivity matrix θ y k of dimension n p t s n θ t where n p t s is the number of sample locations this three dimensional matrix can be thought of as a set of n θ two dimensional n p t s t parameter specific global sensitivity matrices θ y k i one for each parameter θ i as 2 θ y k i d y 1 k d θ i 1 d y t k d θ i 1 d y 1 k d θ i n p t s d y t k d θ i n p t s where the columns correspond to time and each row corresponds to one of the n p t s locations sampled across the feasible parameter space to characterize the relative global sensitivity of flux y k to local perturbations in the parameter θ i aggregate measures that quantify the relative sizes of the θ y k i matrices are derived by analyzing the frequency distributions p 1 t 1 n p t s d y t k d θ i of the n p t s t component values of each θ y k i matrix more specifically when concerned primarily with the strengths of relative sensitivity i e if the signs of the derivatives are not considered important we can use instead the distributions p 1 t 1 n p t s d y t k d θ i of their absolute values on the basis of these frequency distributions derivative based time aggregate and time varying indices for global sensitivity are derived for more detailed discussion see gupta and razavi 2018 2 2 generalized global sensitivity matrix approach the gsm approach described briefly above can be generalized to be compatible with any existing gsa approach as follows as discussed by razavi and gupta 2015 any sensitivity analysis is a study of how some selected system model response r c varies as its different controlling factors c c 1 c n c are perturbed where n c is the number of factors for purposes of presentation and without loss of generality we hereafter take c to be the model parameters θ θ 1 θ n θ first consider the fact that the response r can be defined either as some time aggregate measure of overall model performance thereby referring to some observed system response data as the spatio temporally varying model generated state flux response itself not requiring reference to observed system response data or as some targeted compressed or direct attribute of the latter i e one of the four response types discussed in section 1 next note that the method of analysis can be either derivative based direct response based or variogram based accordingly various combinations of response and method of analysis can be used to carry out the sensitivity analysis in gupta and razavi 2018 we developed the gsm approach based on a derivative based method using finite difference approximations for characterizing sensitivity applied directly to a temporally varying model generated state flux response further for a specific model simulated flux k and each parameter θ i we proposed the use of statistical means μ i k t of the frequency distributions p 1 n p t s d y t k d θ i of the absolute values of the sensitivity coefficients as the sensitivity indices indicating relative parameter importance thereby obtaining an n θ t matrix of time varying sensitivity indices 3 s k s k 1 s k t where each column s k t s 1 k t s n θ k t is a vector of n θ sensitivity indices one for each parameter computed for the t t h time step more generally when the signs of the derivatives are considered to be important we can use instead the frequency distributions p 1 n p t s d y t k d θ i and separately compute the means μ i k t and μ i k t of the parts of the distributions corresponding to positive and negative derivatives respectively in this case there being two statistical quantities of interest s k t becomes a n θ 2 matrix this concept is easily extended as follows in the case that the response varies in three dimensional x y z space each column s k t then becomes an n θ n g matrix of sensitivity indices where n g is the number of spatial locations e g grid points at which the response is being investigated alternatively an n θ n x n y n z matrix where n x n y and n z indicate the number of grid points in each of the x y and z directions further whereas in gupta and razavi 2018 we specifically select the mean to be the appropriate summary statistic due to the typically exponential nature of p 1 n p t s d y t k d θ i frequency distributions we also mention therein that s i k t can be any relevant statistical property of those distributions in general the form of the model response distributions is governed by the combined effects of the distributions of forcings and the model dynamical behaviour accordingly one could instead employ one or more higher order moments variance skewness kurtosis etc as proposed by dell oca et al 2017 this can be relevant and useful in cases where other aspects of the shape of the distribution are important rather than or in addition to its mean similarly one could instead use some other property that characterizes the overall shape extent of the frequency distribution such as its entropy h i k t so if we choose to use several complementary statistical measures e g mean variance skewness kurtosis etc to characterize different aspects of the sensitivity of the selected response each column s k t becomes an n θ n i matrix where n i indicates the number of such statistical measures consider now the case where we prefer to use a direct response based method of analysis again applied directly to the temporally varying model generated state flux response in this case instead of the frequency distributions of the derivatives we focus on the sensitivity information encapsulated in changes to the frequency distributions p 1 n p t s y t k when one or more parameters is no longer permitted to vary across its feasible range i e we focus on the change from the unconditional distribution p 1 n p t s y t k to the conditional distribution p 1 n p t s y t k θ i f i x e d given that θ i can generally be fixed at any arbitrary value within its feasible range we typically apply this approach by evaluating the average change across all possible fixed values this strategy forms the basis for the sobol approach wherein the statistical property used to characterize the nature of the unconditional and conditional distributions is their variance typically the variance change is divided by the variance of the unconditional distribution thereby obtaining a normalized measure of sensitivity so for example we can use s i k t v k t v i k t v k t where v k t is the variance of the unconditional distribution p 1 n p t s y t k and v i k t is the average variance of the conditional distribution p 1 n p t s y t k θ i f i x e d where averaging is done over all possible fixed values of θ i i e the uncertainty in θ i is integrated out this then gives us the so called first order sobol sensitivity index however following dell oca et al 2017 other statistical moments can also or instead be used in place of the variance as for example when the main effects of conditioning are realized in the tails of the distributions in contrast pianosi and wagener 2015 proposed the use of a moment free approach based on differences between the cumulative distribution functions and using for example the kolmogorov smirnov distance between unconditional and conditional distributions to characterize sensitivity more generally a moment free approach can be based on the divergence between the two distributions using either the shannon entropy e g krykacz hausmann 2001 or the kullback leibler divergence park and ahn 1994 alternatively the method of analysis can instead be based on properties of the directional variograms constructed from the model responses as in the vars approach razavi and gupta 2016a b in the examples above we have focused attention directly on every time step of the temporally varying model generated state flux response it should be easy to see that a similar analysis can instead be conducted for distinct or moving window time aggregated portions of the total simulation period for example if the model is run for several years using a daily time step the analysis can focus on average sensitivity properties at the weekly monthly seasonal annual and or total period time scales alternatively one could focus on driven and non driven portions of the model response e g boyle et al 2000 or wet and dry seasons or day time and night time e g rosolem et al 2013 etc for example gupta and razavi 2018 demonstrate the use of time aggregate sensitivity analysis at the yearly time scale to show how annual climatic variations affect parameter importance in such cases we instead obtain an n θ n p matrix of sensitivity indices 4 s k s k 1 s k n p where n p is the number of time periods under investigation in the extreme case n p 1 and we obtain a total period time aggregate sensitivity analysis similarly one could instead focus on a set of targeted b type aspects of model response or a compressed set of c type diagnostic signature properties in which cases the dimension of the sensitivity matrix will vary accordingly finally although we do not recommend it except in the case where the intended focus is an identifiability analysis see gupta and razavi 2018 the response can be selected to be some goodness of fit measure of overall model performance a type computed in aggregate fashion over the entire time period or over distinct temporal sub periods or over moving windows or for targeted events or in regards to some set of compressed diagnostic signature properties of the system response in all of these cases the method of analysis can be either derivative based or direct response based or variogram based to summarize ggsm can be formed by choosing any model response r and any gsa technique s and associated sensitivity index indices to populate s k t including but not limited to the derivative based elementary effects campolongo et al 2007 morris 1991 rakovec et al 2014 variance based total order effects saltelli et al 2008 sobol 2001 variogram based ivars indices such as total variogram effects or ivars50 razavi and gupta 2016a 2016b razavi et al 2019 density based indices dell oca et al 2017 pianosi and wagener 2015 regression based indices kleijnen 1995 sieber and uhlenbrook 2005 and indices based on monte carlo filtering also commonly referred to as regional sensitivity analysis hamby 1994 spear et al 1994 2 3 time normalization of sensitivity when investigating time aggregate total period and time varying sensitivity indices as discussed above we have treated each individual time step as being of equal importance however during some time steps the responses of a desm may exhibit more variability and hence sensitivity than during other time steps reflecting the dynamics of the model and importantly the strength of its forcings in such cases the behaviour of the more dynamically active time steps may when summarized into a single time aggregate sensitivity index obscure the information contained in less active time steps for some purposes therefore it may be desirable to adjust the weights assigned to different time steps to achieve a more desirable balance for such circumstances we suggest normalizing the vectors s k t individually for each time step from t 1 to t possible ways to do this include 1 adjusting each vector such that its vector length becomes unity or 2 dividing each element s i k t of the vector by the sum of all the elements of that vector i e s i k t s n θ k t so that the sum of all the normalized elements becomes unity both procedures are implemented in vars tool razavi et al 2019 www vars tool com for the illustrative experiments and results shown in this paper we use the second procedure each row of the normalized ggsm represents the normalized time varying sensitivity and the time normalized aggregate sensitivity index can be obtained by application of eqn 4 to the normalized matrix in this context we note that the sensitivity indices computed by some approaches are essentially already normalized by definition for example sobol variance based indices main first order effects interaction higher order effects and total order effects are defined as the ratio of the variance contribution of each parameter or a collection of parameters to the total variance in such cases the respective time varying sensitivity indices are already time normalized and additional normalization may not be necessary 3 implementation of the generalized global sensitivity matrix approach within vars tool the ggsm approach has been implemented within vars tool a comprehensive software toolbox for global sensitivity analysis razavi et al 2019 a particular feature of this implementation is the star vars algorithm razavi and gupta 2016b which facilitates simultaneous computation of derivative based sensitivity indices e g morris absolute elementary effects termed vars abe variance based sensitivity indices e g sobol total order effects termed vars to and variogram based sensitivity indices e g vars total variogram effects termed ivars50 each time varying index can be computed for each time step normalized if desired and time aggregated to obtain a total period summary index if desired the indices can also be computed for a type model responses see section 1 for which observed data are available using any performance metric of interest e g see haghnegahdar et al 2017 accordingly a single run of the ggsm approach i e a single set of representative model runs sampled throughout the factor space can be used to generate a wide variety of different sensitivity indices representing different gsa strategies that have been proposed in the literature for the 12 parameter modelling results presented in this paper the star vars sampling strategy was implemented with a resolution 0 01 step size equal to 1 of the parameter range and a number of star centers 10 the star centers were generated using progressive latin hypercube sampling plhs sheikholeslami and razavi 2017 that preserves the distributional properties of the sample as additional sample points are progressively collected 4 case study and results 4 1 the hydrologic model and data to demonstrate the ggsm approach we use the hbv sask hydrologic model fig 1 and the oldman watershed case study embedded within vars tool the 1434 73 km2 oldman watershed is located in the rocky mountains of alberta canada historical data is available for the period 1979 2008 from which we estimate average annual precipitation rainfall snowfall to be 611 mm and average annual streamflow to be 11 7 m3 s at gauge 05aa023 on the oldman river runoff ratio 0 42 further details of the case study are available in razavi et al 2019 and the vars tool users manual the parameters of the model and their uncertainty ranges are presented in table 1 the last parameter listed in the table is a precipitation multiplier pm that helps to account for bias in the precipitation estimates our goal is to characterize how the uncertainty in each of these parameters contributes in a relative sense to uncertainty in the model state here soil moisture storage and output fluxes here streamflow and evapotranspiration for this demonstration we compare three gsa approaches morris sobol and vars for each approach a separate ggsm and its normalized version is generated for each of the three response variables 4 2 time varying gsa results we discuss here time varying gsa results based on the following gsa indices ivars50 vars total variogram effect vars abe morris derivative based absolute elementary effect and vars to sobol variance based total order effect all these indices were computed from the same simulated model responses generated using the same set of parameter samples through the star vars algorithm generating all of the different indices at once enables us to efficiently compare and contrast them in the following discussion we begin with ivars50 as a comprehensive index that integrates both derivative based and variance based information and then compare those results with the assessments provided by vars abe and vars to fig 2 subplots b c d shows the daily time variation of ivars50 i e total variogram effect of simulated streamflow and evapotranspiration output fluxes and soil moisture storage state variable for four of the parameters along a representative one year period the topmost subplot a shows the corresponding hydro meteorological trajectories of observed precipitation temperature and streamflow it can be seen that a the sensitivity index trajectories for simulated streamflow and evapotranspiration corresponding to parameters tt etf and pm vary strongly with time showing relatively large sensitivity to variability in the forcings particularly to precipitation as might be expected b the trajectory of the sensitivity index of simulated streamflow to parameter k2 is relatively smooth due to the fact that this parameter controls the baseflow process which is less sensitive to high frequency variations in the forcings c evapotranspiration is as it should be fully insensitive to all of the parameters when the temperature falls below some certain threshold and the evapotranspiration process becomes inactive d the trajectories of the sensitivity indices of simulated soil moisture storage to parameters tt etf and pm also change relatively smoothly consistent with the function of the soil storage process which has a relatively longer term memory within the system this is also observable in the sample time series of sm shown in fig 1 e parameter k2 has no influence on either evapotranspiration or soil moisture which makes sense given that this parameter controls only the horizontal routing processes overall it is clear that the relative importance of different parameters varies with time in a manner that is reflective of the dynamics of the system and the strength of the forcings in particular we see that parameter pm precipitation multiplier has the strongest influence on all three response variables simulated streamflow evapotranspiration and soil moisture consistent with its role as the main driver of a system that is not heavily damped fig 3 shows similar daily time variations of vars abe morris derivative based elementary effect and vars to sobol variance based total order effect for simulated streamflow and evapotranspiration output fluxes and soil moisture storage state variable with respect to the same four out of 12 parameters to ensure consistency in comparisons of the three gsa indices for vars to we show at each time step the multiplication of actual vars to which is a normalized index see section 2 3 by the total variance of model response at that time step our assessment is outlined below a a comparison of figs 2 and 3 shows that for each parameter the three gsa methods provide indices that generally follow similar patterns over time these patterns are primary driven by the forcings and in particular by precipitation b a closer look at the parameter rankings at different time steps show some differences with as expected the differences between vars abe derivative based and vars to variance based being more pronounced for example at day 2006 07 15 vars abeq indicates a parameter ranking of pm k2 etf tt from most strongly to most weakly influential whereas vars toq indicates a ranking of pm tt etf k2 for this time step ivars 50 q agrees with vars toq except that it assesses parameters etf and k2 as being of almost equal influence c the timing and dynamics of change in indices provided by the three gsa methods are somewhat different for example vars abeq derivative based for parameter k2 remains relatively constant with time while the vars toq variance based and ivars 50 q variogram based are more variable d for soil moisture storage all three methods provide indices that vary smoothly and similarly with time with some difference in timing for example according to vars abesm the relative rankings of parameters tt and etf are reversed at about the date 2006 1 14 whereas vars tosm and ivars 50 q indicate that a similar reversal occurs about two weeks later 4 3 time aggregate gsa results fig 4 shows time aggregate 1982 2008 cumulative frequency distributions for the ivars50 vars abe and vars to indices computed for simulated streamflow evapotranspiration and soil moisture storage with respect to all 12 parameters in general the distributions are exponential like and ones that are more extended to the right correspond to parameters that are more strongly influential note that the cumulative frequency distributions for evapotranspiration start at values greater than zero due to the fact that the evapotranspiration process is inactive during a significant portion of the year cumulative distributions that stochastically dominate i e are distinctly to the right of the others correspond to parameters with higher time aggregate sensitivities for example etf temperature anomaly correction of potential evapotranspiration and fc field capacity are clearly the most influential parameters with respect to simulated evapotranspiration and soil moisture respectively based on all the three indices fig 4 further demonstrates the similarities and differences between the assessments provided by the three gsa methods this can be assessed based on the position of a parameter s cumulative distribution curve with respect to the others and how e g at what frequency they cross each other for example a ivars50 plot a indicates that parameter tt is the absolutely dominant parameter for streamflow in about 35 percent of time steps in contrast according to vars abe plot d and vars to plot g tt is dominant but only marginally for streamflow in only 10 percent and 20 percent of the time steps respectively further based on vars abe parameter tt competes with c0 for dominance while based on vars to it competes with pm b consider the effect of parameter c0 on soil moisture storage the plots in the right column based on both ivars50 and vars to the position of this parameter s cumulative frequency distribution with respect to the others tends to be similar indicating that c0 is a relatively uninfluential parameter however for vars abe the distribution is further to the right suggesting that c0 is more influential than all but two of the parameters in almost 100 percent of the time steps fig 5 compares the time aggregate indices provided by the three gsa approaches at the aggregate level the approaches agree on the relative importance of some parameters while disagreeing on others for example a ivars50 suggests that tt has the strongest influence on q whereas vars abe and vars to favor c0 and pm respectively b all three methods agree that etf exerts the strongest influence on et that fc exerts the strongest influence on sm and that frac k1 alpha k2 and ubas have no influence on either et or sm c as expected different parameters are seen to control different model outputs so while ivars50 indicates that tt has by far the strongest influence on q it also has the weakest influence on et ignoring parameters that have zero influence d also as seems reasonable all three methods indicate that the precipitation multiplier pm exerts a strong influence on all three responses q et and sm in keeping with the dominant role of precipitation as the primary driver of the system comparing the right and left columns of the subplots in fig 5 we see that time step normalization somewhat alters the sensitivity assessments for certain parameters in this regard the most obvious example is that parameter pm replaces tt as the most influential parameter based on ivars50 however the overall effect of time normalization on the relative influences of the parameters on et and sm is seen to be negligible for this particular case study see gupta and razavi 2018 for some more dramatic effects 5 discussion the ggsm approach coupled with the star vars sampling method can enable perhaps the most comprehensive assessment of information about the sensitivity behaviour of a model because 1 it can work with all the four possible types a d of model responses used in the literature as reviewed in section 1 and gupta and razavi 2018 including the most detailed spatio temporally varying sensitivities of state flux responses d type and 2 it can simultaneously generate from a single sample a wide range of sensitivity indices based on derivative based variance based and variogram based approaches while in theory this wealth of information provided by the ggsm approach should prove useful in practice one may ask the following two questions 1 how should the user interpret and make use of the substantial amount of information contained in the ggsm particularly in the case of d type responses that may vary on temporal or spatio temporal domains 2 how should the user reconcile and make use of the sensitivity information provided by the different derivative variance and variogram based approaches in regards to the first question the comprehensive nature of the information provided by the ggsm approach can support users with a range of needs and backgrounds in modelling from beginner to advanced the case study presented in section 4 was designed to exemplify two rather extreme cases for example figs 2 and 3 showed how the information provided by time varying indices can be used to assess how the model behaves over time while fig 5 showed how all of that information can be summarized into time aggregate indices in between the above two extremes one can consider different degrees of compression of the information content as described in section 2 for example by aggregating the information at different time or space time scales driven versus non driven portions of the model response or wet versus dry seasons etc in general the user can come up with different ways to do this depending on the problem at hand as another example fig 4 showed how the ggsm information can be summarized into a single cdf plot that demonstrates and compares the importance of parameters regardless of the dynamics and timings in regards to the second question as discussed by razavi and gupta 2015 different approaches to gsa are based in different theories and philosophies and therefore may lead to conflicting assessments of sensitivity specifically the derivative based and variance based approaches focus on two distinct properties of response surfaces while the variogram based approach acts as a unifying theory that bridges the two and supplements them with a spectrum of extra information razavi and gupta 2016a therefore while advanced users are encouraged to take advantage of the full spectrum of information provided by the vars approach they may instead choose to focus only on total variogram effects ivars50 as being the most comprehensive index of global sensitivity in a follow up paper we will revisit this question and try to further explain how the different gsa theories can map to the purpose of a study 6 concluding remarks in this paper we formalized a general approach to multi method global sensitivity analysis that accounts for the temporal dynamics of earth and environmental systems models while enabling efficient comparison of the results provided by different philosophical approaches specifically the approach helps to address some issues and shortcomings raised by gupta and razavi 2018 by implementing within a single framework several commonly used approaches to gsa including previous informal uses of time and spatially varying sensitivity matrices we conclude with the following remarks the generalized global sensitivity matrix ggsm approach presented here accounts for the dynamical nature of desms and can be used with any gsa method and any model response the approach generates both time varying and total period time aggregate assessments of relative sensitivity time varying gsa enables the user to better understand the model behaviour over time in response to system forcings this capability can be used to facilitate a diagnostic testing and detection of potential defects in different parts of a model thereby helping to improve model realism and b attribution of variability and therefore uncertainty in a model response to different factors such as model parameters forcings and boundary conditions thereby helping to pinpoint the dominant controls of predictive uncertainty at different points in time the latter can provide insight into what sources of uncertainty control the uncertainty in a model prediction at different times and under different forcing conditions when a summary assessment is required the total period time aggregate gsa indices provide information regarding the overall global sensitivity of a model response with respect to its controlling factors the ggsm approach can be applied to any type of model response types a d as explained in section 1 including those with no measurements available it can be used to assess the internal functioning of a model and the controls exerted by different factors on any of its components including both state and fluxes this is important because 1 gsa is not limited to response variables for which observations are available and 2 the assessments cannot be obscured or distorted by errors and uncertainties in the response data within a single run i e using a single sample set of the star vars implementation the ggsm approach can generate a range of time varying and time aggregate gsa indices based on a variety of approaches to global sensitivity analysis including but not limited to the derivative based e g morris elementary effects moment based e g sobol variance based total order effects and variogram based e g vars total variogram effects this capability enables a user to explore compare and contrast the assessments provided by a variety of different approaches to gsa the ggsm approach has been implemented in the vars tool toolbox available free of charge for research purposes at www vars tool com or from the first author upon request in conclusion razavi and gupta 2015 discuss the fact that different approaches to gsa are based on different theories and philosophies because of which practitioners often encounter conflicting assessments of sensitivity when applying different approaches to the same problem practitioners can therefore either a take a multi method approach to exploring alternative gsa assessments to arrive at an overall conclusion or b base their assessment on a unifying theory that places different approaches within an encompassing framework also see razavi 2017 coupled together the vars theoretical framework provided by razavi and gupta 2016a the global sensitivity matrix gsm approach developed by gupta and razavi 2018 its extension the ggsm approach developed in this paper the progressive latin hypercube sampling plhs sheikholeslami and razavi 2017 and the implementation of the star vars sampling method razavi and gupta 2016b within the vars tool software package razavi et al 2019 provide an efficient vehicle for achieving both a and b above although to remain focused on the general framework this paper has not discussed the efficiency benefits of plhs implementation or the use of bootstrapping to obtain assessments of robustness reliability also implemented within vars tool the reader can find detailed discussions of the first in sheikholeslami and razavi 2017 and of the second in razavi and gupta 2016b and sheikholeslami et al 2018 as always we invite discussion and collaboration on these and other issues related to diagnostic evaluation and improvement of desms especially with regard to high dimensional representations of complex systems acknowledgements the first author was supported in part by his nserc discovery grant and global water futures gwf integrated modelling program for canada impc funded by canada first research excellence fund cfref the second author received partial support from the australian research council through the centre of excellence for climate system science grant ce110001028 
26247,many applications of global sensitivity analysis gsa do not adequately account for the dynamical nature of earth and environmental systems models gupta and razavi 2018 highlight this fact and develop a sensitivity analysis framework from first principles based on the sensitivity information contained in trajectories of partial derivatives of the dynamical model responses with respect to controlling factors here we extend and generalize that framework to accommodate any gsa philosophy including derivative based approaches such as morris and delsa direct response based approaches such as the variance based sobol distribution based pawn and higher moment based methods and unifying variogram based approaches such as vars the framework is implemented within the vars tool software toolbox and demonstrated using the hbv sask model applied to the oldman watershed canada this enables a comprehensive multi variate investigation of the influence of parameters and forcings on different modeled state variables and responses without the need for observational data regarding those responses keywords global sensitivity analysis time varying sensitivity analysis parameter importance uncertainty variogram analysis of response surfaces vars sobol morris progressive latin hypercube sampling plhs dynamical systems models performance metrics sensitivity indices software availability the ggsm method and case study presented in this paper are embedded in the vars tool software package available at www vars tool com 1 introduction and background advanced dynamical earth systems models desms are commonly used to simulate the spatio temporal behaviors of natural and engineered processes over large domains bennett et al 2013 haghnegahdar et al 2017 their value derives from their capacity to help us understand how the behaviors and functioning of such systems vary dynamically with time in response to the interactions between complex internal dynamics and external forcings oreskes 2003 yassin et al 2017 to be able to diagnostically improve the predictive abilities of such models it is important to understand the dynamics underlying such behaviors and the extent to which different factors e g model parameters forcings and boundary and initial conditions exert controls on those behaviors over time razavi and gupta 2015 in recognition of this importance a variety of approaches and tools for so called global sensitivity analysis gsa have been developed most available gsa approaches fall under the two general categories of derivative based and direct response based e g variance based higher moment based or distribution based the former globalizes the concept of local sensitivity based on partial derivatives of some selected model response campolongo et al 2007 morris 1991 sobol and kucherenko 2009 whereas the latter measures the component contribution of different factors to the overall global frequency distribution of the response itself the most well known of these being the variance based sobol method saltelli et al 2008 sobol 2001 recently razavi and gupta 2016a 2016b developed a more general variogram based approach that bridges across and unifies the derivative based and variance based approaches by exploiting variograms cressie 1993 to account for the covariance in the factor space structure of the response and to study variability as a function of perturbation scale haghnegahdar and razavi 2017 importantly razavi and gupta 2016a 2016b showed that the variogram analysis of response surfaces vars approach is computationally more efficient than the derivative based and variance based methods while providing deeper insight into the global sensitivity of model response to uncertainties in and perturbations of various controlling factors in general gsa is a systems theoretic approach to characterizing the sensitivity of one or more model responses to different controlling but uncertain factors e g model parameters forcings and boundary and initial conditions saltelli et al 2008 razavi and gupta 2015 as discussed by gupta and razavi 2018 depending on the application model responses are typically selected to be either a one or more model performance metrics that quantify the closeness of the desm dynamic state flux responses to observed data b a specific targeted aspect of those state flux responses c a compressed set of properties e g signatures that characterize those state flux responses or d the spatio temporally varying state flux responses themselves use of the a type responses performance metrics is informative mainly in a model calibration context where typically the metrics provide spatio temporally aggregate measures of the model s ability to reproduce observations during some calibration period e g borgonovo et al 2017 haghnegahdar et al 2017 rosolem et al 2012 van werkhoven et al 2008 some studies have extended this aggregate metric approach to account for systems dynamics and time varying model performance by applying a moving window approach to the computation of the metric e g cibin et al 2010 herman et al 2013 pianosi and wagener 2016 unlike the metric based approaches methods that use other types of responses b d do not require the availability of corresponding data targeted b type responses are often used in a decision context for example flow peaks may be targeted when concerned with flood protection gsa studies using b type responses include van griensven et al 2006 who studied average catchment outflow over a period of time and savage et al 2016 who studied spatio temporally averaged maximum water depth compressed c type responses extend this targeted approach to investigate characteristic signature properties of the overall dynamical spatio temporal state flux response in this context a variety of methods both data driven campbell et al 2006 lamboni et al 2009 2011 marrel et al 2011 and physics driven gupta et al 2008 yilmaz et al 2008 have been used to compress the spatio temporal state flux responses of a model into sets of characteristic signature properties that are of a diagnostic contextually meaningful nature clearly the d type responses provide the most detailed spatio temporally varying information about the state flux behaviour of the model for example assessment of the time dependent sensitivity of model simulated streamflow can enable study of how model component process dominance varies over time and under different conditions guse et al 2014 reusser et al 2011 similarly one can study the sensitivity of spatial flood maps to different sources of uncertainty abily et al 2016 gsa based on the d type model responses is in principle particularly useful when we wish to better understand the dynamical behaviour of a desm further discussion of the use of these four different types of model responses in gsa appears in gupta and razavi 2018 in gupta and razavi 2018 we revisited the fundamental basis of gsa for dynamical systems models considering the four different model response types described above in particular we raised serious concerns about the use of performance metrics a type responses for gsa of desms we showed that an approach based on performance metrics is fundamentally inconsistent and incomplete is more correctly viewed as a form of model identifiability analysis rather than an analysis of sensitivity and that use of a performance metric to assess sensitivity unavoidably distorts the information provided by the model about relative parameter importance consequently we concluded that 1 it is a serious conceptual flaw to interpret the results of performance metric based analyses as being consistent and accurate indications of the importance of model parameters and 2 given that such approaches depend on availability of system state flux output observational data they are unable to provide an assessment of the sensitivity of responses for which such data are not available and so the analysis they provide is necessarily incomplete to address these issues and in keeping with our position that the most important goal of gsa is to gain better understanding of a model and the underlying system in gupta and razavi 2018 we framed the gsa problem from first principles starting from the theoretical basis for sensitivity the magnitudes and signs of the partial derivatives of model output trajectories with respect to their controlling factors based on those principles we then developed a global sensitivity matrix gsm approach that generates globalized derivative based parameter importance indices that account for the dynamical time varying nature of sensitivity of desms here we build upon gupta and razavi 2018 and previous work on gsa with d type responses and formalize a generalized global sensitivity matrix ggsm approach that can a be used with any gsa approach including derivative based direct response based such as variance based or more generally moment based and distribution based and variogram based methods b be applied to model responses of any type including a type if desired and justified in a calibration context and c be used to provide a multi method assessment of both time aggregate and time varying parameter importance within a single gsa experiment using the same set of parameter samples when coupled with the star vars algorithm razavi and gupta 2016b further we discuss the need for time normalization of sensitivity assessment of desms to better account for the dynamics of models and importantly their forcings we show herein how ggsm can be designed and how it can be used to assess the sensitivity of different model responses i e internal fluxes output fluxes state variables to various controlling factors including model parameters and forcings in the application example we demonstrate the capabilities of ggsm by evaluating morris elementary effects sobol total order effects and vars total variogram effects for the parameters of the hbv sask hydrologic model applied to the oldman watershed in canada this case study illustrates how the star vars algorithm coupled with the ggsm approach provides a computationally efficient vehicle for comparing gsa results provided by different methods and how it enables learning about the dynamically varying parameter and process importance of a dynamical system model our implementation of the ggsm approach within the vars tool software toolbox razavi et al 2019 provides the user with a comprehensive efficient and robust tool for global sensitivity analysis and provides a unified framework for ongoing research thereof both the model and the data set used in this paper are embedded within the vars tool software toolbox www vars tool com 2 generalized global sensitivity matrix approach to global sensitivity analysis 2 1 local and global sensitivity matrices in brief the gsa method developed by gupta and razavi 2018 proceeds as follows consider a desm driven by input sequence u t u t 1 u t d u and controlled by parameter set θ θ 1 θ n θ from initial state x 0 x 0 1 x 0 d x that gives rise to sequences of states x t x t 1 x t d x and output fluxes y t y t 1 y t d y from simulation time step t 1 to t here d u n θ d x and d y are the dimensions of the input parameter state and flux vectors respectively of interest is to analyze the sensitivity of states and or fluxes to parameters over time with no loss of generality the following mathematical development will be restricted to that of a specific model simulated flux y k y 1 k y t k and to its sensitivity to the model parameters θ this development is readily extended to any model generated response including but not limited to the modeled state variables x or to targeted or compressed quantities derived from x and or y and to other controlling factors forcings boundary and initial conditions etc note that these mathematical developments are also directly applicable to models without state variables that directly map inputs u t onto outputs y t the most basic way to characterize the local at a specific parameter location θ j sensitivity of y k θ to changes in θ is via the local sensitivity matrix θ j y k composed of the mathematical derivatives 1 θ j y k d y 1 k d θ 1 j d y t k d θ 1 j d y 1 k d θ n θ j d y t k d θ n θ j having n θ rows one for each parameter and t columns one for each time step where the index j indicates a specific location set of values for the parameters θ anywhere within the feasible space φ θ note that the values of the derivatives will in general vary with the location of θ j θ 1 j θ n θ j and so this matrix can be extended to a three dimensional global sensitivity matrix θ y k of dimension n p t s n θ t where n p t s is the number of sample locations this three dimensional matrix can be thought of as a set of n θ two dimensional n p t s t parameter specific global sensitivity matrices θ y k i one for each parameter θ i as 2 θ y k i d y 1 k d θ i 1 d y t k d θ i 1 d y 1 k d θ i n p t s d y t k d θ i n p t s where the columns correspond to time and each row corresponds to one of the n p t s locations sampled across the feasible parameter space to characterize the relative global sensitivity of flux y k to local perturbations in the parameter θ i aggregate measures that quantify the relative sizes of the θ y k i matrices are derived by analyzing the frequency distributions p 1 t 1 n p t s d y t k d θ i of the n p t s t component values of each θ y k i matrix more specifically when concerned primarily with the strengths of relative sensitivity i e if the signs of the derivatives are not considered important we can use instead the distributions p 1 t 1 n p t s d y t k d θ i of their absolute values on the basis of these frequency distributions derivative based time aggregate and time varying indices for global sensitivity are derived for more detailed discussion see gupta and razavi 2018 2 2 generalized global sensitivity matrix approach the gsm approach described briefly above can be generalized to be compatible with any existing gsa approach as follows as discussed by razavi and gupta 2015 any sensitivity analysis is a study of how some selected system model response r c varies as its different controlling factors c c 1 c n c are perturbed where n c is the number of factors for purposes of presentation and without loss of generality we hereafter take c to be the model parameters θ θ 1 θ n θ first consider the fact that the response r can be defined either as some time aggregate measure of overall model performance thereby referring to some observed system response data as the spatio temporally varying model generated state flux response itself not requiring reference to observed system response data or as some targeted compressed or direct attribute of the latter i e one of the four response types discussed in section 1 next note that the method of analysis can be either derivative based direct response based or variogram based accordingly various combinations of response and method of analysis can be used to carry out the sensitivity analysis in gupta and razavi 2018 we developed the gsm approach based on a derivative based method using finite difference approximations for characterizing sensitivity applied directly to a temporally varying model generated state flux response further for a specific model simulated flux k and each parameter θ i we proposed the use of statistical means μ i k t of the frequency distributions p 1 n p t s d y t k d θ i of the absolute values of the sensitivity coefficients as the sensitivity indices indicating relative parameter importance thereby obtaining an n θ t matrix of time varying sensitivity indices 3 s k s k 1 s k t where each column s k t s 1 k t s n θ k t is a vector of n θ sensitivity indices one for each parameter computed for the t t h time step more generally when the signs of the derivatives are considered to be important we can use instead the frequency distributions p 1 n p t s d y t k d θ i and separately compute the means μ i k t and μ i k t of the parts of the distributions corresponding to positive and negative derivatives respectively in this case there being two statistical quantities of interest s k t becomes a n θ 2 matrix this concept is easily extended as follows in the case that the response varies in three dimensional x y z space each column s k t then becomes an n θ n g matrix of sensitivity indices where n g is the number of spatial locations e g grid points at which the response is being investigated alternatively an n θ n x n y n z matrix where n x n y and n z indicate the number of grid points in each of the x y and z directions further whereas in gupta and razavi 2018 we specifically select the mean to be the appropriate summary statistic due to the typically exponential nature of p 1 n p t s d y t k d θ i frequency distributions we also mention therein that s i k t can be any relevant statistical property of those distributions in general the form of the model response distributions is governed by the combined effects of the distributions of forcings and the model dynamical behaviour accordingly one could instead employ one or more higher order moments variance skewness kurtosis etc as proposed by dell oca et al 2017 this can be relevant and useful in cases where other aspects of the shape of the distribution are important rather than or in addition to its mean similarly one could instead use some other property that characterizes the overall shape extent of the frequency distribution such as its entropy h i k t so if we choose to use several complementary statistical measures e g mean variance skewness kurtosis etc to characterize different aspects of the sensitivity of the selected response each column s k t becomes an n θ n i matrix where n i indicates the number of such statistical measures consider now the case where we prefer to use a direct response based method of analysis again applied directly to the temporally varying model generated state flux response in this case instead of the frequency distributions of the derivatives we focus on the sensitivity information encapsulated in changes to the frequency distributions p 1 n p t s y t k when one or more parameters is no longer permitted to vary across its feasible range i e we focus on the change from the unconditional distribution p 1 n p t s y t k to the conditional distribution p 1 n p t s y t k θ i f i x e d given that θ i can generally be fixed at any arbitrary value within its feasible range we typically apply this approach by evaluating the average change across all possible fixed values this strategy forms the basis for the sobol approach wherein the statistical property used to characterize the nature of the unconditional and conditional distributions is their variance typically the variance change is divided by the variance of the unconditional distribution thereby obtaining a normalized measure of sensitivity so for example we can use s i k t v k t v i k t v k t where v k t is the variance of the unconditional distribution p 1 n p t s y t k and v i k t is the average variance of the conditional distribution p 1 n p t s y t k θ i f i x e d where averaging is done over all possible fixed values of θ i i e the uncertainty in θ i is integrated out this then gives us the so called first order sobol sensitivity index however following dell oca et al 2017 other statistical moments can also or instead be used in place of the variance as for example when the main effects of conditioning are realized in the tails of the distributions in contrast pianosi and wagener 2015 proposed the use of a moment free approach based on differences between the cumulative distribution functions and using for example the kolmogorov smirnov distance between unconditional and conditional distributions to characterize sensitivity more generally a moment free approach can be based on the divergence between the two distributions using either the shannon entropy e g krykacz hausmann 2001 or the kullback leibler divergence park and ahn 1994 alternatively the method of analysis can instead be based on properties of the directional variograms constructed from the model responses as in the vars approach razavi and gupta 2016a b in the examples above we have focused attention directly on every time step of the temporally varying model generated state flux response it should be easy to see that a similar analysis can instead be conducted for distinct or moving window time aggregated portions of the total simulation period for example if the model is run for several years using a daily time step the analysis can focus on average sensitivity properties at the weekly monthly seasonal annual and or total period time scales alternatively one could focus on driven and non driven portions of the model response e g boyle et al 2000 or wet and dry seasons or day time and night time e g rosolem et al 2013 etc for example gupta and razavi 2018 demonstrate the use of time aggregate sensitivity analysis at the yearly time scale to show how annual climatic variations affect parameter importance in such cases we instead obtain an n θ n p matrix of sensitivity indices 4 s k s k 1 s k n p where n p is the number of time periods under investigation in the extreme case n p 1 and we obtain a total period time aggregate sensitivity analysis similarly one could instead focus on a set of targeted b type aspects of model response or a compressed set of c type diagnostic signature properties in which cases the dimension of the sensitivity matrix will vary accordingly finally although we do not recommend it except in the case where the intended focus is an identifiability analysis see gupta and razavi 2018 the response can be selected to be some goodness of fit measure of overall model performance a type computed in aggregate fashion over the entire time period or over distinct temporal sub periods or over moving windows or for targeted events or in regards to some set of compressed diagnostic signature properties of the system response in all of these cases the method of analysis can be either derivative based or direct response based or variogram based to summarize ggsm can be formed by choosing any model response r and any gsa technique s and associated sensitivity index indices to populate s k t including but not limited to the derivative based elementary effects campolongo et al 2007 morris 1991 rakovec et al 2014 variance based total order effects saltelli et al 2008 sobol 2001 variogram based ivars indices such as total variogram effects or ivars50 razavi and gupta 2016a 2016b razavi et al 2019 density based indices dell oca et al 2017 pianosi and wagener 2015 regression based indices kleijnen 1995 sieber and uhlenbrook 2005 and indices based on monte carlo filtering also commonly referred to as regional sensitivity analysis hamby 1994 spear et al 1994 2 3 time normalization of sensitivity when investigating time aggregate total period and time varying sensitivity indices as discussed above we have treated each individual time step as being of equal importance however during some time steps the responses of a desm may exhibit more variability and hence sensitivity than during other time steps reflecting the dynamics of the model and importantly the strength of its forcings in such cases the behaviour of the more dynamically active time steps may when summarized into a single time aggregate sensitivity index obscure the information contained in less active time steps for some purposes therefore it may be desirable to adjust the weights assigned to different time steps to achieve a more desirable balance for such circumstances we suggest normalizing the vectors s k t individually for each time step from t 1 to t possible ways to do this include 1 adjusting each vector such that its vector length becomes unity or 2 dividing each element s i k t of the vector by the sum of all the elements of that vector i e s i k t s n θ k t so that the sum of all the normalized elements becomes unity both procedures are implemented in vars tool razavi et al 2019 www vars tool com for the illustrative experiments and results shown in this paper we use the second procedure each row of the normalized ggsm represents the normalized time varying sensitivity and the time normalized aggregate sensitivity index can be obtained by application of eqn 4 to the normalized matrix in this context we note that the sensitivity indices computed by some approaches are essentially already normalized by definition for example sobol variance based indices main first order effects interaction higher order effects and total order effects are defined as the ratio of the variance contribution of each parameter or a collection of parameters to the total variance in such cases the respective time varying sensitivity indices are already time normalized and additional normalization may not be necessary 3 implementation of the generalized global sensitivity matrix approach within vars tool the ggsm approach has been implemented within vars tool a comprehensive software toolbox for global sensitivity analysis razavi et al 2019 a particular feature of this implementation is the star vars algorithm razavi and gupta 2016b which facilitates simultaneous computation of derivative based sensitivity indices e g morris absolute elementary effects termed vars abe variance based sensitivity indices e g sobol total order effects termed vars to and variogram based sensitivity indices e g vars total variogram effects termed ivars50 each time varying index can be computed for each time step normalized if desired and time aggregated to obtain a total period summary index if desired the indices can also be computed for a type model responses see section 1 for which observed data are available using any performance metric of interest e g see haghnegahdar et al 2017 accordingly a single run of the ggsm approach i e a single set of representative model runs sampled throughout the factor space can be used to generate a wide variety of different sensitivity indices representing different gsa strategies that have been proposed in the literature for the 12 parameter modelling results presented in this paper the star vars sampling strategy was implemented with a resolution 0 01 step size equal to 1 of the parameter range and a number of star centers 10 the star centers were generated using progressive latin hypercube sampling plhs sheikholeslami and razavi 2017 that preserves the distributional properties of the sample as additional sample points are progressively collected 4 case study and results 4 1 the hydrologic model and data to demonstrate the ggsm approach we use the hbv sask hydrologic model fig 1 and the oldman watershed case study embedded within vars tool the 1434 73 km2 oldman watershed is located in the rocky mountains of alberta canada historical data is available for the period 1979 2008 from which we estimate average annual precipitation rainfall snowfall to be 611 mm and average annual streamflow to be 11 7 m3 s at gauge 05aa023 on the oldman river runoff ratio 0 42 further details of the case study are available in razavi et al 2019 and the vars tool users manual the parameters of the model and their uncertainty ranges are presented in table 1 the last parameter listed in the table is a precipitation multiplier pm that helps to account for bias in the precipitation estimates our goal is to characterize how the uncertainty in each of these parameters contributes in a relative sense to uncertainty in the model state here soil moisture storage and output fluxes here streamflow and evapotranspiration for this demonstration we compare three gsa approaches morris sobol and vars for each approach a separate ggsm and its normalized version is generated for each of the three response variables 4 2 time varying gsa results we discuss here time varying gsa results based on the following gsa indices ivars50 vars total variogram effect vars abe morris derivative based absolute elementary effect and vars to sobol variance based total order effect all these indices were computed from the same simulated model responses generated using the same set of parameter samples through the star vars algorithm generating all of the different indices at once enables us to efficiently compare and contrast them in the following discussion we begin with ivars50 as a comprehensive index that integrates both derivative based and variance based information and then compare those results with the assessments provided by vars abe and vars to fig 2 subplots b c d shows the daily time variation of ivars50 i e total variogram effect of simulated streamflow and evapotranspiration output fluxes and soil moisture storage state variable for four of the parameters along a representative one year period the topmost subplot a shows the corresponding hydro meteorological trajectories of observed precipitation temperature and streamflow it can be seen that a the sensitivity index trajectories for simulated streamflow and evapotranspiration corresponding to parameters tt etf and pm vary strongly with time showing relatively large sensitivity to variability in the forcings particularly to precipitation as might be expected b the trajectory of the sensitivity index of simulated streamflow to parameter k2 is relatively smooth due to the fact that this parameter controls the baseflow process which is less sensitive to high frequency variations in the forcings c evapotranspiration is as it should be fully insensitive to all of the parameters when the temperature falls below some certain threshold and the evapotranspiration process becomes inactive d the trajectories of the sensitivity indices of simulated soil moisture storage to parameters tt etf and pm also change relatively smoothly consistent with the function of the soil storage process which has a relatively longer term memory within the system this is also observable in the sample time series of sm shown in fig 1 e parameter k2 has no influence on either evapotranspiration or soil moisture which makes sense given that this parameter controls only the horizontal routing processes overall it is clear that the relative importance of different parameters varies with time in a manner that is reflective of the dynamics of the system and the strength of the forcings in particular we see that parameter pm precipitation multiplier has the strongest influence on all three response variables simulated streamflow evapotranspiration and soil moisture consistent with its role as the main driver of a system that is not heavily damped fig 3 shows similar daily time variations of vars abe morris derivative based elementary effect and vars to sobol variance based total order effect for simulated streamflow and evapotranspiration output fluxes and soil moisture storage state variable with respect to the same four out of 12 parameters to ensure consistency in comparisons of the three gsa indices for vars to we show at each time step the multiplication of actual vars to which is a normalized index see section 2 3 by the total variance of model response at that time step our assessment is outlined below a a comparison of figs 2 and 3 shows that for each parameter the three gsa methods provide indices that generally follow similar patterns over time these patterns are primary driven by the forcings and in particular by precipitation b a closer look at the parameter rankings at different time steps show some differences with as expected the differences between vars abe derivative based and vars to variance based being more pronounced for example at day 2006 07 15 vars abeq indicates a parameter ranking of pm k2 etf tt from most strongly to most weakly influential whereas vars toq indicates a ranking of pm tt etf k2 for this time step ivars 50 q agrees with vars toq except that it assesses parameters etf and k2 as being of almost equal influence c the timing and dynamics of change in indices provided by the three gsa methods are somewhat different for example vars abeq derivative based for parameter k2 remains relatively constant with time while the vars toq variance based and ivars 50 q variogram based are more variable d for soil moisture storage all three methods provide indices that vary smoothly and similarly with time with some difference in timing for example according to vars abesm the relative rankings of parameters tt and etf are reversed at about the date 2006 1 14 whereas vars tosm and ivars 50 q indicate that a similar reversal occurs about two weeks later 4 3 time aggregate gsa results fig 4 shows time aggregate 1982 2008 cumulative frequency distributions for the ivars50 vars abe and vars to indices computed for simulated streamflow evapotranspiration and soil moisture storage with respect to all 12 parameters in general the distributions are exponential like and ones that are more extended to the right correspond to parameters that are more strongly influential note that the cumulative frequency distributions for evapotranspiration start at values greater than zero due to the fact that the evapotranspiration process is inactive during a significant portion of the year cumulative distributions that stochastically dominate i e are distinctly to the right of the others correspond to parameters with higher time aggregate sensitivities for example etf temperature anomaly correction of potential evapotranspiration and fc field capacity are clearly the most influential parameters with respect to simulated evapotranspiration and soil moisture respectively based on all the three indices fig 4 further demonstrates the similarities and differences between the assessments provided by the three gsa methods this can be assessed based on the position of a parameter s cumulative distribution curve with respect to the others and how e g at what frequency they cross each other for example a ivars50 plot a indicates that parameter tt is the absolutely dominant parameter for streamflow in about 35 percent of time steps in contrast according to vars abe plot d and vars to plot g tt is dominant but only marginally for streamflow in only 10 percent and 20 percent of the time steps respectively further based on vars abe parameter tt competes with c0 for dominance while based on vars to it competes with pm b consider the effect of parameter c0 on soil moisture storage the plots in the right column based on both ivars50 and vars to the position of this parameter s cumulative frequency distribution with respect to the others tends to be similar indicating that c0 is a relatively uninfluential parameter however for vars abe the distribution is further to the right suggesting that c0 is more influential than all but two of the parameters in almost 100 percent of the time steps fig 5 compares the time aggregate indices provided by the three gsa approaches at the aggregate level the approaches agree on the relative importance of some parameters while disagreeing on others for example a ivars50 suggests that tt has the strongest influence on q whereas vars abe and vars to favor c0 and pm respectively b all three methods agree that etf exerts the strongest influence on et that fc exerts the strongest influence on sm and that frac k1 alpha k2 and ubas have no influence on either et or sm c as expected different parameters are seen to control different model outputs so while ivars50 indicates that tt has by far the strongest influence on q it also has the weakest influence on et ignoring parameters that have zero influence d also as seems reasonable all three methods indicate that the precipitation multiplier pm exerts a strong influence on all three responses q et and sm in keeping with the dominant role of precipitation as the primary driver of the system comparing the right and left columns of the subplots in fig 5 we see that time step normalization somewhat alters the sensitivity assessments for certain parameters in this regard the most obvious example is that parameter pm replaces tt as the most influential parameter based on ivars50 however the overall effect of time normalization on the relative influences of the parameters on et and sm is seen to be negligible for this particular case study see gupta and razavi 2018 for some more dramatic effects 5 discussion the ggsm approach coupled with the star vars sampling method can enable perhaps the most comprehensive assessment of information about the sensitivity behaviour of a model because 1 it can work with all the four possible types a d of model responses used in the literature as reviewed in section 1 and gupta and razavi 2018 including the most detailed spatio temporally varying sensitivities of state flux responses d type and 2 it can simultaneously generate from a single sample a wide range of sensitivity indices based on derivative based variance based and variogram based approaches while in theory this wealth of information provided by the ggsm approach should prove useful in practice one may ask the following two questions 1 how should the user interpret and make use of the substantial amount of information contained in the ggsm particularly in the case of d type responses that may vary on temporal or spatio temporal domains 2 how should the user reconcile and make use of the sensitivity information provided by the different derivative variance and variogram based approaches in regards to the first question the comprehensive nature of the information provided by the ggsm approach can support users with a range of needs and backgrounds in modelling from beginner to advanced the case study presented in section 4 was designed to exemplify two rather extreme cases for example figs 2 and 3 showed how the information provided by time varying indices can be used to assess how the model behaves over time while fig 5 showed how all of that information can be summarized into time aggregate indices in between the above two extremes one can consider different degrees of compression of the information content as described in section 2 for example by aggregating the information at different time or space time scales driven versus non driven portions of the model response or wet versus dry seasons etc in general the user can come up with different ways to do this depending on the problem at hand as another example fig 4 showed how the ggsm information can be summarized into a single cdf plot that demonstrates and compares the importance of parameters regardless of the dynamics and timings in regards to the second question as discussed by razavi and gupta 2015 different approaches to gsa are based in different theories and philosophies and therefore may lead to conflicting assessments of sensitivity specifically the derivative based and variance based approaches focus on two distinct properties of response surfaces while the variogram based approach acts as a unifying theory that bridges the two and supplements them with a spectrum of extra information razavi and gupta 2016a therefore while advanced users are encouraged to take advantage of the full spectrum of information provided by the vars approach they may instead choose to focus only on total variogram effects ivars50 as being the most comprehensive index of global sensitivity in a follow up paper we will revisit this question and try to further explain how the different gsa theories can map to the purpose of a study 6 concluding remarks in this paper we formalized a general approach to multi method global sensitivity analysis that accounts for the temporal dynamics of earth and environmental systems models while enabling efficient comparison of the results provided by different philosophical approaches specifically the approach helps to address some issues and shortcomings raised by gupta and razavi 2018 by implementing within a single framework several commonly used approaches to gsa including previous informal uses of time and spatially varying sensitivity matrices we conclude with the following remarks the generalized global sensitivity matrix ggsm approach presented here accounts for the dynamical nature of desms and can be used with any gsa method and any model response the approach generates both time varying and total period time aggregate assessments of relative sensitivity time varying gsa enables the user to better understand the model behaviour over time in response to system forcings this capability can be used to facilitate a diagnostic testing and detection of potential defects in different parts of a model thereby helping to improve model realism and b attribution of variability and therefore uncertainty in a model response to different factors such as model parameters forcings and boundary conditions thereby helping to pinpoint the dominant controls of predictive uncertainty at different points in time the latter can provide insight into what sources of uncertainty control the uncertainty in a model prediction at different times and under different forcing conditions when a summary assessment is required the total period time aggregate gsa indices provide information regarding the overall global sensitivity of a model response with respect to its controlling factors the ggsm approach can be applied to any type of model response types a d as explained in section 1 including those with no measurements available it can be used to assess the internal functioning of a model and the controls exerted by different factors on any of its components including both state and fluxes this is important because 1 gsa is not limited to response variables for which observations are available and 2 the assessments cannot be obscured or distorted by errors and uncertainties in the response data within a single run i e using a single sample set of the star vars implementation the ggsm approach can generate a range of time varying and time aggregate gsa indices based on a variety of approaches to global sensitivity analysis including but not limited to the derivative based e g morris elementary effects moment based e g sobol variance based total order effects and variogram based e g vars total variogram effects this capability enables a user to explore compare and contrast the assessments provided by a variety of different approaches to gsa the ggsm approach has been implemented in the vars tool toolbox available free of charge for research purposes at www vars tool com or from the first author upon request in conclusion razavi and gupta 2015 discuss the fact that different approaches to gsa are based on different theories and philosophies because of which practitioners often encounter conflicting assessments of sensitivity when applying different approaches to the same problem practitioners can therefore either a take a multi method approach to exploring alternative gsa assessments to arrive at an overall conclusion or b base their assessment on a unifying theory that places different approaches within an encompassing framework also see razavi 2017 coupled together the vars theoretical framework provided by razavi and gupta 2016a the global sensitivity matrix gsm approach developed by gupta and razavi 2018 its extension the ggsm approach developed in this paper the progressive latin hypercube sampling plhs sheikholeslami and razavi 2017 and the implementation of the star vars sampling method razavi and gupta 2016b within the vars tool software package razavi et al 2019 provide an efficient vehicle for achieving both a and b above although to remain focused on the general framework this paper has not discussed the efficiency benefits of plhs implementation or the use of bootstrapping to obtain assessments of robustness reliability also implemented within vars tool the reader can find detailed discussions of the first in sheikholeslami and razavi 2017 and of the second in razavi and gupta 2016b and sheikholeslami et al 2018 as always we invite discussion and collaboration on these and other issues related to diagnostic evaluation and improvement of desms especially with regard to high dimensional representations of complex systems acknowledgements the first author was supported in part by his nserc discovery grant and global water futures gwf integrated modelling program for canada impc funded by canada first research excellence fund cfref the second author received partial support from the australian research council through the centre of excellence for climate system science grant ce110001028 
26248,rivers are among the ecosystems most sensitive to climate change whilst methods quantifying the impact and uncertainty of climate change on flow regime are well established the impact on hydroecological response is not well understood typically investigative methods are qualitative in nature or follow quantitative methods of limited scope whilst the effect of uncertainty is frequently minimised this paper proposes a coupled hydrological and hydroecological modelling framework to assess the impact of climate change on hydroecological response quantitatively the characterisation and reduction of modelling uncertainties was critical to the development of the framework the ability of the framework is illustrated through application to a case study river the river nar norfolk england using the ukcp09 probabilistic climate projections high emissions scenario sres a1f1 the results show that by the 2050s a reduction in instream biodiversity is virtually certain if future emissions follow the assumptions of sres a1f1 disruption to the natural low flow processes essential to ecosystem functioning is also indicated these findings highlight the importance of the framework in water resources adaptation particularly with respect to future environmental flows management keywords climate change impact coupled hydrological and hydroecological model modelling framework probabilistic climate change projections ukcp09 hydroecological impact uncertainty 1 introduction the global climate system is changing with changes to climatic behaviour mean and variability projected beyond the 21st century ipcc 2014 climate change is expected to amplify existing pressures on natural resources as well as create new ones ipcc 2012 amongst these freshwater is considered the most essential vörösmarty et al 2010 rivers and their ecosystems provide a diverse range of services upon which humans are dependent yeakley et al 2016 these include fresh water supply for human consumption hydro hazard regulation and water purification gilvear et al 2017 it is thus through freshwater resources particularly rivers that some of the most significant impacts of climate change will be felt ostfeld et al 2012 consequently there are significant questions over the long term sustainability of water resources gleick 1998 2016 klaar et al 2014 it is clear that effective water management is central to successful climate change adaption ostfeld et al 2012 climate is a major determinant of hydrological processes where precipitation temperature and evaporation represent the dominant drivers ipcc 2007 consequently a changing climate will inevitably lead to alterations of river flow regimes rahel and olden 2008 arnell and gosling 2016 attempts to model the impact of climate change on water resources have been ongoing since the mid 1980s arnell and reynard 1996 christierson et al 2012 climate projections are however subject to large unquantified uncertainties murphy et al 2004 leading to concerns over their suitability for water resources adaptation and planning kundzewicz et al 2008 wilby 2016 examples of these uncertainties include clark et al 2016 wilby 2016 1 epistemic uncertainty the inability to properly capture the underlying processes and feedbacks and 2 accounting for variation due to natural climatic variability in practice uncertainty dictates the usefulness of the model inaccurate appreciation of this uncertainty precludes meaningful interpretation of the model leading to sub optimal decision making warmink et al 2010 when considering future projections clark et al 2016 posit that research which focuses on characterising reducing and representing quantifying these uncertainties may allow for the provision of plausible flow projections under climate change difficulties with regards to the quantification of climate uncertainty may be addressed through the use of a perturbed physics ensemble an ensemble of gcms where variation of the model parameters allows quantification of uncertainty murphy et al 2004 clark et al 2016 such enhanced projections have been available for the uk since 2009 through ukcp09 variability in the flow regime is widely acknowledged as the major determinant of ecological health in riverine ecosystems e g power et al 1995 lytle and poff 2004 arthington et al 2006 alteration of this natural flow regime threatens the ability to provide ecosystem services rahel and olden 2008 vörösmarty et al 2010 arthington 2012 despite this and perhaps surprisingly the effects of climate change on river health are rarely considered as observed in durance and ormerod 2007 seven years later schlabing et al 2014 describes little change noting that even when accounted for the methodology employed is often over simplified and rudimentary a brief review of such work performed over the last two decades is thus indicated see also fig 1 a large number of the studies investigating the impact of climate change on hydroecological response have been qualitative in nature fig 1 level 1 to 3 directly not pictured examples include meyer et al 1999 ostfeld et al 2012 filipe et al 2013 and death et al 2015 whilst the number of quantitative studies has increased their scope is often limited to the direct links between climate temperature and hydroecological response fig 1 for example durance and ormerod 2007 kupisch et al 2012 and jyväsjärvi et al 2015 in these studies the impact of the altered flow regime is not considered and rarely acknowledged döll and zhang 2010 were the first to consider the impact of climate change on the flow regime at a global scale assessment of the hydroecological impacts was qualitative in nature with limited consideration of changes in the number of endemic fish species counts are not considered meaningful bioindicators for example see li et al 2010 the authors acknowledge that quantitative estimates of ecosystem response have not yet been derived following this studies of a similar nature have been undertaken at higher resolutions catchment level examples include tang et al 2015 hassanzadeh et al 2017 and o keeffe et al 2018 advances have also been made in the assessment of direct climate change impacts on the provision of freshwater ecosystem services see conceptual framework in pham et al 2019 though again these are at present qualitative in nature merriam et al 2017 perform a habitat assessment using a coupled stream temperature and hydrological model whilst the focus is on the availability of thermally suitable habitats for brook trout and not flow alteration directly the study represents an important advancement towards fully quantitative assessment of the instream ecological impacts of climate change nevertheless significant questions arise as to the robustness of the applied methodology the authors make assertions using phraseology such as high degree of certainty when discussing results based upon r squared values and rmse a statistical measure of average inaccuracy yet the problems inherent to rmse have been recognised for some three decades willmott et al 1985 and more recently willmott and matsuura 2005 conclude that in the context of climate study model performance evaluations based primarily on rmse are questionable and should be reconsidered further issues arise in the calibration of the hydrologic model where performance is assessed in terms of nash sutcliffe a statistic subject to long standing broad and sustained criticism legates and mccabe 1999 seibert 2001 criss and winston 2008 indeed clark et al 2016 state that when modelling the hydrological impacts of climate change nash sutcliffe and similar efficiency criteria introduces additional unaccounted uncertainties this disregard of uncertainty throughout the paper calls into question the validity of the results it is clear that methods to quantify the impact and associated uncertainties of climate change on the flow regime are well established fig 1 level 1 and 2 the hydroecological implications are less well understood and are rarely considered quantitatively where attempts have been made the effect of uncertainty is underplayed consequently the impact of climate change on hydroecological response is unclear and the fallout for ecosystem services poorly understood this paper proposes a coupled hydrological and hydroecological modelling framework to assess the impact of climate change on hydroecological response quantitatively the development of each stage of the framework has centred around the characterisation and reduction of uncertainty in line with the recommendations in clark et al 2016 the outputs from this framework are quantitative hydroecological projections of climate change impacts these outputs are intended to support water resources adaptation for example in the equitable allocation of water for human use and the environment known as environmental flows in order to validate and demonstrate the ability of the framework this paper features an application to a case study river the river nar in norfolk england 2 framework an overview of the three main stages of the proposed framework is presented in fig 2 in stage 1 the hydroecological model is developed based on advances made in 1 visser et al 2017 where lag in ecological response an important component of flow variability monk et al 2017 is accounted for through the consideration of multi annual hydrological indicators and 2 visser et al 2018a present an information theory it approach to minimise and quantify structural and parameter uncertainty the second stage of the framework is the parameterisation of the hydroecological following a modified covariance approach visser et al 2018b the modified covariance approach focuses on the replication of specific hydrological characteristics identified in stage 1 whilst also addressing a number of known limitations and uncertainties in hydrological modelling in stage 3 climate projections serve as the input to the coupled model providing the quantitative hydroecological projections of climate change impacts application of the framework to a case study river catchment is subsequently considered in 3 case study application a holistic depiction of uncertainty was central to the development of the proposed framework additional commentary on the characterisation and reduction of the sources of uncertainty following clark et al 2016 is provided in appendix a in the development of this framework it is necessarily assumed that the hydroecological relationship remains stationary as in hydroclimatological modelling the evolution of such relationships remains an unknown and is beyond the scope of this paper a further important limiting factor of many hydroclimatological studies is the focus on extreme climatic events e g filipe et al 2013 thornton et al 2014 death et al 2015 climate models are well known for their ineffective simulation of extreme climate particularly with regards to precipitation ipcc 2014 knowledge of the impacts of extreme events is therefore limited in an effort to address this the ukcp09 climate projections distribution tails are clipped 5 and 95 probability levels murphy and sexton 2013 in addition changes in climate mean and variability may lead to compound events or clustered multiple events these events are not extreme in themselves but can lead to extreme events and or impacts ipcc 2012 essentially severe impacts can occur from minor climatic events the focus of the framework is therefore on these impacts rather than stochastic individual extreme events 2 1 stage 1 hydroecological model statistical methods are well established for the testing of hydroecological hypotheses these include multiple linear regression for example clarke and dunbar 2005 and monk et al 2007 and multi level models recent examples include bradley et al 2017 and chadd et al 2017 hydrological indicators his and ecological data serve as the basis for the development of these models with their sensitivity to change macroinvertebrates are ideal indicators of river health acreman et al 2008 ea 2013 this response is determined by considering macroinvertebrate flow velocity preferences as described by the lotic invertebrate index for flow evaluation life extence et al 1999 a weighted index which takes into account the flow velocity preferences of the macroinvertebrate community life scores can range from one to twelve indicating a preference for limited flow standing water to rapid flows respectively 2 1 1 data the structure of the benthic macroinvertebrate community is subject to change throughout the year typically peaks of activity occur in spring amj and autumn ond seasonal focus in hydroecological modelling is determined by factors such as the quantity and quality of the available data and the overall modelling objective for example fishing is vital to the communities along the case study river the river nar garbe et al 2016 if the goal was to preserve future brown trout populations then modelling efforts would seek to protect their primary food source ephemeroptera baetidae mayfly which hatch during the spring season macroinvertebrate data may be utilised at the species or family level however it should be noted that the use of family level data may mask species specific information monk et al 2012 leading to a reduction in accuracy extence et al 1999 a hydroecological dataset is created by pairing the ecological data with his these indicators should be ecologically relevant reflecting the five facets of the flow regime required to support the riverine ecosystem richter et al 1996 magnitude frequency duration timing and rate of change to date over 200 ecologically relevant hydrologic indices have been proposed olden and poff 2003 monk et al 2006 thompson et al 2013 should seasonality be present in the hydrological regime the time series is split into relevant hydrological seasons the his are then calculated for each a number of studies on groundwater fed rivers have observed a delay in macroinvertebrate response for example boulton 2003 durance and ormerod 2007 visser et al 2017 and visser et al 2018a propose the incorporation of time offset his to account for this effect the time offset may require fine tuning if the number of indicators cannot be sufficiently reduced in the steps below beyond this no additional work is required with a large number of his both variable redundancy and computational effort represent significant challenges in response principal component analysis pca is applied allowing only those indices which describe major aspects of the flow regime to be identified following olden and poff 2003 the most relevant indices are selected proportionally from the five facets of the flow regime described above the ecological data is then be paired with this set of ecologically relevant his 2 1 2 statistical modelling as further aspects of hydroecological relationships are understood such as ecological lag in response the likelihood of modelling errors and uncertainty is increased to account for this the proposed framework makes use of an it approach to determine the structure of the hydroecological model after visser et al 2018a the it approach provides a robust measure of both structural and parameter uncertainty see appendix a 1 as well as a measure of the statistical importance of the model parameters his a central factor in the parameterisation of the hydrological model stage 2 the application of the it approach consists of 4 steps for a more detailed discussion see appendix b 1 or visser et al 2018a to summarise 1 candidate models are evaluated with respect to the second order bias corrected akaike information criterion aicc equation b1 after burnham and anderson 2002 2 a best approximating model is inferred from a weighted combination of all the candidate models 3 the parameters are ranked such that the highest value akaike weight equation b3 represents the most important in the model 4 measures of uncertainty structural and parameter are made in the development and application of the framework the it approach was applied using the r package glmulti calcagno 2013 developed and applied in a relevant discipline see isbell et al 2011 in glmulti a genetic algorithm ga a type of optimisation that mimics biological evolution is used to select a subset of models each assessed based on the above it approach the ga incorporates an immigration operator allowing removed his to be reconsidered immigration sees the level of randomisation increase and hence the likelihood of model convergence on the global optima rather than some local optima calcagno and de mazancourt 2010 inference from a consensus of 5 replicate ga runs has been shown to greatly improve convergence calcagno and de mazancourt 2010 the multi model average is subsequently derived from this subset of models parameters where the estimate and confidence intervals are zero i e certainty that the index is not to be included are then removed in line with anderson 2007 the set of model parameters is reduced to those accounting for 95 of the cumulative information see appendix b 2 for validation of the hydroecological model see appendix c 2 2 2 stage 2 hydrological model the his identified in stage 1 represent those characteristics of the flow regime which dominate ecological response driven by climate projections fig 2 changes to these his may be determined from flow time series simulated via hydrological model climate projections input to the coupled hydrological and hydroecological model allow the impacts of climate change on hydroecological response to be determined quantitatively stage 3 this second stage of the proposed framework focusses on the parameterisation of the hydrological model clark et al 2016 highlight model parameterisation as a major source of uncertainty typically hydrological models are parameterised following a split sample calibration validation approach with calibration focussing on the goodness of fit between observed and simulated flow limitations of the approach are widely acknowledged these include westerberg et al 2011 clark et al 2016 1 bias in the model parametrisation as the result of disinformative data pelletier 1988 montanari et al 2013 2 the arbitrary nature of gof statistics and 3 equifinality beven 2006 in this proposed framework the modified covariance approach visser et al 2018b based on vogel and sankarasubramanian 2003 is applied in an attempt to address these limitations see also appendix a 1 in visser et al 2018b comparison relative to studies with similar modelling objectives the simulation of ecologically relevant his showed improvement in both model performance and consistency a further major advantage of the approach lies in the focus on identifying the region of parameter space which best captures the characteristics of the his providing a greater understanding of model suitability limitations and uncertainty 2 2 1 hydrological model to further minimise uncertainty a parsimonious lumped hydrological model should be selected in the development of the framework the daily models from the gr génie rural suite of hydrological models were considered gr4j gr5j and gr6j 4 6 free parameters the grj models have been applied in a variety of hydrological contexts examples include le moine et al 2008 perrin et al 2008 coron et al 2012 smith et al 2012 coron et al 2017 with observed moments lying outwith the simulated moments see section 2 2 2 the five and six parameter models gr5j and gr6j were rejected continuous daily time series of flow precipitation and potential evapotranspiration serve as model input the time series should be of sufficient length for validation on the climate baseline in stage 3 for example the ukcp09 baseline is 1961 1990 2 2 2 modified covariance approach the hydrological model is parameterised following the modified covariance approach as set out in visser et al 2018b in using this approach the modelling objective is not the replication of a flow time series rather it is the identification of the region of parameter space which is best able to replicate the his for a more extensive discussion of the modified covariance approach see visser et al 2018b in the application of this approach the complete parameter space of the hydrological model is sampled the number of parameter sets is dependent upon the number of free parameters and the level of uncertainty adjudged acceptable to reduce bias the parameter space should be sampled uniformly for example using sobol quasi random sequences a quasi monte carlo method caflisch 1998 the parameter sets thus established the hydrological model is run in simulation mode using observed climate data for each of the n time series the covariance between observed climate and simulated flow is calculated this is repeated for the observed flow data the his identified in stage 1 are then determined from both the observed and simulated flows prior to the selection of a parameter set it is first necessary to validate the hydrological model structure this is facilitated through plots of the observed and simulated relationships between the covariances and his the model is validated if the moments agree i e observed moments lie within the simulated moments sampled parameter space error thresholds in combination with index importance determined in stage 1 are used to identify a suitable parameter set a linear relationship between the minimum and maximum error thresholds and index importance is defined parameter sets which fall below this defined limit are rejected for additional details see section 3 case study application or visser et al 2018b the focus of the covariance approach is on the replication of specific hydrological characteristics in the catchment the his as opposed to flow consequently the hydrological model should be assessed in terms of its ability to replicate these characteristics rather than the observed flow time series indeed the replication of the time series is anticipated to be poor consistent with similar work focussed on the replication of catchment characteristics e g see seibert 2000 2 3 stage 3 projections 2 3 1 ukcp09 weather generator the ukcp09 weather generator wg was selected due to its ability to represent natural climatic variability murphy et al 2009 kay and jones 2012 this consideration of natural variability allows extraordinary low probability climatic events to be captured more effectively schlabing et al 2014 which is particularly important for ecosystems wigley 1985 the wg creates synthetic stochastic time series of climate variables based on observed climate statistics the wg is perturbed to represent future climate through the application of change factors projections are at a 5 km resolution allowing for representative simulation across smaller catchments 1000 km2 typical in the uk kilsby et al 2007 jones et al 2010 data requests are submitted using the ukcp09 web based portal http ukclimateprojections ui metoffice gov uk ui admin login php the climate variables of interest are precipitation and potential evapotranspiration note that potential evapotranspiration may also be computed from the hourly time series the cmip4 sres scenarios upon which ukcp09 was based does not assign probabilities to specific emissions scenarios wigley and raper 2001 meehl et al 2007 murphy et al 2009 consequently it is assumed that each emissions scenario is equally probable murphy et al 2009 for the validation of the wg output ukcp09 recommend comparison of the observed and baseline climate data in the form of bi monthly and seasonal plots of the mean and 95 confidence intervals for each climate variable defra 2011 to this end linear bias correction is applied bi monthly where necessary 2 3 2 baseline validation the baseline climate data is used to validate the framework the generated climate variables are input into the hydrological model generating a range of possible flow time series for each time series the important his are calculated per hydrological year season these indices can be assessed relative to the observed indices determined in stage 2 validation is through cumulative distribution and probability density functions cdf and pdf respectively comparison of the mean and 95 confidence intervals with these indices and the hydroecological model a range of possible life scores for the baseline period may be determined validation is as above if the length of the ecological time series is insufficient an alternative approach may be applied this is further considered in the application of the framework 2 3 3 future hydroecological projections simulation of future hydroecological projections life is analogous to the validation with the exception that the future climate projections serve as the input data each emissions scenario time period should be considered distinct 3 case study application the ability of the framework is both validated and demonstrated through application to a uk case study river descriptions focus on the case study specific data acquisition and preparation the subsequent analysis being as per the described framework 3 1 study area the nar represents a vulnerable and important river type groundwater fed chalk stream already subject to significant stress nrt 2012 the additional threat of climate change to its ecological potential cannot be understated it is intended that the power of this new proposed framework be illustrated in its application to this case study river the spring fed river nar rises in the norfolk chalk hills 52 749 n 0 812 e 60 m above sea level flowing west for 42 km before joining the river great ouse 52 748 n 0 394 e the formation of the fen basin and resultant dissection of the chalk created two distinct river units delineated by a significant gradient change at narborough fig 3 sear et al 2005 with a greater abundance and quality of ecological data visser 2015 the focus is on the 153 3 km2 chalk sub catchment the hydrology of the river nar is characteristic of pure chalk streams sear et al 1999 with a high base flow index 0 91 sear et al 2005 and relatively low flows mean 1 12 m3 s q10 2 03 m3 s and q90 0 49 m3 s over the available record where q10 and q90 represent the 10 and 90 flow exceedance respectively equivalent to 90th and 10th percentiles a reliance on groundwater results in a highly seasonal flow regime where aquifer recharge primarily occurs in winter months leading to a progressive rise in river flow until march april 3 2 stage 1 hydroecological model routine macroinvertebrate sampling by the environment agency and prior custodians has been ongoing since 1985 nrt 2012 from 1992 the sampling methodology follows the environment agency s standard semi quantitative protocol see murray bligh 1999 data available upon request from the environment agency 2018 only samples identified to species level and collected in the spring season amj peak of macroinvertebrate activity were considered the life scores were calculated for a total of seventy two macroinvertebrate samples 1993 2012 hydrological data was extracted from the national river flow archive 1990 2012 ceh 2018 at the marham gauge 52 678 n 0 548 e fig 3 the hydrological data was subdivided into six subsets two hydrological seasons winter ondjfm and summer amjjas and three time offsets 0 2 years a total of 63 6 ecologically relevant his were considered this was reduced to 29 through pca this reduced set of his were then paired with the life scores to create the hydroecological dataset 3 3 stage 2 hydrological model for parameterisation of the hydrological model 54 years of daily average flow recorded at the marham gauge were extracted september 1961 to 2015 the corresponding climate variables were computed from daily average rainfall and hourly temperature data at 5 midas stations in and around the catchment fig 3 met office 2016 the parameters of interest are the average daily precipitation p and potential evapotranspiration pe p is determined via the computation of the daily catchment average rainfall whilst pe is estimated from hourly temperature data using the temperature based pe model from oudin et al 2005 in order to verify the method of investigation n 100 000 parameter sets were generated using sobol sequencing the his used to parameterise the model are those indicated by the hydroecological model in stage 1 in the parameterisation of the hydrological model the minimum and maximum error were specified as 17 5 and 35 2 e r r o r m i n respectively from which the linear threshold was determined the relationship between the minimum and maximum allowable error and the relative importance of the variables covariances were assigned a relative importance of 1 3 4 stage 3 projections to address a number of the uncertainties indicated in the introduction see also appendix a 2 the ukcp09 probabilistic climate projections are used the 2050s 2040 2069 high emissions scenario a1f1 sres is considered this emissions scenario is approximately equivalent to a change in temperature of 4 3 c by 2081 2100 relative to the pre industrial period 1850 1900 riahi et al 2011 met office 2018b for demonstrative purposes the ukcp09 wg was run for the full range of 10 000 variants 4 results and discussion 4 1 stage 1 hydroecological model the hydroecological model a linear multi model average is depicted in equation 1 overleaf summaries of the his are provided in table 1 importance represents the relative weight of evidence in support of each index in the model according to it whilst the relative parameter uncertainty is the 95 confidence interval relative to the parameter estimate the underlying hydroecological processes are first considered followed by a review of the predictive ability and uncertainty associated with the hydroecological model 1 l i f e 0 07 10 r 90 l o g w t 0 0 07 r i s e m n w t 0 0 93 q 80 q 50 s t 0 0 02 q 90 q 50 s t 0 0 3 q 90 q 50 s t 1 0 11 q 70 q 50 s t 1 0 04 r e v p o s s t 1 0 5 l o g q v a r s t 1 4 1 1 underlying hydroecological processes the winter hydrological season when the chalk aquifer recharges features both the most and least important his 10r90log and risemn respectively the indicator 10r90log ratio of low flows to high flows 10th to 90th percentiles is described as an indicator of responsiveness richards 1990 the log scale of the index coupled with its importance means that there is scope for 10r90log to dominate the hydroecological model both positively and negatively fig 4 clearly illustrates that large values of 10r90log correspond with the highest life scores and vice versa it is only when 10r90log is small 0 that the other six indices contribute to life score varying high and low flows shows that the highest values of 10r90log and hence life are achieved when high flows are medium high 0 exp p 90 log q 1 given the log space the scope for a negative impact exp p 90 log q 1 is large surprisingly then whilst magnitude of flow is of importance for the recharge of the aquifer higher winter flows may actually negatively impact the macroinvertebrate community the negative sign of the hi risemn indicates a preference for a low mean rise rate in winter flows however the low importance of the index table 1 sees it consistently contribute less than 2 5 to the life score fig 4 in terms of hydrological season the summer months amjjas dominate the hydroecological model there is an indication that in the summer months consistency in flow low range variation is preferred 1 a sustained increase in flow revpos sees a large negative impact on life importance 0 80 2 though not as important logqvar similarly implicates large variation in flow and 3 minimising the range between low and median flows q70q50 q80q50 and q90q50 has an increasing effect on life looking to fig 5 four out of the five summer his are lagged s 1 essentially these indicators are influencing the health of the river two years in advance should there be a bad summer with lots of variation the consequences could be severe however the presence of the q80q50 indicator in the immediately preceding season gives some scope for improvement however it is also worth noting that the negative impacts of a bad summer would only be felt if the value of the index 10r90log was low whilst if 10r90log is largely positive or negative the preceding summers are of limited importance in terms of management it is clear that summer floods in particular could be detrimental to the health of the river perhaps representing an argument for improving connections to flood plains similarly extremely high winter flows may be harmful indicating there may be scope to abstract and store waters during the winter months for use in summer however it is worth noting that negative impacts are also a necessary component of the proper ecosystem function for example they might act as a natural reset button everard 1996 lake 2003 interestingly the majority of the indices are dimensionless with the exception of logqvar and risemn this allows for some scope for variation in flow without causing excessive damage for instance in summer a need for increased abstraction need not necessarily be a detriment to river health though this assumption ignores the other effects of decreased flow 4 1 2 predictive ability and uncertainty the predictive ability of the model is first indicated by the relative parameter uncertainty unconditional variance or 95 ci relative to the parameter estimate table 1 generally as relative parameter uncertainty increases the importance of the index decreases this is one of the advantages of the weighting of the his in the it approach visser et al 2018a the fact that the most important index 10r90log has greater uncertainty than the second most important revpos suggests that this may be the best parameterisation possible in the model with regards to the implications of parameter uncertainty further inference may be made through the consideration of the 10 000 monte carlo simulations fig 6 the plot shows that the hydroecological model performs well low interquartile range of 0 44 and relative error centred around one perfect agreement this level of uncertainty is considered satisfactory 4 2 stage 2 hydrological model fig 7 a depicts the observed and simulated relationship between the covariance of precipitation and flow ρ p q and the hi q80q50 fig 7 b depicts the same relationship for the climate variable potential evapotranspiration ρ p e q for all seven his the observed moments lie within the simulated moments validating the use of the hydrological model the capacities of the production x1 and routing x3 stores were estimated as 511 and 311 mm respectively the time elapsed for flow routing is approximately 1 17 days x2 inflow from the chalk aquifer is represented by a positive groundwater exchange coefficient x4 of 2 84 mm per day the level of agreement for all seven his is summarised in table 2 with a value of 0 8 the largest covariance relative error is for potential evapotranspiration this is considered acceptable as precipitation is considered the principal determinant of flows in the east anglia region kay et al 2013 the hi relative errors are below 11 with the exception of the least important index risemn relative error 34 overall the level of relative error in the hydrological model is considered satisfactory the impacts of the error in the index risemn are likely negligible based on the findings from the hydroecological model for standard model validation see appendix c 3 4 3 stage 3 projections 4 3 1 baseline validation the ability of the framework to reproduce the observed data hydrological and hydroecological is assessed via cdfs and pdfs for the cdf plots the observed function should situate within the boundaries of the baseline projections ideally centrally the pdf plots focus on relative error where a value of 1 0 indicates perfect agreement here the objective is on a low interquartile range iqr in the interests of concision validation of the hydrological projections centres on the index q80q50 selected both due to its high importance 0 80 and ease of interpretation ratio of moderate low flows to median flows summary tables for all seven his are available in appendix c 4 4 3 1 1 hydrological model validation the validation plots for the hi q80q50 are presented in fig 8 fig 8a d are based on the ukcp09 baseline 1961 1990 both satisfy the objectives outlined above the cdf of the observed values lies within the projections and the pdf shows a low iqr comparatively the 95 confidence interval ci appears large however given the probabilistic nature of the projections this is not unexpected the baseline interval for which ecological data is available 1986 1990 is summarised in fig 8b e the iqr is similar to the 30 year baseline with a minor improvement in the 95 ci given the limited time period the right skew of the pdf fig 8e cannot be ascribed significance on the alternative baseline 2010 2017 the cdf is notably stepped fig 8c this is reflected in the pdf fig 8f with a local maximum and a median not equivalent to one perfect agreement despite this the iqr is the lowest of the three validation plots overall for the cdf s the observations lie centrally within the probabilistic projections whilst the pdf s reveal low iqr s the plots satisfactorily validate of the use of the ukcp09 projections through the hydrological model 4 3 1 2 coupled hydrological hydroecological model validation there is no ecological data species life available for the period 1961 1990 baseline validation period however sampling and identification of macroinvertebrates to the family level was carried out during the period 1989 1990 allowing for some comparison to further address this an alternative baseline was established through consideration of the earliest time period considered by the ukcpo9 wg 2010 2039 reduced to 2010 2015 run for the medium emissions scenario for 1000 randomly sampled variants subsequently the climate variables are bias corrected relative to the observed data in this period validation plots for the hydroecological response life as predicted by the coupled model is presented in fig 9 a c for the baseline 1986 1990 and fig 9b d for the alternative baseline 2010 2017 recall that for the period 1986 1990 only family life data is available on this baseline period the cdf s fig 9a are in agreement with a small iqr for the relative error of approximately 0 1 fig 9c the somewhat swollen 95 ci may have a threefold explanation 1 family level application of the life methodology tends to underestimate hydroecological response extence et al 1999 monk et al 2012 2 the limited number of years data points and 3 the probabilistic nature of the projections for the alternative baseline 2010 2017 the cdf fig 9b is in agreement the pdf fig 9d also reveals a lower iqr relative to fig 9c as well as an improved 95 ci although the temporal range of the validation is limited both time periods are able to achieve a satisfactory level of performance thereby validating the use of the ukcp09 projections and the coupled hydrological hydroecological model the use of the coupled model is thus considered fit for purpose in application to future projections 4 3 2 future projections the ukcp09 climate projections for the 2050s time slice of the high emissions scenario were inputted to the coupled hydrological and hydroecological model the focus herein is on this hydroecological response the projections of hydroecological response are first considered relative to the baseline through the cdfs and pdfs in fig 10 looking to the means first dashed lines the projected change is relatively small however there is a consistent increase in the range of possible life scores across the distribution the increase in maximum life scores appears responsible for the majority of this change though some may also be attributes to the minimum values specifically the tails of the distribution percentile 0 375 in fig 4 it was shown that the index 10r90log was the main determinant of higher life scores it can thus be presumed that from percentile 0 75 the increase in life scores is the result of an increased stability in the ratio of high to low flows in the winter season where percentile 0 75 the five summer his are likely to dominate a further indication of increased stability of flows in the river fig 11 gives an indication of the probability of these hydroecological projections these probabilities are in line with calibrated language used by the ipcc since ar4 treut et al 2007 mastrandrea et al 2010 table 3 widening the confidence interval from about as likely as not to virtually certain increases likelihood but results in a wider estimate overall the bounds of uncertainty are relatively narrow however it is clear that the greatest confidence lies in the interquartile range rather than the tails of the distribution it should also be noted that whilst the change to maximum life scores is still in evidence the decrease in life scores at the lower distribution has disappeared indicating a lack of certainty in those projections whilst schlabing et al 2014 also observed limited changes in the central tendencies they also note that it is important to consider the tails of the distribution although these events lie outwith the probabilities indicated in fig 11 this may be justified due to the ability of the wg to capture low frequency events dubrovský et al 2004 mehrotra and sharma 2007 as in schlabing et al 2014 fig 12 looks to the hydroecological response at the 5th and 95th percentiles it is important to note that life scores at these percentiles will be primarily determined through the winter hi 10r90log at the 5th percentile life scores 4 5 account for only 0 016 of observations and are therefore omitted broadly speaking the frequency of lower life scores appears to decrease under the future projections consequently there is almost a 10 increase in the number of life scores equal to 7 for the 95th percentile life scores 9 5 account for 0 0244 of the total observations and are therefore omitted with a marked increase in the frequency of life 8 the positive change in hydroecological response previously observed figs 10 and 11 is clearly in evidence 4 4 implications for the case study river the proposed framework has indicated a clear hydroecological response to the projected changed climate under the a1f1 high emissions scenario in the 2050s however the magnitude and direction of change is projected to be both small and positive the scale of this change is in line with the ukcp09 projections for the east anglia region under this scenario in this region the projected change in mean annual precipitation is small ranging from 5 across the 10th to 90th percentiles met office 2018 note that kay et al 2013 observe that hydrological response in east anglia is principally determined by precipitation further the range of life values is known to be small particularly in the iqr based on the biosys database of ecological data across 548 catchments in england environment agency 2018 the iqr for 546 catchments is 1 therefore the observed change signal may be presumed to represent a true change in community structure figs 10 and 11 indicated a clustering of life scores visually this is most clear with the 1 375 increase in the mode fig 10b this is reflected in the summary statistics for example the kurtosis of the life score distribution increases from 16 6 to 18 4 this flattening of the hydroecological response is a possible indication of a reduction in biodiversity if this were to be the case this would increase the vulnerability of the river overall monocultures being far more susceptible to local extinction further the reduced frequency of events where life scores fall very low could impact negatively upon the river robbing the ecosystem of vital natural reset events everard 1996 lake 2003 4 5 framework limitations limitations of the framework centre around the assumptions of stationarity and data availability in climate change modelling projections are often based on historic climate with the assumption that the statistical properties of the climate remain stationary this assumption is inherited under both hydroclimatic and coupled modelling the corollary is an enforced assumption that ecological response remains the same as it is now consequently at this time it is not possible to account for the adaptive response of the riverine community a barrier to hydroecological studies has been the lack of paired long term hydrological and ecological time series monk et al 2007 2012 this problem persisted in the development of the hydroecological model for example in the uk routine macroinvertebrate sampling began circa 1990 murray bligh 1992 therefore given the baseline of 1960 1990 validation is limited to address this an alternative baseline was derived the use of climate projections with a more up to date baseline for example the soon to be released ukcp18 projections or a wg trained using cmip5 or cmip6 climate data would also address this 5 conclusions the implications for flow regime make rivers among those ecosystems most sensitive to climate change death et al 2015 watts et al 2015 whilst studies have attempted to assess the impact of climate change on hydroecological response methods are often qualitative or follow quantitative methods of limited scope the resulting lack of clarity renders the fallout for ecosystem services effectively an unknown in answer the proposed framework provides a quantitative approach developed using methods which minimise uncertainty in line with recommendations in clark et al 2016 the ability of the framework has been illustrated through application to a case study river the projected hydroecological response in april june the period of peak mi activity in the river is considered under the a1f1 high emissions scenario in the 2050s the hydroecological response is in line with climate projections for the east anglia region the projections indicate that a reduction in biodiversity is virtually certain a possible disruption to low flow processes essential to ecosystem functioning is less strongly indicated it should be noted that whilst the projected hydroecological change may be limited the river nar is strongly influenced by groundwater bfi 0 91 consequently the impact of changes in precipitation may be reduced thus greater change in response might be expected in catchments where surface runoff dominates in summary the proposed framework serves as a new and dynamic tool with the potential to provide valuable information in the pursuit of more accurate assessments of the impact of climate change on river ecosystems critically and possibly uniquely in the field bennett et al 2013 the end user will also be provided with a quantifiable measure of uncertainty in the hydroecological projections further applications of the framework include water resources planning and future environmental flow management in recent years hydroecological modelling is often undertaken using a regime based spatial framework for example monk et al 2011 zhang et al 2012 in a similar manner the proposed framework may be extended to cover multiple rivers of similar flow regime classification such generic projections of the impact of climate change on hydroecological response might thus be used to plan wider adaption measures including for ungauged rivers where appropriate the projections may also be used to assess the implications of climate change on the provision of instream ecosystem services e g through the framework set out in ncube et al 2018 data availability consent has not been given to share the data used in this study however these data are freely available from the original sources the environment agency ea 2018 macroinvertebrate sampling records met office 2016 climate precipitation and temperature national river flow archive ceh 2018 gauged flow at marham and data requests for the climate projections may be submitted using the ukcp09 web based portal http ukclimateprojections ui metoffice gov uk ui admin login php acknowledgements the authors gratefully acknowledge funding from the engineering and physical science research council through award 1786424 further thanks go to the environment agency and the centre for ecology and hydrology for the provision of data appendix d supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix d supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 01 004 appendix a uncertainty a 1 stages 1 and 2 table a1 types and sources of uncertainty that are addressed in stages 1 and 2 hydroecological and hydrological modelling of the proposed framework table a1 stage type of uncertainty source controls 1 sampling and measurement error input data standardised methodology for the collection of macroinvertebrate samples case study specific quality control checks on observed flow data case study specific variability climate internal variability length of observed time series covering range of climatic periods wet dry case study specific structural uncertainty model selection pca addresses parameter redundancy it genetic algorithm searches for global optimum rather than local multi model average not a single best model parameter uncertainty model selection and parameterisation confidence intervals weight of supporting evidence referred to as importance 2 measurement error input data quality control checks on observed flow and climatic data the modified covariance approach does not calibrate based on goodness of fit statistics thereby reducing the bias of the parameterised model see also ref myself variability climate internal variability length of observed time series covering range of climatic periods case study specific structural uncertainty model structure the modified covariance approach rejects model structures if the observed and simulated moments do not coincide parameter uncertainty parameterisation the modified covariance approach focusses on replicating the essential characteristics of the catchment explicitly the relative importance of the his is known error thresholds for each hi may be specified accordingly equifinality the modified covariance approach considers the full parameter space which is narrowed down to a small region which is able to best replicate the his of interest a 2 stage 3 climate projections climate change projections are central to the application of the proposed framework it is recommended that probabilistic climate projections which consider a range of impacts be used when applying the proposed framework in the case study application the ukcp09 probabilistic climate change projections are used specifically the weather generator product the application of the framework is not limited to ukcp09 other sources of probabilistic climate change projections include the comepro project in the mediterranean region kaspar ott et al 2016 ring et al 2018 the mit igsm cam framework monier et al 2013a applied over northern eurasia monier et al 2013b and ukcp18 the next iteration of uk climate projections based on the research concentration pathways from ar5 met office 2018a equally projections may be produced via weather generator may be used directly for example the vector autoregressive weather generator schlabing et al 2014 the ukcp09 identifies three major sources of uncertainties in their climate projections murphy et al 2010 epistemic uncertainty incomplete understanding of climate system processes internal climate variability and scenario uncertainty a summary of the controls introduced in ukcp09 to minimise this uncertainty is detailed in table a2 table a2 sources of uncertainty in the ukcp09 weather generator climate projections and the controls in place to minimise this based on murphy et al 2010 table a2 source controls epistemic uncertainty a perturbed physics ensemble of the variance clark et al 2016 e g different mathematical representation of the processes interactions and feedbacks variability multiple runs with the same initial conditions for each ensemble weather generator simulations based on statistical characteristics in the observed data simulations pick up more extreme climatic events schlabing et al 2014 scenario uncertainty there is a lack of agreement in how relative probability should be assigned to emissions scenario to address this ukcp09 presents three emissions scenarios low medium and high appendix b information theory b 1 model evaluation although the overall concept of information theoretics may be unknown to the reader certain aspects should be familiar based on deep theoretical foundations burnham and anderson 2001 p 244 the concept and application are conspicuously simple candidate models are evaluated over three steps 1 measuring the information lost in each approximating model 2 determination of the evidence in support of each model and 3 multi model inference of a final model structure from the candidate set step 1 loss of information from model f kullback leibler k l gives a measure of the amount of information that is lost when model g is used to approximate reality f a model which loses the least information i e has the most supporting evidence out of the candidates can be considered the best approximation of reality the information loss i f g is determined through computation of information criteria ic a multitude of ic exist some of which with the reader is undoubtedly familiar the akaike information criterion aic represents the standard estimate of kullback leibler information burnham and anderson 2002 in hydroecological modelling the sample size n is often small relative to the number of variables k a second order bias corrected version of aic aicc can account for this through the addition of a second penalty burnham and anderson 2002 b1 a i c c a i c 2 k k 1 n k 1 step 2 evidence in support of model g i the value of aic is dependent on the scale of the data the goal is to achieve the smallest loss of information given the data this difference is rescaled and ranked relative to a i c m i n b2 δ i a i c c i a i c c min f o r i 1 2 r the value of δ i may be interpreted through a rule of thumb based on likelihood intervals δ i 2 there is substantial supporting evidence for model g i 4 δ i 7 the models are not as competitive and if δ i 10 it can be assumed that there is strong evidence against model g i burnham and anderson 2002 from this measure of evidence the likelihood that model g i is the best approximating model can be determined this is known as the akaike weight w ranging from 1 to 0 for the most and least likely models respectively b3 w i e x p 1 2 δ i r 1 r exp 1 2 δ r the use of the akaike weight allows for clearer inference when considering the candidate models step 3 multi model inference when using information theory model selection the best approximating model is inferred across the entire candidate set this is achieved through consideration of a weighted combination of all candidates parameter averages θ ˆ are simply the sum of the akaike weights for each model that contains the predictor θ ˆ b4 θ ˆ i 1 r w i θ ˆ i as a result the parameter averages are ranked such that the highest value represents the most important in the model this eliminates the problem of multiple equally plausible models with different parameter structures equifinality b 2 application using glmulti the package glmulti streamlines the above steps into a single function calcagno 2013 the fundamentals of the algorithm and approach are available in calcagno and de mazancourt 2010 a subset of models was determined using the function glmulti the function was applied five times to ensure convergence to a consensus of model subsets with the function coef applied to determine the it multi model average the number of indices is reduced by removing those indices where both coefficient and standard error are zero and to within the 95 confidence interval by ordering by descending importance importancei cumsum importance 0 95 this is illustrated in table b1 overleaf table b1 the structure of the hydroecological model prior to final removals detailed above hydrological seasons are indicated by w winter and s summer the facets of the flow regime are denoted as m magnitude and r rate of change the removed indices occupy the final five rows reasons indicated in bold table b1 season time offset index facet definition coefficient importance unconditional variance cumulative evidence weight 0 intercept 7 64 1 2 36 0 1 w 0 10r90log m log ratio 10th 90th percentile flows 0 07 0 86 0 0 27 2 w 0 risemn r mean rise rate in flow 0 07 0 07 0 05 0 95 3 s 0 q80q50 m q80 flows relative to median 0 93 0 51 1 3 0 68 4 s 1 logqvar m variance in log flows 0 5 0 37 0 57 0 8 5 s 1 q90q50 m q90 flows relative to median 0 3 0 19 0 28 0 86 6 s 1 q70q50 m q70 flows relative to median 0 11 0 09 0 05 0 93 7 s 1 revpos r number days when flow is increasing positive reversals 0 04 0 8 0 0 52 w 0 25r75log m log ratio 25th 75th percentile flows 0 0 12 0 0 9 w 0 20r80log m log ratio 20th 80th percentile flows 0 0 05 0 0 97 w 2 maxq50 m maximum flow relative to median 0 0 04 0 0 98 w 2 maxmonthlymed m median maximum flow relative to median flow across all years 0 0 03 0 1 s 0 q90q50 m q90 flows relative to median 0 02 0 04 0 01 0 99 appendix c case study c 1 hydroecological model index contribution the contribution of each hi to hydroecological response was determined using the baseline data 1961 1990 each of the 10 000 wg variants and hydrological year were considered independently for each of these iterations the relative contribution of the hi was determined c1 i n d e x v a l u e i n d e x c o e f f i c i e n t i n d e x v a l u e i n d e x c o e f f i c i e n t c 2 hydroecological model validation with observed data data represents a key limiting factor to hydroecological modelling with long term 15 20 years macroinvertebrate community data at the species level uncommon monk et al 2012 consequently the length of the time series in hydroecological modelling is insufficient for split sampling calibration and validation this is commonplace in hydroecological modelling monk et al 2012 environment agency 2018 the exploration of the model uncertainty serves as one approach to address the validation further validation is considered through comparison of simulated species life scores 1991 2017 to three data sources summarised in table c1 see figure c1 for validation the following should be considered when interpreting figure c1 as discussed previously life score differences across taxonomic level are inevitable differences in life scores of the same taxonomic level are due to known errors within the biosys records biosys stat that corrections to address these inconsistencies are in progress environment agency 2018 april 1995 september 1997 saw extremely low rainfall leading to errors in the recording of low flows nrfa 2014 this discrepancy may be the reason for the differences in observed and simulated values it should be noted that the hydroecological model was not trained on data marked as suspect this data is included in figure c1 to allow for the fitting of trendlines table c1 data sources considered for the additional validation of the hydroecological model years of data excluding training data 1993 2012 table c1 raw mi data provided taxonomic level source of life score years yes species this study 2013 2014 yes family this study 1986 2014 no family the freshwater and marine biological surveys england archive known as biosys environment agency 2018 1991 2018 fig c1 comparison of life scores across data sources see table c1 trend lines are fitted for each data source with the exception of species life 2013 2014 insufficient data fig c1 the two dashed lines represent the comparison of the training data light green and model simulations dark green the similarity in the slope of the lines indicates a high level of agreement in life scores only two additional species life scores are available 2013 and 2014 solid blue circles it can be seen that these values are consistent with the observed training data trendline light green dashed line two sources of observed family life scores are available see above for notes on differences between the data sources the slope of the trendline for family life scores determined as part of this study solid blue line is similar to the observed training data the underestimation of life scores may be attributed to the difference in taxonomic level for the ea biosys data a good level of agreement is again indicated though it can be seen that the validity of the model improves over time c 3 hydrological model validation with observed data fig c2 validation of the hydrological model using observed data a comparison of the mean and 95 confidence interval 2 standard deviations b probability density function of the relative error a value of one indicates perfect agreement between model observations fig c2 c 4 hydrological model validation with projections differences in the 95 ci for the index 10r90log are the result of a single outlier observation during the 30 year time period note that the other two time periods are 4 and 7 years in length it is also observed that the impact on hydroecological response reduces as 10r90log increases to more extreme values 10 specifically table c2 validation of the framework using the ukcp09 projections on the baseline table c2 index time period lower quantile median upper quantile 95 ci 95 ci 10r90log 1961 1990 1 39 0 15 2 17 96 58 96 88 10r90log 1986 1990 1 35 0 01 0 81 27 83 27 86 10r90log 2010 2017 0 17 0 05 0 43 19 47 19 58 revpos 1961 1990 0 86 1 1 2 0 28 1 72 revpos 1986 1990 0 82 0 9 1 0 65 1 15 revpos 2010 2017 0 85 0 95 1 07 0 59 1 32 q80q50 1961 1990 0 9 1 01 1 13 0 65 1 37 q80q50 1986 1990 0 9 0 99 1 06 0 76 1 22 q80q50 2010 2017 0 98 1 05 1 13 0 78 1 32 logqvar 1961 1990 0 57 0 95 1 59 1 28 3 17 logqvar 1986 1990 0 55 0 92 1 49 0 75 2 58 logqvar 2010 2017 0 34 0 58 1 06 1 24 2 41 q90q50 1961 1990 0 87 1 02 1 19 0 56 1 48 q90q50 1986 1990 0 86 0 98 1 08 0 67 1 28 q90q50 2010 2017 0 92 1 02 1 09 0 71 1 33 q70q50 1961 1990 0 92 1 01 1 1 0 71 1 3 q70q50 1986 1990 0 92 0 98 1 04 0 8 1 16 q70q50 2010 2017 0 96 1 02 1 13 0 76 1 28 risemn 1961 1990 0 65 1 08 1 89 1 36 3 53 risemn 1986 1990 0 56 0 93 1 36 0 38 2 24 risemn 2010 2017 0 87 1 43 2 45 1 79 4 66 
26248,rivers are among the ecosystems most sensitive to climate change whilst methods quantifying the impact and uncertainty of climate change on flow regime are well established the impact on hydroecological response is not well understood typically investigative methods are qualitative in nature or follow quantitative methods of limited scope whilst the effect of uncertainty is frequently minimised this paper proposes a coupled hydrological and hydroecological modelling framework to assess the impact of climate change on hydroecological response quantitatively the characterisation and reduction of modelling uncertainties was critical to the development of the framework the ability of the framework is illustrated through application to a case study river the river nar norfolk england using the ukcp09 probabilistic climate projections high emissions scenario sres a1f1 the results show that by the 2050s a reduction in instream biodiversity is virtually certain if future emissions follow the assumptions of sres a1f1 disruption to the natural low flow processes essential to ecosystem functioning is also indicated these findings highlight the importance of the framework in water resources adaptation particularly with respect to future environmental flows management keywords climate change impact coupled hydrological and hydroecological model modelling framework probabilistic climate change projections ukcp09 hydroecological impact uncertainty 1 introduction the global climate system is changing with changes to climatic behaviour mean and variability projected beyond the 21st century ipcc 2014 climate change is expected to amplify existing pressures on natural resources as well as create new ones ipcc 2012 amongst these freshwater is considered the most essential vörösmarty et al 2010 rivers and their ecosystems provide a diverse range of services upon which humans are dependent yeakley et al 2016 these include fresh water supply for human consumption hydro hazard regulation and water purification gilvear et al 2017 it is thus through freshwater resources particularly rivers that some of the most significant impacts of climate change will be felt ostfeld et al 2012 consequently there are significant questions over the long term sustainability of water resources gleick 1998 2016 klaar et al 2014 it is clear that effective water management is central to successful climate change adaption ostfeld et al 2012 climate is a major determinant of hydrological processes where precipitation temperature and evaporation represent the dominant drivers ipcc 2007 consequently a changing climate will inevitably lead to alterations of river flow regimes rahel and olden 2008 arnell and gosling 2016 attempts to model the impact of climate change on water resources have been ongoing since the mid 1980s arnell and reynard 1996 christierson et al 2012 climate projections are however subject to large unquantified uncertainties murphy et al 2004 leading to concerns over their suitability for water resources adaptation and planning kundzewicz et al 2008 wilby 2016 examples of these uncertainties include clark et al 2016 wilby 2016 1 epistemic uncertainty the inability to properly capture the underlying processes and feedbacks and 2 accounting for variation due to natural climatic variability in practice uncertainty dictates the usefulness of the model inaccurate appreciation of this uncertainty precludes meaningful interpretation of the model leading to sub optimal decision making warmink et al 2010 when considering future projections clark et al 2016 posit that research which focuses on characterising reducing and representing quantifying these uncertainties may allow for the provision of plausible flow projections under climate change difficulties with regards to the quantification of climate uncertainty may be addressed through the use of a perturbed physics ensemble an ensemble of gcms where variation of the model parameters allows quantification of uncertainty murphy et al 2004 clark et al 2016 such enhanced projections have been available for the uk since 2009 through ukcp09 variability in the flow regime is widely acknowledged as the major determinant of ecological health in riverine ecosystems e g power et al 1995 lytle and poff 2004 arthington et al 2006 alteration of this natural flow regime threatens the ability to provide ecosystem services rahel and olden 2008 vörösmarty et al 2010 arthington 2012 despite this and perhaps surprisingly the effects of climate change on river health are rarely considered as observed in durance and ormerod 2007 seven years later schlabing et al 2014 describes little change noting that even when accounted for the methodology employed is often over simplified and rudimentary a brief review of such work performed over the last two decades is thus indicated see also fig 1 a large number of the studies investigating the impact of climate change on hydroecological response have been qualitative in nature fig 1 level 1 to 3 directly not pictured examples include meyer et al 1999 ostfeld et al 2012 filipe et al 2013 and death et al 2015 whilst the number of quantitative studies has increased their scope is often limited to the direct links between climate temperature and hydroecological response fig 1 for example durance and ormerod 2007 kupisch et al 2012 and jyväsjärvi et al 2015 in these studies the impact of the altered flow regime is not considered and rarely acknowledged döll and zhang 2010 were the first to consider the impact of climate change on the flow regime at a global scale assessment of the hydroecological impacts was qualitative in nature with limited consideration of changes in the number of endemic fish species counts are not considered meaningful bioindicators for example see li et al 2010 the authors acknowledge that quantitative estimates of ecosystem response have not yet been derived following this studies of a similar nature have been undertaken at higher resolutions catchment level examples include tang et al 2015 hassanzadeh et al 2017 and o keeffe et al 2018 advances have also been made in the assessment of direct climate change impacts on the provision of freshwater ecosystem services see conceptual framework in pham et al 2019 though again these are at present qualitative in nature merriam et al 2017 perform a habitat assessment using a coupled stream temperature and hydrological model whilst the focus is on the availability of thermally suitable habitats for brook trout and not flow alteration directly the study represents an important advancement towards fully quantitative assessment of the instream ecological impacts of climate change nevertheless significant questions arise as to the robustness of the applied methodology the authors make assertions using phraseology such as high degree of certainty when discussing results based upon r squared values and rmse a statistical measure of average inaccuracy yet the problems inherent to rmse have been recognised for some three decades willmott et al 1985 and more recently willmott and matsuura 2005 conclude that in the context of climate study model performance evaluations based primarily on rmse are questionable and should be reconsidered further issues arise in the calibration of the hydrologic model where performance is assessed in terms of nash sutcliffe a statistic subject to long standing broad and sustained criticism legates and mccabe 1999 seibert 2001 criss and winston 2008 indeed clark et al 2016 state that when modelling the hydrological impacts of climate change nash sutcliffe and similar efficiency criteria introduces additional unaccounted uncertainties this disregard of uncertainty throughout the paper calls into question the validity of the results it is clear that methods to quantify the impact and associated uncertainties of climate change on the flow regime are well established fig 1 level 1 and 2 the hydroecological implications are less well understood and are rarely considered quantitatively where attempts have been made the effect of uncertainty is underplayed consequently the impact of climate change on hydroecological response is unclear and the fallout for ecosystem services poorly understood this paper proposes a coupled hydrological and hydroecological modelling framework to assess the impact of climate change on hydroecological response quantitatively the development of each stage of the framework has centred around the characterisation and reduction of uncertainty in line with the recommendations in clark et al 2016 the outputs from this framework are quantitative hydroecological projections of climate change impacts these outputs are intended to support water resources adaptation for example in the equitable allocation of water for human use and the environment known as environmental flows in order to validate and demonstrate the ability of the framework this paper features an application to a case study river the river nar in norfolk england 2 framework an overview of the three main stages of the proposed framework is presented in fig 2 in stage 1 the hydroecological model is developed based on advances made in 1 visser et al 2017 where lag in ecological response an important component of flow variability monk et al 2017 is accounted for through the consideration of multi annual hydrological indicators and 2 visser et al 2018a present an information theory it approach to minimise and quantify structural and parameter uncertainty the second stage of the framework is the parameterisation of the hydroecological following a modified covariance approach visser et al 2018b the modified covariance approach focuses on the replication of specific hydrological characteristics identified in stage 1 whilst also addressing a number of known limitations and uncertainties in hydrological modelling in stage 3 climate projections serve as the input to the coupled model providing the quantitative hydroecological projections of climate change impacts application of the framework to a case study river catchment is subsequently considered in 3 case study application a holistic depiction of uncertainty was central to the development of the proposed framework additional commentary on the characterisation and reduction of the sources of uncertainty following clark et al 2016 is provided in appendix a in the development of this framework it is necessarily assumed that the hydroecological relationship remains stationary as in hydroclimatological modelling the evolution of such relationships remains an unknown and is beyond the scope of this paper a further important limiting factor of many hydroclimatological studies is the focus on extreme climatic events e g filipe et al 2013 thornton et al 2014 death et al 2015 climate models are well known for their ineffective simulation of extreme climate particularly with regards to precipitation ipcc 2014 knowledge of the impacts of extreme events is therefore limited in an effort to address this the ukcp09 climate projections distribution tails are clipped 5 and 95 probability levels murphy and sexton 2013 in addition changes in climate mean and variability may lead to compound events or clustered multiple events these events are not extreme in themselves but can lead to extreme events and or impacts ipcc 2012 essentially severe impacts can occur from minor climatic events the focus of the framework is therefore on these impacts rather than stochastic individual extreme events 2 1 stage 1 hydroecological model statistical methods are well established for the testing of hydroecological hypotheses these include multiple linear regression for example clarke and dunbar 2005 and monk et al 2007 and multi level models recent examples include bradley et al 2017 and chadd et al 2017 hydrological indicators his and ecological data serve as the basis for the development of these models with their sensitivity to change macroinvertebrates are ideal indicators of river health acreman et al 2008 ea 2013 this response is determined by considering macroinvertebrate flow velocity preferences as described by the lotic invertebrate index for flow evaluation life extence et al 1999 a weighted index which takes into account the flow velocity preferences of the macroinvertebrate community life scores can range from one to twelve indicating a preference for limited flow standing water to rapid flows respectively 2 1 1 data the structure of the benthic macroinvertebrate community is subject to change throughout the year typically peaks of activity occur in spring amj and autumn ond seasonal focus in hydroecological modelling is determined by factors such as the quantity and quality of the available data and the overall modelling objective for example fishing is vital to the communities along the case study river the river nar garbe et al 2016 if the goal was to preserve future brown trout populations then modelling efforts would seek to protect their primary food source ephemeroptera baetidae mayfly which hatch during the spring season macroinvertebrate data may be utilised at the species or family level however it should be noted that the use of family level data may mask species specific information monk et al 2012 leading to a reduction in accuracy extence et al 1999 a hydroecological dataset is created by pairing the ecological data with his these indicators should be ecologically relevant reflecting the five facets of the flow regime required to support the riverine ecosystem richter et al 1996 magnitude frequency duration timing and rate of change to date over 200 ecologically relevant hydrologic indices have been proposed olden and poff 2003 monk et al 2006 thompson et al 2013 should seasonality be present in the hydrological regime the time series is split into relevant hydrological seasons the his are then calculated for each a number of studies on groundwater fed rivers have observed a delay in macroinvertebrate response for example boulton 2003 durance and ormerod 2007 visser et al 2017 and visser et al 2018a propose the incorporation of time offset his to account for this effect the time offset may require fine tuning if the number of indicators cannot be sufficiently reduced in the steps below beyond this no additional work is required with a large number of his both variable redundancy and computational effort represent significant challenges in response principal component analysis pca is applied allowing only those indices which describe major aspects of the flow regime to be identified following olden and poff 2003 the most relevant indices are selected proportionally from the five facets of the flow regime described above the ecological data is then be paired with this set of ecologically relevant his 2 1 2 statistical modelling as further aspects of hydroecological relationships are understood such as ecological lag in response the likelihood of modelling errors and uncertainty is increased to account for this the proposed framework makes use of an it approach to determine the structure of the hydroecological model after visser et al 2018a the it approach provides a robust measure of both structural and parameter uncertainty see appendix a 1 as well as a measure of the statistical importance of the model parameters his a central factor in the parameterisation of the hydrological model stage 2 the application of the it approach consists of 4 steps for a more detailed discussion see appendix b 1 or visser et al 2018a to summarise 1 candidate models are evaluated with respect to the second order bias corrected akaike information criterion aicc equation b1 after burnham and anderson 2002 2 a best approximating model is inferred from a weighted combination of all the candidate models 3 the parameters are ranked such that the highest value akaike weight equation b3 represents the most important in the model 4 measures of uncertainty structural and parameter are made in the development and application of the framework the it approach was applied using the r package glmulti calcagno 2013 developed and applied in a relevant discipline see isbell et al 2011 in glmulti a genetic algorithm ga a type of optimisation that mimics biological evolution is used to select a subset of models each assessed based on the above it approach the ga incorporates an immigration operator allowing removed his to be reconsidered immigration sees the level of randomisation increase and hence the likelihood of model convergence on the global optima rather than some local optima calcagno and de mazancourt 2010 inference from a consensus of 5 replicate ga runs has been shown to greatly improve convergence calcagno and de mazancourt 2010 the multi model average is subsequently derived from this subset of models parameters where the estimate and confidence intervals are zero i e certainty that the index is not to be included are then removed in line with anderson 2007 the set of model parameters is reduced to those accounting for 95 of the cumulative information see appendix b 2 for validation of the hydroecological model see appendix c 2 2 2 stage 2 hydrological model the his identified in stage 1 represent those characteristics of the flow regime which dominate ecological response driven by climate projections fig 2 changes to these his may be determined from flow time series simulated via hydrological model climate projections input to the coupled hydrological and hydroecological model allow the impacts of climate change on hydroecological response to be determined quantitatively stage 3 this second stage of the proposed framework focusses on the parameterisation of the hydrological model clark et al 2016 highlight model parameterisation as a major source of uncertainty typically hydrological models are parameterised following a split sample calibration validation approach with calibration focussing on the goodness of fit between observed and simulated flow limitations of the approach are widely acknowledged these include westerberg et al 2011 clark et al 2016 1 bias in the model parametrisation as the result of disinformative data pelletier 1988 montanari et al 2013 2 the arbitrary nature of gof statistics and 3 equifinality beven 2006 in this proposed framework the modified covariance approach visser et al 2018b based on vogel and sankarasubramanian 2003 is applied in an attempt to address these limitations see also appendix a 1 in visser et al 2018b comparison relative to studies with similar modelling objectives the simulation of ecologically relevant his showed improvement in both model performance and consistency a further major advantage of the approach lies in the focus on identifying the region of parameter space which best captures the characteristics of the his providing a greater understanding of model suitability limitations and uncertainty 2 2 1 hydrological model to further minimise uncertainty a parsimonious lumped hydrological model should be selected in the development of the framework the daily models from the gr génie rural suite of hydrological models were considered gr4j gr5j and gr6j 4 6 free parameters the grj models have been applied in a variety of hydrological contexts examples include le moine et al 2008 perrin et al 2008 coron et al 2012 smith et al 2012 coron et al 2017 with observed moments lying outwith the simulated moments see section 2 2 2 the five and six parameter models gr5j and gr6j were rejected continuous daily time series of flow precipitation and potential evapotranspiration serve as model input the time series should be of sufficient length for validation on the climate baseline in stage 3 for example the ukcp09 baseline is 1961 1990 2 2 2 modified covariance approach the hydrological model is parameterised following the modified covariance approach as set out in visser et al 2018b in using this approach the modelling objective is not the replication of a flow time series rather it is the identification of the region of parameter space which is best able to replicate the his for a more extensive discussion of the modified covariance approach see visser et al 2018b in the application of this approach the complete parameter space of the hydrological model is sampled the number of parameter sets is dependent upon the number of free parameters and the level of uncertainty adjudged acceptable to reduce bias the parameter space should be sampled uniformly for example using sobol quasi random sequences a quasi monte carlo method caflisch 1998 the parameter sets thus established the hydrological model is run in simulation mode using observed climate data for each of the n time series the covariance between observed climate and simulated flow is calculated this is repeated for the observed flow data the his identified in stage 1 are then determined from both the observed and simulated flows prior to the selection of a parameter set it is first necessary to validate the hydrological model structure this is facilitated through plots of the observed and simulated relationships between the covariances and his the model is validated if the moments agree i e observed moments lie within the simulated moments sampled parameter space error thresholds in combination with index importance determined in stage 1 are used to identify a suitable parameter set a linear relationship between the minimum and maximum error thresholds and index importance is defined parameter sets which fall below this defined limit are rejected for additional details see section 3 case study application or visser et al 2018b the focus of the covariance approach is on the replication of specific hydrological characteristics in the catchment the his as opposed to flow consequently the hydrological model should be assessed in terms of its ability to replicate these characteristics rather than the observed flow time series indeed the replication of the time series is anticipated to be poor consistent with similar work focussed on the replication of catchment characteristics e g see seibert 2000 2 3 stage 3 projections 2 3 1 ukcp09 weather generator the ukcp09 weather generator wg was selected due to its ability to represent natural climatic variability murphy et al 2009 kay and jones 2012 this consideration of natural variability allows extraordinary low probability climatic events to be captured more effectively schlabing et al 2014 which is particularly important for ecosystems wigley 1985 the wg creates synthetic stochastic time series of climate variables based on observed climate statistics the wg is perturbed to represent future climate through the application of change factors projections are at a 5 km resolution allowing for representative simulation across smaller catchments 1000 km2 typical in the uk kilsby et al 2007 jones et al 2010 data requests are submitted using the ukcp09 web based portal http ukclimateprojections ui metoffice gov uk ui admin login php the climate variables of interest are precipitation and potential evapotranspiration note that potential evapotranspiration may also be computed from the hourly time series the cmip4 sres scenarios upon which ukcp09 was based does not assign probabilities to specific emissions scenarios wigley and raper 2001 meehl et al 2007 murphy et al 2009 consequently it is assumed that each emissions scenario is equally probable murphy et al 2009 for the validation of the wg output ukcp09 recommend comparison of the observed and baseline climate data in the form of bi monthly and seasonal plots of the mean and 95 confidence intervals for each climate variable defra 2011 to this end linear bias correction is applied bi monthly where necessary 2 3 2 baseline validation the baseline climate data is used to validate the framework the generated climate variables are input into the hydrological model generating a range of possible flow time series for each time series the important his are calculated per hydrological year season these indices can be assessed relative to the observed indices determined in stage 2 validation is through cumulative distribution and probability density functions cdf and pdf respectively comparison of the mean and 95 confidence intervals with these indices and the hydroecological model a range of possible life scores for the baseline period may be determined validation is as above if the length of the ecological time series is insufficient an alternative approach may be applied this is further considered in the application of the framework 2 3 3 future hydroecological projections simulation of future hydroecological projections life is analogous to the validation with the exception that the future climate projections serve as the input data each emissions scenario time period should be considered distinct 3 case study application the ability of the framework is both validated and demonstrated through application to a uk case study river descriptions focus on the case study specific data acquisition and preparation the subsequent analysis being as per the described framework 3 1 study area the nar represents a vulnerable and important river type groundwater fed chalk stream already subject to significant stress nrt 2012 the additional threat of climate change to its ecological potential cannot be understated it is intended that the power of this new proposed framework be illustrated in its application to this case study river the spring fed river nar rises in the norfolk chalk hills 52 749 n 0 812 e 60 m above sea level flowing west for 42 km before joining the river great ouse 52 748 n 0 394 e the formation of the fen basin and resultant dissection of the chalk created two distinct river units delineated by a significant gradient change at narborough fig 3 sear et al 2005 with a greater abundance and quality of ecological data visser 2015 the focus is on the 153 3 km2 chalk sub catchment the hydrology of the river nar is characteristic of pure chalk streams sear et al 1999 with a high base flow index 0 91 sear et al 2005 and relatively low flows mean 1 12 m3 s q10 2 03 m3 s and q90 0 49 m3 s over the available record where q10 and q90 represent the 10 and 90 flow exceedance respectively equivalent to 90th and 10th percentiles a reliance on groundwater results in a highly seasonal flow regime where aquifer recharge primarily occurs in winter months leading to a progressive rise in river flow until march april 3 2 stage 1 hydroecological model routine macroinvertebrate sampling by the environment agency and prior custodians has been ongoing since 1985 nrt 2012 from 1992 the sampling methodology follows the environment agency s standard semi quantitative protocol see murray bligh 1999 data available upon request from the environment agency 2018 only samples identified to species level and collected in the spring season amj peak of macroinvertebrate activity were considered the life scores were calculated for a total of seventy two macroinvertebrate samples 1993 2012 hydrological data was extracted from the national river flow archive 1990 2012 ceh 2018 at the marham gauge 52 678 n 0 548 e fig 3 the hydrological data was subdivided into six subsets two hydrological seasons winter ondjfm and summer amjjas and three time offsets 0 2 years a total of 63 6 ecologically relevant his were considered this was reduced to 29 through pca this reduced set of his were then paired with the life scores to create the hydroecological dataset 3 3 stage 2 hydrological model for parameterisation of the hydrological model 54 years of daily average flow recorded at the marham gauge were extracted september 1961 to 2015 the corresponding climate variables were computed from daily average rainfall and hourly temperature data at 5 midas stations in and around the catchment fig 3 met office 2016 the parameters of interest are the average daily precipitation p and potential evapotranspiration pe p is determined via the computation of the daily catchment average rainfall whilst pe is estimated from hourly temperature data using the temperature based pe model from oudin et al 2005 in order to verify the method of investigation n 100 000 parameter sets were generated using sobol sequencing the his used to parameterise the model are those indicated by the hydroecological model in stage 1 in the parameterisation of the hydrological model the minimum and maximum error were specified as 17 5 and 35 2 e r r o r m i n respectively from which the linear threshold was determined the relationship between the minimum and maximum allowable error and the relative importance of the variables covariances were assigned a relative importance of 1 3 4 stage 3 projections to address a number of the uncertainties indicated in the introduction see also appendix a 2 the ukcp09 probabilistic climate projections are used the 2050s 2040 2069 high emissions scenario a1f1 sres is considered this emissions scenario is approximately equivalent to a change in temperature of 4 3 c by 2081 2100 relative to the pre industrial period 1850 1900 riahi et al 2011 met office 2018b for demonstrative purposes the ukcp09 wg was run for the full range of 10 000 variants 4 results and discussion 4 1 stage 1 hydroecological model the hydroecological model a linear multi model average is depicted in equation 1 overleaf summaries of the his are provided in table 1 importance represents the relative weight of evidence in support of each index in the model according to it whilst the relative parameter uncertainty is the 95 confidence interval relative to the parameter estimate the underlying hydroecological processes are first considered followed by a review of the predictive ability and uncertainty associated with the hydroecological model 1 l i f e 0 07 10 r 90 l o g w t 0 0 07 r i s e m n w t 0 0 93 q 80 q 50 s t 0 0 02 q 90 q 50 s t 0 0 3 q 90 q 50 s t 1 0 11 q 70 q 50 s t 1 0 04 r e v p o s s t 1 0 5 l o g q v a r s t 1 4 1 1 underlying hydroecological processes the winter hydrological season when the chalk aquifer recharges features both the most and least important his 10r90log and risemn respectively the indicator 10r90log ratio of low flows to high flows 10th to 90th percentiles is described as an indicator of responsiveness richards 1990 the log scale of the index coupled with its importance means that there is scope for 10r90log to dominate the hydroecological model both positively and negatively fig 4 clearly illustrates that large values of 10r90log correspond with the highest life scores and vice versa it is only when 10r90log is small 0 that the other six indices contribute to life score varying high and low flows shows that the highest values of 10r90log and hence life are achieved when high flows are medium high 0 exp p 90 log q 1 given the log space the scope for a negative impact exp p 90 log q 1 is large surprisingly then whilst magnitude of flow is of importance for the recharge of the aquifer higher winter flows may actually negatively impact the macroinvertebrate community the negative sign of the hi risemn indicates a preference for a low mean rise rate in winter flows however the low importance of the index table 1 sees it consistently contribute less than 2 5 to the life score fig 4 in terms of hydrological season the summer months amjjas dominate the hydroecological model there is an indication that in the summer months consistency in flow low range variation is preferred 1 a sustained increase in flow revpos sees a large negative impact on life importance 0 80 2 though not as important logqvar similarly implicates large variation in flow and 3 minimising the range between low and median flows q70q50 q80q50 and q90q50 has an increasing effect on life looking to fig 5 four out of the five summer his are lagged s 1 essentially these indicators are influencing the health of the river two years in advance should there be a bad summer with lots of variation the consequences could be severe however the presence of the q80q50 indicator in the immediately preceding season gives some scope for improvement however it is also worth noting that the negative impacts of a bad summer would only be felt if the value of the index 10r90log was low whilst if 10r90log is largely positive or negative the preceding summers are of limited importance in terms of management it is clear that summer floods in particular could be detrimental to the health of the river perhaps representing an argument for improving connections to flood plains similarly extremely high winter flows may be harmful indicating there may be scope to abstract and store waters during the winter months for use in summer however it is worth noting that negative impacts are also a necessary component of the proper ecosystem function for example they might act as a natural reset button everard 1996 lake 2003 interestingly the majority of the indices are dimensionless with the exception of logqvar and risemn this allows for some scope for variation in flow without causing excessive damage for instance in summer a need for increased abstraction need not necessarily be a detriment to river health though this assumption ignores the other effects of decreased flow 4 1 2 predictive ability and uncertainty the predictive ability of the model is first indicated by the relative parameter uncertainty unconditional variance or 95 ci relative to the parameter estimate table 1 generally as relative parameter uncertainty increases the importance of the index decreases this is one of the advantages of the weighting of the his in the it approach visser et al 2018a the fact that the most important index 10r90log has greater uncertainty than the second most important revpos suggests that this may be the best parameterisation possible in the model with regards to the implications of parameter uncertainty further inference may be made through the consideration of the 10 000 monte carlo simulations fig 6 the plot shows that the hydroecological model performs well low interquartile range of 0 44 and relative error centred around one perfect agreement this level of uncertainty is considered satisfactory 4 2 stage 2 hydrological model fig 7 a depicts the observed and simulated relationship between the covariance of precipitation and flow ρ p q and the hi q80q50 fig 7 b depicts the same relationship for the climate variable potential evapotranspiration ρ p e q for all seven his the observed moments lie within the simulated moments validating the use of the hydrological model the capacities of the production x1 and routing x3 stores were estimated as 511 and 311 mm respectively the time elapsed for flow routing is approximately 1 17 days x2 inflow from the chalk aquifer is represented by a positive groundwater exchange coefficient x4 of 2 84 mm per day the level of agreement for all seven his is summarised in table 2 with a value of 0 8 the largest covariance relative error is for potential evapotranspiration this is considered acceptable as precipitation is considered the principal determinant of flows in the east anglia region kay et al 2013 the hi relative errors are below 11 with the exception of the least important index risemn relative error 34 overall the level of relative error in the hydrological model is considered satisfactory the impacts of the error in the index risemn are likely negligible based on the findings from the hydroecological model for standard model validation see appendix c 3 4 3 stage 3 projections 4 3 1 baseline validation the ability of the framework to reproduce the observed data hydrological and hydroecological is assessed via cdfs and pdfs for the cdf plots the observed function should situate within the boundaries of the baseline projections ideally centrally the pdf plots focus on relative error where a value of 1 0 indicates perfect agreement here the objective is on a low interquartile range iqr in the interests of concision validation of the hydrological projections centres on the index q80q50 selected both due to its high importance 0 80 and ease of interpretation ratio of moderate low flows to median flows summary tables for all seven his are available in appendix c 4 4 3 1 1 hydrological model validation the validation plots for the hi q80q50 are presented in fig 8 fig 8a d are based on the ukcp09 baseline 1961 1990 both satisfy the objectives outlined above the cdf of the observed values lies within the projections and the pdf shows a low iqr comparatively the 95 confidence interval ci appears large however given the probabilistic nature of the projections this is not unexpected the baseline interval for which ecological data is available 1986 1990 is summarised in fig 8b e the iqr is similar to the 30 year baseline with a minor improvement in the 95 ci given the limited time period the right skew of the pdf fig 8e cannot be ascribed significance on the alternative baseline 2010 2017 the cdf is notably stepped fig 8c this is reflected in the pdf fig 8f with a local maximum and a median not equivalent to one perfect agreement despite this the iqr is the lowest of the three validation plots overall for the cdf s the observations lie centrally within the probabilistic projections whilst the pdf s reveal low iqr s the plots satisfactorily validate of the use of the ukcp09 projections through the hydrological model 4 3 1 2 coupled hydrological hydroecological model validation there is no ecological data species life available for the period 1961 1990 baseline validation period however sampling and identification of macroinvertebrates to the family level was carried out during the period 1989 1990 allowing for some comparison to further address this an alternative baseline was established through consideration of the earliest time period considered by the ukcpo9 wg 2010 2039 reduced to 2010 2015 run for the medium emissions scenario for 1000 randomly sampled variants subsequently the climate variables are bias corrected relative to the observed data in this period validation plots for the hydroecological response life as predicted by the coupled model is presented in fig 9 a c for the baseline 1986 1990 and fig 9b d for the alternative baseline 2010 2017 recall that for the period 1986 1990 only family life data is available on this baseline period the cdf s fig 9a are in agreement with a small iqr for the relative error of approximately 0 1 fig 9c the somewhat swollen 95 ci may have a threefold explanation 1 family level application of the life methodology tends to underestimate hydroecological response extence et al 1999 monk et al 2012 2 the limited number of years data points and 3 the probabilistic nature of the projections for the alternative baseline 2010 2017 the cdf fig 9b is in agreement the pdf fig 9d also reveals a lower iqr relative to fig 9c as well as an improved 95 ci although the temporal range of the validation is limited both time periods are able to achieve a satisfactory level of performance thereby validating the use of the ukcp09 projections and the coupled hydrological hydroecological model the use of the coupled model is thus considered fit for purpose in application to future projections 4 3 2 future projections the ukcp09 climate projections for the 2050s time slice of the high emissions scenario were inputted to the coupled hydrological and hydroecological model the focus herein is on this hydroecological response the projections of hydroecological response are first considered relative to the baseline through the cdfs and pdfs in fig 10 looking to the means first dashed lines the projected change is relatively small however there is a consistent increase in the range of possible life scores across the distribution the increase in maximum life scores appears responsible for the majority of this change though some may also be attributes to the minimum values specifically the tails of the distribution percentile 0 375 in fig 4 it was shown that the index 10r90log was the main determinant of higher life scores it can thus be presumed that from percentile 0 75 the increase in life scores is the result of an increased stability in the ratio of high to low flows in the winter season where percentile 0 75 the five summer his are likely to dominate a further indication of increased stability of flows in the river fig 11 gives an indication of the probability of these hydroecological projections these probabilities are in line with calibrated language used by the ipcc since ar4 treut et al 2007 mastrandrea et al 2010 table 3 widening the confidence interval from about as likely as not to virtually certain increases likelihood but results in a wider estimate overall the bounds of uncertainty are relatively narrow however it is clear that the greatest confidence lies in the interquartile range rather than the tails of the distribution it should also be noted that whilst the change to maximum life scores is still in evidence the decrease in life scores at the lower distribution has disappeared indicating a lack of certainty in those projections whilst schlabing et al 2014 also observed limited changes in the central tendencies they also note that it is important to consider the tails of the distribution although these events lie outwith the probabilities indicated in fig 11 this may be justified due to the ability of the wg to capture low frequency events dubrovský et al 2004 mehrotra and sharma 2007 as in schlabing et al 2014 fig 12 looks to the hydroecological response at the 5th and 95th percentiles it is important to note that life scores at these percentiles will be primarily determined through the winter hi 10r90log at the 5th percentile life scores 4 5 account for only 0 016 of observations and are therefore omitted broadly speaking the frequency of lower life scores appears to decrease under the future projections consequently there is almost a 10 increase in the number of life scores equal to 7 for the 95th percentile life scores 9 5 account for 0 0244 of the total observations and are therefore omitted with a marked increase in the frequency of life 8 the positive change in hydroecological response previously observed figs 10 and 11 is clearly in evidence 4 4 implications for the case study river the proposed framework has indicated a clear hydroecological response to the projected changed climate under the a1f1 high emissions scenario in the 2050s however the magnitude and direction of change is projected to be both small and positive the scale of this change is in line with the ukcp09 projections for the east anglia region under this scenario in this region the projected change in mean annual precipitation is small ranging from 5 across the 10th to 90th percentiles met office 2018 note that kay et al 2013 observe that hydrological response in east anglia is principally determined by precipitation further the range of life values is known to be small particularly in the iqr based on the biosys database of ecological data across 548 catchments in england environment agency 2018 the iqr for 546 catchments is 1 therefore the observed change signal may be presumed to represent a true change in community structure figs 10 and 11 indicated a clustering of life scores visually this is most clear with the 1 375 increase in the mode fig 10b this is reflected in the summary statistics for example the kurtosis of the life score distribution increases from 16 6 to 18 4 this flattening of the hydroecological response is a possible indication of a reduction in biodiversity if this were to be the case this would increase the vulnerability of the river overall monocultures being far more susceptible to local extinction further the reduced frequency of events where life scores fall very low could impact negatively upon the river robbing the ecosystem of vital natural reset events everard 1996 lake 2003 4 5 framework limitations limitations of the framework centre around the assumptions of stationarity and data availability in climate change modelling projections are often based on historic climate with the assumption that the statistical properties of the climate remain stationary this assumption is inherited under both hydroclimatic and coupled modelling the corollary is an enforced assumption that ecological response remains the same as it is now consequently at this time it is not possible to account for the adaptive response of the riverine community a barrier to hydroecological studies has been the lack of paired long term hydrological and ecological time series monk et al 2007 2012 this problem persisted in the development of the hydroecological model for example in the uk routine macroinvertebrate sampling began circa 1990 murray bligh 1992 therefore given the baseline of 1960 1990 validation is limited to address this an alternative baseline was derived the use of climate projections with a more up to date baseline for example the soon to be released ukcp18 projections or a wg trained using cmip5 or cmip6 climate data would also address this 5 conclusions the implications for flow regime make rivers among those ecosystems most sensitive to climate change death et al 2015 watts et al 2015 whilst studies have attempted to assess the impact of climate change on hydroecological response methods are often qualitative or follow quantitative methods of limited scope the resulting lack of clarity renders the fallout for ecosystem services effectively an unknown in answer the proposed framework provides a quantitative approach developed using methods which minimise uncertainty in line with recommendations in clark et al 2016 the ability of the framework has been illustrated through application to a case study river the projected hydroecological response in april june the period of peak mi activity in the river is considered under the a1f1 high emissions scenario in the 2050s the hydroecological response is in line with climate projections for the east anglia region the projections indicate that a reduction in biodiversity is virtually certain a possible disruption to low flow processes essential to ecosystem functioning is less strongly indicated it should be noted that whilst the projected hydroecological change may be limited the river nar is strongly influenced by groundwater bfi 0 91 consequently the impact of changes in precipitation may be reduced thus greater change in response might be expected in catchments where surface runoff dominates in summary the proposed framework serves as a new and dynamic tool with the potential to provide valuable information in the pursuit of more accurate assessments of the impact of climate change on river ecosystems critically and possibly uniquely in the field bennett et al 2013 the end user will also be provided with a quantifiable measure of uncertainty in the hydroecological projections further applications of the framework include water resources planning and future environmental flow management in recent years hydroecological modelling is often undertaken using a regime based spatial framework for example monk et al 2011 zhang et al 2012 in a similar manner the proposed framework may be extended to cover multiple rivers of similar flow regime classification such generic projections of the impact of climate change on hydroecological response might thus be used to plan wider adaption measures including for ungauged rivers where appropriate the projections may also be used to assess the implications of climate change on the provision of instream ecosystem services e g through the framework set out in ncube et al 2018 data availability consent has not been given to share the data used in this study however these data are freely available from the original sources the environment agency ea 2018 macroinvertebrate sampling records met office 2016 climate precipitation and temperature national river flow archive ceh 2018 gauged flow at marham and data requests for the climate projections may be submitted using the ukcp09 web based portal http ukclimateprojections ui metoffice gov uk ui admin login php acknowledgements the authors gratefully acknowledge funding from the engineering and physical science research council through award 1786424 further thanks go to the environment agency and the centre for ecology and hydrology for the provision of data appendix d supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix d supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 01 004 appendix a uncertainty a 1 stages 1 and 2 table a1 types and sources of uncertainty that are addressed in stages 1 and 2 hydroecological and hydrological modelling of the proposed framework table a1 stage type of uncertainty source controls 1 sampling and measurement error input data standardised methodology for the collection of macroinvertebrate samples case study specific quality control checks on observed flow data case study specific variability climate internal variability length of observed time series covering range of climatic periods wet dry case study specific structural uncertainty model selection pca addresses parameter redundancy it genetic algorithm searches for global optimum rather than local multi model average not a single best model parameter uncertainty model selection and parameterisation confidence intervals weight of supporting evidence referred to as importance 2 measurement error input data quality control checks on observed flow and climatic data the modified covariance approach does not calibrate based on goodness of fit statistics thereby reducing the bias of the parameterised model see also ref myself variability climate internal variability length of observed time series covering range of climatic periods case study specific structural uncertainty model structure the modified covariance approach rejects model structures if the observed and simulated moments do not coincide parameter uncertainty parameterisation the modified covariance approach focusses on replicating the essential characteristics of the catchment explicitly the relative importance of the his is known error thresholds for each hi may be specified accordingly equifinality the modified covariance approach considers the full parameter space which is narrowed down to a small region which is able to best replicate the his of interest a 2 stage 3 climate projections climate change projections are central to the application of the proposed framework it is recommended that probabilistic climate projections which consider a range of impacts be used when applying the proposed framework in the case study application the ukcp09 probabilistic climate change projections are used specifically the weather generator product the application of the framework is not limited to ukcp09 other sources of probabilistic climate change projections include the comepro project in the mediterranean region kaspar ott et al 2016 ring et al 2018 the mit igsm cam framework monier et al 2013a applied over northern eurasia monier et al 2013b and ukcp18 the next iteration of uk climate projections based on the research concentration pathways from ar5 met office 2018a equally projections may be produced via weather generator may be used directly for example the vector autoregressive weather generator schlabing et al 2014 the ukcp09 identifies three major sources of uncertainties in their climate projections murphy et al 2010 epistemic uncertainty incomplete understanding of climate system processes internal climate variability and scenario uncertainty a summary of the controls introduced in ukcp09 to minimise this uncertainty is detailed in table a2 table a2 sources of uncertainty in the ukcp09 weather generator climate projections and the controls in place to minimise this based on murphy et al 2010 table a2 source controls epistemic uncertainty a perturbed physics ensemble of the variance clark et al 2016 e g different mathematical representation of the processes interactions and feedbacks variability multiple runs with the same initial conditions for each ensemble weather generator simulations based on statistical characteristics in the observed data simulations pick up more extreme climatic events schlabing et al 2014 scenario uncertainty there is a lack of agreement in how relative probability should be assigned to emissions scenario to address this ukcp09 presents three emissions scenarios low medium and high appendix b information theory b 1 model evaluation although the overall concept of information theoretics may be unknown to the reader certain aspects should be familiar based on deep theoretical foundations burnham and anderson 2001 p 244 the concept and application are conspicuously simple candidate models are evaluated over three steps 1 measuring the information lost in each approximating model 2 determination of the evidence in support of each model and 3 multi model inference of a final model structure from the candidate set step 1 loss of information from model f kullback leibler k l gives a measure of the amount of information that is lost when model g is used to approximate reality f a model which loses the least information i e has the most supporting evidence out of the candidates can be considered the best approximation of reality the information loss i f g is determined through computation of information criteria ic a multitude of ic exist some of which with the reader is undoubtedly familiar the akaike information criterion aic represents the standard estimate of kullback leibler information burnham and anderson 2002 in hydroecological modelling the sample size n is often small relative to the number of variables k a second order bias corrected version of aic aicc can account for this through the addition of a second penalty burnham and anderson 2002 b1 a i c c a i c 2 k k 1 n k 1 step 2 evidence in support of model g i the value of aic is dependent on the scale of the data the goal is to achieve the smallest loss of information given the data this difference is rescaled and ranked relative to a i c m i n b2 δ i a i c c i a i c c min f o r i 1 2 r the value of δ i may be interpreted through a rule of thumb based on likelihood intervals δ i 2 there is substantial supporting evidence for model g i 4 δ i 7 the models are not as competitive and if δ i 10 it can be assumed that there is strong evidence against model g i burnham and anderson 2002 from this measure of evidence the likelihood that model g i is the best approximating model can be determined this is known as the akaike weight w ranging from 1 to 0 for the most and least likely models respectively b3 w i e x p 1 2 δ i r 1 r exp 1 2 δ r the use of the akaike weight allows for clearer inference when considering the candidate models step 3 multi model inference when using information theory model selection the best approximating model is inferred across the entire candidate set this is achieved through consideration of a weighted combination of all candidates parameter averages θ ˆ are simply the sum of the akaike weights for each model that contains the predictor θ ˆ b4 θ ˆ i 1 r w i θ ˆ i as a result the parameter averages are ranked such that the highest value represents the most important in the model this eliminates the problem of multiple equally plausible models with different parameter structures equifinality b 2 application using glmulti the package glmulti streamlines the above steps into a single function calcagno 2013 the fundamentals of the algorithm and approach are available in calcagno and de mazancourt 2010 a subset of models was determined using the function glmulti the function was applied five times to ensure convergence to a consensus of model subsets with the function coef applied to determine the it multi model average the number of indices is reduced by removing those indices where both coefficient and standard error are zero and to within the 95 confidence interval by ordering by descending importance importancei cumsum importance 0 95 this is illustrated in table b1 overleaf table b1 the structure of the hydroecological model prior to final removals detailed above hydrological seasons are indicated by w winter and s summer the facets of the flow regime are denoted as m magnitude and r rate of change the removed indices occupy the final five rows reasons indicated in bold table b1 season time offset index facet definition coefficient importance unconditional variance cumulative evidence weight 0 intercept 7 64 1 2 36 0 1 w 0 10r90log m log ratio 10th 90th percentile flows 0 07 0 86 0 0 27 2 w 0 risemn r mean rise rate in flow 0 07 0 07 0 05 0 95 3 s 0 q80q50 m q80 flows relative to median 0 93 0 51 1 3 0 68 4 s 1 logqvar m variance in log flows 0 5 0 37 0 57 0 8 5 s 1 q90q50 m q90 flows relative to median 0 3 0 19 0 28 0 86 6 s 1 q70q50 m q70 flows relative to median 0 11 0 09 0 05 0 93 7 s 1 revpos r number days when flow is increasing positive reversals 0 04 0 8 0 0 52 w 0 25r75log m log ratio 25th 75th percentile flows 0 0 12 0 0 9 w 0 20r80log m log ratio 20th 80th percentile flows 0 0 05 0 0 97 w 2 maxq50 m maximum flow relative to median 0 0 04 0 0 98 w 2 maxmonthlymed m median maximum flow relative to median flow across all years 0 0 03 0 1 s 0 q90q50 m q90 flows relative to median 0 02 0 04 0 01 0 99 appendix c case study c 1 hydroecological model index contribution the contribution of each hi to hydroecological response was determined using the baseline data 1961 1990 each of the 10 000 wg variants and hydrological year were considered independently for each of these iterations the relative contribution of the hi was determined c1 i n d e x v a l u e i n d e x c o e f f i c i e n t i n d e x v a l u e i n d e x c o e f f i c i e n t c 2 hydroecological model validation with observed data data represents a key limiting factor to hydroecological modelling with long term 15 20 years macroinvertebrate community data at the species level uncommon monk et al 2012 consequently the length of the time series in hydroecological modelling is insufficient for split sampling calibration and validation this is commonplace in hydroecological modelling monk et al 2012 environment agency 2018 the exploration of the model uncertainty serves as one approach to address the validation further validation is considered through comparison of simulated species life scores 1991 2017 to three data sources summarised in table c1 see figure c1 for validation the following should be considered when interpreting figure c1 as discussed previously life score differences across taxonomic level are inevitable differences in life scores of the same taxonomic level are due to known errors within the biosys records biosys stat that corrections to address these inconsistencies are in progress environment agency 2018 april 1995 september 1997 saw extremely low rainfall leading to errors in the recording of low flows nrfa 2014 this discrepancy may be the reason for the differences in observed and simulated values it should be noted that the hydroecological model was not trained on data marked as suspect this data is included in figure c1 to allow for the fitting of trendlines table c1 data sources considered for the additional validation of the hydroecological model years of data excluding training data 1993 2012 table c1 raw mi data provided taxonomic level source of life score years yes species this study 2013 2014 yes family this study 1986 2014 no family the freshwater and marine biological surveys england archive known as biosys environment agency 2018 1991 2018 fig c1 comparison of life scores across data sources see table c1 trend lines are fitted for each data source with the exception of species life 2013 2014 insufficient data fig c1 the two dashed lines represent the comparison of the training data light green and model simulations dark green the similarity in the slope of the lines indicates a high level of agreement in life scores only two additional species life scores are available 2013 and 2014 solid blue circles it can be seen that these values are consistent with the observed training data trendline light green dashed line two sources of observed family life scores are available see above for notes on differences between the data sources the slope of the trendline for family life scores determined as part of this study solid blue line is similar to the observed training data the underestimation of life scores may be attributed to the difference in taxonomic level for the ea biosys data a good level of agreement is again indicated though it can be seen that the validity of the model improves over time c 3 hydrological model validation with observed data fig c2 validation of the hydrological model using observed data a comparison of the mean and 95 confidence interval 2 standard deviations b probability density function of the relative error a value of one indicates perfect agreement between model observations fig c2 c 4 hydrological model validation with projections differences in the 95 ci for the index 10r90log are the result of a single outlier observation during the 30 year time period note that the other two time periods are 4 and 7 years in length it is also observed that the impact on hydroecological response reduces as 10r90log increases to more extreme values 10 specifically table c2 validation of the framework using the ukcp09 projections on the baseline table c2 index time period lower quantile median upper quantile 95 ci 95 ci 10r90log 1961 1990 1 39 0 15 2 17 96 58 96 88 10r90log 1986 1990 1 35 0 01 0 81 27 83 27 86 10r90log 2010 2017 0 17 0 05 0 43 19 47 19 58 revpos 1961 1990 0 86 1 1 2 0 28 1 72 revpos 1986 1990 0 82 0 9 1 0 65 1 15 revpos 2010 2017 0 85 0 95 1 07 0 59 1 32 q80q50 1961 1990 0 9 1 01 1 13 0 65 1 37 q80q50 1986 1990 0 9 0 99 1 06 0 76 1 22 q80q50 2010 2017 0 98 1 05 1 13 0 78 1 32 logqvar 1961 1990 0 57 0 95 1 59 1 28 3 17 logqvar 1986 1990 0 55 0 92 1 49 0 75 2 58 logqvar 2010 2017 0 34 0 58 1 06 1 24 2 41 q90q50 1961 1990 0 87 1 02 1 19 0 56 1 48 q90q50 1986 1990 0 86 0 98 1 08 0 67 1 28 q90q50 2010 2017 0 92 1 02 1 09 0 71 1 33 q70q50 1961 1990 0 92 1 01 1 1 0 71 1 3 q70q50 1986 1990 0 92 0 98 1 04 0 8 1 16 q70q50 2010 2017 0 96 1 02 1 13 0 76 1 28 risemn 1961 1990 0 65 1 08 1 89 1 36 3 53 risemn 1986 1990 0 56 0 93 1 36 0 38 2 24 risemn 2010 2017 0 87 1 43 2 45 1 79 4 66 
26249,sensitivity analysis provides information on the relative importance of model input parameters and assumptions it is distinct from uncertainty analysis which addresses the question how uncertain is the prediction uncertainty analysis needs to map what a model does when selected input assumptions and parameters are left free to vary over their range of existence and this is equally true of a sensitivity analysis despite this many uncertainty and sensitivity analyses still explore the input space moving along one dimensional corridors leaving space of the input factors mostly unexplored our extensive systematic literature review shows that many highly cited papers 42 in the present analysis fail the elementary requirement to properly explore the space of the input factors the results while discipline dependent point to a worrying lack of standards and recognized good practices we end by exploring possible reasons for this problem and suggest some guidelines for proper use of the methods 1 introduction mathematical models have become increasingly prominent tools in decision making processes in engineering science economics and policy making among other applications driven by increasing computing power coupled with the abundance of available data models have also become increasingly complex examples include large climate or economic models which aim to include ever more processes at an ever higher resolution however this increased complexity requires much more information to be specified as model inputs parameters and other assumptions used in the model construction and typically this information is not well known it is therefore essential to understand the impact of these uncertainties on the model output if the model is to be used effectively and responsibly in any decision making process sensitivity analysis sa and uncertainty analysis ua are the two main tools used in exploring the uncertainty of such models one definition of sensitivity analysis is the study of how the uncertainty in the output of a model numerical or otherwise can be apportioned to different sources of uncertainty in the model input saltelli 2002 as such it is very much related to but distinct from uncertainty analysis ua which as we define it here characterizes the uncertainty in model prediction without identifying which assumptions are primarily responsible uncertainty analysis can include a broad range of applications relating to uncertainty a very thorough reference can be found in ghanem et al 2017 ideally an uncertainty analysis precedes a sensitivity analysis before uncertainty can be apportioned it needs to be estimated however this is not necessarily the case and applications involving model calibration optimisation may not require the quantification of uncertainty other taxonomies are also possible relating ua to sa see e g razavi et al 2019 although for the purpose of the present work we remain with the definitions above before proceeding let us clarify terminology in building a model a number of things must be specified including the type and structure of model parameters resolution calibration data and so forth see fig 1 each of these has an associated uncertainty and is therefore an assumption in a quantitative analysis of uncertainty we can only investigate vary a subset of these assumptions this subset we call the input factors note that this includes all items varied in a sa or ua i e model parameters as well as any other types of assumption that will be varied in performing any uncertainty and sensitivity analysis it is crucial to keep in mind that the uncertainty in the assumptions that are outside the set of input factors will not be explored nearing and gupta 2018 saltelli et al 2013 the results of the model for any values of the input factors we call the model output focusing now on the uncertainty in the input factors alone if the model is deterministic then assessing the uncertainty in the output boils down to propagating the uncertainty from the input factors to the output for example by repeatedly running the model using different values for the uncertain inputs within their plausible ranges this can be done with a monte carlo simulation or with some ad hoc design to generate a distribution of possible model results the grey area in fig 1 characterising the output distribution e g by constructing it empirically from the output data points constitutes an uncertainty analysis the ua may also involve extracting summary statistics such as the mean median and variance from this distribution and possibly by assigning confidence bounds e g on the mean once this is done the next step could be to use sensitivity analysis to assign this uncertainty to the input factors sensitivity analysis allows us to infer that for example this factor alone is responsible for 70 of the uncertainty in the output sensitivity analysis is used for many purposes primarily it is used as a tool to quantify the contributions of model inputs or sub groups of inputs to the uncertainty in the model output examples of such applications include eisenhower o neill narayanan fonoberov and mezić 2012 and becker et al 2012 this use of sensitivity analysis will be the focus of the present paper in this uncertainty setting typical objectives are to identify which input factors contribute the most to model uncertainty factor prioritisation so that further information might be collected about these parameters to reduce model uncertainty or to identify factors which contribute very little and can potentially be fixed factor fixing saltelli and tarantola 2002 other applications that are not necessarily related to uncertainty are for example in engineering design where design sensitivity analysis is used as a tool for structural optimisation allaire et al 2004 sensitivity analysis can also be used to better understand processes within models and thereby the natural systems on which they are based becker et al 2011 or as a quality assurance tool an unexpected strong dependence of the output upon an input deemed irrelevant might either illuminate the analyst on an unexpected feature of the system or reveal a conceptual or coding error the importance of sensitivity analysis is widely acknowledged sensitivity analysis is prescribed in national and international guidelines in the context of impact assessment e g european commission 2009 office of management and budget 2006 u s environmental protection agency epa 2009 when the output of a model feeds into policy prescription and planning a sensitivity analysis would appear as an essential element of due diligence despite the clear importance of sensitivity analysis there are a number of problems observed in practical sensitivity analysis and uncertainty analysis which can be found in all fields of research these problems range from confusions in terminology to statistically inaccurate techniques which can perhaps dangerously underestimate model uncertainty specifically while most practitioners of sa distinguish it from ua modellers overall tend to conflate the two terms e g performing an uncertainty analysis and calling it a sensitivity analysis the sensitivity analysis methodology often relies on so called local techniques which are invalid for nonlinear models one of the main aims of this paper is to back up these assertions with evidence demonstrating that there is a systematic problem in practical sensitivity analysis might be a first step towards improving the situation some reviews of sensitivity analysis practice do already exist in ferretti et al 2016 an assessment of the state of sensitivity analysis was performed using a bibliometric approach shin et al 2013 review the state of sensitivity analysis or lack thereof in hydrological modelling however to the authors knowledge there is no detailed cross disciplinary assessment of the state of sensitivity analysis as practised by modellers accordingly this paper has the following objectives to assess the state of sensitivity analysis across a range of academic disciplines we do this by a systematic review of a large number of highly cited papers in which sensitivity analysis is the focus in some respect to discuss based on this review known problems and misinterpretations of sensitivity analysis why these might occur and propose some ideas for how these problems might be addressed following these objectives in section 2 we outline in more detail what we consider to be the basic requirements of a valid sensitivity analysis as well as explaining commonly observed problems in section 3 we outline a procedure for systematically selecting highly cited sensitivity analysis papers across a range of disciplines and criteria for review the results of this systematic review are presented in section 4 which is followed by a discussion on the root of the problems observed with some suggestions to improve the situation section 6 reports our main conclusions 2 common pitfalls of sensitivity analysis there are a range of practical problems and methodological difficulties associated with sensitivity analysis here we highlight two particular issues which we believe are particularly prevalent and could be addressed the first is a simple issue of terminology many scientists conflate the meaning of sa and ua in a large class of instances e g in economics sa is understood as an analysis of the robustness of the prediction ua this is perhaps due to an influential econometric paper leamer 1985 entitled sensitivity analysis would help whose problem setting and motivation were to ensure the robustness of a regression analysis with respect to various modelling choices e g in the selection of regressors as a result in economics and finance it is common to see the expression sensitivity analysis used to mean what we have defined here as uncertainty analysis clearly this can have an impact on the quality of an uncertainty and sensitivity analysis if the objectives are not even clear the second issue is that modellers tend to change factors one at a time instead of globally possibly as a result of their training and methodological disposition to think in terms of derivatives here we explore this technical issue in more depth many practitioners accept a taxonomy of sensitivity analysis based on distinguishing between local and global methods saltelli et al 2008 let f be a generic black box representation of a model which has input factors x x 1 x 2 x k and a scalar output y such that y f x a local method in its simplest form yields the partial derivative of the model with respect to one of its input factors i e y x i two notable deficiencies of this definition of sensitivity are that first if f is nonlinear with respect to x i then its partial derivative will change depending on where in the range of x i you choose to measure second and more generally if there are interactions between model inputs then y x i will change depending on the values of the remaining input factors as well in short first partial derivatives are only a valid measure of sensitivity when the model is linear in which case y x i will remain constant for any x a common variation of the first partial derivative is usually referred to as the one at a time oat approach let x i be the nominal value of the ith input factor now define y i max f x 1 x 2 x i max x k as the model output where all input factors are at nominal values except the ith which is set to its maximum an oat sensitivity measure is e g δ i y i max y i min x i max x i min where y i min follows a similar definition the oat approach and partial derivatives which are a type of oat approach keep all other input factors fixed except the one that is being perturbed from here on we use the term oat to refer to both local sensitivity analysis approaches and oat of the type discussed in the preceding paragraph a global sensitivity analysis method at the other extreme could be an analysis of variance anova as usually taught in experimental design which informs the analyst about factors global influence in terms of their contribution to the variance of the model output including the effect of interactions among factors box et al 2005 perhaps the most prevalent example of a global measure is the first order sensitivity index sobol 1993 s i v x i e x i y x i v y where v y is the unconditional variance of y obtained when all factors x i are allowed to vary and e x i y x i is the mean of y when one factor is fixed incidentally this measure was originally proposed by karl pearson to measure nonlinear dependence between random variables pearson 1905 the first order sensitivity index is part of a class of sensitivity measures which are called variance based its meaning under the assumption of independence between input factors can be expressed in plain english s i is the expected fractional reduction in the variance of y that would be achieved if factor x i could be fixed s i 1 implies that all of the variance of y is driven by x i and hence that fixing it also uniquely determines y other global approaches to sensitivity analysis include the elementary effects approach morris 1991 global derivative based measures sobol kucherenko 2009 moment independent methods da veiga 2015 variogram based approaches razavi et al 2019 and many others a further discussion of the theory of sensitivity indices is beyond the scope of this paper and the reader is referred e g to saltelli et al 2008 and ghanem et al 2017 global approaches are requisite to performing a valid sensitivity analysis when models feature nonlinearities and interactions to understand the issue it is helpful to think of the set of all possible combinations of input factors as an input space for example with two model inputs any combination of values could be marked as a point on a two dimensional plane with the range of factor 1 on one axis and the range of factor 2 on the other in the case of three input factors the input space would be a cube and for higher numbers a hypercube fig 2 left illustrates an oat design with two input factors and a corresponding global design right that might be used to estimate the global measures discussed in the previous section evidently oat designs cannot effectively explore a multidimensional space we can further illustrate this with a simple example taken from saltelli and annoni 2010 imagine that the input space is a three dimensional cube of side one moving one factor at a time by a distance of ½ away from the centre of the cube generates points on the faces of the cube but never on its corners all these points are in fact on the surface of a sphere internal and tangent to the cube as illustrated in fig 3 the volume of the sphere divided by the volume of the cube is about ½ if we increase the number of dimensions this ratio goes towards zero very quickly in ten dimensions the volume of the hypersphere divided by the volume of the hypercube is 0 0025 one fourth of one percent in practice it is even more restrictive than that because the oat design does not even explore inside the hypersphere and is limited to a hypercross in other words moving factors oat in ten dimensions leaves over 99 75 of the input space totally unexplored this under exploration of the input space directly translates into a deficient sensitivity analysis and is but one of the many incarnations of the so called curse of dimensionality and the reason why an oat sa is perfunctory unless the model is proven to be linear statisticians are well acquainted with this problem this is why in the theory of experimental design box et al 2005 factors are moved in groups rather than oat to optimize the exploration of the space of the factors in sensitivity analysis global designs are either based on random quasi random or space filling designs see fig 2 right or on oat designs that are repeated in multiple locations of the input space the latter are used for e g global derivative based measures monte carlo estimation of variance based sensitivity indices and elementary effects among others 3 meta analysis in order to understand the prevalence and type of sensitivity analysis across different fields and to understand the extent of the issues discussed in the previous section an extensive literature review a meta study was carried out the review was based on highly cited articles that have a focus on sensitivity analysis the reasoning here was that the most highly cited articles should represent on average commonest practice relative to that field therefore by analysing these papers we should be able to conclude with reasonable confidence that the rigour of sensitivity analysis in a given field is at or below the level of its top cited papers 3 1 selection procedure the literature search was conducted on the scopus database in order to identify relevant papers the following search criteria were used after a few iterations of analysis and refinement 1 1 exact query specifications available in the additional online material retrieved from https www scopus com between march and may 2017 first the strings sensitivity analysis and model modelling and uncertainty were required to be present in the title abstract or keywords this ensures that the paper has a significant focus on sensitivity analysis that it is related to mathematical models and concerns uncertainty as opposed to e g design sensitivity analysis and optimisation which is a separate topic second the papers were restricted to the years 2012 2017 in order to provide a sample of recent research finally the results were required to be journal articles and in english the latter for ease of reviewing this search resulted in around 6000 articles the search query is deliberately restrictive in that sensitivity analysis articles exist that do not mention model in the abstract title or keywords for example however it was considered to be an unbiased way of automatically selecting sensitivity analysis papers across fields preliminary attempts indicated that simply mentioning sensitivity analysis yielded far too many irrelevant articles around 47 000 the sample here therefore can be considered as representative but the numbers of papers returned are significantly below the true number of sensitivity analysis papers in the literature each paper returned by the search is tagged using one or more subject identifiers subject areas with less than 100 articles meeting the search criteria of which there were eight were not examined in this study the resulting 19 subject areas are as follows agrbiosci agricultural and biological sciences biochemgenmbio biochemistry genetics and molecular biology busmanacc business management and accounting chemi chemistry chemeng chemical engineering compsci computer science decsci decisional science earthsci earth and planetary sciences econfin economy and finance energy energy engineering engineering envsci environmental science immunmicrobio immunology and microbiology matsci material science math math medicine medicine phartox pharmacology and toxicology physastro physics and astronomy socsci social science in order to provide a manageable sample of articles for review the top twenty most cited papers from each field were selected since most papers include more than one subject identifier some papers featured in more than one of the top twenty lists the reviewing was distributed between the authors of the present article even though the initial search criteria had been refined to focus on model related sensitivity analysis a total of 44 papers had to be discarded as not including a sensitivity analysis nor an uncertainty analysis or because they reported an analysis of the dependence of the output upon just one factor which does not constitute a sensitivity analysis a total of 280 papers were finally retained for the analysis though in total 324 papers were reviewed a limitation of this selection procedure is that older papers are more likely to be well cited see e g davis and cochran 2015 therefore the distribution of papers reviewed will be biased towards older articles our results confirm this bias however our reasoning is that first it is only after a few years that it is possible to reliably identify influential well cited papers from less influential ones so it would be very difficult to identify influential papers only from 2017 for example moreover we believe that highly cited older papers will be used as a benchmark by many researchers to guide their methodology so highly cited papers even if a few years old can still be used as an indicator of the state of sensitivity analysis in a given field 3 2 review criteria each paper was reviewed against a set of simple criteria as follows 1 was an uncertainty analysis performed if so was a global or local approach used 2 was a sensitivity analysis performed if so was a global or local approach used 3 was the paper primarily focused on the method of sensitivity analysis or on the model application 4 was the model used linear nonlinear or was it unclear these criteria are explained in more detail below additional to these criteria some general notes on each paper were taken 3 2 1 oat global uncertainty and sensitivity analysis the identification of oat and global sensitivity analyses is one of the focal points of this study in reviewing each paper we noted whether an uncertainty analysis or sensitivity analysis had been performed or both for both the uncertainty and sensitivity analysis we checked to see if the results had been generated using global or oat methods as discussed in section 3 2 as discussed we define oat methods as all approaches where factors are moved only one at a time even when derivatives are computed efficiently such as when using the adjoint method cacuci 2005 note that some methods such as that in sobol kucherenko 2009 or in morris 1991 are based on derivatives but are classified as global methods because they sample partial derivatives or incremental ratios at multiple locations in the input space we have defined as global any approach that is based on moving factors together such as in design of experiment doe a monte carlo analysis followed by an analysis of the scatterplots of y versus the various input factors x i is also classified as global albeit qualitative as well as approaches based on regression coefficients of y versus the x i the use of sobol sensitivity indices independently of the way these are computed screening methods such as the method of morris monte carlo filtering various methods known as moment independent and so on see saltelli et al 2008 for a description and the additional online material for the methods met in the papers reviewed useful recent reviews are norton 2015 pianosi et al 2016 one might wonder what an oat uncertainty analysis looks like in fact some papers quantify uncertainty by observing y i max and y i min for each input factor during an oat experiment and assign the range of uncertainty on y as y min y max where y min min i y i min and similarly for y i max clearly this ignores the additional uncertainty in y when more than one factor at a time is set to its maximum or minimum values 3 2 2 method model it is useful to make a distinction between method and model focused papers model focused papers are defined as those which focus on a model and use sensitivity analysis as a tool to investigate uncertainty or other aspects of the model the primary conclusions of the paper are therefore related to the model these types of paper will often have a greater impact on the application which is ultimately the outcome of concern for example in assessing the uncertainty sensitivity of climate models or other models used in decision making method focused papers are those that introduce sensitivity analysis methodology and use a model as a case study to demonstrate the new approach conclusions are therefore focused on the performance of the method and results relating to the model are of secondary interest typically the authors are familiar with sensitivity analysis techniques which allows them to propose new approaches these papers are more likely to feature high quality sensitivity analysis techniques 3 2 3 model linearity finally since oat approaches are only valid in the case of a linear model each paper was assessed to see if the application model was demonstrably linear or not in many cases this was unclear but where it was possible to ascertain linearity this was recorded 4 results the full results of this study including the scoring matrix as well as the authors review notes are given in the additional online material and a summary table is given in the appendix 4 1 prevalence across disciplines fig 4 shows the distribution of sensitivity analysis papers across research fields by density number of sa papers divided by the total number in the search period and by number given that model use is pervasive in the disciplines investigated these densities are very low even accounting for the fact that not all sensitivity analysis papers will have been picked up by the search this observation is indeed supported in investigations focusing on one discipline such as hydrology shin et al 2013 the greatest density of papers is found in decision science as well as model intensive subjects such as earth sciences environmental science and energy the greatest raw numbers are found in environmental science engineering and medicine although the latter does not have a high density due to the very large overall research output note that articles can be tagged with more than one subject identifier 4 2 uncertainty analysis although as discussed uncertainty analysis and sensitivity analysis are distinct but related disciplines in the literature the term sensitivity analysis is sometimes used to describe both terms as a result the set of papers reviewed also included number of papers that were concerned with pure ua indeed of the 280 papers reviewed 24 did not contain any kind of sensitivity analysis and instead only concerned uncertainty analysis these represent clear conflations of sensitivity and uncertainty analysis table 1 reports the occurrence of ua found in the literature review in about ¾ of papers there was either no ua present or the methodology was not clearly specified the former is due to the fact that our search query specifically targeted sensitivity analysis papers so it is unsurprising that there are a large proportion of papers with little attention given to the ua part on the other hand about ¾ of the uas that were observed were global in nature this is most likely because a monte carlo analysis randomly sampling from input distributions is fairly intuitive and accessible to most researchers whereas an oat uncertainty analysis is arguably less intuitive the same analysis can be applied by subject area see fig 5 here we see that uncertainty analysis was found much more commonly in pharmacology and toxicology and medicine within the papers that we reviewed than social sciences and computer science for example this should not be taken as an overall indication of the quantity of uncertainty analysis because our sample has overwhelmingly targeted sensitivity analysis papers however it indicates that in pharmacology and toxicology and medicine either it is particularly common to perform ua simultaneously with sa or the terms are confused taking the case of pharmacology and toxicology we find that of the papers reviewed only four had a sensitivity analysis whereas ten had an uncertainty analysis this flags that sensitivity analysis may often refer to uncertainty analysis within this field on the other hand a quite prevalent trend in some fields is the practice of performing a global ua i e via a monte carlo analysis side by side with an oat sa this was observed in particular in medicine and in economics finance in medicine for example it seems to be common to perform an oat sensitivity analysis presenting the results in a tornado plot a bar chart which shows the effect on the output of varying each assumption by a fixed amount in either direction we speculate that the authors involved were unaware of the chance to use elementary scatterplots of the output versus the input to rank the factors by importance or simply they did not find this kind of analysis relevant or useful in any case once a certain practice becomes established within a given field i e found in highly cited papers it sets a strong precedent which is difficult to supersede researchers and reviewers not unreasonably assume that if a method is found in influential articles then it must be correct 4 3 global vs local sa turning now to sensitivity analysis table 1 shows that 41 of sensitivity analyses use global methods with 34 using oat methods and 25 having an unclear method type or no sensitivity analysis present this is encouraging in that nearly half of studies use global methods still at least one third of highly cited papers matching our search criteria use deficient oat methods fig 6 shows that the distribution of global methods varies widely across disciplines immunology and microbiology show more than 70 of papers featuring global methods this is followed by disciplines that are fairly model intensive such as material science biochemistry computer science and engineering at the other end of the spectrum pharmacology and toxicology and business management and accounting have very low proportions of global sa about 10 and 20 respectively perhaps surprisingly some disciplines that tend to rely heavily on large computer models such as earth science and environmental science still feature quite low rates of global sensitivity analysis this is a concern particularly when large budget models are used for making significant decisions such as climate models in policy making see a discussion in saltelli et al 2015 on the other hand other model heavy subjects such as engineering and materials science have higher ratios yet it is worth recalling that even engineering has only around a half of confirmed global approaches and these are the most highly cited articles as a complement to the manual literature review we also investigated the prevalence of ua and sa methods based purely on text mining by identifying at least one known global sensitivity analysis technique i e variance based metamodeling elementary effects etc in keeping with the methodology of a previous paper from some of the present authors ferretti et al 2016 fig 7 shows the results of that paper as extended to 2015 and 2016 the original analysis stopped at 2014 this is a rougher approach but allows the inclusion of a much larger number of papers here it would seem that an even smaller fraction of papers that feature sensitivity analysis adopts a global sa approach at least three reasons explain the difference with the results in the present paper first as has been well established here sensitivity analysis is often also used to indicate uncertainty analysis so that the upper curve in fig 7 shows a mixture of ua and sa as well as an inevitable share of papers not pertaining to mathematical modelling secondly the estimation of the number of global sa papers is likely an underestimate because papers may apply simpler global methods e g a scatterplot based analysis but not necessarily refer to the articles or techniques listed finally in the manual literature review we focus only on highly cited papers which should ideally be of a higher standard than the average in a given field 4 4 method and model focus table 1 shows that most papers are unsurprisingly focused on the application i e on the model at hand and not on the methods of the total of 280 papers 35 were methodological i e having sa ua methods as their subject of these 24 advocate the use of global methods on the one hand this is encouraging because it shows that global methods are being promoted on the other hand a small but significant fraction of methodological papers are still advising statistically incorrect oat methods we note among the method papers a marked preference for variance based measures of sensitivity such as the sensitivity indices of which the pearson correlation ratio discussed previously is a special case we also see an active line of research in moment independent methods borgonovo et al 2012 4 5 model linearity as discussed if a model is linear an oat or derivative based approach is adequate however the linearity or nonlinearity of the model is rarely evident at least from the manuscripts table 1 shows the proportions of linear and nonlinear models only in 8 of the cases were we able to conclude that the model was definitely linear whereas over half of papers included clearly nonlinear models with the remainder being unclear this demonstrates that first researchers tend to work with nonlinear models second in the large majority of cases global methods are essential to perform a methodologically sound sensitivity analysis 5 discussion 5 1 reasons for bad practice the results of this study clearly show that there are serious methodological deficiencies in highly cited papers in most if not all disciplines why is this so often the case we speculate that this is due to at least five reasons which we outline here first sensitivity analysis is intrinsically attached to modelling which itself is not a unified subject indeed modelling typically requires a set of skills learned through experience and hence includes elements of craft as much as of science rosen 1991 as such every discipline goes about modelling following local disciplinary standards and practices padilla et al 2018 similarly sensitivity analysis practice is found in largely isolated pockets attached to each modelling discipline this fragmentation hinders development of the subject and spreading of good practice while simultaneously allowing malpractice to survive relatively unchallenged this issue is discussed in more depth in the following section a second point is that most scientists conflate the meaning of sa and ua if the meaning of sensitivity analysis is not even understood it is unsurprising that the quality of sensitivity analysis is sometimes lacking third global sensitivity analysis unavoidably requires a good background in statistics to implement and to interpret results some researchers simply haven t enough knowledge and training in statistics and consequently the cost in time and money required to learn and understand the necessary techniques may be considered prohibitive more generally researchers may not even be aware that global sensitivity analysis techniques exist under these circumstances it seems that researchers often revert to the more intuitive oat approach among other things it offers an ease of interpretation in moving just one input factor the change observed in the model output must come from that input alone moreover global methods may be discouraging in that the more factors that are moved the higher the chance that the model will crash or misbehave note that this is precisely the reason why a global sa is a good instrument of model verification it is unusual to run a global sa without detecting model errors modellers call this jokingly lubarsky s law of cybernetic entomology according to which there is always one more bug fourth although mature global sensitivity analysis methods have been around for more than 25 years this still may not be enough time for established good practice to filter down into the many research fields in which modelling is used this may be partly due to a lack of comparative examples across a range of fields moreover researchers tend to emulate methods found in highly cited papers assuming that they are best practice which as this study has demonstrated are often methodologically deficient finally as noted in leamer 2010 the reluctance to take up these methods may be due to their candour a proper method by honestly propagating all of the input uncertainty may lead to an inconveniently wide distribution of the output of interest for example a cost benefit analysis reporting a distribution encompassing possible large losses as well as large gains may not be what the owner of the problem wishes to hear this is the same as to say that the volatility of the inference is exposed and thus is the insufficiency of the evidence according to leamer 2010 as well as to funtowicz and ravetz 1990 this situation may induce modellers to massage the uncertainty in the input factors so that the output falls in a more desirable zone for cases where a considerable asymmetry exists between model developers and users jakeman et al 2006 it might be advisable to resort to sensitivity auditing an extension of sensitivity analysis beyond parametric analysis to include an assessment of the entire knowledge and model generating process for policy related cases saltelli et al 2013 to assess the credibility of degree of uncertainty attributed to each input factor and to make sure that the uncertainty has been neither inflated nor deflated to achieve a desired end inflation and deflation of uncertainty are quite common in e g regulatory controversies typically the regulated tend to inflate uncertainty so as to deter regulation while the opposite is the case for regulators michaels 2008 sensitivity auditing s seven point checklist is recommended by the european commission guidelines for impact assessment european commission 2009 p 393 5 2 isolated communities the scattered state of sensitivity analysis practice merits some further discussion if modelling is a non standardised discipline padilla et al 2018 the same holds a fortiori for uncertainty and sensitivity analysis hence the difficulty for good practices to establish themselves researchers from different fields have difficulties to communicate with one another in a transversal topic such as sa that is practised across a wide range of scientific and modelling disciplines robert rosen a system ecologist tackles the specificities of modelling in the scientific method in his work life itself rosen 1991 here he suggests that when a model is built to represent a natural system we should look at the play of causality the argument is that the natural system is kept together rosen uses the word entailed by material efficient and final causality in contrast the formal system i e the model is only internally entailed by formal causality rosen uses here the four causality categories of aristotle on which we will not dwell here to highlight that no arrow of causality flows from the natural system to the formal one in other words the act of encoding fig 8 is not driven by causality which would fix the model specification but is driven by the needs and the craft of the modeller the implication is that different modelling teams given the same data can produce altogether different models and inference refsgaard van der sluijs brown and van der keur 2006 thus the success of the modelling operation is judged by the usefulness or otherwise of the insights made possible by the operation of decoding which is another way of saying that all models are wrong but some are useful according to an aphorism attributed to george box models thus depend crucially upon craftmanship of the modellers this together with the diversity of modelling applications motives and constraints explain why modelling never became an independent discipline in our opinion this contributes to explaining why modelling is so discipline specific as noted by padilla et al 2018 the spread in modelling practices and cultures may be one of the reasons why methodologies which are ancillary to modelling such as uncertainty and sensitivity analysis are not part of a standardised syllabus being taught across disciplines and are at times ignored even in communities proficient in modelling such as for example hydrology shin et al 2013 despite the fragmentation of sensitivity and uncertainty analysis some cross disciplinary networks exist one such community might be said to have formed around a series of samo conferences for sensitivity analysis of model output see http samo2016 univ reunion fr samo has been held every three years since 1995 this community is active in training and dissemination however samo by no means captures the full spectrum of practitioners interested in uncertainty and sensitivity analysis for example in the united states sa related activities are under the heading of verification validation and uncertainty quantification vvuq for which a journal of the american society of mechanical engineers is available http verification asmedigitalcollection asme org journal aspx other sensitivity analysis related gatherings include the conference on uncertainty quantification organised by the society for industrial and applied mathematics the international conference on uncertainty quantification in computational sciences and engineering organised by the european community on computational methods in applied sciences and sessions in thematic conferences such as the uncertainty in structural dynamics conference organised by department of mechanical engineering of the ku leuven or the session on advances in diagnostics sensitivity and uncertainty analysis of earth and environmental systems models organised annually at the european geosciences union conference in vienna despite these communities the majority of practitioners remain scattered in isolated pockets and sensitivity analysis is hence not part of a recognized syllabus who or what scientific forum can then decide if a method is a good or a bad practice to make an example in nearing and gupta 2018 stark and saltelli 2018 who can authoritatively discourage modellers from over interpreting the results from multi model ensembles as if they were a random sample from a distribution this question remains for the time being unanswered a possible solution to this unsatisfactory state of affairs would be that statistics as a discipline takes responsibility for statistical methods for model validation and verification this would not make modelling into a discipline but would go a long way toward improving modelling practice additionally most if not all the tools of sensitivity analysis are statistical in nature this thesis has been suggested in a discussion paper entitled should statistics rescue mathematical modelling saltelli 2018 5 3 parallels with the p value the systematic problems observed in sensitivity analysis share similarities with the recent crisis in statistics over the p value a paper published in 2005 ioannidis 2005 warned about the poor quality of most published research results the paper was taken up by the media and the periodical the economist devoted its cover to the issue in 2013 how science goes wrong 2013 with a full article describing the subtleties of use and misuse of statistics in deciding about the significance of scientific results the specific subject of concern was the use of the p value the probability under a specified statistical model that a statistical summary of the data e g the sample mean difference between two compared groups would be equal to or more extreme than its observed value wasserstein and lazar 2016 the p value is used as a fundamental tool by researchers to decide if a given result is just the result of chance or indeed an effect worth publishing in 2016 the pressure surrounding the statistical community was so high that the american statistical association felt the need to intervene with a statement wasserstein and lazar 2016 to clarify how the test should be used useful reading on the topic are colquhoun 2014 gigerenzer and marewski 2014 stark and saltelli 2018 these articles show a complex mix of causes from poor training to bad incentives which result in the generalized failure in the use of the p value evidenced by attempts to repeat published results see e g shanks et al 2015 the problem is seen as a combination of confirmation bias authors looking for the effect they presume will be there confirmation bias or authors desperate to publish a positive result publish or perish of p hacking changing the setup of the study or the composition of the sample till an effect emerges and harking formulating the research hypothesis after the results are known kerr 1998 the latter involves repeatedly running comparison tests between different combinations of variables until a significant result is found which violates the conditions of applicability of the p test overall it is clear that the consequences of bad statistics can be dramatic for example when wrong cures for cancer are identified at the pre clinical stage of research and are then passed on to the clinical trial phase begley and ellis 2012 similarly it is not difficult to imagine the consequences of a wrong or missing uncertainty and sensitivity analyses given the pervasive role of models in risk analysis this can lead to ignoring dangerous operating conditions for a facility in decision analysis this can lead to wrong investments or policies a simple sensitivity analysis run on the formula used for the pricing of the complex derivative products at the root of the sub prime mortgage crisis would have revealed the fragility of the formula salmon 2009 wilmott and orrell 2017 whether the quants the experts in charge of these mathematical constructs wanted to know this fragility is of course another story finally a missing uncertainty analysis allows audacious risk or cost benefit analysis to be run over centennial time scales while a proper ua would show clearly that the uncertainties are too big to conclude anything an example discussed in saltelli et al 2015 was the computing the increased crime rate due to increased temperature at the year 2100 5 4 recommendations for best practice it is outside of the scope of this paper to give a detailed guide to sensitivity analysis for thorough references readers are referred to saltelli et al 2008 or ghanem et al 2017 nevertheless and although considerable differences exist in the use of sensitivity analysis among disciplines all fields would benefit from the adoption of good practices our personal list of preferences which agrees with the methodological papers seen in this review would include the following recommendations both uncertainty and sensitivity analysis should be based on a global exploration of the space of input factors be it using an experimental design monte carlo or other ad hoc designs the discussion in this paper has demonstrated that local oat methods do not adequately represent models with nonlinearities with some exceptions it is advisable to perform both uncertainty and sensitivity analysis once an analyst has performed an uncertainty analysis and is informed of the robustness of the inference it would appear natural to ascertain where volatility uncertainty is coming from at the other extreme a sensitivity analysis without uncertainty analysis is usually illogical the relative importance of a factor on the model output has a different relevance depending on whether the output has a small or large variance however there are cases for instance studies to identify the dominant effects on the output for a subsequent model reduction or calibration analysis where the analyst may be satisfied with a pure sa sensitivity and uncertainty analysis should be focused on a question most models have many outputs and these outputs can be used to answer a range of different questions the relationship sensitivity between the input factors and each different model output can be very different for this reason it is essential to focus the sensitivity analysis on the question addressed by the model rather than more generally on the model when sensitivity analysis is performed it should allow the relative importance of input factors and combinations of factors to be assessed either visually scatterplots or quantitatively regression coefficients sensitivity measures or other sensitivity and uncertainty analysis are themselves uncertain because there is considerable uncertainty in quantifying the uncertainty in input factors and modellers should be frank about how they arrived at the supposed uncertainties saltelli et al 2013 this should be kept in mind and efforts made to capture the uncertainty of input assumptions as accurately as possible even an apparently perfect uncertainty and sensitivity analysis is no assurance against error as noted by pilkey and pilkey jarvis 2009 it is important to recognize that the sensitivity of the parameter in the equation is what is being determined not the sensitivity of the parameter in nature if the model is wrong or if it is a poor representation of reality determining the sensitivity of an individual parameter in the model is a meaningless pursuit as regards what method should be used our preference is for methods which are exploratory model independent able to capture interactions and to treat a group of factors a carefully performed uncertainty analysis followed by sensitivity analysis is an important ingredient of the quality assurance of a model as well as a necessary condition for any model based analysis or inference 6 conclusions the main message of the present work is that a carefully performed sensitivity analysis is an important ingredient of the quality assurance of a model as well as a necessary condition for any model based analysis or inference however such analyses are not common enough and often inaccurate indicating that action is urgent on the front of quality assurance procedures for mathematical models in particular a significant fraction of papers investigated use sensitivity analysis approaches which fail elementary considerations of experimental design and do not properly explore the space of the input factors with the result that uncertainty is generally underestimated and sensitivity is wrongly estimated up to 65 of the reviewed highly cited papers are based on inadequate methods i e varying one input factor at a time although even in the most generous interpretation where all models with unclear linearity are assumed linear still over 20 of papers contain inadequate methodology further a significant number of papers confuse sensitivity and uncertainty analysis which is likely to exacerbate the problem with spreading good practice the fact that these figures concern highly cited papers has two implications first if we assume that highly cited papers represent the upper end of methodological rigour in a given field then the overall problem may be even worse second these are some of the most visible papers in their field and are used as guides for best practice therefore they can promote continued deficient methodology in our opinion the problem with sensitivity analysis is partly attributable to the fact that mathematical modelling is not a discipline in its own right and every branch of science and technology approaches modelling following its own culture and practice uncertainty and sensitivity analyses are likewise orphans of a disciplinary home one can also note that signals of distress as to the quality of mathematical modelling are heard from different disciplines from economics reinert 2000 romer 2015 to natural sciences oreskes 2000 oreskes et al 1994 pilkey and pilkey jarvis 2009 the situation has worrying analogies with what we have witnessed in data analysis where misuse of the p value colquhoun 2014 has been singled out as one of the reasons of the present reproducibility crisis affecting science ioannidis 2005 saltelli and funtowicz 2017 the importance of this analogy is in the warning it sounds for the credibility of science if such pervasive weaknesses in methodology are not addressed the need to heed this warning in the case of sensitivity and uncertainty analysis is becoming increasingly urgent appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 01 012 annex table 2 shows the results of the reviews in a condensed form the meaning of the headings is given in section 3 table 2 summary of results by subject identifier table 2 category method model linearity paper focus total reviewed global sa oat sa global ua oat ua other unclear linear nonlinear unclear method model agrbiosci 15 11 6 0 6 1 22 4 3 24 27 biochemgenmbio 23 15 6 1 7 2 19 15 0 36 36 busmanacc 4 7 5 5 1 1 18 2 3 18 21 chemi 10 8 2 0 5 0 17 5 1 21 22 chemeng 12 12 4 0 5 0 16 12 1 27 28 compsci 21 9 1 1 2 8 16 6 11 22 33 decsci 9 7 3 4 0 2 20 1 7 15 22 earthsci 11 13 4 1 17 5 13 24 2 41 43 econfin 5 8 6 3 0 1 16 1 0 18 18 energy 14 15 3 4 2 3 17 16 0 36 36 engineering 38 16 5 5 5 3 51 11 3 62 65 envsci 31 22 14 4 16 6 44 24 11 67 78 immunmicrobio 19 7 3 0 5 2 6 13 0 21 21 math 21 15 3 2 6 4 24 13 11 29 40 matsci 13 4 1 1 0 0 16 2 0 18 18 medicine 26 30 25 4 13 2 24 37 2 62 64 phartox 2 2 9 1 3 1 11 5 1 18 19 physastro 13 9 4 0 0 1 20 2 2 21 23 socsci 10 5 0 4 2 1 14 5 6 15 21 
26249,sensitivity analysis provides information on the relative importance of model input parameters and assumptions it is distinct from uncertainty analysis which addresses the question how uncertain is the prediction uncertainty analysis needs to map what a model does when selected input assumptions and parameters are left free to vary over their range of existence and this is equally true of a sensitivity analysis despite this many uncertainty and sensitivity analyses still explore the input space moving along one dimensional corridors leaving space of the input factors mostly unexplored our extensive systematic literature review shows that many highly cited papers 42 in the present analysis fail the elementary requirement to properly explore the space of the input factors the results while discipline dependent point to a worrying lack of standards and recognized good practices we end by exploring possible reasons for this problem and suggest some guidelines for proper use of the methods 1 introduction mathematical models have become increasingly prominent tools in decision making processes in engineering science economics and policy making among other applications driven by increasing computing power coupled with the abundance of available data models have also become increasingly complex examples include large climate or economic models which aim to include ever more processes at an ever higher resolution however this increased complexity requires much more information to be specified as model inputs parameters and other assumptions used in the model construction and typically this information is not well known it is therefore essential to understand the impact of these uncertainties on the model output if the model is to be used effectively and responsibly in any decision making process sensitivity analysis sa and uncertainty analysis ua are the two main tools used in exploring the uncertainty of such models one definition of sensitivity analysis is the study of how the uncertainty in the output of a model numerical or otherwise can be apportioned to different sources of uncertainty in the model input saltelli 2002 as such it is very much related to but distinct from uncertainty analysis ua which as we define it here characterizes the uncertainty in model prediction without identifying which assumptions are primarily responsible uncertainty analysis can include a broad range of applications relating to uncertainty a very thorough reference can be found in ghanem et al 2017 ideally an uncertainty analysis precedes a sensitivity analysis before uncertainty can be apportioned it needs to be estimated however this is not necessarily the case and applications involving model calibration optimisation may not require the quantification of uncertainty other taxonomies are also possible relating ua to sa see e g razavi et al 2019 although for the purpose of the present work we remain with the definitions above before proceeding let us clarify terminology in building a model a number of things must be specified including the type and structure of model parameters resolution calibration data and so forth see fig 1 each of these has an associated uncertainty and is therefore an assumption in a quantitative analysis of uncertainty we can only investigate vary a subset of these assumptions this subset we call the input factors note that this includes all items varied in a sa or ua i e model parameters as well as any other types of assumption that will be varied in performing any uncertainty and sensitivity analysis it is crucial to keep in mind that the uncertainty in the assumptions that are outside the set of input factors will not be explored nearing and gupta 2018 saltelli et al 2013 the results of the model for any values of the input factors we call the model output focusing now on the uncertainty in the input factors alone if the model is deterministic then assessing the uncertainty in the output boils down to propagating the uncertainty from the input factors to the output for example by repeatedly running the model using different values for the uncertain inputs within their plausible ranges this can be done with a monte carlo simulation or with some ad hoc design to generate a distribution of possible model results the grey area in fig 1 characterising the output distribution e g by constructing it empirically from the output data points constitutes an uncertainty analysis the ua may also involve extracting summary statistics such as the mean median and variance from this distribution and possibly by assigning confidence bounds e g on the mean once this is done the next step could be to use sensitivity analysis to assign this uncertainty to the input factors sensitivity analysis allows us to infer that for example this factor alone is responsible for 70 of the uncertainty in the output sensitivity analysis is used for many purposes primarily it is used as a tool to quantify the contributions of model inputs or sub groups of inputs to the uncertainty in the model output examples of such applications include eisenhower o neill narayanan fonoberov and mezić 2012 and becker et al 2012 this use of sensitivity analysis will be the focus of the present paper in this uncertainty setting typical objectives are to identify which input factors contribute the most to model uncertainty factor prioritisation so that further information might be collected about these parameters to reduce model uncertainty or to identify factors which contribute very little and can potentially be fixed factor fixing saltelli and tarantola 2002 other applications that are not necessarily related to uncertainty are for example in engineering design where design sensitivity analysis is used as a tool for structural optimisation allaire et al 2004 sensitivity analysis can also be used to better understand processes within models and thereby the natural systems on which they are based becker et al 2011 or as a quality assurance tool an unexpected strong dependence of the output upon an input deemed irrelevant might either illuminate the analyst on an unexpected feature of the system or reveal a conceptual or coding error the importance of sensitivity analysis is widely acknowledged sensitivity analysis is prescribed in national and international guidelines in the context of impact assessment e g european commission 2009 office of management and budget 2006 u s environmental protection agency epa 2009 when the output of a model feeds into policy prescription and planning a sensitivity analysis would appear as an essential element of due diligence despite the clear importance of sensitivity analysis there are a number of problems observed in practical sensitivity analysis and uncertainty analysis which can be found in all fields of research these problems range from confusions in terminology to statistically inaccurate techniques which can perhaps dangerously underestimate model uncertainty specifically while most practitioners of sa distinguish it from ua modellers overall tend to conflate the two terms e g performing an uncertainty analysis and calling it a sensitivity analysis the sensitivity analysis methodology often relies on so called local techniques which are invalid for nonlinear models one of the main aims of this paper is to back up these assertions with evidence demonstrating that there is a systematic problem in practical sensitivity analysis might be a first step towards improving the situation some reviews of sensitivity analysis practice do already exist in ferretti et al 2016 an assessment of the state of sensitivity analysis was performed using a bibliometric approach shin et al 2013 review the state of sensitivity analysis or lack thereof in hydrological modelling however to the authors knowledge there is no detailed cross disciplinary assessment of the state of sensitivity analysis as practised by modellers accordingly this paper has the following objectives to assess the state of sensitivity analysis across a range of academic disciplines we do this by a systematic review of a large number of highly cited papers in which sensitivity analysis is the focus in some respect to discuss based on this review known problems and misinterpretations of sensitivity analysis why these might occur and propose some ideas for how these problems might be addressed following these objectives in section 2 we outline in more detail what we consider to be the basic requirements of a valid sensitivity analysis as well as explaining commonly observed problems in section 3 we outline a procedure for systematically selecting highly cited sensitivity analysis papers across a range of disciplines and criteria for review the results of this systematic review are presented in section 4 which is followed by a discussion on the root of the problems observed with some suggestions to improve the situation section 6 reports our main conclusions 2 common pitfalls of sensitivity analysis there are a range of practical problems and methodological difficulties associated with sensitivity analysis here we highlight two particular issues which we believe are particularly prevalent and could be addressed the first is a simple issue of terminology many scientists conflate the meaning of sa and ua in a large class of instances e g in economics sa is understood as an analysis of the robustness of the prediction ua this is perhaps due to an influential econometric paper leamer 1985 entitled sensitivity analysis would help whose problem setting and motivation were to ensure the robustness of a regression analysis with respect to various modelling choices e g in the selection of regressors as a result in economics and finance it is common to see the expression sensitivity analysis used to mean what we have defined here as uncertainty analysis clearly this can have an impact on the quality of an uncertainty and sensitivity analysis if the objectives are not even clear the second issue is that modellers tend to change factors one at a time instead of globally possibly as a result of their training and methodological disposition to think in terms of derivatives here we explore this technical issue in more depth many practitioners accept a taxonomy of sensitivity analysis based on distinguishing between local and global methods saltelli et al 2008 let f be a generic black box representation of a model which has input factors x x 1 x 2 x k and a scalar output y such that y f x a local method in its simplest form yields the partial derivative of the model with respect to one of its input factors i e y x i two notable deficiencies of this definition of sensitivity are that first if f is nonlinear with respect to x i then its partial derivative will change depending on where in the range of x i you choose to measure second and more generally if there are interactions between model inputs then y x i will change depending on the values of the remaining input factors as well in short first partial derivatives are only a valid measure of sensitivity when the model is linear in which case y x i will remain constant for any x a common variation of the first partial derivative is usually referred to as the one at a time oat approach let x i be the nominal value of the ith input factor now define y i max f x 1 x 2 x i max x k as the model output where all input factors are at nominal values except the ith which is set to its maximum an oat sensitivity measure is e g δ i y i max y i min x i max x i min where y i min follows a similar definition the oat approach and partial derivatives which are a type of oat approach keep all other input factors fixed except the one that is being perturbed from here on we use the term oat to refer to both local sensitivity analysis approaches and oat of the type discussed in the preceding paragraph a global sensitivity analysis method at the other extreme could be an analysis of variance anova as usually taught in experimental design which informs the analyst about factors global influence in terms of their contribution to the variance of the model output including the effect of interactions among factors box et al 2005 perhaps the most prevalent example of a global measure is the first order sensitivity index sobol 1993 s i v x i e x i y x i v y where v y is the unconditional variance of y obtained when all factors x i are allowed to vary and e x i y x i is the mean of y when one factor is fixed incidentally this measure was originally proposed by karl pearson to measure nonlinear dependence between random variables pearson 1905 the first order sensitivity index is part of a class of sensitivity measures which are called variance based its meaning under the assumption of independence between input factors can be expressed in plain english s i is the expected fractional reduction in the variance of y that would be achieved if factor x i could be fixed s i 1 implies that all of the variance of y is driven by x i and hence that fixing it also uniquely determines y other global approaches to sensitivity analysis include the elementary effects approach morris 1991 global derivative based measures sobol kucherenko 2009 moment independent methods da veiga 2015 variogram based approaches razavi et al 2019 and many others a further discussion of the theory of sensitivity indices is beyond the scope of this paper and the reader is referred e g to saltelli et al 2008 and ghanem et al 2017 global approaches are requisite to performing a valid sensitivity analysis when models feature nonlinearities and interactions to understand the issue it is helpful to think of the set of all possible combinations of input factors as an input space for example with two model inputs any combination of values could be marked as a point on a two dimensional plane with the range of factor 1 on one axis and the range of factor 2 on the other in the case of three input factors the input space would be a cube and for higher numbers a hypercube fig 2 left illustrates an oat design with two input factors and a corresponding global design right that might be used to estimate the global measures discussed in the previous section evidently oat designs cannot effectively explore a multidimensional space we can further illustrate this with a simple example taken from saltelli and annoni 2010 imagine that the input space is a three dimensional cube of side one moving one factor at a time by a distance of ½ away from the centre of the cube generates points on the faces of the cube but never on its corners all these points are in fact on the surface of a sphere internal and tangent to the cube as illustrated in fig 3 the volume of the sphere divided by the volume of the cube is about ½ if we increase the number of dimensions this ratio goes towards zero very quickly in ten dimensions the volume of the hypersphere divided by the volume of the hypercube is 0 0025 one fourth of one percent in practice it is even more restrictive than that because the oat design does not even explore inside the hypersphere and is limited to a hypercross in other words moving factors oat in ten dimensions leaves over 99 75 of the input space totally unexplored this under exploration of the input space directly translates into a deficient sensitivity analysis and is but one of the many incarnations of the so called curse of dimensionality and the reason why an oat sa is perfunctory unless the model is proven to be linear statisticians are well acquainted with this problem this is why in the theory of experimental design box et al 2005 factors are moved in groups rather than oat to optimize the exploration of the space of the factors in sensitivity analysis global designs are either based on random quasi random or space filling designs see fig 2 right or on oat designs that are repeated in multiple locations of the input space the latter are used for e g global derivative based measures monte carlo estimation of variance based sensitivity indices and elementary effects among others 3 meta analysis in order to understand the prevalence and type of sensitivity analysis across different fields and to understand the extent of the issues discussed in the previous section an extensive literature review a meta study was carried out the review was based on highly cited articles that have a focus on sensitivity analysis the reasoning here was that the most highly cited articles should represent on average commonest practice relative to that field therefore by analysing these papers we should be able to conclude with reasonable confidence that the rigour of sensitivity analysis in a given field is at or below the level of its top cited papers 3 1 selection procedure the literature search was conducted on the scopus database in order to identify relevant papers the following search criteria were used after a few iterations of analysis and refinement 1 1 exact query specifications available in the additional online material retrieved from https www scopus com between march and may 2017 first the strings sensitivity analysis and model modelling and uncertainty were required to be present in the title abstract or keywords this ensures that the paper has a significant focus on sensitivity analysis that it is related to mathematical models and concerns uncertainty as opposed to e g design sensitivity analysis and optimisation which is a separate topic second the papers were restricted to the years 2012 2017 in order to provide a sample of recent research finally the results were required to be journal articles and in english the latter for ease of reviewing this search resulted in around 6000 articles the search query is deliberately restrictive in that sensitivity analysis articles exist that do not mention model in the abstract title or keywords for example however it was considered to be an unbiased way of automatically selecting sensitivity analysis papers across fields preliminary attempts indicated that simply mentioning sensitivity analysis yielded far too many irrelevant articles around 47 000 the sample here therefore can be considered as representative but the numbers of papers returned are significantly below the true number of sensitivity analysis papers in the literature each paper returned by the search is tagged using one or more subject identifiers subject areas with less than 100 articles meeting the search criteria of which there were eight were not examined in this study the resulting 19 subject areas are as follows agrbiosci agricultural and biological sciences biochemgenmbio biochemistry genetics and molecular biology busmanacc business management and accounting chemi chemistry chemeng chemical engineering compsci computer science decsci decisional science earthsci earth and planetary sciences econfin economy and finance energy energy engineering engineering envsci environmental science immunmicrobio immunology and microbiology matsci material science math math medicine medicine phartox pharmacology and toxicology physastro physics and astronomy socsci social science in order to provide a manageable sample of articles for review the top twenty most cited papers from each field were selected since most papers include more than one subject identifier some papers featured in more than one of the top twenty lists the reviewing was distributed between the authors of the present article even though the initial search criteria had been refined to focus on model related sensitivity analysis a total of 44 papers had to be discarded as not including a sensitivity analysis nor an uncertainty analysis or because they reported an analysis of the dependence of the output upon just one factor which does not constitute a sensitivity analysis a total of 280 papers were finally retained for the analysis though in total 324 papers were reviewed a limitation of this selection procedure is that older papers are more likely to be well cited see e g davis and cochran 2015 therefore the distribution of papers reviewed will be biased towards older articles our results confirm this bias however our reasoning is that first it is only after a few years that it is possible to reliably identify influential well cited papers from less influential ones so it would be very difficult to identify influential papers only from 2017 for example moreover we believe that highly cited older papers will be used as a benchmark by many researchers to guide their methodology so highly cited papers even if a few years old can still be used as an indicator of the state of sensitivity analysis in a given field 3 2 review criteria each paper was reviewed against a set of simple criteria as follows 1 was an uncertainty analysis performed if so was a global or local approach used 2 was a sensitivity analysis performed if so was a global or local approach used 3 was the paper primarily focused on the method of sensitivity analysis or on the model application 4 was the model used linear nonlinear or was it unclear these criteria are explained in more detail below additional to these criteria some general notes on each paper were taken 3 2 1 oat global uncertainty and sensitivity analysis the identification of oat and global sensitivity analyses is one of the focal points of this study in reviewing each paper we noted whether an uncertainty analysis or sensitivity analysis had been performed or both for both the uncertainty and sensitivity analysis we checked to see if the results had been generated using global or oat methods as discussed in section 3 2 as discussed we define oat methods as all approaches where factors are moved only one at a time even when derivatives are computed efficiently such as when using the adjoint method cacuci 2005 note that some methods such as that in sobol kucherenko 2009 or in morris 1991 are based on derivatives but are classified as global methods because they sample partial derivatives or incremental ratios at multiple locations in the input space we have defined as global any approach that is based on moving factors together such as in design of experiment doe a monte carlo analysis followed by an analysis of the scatterplots of y versus the various input factors x i is also classified as global albeit qualitative as well as approaches based on regression coefficients of y versus the x i the use of sobol sensitivity indices independently of the way these are computed screening methods such as the method of morris monte carlo filtering various methods known as moment independent and so on see saltelli et al 2008 for a description and the additional online material for the methods met in the papers reviewed useful recent reviews are norton 2015 pianosi et al 2016 one might wonder what an oat uncertainty analysis looks like in fact some papers quantify uncertainty by observing y i max and y i min for each input factor during an oat experiment and assign the range of uncertainty on y as y min y max where y min min i y i min and similarly for y i max clearly this ignores the additional uncertainty in y when more than one factor at a time is set to its maximum or minimum values 3 2 2 method model it is useful to make a distinction between method and model focused papers model focused papers are defined as those which focus on a model and use sensitivity analysis as a tool to investigate uncertainty or other aspects of the model the primary conclusions of the paper are therefore related to the model these types of paper will often have a greater impact on the application which is ultimately the outcome of concern for example in assessing the uncertainty sensitivity of climate models or other models used in decision making method focused papers are those that introduce sensitivity analysis methodology and use a model as a case study to demonstrate the new approach conclusions are therefore focused on the performance of the method and results relating to the model are of secondary interest typically the authors are familiar with sensitivity analysis techniques which allows them to propose new approaches these papers are more likely to feature high quality sensitivity analysis techniques 3 2 3 model linearity finally since oat approaches are only valid in the case of a linear model each paper was assessed to see if the application model was demonstrably linear or not in many cases this was unclear but where it was possible to ascertain linearity this was recorded 4 results the full results of this study including the scoring matrix as well as the authors review notes are given in the additional online material and a summary table is given in the appendix 4 1 prevalence across disciplines fig 4 shows the distribution of sensitivity analysis papers across research fields by density number of sa papers divided by the total number in the search period and by number given that model use is pervasive in the disciplines investigated these densities are very low even accounting for the fact that not all sensitivity analysis papers will have been picked up by the search this observation is indeed supported in investigations focusing on one discipline such as hydrology shin et al 2013 the greatest density of papers is found in decision science as well as model intensive subjects such as earth sciences environmental science and energy the greatest raw numbers are found in environmental science engineering and medicine although the latter does not have a high density due to the very large overall research output note that articles can be tagged with more than one subject identifier 4 2 uncertainty analysis although as discussed uncertainty analysis and sensitivity analysis are distinct but related disciplines in the literature the term sensitivity analysis is sometimes used to describe both terms as a result the set of papers reviewed also included number of papers that were concerned with pure ua indeed of the 280 papers reviewed 24 did not contain any kind of sensitivity analysis and instead only concerned uncertainty analysis these represent clear conflations of sensitivity and uncertainty analysis table 1 reports the occurrence of ua found in the literature review in about ¾ of papers there was either no ua present or the methodology was not clearly specified the former is due to the fact that our search query specifically targeted sensitivity analysis papers so it is unsurprising that there are a large proportion of papers with little attention given to the ua part on the other hand about ¾ of the uas that were observed were global in nature this is most likely because a monte carlo analysis randomly sampling from input distributions is fairly intuitive and accessible to most researchers whereas an oat uncertainty analysis is arguably less intuitive the same analysis can be applied by subject area see fig 5 here we see that uncertainty analysis was found much more commonly in pharmacology and toxicology and medicine within the papers that we reviewed than social sciences and computer science for example this should not be taken as an overall indication of the quantity of uncertainty analysis because our sample has overwhelmingly targeted sensitivity analysis papers however it indicates that in pharmacology and toxicology and medicine either it is particularly common to perform ua simultaneously with sa or the terms are confused taking the case of pharmacology and toxicology we find that of the papers reviewed only four had a sensitivity analysis whereas ten had an uncertainty analysis this flags that sensitivity analysis may often refer to uncertainty analysis within this field on the other hand a quite prevalent trend in some fields is the practice of performing a global ua i e via a monte carlo analysis side by side with an oat sa this was observed in particular in medicine and in economics finance in medicine for example it seems to be common to perform an oat sensitivity analysis presenting the results in a tornado plot a bar chart which shows the effect on the output of varying each assumption by a fixed amount in either direction we speculate that the authors involved were unaware of the chance to use elementary scatterplots of the output versus the input to rank the factors by importance or simply they did not find this kind of analysis relevant or useful in any case once a certain practice becomes established within a given field i e found in highly cited papers it sets a strong precedent which is difficult to supersede researchers and reviewers not unreasonably assume that if a method is found in influential articles then it must be correct 4 3 global vs local sa turning now to sensitivity analysis table 1 shows that 41 of sensitivity analyses use global methods with 34 using oat methods and 25 having an unclear method type or no sensitivity analysis present this is encouraging in that nearly half of studies use global methods still at least one third of highly cited papers matching our search criteria use deficient oat methods fig 6 shows that the distribution of global methods varies widely across disciplines immunology and microbiology show more than 70 of papers featuring global methods this is followed by disciplines that are fairly model intensive such as material science biochemistry computer science and engineering at the other end of the spectrum pharmacology and toxicology and business management and accounting have very low proportions of global sa about 10 and 20 respectively perhaps surprisingly some disciplines that tend to rely heavily on large computer models such as earth science and environmental science still feature quite low rates of global sensitivity analysis this is a concern particularly when large budget models are used for making significant decisions such as climate models in policy making see a discussion in saltelli et al 2015 on the other hand other model heavy subjects such as engineering and materials science have higher ratios yet it is worth recalling that even engineering has only around a half of confirmed global approaches and these are the most highly cited articles as a complement to the manual literature review we also investigated the prevalence of ua and sa methods based purely on text mining by identifying at least one known global sensitivity analysis technique i e variance based metamodeling elementary effects etc in keeping with the methodology of a previous paper from some of the present authors ferretti et al 2016 fig 7 shows the results of that paper as extended to 2015 and 2016 the original analysis stopped at 2014 this is a rougher approach but allows the inclusion of a much larger number of papers here it would seem that an even smaller fraction of papers that feature sensitivity analysis adopts a global sa approach at least three reasons explain the difference with the results in the present paper first as has been well established here sensitivity analysis is often also used to indicate uncertainty analysis so that the upper curve in fig 7 shows a mixture of ua and sa as well as an inevitable share of papers not pertaining to mathematical modelling secondly the estimation of the number of global sa papers is likely an underestimate because papers may apply simpler global methods e g a scatterplot based analysis but not necessarily refer to the articles or techniques listed finally in the manual literature review we focus only on highly cited papers which should ideally be of a higher standard than the average in a given field 4 4 method and model focus table 1 shows that most papers are unsurprisingly focused on the application i e on the model at hand and not on the methods of the total of 280 papers 35 were methodological i e having sa ua methods as their subject of these 24 advocate the use of global methods on the one hand this is encouraging because it shows that global methods are being promoted on the other hand a small but significant fraction of methodological papers are still advising statistically incorrect oat methods we note among the method papers a marked preference for variance based measures of sensitivity such as the sensitivity indices of which the pearson correlation ratio discussed previously is a special case we also see an active line of research in moment independent methods borgonovo et al 2012 4 5 model linearity as discussed if a model is linear an oat or derivative based approach is adequate however the linearity or nonlinearity of the model is rarely evident at least from the manuscripts table 1 shows the proportions of linear and nonlinear models only in 8 of the cases were we able to conclude that the model was definitely linear whereas over half of papers included clearly nonlinear models with the remainder being unclear this demonstrates that first researchers tend to work with nonlinear models second in the large majority of cases global methods are essential to perform a methodologically sound sensitivity analysis 5 discussion 5 1 reasons for bad practice the results of this study clearly show that there are serious methodological deficiencies in highly cited papers in most if not all disciplines why is this so often the case we speculate that this is due to at least five reasons which we outline here first sensitivity analysis is intrinsically attached to modelling which itself is not a unified subject indeed modelling typically requires a set of skills learned through experience and hence includes elements of craft as much as of science rosen 1991 as such every discipline goes about modelling following local disciplinary standards and practices padilla et al 2018 similarly sensitivity analysis practice is found in largely isolated pockets attached to each modelling discipline this fragmentation hinders development of the subject and spreading of good practice while simultaneously allowing malpractice to survive relatively unchallenged this issue is discussed in more depth in the following section a second point is that most scientists conflate the meaning of sa and ua if the meaning of sensitivity analysis is not even understood it is unsurprising that the quality of sensitivity analysis is sometimes lacking third global sensitivity analysis unavoidably requires a good background in statistics to implement and to interpret results some researchers simply haven t enough knowledge and training in statistics and consequently the cost in time and money required to learn and understand the necessary techniques may be considered prohibitive more generally researchers may not even be aware that global sensitivity analysis techniques exist under these circumstances it seems that researchers often revert to the more intuitive oat approach among other things it offers an ease of interpretation in moving just one input factor the change observed in the model output must come from that input alone moreover global methods may be discouraging in that the more factors that are moved the higher the chance that the model will crash or misbehave note that this is precisely the reason why a global sa is a good instrument of model verification it is unusual to run a global sa without detecting model errors modellers call this jokingly lubarsky s law of cybernetic entomology according to which there is always one more bug fourth although mature global sensitivity analysis methods have been around for more than 25 years this still may not be enough time for established good practice to filter down into the many research fields in which modelling is used this may be partly due to a lack of comparative examples across a range of fields moreover researchers tend to emulate methods found in highly cited papers assuming that they are best practice which as this study has demonstrated are often methodologically deficient finally as noted in leamer 2010 the reluctance to take up these methods may be due to their candour a proper method by honestly propagating all of the input uncertainty may lead to an inconveniently wide distribution of the output of interest for example a cost benefit analysis reporting a distribution encompassing possible large losses as well as large gains may not be what the owner of the problem wishes to hear this is the same as to say that the volatility of the inference is exposed and thus is the insufficiency of the evidence according to leamer 2010 as well as to funtowicz and ravetz 1990 this situation may induce modellers to massage the uncertainty in the input factors so that the output falls in a more desirable zone for cases where a considerable asymmetry exists between model developers and users jakeman et al 2006 it might be advisable to resort to sensitivity auditing an extension of sensitivity analysis beyond parametric analysis to include an assessment of the entire knowledge and model generating process for policy related cases saltelli et al 2013 to assess the credibility of degree of uncertainty attributed to each input factor and to make sure that the uncertainty has been neither inflated nor deflated to achieve a desired end inflation and deflation of uncertainty are quite common in e g regulatory controversies typically the regulated tend to inflate uncertainty so as to deter regulation while the opposite is the case for regulators michaels 2008 sensitivity auditing s seven point checklist is recommended by the european commission guidelines for impact assessment european commission 2009 p 393 5 2 isolated communities the scattered state of sensitivity analysis practice merits some further discussion if modelling is a non standardised discipline padilla et al 2018 the same holds a fortiori for uncertainty and sensitivity analysis hence the difficulty for good practices to establish themselves researchers from different fields have difficulties to communicate with one another in a transversal topic such as sa that is practised across a wide range of scientific and modelling disciplines robert rosen a system ecologist tackles the specificities of modelling in the scientific method in his work life itself rosen 1991 here he suggests that when a model is built to represent a natural system we should look at the play of causality the argument is that the natural system is kept together rosen uses the word entailed by material efficient and final causality in contrast the formal system i e the model is only internally entailed by formal causality rosen uses here the four causality categories of aristotle on which we will not dwell here to highlight that no arrow of causality flows from the natural system to the formal one in other words the act of encoding fig 8 is not driven by causality which would fix the model specification but is driven by the needs and the craft of the modeller the implication is that different modelling teams given the same data can produce altogether different models and inference refsgaard van der sluijs brown and van der keur 2006 thus the success of the modelling operation is judged by the usefulness or otherwise of the insights made possible by the operation of decoding which is another way of saying that all models are wrong but some are useful according to an aphorism attributed to george box models thus depend crucially upon craftmanship of the modellers this together with the diversity of modelling applications motives and constraints explain why modelling never became an independent discipline in our opinion this contributes to explaining why modelling is so discipline specific as noted by padilla et al 2018 the spread in modelling practices and cultures may be one of the reasons why methodologies which are ancillary to modelling such as uncertainty and sensitivity analysis are not part of a standardised syllabus being taught across disciplines and are at times ignored even in communities proficient in modelling such as for example hydrology shin et al 2013 despite the fragmentation of sensitivity and uncertainty analysis some cross disciplinary networks exist one such community might be said to have formed around a series of samo conferences for sensitivity analysis of model output see http samo2016 univ reunion fr samo has been held every three years since 1995 this community is active in training and dissemination however samo by no means captures the full spectrum of practitioners interested in uncertainty and sensitivity analysis for example in the united states sa related activities are under the heading of verification validation and uncertainty quantification vvuq for which a journal of the american society of mechanical engineers is available http verification asmedigitalcollection asme org journal aspx other sensitivity analysis related gatherings include the conference on uncertainty quantification organised by the society for industrial and applied mathematics the international conference on uncertainty quantification in computational sciences and engineering organised by the european community on computational methods in applied sciences and sessions in thematic conferences such as the uncertainty in structural dynamics conference organised by department of mechanical engineering of the ku leuven or the session on advances in diagnostics sensitivity and uncertainty analysis of earth and environmental systems models organised annually at the european geosciences union conference in vienna despite these communities the majority of practitioners remain scattered in isolated pockets and sensitivity analysis is hence not part of a recognized syllabus who or what scientific forum can then decide if a method is a good or a bad practice to make an example in nearing and gupta 2018 stark and saltelli 2018 who can authoritatively discourage modellers from over interpreting the results from multi model ensembles as if they were a random sample from a distribution this question remains for the time being unanswered a possible solution to this unsatisfactory state of affairs would be that statistics as a discipline takes responsibility for statistical methods for model validation and verification this would not make modelling into a discipline but would go a long way toward improving modelling practice additionally most if not all the tools of sensitivity analysis are statistical in nature this thesis has been suggested in a discussion paper entitled should statistics rescue mathematical modelling saltelli 2018 5 3 parallels with the p value the systematic problems observed in sensitivity analysis share similarities with the recent crisis in statistics over the p value a paper published in 2005 ioannidis 2005 warned about the poor quality of most published research results the paper was taken up by the media and the periodical the economist devoted its cover to the issue in 2013 how science goes wrong 2013 with a full article describing the subtleties of use and misuse of statistics in deciding about the significance of scientific results the specific subject of concern was the use of the p value the probability under a specified statistical model that a statistical summary of the data e g the sample mean difference between two compared groups would be equal to or more extreme than its observed value wasserstein and lazar 2016 the p value is used as a fundamental tool by researchers to decide if a given result is just the result of chance or indeed an effect worth publishing in 2016 the pressure surrounding the statistical community was so high that the american statistical association felt the need to intervene with a statement wasserstein and lazar 2016 to clarify how the test should be used useful reading on the topic are colquhoun 2014 gigerenzer and marewski 2014 stark and saltelli 2018 these articles show a complex mix of causes from poor training to bad incentives which result in the generalized failure in the use of the p value evidenced by attempts to repeat published results see e g shanks et al 2015 the problem is seen as a combination of confirmation bias authors looking for the effect they presume will be there confirmation bias or authors desperate to publish a positive result publish or perish of p hacking changing the setup of the study or the composition of the sample till an effect emerges and harking formulating the research hypothesis after the results are known kerr 1998 the latter involves repeatedly running comparison tests between different combinations of variables until a significant result is found which violates the conditions of applicability of the p test overall it is clear that the consequences of bad statistics can be dramatic for example when wrong cures for cancer are identified at the pre clinical stage of research and are then passed on to the clinical trial phase begley and ellis 2012 similarly it is not difficult to imagine the consequences of a wrong or missing uncertainty and sensitivity analyses given the pervasive role of models in risk analysis this can lead to ignoring dangerous operating conditions for a facility in decision analysis this can lead to wrong investments or policies a simple sensitivity analysis run on the formula used for the pricing of the complex derivative products at the root of the sub prime mortgage crisis would have revealed the fragility of the formula salmon 2009 wilmott and orrell 2017 whether the quants the experts in charge of these mathematical constructs wanted to know this fragility is of course another story finally a missing uncertainty analysis allows audacious risk or cost benefit analysis to be run over centennial time scales while a proper ua would show clearly that the uncertainties are too big to conclude anything an example discussed in saltelli et al 2015 was the computing the increased crime rate due to increased temperature at the year 2100 5 4 recommendations for best practice it is outside of the scope of this paper to give a detailed guide to sensitivity analysis for thorough references readers are referred to saltelli et al 2008 or ghanem et al 2017 nevertheless and although considerable differences exist in the use of sensitivity analysis among disciplines all fields would benefit from the adoption of good practices our personal list of preferences which agrees with the methodological papers seen in this review would include the following recommendations both uncertainty and sensitivity analysis should be based on a global exploration of the space of input factors be it using an experimental design monte carlo or other ad hoc designs the discussion in this paper has demonstrated that local oat methods do not adequately represent models with nonlinearities with some exceptions it is advisable to perform both uncertainty and sensitivity analysis once an analyst has performed an uncertainty analysis and is informed of the robustness of the inference it would appear natural to ascertain where volatility uncertainty is coming from at the other extreme a sensitivity analysis without uncertainty analysis is usually illogical the relative importance of a factor on the model output has a different relevance depending on whether the output has a small or large variance however there are cases for instance studies to identify the dominant effects on the output for a subsequent model reduction or calibration analysis where the analyst may be satisfied with a pure sa sensitivity and uncertainty analysis should be focused on a question most models have many outputs and these outputs can be used to answer a range of different questions the relationship sensitivity between the input factors and each different model output can be very different for this reason it is essential to focus the sensitivity analysis on the question addressed by the model rather than more generally on the model when sensitivity analysis is performed it should allow the relative importance of input factors and combinations of factors to be assessed either visually scatterplots or quantitatively regression coefficients sensitivity measures or other sensitivity and uncertainty analysis are themselves uncertain because there is considerable uncertainty in quantifying the uncertainty in input factors and modellers should be frank about how they arrived at the supposed uncertainties saltelli et al 2013 this should be kept in mind and efforts made to capture the uncertainty of input assumptions as accurately as possible even an apparently perfect uncertainty and sensitivity analysis is no assurance against error as noted by pilkey and pilkey jarvis 2009 it is important to recognize that the sensitivity of the parameter in the equation is what is being determined not the sensitivity of the parameter in nature if the model is wrong or if it is a poor representation of reality determining the sensitivity of an individual parameter in the model is a meaningless pursuit as regards what method should be used our preference is for methods which are exploratory model independent able to capture interactions and to treat a group of factors a carefully performed uncertainty analysis followed by sensitivity analysis is an important ingredient of the quality assurance of a model as well as a necessary condition for any model based analysis or inference 6 conclusions the main message of the present work is that a carefully performed sensitivity analysis is an important ingredient of the quality assurance of a model as well as a necessary condition for any model based analysis or inference however such analyses are not common enough and often inaccurate indicating that action is urgent on the front of quality assurance procedures for mathematical models in particular a significant fraction of papers investigated use sensitivity analysis approaches which fail elementary considerations of experimental design and do not properly explore the space of the input factors with the result that uncertainty is generally underestimated and sensitivity is wrongly estimated up to 65 of the reviewed highly cited papers are based on inadequate methods i e varying one input factor at a time although even in the most generous interpretation where all models with unclear linearity are assumed linear still over 20 of papers contain inadequate methodology further a significant number of papers confuse sensitivity and uncertainty analysis which is likely to exacerbate the problem with spreading good practice the fact that these figures concern highly cited papers has two implications first if we assume that highly cited papers represent the upper end of methodological rigour in a given field then the overall problem may be even worse second these are some of the most visible papers in their field and are used as guides for best practice therefore they can promote continued deficient methodology in our opinion the problem with sensitivity analysis is partly attributable to the fact that mathematical modelling is not a discipline in its own right and every branch of science and technology approaches modelling following its own culture and practice uncertainty and sensitivity analyses are likewise orphans of a disciplinary home one can also note that signals of distress as to the quality of mathematical modelling are heard from different disciplines from economics reinert 2000 romer 2015 to natural sciences oreskes 2000 oreskes et al 1994 pilkey and pilkey jarvis 2009 the situation has worrying analogies with what we have witnessed in data analysis where misuse of the p value colquhoun 2014 has been singled out as one of the reasons of the present reproducibility crisis affecting science ioannidis 2005 saltelli and funtowicz 2017 the importance of this analogy is in the warning it sounds for the credibility of science if such pervasive weaknesses in methodology are not addressed the need to heed this warning in the case of sensitivity and uncertainty analysis is becoming increasingly urgent appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 01 012 annex table 2 shows the results of the reviews in a condensed form the meaning of the headings is given in section 3 table 2 summary of results by subject identifier table 2 category method model linearity paper focus total reviewed global sa oat sa global ua oat ua other unclear linear nonlinear unclear method model agrbiosci 15 11 6 0 6 1 22 4 3 24 27 biochemgenmbio 23 15 6 1 7 2 19 15 0 36 36 busmanacc 4 7 5 5 1 1 18 2 3 18 21 chemi 10 8 2 0 5 0 17 5 1 21 22 chemeng 12 12 4 0 5 0 16 12 1 27 28 compsci 21 9 1 1 2 8 16 6 11 22 33 decsci 9 7 3 4 0 2 20 1 7 15 22 earthsci 11 13 4 1 17 5 13 24 2 41 43 econfin 5 8 6 3 0 1 16 1 0 18 18 energy 14 15 3 4 2 3 17 16 0 36 36 engineering 38 16 5 5 5 3 51 11 3 62 65 envsci 31 22 14 4 16 6 44 24 11 67 78 immunmicrobio 19 7 3 0 5 2 6 13 0 21 21 math 21 15 3 2 6 4 24 13 11 29 40 matsci 13 4 1 1 0 0 16 2 0 18 18 medicine 26 30 25 4 13 2 24 37 2 62 64 phartox 2 2 9 1 3 1 11 5 1 18 19 physastro 13 9 4 0 0 1 20 2 2 21 23 socsci 10 5 0 4 2 1 14 5 6 15 21 
