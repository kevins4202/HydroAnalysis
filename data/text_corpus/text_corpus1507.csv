index,text
7535,downscaled general circulation models gcms output are used to forecast climate change and provide information used as input for hydrological modelling given that our understanding of climate change points towards an increasing frequency timing and intensity of extreme hydrological events there is therefore the need to assess the ability of downscaled gcms to capture these extreme hydrological events extreme hydrological events play a significant role in regulating the structure and function of rivers and associated ecosystems in this study the indicators of hydrologic alteration iha method was adapted to assess the ability of simulated streamflow using downscaled gcms dgcms in capturing extreme river dynamics high and low flows as compared to streamflow simulated using historical climate data from 1960 to 2000 the acru hydrological model was used for simulating streamflow for the 13 water management units of the umngeni catchment south africa statistically downscaled climate models obtained from the climate system analysis group at the university of cape town were used as input for the acru model results indicated that high flows and extreme high flows one in ten year high flows large flood events were poorly represented both in terms of timing frequency and magnitude simulated streamflow using dgcms data also captures more low flows and extreme low flows one in ten year lowest flows than that captured in streamflow simulated using historical climate data the overall conclusion was that although dgcms output can reasonably be used to simulate overall streamflow it performs poorly when simulating extreme high and low flows streamflow simulation from dgcms must thus be used with caution in hydrological applications particularly for design hydrology as extreme high and low flows are still poorly represented this arguably calls for the further improvement of downscaling techniques in order to generate climate data more relevant and useful for hydrological applications such as in design hydrology nevertheless the availability of downscaled climatic output provide the potential of exploring climate model uncertainties in different hydro climatic regions at local scales where forcing data is often less accessible but more accurate at finer spatial scales and with adequate spatial detail keywords downscaled gcm output acru model uncertainty low flows high flows 1 introduction several studies from southern africa have shown that extreme hydrological events are increasing in frequency and magnitude e g kabat et al 2002 reason and keibel 2004 kadomura 2005 patt and schröter 2008 yanda 2010 goodess 2013 for example the el niño southern oscillation effect has continued to strengthen in recent decades 1990 2000 2000 2010 timmermann et al 1999 manatsa et al 2011 resulting in more floods and droughts several scholars believe that the el niño southern oscillations are becoming more intense as a result of climate change e g mcmichael et al 2006 collins et al 2010 manatsa and behera 2013 for example in southern africa the frequency of droughts is projected to increase and will most likely increase the frequency of extreme low flows and low storage episodes desanker and magadza 2001 these will inevitably affect aquatic ecosystems water supply irrigation leisure and hydro power generation on the other hand kusangaya et al 2014 reported that some areas in southern africa have and will experience an increase in the frequency and magnitude of heavy precipitation events which in most cases will result in flooding and extreme high flows to date examples include recurring floods as reported by artur and hilhorst 2012 overall given the potential negative effects of extreme hydrological events understanding is thus required as to whether and to what extent these extreme hydrological events floods and droughts and the subsequent river dynamics high and low flows are currently being captured when using general circulation models gcms for climate projections insights generated from such studies are not only important in increasing confidence in the use of the gcms output for impacts assessment but also in using results generated from such output for assessing potential climate change impacts in future gcms are computer models which mathematically represent various physical processes of the earth s system and in which physical and biogeochemical processes are described numerically to simulate the climate system as realistically as possible wilby et al 1999 hulme et al 2001 randall et al 2007 although the physical processes are generally well known they are usually not fully represented in climate models owing to limitations in computing resources inadequate input data and limited understanding of processes schulze 2000a thornton et al 2009 tadross et al 2011 ganguly et al 2014 nevertheless gcms also provide climate output critical in driving hydrological models to date the availability of downscaled gcms dgcms output covering watersheds at finer spatial scales provides data for input into hydrological models for climate change impacts assessments whilst downscaling produces climate information at scales finer than the initial projections it uses additional information data and assumptions which lead to further uncertainties and limitations see jones et al 2011 chen et al 2012 nikulin et al 2012 wilby and dawson 2013 dosio et al 2014 owing to these limitations gcms are considered to be the largest source of uncertainty in quantifying climate change impacts hewitson and tadross 2010 thornton et al 2010 consequently as reported elsewhere downscaled gcms output have often failed to capture the spatial variability and magnitudes of extreme hydrological events nemeth 2010 prudhomme et al 2010 willems et al 2012 ntegeka et al 2014 and hence fail to capture the subsequent river dynamics high and low flows this is so despite the fact that high and low flows play a significant role in regulating the structure and function of rivers and associated ecosystems field 2012 given that our understanding of climate change points towards an increasing frequency timing and intensity of extreme hydrological events e g easterling et al 2000 groisman et al 2005 osbahr et al 2008 yanda 2010 yamba et al 2011 goodess 2013 there is therefore the need to assess the ability of downscaled gcms to capture these extreme hydrological events for southern africa in both urban and rural areas on one hand both urban and rural areas contain many types of hydraulic engineering structures such as dams levees water distribution networks water collection networks sewage collection networks storm water management etc which need to be designed to accommodate peak flows of a certain magnitude in order to function safely at a given level of risk schulze et al 2014 should the structures fail especially where human settlement is dense there are potential economic environmental and societal consequences schulze et al 2014 it is thus hypothesised that climate change could result in changes in the intensity and frequency of extreme rainfall events the emerging question is thus how is the simulated streamflow simulated using downscaled gcm climate output capturing the spatial variability and magnitudes of extreme hydrological events and the subsequent river dynamics high and low flows for the historical climate 1960 2000 for these gcms to be used with confidence in future climate projections the umngeni catchment in south africa is used as a case study the acru hydrological model was used in simulating streamflows 2 materials and methods 2 1 study area the umngeni catchment 4349 km2 subdivided into 13 water management units wmus and is located in the kwazulu natal province of south africa fig 1 the mean annual precipitation map of the catchment varies from 1550 mm per annum in the main water source areas in the west of the catchment to 700 mm per annum in the drier middle reaches of the catchment summerton and schulze 2009 summerton et al 2009 the catchment is however characterised by high intra and inter annual rainfall variability mean annual temperature ranges from 12 c in the escarpment areas to 20 c towards the coastal areas of the catchment summerton et al 2009 fig 1 shows the location of umngeni catchment in south africa and the acocks 1988 land cover types the acocks 1988 veld types baseline landcover were used in simulating streamflows for both the historical and dgcm based streamflow simulations that would occur under natural landcover as stated in kusangaya et al 2017 the acocks veld type maps are the most scientifically respected and generally accepted maps of natural vegetation for south africa estimates of streamflow responses from the acocks veld types have formed the basis for which streamflow reductions owing to land use change as outlined in the south african national water act nwa 1998 are assessed since 1998 gush et al 2002 and more recently streamflow changes owing to climatic changes warburton et al 2012 the umngeni catchment supplies water to nearly 15 of sa s population including the two largest cities pietermaritzburg with 220 000 people and durban with 600 000 people stats sa 2012 thus the catchment is home to a significant population who are potentially vulnerable to climate change especially given the projected increase in the occurrence of extreme hydrological events within the umngeni catchment there are four major dams alberts falls midmar nagle and inanda which are used for water supply and leisure boating and fishing activities fig 1 these activities are highly dependent on water availability and are likely to be affected by climate change the catchment is regarded as water stressed and regularly experiences droughts alexander 1985 diab and scott 1989 alexander 2001 summerton 2008 summerton et al 2010 as observed during the 2015 2016 rainfall season umngeni water 2016 for example the 2015 2016 season has been regarded as the worst drought season since 1904 with the kwazulu natal province being one of the hardest hit provinces in south africa saws 2017 losses resulting from this drought have been estimated at over r400 million agri sa 2016 the impacts were particularly felt by the farming community which comprises of both commercial and subsistence farmers with almost 10 000 provincial farmers affected leaving around 40 000 heads of cattle dead and severe crop losses agri sa 2016 some of the major dams storage capacity as of may 2016 were well below 50 with midmar dam at 45 72 and albert falls dam at 32 9 umngeni water 2016 as a result umgeni water has reduced the production of potable water water rationing and restrictions were implemented and this is likely to affect people s livelihoods umngeni water 2016 given the population at risk of these extreme hydrological events and financial losses incurred thereof it is important that we understand the frequency of occurrence and impacts of these events in the umngeni catchment however this requires an understanding of the uncertainties associated with the downscaled output in the first place previous studies considering climate change impacts on hydrology in the umngeni catchment see e g tarboton et al 1992 summerton 2008 summerton et al 2009 summerton et al 2010 warburton et al 2012 did not consider the uncertainty of downscaled products in capturing extreme high and low flows yet the understanding of these uncertainties is important in improving characterisation of climate related hydrological impacts in the catchment understanding how downscaled climate output captures extreme hydrological events and extreme river dynamics increases our confidence in the use of downscaled climate products for future hydrological projections as well as climate change impact studies 2 2 downscaled climate model output the downscaled climate output were obtained from the climate system analysis group at the university of cape town csag uct the climate data were statistically downscaled from the fifth phase of the coupled model inter comparison project cmip5 at csag uct using the self organizing map downscaling somd method of hewitson and crane 2006 in a previous study kusangaya et al 2016 observed that out of the ten downscaled gcms used only miub echo g e11 gfdl cm2 0 g11 mri cgcm2 3 2a mr1 csiro mk3 5 cs1 and mpi echam5 e12 simulated historical rainfall 1960 2000 satisfactorily with cumulative rainfall found to be within 10 of historical rainfall in the majority 7 13 of water management units of the umngeni catchment additionally evaluation of the intra gcm variability showed that these five dgcm showed limited variability for mean rainfall as all dgcms inter quartile ranges were between 15 based on these findings climate output from the five dgcms were considered acceptable 10 of rainfall mean and 15 of rainfall variability to used as input to the acru hydrological model in simulating streamflow table 1 provides the details of the five selected dgcms 2 3 hydrological modelling downscaled gcms do not simulate streamflow thus projected hydrological time series were obtained through inputting dgcm climate data to drive a rainfall runoff model and obtain projected streamflow responses the acru hydrological model was used to simulate streamflow for the period 1961 2000 under a baseline land cover of acocks 1988 veld types the acru model schulze 1995 is a physical conceptual daily time step multi level multi purpose model that is conceptualized to adequately represent hydrological processes and has also been shown to adequately represent catchment responses under different climate scenarios the acru model has been applied extensively in both land use and climate change impact studies in different hydro climatological regions of southern africa as well as in modelling high and low flows e g tarboton et al 1992 schulze and perks 2000 schulze 2000b schulze et al 2003 schulze 2005 graham et al 2011 schulze 2011 kienzle et al 2012 warburton et al 2010 showed that the acru model can be used successfully for climate change impact studies in catchments with diverse land uses and climate regions this increases the confidence that the acru model provides a suitable and accurate representation of historical streamflow and reduces the uncertainty regarding the model s ability to simulate adequately different climate scenarios as obtained from the different dgcms warburton et al 2012 the acru hydrological model is designed to simulate daily soil water budgets from which monthly and annual values can then be derived smithers and schulze 1995 smithers et al 1997 schulze and perks 2000 the generation of streamflow in acru is based on the premise that after initial abstractions interception depression storage and infiltration streamflow produced is a function of the magnitude of the rainfall and the soil water deficit from a critical response depth of the soil schulze 1995 the acru hydrological model is not a model in which parameters are calibrated to produce a good fit rather values of input variables are estimated from the physical characteristics of the catchment smithers and schulze 1995 using available information details of acru configuration used in this study can be obtained from warburton et al 2010 the acru model is explained in detail by schulze 1995 and smithers and schulze 1995 fig 2 illustrates the conceptualization of the water budget in the acru model parameter input values soils catchment characteristics topography etc as used by warburton et al 2010 in the configuration of the acru model were adopted for this study the umngeni catchment was delineated into 13 wmus which were further subdivided into 145 sub catchments with respect to altitude topography soil properties and land cover as well as the river sub catchment drainage system the sub catchments were configured to cascade downstream in a logical sequence representative of river flow fig 3 cumulative streamflows from the outlets of five water management areas were selected for analysis of hydrological alteration namely midmar and karkloof in the upper reaches of the catchment table mountain and nagle in the middle reaches of the catchment and durban at the lower catchment outlet fig 3 for the simulation of historical streamflow daily rainfall data required as input to the acru model were extracted from a daily rainfall database for south africa compiled by lynch 2004 for the historical time period 1960 2000 daily temperatures were also extracted from a gridded database of daily temperatures for south africa compiled by schulze and maharaj 2004 for the same time period 1960 2000 on the other hand for the simulation of gcm streamflow statistically downscaled gcm rainfall and temperature data obtained from csag were used to as input for the acru hydrological model for the different downscaled climate models from 1960 to 2000 the acocks 1988 veld types baseline land cover were used in simulating streamflows for both the historical and dgcm based streamflow simulations that would occur under natural land cover as stated in kusangaya et al 2017 the acocks veld type maps are the most scientifically respected and generally accepted maps of natural vegetation for south africa assessment of extreme hydrological events was carried out on cumulative streamflow data at the catchment outlet streamflow in acru based on the assumption that any variation of streamflow at the catchment outlet is representative of variation accumulated from the whole catchment i e it is the integrated flow at a point of the entire upstream area streamflows thus carry the memory or footprint of all rainfall runoff processes dependent on the climatic regime soils slope and land cover which occurred upstream 2 4 assessment of hydrological variability for this study the indicators of hydrologic alteration iha method richter et al 1996 richter and thomas 2007 of the us nature conservancy http www nature org was adapted to assess how streamflow simulated using downscaled climate output adequately captures and approximates streamflow simulated using historical climate data the iha method gives a comprehensive list of indices which are widely and effectively used to analyse both high and low flows previous studies have successfully applied the methodology of iha to assess impacts of anthropogenic drivers such as dam construction on streamflow e g taylor et al 2003 mathews and richter 2007 yang et al 2008 zhang and döll 2008 zhang et al 2009 de winnaar and jewitt 2010 kim et al 2011 saraiva okello et al 2015 furthermore olden and poff 2003 evaluated patterns of statistical variation among 171 published hydrological indicators and concluded that the 33 iha indices capture the majority of the variation and thus can be used to represent the major aspects of the flow regimes recently wobus et al 2015 also effectively adapted the iha methodology to characterize changes in the hydrology under a range of future climate scenarios iha can thus be considered to be a robust methodology as it can indicate the degree of uncertainty in capturing extreme hydrological events between streamflow simulated using dgcm output and streamflow simulated using historical climate data the data record length for analysis was from 1961 to 2000 the iha approach compares hydrological data sets by calculating multi variate statistics to assess the degree of hydrological alteration therefore enabling the characterization of river and catchment conditions puckridge et al 1998 clausen and biggs 2000 pettit et al 2001 and responses of the catchments to climate forcing wobus et al 2015 this multi variable approach allows the investigation of the multi impacts of hydrological changes on the river channel and associated ecosystems mathews and richter 2007 the iha software calculates a total of 67 parameters subdivided into two groups 33 hydrologic alteration ha parameters and 34 environmental flow component efc parameters richter et al 1996 for this study 26 of the 33 ha group 1 2 and 4 and 20 of the 34 efc were adapted for assessing hydrologic alterations in terms of streamflow magnitude frequency duration and rate of change as well as performing a range of variability analysis table 2 these chosen indices are deemed cable of completely characterising hydrological characteristics of both high and low flows the flows exceeding the 75 q75 of the daily flows for the 40 year period were classified as high flows flows that were below the 25 q25 of the daily flows for the 40 year period were classified as low flows a small flood event was defined as an initial high flow with a peak flow greater than that of the 2 year return interval event whilst a large flood event was taken as an initial high flow with peak flow greater than that of the 10 year return interval event an extreme low flow was defined as an initial low flow below 10 of daily flows for the period frequency refers to how often a flow above a given magnitude recurs over some specified time interval duration is the period of time associated with a specific flow condition peak flow denotes the maximum flow during an event and timing refers to julian date of the flow nature conservancy 2009 non parametric median and percentile statistics were used because of the skewed non normal nature of hydrological datasets ha parameters are calculated and organized in the output tables by water year from october 1 to september 30 the hydrologic alteration factor for each of the parameters was calculated as outline by richter et al 1996 richter et al 1998 mathews and richter 2007 as follows 1 for each parameter iha divides the full range of pre impact data i e simulations from historical data into three different categories generally percentiles e g lowest third middle third and highest third 2 the post impact data i e simulations from dgcm output is then analysed and the observed distribution of data with the distribution expected from the pre impact data compared 3 hydrologic alteration ha factor was calculated as observed frequency expected frequency expected frequency 4 a positive ha factor means that the frequency of values in the category percentile grouping had increased in the post impact period while a negative ha factor means that the frequency of values in the category percentile grouping had decreased in the post impact period the maximum ha value is infinity with a minimum value of 1 a positive ha value implies that the frequency of values in the category high medium and low rva has increased from the streamflow simulated using the historical climate data pre impact to the streamflow simulated using the dgcm output post impact a negative value shows that the frequency of values in the category is lower for the streamflow simulated using the historical climate data as compared to streamflow simulated using the dgcm output see richter et al 1997 richter et al 1998 mathews and richter 2007 2 4 1 range of variability analysis the range of variability approach rva described in richter et al 1997 was implemented within the iha for analysing the change between flows simulated using historical rainfall vs flows simulated using downscaled climate output the rva used the flows simulated using historical climate data as a reference for defining the extent to which natural flow regimes differed from flows simulated using dgcm output for the rva the streamflow data were split into 3 categories 1 data above 66th percentile as high rva analysis of high flows 2 data between 33rd and 66th percentile as medium rva and 3 data below 33rd percentile as low rva analysis of low flows it was hypothesised that the high rva would capture the increase or decrease of high values including extreme high flow values floods in streamflow simulated using dgcm output likewise the low rva was hypothesised to be able to capture the increase or decrease of low values including extreme low flow values droughts in streamflow simulated using dgcm output hydrologic alteration factors quantifying the degree of alteration for the 33 iha flow parameters table 2 were calculated 3 results the following sections detail the performance of the dgcms in capturing historical rainfall the performance of the acru model in simulating streamflow and how the simulated streamflow simulated using dgcm output captures the spatial variability and magnitudes of high and low flows compared to streamflow simulated using historical climate data 3 1 evaluation of the downscaled rainfall kusangaya et al 2016 in a study aimed at evaluating how downscaled climate output represented the underlying historical precipitation characteristics beyond the means and variances showed that using the means and variances most dgcms could satisfactorily represent historical rainfall e g 7 10 dgcms were within 10 of historical rainfall mean whilst few 3 10 dgcms did not capture the mean and variability of rainfall well table 3 additionally beyond the means and variances the performance of the downscaled rainfall showed that high rainfall events were well captured whilst raindays and low rainfall events were poorly captured furthermore raindays and low rainfall events were poorly captured in summer december february as compared to the low rainfall winter months june august 3 1 1 evaluation of extreme rainfall evaluation of extreme rainfall was carried out using the number of raindays with rainfall exceeding 50 mm per day to the number of raindays with rainfall exceeding 200 mm per day results table 4 indicated that all the gcms captured the extreme raindays very well with correlation coefficient ranging from 0 923 to 0 998 3 2 evaluation of the performance of the arcu hydrological model warburton et al 2010 showed that overall the acru model performed well on each of the four wmus which had flow measuring stations hence the acru model can confidently to simulate streamflows of the mgeni catchment for example for the mpendle wmu the total flows are adequately simulated with the percentage difference between the observed and simulated standard deviation were found to be less than 15 the r2 of daily values was 0 836 and the nash sutcliffe ef was 0 802 therefore simulation of streamflow in the mpendle wmu was considered highly acceptable the lions river wmu on the other hand similarly produced acceptable results with an r2 of 0 882 for the karkloof wmu the nash sutcliffe ef of 0 655 and the other statistics were also considered acceptable table 5 the results of the confirmation study for the henley wmu were considered reasonable as all statistics except for the percentage difference between the standard deviations were acceptable and comparison of daily simulated and observed streamflows indicated that the variability of streamflow was adequately simulated 3 3 annual and monthly variability of streamflows streamflow simulated using historical climate data had a greater range of annual flow variability 0 002 0 016 m3 s when compared with streamflow simulated using dgcm output with variability ranging from 0 001 to 0 010 m3 s fig 4 the inability of streamflow simulated using dgcm output to capture the variability in annual data series implied that streamflow simulated using dgcms is not capable of replicating extreme river dynamics high and low flows observed in streamflow simulated using historical climate data on a monthly basis it was also established that streamflow simulated using historical climate data was largely underestimated by as much as 50 in the high rainfall months in summer december march this could largely be attributed to the failure of the dgcm simulated streamflow in capturing local storms and related streamflow peaks which normally occur during the summer season these summer months were also characterised by a greater range in variability in all the study catchments for the winter months may july streamflow simulated using historical climate data was underestimated fig 5 shows the monthly median flow variability for midmar karkloof nagle and durban catchments results from the deviation factor and the corresponding significant count analysis table 6 indicate that for all catchments midmar karkloof nagle and durban the deviation of the streamflow simulated using dgcms from the streamflow simulated using the historical data were highly significant in most of the metrics used this implied that there were significant differences between the streamflow simulated using historical climate data and streamflow simulated using dgcm output 3 4 magnitude and duration of annual extreme flows a suite of indices depicting magnitude and duration of annual extreme streamflow conditions low and high flows indices were calculated these included the 1 3 7 30 and 90 day minimums and maximums taken from moving averages calculated as means for every possible period that is completely within the water year 1 october to 30 september nature conservancy 2009 results indicated that for all wmus the magnitude of low flows increased with increasing frequency of low flows 1 3 7 30 90 day minimum fig 6 even though the simulated streamflow from dgcms failed to correctly capture the magnitude underestimation the increasing pattern from 0 001 for 1 day minimum to 0 004 for the 90 day minimum was well captured in all the wmus the magnitude of high flows decreases with increasing frequency of high flows 1 3 7 30 and 90 day maximums fig 7 for most of the wmus high flows were overestimated especially in the catchment areas of umngeni catchment receiving moderate rainfall between 700 and 900 mm on the other hand for the high rainfall karkloof wmu streamflow simulated using dgcms climate output underestimated streamflow simulated using historical climate data again as in the low flow analysis the decreasing pattern with increasing frequency 1 3 7 30 and 90 day maximums in high flow magnitude was well captured in all wmus 3 5 hydrological variability of high and low flows as stated above for the rva the streamflow data were split into three categories a data above 66th percentile as high rva analysis of high flows b data between 33rd and 66th percentile as medium rva and c data below 33rd percentile as low rva analysis of low flows it was hypothesised that the high rva would capture the increase or decrease of high values including extreme high flows floods in streamflow simulated using dgcm output likewise the low rva was hypothesised to be able to capture the increase or decrease of low flows including extreme low values droughts in streamflow simulated using dgcm output overall results from the four selected wmus showed that in the high and middle rva ranges the majority of iha parameters considered in this study were characterised by negative ha values a negative value means that the frequency of values in the high and middle rva ranges was lower for the streamflow simulated using the historical climate data as compared to streamflow simulated using the dgcm output this implied that streamflow simulated using dgcm output did not capture all the high and extreme high flow values as captured in the streamflow simulated using historical climate data similarly all iha parameters were characterised by negative ha values for the low rva this implied that the frequency of values in the low iha category was lower for the streamflow simulated using the historical climate data as compared to streamflow simulated using the dgcm output this meant that streamflow simulated using dgcm output was capturing less episodes of low flows as compared to the low flows simulated using historical climate data fig 8 shows a summary of hydrological indicators for all the parameters analysed for high and low flow assessments using the iha range of variability approach generally fig 8 shows that high rva for high flow analysis the ha factor for low pulse count high pulse count rise rate and fall rate is more than one for karklof midmar nagle and durban wmus on the other hand the 1 3 and 7 day minimum as well as the monthly median flows were also characterised by a ha greater than one for the low rva for low flow analysis as stated before a positive ha value implies that the frequency of values in the category high or low rva has increased from the streamflow simulated using the historical climate data pre impact to the streamflow simulated using the dgcm output post impact 3 6 extreme high and low flow parameters using the range of variability analysis of environmental flow conditions parameters examining extreme high low flows were considered of the 20 the following 10 efc parameters were found to be significantly significant count less than 50 different from the streamflow simulated using historical climate data from at least one dgcm extreme low peak extreme low frequency high flow peak high flow duration high flow frequency high flow rise rate high flow fall rate small flood duration small flood rise rate small flood fall rate fig 9 as such the remaining 10 parameters extreme low duration extreme low timing high flow timing small flood peak small flood timing large flood peak large flood duration large flood timing large flood rise rate and large flood fall rate showed that there was no significant difference in these parameters between streamflow simulated using historical climate data and streamflow simulated using dgcm output fig 9 most of the streamflow simulated using climate data from dgcms successfully captured the large flood peak between 0 4 and 1 0 m3 s fig 10 a except for dgcm e11 which was characterised by variability ranging between 0 5 and 1 3 m3 s large flood duration however was underestimated by streamflow simulated using dgcm output fig 10b by as much as 25 30 days extreme low flow frequency fig 10c was overestimated in streamflow simulated using historical climate data than streamflow simulated using dgcm output extreme low flow duration fig 10d on the other hand was comparable all between 2 and 4 days between simulated streamflow from downscaled dgcms output and historical climate data 4 discussion and conclusions this study tested how simulated streamflow using dgcm output captures river dynamics high and low flows in the umngeni catchment of south africa results of this study indicate that in moderately high to high rainfall catchments streamflow simulated using historical climate data were underestimated compared to streamflow simulated using dgcms output this was despite the fact that all the chosen dgcms had captured overall rainfall mean and variability reasonably well e g were within 10 of historical rainfall mean such a result implies that dgcms introduce greater uncertainty in high rainfall areas possibly owing to the failure of the dgcms to capture adequately other rainfall characteristics such as a total rainfall quantity b the interval between rainfall events or c individual event size d rainfall duration and e rainfall intensity thus when used to simulate streamflow it resulted in the peak duration and frequency of streamflow events not being adequately captured especially in high rainfall areas the inability of dgcms to correctly represent rainfall characteristics is widely reported in literature see e g nemeth 2010 prudhomme et al 2010 tramblay et al 2012 willems et al 2012 ntegeka et al 2014 recently teng et al 2015 showed that raw regional climate model rcm precipitation exhibited negative errors in annual and seasonal means with the median errors in raw rcm annual means generally it has been established that input data uncertainty in particular uncertainty associated with rainfall data input significantly affects the total simulation uncertainty andréassian et al 2001 pappenberger et al 2005 on the other hand schulze 1995 established that output simulated by the acru hydrological model is most sensitive to input rainfall consequently the uncertainties in river dynamics high and low flows described in this study were hypothesised to be largely due to the uncertainties in the precipitation outputs from dgcms rather than uncertainties in the acru model itself the results obtained in this study for umngeni catchment thus concur with other studies elsewhere as cited above during the summer rainfall months simulated streamflow was characterised by greater variability and during the low rainfall winter months simulated streamflow was characterised by less variability this result suggests that for summer rainfall characterisation dgcms fail to capture the extremes of rainfall i e rainfall peaks this error was carried on and even propagated to the simulated streamflow as demonstrated by poor detection of simulated streamflow extremes this is understandable since rainfall is the major driver of the hydrological cycle thus largely determining characteristics of simulated streamflow the above conclusions have been collaborated in literature for example chiew et al 2009 teng et al 2012 teng et al 2015 have reported the so called drizzle effect whereby downscaled climate output simulates too many low intensity precipitation events and too few high intensity precipitation events these results are also in agreement with recent findings from coppola et al 2014 who using eight scenario runs from two regional climate models established that the uncertainty associated with the winter streamflow change was larger compared to that in the fall this study was carried out in italy for the po river which is characterised by two annual discharge maxima one in spring because of snow melting in the alps and one in the fall due to rainfall in the area at this time of the year the umngeni catchment however only experiences one annual discharge maximum as it receives most rainfall during the summer months december march when streamflow data were split into percentiles median flows for all the streamflow data above 67th percentile and data between 33rd and 66th percentile was largely characterised by negative ha values implying that the frequency of values in the high and middle rva ranges was lower for the streamflow simulated using the historical climate data this implied that streamflow simulated using dgcm output did not capture the high and extreme high flows that were captured in the streamflow simulated using historical climate data it is thus concluded that dgcms do not adequately capture extreme high values characteristic of for example cyclonic heavy storm events further translating to extreme high flows not being well represented in hydrological modelling these results are in agreement with results by mandal et al 2016 who found that a large amount of uncertainty was evident in summer high precipitation months june july and august as compared to the low rainfall periods on the other hand for the data below the 33rd percentile positive ha meant that the frequency of values in the low ha category was higher for the streamflow simulated using the historical climate data as compared to streamflow simulated using the dgcm output this implies that simulated streamflow using dgcm output captured more low flows and more extreme low flows than what actually was occurring as in the streamflow simulated using historical climate data the so called drizzle effect reported also by teng et al 2012 consequently simulated dgcm streamflow would show more drying droughts than actually exists thus as reported also in nemeth 2010 prudhomme et al 2010 willems et al 2012 ntegeka et al 2014 downscaled climate products are unable to capture the spatial variability and magnitudes of extreme hydrological events which are often of particular concern in hydrology although the method of iha is effective as it uses a range of indices to analyse various aspect of flow regimes it utilises an extensive set of indices which in most cases are correlated olden and poff 2003 gao et al 2009 hence this results not only in a level of numerical redundancy in data processing but also in considerable information redundancy yang et al 2008 identified a small subset of hydrological indicators that were the most representative of ecological flow regimes oueslati et al 2010 used principal components analysis to reduce 40 hydrological indices to identify subsets of hydrological indices that describe the major sources of variations while minimizing redundancy and showed that the number of statistically significant principal components varied from three to four for this study a total of 40 of the 67 iha indices 26 of the 33 ha group 1 2 and 4 and 20 of the 34 efc were used this resulted not only in a level of numerical redundancy in data processing but also in considerable information redundancy however the ideal approach as reviewed from literature is to use a suite of indices in the analysis of both high and low flows see e g khomsi et al 2016 from the above discussion this study concludes that developing a small number of statistics that capture key components of hydrologically relevant flow variation will 1 contribute to a general approach for characterizing flow alteration 2 minimize statistical redundancy and computational effort in future analyses the overall conclusion of this study is that dgcm output were characterised by higher uncertainties especially in capturing extreme hydrological parameters such as extreme high and low flows streamflow simulation using dgcm output does not capture extreme hydrological events which are often of particular concern in design hydrology especially under conditions of increasing frequency of extreme hydrological events floods and droughts and the subsequent changes in river dynamics high and low flows consequently streamflow simulated using dgcms should be used with caution for hydrological applications particularly when considering extreme flows for example ruelland et al 2015 cautioned that the use of streamflow simulated from gcms must be limited to the analysis of mean shifts in the future and that it cannot be designed for analysing the impacts of potentially more intense drought or flood events it is therefore recommended that downscaling techniques be improved in order to generate climate data that are relevant for hydrological assessments be that as it may the availability of downscaled climatic output provide the potential of exploring climate model uncertainties in different hydro climatic regions at local scales where forcing data is often less accessible but more accurate at finer spatial scales and with adequate spatial detail acknowledgements we would like to thank the applied centre for climate and earth systems science access for supporting this research work by providing funds for undertaking the phd research under the theme climate change impacts and adaptation additionally we would like to thank the climate system analysis group at the university of cape town for providing the downscaled rainfall data 
7535,downscaled general circulation models gcms output are used to forecast climate change and provide information used as input for hydrological modelling given that our understanding of climate change points towards an increasing frequency timing and intensity of extreme hydrological events there is therefore the need to assess the ability of downscaled gcms to capture these extreme hydrological events extreme hydrological events play a significant role in regulating the structure and function of rivers and associated ecosystems in this study the indicators of hydrologic alteration iha method was adapted to assess the ability of simulated streamflow using downscaled gcms dgcms in capturing extreme river dynamics high and low flows as compared to streamflow simulated using historical climate data from 1960 to 2000 the acru hydrological model was used for simulating streamflow for the 13 water management units of the umngeni catchment south africa statistically downscaled climate models obtained from the climate system analysis group at the university of cape town were used as input for the acru model results indicated that high flows and extreme high flows one in ten year high flows large flood events were poorly represented both in terms of timing frequency and magnitude simulated streamflow using dgcms data also captures more low flows and extreme low flows one in ten year lowest flows than that captured in streamflow simulated using historical climate data the overall conclusion was that although dgcms output can reasonably be used to simulate overall streamflow it performs poorly when simulating extreme high and low flows streamflow simulation from dgcms must thus be used with caution in hydrological applications particularly for design hydrology as extreme high and low flows are still poorly represented this arguably calls for the further improvement of downscaling techniques in order to generate climate data more relevant and useful for hydrological applications such as in design hydrology nevertheless the availability of downscaled climatic output provide the potential of exploring climate model uncertainties in different hydro climatic regions at local scales where forcing data is often less accessible but more accurate at finer spatial scales and with adequate spatial detail keywords downscaled gcm output acru model uncertainty low flows high flows 1 introduction several studies from southern africa have shown that extreme hydrological events are increasing in frequency and magnitude e g kabat et al 2002 reason and keibel 2004 kadomura 2005 patt and schröter 2008 yanda 2010 goodess 2013 for example the el niño southern oscillation effect has continued to strengthen in recent decades 1990 2000 2000 2010 timmermann et al 1999 manatsa et al 2011 resulting in more floods and droughts several scholars believe that the el niño southern oscillations are becoming more intense as a result of climate change e g mcmichael et al 2006 collins et al 2010 manatsa and behera 2013 for example in southern africa the frequency of droughts is projected to increase and will most likely increase the frequency of extreme low flows and low storage episodes desanker and magadza 2001 these will inevitably affect aquatic ecosystems water supply irrigation leisure and hydro power generation on the other hand kusangaya et al 2014 reported that some areas in southern africa have and will experience an increase in the frequency and magnitude of heavy precipitation events which in most cases will result in flooding and extreme high flows to date examples include recurring floods as reported by artur and hilhorst 2012 overall given the potential negative effects of extreme hydrological events understanding is thus required as to whether and to what extent these extreme hydrological events floods and droughts and the subsequent river dynamics high and low flows are currently being captured when using general circulation models gcms for climate projections insights generated from such studies are not only important in increasing confidence in the use of the gcms output for impacts assessment but also in using results generated from such output for assessing potential climate change impacts in future gcms are computer models which mathematically represent various physical processes of the earth s system and in which physical and biogeochemical processes are described numerically to simulate the climate system as realistically as possible wilby et al 1999 hulme et al 2001 randall et al 2007 although the physical processes are generally well known they are usually not fully represented in climate models owing to limitations in computing resources inadequate input data and limited understanding of processes schulze 2000a thornton et al 2009 tadross et al 2011 ganguly et al 2014 nevertheless gcms also provide climate output critical in driving hydrological models to date the availability of downscaled gcms dgcms output covering watersheds at finer spatial scales provides data for input into hydrological models for climate change impacts assessments whilst downscaling produces climate information at scales finer than the initial projections it uses additional information data and assumptions which lead to further uncertainties and limitations see jones et al 2011 chen et al 2012 nikulin et al 2012 wilby and dawson 2013 dosio et al 2014 owing to these limitations gcms are considered to be the largest source of uncertainty in quantifying climate change impacts hewitson and tadross 2010 thornton et al 2010 consequently as reported elsewhere downscaled gcms output have often failed to capture the spatial variability and magnitudes of extreme hydrological events nemeth 2010 prudhomme et al 2010 willems et al 2012 ntegeka et al 2014 and hence fail to capture the subsequent river dynamics high and low flows this is so despite the fact that high and low flows play a significant role in regulating the structure and function of rivers and associated ecosystems field 2012 given that our understanding of climate change points towards an increasing frequency timing and intensity of extreme hydrological events e g easterling et al 2000 groisman et al 2005 osbahr et al 2008 yanda 2010 yamba et al 2011 goodess 2013 there is therefore the need to assess the ability of downscaled gcms to capture these extreme hydrological events for southern africa in both urban and rural areas on one hand both urban and rural areas contain many types of hydraulic engineering structures such as dams levees water distribution networks water collection networks sewage collection networks storm water management etc which need to be designed to accommodate peak flows of a certain magnitude in order to function safely at a given level of risk schulze et al 2014 should the structures fail especially where human settlement is dense there are potential economic environmental and societal consequences schulze et al 2014 it is thus hypothesised that climate change could result in changes in the intensity and frequency of extreme rainfall events the emerging question is thus how is the simulated streamflow simulated using downscaled gcm climate output capturing the spatial variability and magnitudes of extreme hydrological events and the subsequent river dynamics high and low flows for the historical climate 1960 2000 for these gcms to be used with confidence in future climate projections the umngeni catchment in south africa is used as a case study the acru hydrological model was used in simulating streamflows 2 materials and methods 2 1 study area the umngeni catchment 4349 km2 subdivided into 13 water management units wmus and is located in the kwazulu natal province of south africa fig 1 the mean annual precipitation map of the catchment varies from 1550 mm per annum in the main water source areas in the west of the catchment to 700 mm per annum in the drier middle reaches of the catchment summerton and schulze 2009 summerton et al 2009 the catchment is however characterised by high intra and inter annual rainfall variability mean annual temperature ranges from 12 c in the escarpment areas to 20 c towards the coastal areas of the catchment summerton et al 2009 fig 1 shows the location of umngeni catchment in south africa and the acocks 1988 land cover types the acocks 1988 veld types baseline landcover were used in simulating streamflows for both the historical and dgcm based streamflow simulations that would occur under natural landcover as stated in kusangaya et al 2017 the acocks veld type maps are the most scientifically respected and generally accepted maps of natural vegetation for south africa estimates of streamflow responses from the acocks veld types have formed the basis for which streamflow reductions owing to land use change as outlined in the south african national water act nwa 1998 are assessed since 1998 gush et al 2002 and more recently streamflow changes owing to climatic changes warburton et al 2012 the umngeni catchment supplies water to nearly 15 of sa s population including the two largest cities pietermaritzburg with 220 000 people and durban with 600 000 people stats sa 2012 thus the catchment is home to a significant population who are potentially vulnerable to climate change especially given the projected increase in the occurrence of extreme hydrological events within the umngeni catchment there are four major dams alberts falls midmar nagle and inanda which are used for water supply and leisure boating and fishing activities fig 1 these activities are highly dependent on water availability and are likely to be affected by climate change the catchment is regarded as water stressed and regularly experiences droughts alexander 1985 diab and scott 1989 alexander 2001 summerton 2008 summerton et al 2010 as observed during the 2015 2016 rainfall season umngeni water 2016 for example the 2015 2016 season has been regarded as the worst drought season since 1904 with the kwazulu natal province being one of the hardest hit provinces in south africa saws 2017 losses resulting from this drought have been estimated at over r400 million agri sa 2016 the impacts were particularly felt by the farming community which comprises of both commercial and subsistence farmers with almost 10 000 provincial farmers affected leaving around 40 000 heads of cattle dead and severe crop losses agri sa 2016 some of the major dams storage capacity as of may 2016 were well below 50 with midmar dam at 45 72 and albert falls dam at 32 9 umngeni water 2016 as a result umgeni water has reduced the production of potable water water rationing and restrictions were implemented and this is likely to affect people s livelihoods umngeni water 2016 given the population at risk of these extreme hydrological events and financial losses incurred thereof it is important that we understand the frequency of occurrence and impacts of these events in the umngeni catchment however this requires an understanding of the uncertainties associated with the downscaled output in the first place previous studies considering climate change impacts on hydrology in the umngeni catchment see e g tarboton et al 1992 summerton 2008 summerton et al 2009 summerton et al 2010 warburton et al 2012 did not consider the uncertainty of downscaled products in capturing extreme high and low flows yet the understanding of these uncertainties is important in improving characterisation of climate related hydrological impacts in the catchment understanding how downscaled climate output captures extreme hydrological events and extreme river dynamics increases our confidence in the use of downscaled climate products for future hydrological projections as well as climate change impact studies 2 2 downscaled climate model output the downscaled climate output were obtained from the climate system analysis group at the university of cape town csag uct the climate data were statistically downscaled from the fifth phase of the coupled model inter comparison project cmip5 at csag uct using the self organizing map downscaling somd method of hewitson and crane 2006 in a previous study kusangaya et al 2016 observed that out of the ten downscaled gcms used only miub echo g e11 gfdl cm2 0 g11 mri cgcm2 3 2a mr1 csiro mk3 5 cs1 and mpi echam5 e12 simulated historical rainfall 1960 2000 satisfactorily with cumulative rainfall found to be within 10 of historical rainfall in the majority 7 13 of water management units of the umngeni catchment additionally evaluation of the intra gcm variability showed that these five dgcm showed limited variability for mean rainfall as all dgcms inter quartile ranges were between 15 based on these findings climate output from the five dgcms were considered acceptable 10 of rainfall mean and 15 of rainfall variability to used as input to the acru hydrological model in simulating streamflow table 1 provides the details of the five selected dgcms 2 3 hydrological modelling downscaled gcms do not simulate streamflow thus projected hydrological time series were obtained through inputting dgcm climate data to drive a rainfall runoff model and obtain projected streamflow responses the acru hydrological model was used to simulate streamflow for the period 1961 2000 under a baseline land cover of acocks 1988 veld types the acru model schulze 1995 is a physical conceptual daily time step multi level multi purpose model that is conceptualized to adequately represent hydrological processes and has also been shown to adequately represent catchment responses under different climate scenarios the acru model has been applied extensively in both land use and climate change impact studies in different hydro climatological regions of southern africa as well as in modelling high and low flows e g tarboton et al 1992 schulze and perks 2000 schulze 2000b schulze et al 2003 schulze 2005 graham et al 2011 schulze 2011 kienzle et al 2012 warburton et al 2010 showed that the acru model can be used successfully for climate change impact studies in catchments with diverse land uses and climate regions this increases the confidence that the acru model provides a suitable and accurate representation of historical streamflow and reduces the uncertainty regarding the model s ability to simulate adequately different climate scenarios as obtained from the different dgcms warburton et al 2012 the acru hydrological model is designed to simulate daily soil water budgets from which monthly and annual values can then be derived smithers and schulze 1995 smithers et al 1997 schulze and perks 2000 the generation of streamflow in acru is based on the premise that after initial abstractions interception depression storage and infiltration streamflow produced is a function of the magnitude of the rainfall and the soil water deficit from a critical response depth of the soil schulze 1995 the acru hydrological model is not a model in which parameters are calibrated to produce a good fit rather values of input variables are estimated from the physical characteristics of the catchment smithers and schulze 1995 using available information details of acru configuration used in this study can be obtained from warburton et al 2010 the acru model is explained in detail by schulze 1995 and smithers and schulze 1995 fig 2 illustrates the conceptualization of the water budget in the acru model parameter input values soils catchment characteristics topography etc as used by warburton et al 2010 in the configuration of the acru model were adopted for this study the umngeni catchment was delineated into 13 wmus which were further subdivided into 145 sub catchments with respect to altitude topography soil properties and land cover as well as the river sub catchment drainage system the sub catchments were configured to cascade downstream in a logical sequence representative of river flow fig 3 cumulative streamflows from the outlets of five water management areas were selected for analysis of hydrological alteration namely midmar and karkloof in the upper reaches of the catchment table mountain and nagle in the middle reaches of the catchment and durban at the lower catchment outlet fig 3 for the simulation of historical streamflow daily rainfall data required as input to the acru model were extracted from a daily rainfall database for south africa compiled by lynch 2004 for the historical time period 1960 2000 daily temperatures were also extracted from a gridded database of daily temperatures for south africa compiled by schulze and maharaj 2004 for the same time period 1960 2000 on the other hand for the simulation of gcm streamflow statistically downscaled gcm rainfall and temperature data obtained from csag were used to as input for the acru hydrological model for the different downscaled climate models from 1960 to 2000 the acocks 1988 veld types baseline land cover were used in simulating streamflows for both the historical and dgcm based streamflow simulations that would occur under natural land cover as stated in kusangaya et al 2017 the acocks veld type maps are the most scientifically respected and generally accepted maps of natural vegetation for south africa assessment of extreme hydrological events was carried out on cumulative streamflow data at the catchment outlet streamflow in acru based on the assumption that any variation of streamflow at the catchment outlet is representative of variation accumulated from the whole catchment i e it is the integrated flow at a point of the entire upstream area streamflows thus carry the memory or footprint of all rainfall runoff processes dependent on the climatic regime soils slope and land cover which occurred upstream 2 4 assessment of hydrological variability for this study the indicators of hydrologic alteration iha method richter et al 1996 richter and thomas 2007 of the us nature conservancy http www nature org was adapted to assess how streamflow simulated using downscaled climate output adequately captures and approximates streamflow simulated using historical climate data the iha method gives a comprehensive list of indices which are widely and effectively used to analyse both high and low flows previous studies have successfully applied the methodology of iha to assess impacts of anthropogenic drivers such as dam construction on streamflow e g taylor et al 2003 mathews and richter 2007 yang et al 2008 zhang and döll 2008 zhang et al 2009 de winnaar and jewitt 2010 kim et al 2011 saraiva okello et al 2015 furthermore olden and poff 2003 evaluated patterns of statistical variation among 171 published hydrological indicators and concluded that the 33 iha indices capture the majority of the variation and thus can be used to represent the major aspects of the flow regimes recently wobus et al 2015 also effectively adapted the iha methodology to characterize changes in the hydrology under a range of future climate scenarios iha can thus be considered to be a robust methodology as it can indicate the degree of uncertainty in capturing extreme hydrological events between streamflow simulated using dgcm output and streamflow simulated using historical climate data the data record length for analysis was from 1961 to 2000 the iha approach compares hydrological data sets by calculating multi variate statistics to assess the degree of hydrological alteration therefore enabling the characterization of river and catchment conditions puckridge et al 1998 clausen and biggs 2000 pettit et al 2001 and responses of the catchments to climate forcing wobus et al 2015 this multi variable approach allows the investigation of the multi impacts of hydrological changes on the river channel and associated ecosystems mathews and richter 2007 the iha software calculates a total of 67 parameters subdivided into two groups 33 hydrologic alteration ha parameters and 34 environmental flow component efc parameters richter et al 1996 for this study 26 of the 33 ha group 1 2 and 4 and 20 of the 34 efc were adapted for assessing hydrologic alterations in terms of streamflow magnitude frequency duration and rate of change as well as performing a range of variability analysis table 2 these chosen indices are deemed cable of completely characterising hydrological characteristics of both high and low flows the flows exceeding the 75 q75 of the daily flows for the 40 year period were classified as high flows flows that were below the 25 q25 of the daily flows for the 40 year period were classified as low flows a small flood event was defined as an initial high flow with a peak flow greater than that of the 2 year return interval event whilst a large flood event was taken as an initial high flow with peak flow greater than that of the 10 year return interval event an extreme low flow was defined as an initial low flow below 10 of daily flows for the period frequency refers to how often a flow above a given magnitude recurs over some specified time interval duration is the period of time associated with a specific flow condition peak flow denotes the maximum flow during an event and timing refers to julian date of the flow nature conservancy 2009 non parametric median and percentile statistics were used because of the skewed non normal nature of hydrological datasets ha parameters are calculated and organized in the output tables by water year from october 1 to september 30 the hydrologic alteration factor for each of the parameters was calculated as outline by richter et al 1996 richter et al 1998 mathews and richter 2007 as follows 1 for each parameter iha divides the full range of pre impact data i e simulations from historical data into three different categories generally percentiles e g lowest third middle third and highest third 2 the post impact data i e simulations from dgcm output is then analysed and the observed distribution of data with the distribution expected from the pre impact data compared 3 hydrologic alteration ha factor was calculated as observed frequency expected frequency expected frequency 4 a positive ha factor means that the frequency of values in the category percentile grouping had increased in the post impact period while a negative ha factor means that the frequency of values in the category percentile grouping had decreased in the post impact period the maximum ha value is infinity with a minimum value of 1 a positive ha value implies that the frequency of values in the category high medium and low rva has increased from the streamflow simulated using the historical climate data pre impact to the streamflow simulated using the dgcm output post impact a negative value shows that the frequency of values in the category is lower for the streamflow simulated using the historical climate data as compared to streamflow simulated using the dgcm output see richter et al 1997 richter et al 1998 mathews and richter 2007 2 4 1 range of variability analysis the range of variability approach rva described in richter et al 1997 was implemented within the iha for analysing the change between flows simulated using historical rainfall vs flows simulated using downscaled climate output the rva used the flows simulated using historical climate data as a reference for defining the extent to which natural flow regimes differed from flows simulated using dgcm output for the rva the streamflow data were split into 3 categories 1 data above 66th percentile as high rva analysis of high flows 2 data between 33rd and 66th percentile as medium rva and 3 data below 33rd percentile as low rva analysis of low flows it was hypothesised that the high rva would capture the increase or decrease of high values including extreme high flow values floods in streamflow simulated using dgcm output likewise the low rva was hypothesised to be able to capture the increase or decrease of low values including extreme low flow values droughts in streamflow simulated using dgcm output hydrologic alteration factors quantifying the degree of alteration for the 33 iha flow parameters table 2 were calculated 3 results the following sections detail the performance of the dgcms in capturing historical rainfall the performance of the acru model in simulating streamflow and how the simulated streamflow simulated using dgcm output captures the spatial variability and magnitudes of high and low flows compared to streamflow simulated using historical climate data 3 1 evaluation of the downscaled rainfall kusangaya et al 2016 in a study aimed at evaluating how downscaled climate output represented the underlying historical precipitation characteristics beyond the means and variances showed that using the means and variances most dgcms could satisfactorily represent historical rainfall e g 7 10 dgcms were within 10 of historical rainfall mean whilst few 3 10 dgcms did not capture the mean and variability of rainfall well table 3 additionally beyond the means and variances the performance of the downscaled rainfall showed that high rainfall events were well captured whilst raindays and low rainfall events were poorly captured furthermore raindays and low rainfall events were poorly captured in summer december february as compared to the low rainfall winter months june august 3 1 1 evaluation of extreme rainfall evaluation of extreme rainfall was carried out using the number of raindays with rainfall exceeding 50 mm per day to the number of raindays with rainfall exceeding 200 mm per day results table 4 indicated that all the gcms captured the extreme raindays very well with correlation coefficient ranging from 0 923 to 0 998 3 2 evaluation of the performance of the arcu hydrological model warburton et al 2010 showed that overall the acru model performed well on each of the four wmus which had flow measuring stations hence the acru model can confidently to simulate streamflows of the mgeni catchment for example for the mpendle wmu the total flows are adequately simulated with the percentage difference between the observed and simulated standard deviation were found to be less than 15 the r2 of daily values was 0 836 and the nash sutcliffe ef was 0 802 therefore simulation of streamflow in the mpendle wmu was considered highly acceptable the lions river wmu on the other hand similarly produced acceptable results with an r2 of 0 882 for the karkloof wmu the nash sutcliffe ef of 0 655 and the other statistics were also considered acceptable table 5 the results of the confirmation study for the henley wmu were considered reasonable as all statistics except for the percentage difference between the standard deviations were acceptable and comparison of daily simulated and observed streamflows indicated that the variability of streamflow was adequately simulated 3 3 annual and monthly variability of streamflows streamflow simulated using historical climate data had a greater range of annual flow variability 0 002 0 016 m3 s when compared with streamflow simulated using dgcm output with variability ranging from 0 001 to 0 010 m3 s fig 4 the inability of streamflow simulated using dgcm output to capture the variability in annual data series implied that streamflow simulated using dgcms is not capable of replicating extreme river dynamics high and low flows observed in streamflow simulated using historical climate data on a monthly basis it was also established that streamflow simulated using historical climate data was largely underestimated by as much as 50 in the high rainfall months in summer december march this could largely be attributed to the failure of the dgcm simulated streamflow in capturing local storms and related streamflow peaks which normally occur during the summer season these summer months were also characterised by a greater range in variability in all the study catchments for the winter months may july streamflow simulated using historical climate data was underestimated fig 5 shows the monthly median flow variability for midmar karkloof nagle and durban catchments results from the deviation factor and the corresponding significant count analysis table 6 indicate that for all catchments midmar karkloof nagle and durban the deviation of the streamflow simulated using dgcms from the streamflow simulated using the historical data were highly significant in most of the metrics used this implied that there were significant differences between the streamflow simulated using historical climate data and streamflow simulated using dgcm output 3 4 magnitude and duration of annual extreme flows a suite of indices depicting magnitude and duration of annual extreme streamflow conditions low and high flows indices were calculated these included the 1 3 7 30 and 90 day minimums and maximums taken from moving averages calculated as means for every possible period that is completely within the water year 1 october to 30 september nature conservancy 2009 results indicated that for all wmus the magnitude of low flows increased with increasing frequency of low flows 1 3 7 30 90 day minimum fig 6 even though the simulated streamflow from dgcms failed to correctly capture the magnitude underestimation the increasing pattern from 0 001 for 1 day minimum to 0 004 for the 90 day minimum was well captured in all the wmus the magnitude of high flows decreases with increasing frequency of high flows 1 3 7 30 and 90 day maximums fig 7 for most of the wmus high flows were overestimated especially in the catchment areas of umngeni catchment receiving moderate rainfall between 700 and 900 mm on the other hand for the high rainfall karkloof wmu streamflow simulated using dgcms climate output underestimated streamflow simulated using historical climate data again as in the low flow analysis the decreasing pattern with increasing frequency 1 3 7 30 and 90 day maximums in high flow magnitude was well captured in all wmus 3 5 hydrological variability of high and low flows as stated above for the rva the streamflow data were split into three categories a data above 66th percentile as high rva analysis of high flows b data between 33rd and 66th percentile as medium rva and c data below 33rd percentile as low rva analysis of low flows it was hypothesised that the high rva would capture the increase or decrease of high values including extreme high flows floods in streamflow simulated using dgcm output likewise the low rva was hypothesised to be able to capture the increase or decrease of low flows including extreme low values droughts in streamflow simulated using dgcm output overall results from the four selected wmus showed that in the high and middle rva ranges the majority of iha parameters considered in this study were characterised by negative ha values a negative value means that the frequency of values in the high and middle rva ranges was lower for the streamflow simulated using the historical climate data as compared to streamflow simulated using the dgcm output this implied that streamflow simulated using dgcm output did not capture all the high and extreme high flow values as captured in the streamflow simulated using historical climate data similarly all iha parameters were characterised by negative ha values for the low rva this implied that the frequency of values in the low iha category was lower for the streamflow simulated using the historical climate data as compared to streamflow simulated using the dgcm output this meant that streamflow simulated using dgcm output was capturing less episodes of low flows as compared to the low flows simulated using historical climate data fig 8 shows a summary of hydrological indicators for all the parameters analysed for high and low flow assessments using the iha range of variability approach generally fig 8 shows that high rva for high flow analysis the ha factor for low pulse count high pulse count rise rate and fall rate is more than one for karklof midmar nagle and durban wmus on the other hand the 1 3 and 7 day minimum as well as the monthly median flows were also characterised by a ha greater than one for the low rva for low flow analysis as stated before a positive ha value implies that the frequency of values in the category high or low rva has increased from the streamflow simulated using the historical climate data pre impact to the streamflow simulated using the dgcm output post impact 3 6 extreme high and low flow parameters using the range of variability analysis of environmental flow conditions parameters examining extreme high low flows were considered of the 20 the following 10 efc parameters were found to be significantly significant count less than 50 different from the streamflow simulated using historical climate data from at least one dgcm extreme low peak extreme low frequency high flow peak high flow duration high flow frequency high flow rise rate high flow fall rate small flood duration small flood rise rate small flood fall rate fig 9 as such the remaining 10 parameters extreme low duration extreme low timing high flow timing small flood peak small flood timing large flood peak large flood duration large flood timing large flood rise rate and large flood fall rate showed that there was no significant difference in these parameters between streamflow simulated using historical climate data and streamflow simulated using dgcm output fig 9 most of the streamflow simulated using climate data from dgcms successfully captured the large flood peak between 0 4 and 1 0 m3 s fig 10 a except for dgcm e11 which was characterised by variability ranging between 0 5 and 1 3 m3 s large flood duration however was underestimated by streamflow simulated using dgcm output fig 10b by as much as 25 30 days extreme low flow frequency fig 10c was overestimated in streamflow simulated using historical climate data than streamflow simulated using dgcm output extreme low flow duration fig 10d on the other hand was comparable all between 2 and 4 days between simulated streamflow from downscaled dgcms output and historical climate data 4 discussion and conclusions this study tested how simulated streamflow using dgcm output captures river dynamics high and low flows in the umngeni catchment of south africa results of this study indicate that in moderately high to high rainfall catchments streamflow simulated using historical climate data were underestimated compared to streamflow simulated using dgcms output this was despite the fact that all the chosen dgcms had captured overall rainfall mean and variability reasonably well e g were within 10 of historical rainfall mean such a result implies that dgcms introduce greater uncertainty in high rainfall areas possibly owing to the failure of the dgcms to capture adequately other rainfall characteristics such as a total rainfall quantity b the interval between rainfall events or c individual event size d rainfall duration and e rainfall intensity thus when used to simulate streamflow it resulted in the peak duration and frequency of streamflow events not being adequately captured especially in high rainfall areas the inability of dgcms to correctly represent rainfall characteristics is widely reported in literature see e g nemeth 2010 prudhomme et al 2010 tramblay et al 2012 willems et al 2012 ntegeka et al 2014 recently teng et al 2015 showed that raw regional climate model rcm precipitation exhibited negative errors in annual and seasonal means with the median errors in raw rcm annual means generally it has been established that input data uncertainty in particular uncertainty associated with rainfall data input significantly affects the total simulation uncertainty andréassian et al 2001 pappenberger et al 2005 on the other hand schulze 1995 established that output simulated by the acru hydrological model is most sensitive to input rainfall consequently the uncertainties in river dynamics high and low flows described in this study were hypothesised to be largely due to the uncertainties in the precipitation outputs from dgcms rather than uncertainties in the acru model itself the results obtained in this study for umngeni catchment thus concur with other studies elsewhere as cited above during the summer rainfall months simulated streamflow was characterised by greater variability and during the low rainfall winter months simulated streamflow was characterised by less variability this result suggests that for summer rainfall characterisation dgcms fail to capture the extremes of rainfall i e rainfall peaks this error was carried on and even propagated to the simulated streamflow as demonstrated by poor detection of simulated streamflow extremes this is understandable since rainfall is the major driver of the hydrological cycle thus largely determining characteristics of simulated streamflow the above conclusions have been collaborated in literature for example chiew et al 2009 teng et al 2012 teng et al 2015 have reported the so called drizzle effect whereby downscaled climate output simulates too many low intensity precipitation events and too few high intensity precipitation events these results are also in agreement with recent findings from coppola et al 2014 who using eight scenario runs from two regional climate models established that the uncertainty associated with the winter streamflow change was larger compared to that in the fall this study was carried out in italy for the po river which is characterised by two annual discharge maxima one in spring because of snow melting in the alps and one in the fall due to rainfall in the area at this time of the year the umngeni catchment however only experiences one annual discharge maximum as it receives most rainfall during the summer months december march when streamflow data were split into percentiles median flows for all the streamflow data above 67th percentile and data between 33rd and 66th percentile was largely characterised by negative ha values implying that the frequency of values in the high and middle rva ranges was lower for the streamflow simulated using the historical climate data this implied that streamflow simulated using dgcm output did not capture the high and extreme high flows that were captured in the streamflow simulated using historical climate data it is thus concluded that dgcms do not adequately capture extreme high values characteristic of for example cyclonic heavy storm events further translating to extreme high flows not being well represented in hydrological modelling these results are in agreement with results by mandal et al 2016 who found that a large amount of uncertainty was evident in summer high precipitation months june july and august as compared to the low rainfall periods on the other hand for the data below the 33rd percentile positive ha meant that the frequency of values in the low ha category was higher for the streamflow simulated using the historical climate data as compared to streamflow simulated using the dgcm output this implies that simulated streamflow using dgcm output captured more low flows and more extreme low flows than what actually was occurring as in the streamflow simulated using historical climate data the so called drizzle effect reported also by teng et al 2012 consequently simulated dgcm streamflow would show more drying droughts than actually exists thus as reported also in nemeth 2010 prudhomme et al 2010 willems et al 2012 ntegeka et al 2014 downscaled climate products are unable to capture the spatial variability and magnitudes of extreme hydrological events which are often of particular concern in hydrology although the method of iha is effective as it uses a range of indices to analyse various aspect of flow regimes it utilises an extensive set of indices which in most cases are correlated olden and poff 2003 gao et al 2009 hence this results not only in a level of numerical redundancy in data processing but also in considerable information redundancy yang et al 2008 identified a small subset of hydrological indicators that were the most representative of ecological flow regimes oueslati et al 2010 used principal components analysis to reduce 40 hydrological indices to identify subsets of hydrological indices that describe the major sources of variations while minimizing redundancy and showed that the number of statistically significant principal components varied from three to four for this study a total of 40 of the 67 iha indices 26 of the 33 ha group 1 2 and 4 and 20 of the 34 efc were used this resulted not only in a level of numerical redundancy in data processing but also in considerable information redundancy however the ideal approach as reviewed from literature is to use a suite of indices in the analysis of both high and low flows see e g khomsi et al 2016 from the above discussion this study concludes that developing a small number of statistics that capture key components of hydrologically relevant flow variation will 1 contribute to a general approach for characterizing flow alteration 2 minimize statistical redundancy and computational effort in future analyses the overall conclusion of this study is that dgcm output were characterised by higher uncertainties especially in capturing extreme hydrological parameters such as extreme high and low flows streamflow simulation using dgcm output does not capture extreme hydrological events which are often of particular concern in design hydrology especially under conditions of increasing frequency of extreme hydrological events floods and droughts and the subsequent changes in river dynamics high and low flows consequently streamflow simulated using dgcms should be used with caution for hydrological applications particularly when considering extreme flows for example ruelland et al 2015 cautioned that the use of streamflow simulated from gcms must be limited to the analysis of mean shifts in the future and that it cannot be designed for analysing the impacts of potentially more intense drought or flood events it is therefore recommended that downscaling techniques be improved in order to generate climate data that are relevant for hydrological assessments be that as it may the availability of downscaled climatic output provide the potential of exploring climate model uncertainties in different hydro climatic regions at local scales where forcing data is often less accessible but more accurate at finer spatial scales and with adequate spatial detail acknowledgements we would like to thank the applied centre for climate and earth systems science access for supporting this research work by providing funds for undertaking the phd research under the theme climate change impacts and adaptation additionally we would like to thank the climate system analysis group at the university of cape town for providing the downscaled rainfall data 
7536,natural or planted vegetation at the edge of fields or adjacent to streams also known as vegetative filter strips vfs are commonly used as an environmental mitigation practice for runoff pollution and agrochemical spray drift the vfs position in lowlands near water bodies often implies the presence of a seasonal shallow water table wt in spite of its potential importance there is limited experimental work that systematically studies the effect of shallow wts on vfs efficacy previous research recently coupled a new physically based algorithm describing infiltration into soils bounded by a water table into the vfs numerical overland flow and transport model vfsmod to simulate vfs dynamics under shallow wt conditions in this study we tested the performance of the model against laboratory mesoscale data under controlled conditions a laboratory soil box 1 0 m wide 2 0 m long and 0 7 m deep was used to simulate a vfs and quantify the influence of shallow wts on runoff experiments included planted bermuda grass on repacked silt loam and sandy loam soils a series of experiments were performed including a free drainage case no wt and a static shallow water table 0 3 0 4 m below ground surface for each soil type this research first calibrated vfsmod to the observed outflow hydrograph for the free drainage experiments to parameterize the soil hydraulic and vegetation parameters and then evaluated the model based on outflow hydrographs for the shallow wt experiments this research used several statistical metrics and a new approach based on hypothesis testing of the nash sutcliffe model efficiency coefficient nse to evaluate model performance the new vfsmod routines successfully simulated the outflow hydrographs under both free drainage and shallow wt conditions statistical metrics considered the model performance valid with greater than 99 5 probability across all scenarios this research also simulated the shallow water table experiments with both free drainage and various water table depths to quantify the effect of assuming the former boundary condition for these two soil types shallow wts within 1 0 1 2 m below the soil surface influenced infiltration existing models will suggest a more protective vegetative filter strip than what actually exists if shallow water table conditions are not considered abbreviations bgs below ground surface vfs vegetative filter strip vfsmod vegetative filter strip modeling system keywords best management practices buffer infiltration shallow water table vegetative filter strip 1 introduction a commonly used method for reducing sediment nutrient and pesticide loadings from agricultural fields is the edge of field vegetative filter strip vfs popov et al 2005 reichenberger et al 2007 fox et al 2010 2011 muñoz carpena et al 2010 lacas et al 2012 a vfs reduces sediment nutrient and pesticide movement to streams by decreasing runoff volumes through infiltration into the soil profile allowing contact between dissolved phase solutes and vegetation and or by reducing flow velocities to the point where sediment and sorbed solutes can settle out of the water commonly vfs are placed adjacent to streams in riparian floodplains or adjacent to drainage ditches at the edge of an agricultural field where shallow groundwater tables can be present lacas et al 2005 dosskey et al 2011 carluer et al 2016 while it is commonly recognized that a shallow groundwater table may limit infiltration into a soil profile there is a lack of controlled experimental data available for demonstrating when this limitation becomes important various strategies have been used to model shallow groundwater tables in hydrologic models it is generally well known that shallow groundwater tables can significantly affect infiltration rates and therefore surface runoff during rainfall events for example salvucci and entekhabi 1995 and chu 1997 proposed approximate non uniform green ampt solutions for infiltration with ponded soils bounded by a water table however this effect is often ignored or handled simplistically in many common simulation models specifically as an example for vegetative filter strips or riparian buffers the riparian ecosystem management model remm simulates water movement and storage through a riparian buffer including subsurface lateral flow upward flux from the water table and deep seepage lowrance et al 2000 tilak et al 2014 however water storage and movement in remm and other models is based on simplistic mass balance rate controlled approaches and operates on a daily time scale a widely used watershed model swat arnold and fohrer 2005 that contains a vfs component derived empirically from other model simulations white and arnold 2009 does not account for shallow water table effects a more robust and mechanistic modeling package is needed that is capable of operating at the event time scale process based models can predict vfs efficiency for pollutant removal and thereby allow site specific design of vfs physical characteristics length width slope and type of vegetation the design process is heavily dependent on being able to predict infiltration and sedimentation processes in the vfs one such model vfsmod is a field scale mechanistic storm based numerical model that estimates water and sediment retention for single storm or a time series of storm events muñoz carpena et al 1999 muñoz carpena and parsons 2004 2008 vfsmod routes an incoming hydrograph and sedimentograph from an adjacent field through a vfs to calculate the resulting outflow infiltration and sediment trapping muñoz carpena et al 1993a b 1999 infiltration is modeled based on the green ampt equation for unsteady rainfall the model includes an automated and robust inverse calibration routine ritter et al 2007 to optimize input parameters based on the global multilevel coordinate search algorithm huyer and neumaier 1999 in sequential combination with the local nelder mead simplex algorithm nelder and mead 1965 vfsmod has been globally used to optimize filter placement and design kuo and muñoz carpena 2009 balderacchi et al 2016 pan et al 2017 and has been integrated into pesticide exposure assessment frameworks e g sabbagh et al 2010 bach et al 2017 a recently released version of vfsmod considers the presence of a static shallow water table wt hereon in order to expand its applicability across a wide range of field conditions muñoz carpena et al 2017 lauvernet and muñoz carpena 2017 the swingo shallow water table infiltration algorithm component in vfsmod is a modified form of the integral solution to ponded infiltration for soils bounded by a water table proposed by salvucci and entekhabi 1995 and chu 1997 the modification included making the solution numerically explicit in time and adding new integral formulae for calculation of the singular infiltration times time of ponding and time to soil profile saturation the new routine was validated against a numerical solution of richards equation for varying wt depths and rainfall intensities on five distinct soils muñoz carpena et al 2017 and evaluated on two benchmark studies through global sensitivity and uncertainty analysis lauvernet and muñoz carpena 2017 these preliminary investigations demonstrated that the wt influenced infiltration and runoff for depths shallower than 1 0 1 5 m but was negligible for deeper water tables also soils that exhibited a marked i e more definitive air entry bubbling pressure head on their soil water characteristic curve were more prone to surface hydrology changes as quick saturation was reached when the wetting front reached the capillary fringe above the water table global sensitivity analysis of the modified model showed that wt depth was the first or second most important factor next to saturated hydraulic conductivity in controlling the changes in surface flow sediment and pesticide trapping of the vfs lauvernet and muñoz carpena 2017 however there remains a lack of experimental data for systematic validation of the updated model under controlled hydrologic conditions lauvernet and muñoz carpena 2017 specifically call for laboratory and field research detailing the response of a vfs under both deep and shallow wts therefore the research objectives were to i investigate the influence of shallow wts on outflows from a vfs using meso scale laboratory soil box ii evaluate the performance of a new shallow wt algorithm in vfsmod for simulating shallow wt effects and iii utilize vfsmod to determine the depth at which shallow wt conditions influence vfs effectiveness the research evaluated the performance of vfsmod for experiments without free drainage and with a shallow wt using several statistical metrics including a hypothesis testing approach recently proposed by ritter and muñoz carpena 2013 2 methods and materials 2 1 brief description of the shallow water table modeling component the original infiltration component of vfsmod was based on a modification of the green ampt equation for unsteady rainfall with no restrictions due to the presence of a wt chu 1978 skaggs and khaheel 1982 muñoz carpena et al 1993b full details of the new wt algorithm within vfsmod are provided in muñoz carpena et al 2017 and lauvernet and muñoz carpena 2017 here we provide a brief description to ground the experimental setup and testing of the model performed in this study the new wt algorithm calculates the actual infiltration rate f eq 1 at the vfs soil surface for each time t t assuming the soil surface is not ponded at the beginning of the event 1 f i 0 t t p k s 1 z f 0 l z f k h dh t p t t w min f w i t t w where i i t l t is rainfall intensity z l is the depth from the surface zf l is the wetting front depth l l the depth from the surface to the water table k k h l t is the soil water hydraulic conductivity as a function of the soil suction h l non uniform with depth k s l t is the saturated hydraulic conductivity tp t is the time to surface ponding from the beginning of the event and tw t and fw l t are the time and the vertical flow boundary condition when the wetting front reaches the water table or its capillary fringe hb at depth zw l fig 1 two options for fw are provided a vachaud and thony 1971 condition is commonly used in experiments corresponding to vertical saturated flow fw ks salvucci and entekhabi 1995 liu et al 2011 this boundary condition can overestimate the final f in some field situations particularly when the wt drains to the nearby stream for this case another option is available to simulate lateral drainage following dupuit forchheimer assumptions van hoorn and van der molen 1973 assuming a water table slope equal to the soil surface slope so beven and kirkby 1979 vertessy et al 1993 2 f w k sh s o z w vl where ksh l t is the lateral horizontal soil saturated hydraulic conductivity vl l is the filter length vfs dimension in the flow direction and zw l is the effective saturation depth that depends on l and the soil air entry pressure hb l zw l hb zw 0 zw 0 when l hb i e the soil is effectively saturated by the capillary fringe in this study we applied the experimental vachaud and thony condition the soil water hydraulic conductivity k k h and water content characteristics θ θ h l3 l3 are described in this study based on mualem 1976 and van genuchten 1980 the dynamic coupling of the infiltration rate under wt conditions eq 1 with vfsmod surface and transport routines is done through the lateral recharge term left hand side of eq 3 based on the finite element solution of the kinematic wave equation muñoz carpena et al 1993a 3 h s t q x i f s f s o q s o n h s 5 3 with initial and boundary conditions h s 0 0 x vl t 0 h s h s o x 0 t 0 where hs hs x t l is the overland flow depth q q x t l2 t is discharge per unit width x l is the surface flow direction axis so and sf l l are the bed and water surface friction slopes at each node of the system n is manning s roughness coefficient dependent on soil surface condition and vegetative cover at each node of the system and hs o hs o 0 t l represents the hydrograph runon from the adjacent field input as a time dependent boundary condition at the first node of the vfs surface finite element grid fig 1 the solution of eq 1 in each time step is based on a combination of approaches by salvucci and entekhabi 1995 and chu 1997 with the assumption of a horizontal wetting front with new integral formulae muñoz carpena et al 2017 lauvernet and muñoz carpena 2017 for calculating the required singular times time of ponding tp and time tw when the wetting front reaches the saturated capillary fringe region above the water table at depth zw fig 1 since the solution to eq 1 is implicit i e z depends on f cumulative infiltration f and the singular times depend on z a numerical integration scheme is used 2 2 laboratory experiments a laboratory soil box fig 2 was constructed to simulate the hydraulics in a vfs with dimensions of 1 0 m wide 2 0 m long and 0 7 m deep based on the setup used by fox et al 2011 the laboratory soil box was designed to allow the control of the groundwater level in the soil profile the 2 0 m length fell within the range of vfs flow lengths reported in previous field and laboratory studies 0 5 m to 29 m vfs lengths were reported by sabbagh et al 2009 and poletika et al 2009 0 7 2 7 m lengths were used by stout et al 2005 in laboratory experiments experiments were conducted with two different soils packed into the column the first soil was a silt loam with 19 4 sand 58 3 silt and 22 3 clay based on hydrometer tests on three samples the silt loam soil was packed into the box to a bulk density of 1 40 g cm3 in two lifts approximately 0 3 m deep the other soil was a sandy loam with 54 9 sand 41 4 silt and 3 7 clay the soil was packed to a bulk density of 1 35 g cm3 in two lifts a 7 0 slope was used for all experiments and represented an average slope reported in previous literature muñoz carpena et al 2010 and fox et al 2010 reported 2 5 10 0 slopes to represent a typical vegetation used in a vfs haan et al 1994 bermuda grass cynodon dactylon sod was established in the laboratory soil box the sod was well watered and allowed to establish for two weeks prior to conducting the experiments the laboratory soil box included a porous steel screen at the bottom of the soil bed 70 cm below ground surface bgs to drain the soil profile fox et al 2011 used this setup to simulate an atmospheric boundary condition at the bottom of the soil box percolate that reached the bottom of the steel screen exited through a pipe at the bottom fig 2 the soil box was modified for these experiments by installing a water standpipe that connected to the pipe at the bottom to measure the groundwater table height fig 2c dynamic shallow wts were measured using an automated water level logger hoboware onset computer corp cape cod ma accuracy of 0 5 cm the loggers monitored water pressure and temperature every 60 s a static water table position was maintained by inserting the intake hose of a peristaltic pump into the standpipe fig 2c to the desired wt height and pumping at a sufficient rate to remove all percolate the inflow runon was applied at a constant rate using a peristaltic pump and distributed across the entire vfs width 1 0 m using a plexiglas weir the vfs experienced a step inflow hydrograph with typical inflow rates of 0 05 0 07 l s as opposed to simulating a more natural runon hydrograph to simplify the experimental procedures such step inflow hydrographs are commonly used in vfs field studies for example see sabbagh et al 2009 for a review of several such studies inflow rates were quantified prior to the experiment by recording the time to fill a 2 l container in triplicate trials the outflow runoff was collected by a lateral flume draining to a collector and measured using a weighing scale every 5 s the outflow rates were smoothed with a 60 s moving average sigmaplot v 12 san jose ca to remove sensor noise associated with the high frequency 5 s measurements this research conducted four vfs experiments including both a free drainage experiment and a static wt experiment for each soil type inflow water was applied in each experiment for at least 60 min the monitoring period varied depending on time required for the outflow to reach steady state the free drainage experiments matched those reported in fox et al 2011 with no control on the wt position percolated water was removed from the soil box using the peristaltic pump and the percolate flow rates recorded over time the static water table experiments involved maintaining the groundwater level 0 4 or 0 3 m bgs for the silt loam and sandy loam soil types respectively a static shallow wt was maintained by using the peristaltic pump to extract water from the standpipe at the desired shallow wt height the static wt height was continuously verified with a water level logger in the standpipe percolated water pumped from the standpipe was measured to quantify percolation flow rates cumulative infiltration was calculated as the cumulative inflow minus the cumulative outflow the outflow rates were compared to the inflow rates to quantify hydrological differences between the free drainage case and shallow wt conditions hypothetically for free drainage outflow rates should be less than the inflow rate due to continued infiltration into the soil profile for cases with a shallow wt the outflow rate should initially be less than the inflow rate due to available storage in the soil profile for infiltrated water unless the capillary fringe extends to or above the soil surface from the wt then over time the outflow rate should approach the inflow rate as less available water storage exists within the soil profile 2 3 numerical modeling with vfsmod in applying vfsmod to the laboratory scale experiments this research used the neural network soil hydraulic predictor rosetta v1 1 in retention curve retc presented by van genuchten 1991 to derive the initial estimates for the soil hydraulic parameters soil texture percent sand silt and clay and bulk density were input into rosetta as measured from soil hydrometer tests and soil samples from each laboratory setup the derived parameters included the saturated hydraulic conductivity ks l t residual moisture content θr l3 l3 saturation moisture content θs l3 l3 inverse of the air entry pressure head αs 1 l and van genuchten 1980 parameters n and m assuming m 1 1 n table 1 the free drainage experiment for each soil type was then used to inversely calibrate vfsmod previous studies commonly report the three most important input parameters as k s green ampt s suction at the wetting front sav and manning s roughness rna fox et al 2010 muñoz carpena et al 2010 the initial soil moisture content or antecedent moisture content was measured prior to each experiment by extracting a 2 cm diameter soil core and then refilling the hole with clay the moisture content was obtained by the standard gravimetric method all other soil hydraulic properties were held constant as derived from rosetta v1 1 and reported in table 1 while vfsmod allows the input of variable roughness along the vfs the manning s roughness was assumed constant along the length of the bermuda grass vfs the roughness was calibrated separately for each experimental setup as different plantings were used for other vegetation parameters default values were used as specified for bermuda grass in the vfsmod technical guidance after calibration of vfsmod based on the free drainage experiments for both soil types the corresponding static wt experiment was then simulated with vfsmod using the derived van genuchten parameters αs m and n as reported in table 1 note that the free drainage simulations did not require input of αs m and n an initial hydrostatic water pressure distribution was assumed above the wt to illustrate the impact of a shallow wt the shallow wt experiments were simulated in vfsmod assuming a free drainage boundary condition and with numerous hypothetical wt depths 0 1 2 0 m in order to quantify the effect of assuming a free drainage boundary condition several statistical metrics quantitatively assessed the performance of vfsmod in simulating the observed runoff hydrographs including the root mean squared error rmse the nash sutcliffe model efficiency coefficient nse nash and sutcliffe 1970 4 nse 1 t 1 t o i t o m t 2 t 1 t o i t o 2 where oi and om represent the observed values and the model estimates for the t observation t is is the total number of observations and o is the mean of the observed values and the normalized objective function nof used previously by fox et al 2004 5 nof t 1 t o i t o m t 2 t o note that the nse ranges from to 1 where 1 indicates a perfect match smaller nof values indicated a closer fit fox et al 2004 and site specific model applications should result in an nof value of less than 1 0 pennell et al 1990 loague and green 1991 as discussed by ritter and muñoz carpena 2013 interpretation of the various statistical metrics is often subjective and biased by number of data points repeated data and outliers therefore to strengthen model evaluation this research utilized fiteval a model evaluation tool based on hypothesis testing of the nse exceeding threshold values based on approximated probability distributions obtained by block bootstrapping ritter and muñoz carpena 2013 the technique quantified model performance into four classes proposed based on thresholds of the non linear relationship between nse and the ratio between signal range standard deviation of the observed data and noise model error rmse unsatisfactory acceptable good or very good 3 results and discussion for the free drainage experiments fig 3 the bottom of the column was an atmospheric boundary condition for the silt loam soil the outflow rate moving average was less than the inflow rate and typically between 71 and 86 of the inflow rate fig 3a therefore infiltration was occurring in the vfs reducing the runoff at the outlet fig 3b the cumulative groundwater percolated from the column was approximately 10 l at the time when the inflow water was terminated 60 min and increased to approximately twice that amount two hours after initiating the experiment fig 3c for the higher conductive sandy loam soil outflow rates were typically between 67 and 83 of the inflow rate throughout the experiment fig 3d as expected the higher ks of the sandy loam soil resulted in greater cumulative percolated water 12 5 l after 60 min than in that the silt loam fig 3e the shallow wts were positioned at 0 4 m and 0 3 m bgs for the silt loam and sandy loam experiments respectively fig 4 initial water table depths were selected to be near the soil surface upper 1 0 m but also approximately midway through the soil column different depths were utilized to provide different initial conditions between the shallow water table experiments for model evaluation note the difference in the time scales of these experiments inflow lasted 60 min longer in the silt loam experiment the outflow rates quickly approached 100 of the inflow rate for both soils fig 4a and d but did so faster in the less conductive silt loam soil at 60 min into the experiments percolated flow volumes were approximately 10 l for the silt loam soil fig 4c and 20 l for the sandy loam soil fig 4f also note that in the silt loam experiment several minutes after inflow was cutoff the pump was also stopped and the wt increased quickly after 140 min fig 4a for the silt loam free drainage experiment minimal calibration of vfsmod was necessary from the initial parameter values derived from the pedotransfer function table 2 fig 5 a statistical metrics included a nof of less than 0 20 and nse of 0 89 table 3 the model did predict an earlier arrival of the runoff at the end of the vfs than observed in the experiments and under predicted the runoff rates immediately after runoff initiated for the silt loam calibration for the free drainage case fig 5a fiteval results indicated that the fit passed the statistical significance test for model acceptability nse 0 65 with p 0 003 where the probabilities of very good nse 0 90 1 00 good nse 0 75 0 89 and acceptable nse 0 65 0 75 were 50 5 40 5 and 8 9 respectively see supplemental information fig s 1 for the sandy loam experiment vfsmod was able to match the early time shape of the runoff hydrograph and the time of runoff initiation but was delayed in predicting the falling limb of the outflow hydrograph fig 5c the discrepancy between calibrated soil hydraulic parameters and those estimated from the pedotransfer function rosetta v1 1 table 1 were greater for the sandy loam soil table 2 the nse and nof both indicated an improved calibration for the sandy loam soil as compared to the silt loam free drainage experiments table 3 the corresponding fiteval values for the sandy loam calibration included a valid probability of 99 5 p 0 003 with probabilities of very good good and acceptable of 57 2 33 6 and 8 7 respectively see supplemental information fig s 3 the vfsmod shallow wt component was able to predict both outflow hydrographs with nse 0 90 and nof 0 20 without calibration fig 5b and d an interesting dynamic appeared in the shallow ground water table simulations whereby the model predicted the outflow rates immediately equivalent to the inflow rate when the wetting front reached the static water table this occurred more quickly in the silt loam soil experiment than the sandy loam soil experiment because of the smaller αs the observed data from the laboratory experiments demonstrated a much more gradual increase in the outflow rate we hypothesize that this minor deviation was due to the assumed vertical boundary condition fw ks when t tw as shown in eq 1 the shallow wt experiments were also draining laterally within the bounded soil profile and this introduced an experimental artifact with respect to what was modeled this was also addressed by muñoz carpena et al 2017 and they suggested that likely some empirical weighting of vertical and lateral conditions might be needed under field conditions note that it would be possible to calibrate vfsmod further based on changing αs m and n but such an approach fell outside the scope of this research vfsmod simulations with shallow wts were considered valid see supplemental information figs s 2 and s 4 with a probability of 99 9 p 001 for the silt loam soil probabilities of very good good and acceptable of 69 9 26 8 and 3 2 respectively and 100 p 001 for the sandy loam soil probabilities of very good and good of 97 3 and 2 7 note that simulating the shallow wt experiments with the free drainage assumption significantly under predicted the outflow rates i e greater predicted infiltration as shown in fig 5 and thereby degraded model performance table 3 simulations using the free drainage conditions in the presence of a water table yielded nse values of 0 70 and 0 79 which some may consider satisfactory from a statistical standpoint by contrast when using the water table boundary conditions the simulation improved with nse values 0 90 however the practical implications of the model performance is significant as noted earlier the vfs trapping efficiencies for sediment fox and sabbagh 2009 pan et al 2017 pesticides sabbagh et al 2009 lacas et al 2005 2012 carluer et al 2016 bacteria fox et al 2011 and nutrients fox and penn 2013 depend directly on infiltration in the vfs in all of these cases simulating a free drainage boundary condition will over estimate infiltration and therefore over predict vfs trapping efficiency for sediment pesticides bacteria and nutrients suggesting a more protective vegetative filter strip than what actually exists such estimations would magnify when designing a vfs for a target trapping efficiency over a series of storm events or conducting long term exposure assessments sabbagh et al 2013 for the silt loam and sandy loam soils wt depths of less than approximately 1 0 1 2 m influenced infiltration and therefore the corresponding outflow hydrographs fig 6 these wt depth thresholds closely matched the modeling results performed with and without wts for other simulated soil and hydrologic conditions muñoz carpena et al 2017 lauvernet and muñoz carpena 2017 therefore the impact of shallow wts on vfs trapping efficiencies should be considered in cases where the wt is closer than 1 5 m as a conservative estimate bgs this research proved that the vfsmod mechanistic model is valid for handling this dynamic scenario when vfs are installed in areas with shallow wts such as adjacent to streams and drainage channels 4 conclusions a laboratory scale vfs was able to provide experimental data for validating the performance of new shallow wt routines in vfsmod the laboratory experiments provide a unique data set that currently does not exist in the literature under field conditions in general these routines successfully captured the dynamics of runoff under a shallow wt with minimal calibration and clearly demonstrated the importance of considering shallow wt effects on infiltration and the corresponding vfs removal efficiencies statistical metrics indicated that model performance was valid with greater than 99 5 probability across all scenarios for both free drainage and shallow wt conditions according to vfsmod simulations wt depths of less than approximately 1 0 1 2 m influence infiltration and therefore the corresponding vfs outflow hydrograph for the silt loam and sandy loam soils investigated in this research future research should further validate the vfsmod routines by monitoring in field vfs with different wt depths and the effect on vfs sediment nutrients and pesticide trapping efficiency in addition in further controlled field or laboratory studies the weighting of end time boundary conditions could be explored software availability the updated version of vfsmod can be downloaded free of charge at the following website http abe ufl edu carpena vfsmod acknowledgements this research was supported by foundational grant program grant no 2016 67019 26855 of the usda national institute of food and agriculture nifa titled influence of preferential flow on coupled colloid nitrogen and phosphorus transport through riparian buffers the authors acknowledge dr lucie guertault for reviewing an earlier version of this manuscript appendix a supplementary data supplementary data associated with this article can be found in the online version at https doi org 10 1016 j jhydrol 2017 10 069 appendix a supplementary data supplementary data 1 
7536,natural or planted vegetation at the edge of fields or adjacent to streams also known as vegetative filter strips vfs are commonly used as an environmental mitigation practice for runoff pollution and agrochemical spray drift the vfs position in lowlands near water bodies often implies the presence of a seasonal shallow water table wt in spite of its potential importance there is limited experimental work that systematically studies the effect of shallow wts on vfs efficacy previous research recently coupled a new physically based algorithm describing infiltration into soils bounded by a water table into the vfs numerical overland flow and transport model vfsmod to simulate vfs dynamics under shallow wt conditions in this study we tested the performance of the model against laboratory mesoscale data under controlled conditions a laboratory soil box 1 0 m wide 2 0 m long and 0 7 m deep was used to simulate a vfs and quantify the influence of shallow wts on runoff experiments included planted bermuda grass on repacked silt loam and sandy loam soils a series of experiments were performed including a free drainage case no wt and a static shallow water table 0 3 0 4 m below ground surface for each soil type this research first calibrated vfsmod to the observed outflow hydrograph for the free drainage experiments to parameterize the soil hydraulic and vegetation parameters and then evaluated the model based on outflow hydrographs for the shallow wt experiments this research used several statistical metrics and a new approach based on hypothesis testing of the nash sutcliffe model efficiency coefficient nse to evaluate model performance the new vfsmod routines successfully simulated the outflow hydrographs under both free drainage and shallow wt conditions statistical metrics considered the model performance valid with greater than 99 5 probability across all scenarios this research also simulated the shallow water table experiments with both free drainage and various water table depths to quantify the effect of assuming the former boundary condition for these two soil types shallow wts within 1 0 1 2 m below the soil surface influenced infiltration existing models will suggest a more protective vegetative filter strip than what actually exists if shallow water table conditions are not considered abbreviations bgs below ground surface vfs vegetative filter strip vfsmod vegetative filter strip modeling system keywords best management practices buffer infiltration shallow water table vegetative filter strip 1 introduction a commonly used method for reducing sediment nutrient and pesticide loadings from agricultural fields is the edge of field vegetative filter strip vfs popov et al 2005 reichenberger et al 2007 fox et al 2010 2011 muñoz carpena et al 2010 lacas et al 2012 a vfs reduces sediment nutrient and pesticide movement to streams by decreasing runoff volumes through infiltration into the soil profile allowing contact between dissolved phase solutes and vegetation and or by reducing flow velocities to the point where sediment and sorbed solutes can settle out of the water commonly vfs are placed adjacent to streams in riparian floodplains or adjacent to drainage ditches at the edge of an agricultural field where shallow groundwater tables can be present lacas et al 2005 dosskey et al 2011 carluer et al 2016 while it is commonly recognized that a shallow groundwater table may limit infiltration into a soil profile there is a lack of controlled experimental data available for demonstrating when this limitation becomes important various strategies have been used to model shallow groundwater tables in hydrologic models it is generally well known that shallow groundwater tables can significantly affect infiltration rates and therefore surface runoff during rainfall events for example salvucci and entekhabi 1995 and chu 1997 proposed approximate non uniform green ampt solutions for infiltration with ponded soils bounded by a water table however this effect is often ignored or handled simplistically in many common simulation models specifically as an example for vegetative filter strips or riparian buffers the riparian ecosystem management model remm simulates water movement and storage through a riparian buffer including subsurface lateral flow upward flux from the water table and deep seepage lowrance et al 2000 tilak et al 2014 however water storage and movement in remm and other models is based on simplistic mass balance rate controlled approaches and operates on a daily time scale a widely used watershed model swat arnold and fohrer 2005 that contains a vfs component derived empirically from other model simulations white and arnold 2009 does not account for shallow water table effects a more robust and mechanistic modeling package is needed that is capable of operating at the event time scale process based models can predict vfs efficiency for pollutant removal and thereby allow site specific design of vfs physical characteristics length width slope and type of vegetation the design process is heavily dependent on being able to predict infiltration and sedimentation processes in the vfs one such model vfsmod is a field scale mechanistic storm based numerical model that estimates water and sediment retention for single storm or a time series of storm events muñoz carpena et al 1999 muñoz carpena and parsons 2004 2008 vfsmod routes an incoming hydrograph and sedimentograph from an adjacent field through a vfs to calculate the resulting outflow infiltration and sediment trapping muñoz carpena et al 1993a b 1999 infiltration is modeled based on the green ampt equation for unsteady rainfall the model includes an automated and robust inverse calibration routine ritter et al 2007 to optimize input parameters based on the global multilevel coordinate search algorithm huyer and neumaier 1999 in sequential combination with the local nelder mead simplex algorithm nelder and mead 1965 vfsmod has been globally used to optimize filter placement and design kuo and muñoz carpena 2009 balderacchi et al 2016 pan et al 2017 and has been integrated into pesticide exposure assessment frameworks e g sabbagh et al 2010 bach et al 2017 a recently released version of vfsmod considers the presence of a static shallow water table wt hereon in order to expand its applicability across a wide range of field conditions muñoz carpena et al 2017 lauvernet and muñoz carpena 2017 the swingo shallow water table infiltration algorithm component in vfsmod is a modified form of the integral solution to ponded infiltration for soils bounded by a water table proposed by salvucci and entekhabi 1995 and chu 1997 the modification included making the solution numerically explicit in time and adding new integral formulae for calculation of the singular infiltration times time of ponding and time to soil profile saturation the new routine was validated against a numerical solution of richards equation for varying wt depths and rainfall intensities on five distinct soils muñoz carpena et al 2017 and evaluated on two benchmark studies through global sensitivity and uncertainty analysis lauvernet and muñoz carpena 2017 these preliminary investigations demonstrated that the wt influenced infiltration and runoff for depths shallower than 1 0 1 5 m but was negligible for deeper water tables also soils that exhibited a marked i e more definitive air entry bubbling pressure head on their soil water characteristic curve were more prone to surface hydrology changes as quick saturation was reached when the wetting front reached the capillary fringe above the water table global sensitivity analysis of the modified model showed that wt depth was the first or second most important factor next to saturated hydraulic conductivity in controlling the changes in surface flow sediment and pesticide trapping of the vfs lauvernet and muñoz carpena 2017 however there remains a lack of experimental data for systematic validation of the updated model under controlled hydrologic conditions lauvernet and muñoz carpena 2017 specifically call for laboratory and field research detailing the response of a vfs under both deep and shallow wts therefore the research objectives were to i investigate the influence of shallow wts on outflows from a vfs using meso scale laboratory soil box ii evaluate the performance of a new shallow wt algorithm in vfsmod for simulating shallow wt effects and iii utilize vfsmod to determine the depth at which shallow wt conditions influence vfs effectiveness the research evaluated the performance of vfsmod for experiments without free drainage and with a shallow wt using several statistical metrics including a hypothesis testing approach recently proposed by ritter and muñoz carpena 2013 2 methods and materials 2 1 brief description of the shallow water table modeling component the original infiltration component of vfsmod was based on a modification of the green ampt equation for unsteady rainfall with no restrictions due to the presence of a wt chu 1978 skaggs and khaheel 1982 muñoz carpena et al 1993b full details of the new wt algorithm within vfsmod are provided in muñoz carpena et al 2017 and lauvernet and muñoz carpena 2017 here we provide a brief description to ground the experimental setup and testing of the model performed in this study the new wt algorithm calculates the actual infiltration rate f eq 1 at the vfs soil surface for each time t t assuming the soil surface is not ponded at the beginning of the event 1 f i 0 t t p k s 1 z f 0 l z f k h dh t p t t w min f w i t t w where i i t l t is rainfall intensity z l is the depth from the surface zf l is the wetting front depth l l the depth from the surface to the water table k k h l t is the soil water hydraulic conductivity as a function of the soil suction h l non uniform with depth k s l t is the saturated hydraulic conductivity tp t is the time to surface ponding from the beginning of the event and tw t and fw l t are the time and the vertical flow boundary condition when the wetting front reaches the water table or its capillary fringe hb at depth zw l fig 1 two options for fw are provided a vachaud and thony 1971 condition is commonly used in experiments corresponding to vertical saturated flow fw ks salvucci and entekhabi 1995 liu et al 2011 this boundary condition can overestimate the final f in some field situations particularly when the wt drains to the nearby stream for this case another option is available to simulate lateral drainage following dupuit forchheimer assumptions van hoorn and van der molen 1973 assuming a water table slope equal to the soil surface slope so beven and kirkby 1979 vertessy et al 1993 2 f w k sh s o z w vl where ksh l t is the lateral horizontal soil saturated hydraulic conductivity vl l is the filter length vfs dimension in the flow direction and zw l is the effective saturation depth that depends on l and the soil air entry pressure hb l zw l hb zw 0 zw 0 when l hb i e the soil is effectively saturated by the capillary fringe in this study we applied the experimental vachaud and thony condition the soil water hydraulic conductivity k k h and water content characteristics θ θ h l3 l3 are described in this study based on mualem 1976 and van genuchten 1980 the dynamic coupling of the infiltration rate under wt conditions eq 1 with vfsmod surface and transport routines is done through the lateral recharge term left hand side of eq 3 based on the finite element solution of the kinematic wave equation muñoz carpena et al 1993a 3 h s t q x i f s f s o q s o n h s 5 3 with initial and boundary conditions h s 0 0 x vl t 0 h s h s o x 0 t 0 where hs hs x t l is the overland flow depth q q x t l2 t is discharge per unit width x l is the surface flow direction axis so and sf l l are the bed and water surface friction slopes at each node of the system n is manning s roughness coefficient dependent on soil surface condition and vegetative cover at each node of the system and hs o hs o 0 t l represents the hydrograph runon from the adjacent field input as a time dependent boundary condition at the first node of the vfs surface finite element grid fig 1 the solution of eq 1 in each time step is based on a combination of approaches by salvucci and entekhabi 1995 and chu 1997 with the assumption of a horizontal wetting front with new integral formulae muñoz carpena et al 2017 lauvernet and muñoz carpena 2017 for calculating the required singular times time of ponding tp and time tw when the wetting front reaches the saturated capillary fringe region above the water table at depth zw fig 1 since the solution to eq 1 is implicit i e z depends on f cumulative infiltration f and the singular times depend on z a numerical integration scheme is used 2 2 laboratory experiments a laboratory soil box fig 2 was constructed to simulate the hydraulics in a vfs with dimensions of 1 0 m wide 2 0 m long and 0 7 m deep based on the setup used by fox et al 2011 the laboratory soil box was designed to allow the control of the groundwater level in the soil profile the 2 0 m length fell within the range of vfs flow lengths reported in previous field and laboratory studies 0 5 m to 29 m vfs lengths were reported by sabbagh et al 2009 and poletika et al 2009 0 7 2 7 m lengths were used by stout et al 2005 in laboratory experiments experiments were conducted with two different soils packed into the column the first soil was a silt loam with 19 4 sand 58 3 silt and 22 3 clay based on hydrometer tests on three samples the silt loam soil was packed into the box to a bulk density of 1 40 g cm3 in two lifts approximately 0 3 m deep the other soil was a sandy loam with 54 9 sand 41 4 silt and 3 7 clay the soil was packed to a bulk density of 1 35 g cm3 in two lifts a 7 0 slope was used for all experiments and represented an average slope reported in previous literature muñoz carpena et al 2010 and fox et al 2010 reported 2 5 10 0 slopes to represent a typical vegetation used in a vfs haan et al 1994 bermuda grass cynodon dactylon sod was established in the laboratory soil box the sod was well watered and allowed to establish for two weeks prior to conducting the experiments the laboratory soil box included a porous steel screen at the bottom of the soil bed 70 cm below ground surface bgs to drain the soil profile fox et al 2011 used this setup to simulate an atmospheric boundary condition at the bottom of the soil box percolate that reached the bottom of the steel screen exited through a pipe at the bottom fig 2 the soil box was modified for these experiments by installing a water standpipe that connected to the pipe at the bottom to measure the groundwater table height fig 2c dynamic shallow wts were measured using an automated water level logger hoboware onset computer corp cape cod ma accuracy of 0 5 cm the loggers monitored water pressure and temperature every 60 s a static water table position was maintained by inserting the intake hose of a peristaltic pump into the standpipe fig 2c to the desired wt height and pumping at a sufficient rate to remove all percolate the inflow runon was applied at a constant rate using a peristaltic pump and distributed across the entire vfs width 1 0 m using a plexiglas weir the vfs experienced a step inflow hydrograph with typical inflow rates of 0 05 0 07 l s as opposed to simulating a more natural runon hydrograph to simplify the experimental procedures such step inflow hydrographs are commonly used in vfs field studies for example see sabbagh et al 2009 for a review of several such studies inflow rates were quantified prior to the experiment by recording the time to fill a 2 l container in triplicate trials the outflow runoff was collected by a lateral flume draining to a collector and measured using a weighing scale every 5 s the outflow rates were smoothed with a 60 s moving average sigmaplot v 12 san jose ca to remove sensor noise associated with the high frequency 5 s measurements this research conducted four vfs experiments including both a free drainage experiment and a static wt experiment for each soil type inflow water was applied in each experiment for at least 60 min the monitoring period varied depending on time required for the outflow to reach steady state the free drainage experiments matched those reported in fox et al 2011 with no control on the wt position percolated water was removed from the soil box using the peristaltic pump and the percolate flow rates recorded over time the static water table experiments involved maintaining the groundwater level 0 4 or 0 3 m bgs for the silt loam and sandy loam soil types respectively a static shallow wt was maintained by using the peristaltic pump to extract water from the standpipe at the desired shallow wt height the static wt height was continuously verified with a water level logger in the standpipe percolated water pumped from the standpipe was measured to quantify percolation flow rates cumulative infiltration was calculated as the cumulative inflow minus the cumulative outflow the outflow rates were compared to the inflow rates to quantify hydrological differences between the free drainage case and shallow wt conditions hypothetically for free drainage outflow rates should be less than the inflow rate due to continued infiltration into the soil profile for cases with a shallow wt the outflow rate should initially be less than the inflow rate due to available storage in the soil profile for infiltrated water unless the capillary fringe extends to or above the soil surface from the wt then over time the outflow rate should approach the inflow rate as less available water storage exists within the soil profile 2 3 numerical modeling with vfsmod in applying vfsmod to the laboratory scale experiments this research used the neural network soil hydraulic predictor rosetta v1 1 in retention curve retc presented by van genuchten 1991 to derive the initial estimates for the soil hydraulic parameters soil texture percent sand silt and clay and bulk density were input into rosetta as measured from soil hydrometer tests and soil samples from each laboratory setup the derived parameters included the saturated hydraulic conductivity ks l t residual moisture content θr l3 l3 saturation moisture content θs l3 l3 inverse of the air entry pressure head αs 1 l and van genuchten 1980 parameters n and m assuming m 1 1 n table 1 the free drainage experiment for each soil type was then used to inversely calibrate vfsmod previous studies commonly report the three most important input parameters as k s green ampt s suction at the wetting front sav and manning s roughness rna fox et al 2010 muñoz carpena et al 2010 the initial soil moisture content or antecedent moisture content was measured prior to each experiment by extracting a 2 cm diameter soil core and then refilling the hole with clay the moisture content was obtained by the standard gravimetric method all other soil hydraulic properties were held constant as derived from rosetta v1 1 and reported in table 1 while vfsmod allows the input of variable roughness along the vfs the manning s roughness was assumed constant along the length of the bermuda grass vfs the roughness was calibrated separately for each experimental setup as different plantings were used for other vegetation parameters default values were used as specified for bermuda grass in the vfsmod technical guidance after calibration of vfsmod based on the free drainage experiments for both soil types the corresponding static wt experiment was then simulated with vfsmod using the derived van genuchten parameters αs m and n as reported in table 1 note that the free drainage simulations did not require input of αs m and n an initial hydrostatic water pressure distribution was assumed above the wt to illustrate the impact of a shallow wt the shallow wt experiments were simulated in vfsmod assuming a free drainage boundary condition and with numerous hypothetical wt depths 0 1 2 0 m in order to quantify the effect of assuming a free drainage boundary condition several statistical metrics quantitatively assessed the performance of vfsmod in simulating the observed runoff hydrographs including the root mean squared error rmse the nash sutcliffe model efficiency coefficient nse nash and sutcliffe 1970 4 nse 1 t 1 t o i t o m t 2 t 1 t o i t o 2 where oi and om represent the observed values and the model estimates for the t observation t is is the total number of observations and o is the mean of the observed values and the normalized objective function nof used previously by fox et al 2004 5 nof t 1 t o i t o m t 2 t o note that the nse ranges from to 1 where 1 indicates a perfect match smaller nof values indicated a closer fit fox et al 2004 and site specific model applications should result in an nof value of less than 1 0 pennell et al 1990 loague and green 1991 as discussed by ritter and muñoz carpena 2013 interpretation of the various statistical metrics is often subjective and biased by number of data points repeated data and outliers therefore to strengthen model evaluation this research utilized fiteval a model evaluation tool based on hypothesis testing of the nse exceeding threshold values based on approximated probability distributions obtained by block bootstrapping ritter and muñoz carpena 2013 the technique quantified model performance into four classes proposed based on thresholds of the non linear relationship between nse and the ratio between signal range standard deviation of the observed data and noise model error rmse unsatisfactory acceptable good or very good 3 results and discussion for the free drainage experiments fig 3 the bottom of the column was an atmospheric boundary condition for the silt loam soil the outflow rate moving average was less than the inflow rate and typically between 71 and 86 of the inflow rate fig 3a therefore infiltration was occurring in the vfs reducing the runoff at the outlet fig 3b the cumulative groundwater percolated from the column was approximately 10 l at the time when the inflow water was terminated 60 min and increased to approximately twice that amount two hours after initiating the experiment fig 3c for the higher conductive sandy loam soil outflow rates were typically between 67 and 83 of the inflow rate throughout the experiment fig 3d as expected the higher ks of the sandy loam soil resulted in greater cumulative percolated water 12 5 l after 60 min than in that the silt loam fig 3e the shallow wts were positioned at 0 4 m and 0 3 m bgs for the silt loam and sandy loam experiments respectively fig 4 initial water table depths were selected to be near the soil surface upper 1 0 m but also approximately midway through the soil column different depths were utilized to provide different initial conditions between the shallow water table experiments for model evaluation note the difference in the time scales of these experiments inflow lasted 60 min longer in the silt loam experiment the outflow rates quickly approached 100 of the inflow rate for both soils fig 4a and d but did so faster in the less conductive silt loam soil at 60 min into the experiments percolated flow volumes were approximately 10 l for the silt loam soil fig 4c and 20 l for the sandy loam soil fig 4f also note that in the silt loam experiment several minutes after inflow was cutoff the pump was also stopped and the wt increased quickly after 140 min fig 4a for the silt loam free drainage experiment minimal calibration of vfsmod was necessary from the initial parameter values derived from the pedotransfer function table 2 fig 5 a statistical metrics included a nof of less than 0 20 and nse of 0 89 table 3 the model did predict an earlier arrival of the runoff at the end of the vfs than observed in the experiments and under predicted the runoff rates immediately after runoff initiated for the silt loam calibration for the free drainage case fig 5a fiteval results indicated that the fit passed the statistical significance test for model acceptability nse 0 65 with p 0 003 where the probabilities of very good nse 0 90 1 00 good nse 0 75 0 89 and acceptable nse 0 65 0 75 were 50 5 40 5 and 8 9 respectively see supplemental information fig s 1 for the sandy loam experiment vfsmod was able to match the early time shape of the runoff hydrograph and the time of runoff initiation but was delayed in predicting the falling limb of the outflow hydrograph fig 5c the discrepancy between calibrated soil hydraulic parameters and those estimated from the pedotransfer function rosetta v1 1 table 1 were greater for the sandy loam soil table 2 the nse and nof both indicated an improved calibration for the sandy loam soil as compared to the silt loam free drainage experiments table 3 the corresponding fiteval values for the sandy loam calibration included a valid probability of 99 5 p 0 003 with probabilities of very good good and acceptable of 57 2 33 6 and 8 7 respectively see supplemental information fig s 3 the vfsmod shallow wt component was able to predict both outflow hydrographs with nse 0 90 and nof 0 20 without calibration fig 5b and d an interesting dynamic appeared in the shallow ground water table simulations whereby the model predicted the outflow rates immediately equivalent to the inflow rate when the wetting front reached the static water table this occurred more quickly in the silt loam soil experiment than the sandy loam soil experiment because of the smaller αs the observed data from the laboratory experiments demonstrated a much more gradual increase in the outflow rate we hypothesize that this minor deviation was due to the assumed vertical boundary condition fw ks when t tw as shown in eq 1 the shallow wt experiments were also draining laterally within the bounded soil profile and this introduced an experimental artifact with respect to what was modeled this was also addressed by muñoz carpena et al 2017 and they suggested that likely some empirical weighting of vertical and lateral conditions might be needed under field conditions note that it would be possible to calibrate vfsmod further based on changing αs m and n but such an approach fell outside the scope of this research vfsmod simulations with shallow wts were considered valid see supplemental information figs s 2 and s 4 with a probability of 99 9 p 001 for the silt loam soil probabilities of very good good and acceptable of 69 9 26 8 and 3 2 respectively and 100 p 001 for the sandy loam soil probabilities of very good and good of 97 3 and 2 7 note that simulating the shallow wt experiments with the free drainage assumption significantly under predicted the outflow rates i e greater predicted infiltration as shown in fig 5 and thereby degraded model performance table 3 simulations using the free drainage conditions in the presence of a water table yielded nse values of 0 70 and 0 79 which some may consider satisfactory from a statistical standpoint by contrast when using the water table boundary conditions the simulation improved with nse values 0 90 however the practical implications of the model performance is significant as noted earlier the vfs trapping efficiencies for sediment fox and sabbagh 2009 pan et al 2017 pesticides sabbagh et al 2009 lacas et al 2005 2012 carluer et al 2016 bacteria fox et al 2011 and nutrients fox and penn 2013 depend directly on infiltration in the vfs in all of these cases simulating a free drainage boundary condition will over estimate infiltration and therefore over predict vfs trapping efficiency for sediment pesticides bacteria and nutrients suggesting a more protective vegetative filter strip than what actually exists such estimations would magnify when designing a vfs for a target trapping efficiency over a series of storm events or conducting long term exposure assessments sabbagh et al 2013 for the silt loam and sandy loam soils wt depths of less than approximately 1 0 1 2 m influenced infiltration and therefore the corresponding outflow hydrographs fig 6 these wt depth thresholds closely matched the modeling results performed with and without wts for other simulated soil and hydrologic conditions muñoz carpena et al 2017 lauvernet and muñoz carpena 2017 therefore the impact of shallow wts on vfs trapping efficiencies should be considered in cases where the wt is closer than 1 5 m as a conservative estimate bgs this research proved that the vfsmod mechanistic model is valid for handling this dynamic scenario when vfs are installed in areas with shallow wts such as adjacent to streams and drainage channels 4 conclusions a laboratory scale vfs was able to provide experimental data for validating the performance of new shallow wt routines in vfsmod the laboratory experiments provide a unique data set that currently does not exist in the literature under field conditions in general these routines successfully captured the dynamics of runoff under a shallow wt with minimal calibration and clearly demonstrated the importance of considering shallow wt effects on infiltration and the corresponding vfs removal efficiencies statistical metrics indicated that model performance was valid with greater than 99 5 probability across all scenarios for both free drainage and shallow wt conditions according to vfsmod simulations wt depths of less than approximately 1 0 1 2 m influence infiltration and therefore the corresponding vfs outflow hydrograph for the silt loam and sandy loam soils investigated in this research future research should further validate the vfsmod routines by monitoring in field vfs with different wt depths and the effect on vfs sediment nutrients and pesticide trapping efficiency in addition in further controlled field or laboratory studies the weighting of end time boundary conditions could be explored software availability the updated version of vfsmod can be downloaded free of charge at the following website http abe ufl edu carpena vfsmod acknowledgements this research was supported by foundational grant program grant no 2016 67019 26855 of the usda national institute of food and agriculture nifa titled influence of preferential flow on coupled colloid nitrogen and phosphorus transport through riparian buffers the authors acknowledge dr lucie guertault for reviewing an earlier version of this manuscript appendix a supplementary data supplementary data associated with this article can be found in the online version at https doi org 10 1016 j jhydrol 2017 10 069 appendix a supplementary data supplementary data 1 
7537,uncertainty estimation of climate change impacts on hydrology has received much attention in the research community the choice of a global climate model gcm is usually considered as the largest contributor to the uncertainty of climate change impacts the temporal variation of gcm uncertainty needs to be investigated for making long term decisions to deal with climate change accordingly this study investigated the temporal variation mainly long term of uncertainty related to the choice of a gcm in predicting climate change impacts on hydrology by using multi gcms over multiple continuous future periods specifically twenty cmip5 gcms under rcp4 5 and rcp8 5 emission scenarios were adapted to adequately represent this uncertainty envelope fifty one 30 year future periods moving from 2021 to 2100 with 1 year interval were produced to express the temporal variation future climatic and hydrological regimes over all future periods were compared to those in the reference period 1971 2000 using a set of metrics including mean and extremes the periodicity of climatic and hydrological changes and their uncertainty were analyzed using wavelet analysis while the trend was analyzed using mann kendall trend test and regression analysis the results showed that both future climate change precipitation and temperature and hydrological response predicted by the twenty gcms were highly uncertain and the uncertainty increased significantly over time for example the change of mean annual precipitation increased from 1 4 in 2021 2050 to 6 5 in 2071 2100 for rcp4 5 in terms of the median value of multi models but the projected uncertainty reached 21 7 in 2021 2050 and 25 1 in 2071 2100 for rcp4 5 the uncertainty under a high emission scenario rcp8 5 was much larger than that under a relatively low emission scenario rcp4 5 almost all climatic and hydrological regimes and their uncertainty did not show significant periodicity at the p 05 significance level but their temporal variation could be well modeled by using the fourth order polynomial overall this study further emphasized the importance of using multiple gcms for studying climate change impacts on hydrology furthermore the temporal variation of uncertainty sourced from gcms should be given more attention keywords climate change hydrology uncertainty global climate model temporal variation 1 introduction various sources of uncertainty have been identified in the process of assessing climate change impacts on hydrology wilby and harris 2006 chen et al 2011b understanding quantifying and reducing the uncertainty become crucial to water resource management generally the uncertainty mainly comes from 1 greenhouse gas emission scenario ghges e g maurer 2007 nobrega et al 2011 fang et al 2015 ficklin et al 2016 2 global climate model gcm e g jenkins and lowe 2003 rowell 2006 teng et al 2012 chen et al 2012 3 natural climate variability e g hawkins and sutton 2009 brisson et al 2015 chen et al 2016 4 downscaling method e g wilby et al 1998 fowler et al 2007 samadi et al 2013 etemadi et al 2014 li et al 2015 and 5 hydrological model structure and parameter e g poulin et al 2011 bae et al 2011 brigode et al 2013 eregno et al 2013 jiang et al 2007 many studies have involved in quantifying the uncertainty of climate change impacts on hydrology arising from above sources especially the uncertainty related to gcm and ghges has been extensively studied during the last decade for example maurer 2007 investigated the uncertainty related to the choice of a gcm and ghges in assessing the hydrological impacts of climate change in the sierra nevada california by using eleven gcms under two emission scenarios nobrega et al 2011 quantified the uncertainty of climate change impacts on the discharge of rio grande in south america by using six gcms and four emission scenarios minville et al 2008 employed five gcms and two ghgess to implement the same investigation over a nordic watershed these studies found that both gcm and ghges contribute great uncertainty to the hydrological impacts of climate change the uncertainty of downscaling has also been investigated in several studies for example mpelasoka and chiew 2009 compared three downscaling methods in constructing future runoff across australia chen et al 2011a compared six downscaling methods in simulating the future streamflow of a canadian watershed they found that the uncertainty related to the downscaling method could be significant when quantifying climate change impacts on hydrology especially for extremes studies have also investigated the uncertainty of hydrological modeling when assessing climate change impacts on hydrology for example jiang et al 2007 compared six monthly water balance models in investigating the potential impacts of climate change on the water availability of a chinese basin teng et al 2012 used five lumped conceptual daily rainfall runoff models to simulate climate change impacts on the runoff across southeast australia their results indicated that hydrological model could lead to significant uncertainty in the prediction of future runoff in addition to the investigation of each individual uncertainty source several studies also compared the uncertainty from different sources in the assessment of hydrological climate change impacts for example wilby and harris 2006 compared five uncertainty sources in the prediction of low flows for the river thames uk by using four gcms two ghgess two statistical downscaling techniques two hydrological model structures and two sets of hydrological model parameters their results indicated that the choice of a gcm is the largest contributor to uncertainty followed by the downscaling method and uncertainty related to hydrological modeling and emission scenario is relatively less important prudhomme and davies 2009 employed three gcms two ghgess and two downscaling techniques statistical downscaling model sdsm and dynamical downscaling model rcm hadrm3 to investigate the uncertainty of climate change impacts on the river flow of four british catchments they also found that gcm is the largest source of uncertainty while uncertainty related to downscaling technique and emission scenario is of similar magnitude but generally smaller than that related to gcm similarly chen et al 2011b conducted a comprehensive study of uncertainty using two ghgess six gcms a five member ensemble of a gcm four downscaling techniques three hydrological model structures and ten sets of hydrological model parameters the results showed that the choice of a gcm is a major contributor to uncertainty followed by downscaling method ghges and hydrological model structure the contribution of hydrological model parameters is less important all above studies consistently show that the choice of a gcm is the largest contributor to overall uncertainty of hydrological climate change impact studies thus the use of a multi model ensemble is recommended when implementing climate change studies minville et al 2008 zhang et al 2011 teng et al 2012 although lots of studies have explored the uncertainty of climate change as well as it impacts studies focusing on the temporal variation of uncertainty are relatively fewer and most are qualitative studies for example hawkins and sutton 2009 2011 investigated the temporal variation of uncertainty sourced from internal variability climate model emission scenarios for the 21th century jung et al 2012 compared the uncertainty of climate model hydrological model parameter and emission scenario from ten 20 year time slice periods in 2000 2099 for two river basins in the pacific northwest of us ahmadalipour et al 2017 characterized the uncertainty from climate model downscaling and emission scenarios for period 2010 2099 when assessing the climate changes over columbia river basin the results of above studies showed that the uncertainty from various sources would vary over time and the uncertainty derived from one source might dominate in one period while that derived from the other source might prevail in the other period but beyond that some important questions or puzzles still need to be solved when assessing the hydrological impacts of climate change such as how does the uncertainty caused by single source propagate over time how to quantify the temporal variation of uncertainty answers to such questions would contribute to a more reliable climate change projection as well as better strategies dealing with climate change therefore this study investigated and estimated the temporal variation mainly long term of uncertainty stemming from the major contributor the choice of a gcm in assessing the hydrological impacts of climate change to achieve this twenty cmip5 gcms under two emission scenarios rcp4 5 and rcp8 5 were adopted to adequately represent this uncertainty envelope fifty one 30 year continuous future horizons obtained by moving a 30 year window from 2021 to 2100 with 1 year interval were used to express the temporal variation of uncertainty the temporal variation was analyzed by a set of statistical approaches such as wavelet analysis mann kendall trend test and polynomial regression analysis the paper is structured as follows a brief description of the studied area and data is presented in section 2 followed by a detailed description of the bias correction method hydrological model and data analysis approach in section 3 section 4 elaborates on the results by analyzing the climatic and hydrologic changes and their uncertainty over 51 future periods finally the discussion and conclusion are provided in section 5 2 study area and data 2 1 study area the proposed approach described in section 3 is conducted on the hanjiang river watershed fig 1 which is located in the south central china the river originates from the southern qinling mountain flows through shanxi and hubei provinces and finally merges into the yangtze river the longest river in asia in wuhan city with a main stream length of 1577 km and a drainage area of 159 000 km2 in the hanjiang river watershed the annual mean temperature is about 15 5 c and the annual average rainfall is 873 mm chen et al 2012 the watershed has a subtropical monsoon climate in which the precipitation is mainly caused by the southeastern and southwestern maritime monsoons seventy five percent of its total precipitation occurs between june and october during this time the prolonged rainfalls in autumn and rainstorms in early summer often cause major floods xu et al 2012 the annual mean discharge of the hanjiang river is about 1150 m3 s wang et al 2015 in the middle of the hanjiang river watershed there locates the danjiangkou reservoir which serves as a water source for the middle route of the south to north water transfer project mswtp the sub basin above the danjiangkou reservoir with a drainage area of 89 540 km2 is used in this study the mswtp has relieved chronic water shortage problems in several provinces and cities of china including the capital beijing this is a crucial motivating factor for understanding the uncertainty and its temporal variation of climate change impacts on the hydrology over this watershed as it will be vital for making water diversion decisions for this watershed in the future 2 2 data this study used both observed and gcm simulated daily precipitation prcp minimum and maximum temperature tmin tmax data in the hanjiang river watershed the observed data covered 1971 2000 for the historical period the gcms covered 1971 2000 for the history and 2021 2100 for the future observed meteorological data from eleven weather stations were obtained from the china meteorological data sharing service system the streamflow at the sub basin outlet is the inflow of the danjiangkou reservoir which was provided by the bureau of hydrology of the changjiang water resources commission the climate model simulations were taken from the coupled model intercomparison project phase 5 cmip5 database twenty gcms under two ghgess rcp4 5 and rcp8 5 from fourteen modeling centres were selected to adequately represent the uncertainty of climate models table 1 presents the general information of these twenty gcms in order to run a lumped hydrological model the precipitation and temperature data from both observations and gcm simulations were averaged over several grid boxes within and surrounding the watershed to a single time series using the thiessen polygon method 3 methodology 3 1 bias correction method gcm outputs are usually too coarse to directly drive a hydrological model for hydrological climate change impact studies in this case a downscaling or bias correction technique must be utilized xu 1999 chen et al 2011a jung et al 2012 this study used a bias correction method for its simplicity and good performance teng et al 2012 jung et al 2012 ahmadalipour et al 2017 the used bias correction method is the daily bias correction method dbc chen et al 2013 which is a hybrid advanced bias correction method combing the local intensity scaling method loci schmidli et al 2006 and the daily translation method dt mpelasoka and chiew 2009 to correct both the wet day frequency and the distribution of wet day precipitation the dbc method first takes advantage of the loci method to correct the precipitation occurrence and then instead of applying the same factor loci to scale daily precipitation for a particular month it adopts the dt method to modulate the changes as a function of the percentile differences in the frequency distribution of precipitation or temperature between observation and climate model more details are as follows 1 for a particular month e g january in the reference period a wet day threshold is determined from the gcm to ensure that the threshold exceedance equals the observed wet day frequency for that month 2 the threshold is then applied to the future period to correct the gcm s precipitation in corresponding month 3 a relationship between observation and gcm is established using the distribution mapping technique by calculating the percentiles 100 percentile values from the 1st to the 100th percentile in this study of daily precipitation temperature in the reference period in other words the distributions of observation and gcm are represented by 100 percentile values and 4 the percentile ratios of precipitation are multiplied by the gcm precipitation series in the future period or the percentile differences are added to the gcm temperature series in the future period eq 1 1 x cor j f o 1 f m x fut j where x refers to precipitation or temperature fut means variable in the future period cor means corrected variable the subscript j refers to a specific day in the future period fm is the cumulative distribution function cdf represented by 100 percentile values from the 1st to the 100th of simulated precipitation or temperature in the reference period and fo 1 is the inverse cdf of the observed precipitation or temperature in the reference period for every gcm the dbc method was first calibrated in the reference period i e find the gcm wet day threshold for every month and construct the distribution relationships between observation and gcm and then the calibrated dbc was applied to correct gcm s precipitation and temperature for all fifty one future periods which are 2021 2050 2035s 2022 2051 2036s 2023 2052 2037s 2070 2099 2084s and 2071 2100 2085s note that these future periods were corrected individually from each other the fifty one 30 year continuous future periods with 1 year interval filtered the short term fluctuations and changes such as inter annual variability so they mainly reflected the long term temporal variation of climatic and hydrological changes and their uncertainty additionally when using a bias correction method it is implicitly assumed that climate model s biases are stationary over time maraun 2013 chen et al 2013 3 2 hydrological modeling the hydrological model used in this study is the hmets model the hydrological model of école de technologie supérieure which is a lumped conceptual rainfall runoff model developed at the école de technologie supérieure university of quebec it has been used in several flow forecasting and climate change impact studies e g chen et al 2011b arsenault et al 2014 brigode et al 2015 hmets uses two connected reservoirs representing unsaturated i e vadose and saturated i e phreatic zones and simulates the basic hydrological processes of snow accumulation melting and refreezing evapotranspiration infiltration as well as flow routing to the watershed outlet martel et al 2017 troin et al 2015 the model begins with a snowmelt module which works in three steps overnight refreezing snowmelt and snowpack water retention capacity the water in the snowpack is from snowmelt and liquid precipitation if the amount of water in the snowpack exceeds the water retention capacity the remaining water is then added to the water available for runoff a fraction of the water available for runoff will be directly taken as surface runoff once the surface runoff and real evapotranspiration are taken out the remaining water for runoff will infiltrate to the vadose zone in this process a portion of the infiltrated water which depends on the water level of vadose zone is treated as delayed runoff component the vadose zone offers water for the hypodermic flow component and the exchanges to the saturated zone if the vadose zone fills up the remaining water will be added to the delayed runoff component the saturated zone is mainly used to release groundwater flow if the saturated zone level exceeds the maximum the exceeding water is also added to the delayed runoff component surface and delayed runoff are transferred to the outlet using two unit hydrographs and are summarized to the hypodermic flow and the groundwater flow finally the total streamflow is used to represent the modeled streamflow hmets has up to 21 parameters which are shown in table 2 martel et al 2017 model calibration is done automatically using the covariance matrix adaptation evolution strategy cmaes this strategy is a continuous domain non convex non linear problem optimization algorithm it estimates the derivatives of the covariance matrix of the previously successful candidate solution distribution using the maximum likelihood principle in this way it tends to maximize the likelihood of the distribution more details of this optimization method can be found in hansen and ostermeier 2001 and arsenault et al 2014 hmets requires daily mean temperature and precipitation as inputs if the maximum and minimum temperatures are input to the model they are automatically averaged to mean temperature a natural streamflow time series is also needed for model calibration the optimal combination of parameters is chosen based on the nash sutcliffe efficiency nse criterion in this study 20 years 1961 1980 of daily time series precipitation maximum and minimum temperature and flow were used for model calibration and another 20 years 1981 2000 were used for model validation the optimized parameters yielded nash sutcliffe criterion values of 0 79 for calibration and 0 78 for validation which shows that hmets performs relatively well for this watershed at the daily time step the observed and simulated mean hydrographs in fig 2 further demonstrate the reasonable performance of hmets in this watershed 3 3 data analysis the performance of the dbc method is first verified over the reference period 1971 2000 using nine climate statistics as criteria which include the mean standard deviation 25th 50th 75th 90th 95th and 99th percentiles and maximum of precipitation or temperature at different timescales daily monthly and annual the relative differences between the corrected and observed precipitation series of these statistics were calculated eq 2 while the absolute differences were calculated for temperature eq 3 2 rd p cor p obs p obs 100 3 ad t cor t obs where rd means relative difference ad means absolute difference pobs pcor mean statistics of observed and corrected precipitation respectively tobs tcor mean statistics of observed and corrected temperature respectively for hydrological simulation the corrected precipitation and temperature series in 51 future periods were used to drive the calibrated hmets model fifty one hydrological simulations were obtained by running the hydrological model 51 times using 51 future climate projections all based on the same calibrated parameters the hydrological impacts of climate change were represented by three hydrological variables mean flow high flow 95th percentile of 30 year daily flow and low flow 5th percentile of 30 year daily flow the changes of these variables for future periods relative to the reference period were calculated since the hydrological model is not perfect the simulated streamflow is slightly biased when comparing to observed counterpart to get rid of the influence of the biases caused by the hydrological model the model simulated future period streamflow was compared with the model simulated reference period streamflow rather than the gauged streamflow this method has been widely used in previous studies e g minville et al 2008 chen et al 2011a jung et al 2012 with the changes of future climate and hydrology relative change for precipitation and discharge and absolute change for temperature from 20 gcms and 51 future periods the ensemble median similar to ensemble mean and its uncertainty for each period can be defined as illustrated in fig 3 the upper part of fig 3 presents the calculation of ensemble median and uncertainty for one future period for example in the future period 2021 2050 20 gcms could provide 20 different values in terms of the future change the ensemble median is defined as the median value of the 20 predictions and the uncertainty is defined as the range between the maximum and minimum predictions when excluding outliers when illustrating with a boxplot fig 3 the ensemble median in a future period is q3 and the uncertainty is the range between the upper and lower boundaries q1 q5 the ensemble median and uncertainty range have been widely used to represent the temporal variation of uncertainty in previous studies e g kharin et al 2013 sillmann et al 2013 ipcc 2013 even though resampling methods such as bootstrapping khan et al 2006 samadi et al 2013 can also be used to define uncertainty following this method 51 ensemble medians and 51 uncertainty values were calculated for all 51 future periods to investigate the temporal variation of deterministic changes and their uncertainty the lower part of fig 3 some data analysis techniques wavelet analysis torrence and compo 1998 xu et al 2011 mann kendall trend test mann 1945 kendall 1975 and regression analysis bates and watts 1988 were used to analyze the temporal variation of deterministic changes and their uncertainty specifically wavelet analysis was used to analyze the periodicity and the mann kendall trend test was applied to examine the trend if the mann kendall trend test revealed significant trend at the p 05 significance level then the regression analysis was applied to simulate the changing pattern of the time series besides f test was used to examine the significance of the regression model and three indexes measured the performance of the regression model including willmott s refined index of agreement dr willmott et al 2012 willmott et al 2015 determination coefficient r2 ferraro et al 2011 and mean absolute error mae willmott and matsuura 2005 eq 4 gives the definition of dr which is at the range of 1 0 to 1 0 a dr value closer to 1 means better simulation 4 d r 1 i 1 n p i o i c i 1 n o i o i 1 n p i o i c i 1 n o i o c i 1 n o i o i 1 n p i o i 1 i 1 n p i o i c i 1 n o i o where p i is the model derived prediction o i is the observation n is the length of observations and o is the mean of the observations usually c is equal to 2 0 when applying wavelet analysis choosing an appropriate wavelet is of great importance the wavelet depends on the nature of the time series and on the type of information to be extracted de moortel et al 2004 in this study morelet was used based on its good localization in both time and frequency space torrence and compo 1998 and also based on the principles of self similarity compactness and smoothness ramsey 1999 since this study analyzes the periodicity of a time series the continuous wavelet transform cwt was applied statistical significance level of periodicity was estimated against a red noise model torrence and compo 1998 grinsted et al 2004 free matlab software package of wavelet analysis provided by grinsted et al 2004 was used to implement this analysis when using regression analysis considering the chaotic nature of climate system and nonstationarity of climate change a linear model may not be a proper choice for modeling climatic and hydrologic changes and their uncertainty referred to the studies of hawkins and sutton hawkins and sutton 2009 2011 the fourth order polynomial was used to fit the future climatic and hydrological changes and their uncertainty 4 results 4 1 bias correction method verification the performance of the dbc method was validated with respect to reproducing nine observed statistics fig 4 presents the biases of raw and corrected gcms precipitation and temperature on the daily time scale in 1971 2000 in the figure x axis represents the 20 gcms following the order in table 1 y axis represents the gcms biases in terms of reproducing nine statistics mean standard deviation 25th 50th 75th 90th 95th and 99th percentiles and maximum value from number 1 to 9 darker color means larger bias from the figure the raw gcms were considerably biased in simulating precipitation and temperature the biases in raw temperature series were above 2 c absolute value and the biases in raw precipitation series were mostly above 15 absolute value in terms of the nine statistics but after correction the biases in precipitation considerably reduced to less than 5 particularly the dbc method performed better for mean precipitation than for extremes in addition the dbc method considerably reduced the biases of gcm temperature for all statistics especially for minimum temperature all above results indicate the reasonable performance of the dbc method justifying its use for generating future climate change scenarios for hydrological impact studies 4 2 climate change projection 4 2 1 changes in annual precipitation and temperature fig 5 shows the relative change of mean annual precipitation and the absolute change of mean daily temperature for future periods in the figure each boxplot was constructed using 20 values derived from 20 gcms the 51 boxplots in one subgraph represent 51 future periods the mann kendall trend test revealed that the median of multi model ensemble precipitation minimum and maximum temperature all increased significantly at the significance level of p 05 over time for both emission scenarios for rcp4 5 rcp8 5 the ensemble median of annual precipitation increased from 1 4 1 4 for 2021 2050 to 6 5 10 3 for 2071 2100 rcp8 5 predicted a more rapid increase in precipitation than rcp4 5 in addition all gcms predicted increase in daily temperature for both emission scenarios and all future periods similar to precipitation rcp8 5 predicted a larger and more rapid increase in temperature than rcp4 5 for example rcp4 5 predicted a 1 5 c increase in minimum temperature for 2021 2050 and a 2 7 c increase for 2071 2100 in terms of the median of the 20 gcms while rcp8 5 predicted a 1 7 c increase in minimum temperature for 2021 2050 and a 4 8 c increase for 2071 2100 fig 6 presents the continuous wavelet power spectra of the ensemble medians of future mean annual precipitation mean daily minimum and maximum temperatures the results showed that future precipitation and temperature tended to increase with no significant periodicity at the p 05 significance level at the multi decadal scale as there is no thick contour within the black outlines this is expected as the multiple year averages of precipitation and temperature were calculated using a moving window approach the short term fluctuations such as inter annual or multi year variability were filtered the trend of temporal change in mean annual precipitation and temperature was modeled using the fourth order polynomial table 3 displays the test results of the fitted fourth order polynomials from the table the dr and r2 values are all above 0 90 and mae values are very small which indicate the reasonable performance of the fitted model f tests further show that the fitted models were significant at the p 05 significance level 4 2 2 changes in seasonal precipitation and temperature the climate changes in summer and winter mean precipitation and temperature were also analyzed as the high flow and low flow occur in these two seasons the figures in this part were presented in the supplementary material in order to make the paper in a proper length fig s1 shows that the summer and winter precipitations were all projected to increase for both emission scenarios for summer the overall changes of precipitation between the two emission scenarios showed slight difference with rcp4 5 projecting increases of 1 08 in 2021 2050 and 3 09 in 2071 2100 while rcp8 5 projecting increases of 1 91 in 2021 2050 and 3 42 in 2071 2100 for winter rcp8 5 predicted more increase than rcp4 5 with the relative change of winter precipitation varying from 8 07 in 2021 2050 to 24 41 in 2071 2100 for rcp4 5 and from 8 69 to 26 20 for rcp8 5 generally winter precipitation would likely increase more rapidly than summer precipitation for both emission scenarios for temperature only the results of minimum temperature were presented as maximum temperature showed similar changing trends fig s2 all gcms projected increases in temperature for both seasons and both emission scenarios the temperature increased slightly more in summer than in winter for both emission scenarios additionally rcp8 5 projected a larger and more rapid increase in seasonal temperature than rcp4 5 which is coincident with the case of annual mean daily temperature projection the test and measurement results displayed on the top left corner of each graph in figs s1 and s2 showed that fourth order polynomial showed reasonable performance in modeling the seasonal climate s temporal variation similar to mean annual precipitation and temperature on the whole the ensemble medians of mean seasonal precipitation and temperature did not show significant periodicity at the p 05 significance level tested by wavelet analysis figs s3 and s4 4 3 hydrological impact of climate change 4 3 1 average hydrographs fig 7 presents the averaged hydrographs simulated using raw and bias corrected gcms meteorological data in the reference period 1971 2000 which reflect the performance of bias correction in hydrological modeling the gcm simulated streamflows were compared to those simulated using observed meteorological data the results showed that the streamflows produced using raw gcm data were considerably biased for example the simulations using raw gcms data reached 9728 m3 s while the simulation using observed data was just 3278 m3 s for maximum flow after bias correction the differences among the gcms simulations were remarkably reduced and the difference between gcm and observed simulations was also considerably reduced compared to the large difference 6450 m3 s for maximum flow before bias correction the difference was reduced to 1706 m3 s after bias correction i e the simulated maximum flow was 4984 m3 s overall even though biases remain in maximum flows the bias correction method performed well for most hydrological regimes since some biases remained after bias correction the following section determined the hydrological impacts of climate change in terms of the relative change between the model simulated stream flows in future and reference periods the hydrology in the reference period was simulated using corrected gcm precipitation and temperature 4 3 2 change in mean and extreme flows fig 8 presents the relative change of mean high and low flows for 51 future periods the mann kendall trend test revealed significant trends at the p 05 significance level in all these flows for both emission scenarios rcp4 5 predicted increasing in mean flows for all future periods in terms of the median of 20 gcms projections while rcp8 5 predicted decreasing in mean flows for the periods between 2036 and 2065 and increasing for other periods the ensemble medians of mean flow projections changed from 0 6 in 2021 2050 to 5 4 in 2071 2100 for rcp4 5 and from 0 2 to 7 9 for rcp8 5 overall the mean flow was predicted to increase more for rcp4 5 before the 2042 2071 period and then increase more for rcp8 5 in the last half of the 51 future periods i e from 2048 to 2100 the ensemble medians of high flow was much larger in the first half of 51 future periods for rcp4 5 changing from 5 51 in 2021 2050 to 6 05 in 2046 2075 for rcp4 5 and from 1 67 to 4 06 for rcp8 5 this increase then became larger for rcp8 5 over most of the remaining periods from 6 20 in 2047 2076 to 7 58 in 2071 2100 for rcp4 5 and from 6 48 to 10 48 for rcp8 5 both emission scenarios predicted decreases in low flow for all future periods in terms of the median value of all 20 gcms as the time period goes further from 2030 to 2059 rcp4 5 projected less decrease in low flow while rcp8 5 projected more increase a significant increasing trend was observed in the relative change of low flow for rcp4 5 which varied from 7 7 in 2021 2050 to 3 7 in 2071 2100 the relative change for rcp8 5 varied from 7 3 in 2021 2050 to 7 9 in 2071 2100 with a significant decreasing trend on the whole rcp8 5 predicted a larger decrease in low flow than rcp4 5 similar to precipitation and temperature the periodicity of ensembles medians of mean flow high flow and low flow was analyzed using wavelet analysis and their trend was modeled using fourth order polynomial fig 9 shows that the mean flow and high flow for both emission scenarios and low flow for rcp4 5 do not exhibit significant periodicity at the p 05 significance level for future periods while the low flow for rcp8 5 exhibited a 16 year periodicity but there are some significant cycles in some years such as a 1 4 year cycle from 2050s to 2060s for mean flow in rcp4 5 in addition all three metrics dr mae and r2 values indicated that fourth order polynomial is a suitable choice in simulating the changing trend of future flow expect for low flow in rcp8 5 whose r2 value is only 0 22 while others are above 0 85 table 4 in addition fourth order polynomial performed a bit worse in modeling flow change than modeling climate change this may be because of the non linear process from climate to hydrology 4 4 uncertainty range of projection 4 4 1 uncertainty range in climate change projection fig 10 presents the temporal variation of uncertainty in mean annual precipitation minimum and maximum temperature projections for both emission scenarios in terms of change range of 20 gcms on the whole rcp8 5 showed a large uncertainty in predicting both precipitation and temperature the uncertainty of precipitation changed from 21 7 in 2021 2050 to 25 1 in 2071 2100 for rcp4 5 and from 12 6 in 2021 2050 to 33 5 in 2071 2100 for rcp8 5 the uncertainty of minimum temperature changed from 0 73 c in 2021 2050 to 1 86 c in 2071 2100 for rcp4 5 and from 1 34 c in 2021 2050 to 2 90 c in 2071 2100 for rcp8 5 wavelet analysis showed that the uncertainty variation of precipitation and temperature projections did not contain significant periodicity at the p 05 significance level fig 11 however a significant increase trend was observed by the mann kendall trend test at the p 05 significance level fourth order polynomials performed reasonably well in terms of simulating the temporal variations of precipitation and temperature uncertainty with the exception of precipitation change under rcp4 5 table 5 the dr and r2 values for the five reasonable modeling were all above 0 90 and mae values are very small while for the worse modeling the dr r2 values are 0 60 0 37 respectively the worse performance of fourth order polynomials for uncertainty variation of precipitation change under rcp4 5 is due to the abnormal projection of miroc5 miroc5 projected a much more change in precipitation than other gcms under rcp4 5 see fig 5 particularly in the periods between 2027 and 2096 a much better performance could be achieved when this climate model was excluded probably a pre selection of gcms should be conducted when using multiple gcms to investigate their uncertainty 4 4 2 uncertainty range in streamflow change projection similarly wavelet analysis mann kendall trend test and regression analysis were also used to analyze the uncertainty of streamflow changes fig 12 displays the temporal variation of uncertainty in mean high and low flow projections for both emission scenarios generally the high flow showed the largest uncertainty under both emission scenarios followed by mean and low flows specifically the uncertainty of high flow reached 58 0 in rcp4 5 and 85 4 in rcp8 5 but the uncertainty of mean flow only reached 43 6 in rcp4 5 and 62 7 in rcp8 5 and they were 35 5 in rcp4 5 and 59 0 in rcp8 5 for low flow moreover on the whole rcp8 5 still projected larger uncertainty than rcp4 5 for all flow statistics for example the uncertainty of mean flow was 34 2 for rcp4 5 and 23 6 for rcp8 5 in 2021 2050 and it increased to 43 6 for rcp4 5 and 62 7 for rcp8 5 in 2071 2100 similar to precipitation and temperature significant periodicity was not detected by wavelet analysis for all mean high and low flows at the p 05 significance level fig 13 while a significant increasing trend was observed by the mann kendall trend test at the p 05 significance level the test results of the fourth order polynomials fitted in the streamflow changes are shown in table 6 from the table the fourth order polynomials successfully modeled the temporal variation of uncertainty for streamflow change with most dr values above 0 70 and r2 values above 0 76 the fourth order polynomial model performed a bit worse in simulating the uncertainty variation in low flow projection under rcp4 5 than the mean and high flows uncertainty but it is still statistically significant at p 05 when using f test 5 discussion and conclusion this study investigated and estimated the temporal variation mainly long term of uncertainty related to the choice of a gcm in hydrological climate change impact studies the uncertainty was represented by the envelopes of 20 gcms under two emission scenarios rcp4 5 and rcp8 5 the temporal variation was presented by 51 future time periods moving from 2021 to 2100 with a 30 year window and 1 year interval some statistical approaches including wavelet analysis mann kendall trend test and polynomial regression analysis were used to analyze the temporal variation the precipitation and temperature were projected to increase significantly in the future but with great uncertainty for example the ensemble median of precipitation predictions increased from 1 4 in 2021 2050 to 6 5 in 2071 2100 for rcp4 5 and from 1 4 to 10 3 for rcp8 5 but the projection uncertainty reached 21 7 in 2021 2050 and 25 1 in 2071 2100 for rcp4 5 and reached 12 6 in 2021 2050 and 33 5 in 2071 2100 for rcp8 5 in addition the projection uncertainty under a high greenhouse gas emission scenario rcp8 5 was found to be larger than that under a low greenhouse gas emission scenario rcp4 5 this means these gcms behaved more differently from each other under higher emission scenario in the future despite they were constructed under same radiative forcing wavelet analysis showed insignificant periodicity for the ensemble medians of climatic and hydrological variables and their uncertainty at the p 05 significance level with the exception of low flow in rcp8 5 which showed a 16 year periodicity this is because the 30 year moving average used in this study filtered the inter annual or multi year variability fluctuation in order to examine if there was any short term periodicity wavelet analysis was further applied to analyze the yearly variation of climatic and hydrological variables in 2021 2100 the results showed that the yearly time series of future annual precipitation had a 4 year cycle in rcp4 5 and 8 year 19 year cycles in rcp8 5 and future temperature had no significant periodicity at the p 05 significance level the yearly variation of future mean flow had a 5 year cycle in rcp4 5 and 20 year cycle in rcp8 5 however these small periodicities were filtered out by the 30 year moving window so in the long term no significant periodicity was observed the fourth order polynomial is a reasonable choice for modeling the temporal variation of climatic and hydrological changes as well as their uncertainty the climate system is highly nonlinear and change is often abrupt and episodic rather than slow and gradual rial et al 2004 bond et al 2013 modeling the abrupt changes can be really challenging and also makes little sense so this study chose to use a 30 year moving window to filter out the small fluctuations and changes and highlight the long term changing trend besides changes in climate system tend to be nonstationary due to the increasing anthropogenic activity from this perspective a nonlinear model instead of a linear model is more reasonable to model the temporal variations of climatic and hydrological ensemble medians and their uncertainty the fourth order polynomial was chosen in this case study based on previous studies e g hawkins and sutton 2009 2011 the fourth order polynomial performed reasonably well in simulating the temporal variation of ensemble medians of climate and hydrology as well as their uncertainty with the exception of modeling low flow magnitude in rcp8 5 precipitation uncertainty in rcp4 5 and low flow uncertainty in rcp4 5 the worse performance in modeling precipitation uncertainty in rcp4 5 was because one gcm miroc5 projected much more precipitation than all other gcms the worse performance in modeling low flow magnitude in rcp8 5 and low flow uncertainty in rcp4 5 may be because of the difficulty in accurately simulating low flow by the hydrological model in this basin it is worth mentioning that there are some limitations to the approach presented in this study one is that only one downscaling method was adapted for downscaling purpose downscaling method is usually considered to be another contributor to uncertainty in climate change impact studies wilby and harris 2006 chen et al 2011b among the large community of downscaling methods varying from dynamical to statistical it is less confident to say which method is preferable than others teng et al 2012 every method has its advantages as well as disadvantages this study chose a hybrid downscaling method that combines loci and dt to take account of correction in both wet day frequency and wet day precipitation distribution in addition only one hydrological model was used in this study even though hydrological model is also an important source of uncertainty in hydrological climate change impact studies jiang et al 2007 dibike and coulibaly 2007 this thoughtlessness is also because the aim of the study is mainly to investigate the uncertainty related to the choice of a gcm on the other hand compared to gcm and downscaling method the choice of hydrological models can have less influence on the hydrological projection wilby and harris 2006 chen et al 2011b in this study the hmets model calibrated using nse as criterion was used for simulating discharge in the future after compared with two other hydrological models gr4j model perrin et al 2003 and xinanjiang model xaj zhao 1992 zeng et al 2016 calibrated using nse and kling gupta efficiency kge defined by gupta et al 2009 as criteria on the whole the hmets model calibrated using nse performed best in terms of reproducing the observed streamflow while the gr4j and xaj models can yield approximately same efficiency values as hmets model if nse or kge was selected as criterion but they reproduced low flow with greater error the hmets model calibrated using kge as criterion also failed to capture the variation of low flow moreover the uncertainty of climate change impacts may vary among watersheds due to different hydro climatologic characteristics and combinations between climate simulations and regional conditions kay et al 2009 jung et al 2012 it would be worthwhile to apply the methodology presented here to other watersheds to extract more generalized conclusions acknowledgements this work was partially supported by the national natural science foundation of china grant no 51779176 51525902 51539009 and the thousand youth talents plan from the organization department of ccp central committee wuhan university china the authors would like to acknowledge the contribution of the world climate research program working group on coupled modelling and to thank climate modeling groups for making available their respective climate model outputs the authors wish to thank the china meteorological data sharing service system and the bureau of hydrology of the changjiang water resources commission for providing dataset for the hanjiang river watershed appendix a supplementary data supplementary data associated with this article can be found in the online version at https doi org 10 1016 j jhydrol 2017 11 004 appendix a supplementary data supplementary data 1 
7537,uncertainty estimation of climate change impacts on hydrology has received much attention in the research community the choice of a global climate model gcm is usually considered as the largest contributor to the uncertainty of climate change impacts the temporal variation of gcm uncertainty needs to be investigated for making long term decisions to deal with climate change accordingly this study investigated the temporal variation mainly long term of uncertainty related to the choice of a gcm in predicting climate change impacts on hydrology by using multi gcms over multiple continuous future periods specifically twenty cmip5 gcms under rcp4 5 and rcp8 5 emission scenarios were adapted to adequately represent this uncertainty envelope fifty one 30 year future periods moving from 2021 to 2100 with 1 year interval were produced to express the temporal variation future climatic and hydrological regimes over all future periods were compared to those in the reference period 1971 2000 using a set of metrics including mean and extremes the periodicity of climatic and hydrological changes and their uncertainty were analyzed using wavelet analysis while the trend was analyzed using mann kendall trend test and regression analysis the results showed that both future climate change precipitation and temperature and hydrological response predicted by the twenty gcms were highly uncertain and the uncertainty increased significantly over time for example the change of mean annual precipitation increased from 1 4 in 2021 2050 to 6 5 in 2071 2100 for rcp4 5 in terms of the median value of multi models but the projected uncertainty reached 21 7 in 2021 2050 and 25 1 in 2071 2100 for rcp4 5 the uncertainty under a high emission scenario rcp8 5 was much larger than that under a relatively low emission scenario rcp4 5 almost all climatic and hydrological regimes and their uncertainty did not show significant periodicity at the p 05 significance level but their temporal variation could be well modeled by using the fourth order polynomial overall this study further emphasized the importance of using multiple gcms for studying climate change impacts on hydrology furthermore the temporal variation of uncertainty sourced from gcms should be given more attention keywords climate change hydrology uncertainty global climate model temporal variation 1 introduction various sources of uncertainty have been identified in the process of assessing climate change impacts on hydrology wilby and harris 2006 chen et al 2011b understanding quantifying and reducing the uncertainty become crucial to water resource management generally the uncertainty mainly comes from 1 greenhouse gas emission scenario ghges e g maurer 2007 nobrega et al 2011 fang et al 2015 ficklin et al 2016 2 global climate model gcm e g jenkins and lowe 2003 rowell 2006 teng et al 2012 chen et al 2012 3 natural climate variability e g hawkins and sutton 2009 brisson et al 2015 chen et al 2016 4 downscaling method e g wilby et al 1998 fowler et al 2007 samadi et al 2013 etemadi et al 2014 li et al 2015 and 5 hydrological model structure and parameter e g poulin et al 2011 bae et al 2011 brigode et al 2013 eregno et al 2013 jiang et al 2007 many studies have involved in quantifying the uncertainty of climate change impacts on hydrology arising from above sources especially the uncertainty related to gcm and ghges has been extensively studied during the last decade for example maurer 2007 investigated the uncertainty related to the choice of a gcm and ghges in assessing the hydrological impacts of climate change in the sierra nevada california by using eleven gcms under two emission scenarios nobrega et al 2011 quantified the uncertainty of climate change impacts on the discharge of rio grande in south america by using six gcms and four emission scenarios minville et al 2008 employed five gcms and two ghgess to implement the same investigation over a nordic watershed these studies found that both gcm and ghges contribute great uncertainty to the hydrological impacts of climate change the uncertainty of downscaling has also been investigated in several studies for example mpelasoka and chiew 2009 compared three downscaling methods in constructing future runoff across australia chen et al 2011a compared six downscaling methods in simulating the future streamflow of a canadian watershed they found that the uncertainty related to the downscaling method could be significant when quantifying climate change impacts on hydrology especially for extremes studies have also investigated the uncertainty of hydrological modeling when assessing climate change impacts on hydrology for example jiang et al 2007 compared six monthly water balance models in investigating the potential impacts of climate change on the water availability of a chinese basin teng et al 2012 used five lumped conceptual daily rainfall runoff models to simulate climate change impacts on the runoff across southeast australia their results indicated that hydrological model could lead to significant uncertainty in the prediction of future runoff in addition to the investigation of each individual uncertainty source several studies also compared the uncertainty from different sources in the assessment of hydrological climate change impacts for example wilby and harris 2006 compared five uncertainty sources in the prediction of low flows for the river thames uk by using four gcms two ghgess two statistical downscaling techniques two hydrological model structures and two sets of hydrological model parameters their results indicated that the choice of a gcm is the largest contributor to uncertainty followed by the downscaling method and uncertainty related to hydrological modeling and emission scenario is relatively less important prudhomme and davies 2009 employed three gcms two ghgess and two downscaling techniques statistical downscaling model sdsm and dynamical downscaling model rcm hadrm3 to investigate the uncertainty of climate change impacts on the river flow of four british catchments they also found that gcm is the largest source of uncertainty while uncertainty related to downscaling technique and emission scenario is of similar magnitude but generally smaller than that related to gcm similarly chen et al 2011b conducted a comprehensive study of uncertainty using two ghgess six gcms a five member ensemble of a gcm four downscaling techniques three hydrological model structures and ten sets of hydrological model parameters the results showed that the choice of a gcm is a major contributor to uncertainty followed by downscaling method ghges and hydrological model structure the contribution of hydrological model parameters is less important all above studies consistently show that the choice of a gcm is the largest contributor to overall uncertainty of hydrological climate change impact studies thus the use of a multi model ensemble is recommended when implementing climate change studies minville et al 2008 zhang et al 2011 teng et al 2012 although lots of studies have explored the uncertainty of climate change as well as it impacts studies focusing on the temporal variation of uncertainty are relatively fewer and most are qualitative studies for example hawkins and sutton 2009 2011 investigated the temporal variation of uncertainty sourced from internal variability climate model emission scenarios for the 21th century jung et al 2012 compared the uncertainty of climate model hydrological model parameter and emission scenario from ten 20 year time slice periods in 2000 2099 for two river basins in the pacific northwest of us ahmadalipour et al 2017 characterized the uncertainty from climate model downscaling and emission scenarios for period 2010 2099 when assessing the climate changes over columbia river basin the results of above studies showed that the uncertainty from various sources would vary over time and the uncertainty derived from one source might dominate in one period while that derived from the other source might prevail in the other period but beyond that some important questions or puzzles still need to be solved when assessing the hydrological impacts of climate change such as how does the uncertainty caused by single source propagate over time how to quantify the temporal variation of uncertainty answers to such questions would contribute to a more reliable climate change projection as well as better strategies dealing with climate change therefore this study investigated and estimated the temporal variation mainly long term of uncertainty stemming from the major contributor the choice of a gcm in assessing the hydrological impacts of climate change to achieve this twenty cmip5 gcms under two emission scenarios rcp4 5 and rcp8 5 were adopted to adequately represent this uncertainty envelope fifty one 30 year continuous future horizons obtained by moving a 30 year window from 2021 to 2100 with 1 year interval were used to express the temporal variation of uncertainty the temporal variation was analyzed by a set of statistical approaches such as wavelet analysis mann kendall trend test and polynomial regression analysis the paper is structured as follows a brief description of the studied area and data is presented in section 2 followed by a detailed description of the bias correction method hydrological model and data analysis approach in section 3 section 4 elaborates on the results by analyzing the climatic and hydrologic changes and their uncertainty over 51 future periods finally the discussion and conclusion are provided in section 5 2 study area and data 2 1 study area the proposed approach described in section 3 is conducted on the hanjiang river watershed fig 1 which is located in the south central china the river originates from the southern qinling mountain flows through shanxi and hubei provinces and finally merges into the yangtze river the longest river in asia in wuhan city with a main stream length of 1577 km and a drainage area of 159 000 km2 in the hanjiang river watershed the annual mean temperature is about 15 5 c and the annual average rainfall is 873 mm chen et al 2012 the watershed has a subtropical monsoon climate in which the precipitation is mainly caused by the southeastern and southwestern maritime monsoons seventy five percent of its total precipitation occurs between june and october during this time the prolonged rainfalls in autumn and rainstorms in early summer often cause major floods xu et al 2012 the annual mean discharge of the hanjiang river is about 1150 m3 s wang et al 2015 in the middle of the hanjiang river watershed there locates the danjiangkou reservoir which serves as a water source for the middle route of the south to north water transfer project mswtp the sub basin above the danjiangkou reservoir with a drainage area of 89 540 km2 is used in this study the mswtp has relieved chronic water shortage problems in several provinces and cities of china including the capital beijing this is a crucial motivating factor for understanding the uncertainty and its temporal variation of climate change impacts on the hydrology over this watershed as it will be vital for making water diversion decisions for this watershed in the future 2 2 data this study used both observed and gcm simulated daily precipitation prcp minimum and maximum temperature tmin tmax data in the hanjiang river watershed the observed data covered 1971 2000 for the historical period the gcms covered 1971 2000 for the history and 2021 2100 for the future observed meteorological data from eleven weather stations were obtained from the china meteorological data sharing service system the streamflow at the sub basin outlet is the inflow of the danjiangkou reservoir which was provided by the bureau of hydrology of the changjiang water resources commission the climate model simulations were taken from the coupled model intercomparison project phase 5 cmip5 database twenty gcms under two ghgess rcp4 5 and rcp8 5 from fourteen modeling centres were selected to adequately represent the uncertainty of climate models table 1 presents the general information of these twenty gcms in order to run a lumped hydrological model the precipitation and temperature data from both observations and gcm simulations were averaged over several grid boxes within and surrounding the watershed to a single time series using the thiessen polygon method 3 methodology 3 1 bias correction method gcm outputs are usually too coarse to directly drive a hydrological model for hydrological climate change impact studies in this case a downscaling or bias correction technique must be utilized xu 1999 chen et al 2011a jung et al 2012 this study used a bias correction method for its simplicity and good performance teng et al 2012 jung et al 2012 ahmadalipour et al 2017 the used bias correction method is the daily bias correction method dbc chen et al 2013 which is a hybrid advanced bias correction method combing the local intensity scaling method loci schmidli et al 2006 and the daily translation method dt mpelasoka and chiew 2009 to correct both the wet day frequency and the distribution of wet day precipitation the dbc method first takes advantage of the loci method to correct the precipitation occurrence and then instead of applying the same factor loci to scale daily precipitation for a particular month it adopts the dt method to modulate the changes as a function of the percentile differences in the frequency distribution of precipitation or temperature between observation and climate model more details are as follows 1 for a particular month e g january in the reference period a wet day threshold is determined from the gcm to ensure that the threshold exceedance equals the observed wet day frequency for that month 2 the threshold is then applied to the future period to correct the gcm s precipitation in corresponding month 3 a relationship between observation and gcm is established using the distribution mapping technique by calculating the percentiles 100 percentile values from the 1st to the 100th percentile in this study of daily precipitation temperature in the reference period in other words the distributions of observation and gcm are represented by 100 percentile values and 4 the percentile ratios of precipitation are multiplied by the gcm precipitation series in the future period or the percentile differences are added to the gcm temperature series in the future period eq 1 1 x cor j f o 1 f m x fut j where x refers to precipitation or temperature fut means variable in the future period cor means corrected variable the subscript j refers to a specific day in the future period fm is the cumulative distribution function cdf represented by 100 percentile values from the 1st to the 100th of simulated precipitation or temperature in the reference period and fo 1 is the inverse cdf of the observed precipitation or temperature in the reference period for every gcm the dbc method was first calibrated in the reference period i e find the gcm wet day threshold for every month and construct the distribution relationships between observation and gcm and then the calibrated dbc was applied to correct gcm s precipitation and temperature for all fifty one future periods which are 2021 2050 2035s 2022 2051 2036s 2023 2052 2037s 2070 2099 2084s and 2071 2100 2085s note that these future periods were corrected individually from each other the fifty one 30 year continuous future periods with 1 year interval filtered the short term fluctuations and changes such as inter annual variability so they mainly reflected the long term temporal variation of climatic and hydrological changes and their uncertainty additionally when using a bias correction method it is implicitly assumed that climate model s biases are stationary over time maraun 2013 chen et al 2013 3 2 hydrological modeling the hydrological model used in this study is the hmets model the hydrological model of école de technologie supérieure which is a lumped conceptual rainfall runoff model developed at the école de technologie supérieure university of quebec it has been used in several flow forecasting and climate change impact studies e g chen et al 2011b arsenault et al 2014 brigode et al 2015 hmets uses two connected reservoirs representing unsaturated i e vadose and saturated i e phreatic zones and simulates the basic hydrological processes of snow accumulation melting and refreezing evapotranspiration infiltration as well as flow routing to the watershed outlet martel et al 2017 troin et al 2015 the model begins with a snowmelt module which works in three steps overnight refreezing snowmelt and snowpack water retention capacity the water in the snowpack is from snowmelt and liquid precipitation if the amount of water in the snowpack exceeds the water retention capacity the remaining water is then added to the water available for runoff a fraction of the water available for runoff will be directly taken as surface runoff once the surface runoff and real evapotranspiration are taken out the remaining water for runoff will infiltrate to the vadose zone in this process a portion of the infiltrated water which depends on the water level of vadose zone is treated as delayed runoff component the vadose zone offers water for the hypodermic flow component and the exchanges to the saturated zone if the vadose zone fills up the remaining water will be added to the delayed runoff component the saturated zone is mainly used to release groundwater flow if the saturated zone level exceeds the maximum the exceeding water is also added to the delayed runoff component surface and delayed runoff are transferred to the outlet using two unit hydrographs and are summarized to the hypodermic flow and the groundwater flow finally the total streamflow is used to represent the modeled streamflow hmets has up to 21 parameters which are shown in table 2 martel et al 2017 model calibration is done automatically using the covariance matrix adaptation evolution strategy cmaes this strategy is a continuous domain non convex non linear problem optimization algorithm it estimates the derivatives of the covariance matrix of the previously successful candidate solution distribution using the maximum likelihood principle in this way it tends to maximize the likelihood of the distribution more details of this optimization method can be found in hansen and ostermeier 2001 and arsenault et al 2014 hmets requires daily mean temperature and precipitation as inputs if the maximum and minimum temperatures are input to the model they are automatically averaged to mean temperature a natural streamflow time series is also needed for model calibration the optimal combination of parameters is chosen based on the nash sutcliffe efficiency nse criterion in this study 20 years 1961 1980 of daily time series precipitation maximum and minimum temperature and flow were used for model calibration and another 20 years 1981 2000 were used for model validation the optimized parameters yielded nash sutcliffe criterion values of 0 79 for calibration and 0 78 for validation which shows that hmets performs relatively well for this watershed at the daily time step the observed and simulated mean hydrographs in fig 2 further demonstrate the reasonable performance of hmets in this watershed 3 3 data analysis the performance of the dbc method is first verified over the reference period 1971 2000 using nine climate statistics as criteria which include the mean standard deviation 25th 50th 75th 90th 95th and 99th percentiles and maximum of precipitation or temperature at different timescales daily monthly and annual the relative differences between the corrected and observed precipitation series of these statistics were calculated eq 2 while the absolute differences were calculated for temperature eq 3 2 rd p cor p obs p obs 100 3 ad t cor t obs where rd means relative difference ad means absolute difference pobs pcor mean statistics of observed and corrected precipitation respectively tobs tcor mean statistics of observed and corrected temperature respectively for hydrological simulation the corrected precipitation and temperature series in 51 future periods were used to drive the calibrated hmets model fifty one hydrological simulations were obtained by running the hydrological model 51 times using 51 future climate projections all based on the same calibrated parameters the hydrological impacts of climate change were represented by three hydrological variables mean flow high flow 95th percentile of 30 year daily flow and low flow 5th percentile of 30 year daily flow the changes of these variables for future periods relative to the reference period were calculated since the hydrological model is not perfect the simulated streamflow is slightly biased when comparing to observed counterpart to get rid of the influence of the biases caused by the hydrological model the model simulated future period streamflow was compared with the model simulated reference period streamflow rather than the gauged streamflow this method has been widely used in previous studies e g minville et al 2008 chen et al 2011a jung et al 2012 with the changes of future climate and hydrology relative change for precipitation and discharge and absolute change for temperature from 20 gcms and 51 future periods the ensemble median similar to ensemble mean and its uncertainty for each period can be defined as illustrated in fig 3 the upper part of fig 3 presents the calculation of ensemble median and uncertainty for one future period for example in the future period 2021 2050 20 gcms could provide 20 different values in terms of the future change the ensemble median is defined as the median value of the 20 predictions and the uncertainty is defined as the range between the maximum and minimum predictions when excluding outliers when illustrating with a boxplot fig 3 the ensemble median in a future period is q3 and the uncertainty is the range between the upper and lower boundaries q1 q5 the ensemble median and uncertainty range have been widely used to represent the temporal variation of uncertainty in previous studies e g kharin et al 2013 sillmann et al 2013 ipcc 2013 even though resampling methods such as bootstrapping khan et al 2006 samadi et al 2013 can also be used to define uncertainty following this method 51 ensemble medians and 51 uncertainty values were calculated for all 51 future periods to investigate the temporal variation of deterministic changes and their uncertainty the lower part of fig 3 some data analysis techniques wavelet analysis torrence and compo 1998 xu et al 2011 mann kendall trend test mann 1945 kendall 1975 and regression analysis bates and watts 1988 were used to analyze the temporal variation of deterministic changes and their uncertainty specifically wavelet analysis was used to analyze the periodicity and the mann kendall trend test was applied to examine the trend if the mann kendall trend test revealed significant trend at the p 05 significance level then the regression analysis was applied to simulate the changing pattern of the time series besides f test was used to examine the significance of the regression model and three indexes measured the performance of the regression model including willmott s refined index of agreement dr willmott et al 2012 willmott et al 2015 determination coefficient r2 ferraro et al 2011 and mean absolute error mae willmott and matsuura 2005 eq 4 gives the definition of dr which is at the range of 1 0 to 1 0 a dr value closer to 1 means better simulation 4 d r 1 i 1 n p i o i c i 1 n o i o i 1 n p i o i c i 1 n o i o c i 1 n o i o i 1 n p i o i 1 i 1 n p i o i c i 1 n o i o where p i is the model derived prediction o i is the observation n is the length of observations and o is the mean of the observations usually c is equal to 2 0 when applying wavelet analysis choosing an appropriate wavelet is of great importance the wavelet depends on the nature of the time series and on the type of information to be extracted de moortel et al 2004 in this study morelet was used based on its good localization in both time and frequency space torrence and compo 1998 and also based on the principles of self similarity compactness and smoothness ramsey 1999 since this study analyzes the periodicity of a time series the continuous wavelet transform cwt was applied statistical significance level of periodicity was estimated against a red noise model torrence and compo 1998 grinsted et al 2004 free matlab software package of wavelet analysis provided by grinsted et al 2004 was used to implement this analysis when using regression analysis considering the chaotic nature of climate system and nonstationarity of climate change a linear model may not be a proper choice for modeling climatic and hydrologic changes and their uncertainty referred to the studies of hawkins and sutton hawkins and sutton 2009 2011 the fourth order polynomial was used to fit the future climatic and hydrological changes and their uncertainty 4 results 4 1 bias correction method verification the performance of the dbc method was validated with respect to reproducing nine observed statistics fig 4 presents the biases of raw and corrected gcms precipitation and temperature on the daily time scale in 1971 2000 in the figure x axis represents the 20 gcms following the order in table 1 y axis represents the gcms biases in terms of reproducing nine statistics mean standard deviation 25th 50th 75th 90th 95th and 99th percentiles and maximum value from number 1 to 9 darker color means larger bias from the figure the raw gcms were considerably biased in simulating precipitation and temperature the biases in raw temperature series were above 2 c absolute value and the biases in raw precipitation series were mostly above 15 absolute value in terms of the nine statistics but after correction the biases in precipitation considerably reduced to less than 5 particularly the dbc method performed better for mean precipitation than for extremes in addition the dbc method considerably reduced the biases of gcm temperature for all statistics especially for minimum temperature all above results indicate the reasonable performance of the dbc method justifying its use for generating future climate change scenarios for hydrological impact studies 4 2 climate change projection 4 2 1 changes in annual precipitation and temperature fig 5 shows the relative change of mean annual precipitation and the absolute change of mean daily temperature for future periods in the figure each boxplot was constructed using 20 values derived from 20 gcms the 51 boxplots in one subgraph represent 51 future periods the mann kendall trend test revealed that the median of multi model ensemble precipitation minimum and maximum temperature all increased significantly at the significance level of p 05 over time for both emission scenarios for rcp4 5 rcp8 5 the ensemble median of annual precipitation increased from 1 4 1 4 for 2021 2050 to 6 5 10 3 for 2071 2100 rcp8 5 predicted a more rapid increase in precipitation than rcp4 5 in addition all gcms predicted increase in daily temperature for both emission scenarios and all future periods similar to precipitation rcp8 5 predicted a larger and more rapid increase in temperature than rcp4 5 for example rcp4 5 predicted a 1 5 c increase in minimum temperature for 2021 2050 and a 2 7 c increase for 2071 2100 in terms of the median of the 20 gcms while rcp8 5 predicted a 1 7 c increase in minimum temperature for 2021 2050 and a 4 8 c increase for 2071 2100 fig 6 presents the continuous wavelet power spectra of the ensemble medians of future mean annual precipitation mean daily minimum and maximum temperatures the results showed that future precipitation and temperature tended to increase with no significant periodicity at the p 05 significance level at the multi decadal scale as there is no thick contour within the black outlines this is expected as the multiple year averages of precipitation and temperature were calculated using a moving window approach the short term fluctuations such as inter annual or multi year variability were filtered the trend of temporal change in mean annual precipitation and temperature was modeled using the fourth order polynomial table 3 displays the test results of the fitted fourth order polynomials from the table the dr and r2 values are all above 0 90 and mae values are very small which indicate the reasonable performance of the fitted model f tests further show that the fitted models were significant at the p 05 significance level 4 2 2 changes in seasonal precipitation and temperature the climate changes in summer and winter mean precipitation and temperature were also analyzed as the high flow and low flow occur in these two seasons the figures in this part were presented in the supplementary material in order to make the paper in a proper length fig s1 shows that the summer and winter precipitations were all projected to increase for both emission scenarios for summer the overall changes of precipitation between the two emission scenarios showed slight difference with rcp4 5 projecting increases of 1 08 in 2021 2050 and 3 09 in 2071 2100 while rcp8 5 projecting increases of 1 91 in 2021 2050 and 3 42 in 2071 2100 for winter rcp8 5 predicted more increase than rcp4 5 with the relative change of winter precipitation varying from 8 07 in 2021 2050 to 24 41 in 2071 2100 for rcp4 5 and from 8 69 to 26 20 for rcp8 5 generally winter precipitation would likely increase more rapidly than summer precipitation for both emission scenarios for temperature only the results of minimum temperature were presented as maximum temperature showed similar changing trends fig s2 all gcms projected increases in temperature for both seasons and both emission scenarios the temperature increased slightly more in summer than in winter for both emission scenarios additionally rcp8 5 projected a larger and more rapid increase in seasonal temperature than rcp4 5 which is coincident with the case of annual mean daily temperature projection the test and measurement results displayed on the top left corner of each graph in figs s1 and s2 showed that fourth order polynomial showed reasonable performance in modeling the seasonal climate s temporal variation similar to mean annual precipitation and temperature on the whole the ensemble medians of mean seasonal precipitation and temperature did not show significant periodicity at the p 05 significance level tested by wavelet analysis figs s3 and s4 4 3 hydrological impact of climate change 4 3 1 average hydrographs fig 7 presents the averaged hydrographs simulated using raw and bias corrected gcms meteorological data in the reference period 1971 2000 which reflect the performance of bias correction in hydrological modeling the gcm simulated streamflows were compared to those simulated using observed meteorological data the results showed that the streamflows produced using raw gcm data were considerably biased for example the simulations using raw gcms data reached 9728 m3 s while the simulation using observed data was just 3278 m3 s for maximum flow after bias correction the differences among the gcms simulations were remarkably reduced and the difference between gcm and observed simulations was also considerably reduced compared to the large difference 6450 m3 s for maximum flow before bias correction the difference was reduced to 1706 m3 s after bias correction i e the simulated maximum flow was 4984 m3 s overall even though biases remain in maximum flows the bias correction method performed well for most hydrological regimes since some biases remained after bias correction the following section determined the hydrological impacts of climate change in terms of the relative change between the model simulated stream flows in future and reference periods the hydrology in the reference period was simulated using corrected gcm precipitation and temperature 4 3 2 change in mean and extreme flows fig 8 presents the relative change of mean high and low flows for 51 future periods the mann kendall trend test revealed significant trends at the p 05 significance level in all these flows for both emission scenarios rcp4 5 predicted increasing in mean flows for all future periods in terms of the median of 20 gcms projections while rcp8 5 predicted decreasing in mean flows for the periods between 2036 and 2065 and increasing for other periods the ensemble medians of mean flow projections changed from 0 6 in 2021 2050 to 5 4 in 2071 2100 for rcp4 5 and from 0 2 to 7 9 for rcp8 5 overall the mean flow was predicted to increase more for rcp4 5 before the 2042 2071 period and then increase more for rcp8 5 in the last half of the 51 future periods i e from 2048 to 2100 the ensemble medians of high flow was much larger in the first half of 51 future periods for rcp4 5 changing from 5 51 in 2021 2050 to 6 05 in 2046 2075 for rcp4 5 and from 1 67 to 4 06 for rcp8 5 this increase then became larger for rcp8 5 over most of the remaining periods from 6 20 in 2047 2076 to 7 58 in 2071 2100 for rcp4 5 and from 6 48 to 10 48 for rcp8 5 both emission scenarios predicted decreases in low flow for all future periods in terms of the median value of all 20 gcms as the time period goes further from 2030 to 2059 rcp4 5 projected less decrease in low flow while rcp8 5 projected more increase a significant increasing trend was observed in the relative change of low flow for rcp4 5 which varied from 7 7 in 2021 2050 to 3 7 in 2071 2100 the relative change for rcp8 5 varied from 7 3 in 2021 2050 to 7 9 in 2071 2100 with a significant decreasing trend on the whole rcp8 5 predicted a larger decrease in low flow than rcp4 5 similar to precipitation and temperature the periodicity of ensembles medians of mean flow high flow and low flow was analyzed using wavelet analysis and their trend was modeled using fourth order polynomial fig 9 shows that the mean flow and high flow for both emission scenarios and low flow for rcp4 5 do not exhibit significant periodicity at the p 05 significance level for future periods while the low flow for rcp8 5 exhibited a 16 year periodicity but there are some significant cycles in some years such as a 1 4 year cycle from 2050s to 2060s for mean flow in rcp4 5 in addition all three metrics dr mae and r2 values indicated that fourth order polynomial is a suitable choice in simulating the changing trend of future flow expect for low flow in rcp8 5 whose r2 value is only 0 22 while others are above 0 85 table 4 in addition fourth order polynomial performed a bit worse in modeling flow change than modeling climate change this may be because of the non linear process from climate to hydrology 4 4 uncertainty range of projection 4 4 1 uncertainty range in climate change projection fig 10 presents the temporal variation of uncertainty in mean annual precipitation minimum and maximum temperature projections for both emission scenarios in terms of change range of 20 gcms on the whole rcp8 5 showed a large uncertainty in predicting both precipitation and temperature the uncertainty of precipitation changed from 21 7 in 2021 2050 to 25 1 in 2071 2100 for rcp4 5 and from 12 6 in 2021 2050 to 33 5 in 2071 2100 for rcp8 5 the uncertainty of minimum temperature changed from 0 73 c in 2021 2050 to 1 86 c in 2071 2100 for rcp4 5 and from 1 34 c in 2021 2050 to 2 90 c in 2071 2100 for rcp8 5 wavelet analysis showed that the uncertainty variation of precipitation and temperature projections did not contain significant periodicity at the p 05 significance level fig 11 however a significant increase trend was observed by the mann kendall trend test at the p 05 significance level fourth order polynomials performed reasonably well in terms of simulating the temporal variations of precipitation and temperature uncertainty with the exception of precipitation change under rcp4 5 table 5 the dr and r2 values for the five reasonable modeling were all above 0 90 and mae values are very small while for the worse modeling the dr r2 values are 0 60 0 37 respectively the worse performance of fourth order polynomials for uncertainty variation of precipitation change under rcp4 5 is due to the abnormal projection of miroc5 miroc5 projected a much more change in precipitation than other gcms under rcp4 5 see fig 5 particularly in the periods between 2027 and 2096 a much better performance could be achieved when this climate model was excluded probably a pre selection of gcms should be conducted when using multiple gcms to investigate their uncertainty 4 4 2 uncertainty range in streamflow change projection similarly wavelet analysis mann kendall trend test and regression analysis were also used to analyze the uncertainty of streamflow changes fig 12 displays the temporal variation of uncertainty in mean high and low flow projections for both emission scenarios generally the high flow showed the largest uncertainty under both emission scenarios followed by mean and low flows specifically the uncertainty of high flow reached 58 0 in rcp4 5 and 85 4 in rcp8 5 but the uncertainty of mean flow only reached 43 6 in rcp4 5 and 62 7 in rcp8 5 and they were 35 5 in rcp4 5 and 59 0 in rcp8 5 for low flow moreover on the whole rcp8 5 still projected larger uncertainty than rcp4 5 for all flow statistics for example the uncertainty of mean flow was 34 2 for rcp4 5 and 23 6 for rcp8 5 in 2021 2050 and it increased to 43 6 for rcp4 5 and 62 7 for rcp8 5 in 2071 2100 similar to precipitation and temperature significant periodicity was not detected by wavelet analysis for all mean high and low flows at the p 05 significance level fig 13 while a significant increasing trend was observed by the mann kendall trend test at the p 05 significance level the test results of the fourth order polynomials fitted in the streamflow changes are shown in table 6 from the table the fourth order polynomials successfully modeled the temporal variation of uncertainty for streamflow change with most dr values above 0 70 and r2 values above 0 76 the fourth order polynomial model performed a bit worse in simulating the uncertainty variation in low flow projection under rcp4 5 than the mean and high flows uncertainty but it is still statistically significant at p 05 when using f test 5 discussion and conclusion this study investigated and estimated the temporal variation mainly long term of uncertainty related to the choice of a gcm in hydrological climate change impact studies the uncertainty was represented by the envelopes of 20 gcms under two emission scenarios rcp4 5 and rcp8 5 the temporal variation was presented by 51 future time periods moving from 2021 to 2100 with a 30 year window and 1 year interval some statistical approaches including wavelet analysis mann kendall trend test and polynomial regression analysis were used to analyze the temporal variation the precipitation and temperature were projected to increase significantly in the future but with great uncertainty for example the ensemble median of precipitation predictions increased from 1 4 in 2021 2050 to 6 5 in 2071 2100 for rcp4 5 and from 1 4 to 10 3 for rcp8 5 but the projection uncertainty reached 21 7 in 2021 2050 and 25 1 in 2071 2100 for rcp4 5 and reached 12 6 in 2021 2050 and 33 5 in 2071 2100 for rcp8 5 in addition the projection uncertainty under a high greenhouse gas emission scenario rcp8 5 was found to be larger than that under a low greenhouse gas emission scenario rcp4 5 this means these gcms behaved more differently from each other under higher emission scenario in the future despite they were constructed under same radiative forcing wavelet analysis showed insignificant periodicity for the ensemble medians of climatic and hydrological variables and their uncertainty at the p 05 significance level with the exception of low flow in rcp8 5 which showed a 16 year periodicity this is because the 30 year moving average used in this study filtered the inter annual or multi year variability fluctuation in order to examine if there was any short term periodicity wavelet analysis was further applied to analyze the yearly variation of climatic and hydrological variables in 2021 2100 the results showed that the yearly time series of future annual precipitation had a 4 year cycle in rcp4 5 and 8 year 19 year cycles in rcp8 5 and future temperature had no significant periodicity at the p 05 significance level the yearly variation of future mean flow had a 5 year cycle in rcp4 5 and 20 year cycle in rcp8 5 however these small periodicities were filtered out by the 30 year moving window so in the long term no significant periodicity was observed the fourth order polynomial is a reasonable choice for modeling the temporal variation of climatic and hydrological changes as well as their uncertainty the climate system is highly nonlinear and change is often abrupt and episodic rather than slow and gradual rial et al 2004 bond et al 2013 modeling the abrupt changes can be really challenging and also makes little sense so this study chose to use a 30 year moving window to filter out the small fluctuations and changes and highlight the long term changing trend besides changes in climate system tend to be nonstationary due to the increasing anthropogenic activity from this perspective a nonlinear model instead of a linear model is more reasonable to model the temporal variations of climatic and hydrological ensemble medians and their uncertainty the fourth order polynomial was chosen in this case study based on previous studies e g hawkins and sutton 2009 2011 the fourth order polynomial performed reasonably well in simulating the temporal variation of ensemble medians of climate and hydrology as well as their uncertainty with the exception of modeling low flow magnitude in rcp8 5 precipitation uncertainty in rcp4 5 and low flow uncertainty in rcp4 5 the worse performance in modeling precipitation uncertainty in rcp4 5 was because one gcm miroc5 projected much more precipitation than all other gcms the worse performance in modeling low flow magnitude in rcp8 5 and low flow uncertainty in rcp4 5 may be because of the difficulty in accurately simulating low flow by the hydrological model in this basin it is worth mentioning that there are some limitations to the approach presented in this study one is that only one downscaling method was adapted for downscaling purpose downscaling method is usually considered to be another contributor to uncertainty in climate change impact studies wilby and harris 2006 chen et al 2011b among the large community of downscaling methods varying from dynamical to statistical it is less confident to say which method is preferable than others teng et al 2012 every method has its advantages as well as disadvantages this study chose a hybrid downscaling method that combines loci and dt to take account of correction in both wet day frequency and wet day precipitation distribution in addition only one hydrological model was used in this study even though hydrological model is also an important source of uncertainty in hydrological climate change impact studies jiang et al 2007 dibike and coulibaly 2007 this thoughtlessness is also because the aim of the study is mainly to investigate the uncertainty related to the choice of a gcm on the other hand compared to gcm and downscaling method the choice of hydrological models can have less influence on the hydrological projection wilby and harris 2006 chen et al 2011b in this study the hmets model calibrated using nse as criterion was used for simulating discharge in the future after compared with two other hydrological models gr4j model perrin et al 2003 and xinanjiang model xaj zhao 1992 zeng et al 2016 calibrated using nse and kling gupta efficiency kge defined by gupta et al 2009 as criteria on the whole the hmets model calibrated using nse performed best in terms of reproducing the observed streamflow while the gr4j and xaj models can yield approximately same efficiency values as hmets model if nse or kge was selected as criterion but they reproduced low flow with greater error the hmets model calibrated using kge as criterion also failed to capture the variation of low flow moreover the uncertainty of climate change impacts may vary among watersheds due to different hydro climatologic characteristics and combinations between climate simulations and regional conditions kay et al 2009 jung et al 2012 it would be worthwhile to apply the methodology presented here to other watersheds to extract more generalized conclusions acknowledgements this work was partially supported by the national natural science foundation of china grant no 51779176 51525902 51539009 and the thousand youth talents plan from the organization department of ccp central committee wuhan university china the authors would like to acknowledge the contribution of the world climate research program working group on coupled modelling and to thank climate modeling groups for making available their respective climate model outputs the authors wish to thank the china meteorological data sharing service system and the bureau of hydrology of the changjiang water resources commission for providing dataset for the hanjiang river watershed appendix a supplementary data supplementary data associated with this article can be found in the online version at https doi org 10 1016 j jhydrol 2017 11 004 appendix a supplementary data supplementary data 1 
7538,recent studies have identified the importance of vegetation processes in terrestrial hydrologic systems process based ecohydrological models combine hydrological physical biochemical and ecological processes of the catchments and as such are generally more complex and parametric than conceptual hydrological models thus appropriate calibration objectives and model uncertainty analysis are essential for ecohydrological modeling in recent years bayesian inference has become one of the most popular tools for quantifying the uncertainties in hydrological modeling with the development of markov chain monte carlo mcmc techniques the bayesian approach offers an appealing alternative to traditional multi objective hydrologic model calibrations by defining proper prior distributions that can be considered analogous to the ad hoc weighting often prescribed in multi objective calibration our study aims to develop appropriate prior distributions and likelihood functions that minimize the model uncertainties and bias within a bayesian ecohydrological modeling framework based on a traditional pareto based model calibration technique in our study a pareto based multi objective optimization and a formal bayesian framework are implemented in a conceptual ecohydrological model that combines a hydrological model hymod and a modified bucket grassland model bgm simulations focused on one objective streamflow lai and multiple objectives streamflow and lai with different emphasis defined via the prior distribution of the model error parameters results show more reliable outputs for both predicted streamflow and lai using bayesian multi objective calibration with specified prior distributions for error parameters based on results from the pareto front in the ecohydrological modeling the methodology implemented here provides insight into the usefulness of multiobjective bayesian calibration for ecohydrologic systems and the importance of appropriate prior distributions in such approaches keywords ecohydrology multi objective optimization bayesian inference prior distribution pareto front 1 introduction in recent years the interaction between water resources and ecosystems has become a central topic among scientists because of the important role of vegetation dynamics in the catchment water cycle porporato et al 2002 asbjornsen et al 2011 on one hand water controls vegetation growth photosynthesis respiration and nutrient cycling on the other hand vegetation influences the water cycle through biochemical and biophysical processes such as evapotranspiration and rainfall interception chen et al 2014 ecohydrology integrates catchment hydrological and ecological processes and provides a framework within which the complex interrelationship between vegetation dynamics and water flows can be well investigated asbjornsen et al 2011 the continuous dynamic models that combine conceptual and physical descriptions of hydrological physical and biogeochemical processes such as water heat carbon and energy transmission are called process based ecohydrological models commonly used model structures across scientific research and application include well known models such as swat arnold et al 1993 arnold et al 1998 swim krysanova et al 1998 krysanova et al 2000 rhessys band et al 1993 tague and band 2004 and tethys chloris fatichi et al 2012 conceptual ecohydrological models are also widely used because of their model parsimony which makes it easier for model calibration istanbulluoglu et al 2012 viola et al 2014 due to the development of geographical information systems giss and remote sensing technology studies of ecohydrological models have accelerated however describing the interactions between hydrologic and ecologic systems requires increasing model dimensionality and data for model conditioning as a result appropriate calibration objectives and model uncertainty analysis are essential for ecohydrological modeling ecohydrological model inference is also made more challenging by the need to represent the dynamics of multiple catchment state variables including streamflow and biomass production thus there is a need for approaches to parameter specification that capitalize on all information that may be available to describe catchment scale ecohydrologic processes in recent years multi objective model optimization has emerged as a popular tool across multiple disciplines to derive parameter estimates that reproduce multiple model criteria or types of system observations multi objective model calibrations are a common tool in model optimization of environmental systems these approaches are frequently used when the objectives are under circumstances such that there are in conflicts with each other konak et al 2006 the benefit of multi objective optimization is that it provides a set of possible solutions which satisfies all the objectives in an acceptable range based on the concept of trade offs yapo et al 1998 the output of this approach is a set of solutions which cannot be improved in each of the objectives without deteriorating the other these solutions are usually called non dominated non inferior or pareto solutions efstratiadis and koutsoyiannis 2010 based on this concept many multi objective algorithms have been developed including the multi objective genetic algorithm moga fonseca and fleming 1993 the nondominated sorting genetic algorithm nsga srinivas and deb 1994 the multi objective complex evolution mocom yapo et al 1998 algorithm the multi objective shuffled complex evolution metropolis algorithm moscem vrugt et al 2003a and the multi algorithm genetically adaptive multi objective amalgam vrugt and robinson 2007 method note that individual objectives may represent different statistical summaries of a set of observations or observations of different variables within the system being studied in hydrological studies one example may be when considering the trade offs of the parameterization between high flows and low flows madsen 2000 shafii and smedt 2009 in ecohydrological modeling where typically an ecological submodel is combined with a hydrological component the optimization of the objective describing the ecological component may not be acceptable for the hydrological component and vice versa the benefits of using a multi objective approach in an ecohydrological model where we consider the interrelationship between the hydrologic cycle and vegetation dynamics are obvious as it provides a formal way in which to simultaneously and comprehensively consider both hydrological and ecological components of the model therefore multi objective optimization approaches are well suited to ecohydrological models for instance naseem et al 2015 presented a work of multi response optimization for two conceptual ecohydrological models across 27 catchments in australia results showed that the multi objective optimization provided a better representation of the two response outputs streamflow and leaf area index lai and has the potential to greatly improve ecohydrological modeling and applications while these multi objective optimization approaches are useful for estimating deterministic parameter values that provide appropriate fits to competing calibration objectives pareto based multi objective calibration does not provide probabilistic solutions for the model parameters or any uncertainty analysis of the corresponding model predictions reichert and schuwirth 2012 with the development of markov chain monte carlo mcmc methods and advances in computing power bayesian inference has emerged asa popular tool for uncertainty analysis in hydrological modeling e g bates and campbell 2001 kuczera 1983 marshall et al 2004 smith and marshall 2008 vrugt et al 2003b bayesian inference provides a framework within which prior information and the data can be combined it allows the statistical quantification of model uncertainties compared with purely conceptual rainfall runoff models it can be argued that ecohydrological models may be more susceptible to uncertainty one reason is that process based models usually contain physically based mathematical descriptions as well as conceptual components the complexity of model leads to possibly large model uncertainty arnold et al 2009 another reason is that often in ecohydrologic modeling different sources and sampling methods are used to collect catchment data leading to multiple potential sources of observational uncertainty that influence model predictions and reliability for example in our case leaf area index lai data at an 8 day 1km resolution via modis moderate resolution imaging spectroradiometer are used for calibration of an ecohydrologic model to capture vegetation dynamics compared with daily stream flow data this lai data likely contains more uncertainties or less information because of the coarse resolution and the error from the model that lai is derived from in lumped conceptual ecohydrological models another sources of uncertainty in lai are related to the aggregation of lai pixels inside a catchment to obtain mean catchment scale lai therefore the potential errors associated with lai and streamflow observations would not be the same that is the two objective functions should not be equally weighted in an optimization problem it is thus important to appropriately define the objective functions weights in a way that reflect the confidence level of observations more recently multi objective bayesian inference has been proposed as an alternative to traditional pareto based multi objective optimizations minet et al 2015 sikorska et al 2015 reichert and schuwirth 2012 these approaches are appealing as they allow model conditioning with multiple data sources and provide a framework in which prior information can be used as a surrogate for traditional weighting of different model objectives in this study we implement a multi objective bayesian calibration for a conceptual ecohydrologic model and demonstrate the usefulness of such an approach for inferring multiple catchment variables we aim to develop appropriate prior distributions and likelihood functions that minimize model uncertainties within the framework based on information obtained from pareto based multi objective optimization the overall goal of this study is to combine traditional multi objective calibration of ecohydrologic models with more recent bayesian approaches and demonstrate how an understanding of the pareto front can help inform bayesian calibrations 2 bayesian multi objective inference multi objective optimization aims to provide a set of possible solutions in front of the modelers however it doesn t provide probabilistic solutions in our study we aim to find an alternative for pareto based multi objective optimization for an ecohydrologic model a bayesian multi objective framework is presented considering different emphasis via the prior distribution on each objective reichert and schuwirth 2012 introduced a method combining prior knowledge and multi objective calibration the preferred weight of each objective can be defined in the prior of error parameters the prior of error parameters are defined as zero mean and favoring narrow distribution over wider distribution for one objective when the modeler wants to put more weight on it such an approach holds promise across many modeling studies that attempt to infer multiple catchment or environmental variables recently sikorska et al 2015 showed a comparison between bayesian single and equal weighted multi objective calibrations in a combination of a rainfall runoff model and a total suspended solids tss model minet et al 2015 presented a case study of bayesian multi objective inversion in a dynamic vegetation model with different types of likelihood functions and prior information of the error parameters these studies indicate the potential importance and application of multi objective calibration in the ecohydrological modeling a conceptual ecohydrological model can be expressed as o t f x t θ ε t where o t are the observations for a catchment at time t x t is model inputs at time t θ is the model parameter set f x t θ is the corresponding model output for time t and ε t is an error term in model specification via bayesian inference the posterior distributions of model parameters are estimated via bayes theorem p θ o p θ p o θ p o where p θ is the prior distribution which summarized the existing knowledge of the model parameters through parametric probability distributions p o θ is the likelihood function which summarizes the model for the data given parameters p o is a proportionality constant the posterior distribution p θ o thus summarizes the parameter uncertainty after observing the data if the error term is assumed to be normally distributed uncorrelated with constant variance σ 2 the log likelihood function can be written as l o t θ n 2 log 2 π n 2 log σ 2 1 2 σ 2 t 1 n o t f x t θ 2 ecohydrologic models simulate multiple variables that describe water energy and biophysical fluxes such as streamflow leaf area index lai net primary production and evapotranspiration as such the function presented here is extended so that o is a vector of observations at each time step for a multivariate case considering the error term follows multivariate gaussian distribution x n μ then the log likelihood function can be expressed as l o t θ n 2 log 2 π n 2 log 1 2 t 1 n o t f x t θ t 1 o t f x t θ parameters being calibrated include model parameters and error parameters σ1 2 and σ2 2 for each of the objectives the corresponding prior can be expresses as p θ p σ1 2 and p σ2 2 hence the log posterior distribution can be expressed as lp θ o t l o t θ lp θ lp σ 1 2 lp σ 2 2 3 methodology in our case studies we aim to take advantage of traditional multi objective calibrations to derive meaningful prior distributions for the bayesian approach and demonstrate how the pareto front might be interpreted probabilistically we first use the multi objective shuffled complex evolution metropolis moscem algorithm vrugt et al 2003a to estimate possible parameter solutions along the pareto front then we calibrate the model using bayesian multi objective inference in different cases finally we use the points on pareto front to construct meaningful priors in the bayesian framework and the results are compared with the pareto front 3 1 catchment and data we selected bloomfield river at china camp 108003a located in daintree basin queensland australia the catchment area is 263km2 with an average annual rainfall of 2170mm and average annual runoff of 1551mm catchment elevation ranges between 151 2m and 1272 5m http www ga gov au elvis and 98 of its area is covered by trees lymburner et al 2011 daily precipitation and pet data are from awap australian water availability project daily stream flow data is from hrs hydrologic reference stations australian bureau of meteorology and 8 day lai data from modis moderate resolution imaging spectroradiometer the lai data are filtered at every pixel 1km resolution using the svitzky golay method using the timesat software jonsson and eklundh 2002 2004 then the catchment averaged values are calculated and used in the calibration simulated green lai is compared with modis lai data set from 2001 to 2005 are selected the maximum lai value is 5 8 leaf area m2 ground area m2 3 2 model in our study a hydrological model hymod boyle 2001 wagener et al 2001 and a bucket grassland model bgm istanbulluoglu et al 2012 are coupled fig 1 hymod is a lumped conceptual rainfall runoff model with six parameters in the model a nonlinear soil moisture tank is connected to two series of tanks in parallel quick flow tanks and a slow flow tank with different residence times bgm is a vertically lumped bucket ecohydrological model which simulates biomass dynamics and is coupled to the soil moisture tank in hymod modifications to the bgm are made based on formulation of zhou et al 2013 to simulate biomass growth in trees we also replaced soil moisture dynamics in bgm with hymod soil moisture tank model inputs are daily precipitation and potential evapotranspiration in the coupled model the exchange terms between the two model components are et and lai soil moisture impacts et by adjusting pet based on the available soil moisture and et controls net primary productivity eq 3 in the appendix and subsequently biomass production eq 5 in the appendix estimated green biomass from the model is converted to lai eq 6 in the appendix which ultimately adjusts the input pet eq 2 in appendix adjusted pet with the intercepted rainfall influence the soil water balance model outputs are daily streamflow and lai estimations all the model parameters and the range values are listed in table 1a seven parameters are being calibrated in the simulations which are marked as calibration in the last column in table 1b model equations are described in the appendix 3 3 bayesian multi objective calibration 3 3 1 multi objective likelihood function in the case study developed here we calibrate our ecohydrologic model on observations of leaf area index lai and streamflow q at different time steps assuming the error terms of each objective are independent then the log multi objective likelihood function can be expressed as l multi l q θ l lai θ the type of error i e homoscedastic or heteroscedastic error was checked before this work using smith et al 2015 s method tools such as residual scatterplot quantile quantile plot had been used to check the likelihood function for each of the objective in our case study the error term of streamflow q is found to be gaussian heteroscedastic and independent based on the test thus the log likelihood is defined by applying the box cox transformation box and cox 1964 l q t θ n 2 log 2 π n 2 log σ q 2 1 2 σ q 2 t 1 n q t f x t θ 2 log q t λ where λ is the transformation parameter fixed at 0 3 based on thyer et al 2002 s work for queensland catchments in australia similarly the error term of lai is gaussian homoscedastic and independent l lai t θ n 2 log 2 π n 2 log σ lai 2 1 2 σ lai 2 t 1 n lai t f x t θ 2 so the log multi objective likelihood becomes l multi n log 2 π n 2 log σ q 2 n 2 log σ lai 2 1 2 σ q 2 t 1 n q t f q x t θ 2 1 2 σ lai 2 t 1 n lai t f lai x t θ 2 log q t λ 3 3 2 prior distributions traditional multi objective optimization to estimate deterministic model parameters requires the modeler to specify a weighting on each objective that reflects the modelers belief in the importance of the objective or the reliability of the observations in the bayesian approach this is implied via the specification of the prior distribution of the error variance parameters in the likelihood the two error parameters σ q 2 and σ lai 2 in the likelihood function reflect the weight of each objective to some extent reichert and schuwirth 2012 minet et al 2015 therefore the prior distributions for these variances should be defined carefully based on the modeler s confidence level in the ecohydrologic observations to see the influence of the prior distributions of error parameters we define all the priors for the ecohydrologic model parameters as uniform distributions within the boundary value shown in table 1a then we define and compare three different types of prior distributions for the error parameters in the multi objective calibration cases 1 multivariate uniform both parameters σ q 2 and σ lai 2 are assumed uniformly distributed 2 multivariate weighted σ q 2 and σ lai 2 are defined as normal distributions with mean of 0 truncated at zero and specified standard deviations a smaller standard deviation is selected when more weight is emphasized on one of the objectives while a larger standard deviation is assigned to the less important objective 3 multivariate pareto σ q 2 and σ lai 2 are defined based on the results of the pareto front estimated using an automatic multi objective optimization in the first prior case we consider the two objectives are effectively equally weighted and we consider no a priori information available regarding our confidence in the catchment observations the second prior case follows previous multi objective work in environmental systems reichert and schuwirth 2012 minet et al 2015 the prior distributions are defined based on the principle that smaller errors are preferred and the difference of the standard deviation of the prior distributions reflect the preferred emphasis on each objective in this case a smaller standard deviation in one prior means more weight on this objective the last prior case is based on the pareto front output from this approach five different points on the pareto front are randomly chosen and we consider this selection reflects the modelers different decisions of the weight of each objective we consider the prior in the last case as the most informative case as it forces increased error on the variables in which the modeler has less confidence regarding the validity of the model observations we use pareto front points to construct priors in bayesian multi objective calibrations with the aim of obtaining similar weights for each objective as the points on the pareto front as a result these points will include probabilistic description of the model uncertainties in a bayesian framework 3 4 calibrations setup all calibration approaches and the methods used are listed in fig 2 for each simulation the first 2years of data were used as a warm up or spin up period to minimize the effect of initial condition assumptions regarding the initial lai and catchment storage for the pareto based multi objective calibration we use moscem vrugt et al 2003a to optimize mean squared error mse of box cox transformed q and mse of lai moscem method is the multi objective expansion of the single objective optimization method scem ua vrugt et al 2003b in the simulation 10 complexes and 100 loops are defined for each of the bayesian multi objective calibrations we employ the adaptive metropolis am algorithm haario et al 2001 for each simulation 100 000 iterations are carried out the first 10 000 iterations are discarded and the remaining 90 000 iterations are used to analyze the posterior distributions convergence is diagnosed via visualizing diagnostic plots of multiple mcmc runs to reduce computational demand marshall et al 2004 to compare with the multi objective calibrations single objective calibrations on lai or streamflow are also simulated using both the sce ua optimization vrugt et al 2003b and the bayesian method 4 results 4 1 single objective optimization firstly single objective calibration of lai and streamflow are performed using the sce ua method duan et al 1994 the predicted and observed lai and streamflow are plotted in fig 3 in each plot the observed red dots and predicted lai or q black and green lines from both single objective calibrations are compared it is obvious that for the variable type being optimized simulations are improved and generally representative of the magnitude of the observations however the variable not being optimized is generally poorly simulated this is particularly true for the case where lai only is optimized in this case the simulated streamflow data are mostly close to zero for the recession period and small storm events can t be captured 4 2 pareto front from multi objective optimization the pareto front shows a clear tradeoff when either objective stream flow or lai is optimized generally reflecting the results of the individual variable optimizations fig 4 from these results five points are selected at specified intervals along the pareto front representing different weighting applied to each variable based on these points five sets of priors for error parameters are defined as normal distributions with means set at these values in the multivariate uniform cases variance of the prior distributions 5 and 0 1 are selected to represent different emphasis on each objective in the multivariate pareto cases mean of the prior distributions are selected as the optimized mse results from the five points on the pareto front previous studies shown that the mean of the prior has more influence on the posterior than the variance of the prior tang et al 2016 so here the variances of these priors are defined according to the order of magnitude of the pareto front values for transformed q and lai 0 02 and 0 03 are selected respectively to keep the prior distributions at the same informative level and these variances remained the same for all these prior distributions all the bayesian simulations with different prior distributions are listed in table 2 the posterior distributions of the model parameters for different cases in bayesian scenario are presented in fig 5 in this figure the first two rows are the cases of single objective calibrations the remaining six rows are the posteriors from the multi objective cases using uniform prior and priors that are constructed based on the pareto front p1 to p5 in all the cases the posterior distributions for five model parameters three hydrology parameters and two vegetation parameters which are more important in the coupled processes are shown in the first five columns and error parameters are shown in the last two columns in the figure it can be seen that the posterior distributions in the lai only case are all relatively diffuse especially for the hydrologic parameters suggesting that the hydrologic part of the model hasn t been well calibrated in the lai only case it also can be seen that the model parameter posterior distributions from all the multi objective cases are more similar to the streamflow only case compared to the lai case except for the distribution function shape parameter b which controls the shape of the catchment soil moisture storage the posterior distributions of b in the multi objective cases are closer to the estimation of the posterior from the lai only case comparing the p1 to p5 cases it is clear that the error parameter posteriors differ significantly depending on the assumed prior distribution in addition it is observed that the hydrologic model parameters don t change significantly among the different cases while vegetation parameters show clear differences 4 3 observed and predicted lai data and stream flow data we compare the predicted lai and streamflow in multivariate weighted prior cases and uniform prior cases in fig 6 the kullback leibler divergence kld between the prior and posterior distributions of error parameters is calculated and shown in table 3 following tang et al 2016 the kld is a non symmetric measure of the difference between two probability distributions variations in the kld results can reflect the influence of the prior distributions on the posterior distributions less variations means less impact of the prior on the posterior distributions and large variations means larger impact of the prior on the posterior comparing the changes of kld among different cases it is clear that the changes of kld between the prior and posterior of error parameter lai are significantly larger than q see the dramatic difference between constrained q and constrained lai cases for kld lai the predicted lai from the constrained streamflow and uniform prior cases are very similar showing similar streamflow and lai magnitudes and dynamics in contrast the constrained lai case differs considerably to the other cases and shows a simulation much closer to the observations predictions of streamflow from the three cases are all similar and as expected the predictions from constrained q case are improved in comparison to the other two cases the comparisons between observed and predicted lai and stream flow data with 90 confidence limit in different cases are shown in figs 7 and 8 confidence limits are calculated based on the variances of residual errors σ 2 so in the single objective calibration cases only the objective being calibrated has confidence limits included summary statistics of the computed confidence limit the reliability and sharpness smith et al 2015 are also included the reliability is defined as the percentage of observations located in the 90 limit and the sharpness is the mean of the width of the confidence limit in each case a good result will mean that 90 of observations are captured by the 90 confidence limit in favor of narrower width comparing the predicted lai in the lai only and multi objective uniform cases in fig 7 the single lai calibration case has much narrower confidence interval mean width 3 0344 than the multi objective case mean width 4 7658 however it is mildly overinflated reliability 92 75 compare to the multi objective uniform case reliability 89 13 in the pareto point based prior cases p1 case has the best predicted lai and p5 has the worst as the p1 point is selected with better smaller lai objective value than p5 on the pareto front fig 4 the differences of these statistics are less obvious for stream flow fig 8 however when comparing the five pareto point cases it is clear that the p1 case has the worst results compare to p5 case 4 4 pareto front vs bayesian outputs the posterior distributions for the error parameters of the p1 to p5 cases are plotted as contours on the pareto front fig 9 the open circles shown here are the points on the pareto front and the five black dots represent the selected points used for defining the prior distributions the five groups of contours show the multivariate distributions of the error parameters from the bayesian outputs agreement between pareto solutions and bayesian outputs can been seen from the figure p2 p3 and p4 points all lie in the contour maps and close to the highest density p1 and p5 points are relatively further away from the highest density but still lie in the multivariate distributions the optimized values of the error parameters in 1 uniform prior 2 constrained q prior and 3 constrained lai prior are also plotted as crosses in fig 9 the result for constrained lai case located within the dominated space results from uniform case and constrained q cases are very similar the optimized lai are much worse than the uniform case but the optimized q are similar in all these cases 5 discussion 5 1 ecohydrological models and bayesian multi objective calibrations our study demonstrates the impact that available data has on the calibration process and the difficulty in predicting catchment variables when only considering biomass or streamflow observations alone fig 3 it is clear that optimized parameter values for the single objective calibration lead to poor simulations for the other variable that is not optimized the poor simulation of lai green line from the streamflow single objective calibration in fig 3 a suggests that the calibrated parameter values from calibrating the hydrological component of the model are unable to represent the lai dynamics therefore a multi objective optimization which simultaneously optimizes both objectives lai and q becomes necessary in an ecohydrological framework however in the meantime more sources of uncertainties are involved in predicted outputs to quantify model uncertainties and possibly further classify different sources of errors via a hierarchical approach a comprehensive bayesian framework is essential the bayesian calibration is useful as it provides a clear analysis of the uncertainties in the model parameters in a traditional calibration based on streamflow observations there is no clear difference between the vegetation and streamflow parameters in terms of uncertainty as all parameters lie well within their prior distributions and boundaries the bayesian multi objective approach becomes more powerful when considering the differences between the parameters for different observation types lai or streamflow the uncertainty in the parameters is highly dependent on the variables that are used to condition the models for example there are very different posterior distributions when streamflow is used to calibrate the model in comparison to lai fig 5 top panels streamflow has more information posteriors are more peaked and narrower even for calibrating the vegetation parameters see for example the parameter wue fig 5 top panels however calibrating on lai alone is still useful for providing informative parameter values as each parameter lies well within its a priori specified boundaries as mentioned in table 1b perhaps with the exception of the parameter alpha this has implications for the calibration of hydrologic models when streamflow observations are not available as it suggests that lai observations may be helpful to reduce uncertainty in the parameters of an ecohydrologic model particularly for streamflow predictions in ungauged catchments 5 2 importance of prior distribution according to the bayesian rule the posterior distribution is the prior distribution being updated given the available data sets generally this means that the prior would have great influence on the posterior when data is limited and with increased data length this impact will decrease this leads to one important issue in the bayesian related studies how much information should the prior have on one hand in order to maintain objectivity it can be suggested that the prior should have little information or minimal impact on the data and as such the non informative prior is introduced jeffreys 1946 box and tiao 1973 bernardo 1979 kass and wasserman 1996 on the other hand when the data is limited or the parameter is insensitive to the data an informative prior based on the experts knowledge or previous case studies should be used bates and campbell 2001 freni and mannina 2010 gharari et al 2014 hrachowitz et al 2014 recent research has considered the importance of the prior distribution as a function of the prior mean variance and the length of data and it is clear in some cases the prior can be strongly influential tang et al 2016 although there is no guidance in hydrologic related disciplines of when and how to specify meaningful priors the study presented here a framework in which to formally incorporate prior information for multi objective studies by selecting different error parameter priors based on the results from the pareto front in a multi objective calibration problem results show that posterior distributions are strongly influenced by assuming different locations for the prior distributions of error parameters fig 5 comparing the posterior distributions among p1 to p5 cases with different prior distributions in fig 5 it can be observed that the hydrologic parameters remained similar among all the cases while the vegetation parameters varied considerably depending on different priors defined in addition the much larger differences of kld for lai error parameter among different cases than q table 3 indicates that the error parameter lai is much more sensitive to the prior than q reasons for this could be either the lai data has too little information or the vegetation parameters are very insensitive to the data therefore a more meaningful and constrained prior should be defined for the vegetation parameters and more emphasis should be allocated on the calibration of lai this concept is similar as limit of acceptability introduced by blazkova and beven 2009 which reflects the confidence level of the data and is defined before the simulations in their work in our work as there are more data used in the simulations the amount of uncertainties for each data are suggested to be defined in the prior based on our knowledge of data another possible reason could be the function of the simple conceptual model structure for the vegetation dynamics further study should be considered on the investigation of the relative contribution of different uncertainty sources i e model structure uncertainty input uncertainty and the application for different ecohydrological models and catchments 5 3 defining appropriate weights for each objective the selection of appropriate weights when calibrating to multiple objectives remains a point of interest in many automatic multi objective calibration studies there are two reasons that the objectives may not be just equally weighted in multi objective calibrations first of all frequently sampling methods for obtaining the data being calibrated are different and thus the uncertainty in the data is not equal for different catchment variables secondly the information content in the data are not the same the concept of disinformation is introduced by beven and westerberg 2011 according to their work the information levels for data from different events are different for multiple input data the variable carries more information will strongly influence the multi objective calibrations therefore the calibration weights which are defined in the prior of residual errors in the bayesian approach should be selected appropriately according to the reliability of each data being calibrated or based on the modeler s preference of the accuracy level for the objectives the predicted lai from constrained q and uniform prior cases were very similar that the calibrations tended to optimize the streamflow rather than lai suggesting that the streamflow data has more information than lai fig 5 similar evidence can be seen figs 7 and 8 in that the predicted streamflow from uniform equal weighted multi objective calibration was similar as the case calibrated on streamflow only to balance the different information content in the data it is suggested that more emphasis should be allocated to the observations which have less information in our study we defined the constrained lai and constrained q prior distributions to compare with the uniform prior case in the multi objective calibration the case using the constrained lai case performed better than the other two cases where the predicted lai was much closer to the observations and the predicted streamflow was not significantly worse than the multi uniform case at the same time see fig 9 optimized estimation for weighted lai weighted q and uniform cases are shown as red green and blue crosses the error prior distributions used in this approach were defined according to reichert and schuwirth 2012 s study in contrast to this we implement here a new approach where we define a meaningful prior for the error parameters in a bayesian multi objective framework that represents the weights for each objective based on the results from the pareto front one of the benefits from the pareto based optimization is that the pareto front perfectly shows the values of each objective given the selected weights and as such can be a good reference in which to define the prior distribution in order to find the preferred weight in a bayesian scenario comparing the computed statistics based on 90 confidence limits of the predicted lai and streamflow from the cases using specified error prior distributions figs 7 and 8 p1 to p5 cases it can be clearly seen that in the p1 case representing the prior with the largest implied emphasis on lai and the least for streamflow the predicted lai was forced to fit the observation and the corresponding sharpness was the smallest while the simulated streamflow showed greater uncertainty sharpness was the largest in contrast the p5 case with least emphasis on lai and most for streamflow shows the narrowest confidence band for streamflow and widest band for lai among all five cases it suggests the fit of data the weights for each objective can be determined in the prior distributions we then compared the bayesian outputs with the pareto front to see the level of agreement on each other 5 4 comparison between pareto based multi objective optimization and bayesian multi objective calibration pareto based multi objective optimization aims to optimize all model objectives at the same time and to find a set of possible solutions that satisfy each of the objectives at an acceptable level in our case the mean square error mse of streamflow and lai were simultaneously minimized fig 4 modelers can select any of the points on the pareto front and the process of the selection reflects the modelers preference or the weights of each objective the outputs are a set of possible mse combinations of each objective and the corresponding model parameter values in contrast to this approach in bayesian multi objective calibrations the model log likelihoods are maximized and the residual errors associated with the model are sampled and updated combing the prior distributions the outputs are probability distributions of model parameters in addition there is no direct definition for the objective weights or user preference in the formal bayesian framework this is implied through the prior distribution of the model and error parameters therefore the results from bayesian multi objective calibration and pareto based optimization are not the same pareto based multi objective optimization techniques have several great points to make them useful it provides good consistency between objectives in terms of parameter optimizations and it allows modelers to choose the results they want to use in a set of optimal solutions it is thus meaningful to combine pareto optimization with formal bayesian calibration for utilizing all the information from pareto solutions and incorporating probabilistic analysis in the bayesian framework the results obtained from the pareto based optimization can be used to construct meaningful priors in the bayesian framework this approach is particular useful in a multi objective calibration because it provides a mechanism by which to define meaningful prior distributions fig 9 compares the bayesian outputs contours and the pareto front circles for the same case study for each case p1 p5 the pareto front point used to define the prior distribution of the error variance lies within the error variance posterior distribution this means that the calibrations are properly informed by the prior distributions representing the weights of each objective which are constructed based on the pareto front the agreement between these results demonstrates that our approach works well for simulating both lai and streamflow outputs using the informative priors defined according to the weights selected on the pareto front in the bayesian multi objective calibration framework in addition the probabilistic solutions from the bayesian outputs summarizes the uncertainties for both objectives making it possible for the further statistical analysis to be implemented 5 5 assessing reliability of model simulations as the accuracy of simulated lai are impacted by the quality of satellite observations we performed several quality checks to validate the modis lai observations first we confirmed that the modis landcover classification for the study catchment over the simulation period 2001 2005 are consistent and distribution of land cover types are consistent with google earth observations second we used the modis lai quality flags images for each day to assess the quality of reported lai approximately 69 of all the lai pixels within the catchment are classified as good quality data for the study period the percentages of the good quality pixels for each day are shown in fig 10 in this plot the red dots are the observations from the days that more than 60 of the pixels have good quality the blue dots are the observations from the days when percentages of good quality pixels are less than 60 the black line is the predicted lai from lai only case it can be seen that the periods of the off tracked parts of simulated lai black line always appear after the periods with bad quality lai data blue dots where less than 60 of the pixels have good quality this result demonstrates that the relatively poor calibration of lai is at least partially due to the quality of lai observations nevertheless we would maintain that the lai observations are captured by the uncertainty bands when lai data are used to condition the model fig 7 the uncertainty framework presented here highlights the potential tradeoffs when using a multi objective approach however a detailed study that incorporates lai uncertainty and assesses its impact on model simulations is inevitable in the future 6 conclusions bayesian multi objective calibrations are compared with a pareto based multi objective optimization using an ecohydrological model streamflow and lai data are calibrated making use of different prior distributions that might reflect the modelers degree of confidence in the system observations posterior distributions and confidence limits are compared for different case studies with different priors for error parameters 1 same uniform priors 2 constrained priors on the preferred objective and 3 priors based on points on the pareto front from our results and analysis we summarize the following conclusions and the possible future work as ecohydrological models usually combine hydrological and ecological components in the model it is recommended to apply multi objective calibration to simultaneously optimize hydrological ecological objectives in a bayesian framework the preferred weights of each objective can be constrained by defining different priors for error parameters however it is hard to derive true priors for each error parameters as these must reflect the modelers a priori understanding of the uncertainty inherent in the model and the catchment observations we have demonstrated here the selection of appropriate priors based on the pareto front as a way to define informative priors and to bring a modelers understanding of classical multi objective optimization to the bayesian framework therefore we recommend the use of this approach when a bayesian multi objective calibration is needed the approach we presented is flexible and can be applied to a wide range of models it can be used in the models when accurate estimation of more than one variable is desired for instance different variables such as et soil moisture and other biomass observations can be selected as objectives for ecohydrological modeling depending on the available data and calibration requirements a bayesian multi objective calibration approach which has three likelihood functions with specified weights for each objective could be implemented combing many objective optimization techniques such as hurford et al 2014 s work future work will be further focused on the impact of different sources of uncertainties such as input uncertainties including defining proper error models and investigating the different impacts of uncertainties on each of objective acknowledgements this research is supported by australian research council arc award ft120100269 to dr marshall data used in this study is available by contacting the authors appendix a supplementary data supplementary data associated with this article can be found in the online version at https doi org 10 1016 j jhydrol 2017 07 040 appendix a supplementary data supplementary data 1 
7538,recent studies have identified the importance of vegetation processes in terrestrial hydrologic systems process based ecohydrological models combine hydrological physical biochemical and ecological processes of the catchments and as such are generally more complex and parametric than conceptual hydrological models thus appropriate calibration objectives and model uncertainty analysis are essential for ecohydrological modeling in recent years bayesian inference has become one of the most popular tools for quantifying the uncertainties in hydrological modeling with the development of markov chain monte carlo mcmc techniques the bayesian approach offers an appealing alternative to traditional multi objective hydrologic model calibrations by defining proper prior distributions that can be considered analogous to the ad hoc weighting often prescribed in multi objective calibration our study aims to develop appropriate prior distributions and likelihood functions that minimize the model uncertainties and bias within a bayesian ecohydrological modeling framework based on a traditional pareto based model calibration technique in our study a pareto based multi objective optimization and a formal bayesian framework are implemented in a conceptual ecohydrological model that combines a hydrological model hymod and a modified bucket grassland model bgm simulations focused on one objective streamflow lai and multiple objectives streamflow and lai with different emphasis defined via the prior distribution of the model error parameters results show more reliable outputs for both predicted streamflow and lai using bayesian multi objective calibration with specified prior distributions for error parameters based on results from the pareto front in the ecohydrological modeling the methodology implemented here provides insight into the usefulness of multiobjective bayesian calibration for ecohydrologic systems and the importance of appropriate prior distributions in such approaches keywords ecohydrology multi objective optimization bayesian inference prior distribution pareto front 1 introduction in recent years the interaction between water resources and ecosystems has become a central topic among scientists because of the important role of vegetation dynamics in the catchment water cycle porporato et al 2002 asbjornsen et al 2011 on one hand water controls vegetation growth photosynthesis respiration and nutrient cycling on the other hand vegetation influences the water cycle through biochemical and biophysical processes such as evapotranspiration and rainfall interception chen et al 2014 ecohydrology integrates catchment hydrological and ecological processes and provides a framework within which the complex interrelationship between vegetation dynamics and water flows can be well investigated asbjornsen et al 2011 the continuous dynamic models that combine conceptual and physical descriptions of hydrological physical and biogeochemical processes such as water heat carbon and energy transmission are called process based ecohydrological models commonly used model structures across scientific research and application include well known models such as swat arnold et al 1993 arnold et al 1998 swim krysanova et al 1998 krysanova et al 2000 rhessys band et al 1993 tague and band 2004 and tethys chloris fatichi et al 2012 conceptual ecohydrological models are also widely used because of their model parsimony which makes it easier for model calibration istanbulluoglu et al 2012 viola et al 2014 due to the development of geographical information systems giss and remote sensing technology studies of ecohydrological models have accelerated however describing the interactions between hydrologic and ecologic systems requires increasing model dimensionality and data for model conditioning as a result appropriate calibration objectives and model uncertainty analysis are essential for ecohydrological modeling ecohydrological model inference is also made more challenging by the need to represent the dynamics of multiple catchment state variables including streamflow and biomass production thus there is a need for approaches to parameter specification that capitalize on all information that may be available to describe catchment scale ecohydrologic processes in recent years multi objective model optimization has emerged as a popular tool across multiple disciplines to derive parameter estimates that reproduce multiple model criteria or types of system observations multi objective model calibrations are a common tool in model optimization of environmental systems these approaches are frequently used when the objectives are under circumstances such that there are in conflicts with each other konak et al 2006 the benefit of multi objective optimization is that it provides a set of possible solutions which satisfies all the objectives in an acceptable range based on the concept of trade offs yapo et al 1998 the output of this approach is a set of solutions which cannot be improved in each of the objectives without deteriorating the other these solutions are usually called non dominated non inferior or pareto solutions efstratiadis and koutsoyiannis 2010 based on this concept many multi objective algorithms have been developed including the multi objective genetic algorithm moga fonseca and fleming 1993 the nondominated sorting genetic algorithm nsga srinivas and deb 1994 the multi objective complex evolution mocom yapo et al 1998 algorithm the multi objective shuffled complex evolution metropolis algorithm moscem vrugt et al 2003a and the multi algorithm genetically adaptive multi objective amalgam vrugt and robinson 2007 method note that individual objectives may represent different statistical summaries of a set of observations or observations of different variables within the system being studied in hydrological studies one example may be when considering the trade offs of the parameterization between high flows and low flows madsen 2000 shafii and smedt 2009 in ecohydrological modeling where typically an ecological submodel is combined with a hydrological component the optimization of the objective describing the ecological component may not be acceptable for the hydrological component and vice versa the benefits of using a multi objective approach in an ecohydrological model where we consider the interrelationship between the hydrologic cycle and vegetation dynamics are obvious as it provides a formal way in which to simultaneously and comprehensively consider both hydrological and ecological components of the model therefore multi objective optimization approaches are well suited to ecohydrological models for instance naseem et al 2015 presented a work of multi response optimization for two conceptual ecohydrological models across 27 catchments in australia results showed that the multi objective optimization provided a better representation of the two response outputs streamflow and leaf area index lai and has the potential to greatly improve ecohydrological modeling and applications while these multi objective optimization approaches are useful for estimating deterministic parameter values that provide appropriate fits to competing calibration objectives pareto based multi objective calibration does not provide probabilistic solutions for the model parameters or any uncertainty analysis of the corresponding model predictions reichert and schuwirth 2012 with the development of markov chain monte carlo mcmc methods and advances in computing power bayesian inference has emerged asa popular tool for uncertainty analysis in hydrological modeling e g bates and campbell 2001 kuczera 1983 marshall et al 2004 smith and marshall 2008 vrugt et al 2003b bayesian inference provides a framework within which prior information and the data can be combined it allows the statistical quantification of model uncertainties compared with purely conceptual rainfall runoff models it can be argued that ecohydrological models may be more susceptible to uncertainty one reason is that process based models usually contain physically based mathematical descriptions as well as conceptual components the complexity of model leads to possibly large model uncertainty arnold et al 2009 another reason is that often in ecohydrologic modeling different sources and sampling methods are used to collect catchment data leading to multiple potential sources of observational uncertainty that influence model predictions and reliability for example in our case leaf area index lai data at an 8 day 1km resolution via modis moderate resolution imaging spectroradiometer are used for calibration of an ecohydrologic model to capture vegetation dynamics compared with daily stream flow data this lai data likely contains more uncertainties or less information because of the coarse resolution and the error from the model that lai is derived from in lumped conceptual ecohydrological models another sources of uncertainty in lai are related to the aggregation of lai pixels inside a catchment to obtain mean catchment scale lai therefore the potential errors associated with lai and streamflow observations would not be the same that is the two objective functions should not be equally weighted in an optimization problem it is thus important to appropriately define the objective functions weights in a way that reflect the confidence level of observations more recently multi objective bayesian inference has been proposed as an alternative to traditional pareto based multi objective optimizations minet et al 2015 sikorska et al 2015 reichert and schuwirth 2012 these approaches are appealing as they allow model conditioning with multiple data sources and provide a framework in which prior information can be used as a surrogate for traditional weighting of different model objectives in this study we implement a multi objective bayesian calibration for a conceptual ecohydrologic model and demonstrate the usefulness of such an approach for inferring multiple catchment variables we aim to develop appropriate prior distributions and likelihood functions that minimize model uncertainties within the framework based on information obtained from pareto based multi objective optimization the overall goal of this study is to combine traditional multi objective calibration of ecohydrologic models with more recent bayesian approaches and demonstrate how an understanding of the pareto front can help inform bayesian calibrations 2 bayesian multi objective inference multi objective optimization aims to provide a set of possible solutions in front of the modelers however it doesn t provide probabilistic solutions in our study we aim to find an alternative for pareto based multi objective optimization for an ecohydrologic model a bayesian multi objective framework is presented considering different emphasis via the prior distribution on each objective reichert and schuwirth 2012 introduced a method combining prior knowledge and multi objective calibration the preferred weight of each objective can be defined in the prior of error parameters the prior of error parameters are defined as zero mean and favoring narrow distribution over wider distribution for one objective when the modeler wants to put more weight on it such an approach holds promise across many modeling studies that attempt to infer multiple catchment or environmental variables recently sikorska et al 2015 showed a comparison between bayesian single and equal weighted multi objective calibrations in a combination of a rainfall runoff model and a total suspended solids tss model minet et al 2015 presented a case study of bayesian multi objective inversion in a dynamic vegetation model with different types of likelihood functions and prior information of the error parameters these studies indicate the potential importance and application of multi objective calibration in the ecohydrological modeling a conceptual ecohydrological model can be expressed as o t f x t θ ε t where o t are the observations for a catchment at time t x t is model inputs at time t θ is the model parameter set f x t θ is the corresponding model output for time t and ε t is an error term in model specification via bayesian inference the posterior distributions of model parameters are estimated via bayes theorem p θ o p θ p o θ p o where p θ is the prior distribution which summarized the existing knowledge of the model parameters through parametric probability distributions p o θ is the likelihood function which summarizes the model for the data given parameters p o is a proportionality constant the posterior distribution p θ o thus summarizes the parameter uncertainty after observing the data if the error term is assumed to be normally distributed uncorrelated with constant variance σ 2 the log likelihood function can be written as l o t θ n 2 log 2 π n 2 log σ 2 1 2 σ 2 t 1 n o t f x t θ 2 ecohydrologic models simulate multiple variables that describe water energy and biophysical fluxes such as streamflow leaf area index lai net primary production and evapotranspiration as such the function presented here is extended so that o is a vector of observations at each time step for a multivariate case considering the error term follows multivariate gaussian distribution x n μ then the log likelihood function can be expressed as l o t θ n 2 log 2 π n 2 log 1 2 t 1 n o t f x t θ t 1 o t f x t θ parameters being calibrated include model parameters and error parameters σ1 2 and σ2 2 for each of the objectives the corresponding prior can be expresses as p θ p σ1 2 and p σ2 2 hence the log posterior distribution can be expressed as lp θ o t l o t θ lp θ lp σ 1 2 lp σ 2 2 3 methodology in our case studies we aim to take advantage of traditional multi objective calibrations to derive meaningful prior distributions for the bayesian approach and demonstrate how the pareto front might be interpreted probabilistically we first use the multi objective shuffled complex evolution metropolis moscem algorithm vrugt et al 2003a to estimate possible parameter solutions along the pareto front then we calibrate the model using bayesian multi objective inference in different cases finally we use the points on pareto front to construct meaningful priors in the bayesian framework and the results are compared with the pareto front 3 1 catchment and data we selected bloomfield river at china camp 108003a located in daintree basin queensland australia the catchment area is 263km2 with an average annual rainfall of 2170mm and average annual runoff of 1551mm catchment elevation ranges between 151 2m and 1272 5m http www ga gov au elvis and 98 of its area is covered by trees lymburner et al 2011 daily precipitation and pet data are from awap australian water availability project daily stream flow data is from hrs hydrologic reference stations australian bureau of meteorology and 8 day lai data from modis moderate resolution imaging spectroradiometer the lai data are filtered at every pixel 1km resolution using the svitzky golay method using the timesat software jonsson and eklundh 2002 2004 then the catchment averaged values are calculated and used in the calibration simulated green lai is compared with modis lai data set from 2001 to 2005 are selected the maximum lai value is 5 8 leaf area m2 ground area m2 3 2 model in our study a hydrological model hymod boyle 2001 wagener et al 2001 and a bucket grassland model bgm istanbulluoglu et al 2012 are coupled fig 1 hymod is a lumped conceptual rainfall runoff model with six parameters in the model a nonlinear soil moisture tank is connected to two series of tanks in parallel quick flow tanks and a slow flow tank with different residence times bgm is a vertically lumped bucket ecohydrological model which simulates biomass dynamics and is coupled to the soil moisture tank in hymod modifications to the bgm are made based on formulation of zhou et al 2013 to simulate biomass growth in trees we also replaced soil moisture dynamics in bgm with hymod soil moisture tank model inputs are daily precipitation and potential evapotranspiration in the coupled model the exchange terms between the two model components are et and lai soil moisture impacts et by adjusting pet based on the available soil moisture and et controls net primary productivity eq 3 in the appendix and subsequently biomass production eq 5 in the appendix estimated green biomass from the model is converted to lai eq 6 in the appendix which ultimately adjusts the input pet eq 2 in appendix adjusted pet with the intercepted rainfall influence the soil water balance model outputs are daily streamflow and lai estimations all the model parameters and the range values are listed in table 1a seven parameters are being calibrated in the simulations which are marked as calibration in the last column in table 1b model equations are described in the appendix 3 3 bayesian multi objective calibration 3 3 1 multi objective likelihood function in the case study developed here we calibrate our ecohydrologic model on observations of leaf area index lai and streamflow q at different time steps assuming the error terms of each objective are independent then the log multi objective likelihood function can be expressed as l multi l q θ l lai θ the type of error i e homoscedastic or heteroscedastic error was checked before this work using smith et al 2015 s method tools such as residual scatterplot quantile quantile plot had been used to check the likelihood function for each of the objective in our case study the error term of streamflow q is found to be gaussian heteroscedastic and independent based on the test thus the log likelihood is defined by applying the box cox transformation box and cox 1964 l q t θ n 2 log 2 π n 2 log σ q 2 1 2 σ q 2 t 1 n q t f x t θ 2 log q t λ where λ is the transformation parameter fixed at 0 3 based on thyer et al 2002 s work for queensland catchments in australia similarly the error term of lai is gaussian homoscedastic and independent l lai t θ n 2 log 2 π n 2 log σ lai 2 1 2 σ lai 2 t 1 n lai t f x t θ 2 so the log multi objective likelihood becomes l multi n log 2 π n 2 log σ q 2 n 2 log σ lai 2 1 2 σ q 2 t 1 n q t f q x t θ 2 1 2 σ lai 2 t 1 n lai t f lai x t θ 2 log q t λ 3 3 2 prior distributions traditional multi objective optimization to estimate deterministic model parameters requires the modeler to specify a weighting on each objective that reflects the modelers belief in the importance of the objective or the reliability of the observations in the bayesian approach this is implied via the specification of the prior distribution of the error variance parameters in the likelihood the two error parameters σ q 2 and σ lai 2 in the likelihood function reflect the weight of each objective to some extent reichert and schuwirth 2012 minet et al 2015 therefore the prior distributions for these variances should be defined carefully based on the modeler s confidence level in the ecohydrologic observations to see the influence of the prior distributions of error parameters we define all the priors for the ecohydrologic model parameters as uniform distributions within the boundary value shown in table 1a then we define and compare three different types of prior distributions for the error parameters in the multi objective calibration cases 1 multivariate uniform both parameters σ q 2 and σ lai 2 are assumed uniformly distributed 2 multivariate weighted σ q 2 and σ lai 2 are defined as normal distributions with mean of 0 truncated at zero and specified standard deviations a smaller standard deviation is selected when more weight is emphasized on one of the objectives while a larger standard deviation is assigned to the less important objective 3 multivariate pareto σ q 2 and σ lai 2 are defined based on the results of the pareto front estimated using an automatic multi objective optimization in the first prior case we consider the two objectives are effectively equally weighted and we consider no a priori information available regarding our confidence in the catchment observations the second prior case follows previous multi objective work in environmental systems reichert and schuwirth 2012 minet et al 2015 the prior distributions are defined based on the principle that smaller errors are preferred and the difference of the standard deviation of the prior distributions reflect the preferred emphasis on each objective in this case a smaller standard deviation in one prior means more weight on this objective the last prior case is based on the pareto front output from this approach five different points on the pareto front are randomly chosen and we consider this selection reflects the modelers different decisions of the weight of each objective we consider the prior in the last case as the most informative case as it forces increased error on the variables in which the modeler has less confidence regarding the validity of the model observations we use pareto front points to construct priors in bayesian multi objective calibrations with the aim of obtaining similar weights for each objective as the points on the pareto front as a result these points will include probabilistic description of the model uncertainties in a bayesian framework 3 4 calibrations setup all calibration approaches and the methods used are listed in fig 2 for each simulation the first 2years of data were used as a warm up or spin up period to minimize the effect of initial condition assumptions regarding the initial lai and catchment storage for the pareto based multi objective calibration we use moscem vrugt et al 2003a to optimize mean squared error mse of box cox transformed q and mse of lai moscem method is the multi objective expansion of the single objective optimization method scem ua vrugt et al 2003b in the simulation 10 complexes and 100 loops are defined for each of the bayesian multi objective calibrations we employ the adaptive metropolis am algorithm haario et al 2001 for each simulation 100 000 iterations are carried out the first 10 000 iterations are discarded and the remaining 90 000 iterations are used to analyze the posterior distributions convergence is diagnosed via visualizing diagnostic plots of multiple mcmc runs to reduce computational demand marshall et al 2004 to compare with the multi objective calibrations single objective calibrations on lai or streamflow are also simulated using both the sce ua optimization vrugt et al 2003b and the bayesian method 4 results 4 1 single objective optimization firstly single objective calibration of lai and streamflow are performed using the sce ua method duan et al 1994 the predicted and observed lai and streamflow are plotted in fig 3 in each plot the observed red dots and predicted lai or q black and green lines from both single objective calibrations are compared it is obvious that for the variable type being optimized simulations are improved and generally representative of the magnitude of the observations however the variable not being optimized is generally poorly simulated this is particularly true for the case where lai only is optimized in this case the simulated streamflow data are mostly close to zero for the recession period and small storm events can t be captured 4 2 pareto front from multi objective optimization the pareto front shows a clear tradeoff when either objective stream flow or lai is optimized generally reflecting the results of the individual variable optimizations fig 4 from these results five points are selected at specified intervals along the pareto front representing different weighting applied to each variable based on these points five sets of priors for error parameters are defined as normal distributions with means set at these values in the multivariate uniform cases variance of the prior distributions 5 and 0 1 are selected to represent different emphasis on each objective in the multivariate pareto cases mean of the prior distributions are selected as the optimized mse results from the five points on the pareto front previous studies shown that the mean of the prior has more influence on the posterior than the variance of the prior tang et al 2016 so here the variances of these priors are defined according to the order of magnitude of the pareto front values for transformed q and lai 0 02 and 0 03 are selected respectively to keep the prior distributions at the same informative level and these variances remained the same for all these prior distributions all the bayesian simulations with different prior distributions are listed in table 2 the posterior distributions of the model parameters for different cases in bayesian scenario are presented in fig 5 in this figure the first two rows are the cases of single objective calibrations the remaining six rows are the posteriors from the multi objective cases using uniform prior and priors that are constructed based on the pareto front p1 to p5 in all the cases the posterior distributions for five model parameters three hydrology parameters and two vegetation parameters which are more important in the coupled processes are shown in the first five columns and error parameters are shown in the last two columns in the figure it can be seen that the posterior distributions in the lai only case are all relatively diffuse especially for the hydrologic parameters suggesting that the hydrologic part of the model hasn t been well calibrated in the lai only case it also can be seen that the model parameter posterior distributions from all the multi objective cases are more similar to the streamflow only case compared to the lai case except for the distribution function shape parameter b which controls the shape of the catchment soil moisture storage the posterior distributions of b in the multi objective cases are closer to the estimation of the posterior from the lai only case comparing the p1 to p5 cases it is clear that the error parameter posteriors differ significantly depending on the assumed prior distribution in addition it is observed that the hydrologic model parameters don t change significantly among the different cases while vegetation parameters show clear differences 4 3 observed and predicted lai data and stream flow data we compare the predicted lai and streamflow in multivariate weighted prior cases and uniform prior cases in fig 6 the kullback leibler divergence kld between the prior and posterior distributions of error parameters is calculated and shown in table 3 following tang et al 2016 the kld is a non symmetric measure of the difference between two probability distributions variations in the kld results can reflect the influence of the prior distributions on the posterior distributions less variations means less impact of the prior on the posterior distributions and large variations means larger impact of the prior on the posterior comparing the changes of kld among different cases it is clear that the changes of kld between the prior and posterior of error parameter lai are significantly larger than q see the dramatic difference between constrained q and constrained lai cases for kld lai the predicted lai from the constrained streamflow and uniform prior cases are very similar showing similar streamflow and lai magnitudes and dynamics in contrast the constrained lai case differs considerably to the other cases and shows a simulation much closer to the observations predictions of streamflow from the three cases are all similar and as expected the predictions from constrained q case are improved in comparison to the other two cases the comparisons between observed and predicted lai and stream flow data with 90 confidence limit in different cases are shown in figs 7 and 8 confidence limits are calculated based on the variances of residual errors σ 2 so in the single objective calibration cases only the objective being calibrated has confidence limits included summary statistics of the computed confidence limit the reliability and sharpness smith et al 2015 are also included the reliability is defined as the percentage of observations located in the 90 limit and the sharpness is the mean of the width of the confidence limit in each case a good result will mean that 90 of observations are captured by the 90 confidence limit in favor of narrower width comparing the predicted lai in the lai only and multi objective uniform cases in fig 7 the single lai calibration case has much narrower confidence interval mean width 3 0344 than the multi objective case mean width 4 7658 however it is mildly overinflated reliability 92 75 compare to the multi objective uniform case reliability 89 13 in the pareto point based prior cases p1 case has the best predicted lai and p5 has the worst as the p1 point is selected with better smaller lai objective value than p5 on the pareto front fig 4 the differences of these statistics are less obvious for stream flow fig 8 however when comparing the five pareto point cases it is clear that the p1 case has the worst results compare to p5 case 4 4 pareto front vs bayesian outputs the posterior distributions for the error parameters of the p1 to p5 cases are plotted as contours on the pareto front fig 9 the open circles shown here are the points on the pareto front and the five black dots represent the selected points used for defining the prior distributions the five groups of contours show the multivariate distributions of the error parameters from the bayesian outputs agreement between pareto solutions and bayesian outputs can been seen from the figure p2 p3 and p4 points all lie in the contour maps and close to the highest density p1 and p5 points are relatively further away from the highest density but still lie in the multivariate distributions the optimized values of the error parameters in 1 uniform prior 2 constrained q prior and 3 constrained lai prior are also plotted as crosses in fig 9 the result for constrained lai case located within the dominated space results from uniform case and constrained q cases are very similar the optimized lai are much worse than the uniform case but the optimized q are similar in all these cases 5 discussion 5 1 ecohydrological models and bayesian multi objective calibrations our study demonstrates the impact that available data has on the calibration process and the difficulty in predicting catchment variables when only considering biomass or streamflow observations alone fig 3 it is clear that optimized parameter values for the single objective calibration lead to poor simulations for the other variable that is not optimized the poor simulation of lai green line from the streamflow single objective calibration in fig 3 a suggests that the calibrated parameter values from calibrating the hydrological component of the model are unable to represent the lai dynamics therefore a multi objective optimization which simultaneously optimizes both objectives lai and q becomes necessary in an ecohydrological framework however in the meantime more sources of uncertainties are involved in predicted outputs to quantify model uncertainties and possibly further classify different sources of errors via a hierarchical approach a comprehensive bayesian framework is essential the bayesian calibration is useful as it provides a clear analysis of the uncertainties in the model parameters in a traditional calibration based on streamflow observations there is no clear difference between the vegetation and streamflow parameters in terms of uncertainty as all parameters lie well within their prior distributions and boundaries the bayesian multi objective approach becomes more powerful when considering the differences between the parameters for different observation types lai or streamflow the uncertainty in the parameters is highly dependent on the variables that are used to condition the models for example there are very different posterior distributions when streamflow is used to calibrate the model in comparison to lai fig 5 top panels streamflow has more information posteriors are more peaked and narrower even for calibrating the vegetation parameters see for example the parameter wue fig 5 top panels however calibrating on lai alone is still useful for providing informative parameter values as each parameter lies well within its a priori specified boundaries as mentioned in table 1b perhaps with the exception of the parameter alpha this has implications for the calibration of hydrologic models when streamflow observations are not available as it suggests that lai observations may be helpful to reduce uncertainty in the parameters of an ecohydrologic model particularly for streamflow predictions in ungauged catchments 5 2 importance of prior distribution according to the bayesian rule the posterior distribution is the prior distribution being updated given the available data sets generally this means that the prior would have great influence on the posterior when data is limited and with increased data length this impact will decrease this leads to one important issue in the bayesian related studies how much information should the prior have on one hand in order to maintain objectivity it can be suggested that the prior should have little information or minimal impact on the data and as such the non informative prior is introduced jeffreys 1946 box and tiao 1973 bernardo 1979 kass and wasserman 1996 on the other hand when the data is limited or the parameter is insensitive to the data an informative prior based on the experts knowledge or previous case studies should be used bates and campbell 2001 freni and mannina 2010 gharari et al 2014 hrachowitz et al 2014 recent research has considered the importance of the prior distribution as a function of the prior mean variance and the length of data and it is clear in some cases the prior can be strongly influential tang et al 2016 although there is no guidance in hydrologic related disciplines of when and how to specify meaningful priors the study presented here a framework in which to formally incorporate prior information for multi objective studies by selecting different error parameter priors based on the results from the pareto front in a multi objective calibration problem results show that posterior distributions are strongly influenced by assuming different locations for the prior distributions of error parameters fig 5 comparing the posterior distributions among p1 to p5 cases with different prior distributions in fig 5 it can be observed that the hydrologic parameters remained similar among all the cases while the vegetation parameters varied considerably depending on different priors defined in addition the much larger differences of kld for lai error parameter among different cases than q table 3 indicates that the error parameter lai is much more sensitive to the prior than q reasons for this could be either the lai data has too little information or the vegetation parameters are very insensitive to the data therefore a more meaningful and constrained prior should be defined for the vegetation parameters and more emphasis should be allocated on the calibration of lai this concept is similar as limit of acceptability introduced by blazkova and beven 2009 which reflects the confidence level of the data and is defined before the simulations in their work in our work as there are more data used in the simulations the amount of uncertainties for each data are suggested to be defined in the prior based on our knowledge of data another possible reason could be the function of the simple conceptual model structure for the vegetation dynamics further study should be considered on the investigation of the relative contribution of different uncertainty sources i e model structure uncertainty input uncertainty and the application for different ecohydrological models and catchments 5 3 defining appropriate weights for each objective the selection of appropriate weights when calibrating to multiple objectives remains a point of interest in many automatic multi objective calibration studies there are two reasons that the objectives may not be just equally weighted in multi objective calibrations first of all frequently sampling methods for obtaining the data being calibrated are different and thus the uncertainty in the data is not equal for different catchment variables secondly the information content in the data are not the same the concept of disinformation is introduced by beven and westerberg 2011 according to their work the information levels for data from different events are different for multiple input data the variable carries more information will strongly influence the multi objective calibrations therefore the calibration weights which are defined in the prior of residual errors in the bayesian approach should be selected appropriately according to the reliability of each data being calibrated or based on the modeler s preference of the accuracy level for the objectives the predicted lai from constrained q and uniform prior cases were very similar that the calibrations tended to optimize the streamflow rather than lai suggesting that the streamflow data has more information than lai fig 5 similar evidence can be seen figs 7 and 8 in that the predicted streamflow from uniform equal weighted multi objective calibration was similar as the case calibrated on streamflow only to balance the different information content in the data it is suggested that more emphasis should be allocated to the observations which have less information in our study we defined the constrained lai and constrained q prior distributions to compare with the uniform prior case in the multi objective calibration the case using the constrained lai case performed better than the other two cases where the predicted lai was much closer to the observations and the predicted streamflow was not significantly worse than the multi uniform case at the same time see fig 9 optimized estimation for weighted lai weighted q and uniform cases are shown as red green and blue crosses the error prior distributions used in this approach were defined according to reichert and schuwirth 2012 s study in contrast to this we implement here a new approach where we define a meaningful prior for the error parameters in a bayesian multi objective framework that represents the weights for each objective based on the results from the pareto front one of the benefits from the pareto based optimization is that the pareto front perfectly shows the values of each objective given the selected weights and as such can be a good reference in which to define the prior distribution in order to find the preferred weight in a bayesian scenario comparing the computed statistics based on 90 confidence limits of the predicted lai and streamflow from the cases using specified error prior distributions figs 7 and 8 p1 to p5 cases it can be clearly seen that in the p1 case representing the prior with the largest implied emphasis on lai and the least for streamflow the predicted lai was forced to fit the observation and the corresponding sharpness was the smallest while the simulated streamflow showed greater uncertainty sharpness was the largest in contrast the p5 case with least emphasis on lai and most for streamflow shows the narrowest confidence band for streamflow and widest band for lai among all five cases it suggests the fit of data the weights for each objective can be determined in the prior distributions we then compared the bayesian outputs with the pareto front to see the level of agreement on each other 5 4 comparison between pareto based multi objective optimization and bayesian multi objective calibration pareto based multi objective optimization aims to optimize all model objectives at the same time and to find a set of possible solutions that satisfy each of the objectives at an acceptable level in our case the mean square error mse of streamflow and lai were simultaneously minimized fig 4 modelers can select any of the points on the pareto front and the process of the selection reflects the modelers preference or the weights of each objective the outputs are a set of possible mse combinations of each objective and the corresponding model parameter values in contrast to this approach in bayesian multi objective calibrations the model log likelihoods are maximized and the residual errors associated with the model are sampled and updated combing the prior distributions the outputs are probability distributions of model parameters in addition there is no direct definition for the objective weights or user preference in the formal bayesian framework this is implied through the prior distribution of the model and error parameters therefore the results from bayesian multi objective calibration and pareto based optimization are not the same pareto based multi objective optimization techniques have several great points to make them useful it provides good consistency between objectives in terms of parameter optimizations and it allows modelers to choose the results they want to use in a set of optimal solutions it is thus meaningful to combine pareto optimization with formal bayesian calibration for utilizing all the information from pareto solutions and incorporating probabilistic analysis in the bayesian framework the results obtained from the pareto based optimization can be used to construct meaningful priors in the bayesian framework this approach is particular useful in a multi objective calibration because it provides a mechanism by which to define meaningful prior distributions fig 9 compares the bayesian outputs contours and the pareto front circles for the same case study for each case p1 p5 the pareto front point used to define the prior distribution of the error variance lies within the error variance posterior distribution this means that the calibrations are properly informed by the prior distributions representing the weights of each objective which are constructed based on the pareto front the agreement between these results demonstrates that our approach works well for simulating both lai and streamflow outputs using the informative priors defined according to the weights selected on the pareto front in the bayesian multi objective calibration framework in addition the probabilistic solutions from the bayesian outputs summarizes the uncertainties for both objectives making it possible for the further statistical analysis to be implemented 5 5 assessing reliability of model simulations as the accuracy of simulated lai are impacted by the quality of satellite observations we performed several quality checks to validate the modis lai observations first we confirmed that the modis landcover classification for the study catchment over the simulation period 2001 2005 are consistent and distribution of land cover types are consistent with google earth observations second we used the modis lai quality flags images for each day to assess the quality of reported lai approximately 69 of all the lai pixels within the catchment are classified as good quality data for the study period the percentages of the good quality pixels for each day are shown in fig 10 in this plot the red dots are the observations from the days that more than 60 of the pixels have good quality the blue dots are the observations from the days when percentages of good quality pixels are less than 60 the black line is the predicted lai from lai only case it can be seen that the periods of the off tracked parts of simulated lai black line always appear after the periods with bad quality lai data blue dots where less than 60 of the pixels have good quality this result demonstrates that the relatively poor calibration of lai is at least partially due to the quality of lai observations nevertheless we would maintain that the lai observations are captured by the uncertainty bands when lai data are used to condition the model fig 7 the uncertainty framework presented here highlights the potential tradeoffs when using a multi objective approach however a detailed study that incorporates lai uncertainty and assesses its impact on model simulations is inevitable in the future 6 conclusions bayesian multi objective calibrations are compared with a pareto based multi objective optimization using an ecohydrological model streamflow and lai data are calibrated making use of different prior distributions that might reflect the modelers degree of confidence in the system observations posterior distributions and confidence limits are compared for different case studies with different priors for error parameters 1 same uniform priors 2 constrained priors on the preferred objective and 3 priors based on points on the pareto front from our results and analysis we summarize the following conclusions and the possible future work as ecohydrological models usually combine hydrological and ecological components in the model it is recommended to apply multi objective calibration to simultaneously optimize hydrological ecological objectives in a bayesian framework the preferred weights of each objective can be constrained by defining different priors for error parameters however it is hard to derive true priors for each error parameters as these must reflect the modelers a priori understanding of the uncertainty inherent in the model and the catchment observations we have demonstrated here the selection of appropriate priors based on the pareto front as a way to define informative priors and to bring a modelers understanding of classical multi objective optimization to the bayesian framework therefore we recommend the use of this approach when a bayesian multi objective calibration is needed the approach we presented is flexible and can be applied to a wide range of models it can be used in the models when accurate estimation of more than one variable is desired for instance different variables such as et soil moisture and other biomass observations can be selected as objectives for ecohydrological modeling depending on the available data and calibration requirements a bayesian multi objective calibration approach which has three likelihood functions with specified weights for each objective could be implemented combing many objective optimization techniques such as hurford et al 2014 s work future work will be further focused on the impact of different sources of uncertainties such as input uncertainties including defining proper error models and investigating the different impacts of uncertainties on each of objective acknowledgements this research is supported by australian research council arc award ft120100269 to dr marshall data used in this study is available by contacting the authors appendix a supplementary data supplementary data associated with this article can be found in the online version at https doi org 10 1016 j jhydrol 2017 07 040 appendix a supplementary data supplementary data 1 
7539,as the global demands for the use of freshwater resources continues to rise it has become increasingly important to insure the sustainability of this resources this is accomplished through the use of management strategies that often utilize monitoring and the use of hydrological models however monitoring at large scales is not feasible and therefore model applications are becoming challenging especially when spatially distributed datasets such as evapotranspiration are needed to understand the model performances due to these limitations most of the hydrological models are only calibrated for data obtained from site point observations such as streamflow therefore the main focus of this paper is to examine whether the incorporation of remotely sensed and spatially distributed datasets can improve the overall performance of the model in this study actual evapotranspiration eta data was obtained from the two different sets of satellite based remote sensing data one dataset estimates eta based on the simplified surface energy balance ssebop model while the other one estimates eta based on the atmosphere land exchange inverse alexi model the hydrological model used in this study is the soil and water assessment tool swat which was calibrated against spatially distributed eta and single point streamflow records for the honeyoey creek pine creek watershed located in michigan usa two different techniques multi variable and genetic algorithm were used to calibrate the swat model using the aforementioned datasets the performance of the hydrological model in estimating eta was improved using both calibration techniques by achieving nash sutcliffe efficiency nse values 0 5 0 73 0 85 percent bias pbias values within 25 21 73 and root mean squared error observations standard deviation ratio rsr values 0 7 0 39 0 52 however the genetic algorithm technique was more effective with the eta calibration while significantly reducing the model performance for estimating the streamflow nse 0 32 0 52 pbias 32 73 and rsr 0 63 0 82 meanwhile using the multi variable technique the model performance for estimating the streamflow was maintained with a high level of accuracy nse 0 59 0 61 pbias 13 70 and rsr 0 63 0 64 while the evapotranspiration estimations were improved results from this assessment shows that incorporation of remotely sensed and spatially distributed data can improve the hydrological model performance if it is coupled with a right calibration technique abbreviations alexi atmosphere land exchange inverse alpha bf baseflow recession constant biomix biological mixing efficiency canmx maximum canopy storage canmx maximum canopy storage ch k2 effective hydraulic conductivity of channel ch n2 manning s n value for the main channel cn2 moisture condition ii curve number co2 carbon dioxide concentration epa environmental protection agency epco plant uptake compensation factor esco soil evaporation compensation coefficient et evapotranspiration eta actual evapotranspiration frgmax fraction of maximum stomatal conductance corresponding to the second point on the stomatal conductance curve ga genetic algorithm goes geostationary operational environmental satellites gsi maximum stomatal conductance gw delay delay time for aquifer recharge gw reap revap coefficient gwqmn threshold water level in shallow aquifer for base flow ipet potential evapotranspiration method max temp daily maximum temperature min temp daily minimum temperature modis moderate resolution imaging spectroradiometer nasa national aeronautics and space administration nass national agricultural statistics service ncdc national climatic data center ned national elevation dataset nhdplus national hydrology dataset plus nrcs natural resources conservation service nse nash sutcliffe efficiency nsga ii nondominated sorted genetic algorithm ii of objective function pbias percent bias rchrg dp aquifer percolation coefficient revapmn threshold water level in shallow aquifer for revap rsme root mean squared error rsr root mean squared error observations standard deviation ratio sol awc available water capacity ssebop simplified surface energy balance surlag surface runoff lag coefficient swat soil and water assessment tool usda united states department of agriculture usgs united states geological survey vpdfr vapor pressure deficit corresponding to the fraction given by frgmax wnd sp daily wind speed keywords evapotranspiration streamflow monte carlo simulation nsga ii ssebop alexi 1 introduction as extreme climate conditions and anthropogenic activities continue to impact environmental systems mitigation and restoration related projects have become common furthermore environmental systems such as watersheds are very complex with many relationships and interlocking processes sivakumar and singh 2012 guerrero et al 2013 therefore it can be challenging to determine which management solution s should be selected and implemented herman et al 2015 sabbaghian et al 2016 this has led to the development of many different modeling techniques that can simulate a variety of options and identify the best solution s based on the criteria put forth mostly by stakeholders and policy makers chen et al 2012 beven and smith 2014 giri et al 2016 meanwhile the first step in a model implementation is parameter calibration parameter calibration in model applications is used to adjust model performance to better simulate the natural systems they are trying to describe guerrero et al 2013 zhan et al 2013 rajib et al 2016 while parameter calibration improves the ability of models to more accurately represent natural systems models performances are still limited by the quality and quantity of input data and their availabilities nejadhashemi et al 2011 today most hydrological studies rely on data collected at monitoring stations across the world in fact the united states geological survey usgs has about 1 5 million monitoring sites from which data can be obtained usgs 2016a however even with the existence of all these monitoring sites there are times where higher spatial resolutions are needed by researchers stakeholders and policy makers to more precisely evaluate the hydrologic conditions and to determine the best place to implement mitigation and restoration projects wanders et al 2014 one way to address this issue is the use of remotely sensed data remote sensing is defined as the science of identifying observing and measuring an object without physical contact graham 1999 with the advancements in satellite technology remotely sensed satellite data has become a source of consistent monitoring for the entire globe with applications ranging from crop yields to water resources assessments graham 1999 long et al 2014 in order to model water resources more accurately it is important to examining different components of the hydrologic cycle including water movement processes e g evaporation and streamflow and water storage e g soil moisture water vapor groundwater and surface water bodies while hydrological models simulate all components of the hydrological cycle streamflow is often the only component that the model outputs are compared against during the calibration process since it can be measured more accurately than the other components immerzeel and droogers 2008 wanders et al 2014 rajib et al 2016 this can result in poor simulations of other hydrologic components which ultimately lowers the model performance wanders et al 2014 rajib et al 2016 therefore including additional hydrological components in the parameter calibration process could allow the model to better represent all process occurring in the environment crow et al 2003 in particular evapotranspiration et could be considered an important hydrological component added to the calibration process since it describes the moisture lost to the atmosphere from both biotic e g plants and abiotic e g soils sources hanson 1991 usgs 2016b meanwhile et plays a major role in the cycling of water from land and ocean surface sources into the atmosphere which in turn drives precipitation pan et al 2015 furthermore immerzeel and droogers 2008 found that calibrating a hydrological model for et significantly improved et simulations and that et simulation values were more sensitive to groundwater and meteorological parameters compared to soil and landuse parameters this indicates that including additional parameters in a model calibration can improve the overall model performance however the applicably of different calibration techniques has not been explored when both remotely sensed et and streamflow data are involved in addition this study is unique in a sense that the performance of a hydrologic model for estimating streamflow was evaluated using different remotely sensed et products therefore the objectives for this paper are to 1 determine the performance of a calibrated hydrologic model in estimating et against spatially distributed time series et products obtained from remote sensing 2 determine the impact of et parameter calibration on streamflow estimation and 3 evaluate the performances of two different genetic algorithm based calibration techniques for streamflow and et estimations 2 materials and methods 2 1 study area the study area is the honeyoey creek pine creek watershed hydrologic unit code 0408020203 which is located within the saginaw bay watershed in michigan s lower peninsula fig 1 the us environmental protection agency epa identified the saginaw bay watershed as an area of concern due to the presence of contaminated soils and degradation of fisheries within the region epa 2017 these conditions were caused by the addition of both point and non point source pollutants from a variety of sources such as industrial waste and agricultural runoff epa 2016 the final outlet for this watershed is lake huron via the saginaw river out of the approximately 1100 km2 within the honeyoey watershed agriculture is the dominant landuse 52 followed by forests 23 wetlands 17 and pasturelands 5 the remaining land is classified as urban 3 the honeyoey creek pine creek watershed has been significantly altered by anthropogenic activities as evidenced by the landuse change agricultural lands and urban area are dominant in the region which in turn impacts the natural environment especially water quality and quantity 2 2 data collection 2 2 1 physiographic data several spatial and temporal input datasets were needed to describe the study area in a hydrological model these datasets describe characteristics such as topography landuse soil properties climate and crop management practices data from the usgs were obtained to represent the topography of the region through the use of their 30 m spatial resolution national elevation dataset ned 2014 landuse information was acquired from the 30 m spatial resolution cropland data layer developed by the united states department of agriculture national agricultural statistics service usda nass nass 2012 the natural resources conservation service nrcs soil survey geographic ssurgo database was used to describe the soil properties for the region at a scale of 1 250 000 nrcs 2014 national climatic data center ncdc weather stations two precipitation stations and two temperature stations were used to obtain daily precipitation and temperature data for the time span of 2003 2014 a widely used stochastic weather generator called wxgen was employed sharpley and williams 1990 wallis and griffiths 1995 which is embedded in the soil water assessment tool swat to create climate time series for other climatological records e g relative humidity solar radiation and wind speed that are required for swat to operate neitsch et al 2011 predefined crop management operations schedules and rotations were adopted from previous studies performed in the same region love and nejadhashemi 2011 giri et al 2015 due to limitation of swat in simulating up to 250 different landuse the subwatershed map that was provided by the national hydrology dataset plus nhdplus and the michigan institute for fisheries research at a scale of 1 24 000 were modified to accommodate this limitation einheuser et al 2013 2 2 2 remote sensing data in order to evaluate the role of et remote sensing data in improving a hydrologic model predictability two satellite based et datasets were obtained for the period of 2003 2014 for the study area one dataset was created based on the simplified surface energy balance ssebop model while the other one was based on the atmosphere land exchange inverse alexi model the usgs dataset reported monthly actual evapotranspiration eta using the ssebop model senay et al 2013 eta is limited by the amount of water present at a site since it refers to the actual amount of water that is lost through both evaporation and transpiration noaa 2017 this model utilizes et fractions derived from 1 km moderate resolution imaging spectroradiometer modis thermal imagery collected every 8 days to develop a 1 km monthly eta dataset for the conterminous u s senay et al 2013 velpuri et al 2013 data were obtained from this dataset for each subwatershed in the study area in order to provide an overall eta for each subwatershed all ssebop s eta pixels within each subwatershed were averaged with respect to area to generate the overall area weighted eta average values for each month usgs 2016c the second eta dataset is created based on the alexi model which was sponsored by the usda and us national aeronautics and space administration nasa the alexi model utilizes remotely sensed morning land surface temperatures to determine eta by relating the observed change in temperature to changes in surface moisture and eta anderson et al 1997 2007 for this study 4 km thermal images were obtained from geostationary operational environmental satellites goes and used as to develop a daily 4 km eta dataset for the conterminous u s hain et al 2015 in order to make the second set of eta data comparable to the first set the daily eta values from the alexi model were averaged to create monthly eta values next these values were averaged for each subwatershed with respect to area concerning the quality of eta datasets for the region of study this cannot be performed since there is no observation points for eta in the honeyoey creek pine creek watershed however both sets of eta datasets have been validated in many regions for both ssebop and alexi models the average validation results were reported in terms of a root mean squared error rsme about 1 mm day singh and senay 2015 yang et al 2017 however based on the location of study and time of year the rmse values can vary 2 3 hydrological model swat the eta outputs of both the alexi and ssebop models were used for evaluation of swat models for the study region swat is a widely used continuous time semi distributed hydrological model that was developed by the usda agricultural research service usda ars and texas a m agrilife research texas a m university 2017 by taking into account different spatiotemporal layers of information section 2 2 1 such as topography landuse and climate swat models are able to simulate a variety of hydrological process such as runoff sediment transport and et gassman et al 2007 this makes it a very useful tool for both researchers and policy makers 2 4 calibration approaches for this study all of the collected physiographic data was incorporated into a swat model however there are many default parameters in a swat model that represent an average or more probable condition that may or may not be true for the region of study arnold et al 2012 therefore the swat model used in this study underwent a series of calibration and validation processes to do this all observed time series data were divided into calibration 2003 2008 and validation 2009 2014 periods this process is simply referred to as calibration in the rest of the paper three different types of model calibration were used in this study with the goal of understanding whether 1 calibrating the model for only streamflow can help with improvement of eta predictability 2 calibrating the model for both streamflow and eta improves model predictabilities concerning both streamflow and eta 3 calibrating the model for only eta can help with improvement of streamflow predictability the first approach was a streamflow calibration in which individual swat parameters that influence the streamflow calculations were tested to find their near optimal value through the comparison of simulated streamflows to observed streamflows observed streamflow data was obtained from a usgs streamflow station on the pine river at the outlet of the study area usgs 2016d the next two calibration approaches multi variable and genetic algorithm were used to improve the eta estimation for the study region for these sets of calibrations swat parameters used in eta calculations at the subwatershed level were altered to replicate the values obtained from the alexi and ssebop eta datasets in order to examine the role of these remotely sensed data on the performance of swat for estimating eta a multi variable calibration approach was selected to determine the impact of adding eta calibration on the swat model performance for both eta and streamflow estimation meanwhile the genetic algorithm approach was used since it is able to optimize the system for a single variable detailed descriptions of these calibration approaches are provided below 2 4 1 swat parameters as mentioned above during the swat model calibration the swat parameter values were altered the selection of these variables was done through the use of literature review and sensitivity analysis woznicki and nejadhashemi 2012 with respect to streamflow 15 swat parameters were identified and altered during the calibration process including baseflow recession constant alpha bf biological mixing efficiency biomix maximum canopy storage canmx effective hydraulic conductivity of channel ch k2 manning s n value for the main channel ch n2 moisture condition ii curve number cn2 plant uptake compensation factor epco soil evaporation compensation coefficient esco delay time for aquifer recharge gw delay revap coefficient gw reap threshold water level in shallow aquifer for base flow gwqmn aquifer percolation coefficient rchrg dp threshold water level in shallow aquifer for revap revapmn available water capacity sol awc and surface runoff lag coefficient surlag these parameters were selected based on the information provided by the swat developer arnold et al 2012 table 1 presents the minimum maximum default and calibrated values for all of these parameters for the honeyoey watershed in regards to the eta calibration another set of 10 swat parameters was identified as being influential to the eta calculations neitsch et al 2011 these included maximum canopy storage canmx carbon dioxide concentration co2 soil evaporation compensation coefficient esco fraction of maximum stomatal conductance corresponding to the second point on the stomatal conductance curve frgmax maximum stomatal conductance gsi potential evapotranspiration method ipet daily maximum temperature max temp daily minimum temperature min temp vapor pressure deficit corresponding to the fraction given by frgmax vpdfr and daily wind speed wnd sp however some of these parameters could not be altered since they were provided by either observed data or the weather generator used in this study including max temp min temp and wnd sp in addition since climate change was not a factor for this study co2 was also not altered the common practice in hydrologic modeling is to calibrate a model only for streamflow however it is interesting to know how much additional improvement in model predictability can be gained by tweaking the already calibrated model for the streamflow this can be achieved by only changing the parameters that are not used for streamflow calibration but relevant to the eta calibration therefore any swat parameters already used in the streamflow calibration canmx and esco were also not used during the eta calibration process this reduced the initial set of eta parameters from 10 to four of this set of four parameters three are crop properties and have ranges of 0 001 0 1 for gsi 0 1 for frgmax and 1 5 6 for vpdfr the last parameter used in this study ipet indicates which method to use when calculating potential evapotranspiration etp within swat three different etp methods are available namely the penman monteith method the priestley taylor method and the hargreaves method neitsch et al 2011 all three methods were included in the eta calibration process however it was found that the penman monteith method produced the best results for the study area the list of swat parameters that were used for the calibration procedures are presented in table s1 in the supplementary material section 2 4 2 initial streamflow calibration a streamflow calibration was performed to generate a base condition to which the eta calibrations could be compared in this section it was hypothesized that calibrating the model for only streamflow can help with improvement of eta predictability therefore only parameters that were commonly used for the streamflow calibration were altered in order to evaluate the performance of a hydrological model three statistical criteria that were suggested by moriasi et al 2007 were used in this study these criteria include 1 nash sutcliffe efficiency nse representing the ratio of residual variance and observed data variance nash and sutcliffe 1970 2 percent bias pbias evaluating how much larger smaller simulated data are than their corresponding observed data gupta et al 1999 and 3 root mean squared error rmse observations standard deviation ratio rsr reporting the ratio of rmse and standard deviation of measured data legates and mccabe 1999 for evaluating the performance of a hydrologic model on simulating monthly streamflow values nse values above 0 5 pbias values within 25 and rsr values below 0 7 are considered as satisfactory moriasi et al 2007 in addition we also reported rmse to examine the error associated with the simulated data in which lower values represent the better model performance 2 4 3 multi variable calibration a multi variable calibration procedure based on monte carlo simulation and an evolutionary algorithm was applied to the swat model using both remotely sensed eta datasets and observed streamflow from the study area the aim of the procedure was to identify the pareto optimal frontier and the best trade off solution a solution is classified as pareto optimal also known as non dominated when the value of any objective function cannot be improved without decreasing the performance of at least one other objective function chankong and haimes 1993 tang et al 2005 in multi variable calibration there is at least one objective function per observed variable for this study the minimization objective function of for each variable i e eta and streamflow was based on the nse of 1 nse the objective function for eta was computed using the area weighted average of the monthly simulated from the hydrologic model and satellite based eta time series for each subwatershed which was determined as follows et j 1 a t i 1 n a i et ij where et j is the average eta for month j a t is the total surface area of the watershed a i is the surface area of subwatershed i et ij is the eta for subwatershed i and month j and n is the number of subwatersheds therefore one pair of simulated observed eta series for the whole watershed was obtained to determine a unique nse for this variable this process was not employed for streamflow since there is only one gauging station at the outlet of the study area fig 1 the general outline of the multi variable calibration which is further explained in the following sections is as follows a monte carlo simulation is performed to understand the swat model performance for eta and streamflow with respect to the selected calibration parameters thus 5000 parameter sets were randomly generated via uniform sampling which were then evaluated by executing the swat model for each generated parameter set the results were used to define if possible narrower calibration parameter ranges and to obtain multi objective scatter plots to identify preliminarily pareto optimal solutions the next step consists of the application of a multi objective evolutionary algorithm known as the nondominated sorted genetic algorithm ii nsga ii deb et al 2002 to determine the optimal pareto population finally the decision making method known as the compromise programming deb 2001 using a euclidean distance metric was employed to select the final optimal trade off solution from the resulting pareto optimal population 2 4 3 1 monte carlo simulation a total of 5000 runs for monte carlo simulation were performed using matlab with randomly generated corresponding parameter sets selected from uniform distributions ranges for calibration parameters were defined as follows 0 001 0 1 for gsi 0 1 for frgmax and 1 5 6 for vpdfr a swat model run was executed for each parameter combination computing nse for both eta and streamflow dotty plots relating each of with parameter values were obtained to analyze parameter identifiability and if possible narrower calibration ranges to be explored with the nsga ii algorithm likewise multi objective plots relating eta and streamflow of values were generated for preliminary pareto frontiers identification 2 4 3 2 multi objective evolutionary algorithm nsga ii the nsga ii is a multi objective genetic algorithm that has been widely used in various disciplines and has been successfully implemented in other swat applications zhang et al 2010 2016 lu et al 2014 the nsga ii is a population based algorithm that is comprised of a nondominated ranking process a crowded distance calculation an elitist selection method and offspring reproduction operations deb 2001 for this study a real coded nsga ii with simulated binary crossover sbx and polynomial mutation baskar et al 2015 was applied requiring the prior definition of distribution indexes for each operation defined as 20 for crossover and mutation each other input parameters include the population size defined as 100 the maximum number of generations as stopping criteria defined as 50 and the mutation probability defined as the reciprocal of the number of calibration parameters 2 4 3 3 compromise programing approach the compromise programing approach using the l 2 metric which becomes the euclidean distance metric is used to select the optimal pareto population member that is closest to a reference point deb 2001 in this case the ideal point which is unfeasible and is not located on the pareto frontier is selected as the reference point and it is comprised by the best objective function values deb 2001 before computing the distance between each pareto point and the ideal point the objective function values are normalized employing a euclidian non dimensionalization sayyaadi and mehrabipour 2012 of ij n of ij i 1 m of ij 2 where i is the index for each point in the pareto frontier j is the index for each of m is the total number of the pareto population and n superscript refers to non dimensional the distance d i between each pareto point and the ideal point which is the l 2 metric is calculated as follows l 2 j 1 n of ij of ij ideal 2 where n denotes the total number of objective functions in the compromise programming approach the point with the minimum distance metric value is chosen as the best trade off solution 2 4 4 genetic algorithm calibration the other approach used to calibrate the swat models with respect to the eta datasets was a genetic algorithm ga a ga is an optimization technique that imitates biological process to refine a population of potential solutions to identify the best final or set of final solutions goldberg 1989 conn et al 1991 1997 for this study a ga was used to guide eta calibrations by changing the values of three parameters within the swat model namely gsi frgmax and vpdfr in this section it was hypothesized that calibrating the model for only eta can help with improvement of streamflow predictability therefore only parameters that were commonly used for the eta calibration were altered these are the same parameters that were modified in the multi variable optimization approach and thus the same ranges were used for this optimization with each successive set of parameter values a series of matlab codes were used to update and run the swat model abouali 2017 first the parameter values were accepted by the code which checked the values to the defined ranges and then applied the values to all subwatersheds within the region after this was completed the code executed the swat model and stored the outputs for further analysis in summary the swat model was run 904 900 times while executing these runs will not necessarily develop an ideal model it will generate a landscape of how et changes for each subwatershed based on the specified parameters for each set of parameter values the swat eta outputs were compared to the alexi and ssebop datasets and nse and rmse were calculated for each subwatershed the parameter set that had the largest nse was considered to be the best and the lowest rmse was used as the tiebreaker this allowed for the identification of the best parameter values for each subwatershed which then used to parametrizes the best model that maximizes the eta calibration it should be noted that this is only possible based on the assumption that the eta calculation for one subwatershed is not affected by the eta calculation for another subwatershed otherwise it would not be possible to create the mosaic landscape of parameter values used in the best model which to the best of our knowledge has not been done in other swat studies furthermore after the best parameters for each subwatershed were identified and applied within the swat models the simulated eta values were area averaged to produce a single eta value for the entire watershed this set of eta values was then used to calculate the nse pbias rsr and rsme for the entire region just like was done in the multi variable calibration this was done to allow for a watershed level evaluation of the calibration approaches 2 5 statistical analysis to further evaluate the streamflow and eta outputs from the calibrated models and eta datasets a mixed effects model was used to compare the mean difference between each of the outputs kuznetsova et al 2015 this process was performed twice once for the streamflow datasets observed initial streamflow calibrated model alexi multi variable calibrated model alexi genetic algorithm calibrated model ssebop multi variable calibrated model and ssebop genetic algorithm calibrated model and once for the eta datasets alexi ssebop alexi multi variable calibrated model alexi genetic algorithm calibrated model ssebop multi variable calibrated model and ssebop genetic algorithm calibrated model this allowed for the determination of significant mean differences between the datasets with a 95 confidence level 3 results and discussion 3 1 initial streamflow calibration daily streamflow was calibrated and validated for a 12 year period 6 years calibration and 6 years validation from 2003 to 2014 for the region table 2 shows the nse pbias rsr and rsme values achieved for the calibrated model as shown in the table all criteria nse pbias and rsr are in their respective satisfactory ranges moriasi et al 2007 indicating that the model was successfully calibrated and can be used to simulate streamflow values for the region furthermore while the overall rsme was 6 522 the calibration period had a smaller rsme compared to the validation period indicating a better model fit during the calibration period than the validation period the temporal variability of observed and simulated streamflow is also presented in fig 2 overall the swat model represents observed flow variations very accurately the results of this section present the performance of the swat model in replicating the spatially distributed eta data obtained from two remote sensing products ssebop and alexi datasets table 3 shows the swat model performance for the overall calibration and validation periods based on nse pbias rsr and rmse of the eta for the condition in which only the streamflow calibration was performed these calculations followed the same procedure that was discussed in the multi variable and ga calibration sections in which eta values were area averaged across the watershed and then used to calculate watershed level statistical criteria when considering the entire time period the streamflow calibrated swat model was able to replicate the ssebop eta dataset more accurately than the alexi eta dataset this can be seen by the fact that the statistical criteria for the ssebop eta were better than those for the alexi eta similar results were see for the calibration and validation periods overall this shows that the swat model can better replicate the ssebop eta data compared to the alexi data 3 2 multi variable calibration a combination of 5000 monte carlo simulations and a nsga ii evolutionary algorithm were used to identify the pareto frontiers for the swat model calibrations for both the alexi and ssebop eta datasets fig 3 shows both the entire monte carlo population as well as the pareto frontiers identified by the nsga ii evolutionary algorithm for each eta dataset this shows that pareto frontiers were able to be identified from the monte carlo simulations run for each eta datasets which indicates the first phase of the multi variable optimization was successful for both datasets however the ssebop pareto frontier was able to further minimize streamflow and eta ofs compared to the alexi pareto frontier therefore calibrating the swat model using the ssebop eta data was able to produce a more accurate model performance this can be seen more clearly in fig 4 which shows the pareto frontiers for both the ssebop and alexi datasets this figure also highlights the optimal pareto population member selected by the compromise programing method which shows the optimal model calibration for each dataset this reinforces the conclusions that the ssebop dataset performed better than the alexi dataset and achieved a model calibration that was able to better simulate both streamflow and et values for the entire region in addition the results showed that the multi variable calibration was able to identify a final calibrated model for each dataset that improved both streamflow and et simulations table 4 shows the nse pbias rsr and rmse values achieved for both final calibrated models all values presented in the table fall within the satisfactory ranges and indicate that the models were successfully calibrated furthermore comparison of these values with the base model simulations showed that with respect to et there was an improvement in the statistical criteria for example when considering overall nse the alexi calibrated model had a value of 0 73 compared to the 0 62 for the base model and the ssebop calibrated model had a value of 0 85 compared to the 0 81 for the base model this indicates that the newly calibrated models are better able to simulate eta data however with respect to streamflow all statistical criteria remain within the satisfactory ranges and often similar to the base model statistical criteria suggesting that the streamflow simulations were not heavily impacted by the addition of the et calibration overall the results show that this calibration approach was successful at improving the models performances while maintaining the current streamflow accuracies 3 3 genetic algorithm calibration in addition to the multi variable approach a ga optimization was also performed unlike the multi variable approach this approach focused on only improving the eta estimations for two remotely sensed datasets alexi and ssebop without considering the streamflow calibration after hundreds of runs for each subwatershed the ga was able to identify the optimal parameters values for each subwatershed and the eta datasets these final parameter values were used to develop swat models that represented the optimal eta calibration for each subwatershed table 5 shows the nse pbias rsr and rmse values achieved for both final calibrated models all of the eta statistical criteria values presented in the table fall within the satisfactory ranges and indicate that the models were successfully calibrated with respect to et when compared to the base model it can be seen that the eta calibration performed here was able to improve the simulation of eta values for both the alexi and ssebop datasets for example when considering the overall nse the alexi calibrated model had a value of 0 75 compared to the 0 62 for the base model and the ssebop calibrated model had a value of 0 84 compared to the 0 81 for the base model however when considering the streamflow calibration most of the statistical values have fallen outside the satisfactory ranges nse 0 5 pbias 25 and rsr 0 7 for each criteria this indicates that while this process was able to improve the et simulations it was done at the cost of compromising streamflow simulations this seems logical knowing that this approach did not consider the streamflow calibration during the eta calibration process however this does indicate that this approach would be unsuitable for calibrating models that require accurate streamflow values 3 4 statistical significance the results of the statistical analysis of the mean difference between each of the datasets are presented for streamflow and eta in tables 6 and 7 respectively linear mixed effects models were employed to account for the spatiotemporal effects that cause sample correlation violating the independence assumption for the usual paired t test esfahanian et al 2017 with regard to the streamflow datasets all comparisons were found to be significantly different from each other except for the comparison of the observed dataset with the initial streamflow calibrated model this indicates that the initial calibration was able to closely replicate the observed data to the point where statistically there is no difference between them however the significant difference observed for all other models compared to the observed data indicates that those models are not as accurate when simulating streamflow this seems logical for the models calibrated via the genetic algorithm approach since there was a noticeable decrease in the statistical criteria for the streamflow calibration in these models however we did not expect this for the models calibrated using the multi variable approach since these models showed little to no change in the calibration criteria for streamflow these results indicate that even though the calibration process was able to satisfactorily calibrate streamflow there exist more inconsistencies within the final simulated streamflow when compared to the observed data when considering the comparison of streamflow simulations between the initial model and the other four models tested the significant difference makes sense and indicates that the addition of the eta calibration influenced the streamflow calibration to an extent furthermore since all of these the p values were negative the eta calibrated models all underestimated the streamflow when compared to both the observed dataset and the initial streamflow model this indicates that regardless of the calibration method used or the impact seen on the statistical criteria the eta calibrated models produced lower streamflow values on average finally the comparisons between the four et calibrated models also showed significant difference which seems understandable given the use of different et datasets and calibration process used in this study with regards to the eta datasets table 7 almost all comparisons among datasets showed significant differences except for the ssebop dataset versus the initial streamflow calibrated model column a and row c and the ssebop genetic algorithm calibrated model versus the alexi multi variable calibrated model column e and row f these two cases are rather interesting since the first comparison ssebop versus the initial streamflow calibrated model indicates that by only calibrating for streamflow it was possible to simulate eta so that it is not statistically different from the remotely sensed data meanwhile the second case ssebop genetic algorithm calibrated model versus alexi multi variable calibrated model indicates that in some cases calibrated swat model can generate comparable eta time series even though different approaches and datasets were used considering all of the other significant differences the comparison between the alexi and ssebop data column a and row b in table 7 made the most logical sense since different methodologies were used to calculate these datasets furthermore similar results to the streamflow were also seen when comparing the eta calibrated models to the remotely sensed eta datasets these observations confirm that even though these models were able to satisfactorily simulate eta values the swat simulated eta was statistically different from the remotely sensed data used to calibrate them and thus could not accurately replicate the remotely sensed data however while the streamflow comparisons showed that all of the eta calibrated swat models underestimated streamflow here it can be seen that the ssebop calibrated swat models overestimated eta values while the alexi calibrated swat models underestimated the eta values when compared to the ssebop and alexi datasets respectively in addition similar to the streamflow comparisons the four eta calibrated models were significantly different from the initial streamflow calibrated model which makes sense since all of the eta calibrated models had an increase in the statistical criteria for eta calibration compared to the initial streamflow calibrated model finally the comparisons between the four eta calibrated models columns d through f and rows e through g showed significant difference from each other except for the case of the ssebop genetic algorithm calibrated model versus the alexi multi variable calibrated model discussed previously this is reasonable since different calibration approaches and eta datasets were used 3 5 comparison of the multi variable and genetic algorithm calibrations based on the information provided in tables 4 and 5 it can be concluded that the multi variable approach used in this study was able to generate better overall swat models compared to the ga approach however if the goal of the model is to generate more accurate eta data the ga approach was able to outperform the multi variable approach this shows that depending on the purpose of the model applications different calibration techniques should be used furthermore it is to be noted that for both approaches the models built using the ssebop data were able to achieve higher performances in simulating both streamflow and eta data than the models made based on the alexi data 4 conclusions in this study two different eta calibration techniques were used to evaluate the impact of adding spatially distributed and remotely sensed eta datasets to the traditional streamflow calibration used in hydrological models both techniques multi variable and ga were able to successfully improve the eta calibration for the hydrological model using both remotely sensed eta datasets the ga technique was able to produce better eta calibrations and thus better eta simulations however this was achieved at the cost of lowering the streamflow calibrations meanwhile the multi variable technique was able to improve the eta calibration while maintaining the streamflow calibration therefore future use of these approaches should be driven by the needs of the research for example if a study is focused solely on better eta estimation the ga approach is the better option meanwhile studies focused on better simulating the entire hydrological cycle for a region should use the multi variable approach concerning the eta datasets used in this study the calibrations performed with the ssebop dataset resulted better eta estimations compared to the calibrations based on the alexi dataset for this study area therefore it is recommended that future studies should perform this analysis in other regions to better understand how these datasets compare to each other as well as evaluating the impacts of different climate variabilities e g snow cover statistical analysis of the streamflow and eta showed that the remotely sensed eta datasets were significantly different from each other which was expected moreover except for one exception all of the streamflow and eta datasets produced by the eta calibrated swat models were also significantly different from each other however all four eta calibrated models were also significantly different when compared to the remotely sensed data this indicated that while the overall model calibration was successful it was unable to closely replicate the remotely sensed data showing that there still could be additional improvements in the both in calibration process and the swat model simulations it is to be noted that the eta calibration processes used in this study only altered three parameters within the swat model this was due to temporal and computational limitations however the addition of other parameters to the calibration process such as the soil evaporation compensation factor esco could result in even better model calibrations and thus better model outputs and should be the focus of future studies in addition while adding eta calibration to the overall model calibration process was successful in this study future studies should consider additional hydrological cycle components such as remotely sensed soil moisture datasets this would allow for the development of even more realistic models and thus more accurate results for stakeholders and policy makers who rely on model outputs for managing freshwater resources acknowledgments authors would like to thank dr wade crow from usda ars hydrology and remote sensing laboratory at beltsville maryland for his help with editing the paper this work is supported by the usda national institute of food and agriculture hatch project micl02359 appendix a supplementary data supplementary data associated with this article can be found in the online version at https doi org 10 1016 j jhydrol 2017 11 009 appendix a supplementary data supplementary data 
7539,as the global demands for the use of freshwater resources continues to rise it has become increasingly important to insure the sustainability of this resources this is accomplished through the use of management strategies that often utilize monitoring and the use of hydrological models however monitoring at large scales is not feasible and therefore model applications are becoming challenging especially when spatially distributed datasets such as evapotranspiration are needed to understand the model performances due to these limitations most of the hydrological models are only calibrated for data obtained from site point observations such as streamflow therefore the main focus of this paper is to examine whether the incorporation of remotely sensed and spatially distributed datasets can improve the overall performance of the model in this study actual evapotranspiration eta data was obtained from the two different sets of satellite based remote sensing data one dataset estimates eta based on the simplified surface energy balance ssebop model while the other one estimates eta based on the atmosphere land exchange inverse alexi model the hydrological model used in this study is the soil and water assessment tool swat which was calibrated against spatially distributed eta and single point streamflow records for the honeyoey creek pine creek watershed located in michigan usa two different techniques multi variable and genetic algorithm were used to calibrate the swat model using the aforementioned datasets the performance of the hydrological model in estimating eta was improved using both calibration techniques by achieving nash sutcliffe efficiency nse values 0 5 0 73 0 85 percent bias pbias values within 25 21 73 and root mean squared error observations standard deviation ratio rsr values 0 7 0 39 0 52 however the genetic algorithm technique was more effective with the eta calibration while significantly reducing the model performance for estimating the streamflow nse 0 32 0 52 pbias 32 73 and rsr 0 63 0 82 meanwhile using the multi variable technique the model performance for estimating the streamflow was maintained with a high level of accuracy nse 0 59 0 61 pbias 13 70 and rsr 0 63 0 64 while the evapotranspiration estimations were improved results from this assessment shows that incorporation of remotely sensed and spatially distributed data can improve the hydrological model performance if it is coupled with a right calibration technique abbreviations alexi atmosphere land exchange inverse alpha bf baseflow recession constant biomix biological mixing efficiency canmx maximum canopy storage canmx maximum canopy storage ch k2 effective hydraulic conductivity of channel ch n2 manning s n value for the main channel cn2 moisture condition ii curve number co2 carbon dioxide concentration epa environmental protection agency epco plant uptake compensation factor esco soil evaporation compensation coefficient et evapotranspiration eta actual evapotranspiration frgmax fraction of maximum stomatal conductance corresponding to the second point on the stomatal conductance curve ga genetic algorithm goes geostationary operational environmental satellites gsi maximum stomatal conductance gw delay delay time for aquifer recharge gw reap revap coefficient gwqmn threshold water level in shallow aquifer for base flow ipet potential evapotranspiration method max temp daily maximum temperature min temp daily minimum temperature modis moderate resolution imaging spectroradiometer nasa national aeronautics and space administration nass national agricultural statistics service ncdc national climatic data center ned national elevation dataset nhdplus national hydrology dataset plus nrcs natural resources conservation service nse nash sutcliffe efficiency nsga ii nondominated sorted genetic algorithm ii of objective function pbias percent bias rchrg dp aquifer percolation coefficient revapmn threshold water level in shallow aquifer for revap rsme root mean squared error rsr root mean squared error observations standard deviation ratio sol awc available water capacity ssebop simplified surface energy balance surlag surface runoff lag coefficient swat soil and water assessment tool usda united states department of agriculture usgs united states geological survey vpdfr vapor pressure deficit corresponding to the fraction given by frgmax wnd sp daily wind speed keywords evapotranspiration streamflow monte carlo simulation nsga ii ssebop alexi 1 introduction as extreme climate conditions and anthropogenic activities continue to impact environmental systems mitigation and restoration related projects have become common furthermore environmental systems such as watersheds are very complex with many relationships and interlocking processes sivakumar and singh 2012 guerrero et al 2013 therefore it can be challenging to determine which management solution s should be selected and implemented herman et al 2015 sabbaghian et al 2016 this has led to the development of many different modeling techniques that can simulate a variety of options and identify the best solution s based on the criteria put forth mostly by stakeholders and policy makers chen et al 2012 beven and smith 2014 giri et al 2016 meanwhile the first step in a model implementation is parameter calibration parameter calibration in model applications is used to adjust model performance to better simulate the natural systems they are trying to describe guerrero et al 2013 zhan et al 2013 rajib et al 2016 while parameter calibration improves the ability of models to more accurately represent natural systems models performances are still limited by the quality and quantity of input data and their availabilities nejadhashemi et al 2011 today most hydrological studies rely on data collected at monitoring stations across the world in fact the united states geological survey usgs has about 1 5 million monitoring sites from which data can be obtained usgs 2016a however even with the existence of all these monitoring sites there are times where higher spatial resolutions are needed by researchers stakeholders and policy makers to more precisely evaluate the hydrologic conditions and to determine the best place to implement mitigation and restoration projects wanders et al 2014 one way to address this issue is the use of remotely sensed data remote sensing is defined as the science of identifying observing and measuring an object without physical contact graham 1999 with the advancements in satellite technology remotely sensed satellite data has become a source of consistent monitoring for the entire globe with applications ranging from crop yields to water resources assessments graham 1999 long et al 2014 in order to model water resources more accurately it is important to examining different components of the hydrologic cycle including water movement processes e g evaporation and streamflow and water storage e g soil moisture water vapor groundwater and surface water bodies while hydrological models simulate all components of the hydrological cycle streamflow is often the only component that the model outputs are compared against during the calibration process since it can be measured more accurately than the other components immerzeel and droogers 2008 wanders et al 2014 rajib et al 2016 this can result in poor simulations of other hydrologic components which ultimately lowers the model performance wanders et al 2014 rajib et al 2016 therefore including additional hydrological components in the parameter calibration process could allow the model to better represent all process occurring in the environment crow et al 2003 in particular evapotranspiration et could be considered an important hydrological component added to the calibration process since it describes the moisture lost to the atmosphere from both biotic e g plants and abiotic e g soils sources hanson 1991 usgs 2016b meanwhile et plays a major role in the cycling of water from land and ocean surface sources into the atmosphere which in turn drives precipitation pan et al 2015 furthermore immerzeel and droogers 2008 found that calibrating a hydrological model for et significantly improved et simulations and that et simulation values were more sensitive to groundwater and meteorological parameters compared to soil and landuse parameters this indicates that including additional parameters in a model calibration can improve the overall model performance however the applicably of different calibration techniques has not been explored when both remotely sensed et and streamflow data are involved in addition this study is unique in a sense that the performance of a hydrologic model for estimating streamflow was evaluated using different remotely sensed et products therefore the objectives for this paper are to 1 determine the performance of a calibrated hydrologic model in estimating et against spatially distributed time series et products obtained from remote sensing 2 determine the impact of et parameter calibration on streamflow estimation and 3 evaluate the performances of two different genetic algorithm based calibration techniques for streamflow and et estimations 2 materials and methods 2 1 study area the study area is the honeyoey creek pine creek watershed hydrologic unit code 0408020203 which is located within the saginaw bay watershed in michigan s lower peninsula fig 1 the us environmental protection agency epa identified the saginaw bay watershed as an area of concern due to the presence of contaminated soils and degradation of fisheries within the region epa 2017 these conditions were caused by the addition of both point and non point source pollutants from a variety of sources such as industrial waste and agricultural runoff epa 2016 the final outlet for this watershed is lake huron via the saginaw river out of the approximately 1100 km2 within the honeyoey watershed agriculture is the dominant landuse 52 followed by forests 23 wetlands 17 and pasturelands 5 the remaining land is classified as urban 3 the honeyoey creek pine creek watershed has been significantly altered by anthropogenic activities as evidenced by the landuse change agricultural lands and urban area are dominant in the region which in turn impacts the natural environment especially water quality and quantity 2 2 data collection 2 2 1 physiographic data several spatial and temporal input datasets were needed to describe the study area in a hydrological model these datasets describe characteristics such as topography landuse soil properties climate and crop management practices data from the usgs were obtained to represent the topography of the region through the use of their 30 m spatial resolution national elevation dataset ned 2014 landuse information was acquired from the 30 m spatial resolution cropland data layer developed by the united states department of agriculture national agricultural statistics service usda nass nass 2012 the natural resources conservation service nrcs soil survey geographic ssurgo database was used to describe the soil properties for the region at a scale of 1 250 000 nrcs 2014 national climatic data center ncdc weather stations two precipitation stations and two temperature stations were used to obtain daily precipitation and temperature data for the time span of 2003 2014 a widely used stochastic weather generator called wxgen was employed sharpley and williams 1990 wallis and griffiths 1995 which is embedded in the soil water assessment tool swat to create climate time series for other climatological records e g relative humidity solar radiation and wind speed that are required for swat to operate neitsch et al 2011 predefined crop management operations schedules and rotations were adopted from previous studies performed in the same region love and nejadhashemi 2011 giri et al 2015 due to limitation of swat in simulating up to 250 different landuse the subwatershed map that was provided by the national hydrology dataset plus nhdplus and the michigan institute for fisheries research at a scale of 1 24 000 were modified to accommodate this limitation einheuser et al 2013 2 2 2 remote sensing data in order to evaluate the role of et remote sensing data in improving a hydrologic model predictability two satellite based et datasets were obtained for the period of 2003 2014 for the study area one dataset was created based on the simplified surface energy balance ssebop model while the other one was based on the atmosphere land exchange inverse alexi model the usgs dataset reported monthly actual evapotranspiration eta using the ssebop model senay et al 2013 eta is limited by the amount of water present at a site since it refers to the actual amount of water that is lost through both evaporation and transpiration noaa 2017 this model utilizes et fractions derived from 1 km moderate resolution imaging spectroradiometer modis thermal imagery collected every 8 days to develop a 1 km monthly eta dataset for the conterminous u s senay et al 2013 velpuri et al 2013 data were obtained from this dataset for each subwatershed in the study area in order to provide an overall eta for each subwatershed all ssebop s eta pixels within each subwatershed were averaged with respect to area to generate the overall area weighted eta average values for each month usgs 2016c the second eta dataset is created based on the alexi model which was sponsored by the usda and us national aeronautics and space administration nasa the alexi model utilizes remotely sensed morning land surface temperatures to determine eta by relating the observed change in temperature to changes in surface moisture and eta anderson et al 1997 2007 for this study 4 km thermal images were obtained from geostationary operational environmental satellites goes and used as to develop a daily 4 km eta dataset for the conterminous u s hain et al 2015 in order to make the second set of eta data comparable to the first set the daily eta values from the alexi model were averaged to create monthly eta values next these values were averaged for each subwatershed with respect to area concerning the quality of eta datasets for the region of study this cannot be performed since there is no observation points for eta in the honeyoey creek pine creek watershed however both sets of eta datasets have been validated in many regions for both ssebop and alexi models the average validation results were reported in terms of a root mean squared error rsme about 1 mm day singh and senay 2015 yang et al 2017 however based on the location of study and time of year the rmse values can vary 2 3 hydrological model swat the eta outputs of both the alexi and ssebop models were used for evaluation of swat models for the study region swat is a widely used continuous time semi distributed hydrological model that was developed by the usda agricultural research service usda ars and texas a m agrilife research texas a m university 2017 by taking into account different spatiotemporal layers of information section 2 2 1 such as topography landuse and climate swat models are able to simulate a variety of hydrological process such as runoff sediment transport and et gassman et al 2007 this makes it a very useful tool for both researchers and policy makers 2 4 calibration approaches for this study all of the collected physiographic data was incorporated into a swat model however there are many default parameters in a swat model that represent an average or more probable condition that may or may not be true for the region of study arnold et al 2012 therefore the swat model used in this study underwent a series of calibration and validation processes to do this all observed time series data were divided into calibration 2003 2008 and validation 2009 2014 periods this process is simply referred to as calibration in the rest of the paper three different types of model calibration were used in this study with the goal of understanding whether 1 calibrating the model for only streamflow can help with improvement of eta predictability 2 calibrating the model for both streamflow and eta improves model predictabilities concerning both streamflow and eta 3 calibrating the model for only eta can help with improvement of streamflow predictability the first approach was a streamflow calibration in which individual swat parameters that influence the streamflow calculations were tested to find their near optimal value through the comparison of simulated streamflows to observed streamflows observed streamflow data was obtained from a usgs streamflow station on the pine river at the outlet of the study area usgs 2016d the next two calibration approaches multi variable and genetic algorithm were used to improve the eta estimation for the study region for these sets of calibrations swat parameters used in eta calculations at the subwatershed level were altered to replicate the values obtained from the alexi and ssebop eta datasets in order to examine the role of these remotely sensed data on the performance of swat for estimating eta a multi variable calibration approach was selected to determine the impact of adding eta calibration on the swat model performance for both eta and streamflow estimation meanwhile the genetic algorithm approach was used since it is able to optimize the system for a single variable detailed descriptions of these calibration approaches are provided below 2 4 1 swat parameters as mentioned above during the swat model calibration the swat parameter values were altered the selection of these variables was done through the use of literature review and sensitivity analysis woznicki and nejadhashemi 2012 with respect to streamflow 15 swat parameters were identified and altered during the calibration process including baseflow recession constant alpha bf biological mixing efficiency biomix maximum canopy storage canmx effective hydraulic conductivity of channel ch k2 manning s n value for the main channel ch n2 moisture condition ii curve number cn2 plant uptake compensation factor epco soil evaporation compensation coefficient esco delay time for aquifer recharge gw delay revap coefficient gw reap threshold water level in shallow aquifer for base flow gwqmn aquifer percolation coefficient rchrg dp threshold water level in shallow aquifer for revap revapmn available water capacity sol awc and surface runoff lag coefficient surlag these parameters were selected based on the information provided by the swat developer arnold et al 2012 table 1 presents the minimum maximum default and calibrated values for all of these parameters for the honeyoey watershed in regards to the eta calibration another set of 10 swat parameters was identified as being influential to the eta calculations neitsch et al 2011 these included maximum canopy storage canmx carbon dioxide concentration co2 soil evaporation compensation coefficient esco fraction of maximum stomatal conductance corresponding to the second point on the stomatal conductance curve frgmax maximum stomatal conductance gsi potential evapotranspiration method ipet daily maximum temperature max temp daily minimum temperature min temp vapor pressure deficit corresponding to the fraction given by frgmax vpdfr and daily wind speed wnd sp however some of these parameters could not be altered since they were provided by either observed data or the weather generator used in this study including max temp min temp and wnd sp in addition since climate change was not a factor for this study co2 was also not altered the common practice in hydrologic modeling is to calibrate a model only for streamflow however it is interesting to know how much additional improvement in model predictability can be gained by tweaking the already calibrated model for the streamflow this can be achieved by only changing the parameters that are not used for streamflow calibration but relevant to the eta calibration therefore any swat parameters already used in the streamflow calibration canmx and esco were also not used during the eta calibration process this reduced the initial set of eta parameters from 10 to four of this set of four parameters three are crop properties and have ranges of 0 001 0 1 for gsi 0 1 for frgmax and 1 5 6 for vpdfr the last parameter used in this study ipet indicates which method to use when calculating potential evapotranspiration etp within swat three different etp methods are available namely the penman monteith method the priestley taylor method and the hargreaves method neitsch et al 2011 all three methods were included in the eta calibration process however it was found that the penman monteith method produced the best results for the study area the list of swat parameters that were used for the calibration procedures are presented in table s1 in the supplementary material section 2 4 2 initial streamflow calibration a streamflow calibration was performed to generate a base condition to which the eta calibrations could be compared in this section it was hypothesized that calibrating the model for only streamflow can help with improvement of eta predictability therefore only parameters that were commonly used for the streamflow calibration were altered in order to evaluate the performance of a hydrological model three statistical criteria that were suggested by moriasi et al 2007 were used in this study these criteria include 1 nash sutcliffe efficiency nse representing the ratio of residual variance and observed data variance nash and sutcliffe 1970 2 percent bias pbias evaluating how much larger smaller simulated data are than their corresponding observed data gupta et al 1999 and 3 root mean squared error rmse observations standard deviation ratio rsr reporting the ratio of rmse and standard deviation of measured data legates and mccabe 1999 for evaluating the performance of a hydrologic model on simulating monthly streamflow values nse values above 0 5 pbias values within 25 and rsr values below 0 7 are considered as satisfactory moriasi et al 2007 in addition we also reported rmse to examine the error associated with the simulated data in which lower values represent the better model performance 2 4 3 multi variable calibration a multi variable calibration procedure based on monte carlo simulation and an evolutionary algorithm was applied to the swat model using both remotely sensed eta datasets and observed streamflow from the study area the aim of the procedure was to identify the pareto optimal frontier and the best trade off solution a solution is classified as pareto optimal also known as non dominated when the value of any objective function cannot be improved without decreasing the performance of at least one other objective function chankong and haimes 1993 tang et al 2005 in multi variable calibration there is at least one objective function per observed variable for this study the minimization objective function of for each variable i e eta and streamflow was based on the nse of 1 nse the objective function for eta was computed using the area weighted average of the monthly simulated from the hydrologic model and satellite based eta time series for each subwatershed which was determined as follows et j 1 a t i 1 n a i et ij where et j is the average eta for month j a t is the total surface area of the watershed a i is the surface area of subwatershed i et ij is the eta for subwatershed i and month j and n is the number of subwatersheds therefore one pair of simulated observed eta series for the whole watershed was obtained to determine a unique nse for this variable this process was not employed for streamflow since there is only one gauging station at the outlet of the study area fig 1 the general outline of the multi variable calibration which is further explained in the following sections is as follows a monte carlo simulation is performed to understand the swat model performance for eta and streamflow with respect to the selected calibration parameters thus 5000 parameter sets were randomly generated via uniform sampling which were then evaluated by executing the swat model for each generated parameter set the results were used to define if possible narrower calibration parameter ranges and to obtain multi objective scatter plots to identify preliminarily pareto optimal solutions the next step consists of the application of a multi objective evolutionary algorithm known as the nondominated sorted genetic algorithm ii nsga ii deb et al 2002 to determine the optimal pareto population finally the decision making method known as the compromise programming deb 2001 using a euclidean distance metric was employed to select the final optimal trade off solution from the resulting pareto optimal population 2 4 3 1 monte carlo simulation a total of 5000 runs for monte carlo simulation were performed using matlab with randomly generated corresponding parameter sets selected from uniform distributions ranges for calibration parameters were defined as follows 0 001 0 1 for gsi 0 1 for frgmax and 1 5 6 for vpdfr a swat model run was executed for each parameter combination computing nse for both eta and streamflow dotty plots relating each of with parameter values were obtained to analyze parameter identifiability and if possible narrower calibration ranges to be explored with the nsga ii algorithm likewise multi objective plots relating eta and streamflow of values were generated for preliminary pareto frontiers identification 2 4 3 2 multi objective evolutionary algorithm nsga ii the nsga ii is a multi objective genetic algorithm that has been widely used in various disciplines and has been successfully implemented in other swat applications zhang et al 2010 2016 lu et al 2014 the nsga ii is a population based algorithm that is comprised of a nondominated ranking process a crowded distance calculation an elitist selection method and offspring reproduction operations deb 2001 for this study a real coded nsga ii with simulated binary crossover sbx and polynomial mutation baskar et al 2015 was applied requiring the prior definition of distribution indexes for each operation defined as 20 for crossover and mutation each other input parameters include the population size defined as 100 the maximum number of generations as stopping criteria defined as 50 and the mutation probability defined as the reciprocal of the number of calibration parameters 2 4 3 3 compromise programing approach the compromise programing approach using the l 2 metric which becomes the euclidean distance metric is used to select the optimal pareto population member that is closest to a reference point deb 2001 in this case the ideal point which is unfeasible and is not located on the pareto frontier is selected as the reference point and it is comprised by the best objective function values deb 2001 before computing the distance between each pareto point and the ideal point the objective function values are normalized employing a euclidian non dimensionalization sayyaadi and mehrabipour 2012 of ij n of ij i 1 m of ij 2 where i is the index for each point in the pareto frontier j is the index for each of m is the total number of the pareto population and n superscript refers to non dimensional the distance d i between each pareto point and the ideal point which is the l 2 metric is calculated as follows l 2 j 1 n of ij of ij ideal 2 where n denotes the total number of objective functions in the compromise programming approach the point with the minimum distance metric value is chosen as the best trade off solution 2 4 4 genetic algorithm calibration the other approach used to calibrate the swat models with respect to the eta datasets was a genetic algorithm ga a ga is an optimization technique that imitates biological process to refine a population of potential solutions to identify the best final or set of final solutions goldberg 1989 conn et al 1991 1997 for this study a ga was used to guide eta calibrations by changing the values of three parameters within the swat model namely gsi frgmax and vpdfr in this section it was hypothesized that calibrating the model for only eta can help with improvement of streamflow predictability therefore only parameters that were commonly used for the eta calibration were altered these are the same parameters that were modified in the multi variable optimization approach and thus the same ranges were used for this optimization with each successive set of parameter values a series of matlab codes were used to update and run the swat model abouali 2017 first the parameter values were accepted by the code which checked the values to the defined ranges and then applied the values to all subwatersheds within the region after this was completed the code executed the swat model and stored the outputs for further analysis in summary the swat model was run 904 900 times while executing these runs will not necessarily develop an ideal model it will generate a landscape of how et changes for each subwatershed based on the specified parameters for each set of parameter values the swat eta outputs were compared to the alexi and ssebop datasets and nse and rmse were calculated for each subwatershed the parameter set that had the largest nse was considered to be the best and the lowest rmse was used as the tiebreaker this allowed for the identification of the best parameter values for each subwatershed which then used to parametrizes the best model that maximizes the eta calibration it should be noted that this is only possible based on the assumption that the eta calculation for one subwatershed is not affected by the eta calculation for another subwatershed otherwise it would not be possible to create the mosaic landscape of parameter values used in the best model which to the best of our knowledge has not been done in other swat studies furthermore after the best parameters for each subwatershed were identified and applied within the swat models the simulated eta values were area averaged to produce a single eta value for the entire watershed this set of eta values was then used to calculate the nse pbias rsr and rsme for the entire region just like was done in the multi variable calibration this was done to allow for a watershed level evaluation of the calibration approaches 2 5 statistical analysis to further evaluate the streamflow and eta outputs from the calibrated models and eta datasets a mixed effects model was used to compare the mean difference between each of the outputs kuznetsova et al 2015 this process was performed twice once for the streamflow datasets observed initial streamflow calibrated model alexi multi variable calibrated model alexi genetic algorithm calibrated model ssebop multi variable calibrated model and ssebop genetic algorithm calibrated model and once for the eta datasets alexi ssebop alexi multi variable calibrated model alexi genetic algorithm calibrated model ssebop multi variable calibrated model and ssebop genetic algorithm calibrated model this allowed for the determination of significant mean differences between the datasets with a 95 confidence level 3 results and discussion 3 1 initial streamflow calibration daily streamflow was calibrated and validated for a 12 year period 6 years calibration and 6 years validation from 2003 to 2014 for the region table 2 shows the nse pbias rsr and rsme values achieved for the calibrated model as shown in the table all criteria nse pbias and rsr are in their respective satisfactory ranges moriasi et al 2007 indicating that the model was successfully calibrated and can be used to simulate streamflow values for the region furthermore while the overall rsme was 6 522 the calibration period had a smaller rsme compared to the validation period indicating a better model fit during the calibration period than the validation period the temporal variability of observed and simulated streamflow is also presented in fig 2 overall the swat model represents observed flow variations very accurately the results of this section present the performance of the swat model in replicating the spatially distributed eta data obtained from two remote sensing products ssebop and alexi datasets table 3 shows the swat model performance for the overall calibration and validation periods based on nse pbias rsr and rmse of the eta for the condition in which only the streamflow calibration was performed these calculations followed the same procedure that was discussed in the multi variable and ga calibration sections in which eta values were area averaged across the watershed and then used to calculate watershed level statistical criteria when considering the entire time period the streamflow calibrated swat model was able to replicate the ssebop eta dataset more accurately than the alexi eta dataset this can be seen by the fact that the statistical criteria for the ssebop eta were better than those for the alexi eta similar results were see for the calibration and validation periods overall this shows that the swat model can better replicate the ssebop eta data compared to the alexi data 3 2 multi variable calibration a combination of 5000 monte carlo simulations and a nsga ii evolutionary algorithm were used to identify the pareto frontiers for the swat model calibrations for both the alexi and ssebop eta datasets fig 3 shows both the entire monte carlo population as well as the pareto frontiers identified by the nsga ii evolutionary algorithm for each eta dataset this shows that pareto frontiers were able to be identified from the monte carlo simulations run for each eta datasets which indicates the first phase of the multi variable optimization was successful for both datasets however the ssebop pareto frontier was able to further minimize streamflow and eta ofs compared to the alexi pareto frontier therefore calibrating the swat model using the ssebop eta data was able to produce a more accurate model performance this can be seen more clearly in fig 4 which shows the pareto frontiers for both the ssebop and alexi datasets this figure also highlights the optimal pareto population member selected by the compromise programing method which shows the optimal model calibration for each dataset this reinforces the conclusions that the ssebop dataset performed better than the alexi dataset and achieved a model calibration that was able to better simulate both streamflow and et values for the entire region in addition the results showed that the multi variable calibration was able to identify a final calibrated model for each dataset that improved both streamflow and et simulations table 4 shows the nse pbias rsr and rmse values achieved for both final calibrated models all values presented in the table fall within the satisfactory ranges and indicate that the models were successfully calibrated furthermore comparison of these values with the base model simulations showed that with respect to et there was an improvement in the statistical criteria for example when considering overall nse the alexi calibrated model had a value of 0 73 compared to the 0 62 for the base model and the ssebop calibrated model had a value of 0 85 compared to the 0 81 for the base model this indicates that the newly calibrated models are better able to simulate eta data however with respect to streamflow all statistical criteria remain within the satisfactory ranges and often similar to the base model statistical criteria suggesting that the streamflow simulations were not heavily impacted by the addition of the et calibration overall the results show that this calibration approach was successful at improving the models performances while maintaining the current streamflow accuracies 3 3 genetic algorithm calibration in addition to the multi variable approach a ga optimization was also performed unlike the multi variable approach this approach focused on only improving the eta estimations for two remotely sensed datasets alexi and ssebop without considering the streamflow calibration after hundreds of runs for each subwatershed the ga was able to identify the optimal parameters values for each subwatershed and the eta datasets these final parameter values were used to develop swat models that represented the optimal eta calibration for each subwatershed table 5 shows the nse pbias rsr and rmse values achieved for both final calibrated models all of the eta statistical criteria values presented in the table fall within the satisfactory ranges and indicate that the models were successfully calibrated with respect to et when compared to the base model it can be seen that the eta calibration performed here was able to improve the simulation of eta values for both the alexi and ssebop datasets for example when considering the overall nse the alexi calibrated model had a value of 0 75 compared to the 0 62 for the base model and the ssebop calibrated model had a value of 0 84 compared to the 0 81 for the base model however when considering the streamflow calibration most of the statistical values have fallen outside the satisfactory ranges nse 0 5 pbias 25 and rsr 0 7 for each criteria this indicates that while this process was able to improve the et simulations it was done at the cost of compromising streamflow simulations this seems logical knowing that this approach did not consider the streamflow calibration during the eta calibration process however this does indicate that this approach would be unsuitable for calibrating models that require accurate streamflow values 3 4 statistical significance the results of the statistical analysis of the mean difference between each of the datasets are presented for streamflow and eta in tables 6 and 7 respectively linear mixed effects models were employed to account for the spatiotemporal effects that cause sample correlation violating the independence assumption for the usual paired t test esfahanian et al 2017 with regard to the streamflow datasets all comparisons were found to be significantly different from each other except for the comparison of the observed dataset with the initial streamflow calibrated model this indicates that the initial calibration was able to closely replicate the observed data to the point where statistically there is no difference between them however the significant difference observed for all other models compared to the observed data indicates that those models are not as accurate when simulating streamflow this seems logical for the models calibrated via the genetic algorithm approach since there was a noticeable decrease in the statistical criteria for the streamflow calibration in these models however we did not expect this for the models calibrated using the multi variable approach since these models showed little to no change in the calibration criteria for streamflow these results indicate that even though the calibration process was able to satisfactorily calibrate streamflow there exist more inconsistencies within the final simulated streamflow when compared to the observed data when considering the comparison of streamflow simulations between the initial model and the other four models tested the significant difference makes sense and indicates that the addition of the eta calibration influenced the streamflow calibration to an extent furthermore since all of these the p values were negative the eta calibrated models all underestimated the streamflow when compared to both the observed dataset and the initial streamflow model this indicates that regardless of the calibration method used or the impact seen on the statistical criteria the eta calibrated models produced lower streamflow values on average finally the comparisons between the four et calibrated models also showed significant difference which seems understandable given the use of different et datasets and calibration process used in this study with regards to the eta datasets table 7 almost all comparisons among datasets showed significant differences except for the ssebop dataset versus the initial streamflow calibrated model column a and row c and the ssebop genetic algorithm calibrated model versus the alexi multi variable calibrated model column e and row f these two cases are rather interesting since the first comparison ssebop versus the initial streamflow calibrated model indicates that by only calibrating for streamflow it was possible to simulate eta so that it is not statistically different from the remotely sensed data meanwhile the second case ssebop genetic algorithm calibrated model versus alexi multi variable calibrated model indicates that in some cases calibrated swat model can generate comparable eta time series even though different approaches and datasets were used considering all of the other significant differences the comparison between the alexi and ssebop data column a and row b in table 7 made the most logical sense since different methodologies were used to calculate these datasets furthermore similar results to the streamflow were also seen when comparing the eta calibrated models to the remotely sensed eta datasets these observations confirm that even though these models were able to satisfactorily simulate eta values the swat simulated eta was statistically different from the remotely sensed data used to calibrate them and thus could not accurately replicate the remotely sensed data however while the streamflow comparisons showed that all of the eta calibrated swat models underestimated streamflow here it can be seen that the ssebop calibrated swat models overestimated eta values while the alexi calibrated swat models underestimated the eta values when compared to the ssebop and alexi datasets respectively in addition similar to the streamflow comparisons the four eta calibrated models were significantly different from the initial streamflow calibrated model which makes sense since all of the eta calibrated models had an increase in the statistical criteria for eta calibration compared to the initial streamflow calibrated model finally the comparisons between the four eta calibrated models columns d through f and rows e through g showed significant difference from each other except for the case of the ssebop genetic algorithm calibrated model versus the alexi multi variable calibrated model discussed previously this is reasonable since different calibration approaches and eta datasets were used 3 5 comparison of the multi variable and genetic algorithm calibrations based on the information provided in tables 4 and 5 it can be concluded that the multi variable approach used in this study was able to generate better overall swat models compared to the ga approach however if the goal of the model is to generate more accurate eta data the ga approach was able to outperform the multi variable approach this shows that depending on the purpose of the model applications different calibration techniques should be used furthermore it is to be noted that for both approaches the models built using the ssebop data were able to achieve higher performances in simulating both streamflow and eta data than the models made based on the alexi data 4 conclusions in this study two different eta calibration techniques were used to evaluate the impact of adding spatially distributed and remotely sensed eta datasets to the traditional streamflow calibration used in hydrological models both techniques multi variable and ga were able to successfully improve the eta calibration for the hydrological model using both remotely sensed eta datasets the ga technique was able to produce better eta calibrations and thus better eta simulations however this was achieved at the cost of lowering the streamflow calibrations meanwhile the multi variable technique was able to improve the eta calibration while maintaining the streamflow calibration therefore future use of these approaches should be driven by the needs of the research for example if a study is focused solely on better eta estimation the ga approach is the better option meanwhile studies focused on better simulating the entire hydrological cycle for a region should use the multi variable approach concerning the eta datasets used in this study the calibrations performed with the ssebop dataset resulted better eta estimations compared to the calibrations based on the alexi dataset for this study area therefore it is recommended that future studies should perform this analysis in other regions to better understand how these datasets compare to each other as well as evaluating the impacts of different climate variabilities e g snow cover statistical analysis of the streamflow and eta showed that the remotely sensed eta datasets were significantly different from each other which was expected moreover except for one exception all of the streamflow and eta datasets produced by the eta calibrated swat models were also significantly different from each other however all four eta calibrated models were also significantly different when compared to the remotely sensed data this indicated that while the overall model calibration was successful it was unable to closely replicate the remotely sensed data showing that there still could be additional improvements in the both in calibration process and the swat model simulations it is to be noted that the eta calibration processes used in this study only altered three parameters within the swat model this was due to temporal and computational limitations however the addition of other parameters to the calibration process such as the soil evaporation compensation factor esco could result in even better model calibrations and thus better model outputs and should be the focus of future studies in addition while adding eta calibration to the overall model calibration process was successful in this study future studies should consider additional hydrological cycle components such as remotely sensed soil moisture datasets this would allow for the development of even more realistic models and thus more accurate results for stakeholders and policy makers who rely on model outputs for managing freshwater resources acknowledgments authors would like to thank dr wade crow from usda ars hydrology and remote sensing laboratory at beltsville maryland for his help with editing the paper this work is supported by the usda national institute of food and agriculture hatch project micl02359 appendix a supplementary data supplementary data associated with this article can be found in the online version at https doi org 10 1016 j jhydrol 2017 11 009 appendix a supplementary data supplementary data 
