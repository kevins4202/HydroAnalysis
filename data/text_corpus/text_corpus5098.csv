index,text
25490,in recent years the need to reduce water pollution and improve environmental water quality has increased in this study we estimated the spatial distribution of suspended solids ss and the nitrogen to phosphorus np ratio as water quality parameters by combining three types of information satellite remote sensing data water depth and water temperature a water quality estimation method using a neural network was also developed the proposed method is effective and easy to apply as it does not use many parameters the results showed that the maximum improvements in the ss and np ratio estimates compared to the results of the fuzzy regression analysis and the conventional method were 6 mg l and 2 25 respectively in the ss estimation the learning dataset based on texture dissimilarity helped improve the accuracy the proposed method will contribute to a more detailed understanding of water quality conditions keywords remote sensing terra aster neural network algorithm water area water depth water temperature data availability data will be made available on request abbreviations bn batch normalization cnn convolutional neural network dn digital number fra fuzzy regression analysis mse mean square error nir near infrared nn neural network np nitrogen to phosphorus relu rectified linear unit sd standard deviation ss suspended solids sst sea surface temperature swir shortwave infrared tir thermal infrared vnir visible near infrared 1 introduction water is a fundamental component of ecosystems and an extremely important resource for human life water pollution is becoming a serious problem with the development of industrial activities and changes in human lifestyles the discharge of industrial wastewater e g brine degrades water quality rendering it unusable for potable water without desalination and industrial applications panagopoulos 2022 panagopoulos and giannika 2022a 2022b furthermore phytoplankton associated with eutrophication of water bodies are increasing due to the inflows from agricultural wastewater and negatively impacts water quality kim et al 2021 mattei and scardi 2022 schaeffer et al 2018 therefore measures to mitigate water pollution have become increasingly crucial in recent years the sustainable development goals sdgs proposed at the united nations summit in 2015 include objectives for reducing pollution and improving the state of aquatic environments nawaz et al 2022 continuous monitoring of the changes in aquatic environments is required to reduce water pollution generally during investigations of water conditions samples for analysis are obtained directly from the water body although this approach can provide water quality data for local areas smaller than 1 m2 it is difficult to determine the conditions of the entire water body ehmann et al 2019 huangfu et al 2020 generally remote sensing techniques that periodically and spatially obtain wide area data are useful for surveying water quality conditions dias et al 2021 grbčić et al 2022 mattei and scardi 2022 rodríguez lópez et al 2020 wang et al 2017 an empirical approach using reflectance from remote sensing and water quality data is often used to estimate water quality furthermore a regression analysis method has been applied to the remote sensing data and estimation maps have been provided deutsch and alameddine 2019 ha et al 2017 ouma et al 2020 in recent years machine learning approaches have gained attention as estimation methods zhang et al 2022 a b investigated a deep learning approach driven by a range of spectral properties to retrieve 6 year changes in water quality parameters zhang et al 2022 a b used multiple machine learning methods to estimate the chl a concentration in water and solved the problem of multicollinearity between the characteristic bands when using hyperspectral data topographical information water temperature and the meteorological conditions of water bodies are related to water quality iwasa 1990 ji et al 2017 kraemer et al 2017 o farrell et al 2021 for example water surface elevation is related to changes in the vertical water cycle due to water inflows and wind which also affect water quality additionally the water temperature and sunshine duration are factors that affect phytoplankton generation wei et al 2022 revealed that wind and lake information such as depth plays an important role with spatial differences in lakes significantly affecting nutrient loading therefore using remote sensing data as well as features based on the above mentioned elements allows for the creation of maps that consider the characteristics of the water body and improve the accuracy of water quality estimation in general water quality simulation modeling is performed based on the structure of a water body or its meteorological conditions jiang et al 2019 pyo et al 2021 rajib et al 2020 slaughter and mantel 2016 model simulations can be used to identify the factors responsible for potential future river contamination emergencies or reproduce the effects of external and in lake restoration measures on water quality dynamics andersen et al 2022 behzadi et al 2022 however highly accurate simulations require the preparation of large datasets kim et al 2021 therefore simulation models are not applicable to all water bodies pyo et al 2021 deterministic lake models have undergone continuous refinement for predicting ecosystem behavior soares and calijuri 2021 deterministic models are based on a set of differential equations that describe physical chemical and biological processes sediment water exchange and inflows and outflows providing the mechanisms underlying aquatic ecosystem processes however model applications require large volumes of data over broad ranges of time from months to decades and spatial local regional and continental scales several studies jiang et al 2019 rajib et al 2020 have investigated the combination of remote sensing data and features of a target water body to better understand water quality however few studies have targeted the water bodies in which phytoplankton occur isenstein et al 2020 deutsch and alameddine 2019 studied the temporal changes in chl a levels algal bloom incidences water clarity water temperature and reservoir water volume using the landsat thematic mapper and enhanced thematic mapper plus although statistical analyses were conducted for multiple features individual features were not combined into an input feature for model creation in a study by kärcher et al 2020 chl a nutrients and water temperatures were combined and analyzed water quality models were also created and evaluated however these models were not applied to the entire water body to analyze the water quality conditions in our previous studies kageyama et al 2016 matsui et al 2019 2021 nishida and otsuka 1995 wang et al 2012 2013 2014 2016 we estimated the water quality of lake hachiroko japan using spectral data acquired by remote sensing in these studies fuzzy regression analysis fra and fuzzy c means clustering which account for various uncertainties in the data were used to estimate the water quality of lake hachiroko furthermore studies using data obtained from active sensors and unmanned aerial vehicles have been conducted however there have been no attempts to use multiple types of information regarding water bodies as input features this study estimated the spatial distribution of suspended solids ss and nitrogen to phosphorus ratio np ratio as water quality parameters by combining three types of information satellite remote sensing data water depth and water temperature we also developed a water quality estimation method based on neural networks nns the data on lake hachiroko used for this analysis were obtained from the advanced spaceborne thermal emission and reflection radiometer aster onboard the terra satellite the water quality values as the output in the proposed method were estimated based on the fra generally water quality values are estimated by regression analysis based on remote sensing and water quality data obtained from dozens of sampling points however the accuracy of the regression analysis may decrease as remote sensing data contain various disturbances e g atmospheric effects water surface effects and noise from the measurement system and uncertainties caused by the ground resolution of the sensor fra can estimate a water quality value that considers these disturbances and ambiguities previous studies on lake hachiroko have clarified that fra can estimate the water quality value and create overall water quality estimation maps using water quality data obtained from five measurement sites therefore this study used fra to estimate the water quality values the novelty of the proposed method is associated with water temperature water depth and remote sensing data as input features alongside the water quality data obtained by the nn moreover the proposed method allows for the incorporation of the pollution status of water bodies along with the estimation of water quality and is easy to apply as it does not require the use of many parameters therefore this study contributes to a better and more detailed understanding of water quality conditions and provides a new method for water quality estimation in this study we first calculated the water depth of lake hachiroko at the time of aster data acquisition based on a depth elevation map of the lake and water surface elevation next the water temperature was calculated using aster data for the thermal infrared region subsequently using fra the estimated values of the two water quality parameters were calculated based on aster data for the visible and near infrared regions learning processing was then performed using the proposed method with aster data water depth data water temperature data and estimated values subsequently the estimated water quality maps were created finally the error between the estimation maps and measured water quality values was calculated to evaluate the accuracy of the proposed method 2 methods 2 1 study area lake hachiroko is a freshwater lake located approximately 20 km north of akita japan the lake has a surface area of 4732 ha and a water storage capacity of 132 6 km3 and consists of an east waterway a west waterway and an adjustment pond which are collectively known as lake hachiroko fig 1 the depth at the center of the lake generally ranges from approximately 3 to 8 m and the deepest point of the lake is 10 m water flows into the lake from approximately 20 rivers on the east side the pollutant load that flows into lake hachiroko mainly originates from agricultural and natural sources hachiroko environment policy office 2021 blue green algal blooms occur from july to october a summary of the water quality conditions according to experts currently investigating the lake is as follows polluted water including agricultural wastewater flows into the lake from the southern point a in fig 1 and northern drainage pump sites organic matter and nutrients from neighboring rivers flow into lakes pollution levels have increased at the mouth of the bafumigawa river point b in fig 1 because of poor water circulation the center of the adjustment pond is less polluted the lake water is replaced approximately 10 times a year according to the calculated inflow and storage volume of the lake although there is no major movement of lake water water flow may also occur due to wind 2 2 water quality data ss are insoluble particles larger than 2 μm in size that are present in water such as bacteria and algae they can impede the transmission of sunlight alter nutrient concentrations and stagnate running water by blocking waterways chawla et al 2020 in water bodies with eutrophication due to blue green algae ss can be identified via remote sensing data and has been used as a water quality parameter in many water quality estimation studies arango and nairn 2020 neves et al 2021 the np ratio indicates the ratio of nitrogen to phosphorus concentrations in the phytoplankton and water furthermore the np ratio is used to determine whether the water is suitable for the growth of blue green algae francy et al 2020 green and wang 2008 the two water quality parameters used for this analysis i e ss and np ratio were obtained at five sites as shown in fig 1 table 1 lists the water quality data used in the analysis hachiroko environment policy office 2021 ministry of the environment 2021 the water quality data acquisition dates were no more than one week apart from the aster acquisition dates there was no rainfall when the water quality data measurements and remote sensing data collection were conducted japan meteorological agency 2022 2 3 terra aster the aster onboard the terra satellite is a multispectral imager abrams et al 2002 the aster data can be used to create detailed surface temperature emissivity and altitude maps aster has three spectral bands in the visible near infrared vnir region six in the shortwave infrared swir region and five in the thermal infrared tir region since april 2008 aster swir detectors have not been functioning because of the anomalously high swir detector temperatures therefore the data were obtained from the vnir and tir regions for this analysis table s1 in the supplemental notes provides an overview of the aster data used in the analysis the data were recorded on june 3 2004 august 5 2012 september 13 2012 may 26 2015 september 15 2015 and may 12 2019 the aster images obtained on these dates are referred to as astera asterf aster data were downloaded from the landbrowser digital architecture research center 2021 and the meti aist data archive system geological survey of japan 2021 2 4 normalized difference vegetation index experts investigating the water quality of lake hachiroko have found that blue green algae occur mainly in summer while the phytoplankton genus aphanizomenon appears in may and june to analyze such vegetation the normalized difference vegetation index ndvi an indicator of vegetation distribution and activity was used in this study the ndvi is calculated based on the difference in reflectance between visible red r and near infrared nir light which is highly reflective of vegetation the formula for ndvi is as follows 1 n d v i n i r r n i r r where n i r and r are the values obtained from the nir and r bands respectively the resulting ndvi values range from 1 to 1 the ndvi is positive when phytoplankton occur at the surface of the water and negative otherwise rees et al 2001 2 5 water depth water depth was calculated based on a depth elevation map of the lake and the water surface elevation at the time of aster data acquisition fig 2 shows the depth elevation map of lake hachiroko the water surface elevation near the floodgate of lake hachiroko at the times of aster data acquisition were 0 92 june 3 2004 0 77 aug 5 2012 0 56 sept 13 2012 0 95 may 26 2015 0 55 sept 15 2015 and 0 85 may 12 2019 the depth elevation map used in the analysis was provided by the hachiroko environment policy office environmental management division living and environment department akita prefecture japan the surface elevation of water near the floodgate was provided by the akita regional development and promotion bureau agriculture and forestry department hachirogata mainstay facilities administration office 2 6 water temperature sea surface temperature sst algorithms can be used to calculate water temperature using remote sensing data and to obtain the water temperature of inland or bay waters matsuoka et al 2011 tavares et al 2019 in this study the water temperature of lake hachiroko was calculated using a multichannel sst mcsst algorithm matsunaga 1996 for five bands of data acquired in the aster tir region 2 m c s s t a s t e r 1 07 t 1 0 49 t 2 1 13 t 3 0 78 t 4 0 32 t 5 1 16 where m c s s t a s t e r is the estimated water temperature and t 1 t 5 are luminance values observed in the aster tir region tir 1 5 converted to degrees celsius various values for the coefficients in each term of m c s s t a s t e r have been examined previously matsunaga 1996 errors in the measured water temperature were calculated for asterb and astere which had the same water temperature measurement and aster data acquisition dates the coefficient of m c s s t a s t e r used in the analysis had the smallest error in addition the average water temperature error was 0 03 c standard deviation 1 46 c water temperature measurements were provided by the hachiroko environment policy office environmental management division living and environment department akita prefecture 2 7 flow of the proposed method fig 3 shows a flowchart of the proposed method first aster visible green red g r and nir data were preprocessed using geometric correction atmospheric correction and mask processing subsequently the estimated values of the water quality parameters were calculated using fra with aster data and measured water quality data water quality values were estimated for lake hachiroko and used as the training output value in the nn finally an nn was trained using aster data water depth water temperature and estimated values 2 8 preprocessing first geometric correction takagi and shimoda 2004 was used to correct the satellite remote sensing data by expanding and contracting the data to match the referenced data geometric correction was applied to the aster data to match the pixel coordinates for the water depth and temperature in each image asterc was used as the reference data for the correction as the dataset contained low water level data and no data were missing subsequently an atmospheric correction method using a single scattering approximation takagi and shimoda 2004 was used to correct the visible region based on nir data that had a low reflectance from the water surface finally mask processing was applied to extract water bodies for analysis as some parts of the water surface in astere were covered with clouds cloud areas were also removed through this process fig 4 shows the preprocessing results 2 9 fuzzy regression analysis water quality estimations generally use water quality data collected from many points however fra can create a water quality estimation map using less water quality data wang et al 2013 the fra procedure consists of fuzzy regression model calculation and fuzzy level slice processing 2 9 1 fuzzy regression model the fuzzy regression model ishibuchi 1992 is based on fuzzy set theory it considers the coefficients of a linear regression as fuzzy numbers to which the uncertainties of the input and output can be associated in this study the fra used the aster and water quality data fuzzy numbers were specified as water quality measurement sites from astera asterf and it was assumed that the digital number dn values corresponded to areas around the measurement sites additionally a triangular fuzzy number was adapted based on the average value and variance calculated from 50 randomly selected dn values around the water quality measurement sites matsui et al 2019 wang et al 2013 the min and max linear programming problems were solved using interval data and solutions were obtained according to each fuzzy number the min and max problems required linear regression models with minimum and maximum widths which were included in all the interval data however no solution was obtained for the max problem in this analysis therefore the results of the min problem are used 2 9 2 fuzzy level slice processing the fuzzy regression line was sliced at preset levels slice levels of the ss and np ratio values subsequently the degree of fit of the input dn values to each slice level was outputted based on the fuzzy regression model finally the estimated value was calculated by computing the weighted averages of the degrees of fit and the slice levels the ss slice levels used in this study were set based on the environmental standards for ss in lakes ministry of the environment 2019 and a previous study wang et al 2013 because the environmental standard value for the np ratio has not yet been set we used the value based on the highest and lowest np ratios in the analyzed data specifically level 1 was set under the lowest np ratio and level 6 was set higher than the highest np ratio levels 2 5 were set in equally spaced increments of 2 94 the values are presented in supplementary table s2 additionally simplified fuzzy reasoning mizumoto 1992 was employed 2 10 learning processing by neural networks 2 10 1 overview a feedforward nn is included in the proposed method many studies have employed nns to estimate water quality elsayed et al 2021 isiyaka et al 2019 fig 5 presents an overview of the nn learning process learning was performed using nns with the input features being aster visible and nir data water depth and water temperature in addition the inputs included all target and their surrounding pixels in each dataset the output was learned using the estimated ss or np ratio values calculated using fra the network learns the estimated values by considering the water depth and temperature information as well as the dn values in each target and its surrounding pixels however accuracy may not be guaranteed during nn learning based only on the water quality data for the five points obtained from astera asterf thus much of the data were learned based on the fuzzy regression analysis results the proposed method was trained using three types of input data aster data water depth and water temperature for the estimated values obtained by fra a hold out was performed for learning for example when astera was used as the test data asterb asterf was used as the training data the training data were sliced into an input size 3 3 and shuffled before learning ninety percent of the data were used for training and ten percent were used for validation the batch size was 256 the learning data were normalized as follows 3 x x μ 2 σ where x is the value after normalization x is the value prior to normalization and μ and σ are the average and standard deviation of each feature respectively 2 10 2 network configurations nn configurations were selected prior to the learning process the activation functions used to calculate the values of the hidden and output layers were determined from linear rectified linear unit relu sigmoid and tanh activation functions goodfellow et al 2016 okatani 2015 the network structure was designed to include the number of layers in the network and the number of units in each layer goodfellow et al 2016 in general implementing a dropout layer can prevent data overfitting and improve generalization performance therefore the configurations were examined to determine the best combination of parameters as shown in table s3 in the supplemental notes the configuration that minimized the mean square error mse was investigated 4 m s e n 1 n y ˆ i y i 2 n where n is the number of data points y ˆ i is the i th i 1 2 n predicted output result and y i is the i th correct output result first the number of hidden layers was set to 1 and the mses of each combination of the number of units in the hidden layers activation function and dropout rate were verified with 1000 epochs the combination with the lowest mse was then used to investigate the mses of hidden layers 1 4 cross validation of astera f was performed to select configurations adaptive moment estimation was adopted as the optimization algorithm goodfellow et al 2016 the configuration with the lowest mse is shown in fig 6 and has the following structure number of hidden layers 3 number of units in hidden layers 900 activation function a hidden layer relu b output layer linear dropout layer 0 5 2 11 accuracy of water quality estimation to evaluate the accuracy of the water quality estimations the differences between the estimated ss values and np ratios obtained using the proposed method and the measured values of the two parameters from each measurement point were calculated first each water quality measurement point was identified on the map using the estimated values subsequently 50 points with estimated values were sampled around the measurement points finally the error between the average value of the sampling points and measured values estimated error was calculated 3 results and discussion 3 1 accuracy of estimation results table 2 lists the estimated errors of the proposed method and fra as the degree of ss was differed by 10 mg l when the level differed by one level between levels 3 and 6 the estimation error of 10 mg l was set as the criterion value for ss the criterion value for the np ratio was set at 2 94 which is the increment used in the slice level processing for fra the minimum estimated errors for ss and the np ratio via the proposed method were 3 mg l in asterd ndvi standard deviation sd 2 0 and 0 73 in asterf ndvi sd 0 64 respectively the maximum improvements in the ss and np ratio estimates compared to the fra results were 6 mg l astera nir and 2 25 asterf red respectively in the errors estimated via the proposed method some items increased the estimated errors or did not significantly decrease in particular the average estimated errors for all features of asterb using the proposed method and fra exceeded 19 mg l the ss values on august 1 2012 were higher than the maximum slice level value 35 mg l set in this study at the ogata bridge east adjustment pond and center of the lake as the fra output values were estimated based on slice level values the values above the slice level could not be output therefore when the ss value in the target water body is much larger than the maximum value of the preset slice level e g 10 mg l or more new slice levels must be set for estimation fig 7 shows the estimated ss maps for the nir of asterc obtained using the proposed method and fra the average estimated error of the proposed method obtained using fra was 4 mg l sd 2 1 which was lower than 10 mg l moreover the average estimated error for the nir of astera obtained via fra was also lower than 10 mg l in the map obtained using the proposed method the degree of pollution was highest in the areas from the ogata bridge and the west adjustment pond to the floodgate where the water depth was low 1 m the degree of pollution was lowest at the center of the lake where the water depth was greater 6 m vertical circulation of lake water caused by wind and water inflow is more frequent in shallow areas than in deep areas it is assumed that fra could not consider the difference in the degree of ss due to the depth as it was estimated only by the spectral information from the water surface the proposed method learns using water depth information and dn values from the water surface therefore the proposed method improves estimation accuracy as it can learn the differences in pollution status based on the depth of the lake 3 2 conditions for improving ss estimation accuracy the average estimated error of the ss maps in astere and asterf using the proposed method did not improve for all features in particular the error for nir in asterf was 14 mg l which is very large compared with the error obtained via fra although the measured ss values were between 10 and 16 mg l the estimated ss values of the proposed method were between 25 and 35 mg l therefore the proposed method learns that the estimated ss value should be larger than it should be based on these results an analysis of the conditions is required to improve the accuracy of the asterf estimations to understand the combinations of aster data that improved the accuracy of ss estimation the number of combinations included in the learning data was varied and the estimated errors were calculated specifically the estimated errors were calculated when i four datasets were selected from astera astere as the learning datasets a total of five patterns and ii three datasets were selected from astera astere as the learning datasets a total of ten patterns when three datasets were selected the estimated errors were lower than 10 mg l the combination of astera asterb and asterc as well as the combination of astera asterc and astere showed average estimated errors of 4 and 5 mg l respectively fig 8 shows the estimated ss maps obtained using the aforementioned datasets texture analysis was conducted to analyze the characteristics of astera and asterc which were common among the combinations in particular dissimilarity was calculated based on the co occurrence matrix of a 3 3 water surface region for nir the dissimilarity increases when the dn value of the pixel of interest is higher than the dn values of other pixels in the region and when the frequency of such occurrences is high takagi and shimoda 2004 the dissimilarity decreases when the dn values in the region are uniform and increases when the region is characterized by a mixture of different values fig 9 shows the dissimilarity maps for astera asterc and asterf a total of 500 points were randomly sampled from each dataset and distribution graphs were created the distributions of dissimilarity for astera asterc and asterf were similar fig 10 therefore the creation of a learning dataset based on dissimilarity improved ss estimation accuracy the nir band used for texture analysis has low reflectance from water reflectance spectra of sediments and phytoplankton on the water surface were acquired dissimilarity reflects the inflow of sediment from nearby rivers and the occurrence of phytoplankton remaining in the water surface layer therefore it is necessary to create a learning dataset for ss estimation that considers the amount of inflow from rivers and occurrence of phytoplankton 3 3 conditions for improving the accuracy of np ratio estimation asterb and astere had minimum estimation errors of 2 37 sd 1 62 and 2 24 sd 2 54 respectively which were 1 35 units larger than the minimum estimation errors of the other aster datasets of all the water quality measurement points obtained in asterb and astere those in the east adjustment pond had the lowest np ratio however the ss values at these points were higher than the environmental standards table 1 experts investigating the water quality of lake hachiroko have found that blue green algae tend to grow when the np ratio decreases therefore during the time of asterb and astere data acquisition the ss value in the lake increased with the growth of blue green algae therefore the np ratio in asterb and astere could not be learned well as it tended to differ from that of the other aster data based on these results datasets excluding asterb and astere were created and used for holdout training using the proposed method table 3 lists the errors estimated for the np ratio using the constructed dataset the estimated error for the ndvi of asterf was 0 70 sd 0 83 which was the smallest value observed when estimating the np ratio fig 11 shows the results estimated using the constructed dataset for the ndvi of the asterf compared with the results obtained by fra the estimation accuracy of the proposed method was the most improved for astera and asterf to analyze the conditions that contributed to this improvement in accuracy the distribution of the fra estimated np ratios used as training data for the proposed method was analyzed fig 12 the distributions of the np ratios for astera and asterf were similar whereas the np ratios for asterc were lower than those for astera and asterf moreover the distribution of the asterc showed two peaks because the asterc was obtained in september which is a season where blue green algae occur frequently it can be predicted that the number of phytoplankton species was different compared to the data in may and june the spread of the np ratio distribution for asterd was larger than those for astera and asterf and the distributions were dissimilar the above mentioned results suggest that under the proposed method a learning dataset with similar np ratio distributions improves the accuracy of the np ratio estimation because the physical composition of phytoplankton differs among species the np ratio conditions of water bodies are related to the phytoplankton species for example when the np ratio is approximately 4 the environment is predicted to be optimal for microcystis which produces blue green algae therefore to improve the estimation accuracy of the np ratio it is necessary to create a dataset with data on the same species of phytoplankton 3 4 discussion previous studies on water quality estimation using satellite remote sensing have applied empirical approaches or water quality simulation modeling however as it is necessary to prepare a large dataset e g water quality data from dozens of measurement points to create a highly accurate model these methods are not applicable to all water bodies in our previous studies we analyzed the water quality of lake hachiroko using a method based on the fuzzy theory the results showed that it is possible to understand the water quality distribution using water quality data from the five measurement points however the information obtained from remote sensing data was used as the input data there have not yet been attempts to use multiple types of information regarding water bodies as input features moreover a water quality distribution map based on the relationship between multiple periods has not yet been created in this study three types of information remote sensing data water depth and water temperature were combined using an nn moreover the nn extracts features from multiple time periods the results of the proposed method showed improved accuracy compared to the results of the fra furthermore in the ss estimation the creation of a learning dataset based on an analysis of the texture dissimilarity contributed to an improved accuracy in the np ratio estimation the use of the learning dataset with similar np ratio distributions improved the estimation accuracy the above points are the differences between this study and the previous studies in this study the results of the proposed method improved the accuracy of ss and np ratio estimations compared with fra the estimation of the ss and np ratios can be used to predict the occurrence of blue green algae as discussed in section 3 3 the proposed method is useful for estimating the phytoplankton situation and may contribute to a better understanding of the ecology of the aquatic environment however this analysis has not yet been able to determine which phytoplankton species occurred at the time of water quality data acquisition the usefulness of estimating the phytoplankton situation should be examined in more detail the analysis of temporal water quality changes can identify the main factors affecting water quality changes thus contributing to measures against water pollution krishnaraj honnasiddaiah 2022 liu et al 2021 yao et al 2022 analyzed temporal water quality changes water area water level and water storage using multi source satellite remote sensing data and 10 years of data extracted from active and passive sensors this study identified the impact of changes in the water area on human lifestyle and the factors behind changes in water storage saberioon et al 2020 analyzed the temporal changes in chlorophyll a and total ss in inland waters using sentinel 2 the results showed that implementing machine learning algorithms to predict water quality parameters improved the prediction accuracy of complex spectral relationships and interactions in this study spatial water quality estimation was conducted however temporal water quality changes were not analyzed therefore it is necessary to analyze temporal changes in water quality to evaluate the usefulness of the proposed method to analyze this more data collection and applicability to other satellite data must be evaluated to increase the amount of data analyzed convolutional nns cnns perform excellently in image processing a cnn network has convolution and pooling layers and by repeating these layers features are extracted from the input data studies have been conducted on water quality estimation using cnn ilteralp et al 2022 pyo et al 2022 pyo et al 2022 used cnn models to quantitatively and qualitatively analyze algal phenomena even in the proposed method of this study it is important to architect the network with a cnn to improve accuracy however to achieve a high estimation accuracy increasing the amount of data or using high resolution data is required for better feature extraction using a cnn ahmed et al 2022 yang et al 2022 in the future it will be necessary to consider increasing the amount of data and using high resolution 1 m remote sensing data 4 conclusions in this study we discussed a novel method for estimating the ss and np ratio using nns and input features including aster data water depth and water temperature the proposed method allows for water quality estimations to also consider the pollution status of water bodies and is easy to apply because it does not require the use of many parameters the learned models were applied to create estimated water quality maps for the entire water body of lake hachiroko and the maps were compared with actual water quality conditions to evaluate the usefulness of the proposed method furthermore the conditions that contributed to improving the accuracy of the ss and np ratio estimations were investigated the results are summarized as follows 1 the proposed method could estimate the spatial distribution of the ss and np ratios moreover the errors between the estimated value via the proposed method and the measured value for ss and the np ratio were 3 mg l and 0 70 respectively furthermore the maximum improvements in the ss and np ratio estimates compared with the fra results were 6 mg l and 2 25 respectively 2 in the results obtained using the proposed method the highest degree of pollution was observed in areas with shallow water 1 m the degree of pollution was lowest at a deeper water depth 6 m vertical circulation of lake water caused by wind and water inflow is more frequent in shallow areas than in deep areas the fra could not consider the difference in the degree of ss due to the depth because it was estimated only by the spectral information from the water surface in contrast the proposed method can learn the difference in contamination status depending on the depth of the lake which improves the estimation accuracy 3 texture analysis for nir of aster data was performed and the condition of the learning dataset that improved the accuracy of the ss estimation was clarified in ss estimation using the proposed method the creation of a learning dataset based on an analysis of texture dissimilarity contributed to improved accuracy to improve the estimation accuracy of ss it is necessary to construct a dataset that considers the inflow of sediment from nearby rivers and the occurrence state of phytoplankton that remain on the surface of the water 4 the accuracy of the np ratio was improved by excluding the data with different np ratio trends from the dataset the dataset with the best results was analyzed as a result in the np ratio estimation using the proposed method the use of the learning dataset with similar np ratio distributions improved the estimation accuracy therefore to improve the estimation accuracy of the np ratio it is necessary to create a dataset with data on the same species of phytoplankton furthermore dataset creation must consider that the number of phytoplankton species occurring differs depending on the time of data acquisition the proposed method can help estimate the water quality conditions of water bodies where large scale datasets are not available furthermore the results of this study are a new finding regarding the conditions of dataset creation for estimating the water quality of ss and np ratios using nn the proposed method is useful for estimating the phytoplankton situation in water bodies and may contribute to a better understanding of the ecology of the water environment this study has the following limitations i this study was unable to determine which species of phytoplankton occurred at the time of data acquisition and ii the estimated error was large when the ss value in the target water body was larger than the maximum value of the pre set slice level therefore the usefulness of phytoplankton estimation methods should be examined in further detail in the future furthermore future analyses should be conducted by setting new slice levels finally we clarified that datasets based on dissimilarity improved the accuracy of the ss estimation however only one dataset acquired in may was used as test data therefore in the future we will increase the amount of test data and further evaluate the usefulness of the datasets based on dissimilarity in this study we developed a novel water quality estimation method that combines remote sensing data and information obtained from the water body and predicts the overall water quality conditions in the water body as a future study in this field temporal water quality analysis and water quality estimation using cnn must be conducted for this purpose it is necessary to improve the proposed method by increasing the amount of remote sensing data used for analysis and high resolution data temporal water quality analysis and estimation by cnn will contribute to the improvement of estimation accuracy in addition to the identification of the causes of water pollution software and data availability the codes that were used for estimating water quality using python language version 3 7 based on the tensorflow library can be found in github https github com kaimatsui 19006 hachiroko202207 neuralnetworkestimation this repository was created by kai matsui e mail kmatsui19006 gmail com in 2022 and has program cords 16 kb and sample data 200 mb the experimental environment is as follows os windows 10 pro cpu intel r core tm i7 9700 3 00 ghz ram 16 00 gb gpu nvidia geforce rtx 2060 the analyzed remote sensing data can be downloaded free of charge from the landbrowser digital architecture research center 2021 and the meti aist data archive system geological survey of japan 2021 the water quality data in the study area used for analysis were obtained from the water environment comprehensive information site ministry of the environment 2021 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors would like to thank the hachiroko environment policy office environmental management division living and environment department akita prefectural for providing the data used in this study and for their knowledge of the study area the authors would also like to thank the akita regional development and promotional bureau agriculture and forestry department and hachirogata mainstay facilities administration office for providing data for this study this research did not receive any specific grants from funding agencies in the public commercial or not for profit sectors appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105584 
25490,in recent years the need to reduce water pollution and improve environmental water quality has increased in this study we estimated the spatial distribution of suspended solids ss and the nitrogen to phosphorus np ratio as water quality parameters by combining three types of information satellite remote sensing data water depth and water temperature a water quality estimation method using a neural network was also developed the proposed method is effective and easy to apply as it does not use many parameters the results showed that the maximum improvements in the ss and np ratio estimates compared to the results of the fuzzy regression analysis and the conventional method were 6 mg l and 2 25 respectively in the ss estimation the learning dataset based on texture dissimilarity helped improve the accuracy the proposed method will contribute to a more detailed understanding of water quality conditions keywords remote sensing terra aster neural network algorithm water area water depth water temperature data availability data will be made available on request abbreviations bn batch normalization cnn convolutional neural network dn digital number fra fuzzy regression analysis mse mean square error nir near infrared nn neural network np nitrogen to phosphorus relu rectified linear unit sd standard deviation ss suspended solids sst sea surface temperature swir shortwave infrared tir thermal infrared vnir visible near infrared 1 introduction water is a fundamental component of ecosystems and an extremely important resource for human life water pollution is becoming a serious problem with the development of industrial activities and changes in human lifestyles the discharge of industrial wastewater e g brine degrades water quality rendering it unusable for potable water without desalination and industrial applications panagopoulos 2022 panagopoulos and giannika 2022a 2022b furthermore phytoplankton associated with eutrophication of water bodies are increasing due to the inflows from agricultural wastewater and negatively impacts water quality kim et al 2021 mattei and scardi 2022 schaeffer et al 2018 therefore measures to mitigate water pollution have become increasingly crucial in recent years the sustainable development goals sdgs proposed at the united nations summit in 2015 include objectives for reducing pollution and improving the state of aquatic environments nawaz et al 2022 continuous monitoring of the changes in aquatic environments is required to reduce water pollution generally during investigations of water conditions samples for analysis are obtained directly from the water body although this approach can provide water quality data for local areas smaller than 1 m2 it is difficult to determine the conditions of the entire water body ehmann et al 2019 huangfu et al 2020 generally remote sensing techniques that periodically and spatially obtain wide area data are useful for surveying water quality conditions dias et al 2021 grbčić et al 2022 mattei and scardi 2022 rodríguez lópez et al 2020 wang et al 2017 an empirical approach using reflectance from remote sensing and water quality data is often used to estimate water quality furthermore a regression analysis method has been applied to the remote sensing data and estimation maps have been provided deutsch and alameddine 2019 ha et al 2017 ouma et al 2020 in recent years machine learning approaches have gained attention as estimation methods zhang et al 2022 a b investigated a deep learning approach driven by a range of spectral properties to retrieve 6 year changes in water quality parameters zhang et al 2022 a b used multiple machine learning methods to estimate the chl a concentration in water and solved the problem of multicollinearity between the characteristic bands when using hyperspectral data topographical information water temperature and the meteorological conditions of water bodies are related to water quality iwasa 1990 ji et al 2017 kraemer et al 2017 o farrell et al 2021 for example water surface elevation is related to changes in the vertical water cycle due to water inflows and wind which also affect water quality additionally the water temperature and sunshine duration are factors that affect phytoplankton generation wei et al 2022 revealed that wind and lake information such as depth plays an important role with spatial differences in lakes significantly affecting nutrient loading therefore using remote sensing data as well as features based on the above mentioned elements allows for the creation of maps that consider the characteristics of the water body and improve the accuracy of water quality estimation in general water quality simulation modeling is performed based on the structure of a water body or its meteorological conditions jiang et al 2019 pyo et al 2021 rajib et al 2020 slaughter and mantel 2016 model simulations can be used to identify the factors responsible for potential future river contamination emergencies or reproduce the effects of external and in lake restoration measures on water quality dynamics andersen et al 2022 behzadi et al 2022 however highly accurate simulations require the preparation of large datasets kim et al 2021 therefore simulation models are not applicable to all water bodies pyo et al 2021 deterministic lake models have undergone continuous refinement for predicting ecosystem behavior soares and calijuri 2021 deterministic models are based on a set of differential equations that describe physical chemical and biological processes sediment water exchange and inflows and outflows providing the mechanisms underlying aquatic ecosystem processes however model applications require large volumes of data over broad ranges of time from months to decades and spatial local regional and continental scales several studies jiang et al 2019 rajib et al 2020 have investigated the combination of remote sensing data and features of a target water body to better understand water quality however few studies have targeted the water bodies in which phytoplankton occur isenstein et al 2020 deutsch and alameddine 2019 studied the temporal changes in chl a levels algal bloom incidences water clarity water temperature and reservoir water volume using the landsat thematic mapper and enhanced thematic mapper plus although statistical analyses were conducted for multiple features individual features were not combined into an input feature for model creation in a study by kärcher et al 2020 chl a nutrients and water temperatures were combined and analyzed water quality models were also created and evaluated however these models were not applied to the entire water body to analyze the water quality conditions in our previous studies kageyama et al 2016 matsui et al 2019 2021 nishida and otsuka 1995 wang et al 2012 2013 2014 2016 we estimated the water quality of lake hachiroko japan using spectral data acquired by remote sensing in these studies fuzzy regression analysis fra and fuzzy c means clustering which account for various uncertainties in the data were used to estimate the water quality of lake hachiroko furthermore studies using data obtained from active sensors and unmanned aerial vehicles have been conducted however there have been no attempts to use multiple types of information regarding water bodies as input features this study estimated the spatial distribution of suspended solids ss and nitrogen to phosphorus ratio np ratio as water quality parameters by combining three types of information satellite remote sensing data water depth and water temperature we also developed a water quality estimation method based on neural networks nns the data on lake hachiroko used for this analysis were obtained from the advanced spaceborne thermal emission and reflection radiometer aster onboard the terra satellite the water quality values as the output in the proposed method were estimated based on the fra generally water quality values are estimated by regression analysis based on remote sensing and water quality data obtained from dozens of sampling points however the accuracy of the regression analysis may decrease as remote sensing data contain various disturbances e g atmospheric effects water surface effects and noise from the measurement system and uncertainties caused by the ground resolution of the sensor fra can estimate a water quality value that considers these disturbances and ambiguities previous studies on lake hachiroko have clarified that fra can estimate the water quality value and create overall water quality estimation maps using water quality data obtained from five measurement sites therefore this study used fra to estimate the water quality values the novelty of the proposed method is associated with water temperature water depth and remote sensing data as input features alongside the water quality data obtained by the nn moreover the proposed method allows for the incorporation of the pollution status of water bodies along with the estimation of water quality and is easy to apply as it does not require the use of many parameters therefore this study contributes to a better and more detailed understanding of water quality conditions and provides a new method for water quality estimation in this study we first calculated the water depth of lake hachiroko at the time of aster data acquisition based on a depth elevation map of the lake and water surface elevation next the water temperature was calculated using aster data for the thermal infrared region subsequently using fra the estimated values of the two water quality parameters were calculated based on aster data for the visible and near infrared regions learning processing was then performed using the proposed method with aster data water depth data water temperature data and estimated values subsequently the estimated water quality maps were created finally the error between the estimation maps and measured water quality values was calculated to evaluate the accuracy of the proposed method 2 methods 2 1 study area lake hachiroko is a freshwater lake located approximately 20 km north of akita japan the lake has a surface area of 4732 ha and a water storage capacity of 132 6 km3 and consists of an east waterway a west waterway and an adjustment pond which are collectively known as lake hachiroko fig 1 the depth at the center of the lake generally ranges from approximately 3 to 8 m and the deepest point of the lake is 10 m water flows into the lake from approximately 20 rivers on the east side the pollutant load that flows into lake hachiroko mainly originates from agricultural and natural sources hachiroko environment policy office 2021 blue green algal blooms occur from july to october a summary of the water quality conditions according to experts currently investigating the lake is as follows polluted water including agricultural wastewater flows into the lake from the southern point a in fig 1 and northern drainage pump sites organic matter and nutrients from neighboring rivers flow into lakes pollution levels have increased at the mouth of the bafumigawa river point b in fig 1 because of poor water circulation the center of the adjustment pond is less polluted the lake water is replaced approximately 10 times a year according to the calculated inflow and storage volume of the lake although there is no major movement of lake water water flow may also occur due to wind 2 2 water quality data ss are insoluble particles larger than 2 μm in size that are present in water such as bacteria and algae they can impede the transmission of sunlight alter nutrient concentrations and stagnate running water by blocking waterways chawla et al 2020 in water bodies with eutrophication due to blue green algae ss can be identified via remote sensing data and has been used as a water quality parameter in many water quality estimation studies arango and nairn 2020 neves et al 2021 the np ratio indicates the ratio of nitrogen to phosphorus concentrations in the phytoplankton and water furthermore the np ratio is used to determine whether the water is suitable for the growth of blue green algae francy et al 2020 green and wang 2008 the two water quality parameters used for this analysis i e ss and np ratio were obtained at five sites as shown in fig 1 table 1 lists the water quality data used in the analysis hachiroko environment policy office 2021 ministry of the environment 2021 the water quality data acquisition dates were no more than one week apart from the aster acquisition dates there was no rainfall when the water quality data measurements and remote sensing data collection were conducted japan meteorological agency 2022 2 3 terra aster the aster onboard the terra satellite is a multispectral imager abrams et al 2002 the aster data can be used to create detailed surface temperature emissivity and altitude maps aster has three spectral bands in the visible near infrared vnir region six in the shortwave infrared swir region and five in the thermal infrared tir region since april 2008 aster swir detectors have not been functioning because of the anomalously high swir detector temperatures therefore the data were obtained from the vnir and tir regions for this analysis table s1 in the supplemental notes provides an overview of the aster data used in the analysis the data were recorded on june 3 2004 august 5 2012 september 13 2012 may 26 2015 september 15 2015 and may 12 2019 the aster images obtained on these dates are referred to as astera asterf aster data were downloaded from the landbrowser digital architecture research center 2021 and the meti aist data archive system geological survey of japan 2021 2 4 normalized difference vegetation index experts investigating the water quality of lake hachiroko have found that blue green algae occur mainly in summer while the phytoplankton genus aphanizomenon appears in may and june to analyze such vegetation the normalized difference vegetation index ndvi an indicator of vegetation distribution and activity was used in this study the ndvi is calculated based on the difference in reflectance between visible red r and near infrared nir light which is highly reflective of vegetation the formula for ndvi is as follows 1 n d v i n i r r n i r r where n i r and r are the values obtained from the nir and r bands respectively the resulting ndvi values range from 1 to 1 the ndvi is positive when phytoplankton occur at the surface of the water and negative otherwise rees et al 2001 2 5 water depth water depth was calculated based on a depth elevation map of the lake and the water surface elevation at the time of aster data acquisition fig 2 shows the depth elevation map of lake hachiroko the water surface elevation near the floodgate of lake hachiroko at the times of aster data acquisition were 0 92 june 3 2004 0 77 aug 5 2012 0 56 sept 13 2012 0 95 may 26 2015 0 55 sept 15 2015 and 0 85 may 12 2019 the depth elevation map used in the analysis was provided by the hachiroko environment policy office environmental management division living and environment department akita prefecture japan the surface elevation of water near the floodgate was provided by the akita regional development and promotion bureau agriculture and forestry department hachirogata mainstay facilities administration office 2 6 water temperature sea surface temperature sst algorithms can be used to calculate water temperature using remote sensing data and to obtain the water temperature of inland or bay waters matsuoka et al 2011 tavares et al 2019 in this study the water temperature of lake hachiroko was calculated using a multichannel sst mcsst algorithm matsunaga 1996 for five bands of data acquired in the aster tir region 2 m c s s t a s t e r 1 07 t 1 0 49 t 2 1 13 t 3 0 78 t 4 0 32 t 5 1 16 where m c s s t a s t e r is the estimated water temperature and t 1 t 5 are luminance values observed in the aster tir region tir 1 5 converted to degrees celsius various values for the coefficients in each term of m c s s t a s t e r have been examined previously matsunaga 1996 errors in the measured water temperature were calculated for asterb and astere which had the same water temperature measurement and aster data acquisition dates the coefficient of m c s s t a s t e r used in the analysis had the smallest error in addition the average water temperature error was 0 03 c standard deviation 1 46 c water temperature measurements were provided by the hachiroko environment policy office environmental management division living and environment department akita prefecture 2 7 flow of the proposed method fig 3 shows a flowchart of the proposed method first aster visible green red g r and nir data were preprocessed using geometric correction atmospheric correction and mask processing subsequently the estimated values of the water quality parameters were calculated using fra with aster data and measured water quality data water quality values were estimated for lake hachiroko and used as the training output value in the nn finally an nn was trained using aster data water depth water temperature and estimated values 2 8 preprocessing first geometric correction takagi and shimoda 2004 was used to correct the satellite remote sensing data by expanding and contracting the data to match the referenced data geometric correction was applied to the aster data to match the pixel coordinates for the water depth and temperature in each image asterc was used as the reference data for the correction as the dataset contained low water level data and no data were missing subsequently an atmospheric correction method using a single scattering approximation takagi and shimoda 2004 was used to correct the visible region based on nir data that had a low reflectance from the water surface finally mask processing was applied to extract water bodies for analysis as some parts of the water surface in astere were covered with clouds cloud areas were also removed through this process fig 4 shows the preprocessing results 2 9 fuzzy regression analysis water quality estimations generally use water quality data collected from many points however fra can create a water quality estimation map using less water quality data wang et al 2013 the fra procedure consists of fuzzy regression model calculation and fuzzy level slice processing 2 9 1 fuzzy regression model the fuzzy regression model ishibuchi 1992 is based on fuzzy set theory it considers the coefficients of a linear regression as fuzzy numbers to which the uncertainties of the input and output can be associated in this study the fra used the aster and water quality data fuzzy numbers were specified as water quality measurement sites from astera asterf and it was assumed that the digital number dn values corresponded to areas around the measurement sites additionally a triangular fuzzy number was adapted based on the average value and variance calculated from 50 randomly selected dn values around the water quality measurement sites matsui et al 2019 wang et al 2013 the min and max linear programming problems were solved using interval data and solutions were obtained according to each fuzzy number the min and max problems required linear regression models with minimum and maximum widths which were included in all the interval data however no solution was obtained for the max problem in this analysis therefore the results of the min problem are used 2 9 2 fuzzy level slice processing the fuzzy regression line was sliced at preset levels slice levels of the ss and np ratio values subsequently the degree of fit of the input dn values to each slice level was outputted based on the fuzzy regression model finally the estimated value was calculated by computing the weighted averages of the degrees of fit and the slice levels the ss slice levels used in this study were set based on the environmental standards for ss in lakes ministry of the environment 2019 and a previous study wang et al 2013 because the environmental standard value for the np ratio has not yet been set we used the value based on the highest and lowest np ratios in the analyzed data specifically level 1 was set under the lowest np ratio and level 6 was set higher than the highest np ratio levels 2 5 were set in equally spaced increments of 2 94 the values are presented in supplementary table s2 additionally simplified fuzzy reasoning mizumoto 1992 was employed 2 10 learning processing by neural networks 2 10 1 overview a feedforward nn is included in the proposed method many studies have employed nns to estimate water quality elsayed et al 2021 isiyaka et al 2019 fig 5 presents an overview of the nn learning process learning was performed using nns with the input features being aster visible and nir data water depth and water temperature in addition the inputs included all target and their surrounding pixels in each dataset the output was learned using the estimated ss or np ratio values calculated using fra the network learns the estimated values by considering the water depth and temperature information as well as the dn values in each target and its surrounding pixels however accuracy may not be guaranteed during nn learning based only on the water quality data for the five points obtained from astera asterf thus much of the data were learned based on the fuzzy regression analysis results the proposed method was trained using three types of input data aster data water depth and water temperature for the estimated values obtained by fra a hold out was performed for learning for example when astera was used as the test data asterb asterf was used as the training data the training data were sliced into an input size 3 3 and shuffled before learning ninety percent of the data were used for training and ten percent were used for validation the batch size was 256 the learning data were normalized as follows 3 x x μ 2 σ where x is the value after normalization x is the value prior to normalization and μ and σ are the average and standard deviation of each feature respectively 2 10 2 network configurations nn configurations were selected prior to the learning process the activation functions used to calculate the values of the hidden and output layers were determined from linear rectified linear unit relu sigmoid and tanh activation functions goodfellow et al 2016 okatani 2015 the network structure was designed to include the number of layers in the network and the number of units in each layer goodfellow et al 2016 in general implementing a dropout layer can prevent data overfitting and improve generalization performance therefore the configurations were examined to determine the best combination of parameters as shown in table s3 in the supplemental notes the configuration that minimized the mean square error mse was investigated 4 m s e n 1 n y ˆ i y i 2 n where n is the number of data points y ˆ i is the i th i 1 2 n predicted output result and y i is the i th correct output result first the number of hidden layers was set to 1 and the mses of each combination of the number of units in the hidden layers activation function and dropout rate were verified with 1000 epochs the combination with the lowest mse was then used to investigate the mses of hidden layers 1 4 cross validation of astera f was performed to select configurations adaptive moment estimation was adopted as the optimization algorithm goodfellow et al 2016 the configuration with the lowest mse is shown in fig 6 and has the following structure number of hidden layers 3 number of units in hidden layers 900 activation function a hidden layer relu b output layer linear dropout layer 0 5 2 11 accuracy of water quality estimation to evaluate the accuracy of the water quality estimations the differences between the estimated ss values and np ratios obtained using the proposed method and the measured values of the two parameters from each measurement point were calculated first each water quality measurement point was identified on the map using the estimated values subsequently 50 points with estimated values were sampled around the measurement points finally the error between the average value of the sampling points and measured values estimated error was calculated 3 results and discussion 3 1 accuracy of estimation results table 2 lists the estimated errors of the proposed method and fra as the degree of ss was differed by 10 mg l when the level differed by one level between levels 3 and 6 the estimation error of 10 mg l was set as the criterion value for ss the criterion value for the np ratio was set at 2 94 which is the increment used in the slice level processing for fra the minimum estimated errors for ss and the np ratio via the proposed method were 3 mg l in asterd ndvi standard deviation sd 2 0 and 0 73 in asterf ndvi sd 0 64 respectively the maximum improvements in the ss and np ratio estimates compared to the fra results were 6 mg l astera nir and 2 25 asterf red respectively in the errors estimated via the proposed method some items increased the estimated errors or did not significantly decrease in particular the average estimated errors for all features of asterb using the proposed method and fra exceeded 19 mg l the ss values on august 1 2012 were higher than the maximum slice level value 35 mg l set in this study at the ogata bridge east adjustment pond and center of the lake as the fra output values were estimated based on slice level values the values above the slice level could not be output therefore when the ss value in the target water body is much larger than the maximum value of the preset slice level e g 10 mg l or more new slice levels must be set for estimation fig 7 shows the estimated ss maps for the nir of asterc obtained using the proposed method and fra the average estimated error of the proposed method obtained using fra was 4 mg l sd 2 1 which was lower than 10 mg l moreover the average estimated error for the nir of astera obtained via fra was also lower than 10 mg l in the map obtained using the proposed method the degree of pollution was highest in the areas from the ogata bridge and the west adjustment pond to the floodgate where the water depth was low 1 m the degree of pollution was lowest at the center of the lake where the water depth was greater 6 m vertical circulation of lake water caused by wind and water inflow is more frequent in shallow areas than in deep areas it is assumed that fra could not consider the difference in the degree of ss due to the depth as it was estimated only by the spectral information from the water surface the proposed method learns using water depth information and dn values from the water surface therefore the proposed method improves estimation accuracy as it can learn the differences in pollution status based on the depth of the lake 3 2 conditions for improving ss estimation accuracy the average estimated error of the ss maps in astere and asterf using the proposed method did not improve for all features in particular the error for nir in asterf was 14 mg l which is very large compared with the error obtained via fra although the measured ss values were between 10 and 16 mg l the estimated ss values of the proposed method were between 25 and 35 mg l therefore the proposed method learns that the estimated ss value should be larger than it should be based on these results an analysis of the conditions is required to improve the accuracy of the asterf estimations to understand the combinations of aster data that improved the accuracy of ss estimation the number of combinations included in the learning data was varied and the estimated errors were calculated specifically the estimated errors were calculated when i four datasets were selected from astera astere as the learning datasets a total of five patterns and ii three datasets were selected from astera astere as the learning datasets a total of ten patterns when three datasets were selected the estimated errors were lower than 10 mg l the combination of astera asterb and asterc as well as the combination of astera asterc and astere showed average estimated errors of 4 and 5 mg l respectively fig 8 shows the estimated ss maps obtained using the aforementioned datasets texture analysis was conducted to analyze the characteristics of astera and asterc which were common among the combinations in particular dissimilarity was calculated based on the co occurrence matrix of a 3 3 water surface region for nir the dissimilarity increases when the dn value of the pixel of interest is higher than the dn values of other pixels in the region and when the frequency of such occurrences is high takagi and shimoda 2004 the dissimilarity decreases when the dn values in the region are uniform and increases when the region is characterized by a mixture of different values fig 9 shows the dissimilarity maps for astera asterc and asterf a total of 500 points were randomly sampled from each dataset and distribution graphs were created the distributions of dissimilarity for astera asterc and asterf were similar fig 10 therefore the creation of a learning dataset based on dissimilarity improved ss estimation accuracy the nir band used for texture analysis has low reflectance from water reflectance spectra of sediments and phytoplankton on the water surface were acquired dissimilarity reflects the inflow of sediment from nearby rivers and the occurrence of phytoplankton remaining in the water surface layer therefore it is necessary to create a learning dataset for ss estimation that considers the amount of inflow from rivers and occurrence of phytoplankton 3 3 conditions for improving the accuracy of np ratio estimation asterb and astere had minimum estimation errors of 2 37 sd 1 62 and 2 24 sd 2 54 respectively which were 1 35 units larger than the minimum estimation errors of the other aster datasets of all the water quality measurement points obtained in asterb and astere those in the east adjustment pond had the lowest np ratio however the ss values at these points were higher than the environmental standards table 1 experts investigating the water quality of lake hachiroko have found that blue green algae tend to grow when the np ratio decreases therefore during the time of asterb and astere data acquisition the ss value in the lake increased with the growth of blue green algae therefore the np ratio in asterb and astere could not be learned well as it tended to differ from that of the other aster data based on these results datasets excluding asterb and astere were created and used for holdout training using the proposed method table 3 lists the errors estimated for the np ratio using the constructed dataset the estimated error for the ndvi of asterf was 0 70 sd 0 83 which was the smallest value observed when estimating the np ratio fig 11 shows the results estimated using the constructed dataset for the ndvi of the asterf compared with the results obtained by fra the estimation accuracy of the proposed method was the most improved for astera and asterf to analyze the conditions that contributed to this improvement in accuracy the distribution of the fra estimated np ratios used as training data for the proposed method was analyzed fig 12 the distributions of the np ratios for astera and asterf were similar whereas the np ratios for asterc were lower than those for astera and asterf moreover the distribution of the asterc showed two peaks because the asterc was obtained in september which is a season where blue green algae occur frequently it can be predicted that the number of phytoplankton species was different compared to the data in may and june the spread of the np ratio distribution for asterd was larger than those for astera and asterf and the distributions were dissimilar the above mentioned results suggest that under the proposed method a learning dataset with similar np ratio distributions improves the accuracy of the np ratio estimation because the physical composition of phytoplankton differs among species the np ratio conditions of water bodies are related to the phytoplankton species for example when the np ratio is approximately 4 the environment is predicted to be optimal for microcystis which produces blue green algae therefore to improve the estimation accuracy of the np ratio it is necessary to create a dataset with data on the same species of phytoplankton 3 4 discussion previous studies on water quality estimation using satellite remote sensing have applied empirical approaches or water quality simulation modeling however as it is necessary to prepare a large dataset e g water quality data from dozens of measurement points to create a highly accurate model these methods are not applicable to all water bodies in our previous studies we analyzed the water quality of lake hachiroko using a method based on the fuzzy theory the results showed that it is possible to understand the water quality distribution using water quality data from the five measurement points however the information obtained from remote sensing data was used as the input data there have not yet been attempts to use multiple types of information regarding water bodies as input features moreover a water quality distribution map based on the relationship between multiple periods has not yet been created in this study three types of information remote sensing data water depth and water temperature were combined using an nn moreover the nn extracts features from multiple time periods the results of the proposed method showed improved accuracy compared to the results of the fra furthermore in the ss estimation the creation of a learning dataset based on an analysis of the texture dissimilarity contributed to an improved accuracy in the np ratio estimation the use of the learning dataset with similar np ratio distributions improved the estimation accuracy the above points are the differences between this study and the previous studies in this study the results of the proposed method improved the accuracy of ss and np ratio estimations compared with fra the estimation of the ss and np ratios can be used to predict the occurrence of blue green algae as discussed in section 3 3 the proposed method is useful for estimating the phytoplankton situation and may contribute to a better understanding of the ecology of the aquatic environment however this analysis has not yet been able to determine which phytoplankton species occurred at the time of water quality data acquisition the usefulness of estimating the phytoplankton situation should be examined in more detail the analysis of temporal water quality changes can identify the main factors affecting water quality changes thus contributing to measures against water pollution krishnaraj honnasiddaiah 2022 liu et al 2021 yao et al 2022 analyzed temporal water quality changes water area water level and water storage using multi source satellite remote sensing data and 10 years of data extracted from active and passive sensors this study identified the impact of changes in the water area on human lifestyle and the factors behind changes in water storage saberioon et al 2020 analyzed the temporal changes in chlorophyll a and total ss in inland waters using sentinel 2 the results showed that implementing machine learning algorithms to predict water quality parameters improved the prediction accuracy of complex spectral relationships and interactions in this study spatial water quality estimation was conducted however temporal water quality changes were not analyzed therefore it is necessary to analyze temporal changes in water quality to evaluate the usefulness of the proposed method to analyze this more data collection and applicability to other satellite data must be evaluated to increase the amount of data analyzed convolutional nns cnns perform excellently in image processing a cnn network has convolution and pooling layers and by repeating these layers features are extracted from the input data studies have been conducted on water quality estimation using cnn ilteralp et al 2022 pyo et al 2022 pyo et al 2022 used cnn models to quantitatively and qualitatively analyze algal phenomena even in the proposed method of this study it is important to architect the network with a cnn to improve accuracy however to achieve a high estimation accuracy increasing the amount of data or using high resolution data is required for better feature extraction using a cnn ahmed et al 2022 yang et al 2022 in the future it will be necessary to consider increasing the amount of data and using high resolution 1 m remote sensing data 4 conclusions in this study we discussed a novel method for estimating the ss and np ratio using nns and input features including aster data water depth and water temperature the proposed method allows for water quality estimations to also consider the pollution status of water bodies and is easy to apply because it does not require the use of many parameters the learned models were applied to create estimated water quality maps for the entire water body of lake hachiroko and the maps were compared with actual water quality conditions to evaluate the usefulness of the proposed method furthermore the conditions that contributed to improving the accuracy of the ss and np ratio estimations were investigated the results are summarized as follows 1 the proposed method could estimate the spatial distribution of the ss and np ratios moreover the errors between the estimated value via the proposed method and the measured value for ss and the np ratio were 3 mg l and 0 70 respectively furthermore the maximum improvements in the ss and np ratio estimates compared with the fra results were 6 mg l and 2 25 respectively 2 in the results obtained using the proposed method the highest degree of pollution was observed in areas with shallow water 1 m the degree of pollution was lowest at a deeper water depth 6 m vertical circulation of lake water caused by wind and water inflow is more frequent in shallow areas than in deep areas the fra could not consider the difference in the degree of ss due to the depth because it was estimated only by the spectral information from the water surface in contrast the proposed method can learn the difference in contamination status depending on the depth of the lake which improves the estimation accuracy 3 texture analysis for nir of aster data was performed and the condition of the learning dataset that improved the accuracy of the ss estimation was clarified in ss estimation using the proposed method the creation of a learning dataset based on an analysis of texture dissimilarity contributed to improved accuracy to improve the estimation accuracy of ss it is necessary to construct a dataset that considers the inflow of sediment from nearby rivers and the occurrence state of phytoplankton that remain on the surface of the water 4 the accuracy of the np ratio was improved by excluding the data with different np ratio trends from the dataset the dataset with the best results was analyzed as a result in the np ratio estimation using the proposed method the use of the learning dataset with similar np ratio distributions improved the estimation accuracy therefore to improve the estimation accuracy of the np ratio it is necessary to create a dataset with data on the same species of phytoplankton furthermore dataset creation must consider that the number of phytoplankton species occurring differs depending on the time of data acquisition the proposed method can help estimate the water quality conditions of water bodies where large scale datasets are not available furthermore the results of this study are a new finding regarding the conditions of dataset creation for estimating the water quality of ss and np ratios using nn the proposed method is useful for estimating the phytoplankton situation in water bodies and may contribute to a better understanding of the ecology of the water environment this study has the following limitations i this study was unable to determine which species of phytoplankton occurred at the time of data acquisition and ii the estimated error was large when the ss value in the target water body was larger than the maximum value of the pre set slice level therefore the usefulness of phytoplankton estimation methods should be examined in further detail in the future furthermore future analyses should be conducted by setting new slice levels finally we clarified that datasets based on dissimilarity improved the accuracy of the ss estimation however only one dataset acquired in may was used as test data therefore in the future we will increase the amount of test data and further evaluate the usefulness of the datasets based on dissimilarity in this study we developed a novel water quality estimation method that combines remote sensing data and information obtained from the water body and predicts the overall water quality conditions in the water body as a future study in this field temporal water quality analysis and water quality estimation using cnn must be conducted for this purpose it is necessary to improve the proposed method by increasing the amount of remote sensing data used for analysis and high resolution data temporal water quality analysis and estimation by cnn will contribute to the improvement of estimation accuracy in addition to the identification of the causes of water pollution software and data availability the codes that were used for estimating water quality using python language version 3 7 based on the tensorflow library can be found in github https github com kaimatsui 19006 hachiroko202207 neuralnetworkestimation this repository was created by kai matsui e mail kmatsui19006 gmail com in 2022 and has program cords 16 kb and sample data 200 mb the experimental environment is as follows os windows 10 pro cpu intel r core tm i7 9700 3 00 ghz ram 16 00 gb gpu nvidia geforce rtx 2060 the analyzed remote sensing data can be downloaded free of charge from the landbrowser digital architecture research center 2021 and the meti aist data archive system geological survey of japan 2021 the water quality data in the study area used for analysis were obtained from the water environment comprehensive information site ministry of the environment 2021 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors would like to thank the hachiroko environment policy office environmental management division living and environment department akita prefectural for providing the data used in this study and for their knowledge of the study area the authors would also like to thank the akita regional development and promotional bureau agriculture and forestry department and hachirogata mainstay facilities administration office for providing data for this study this research did not receive any specific grants from funding agencies in the public commercial or not for profit sectors appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105584 
25491,robust decision making rdm lempert et al 2006 whose original scheme is reported within the dotted box fig 1 fig 2 the case study area the thick grey lines delimit the flood protected areas i e compartments and the purple lines depict the levee stretches where structural measures can be implemented fig 2 fig 3 the deeply uncertain fragility curve thick blue line and the 15 considered alternative fragility curves grey lines spanning the considered range blue shaded area all 15 alternative fragility curves follow a normal distribution with standard deviation of 0 3 but mean values spanning from 0 6 m to 1 3 m below the height of the levee crest with a step of 5 cm fig 3 fig 4 uncertainty analysis results for each compartment and the overall system each box shows flood damage from the 0 2 annual probability events under status quo blue and after raising orange and strengthening green the critical performance threshold dotted line corresponds to the 3rd quartile of the damages under status quo fig 4 fig 5 location of the stretches identified for the implementation of structural measures fig 5 fig 6 results from the belief analysis step each column indicates a ranking position and each row indicates a compartment for each compartment and ranking position colors indicate which measure rank at the given position across all 15 fragility curves introduced in fig 3 fragility curves at which a rank shift occurs are reported in bold fig 6 table 1 the adopted xlrm framework table 1 uncertainties x measures l model r outcome m duration of the po river s upstream flood hydrograph shape of the po river s upstream flood hydrograph peak and volume of the secchia and panaro rivers flood hydrograph breaching water level at each of the 52 levee stretches deep uncertainty damage model parameter do nothing levee heightening levee strengthening the flood impact modeling chain described in subsection 3 2 the 0 2 annual probability damage at each compartment table 2 description of the considered uncertainties distributions and parametrizations table 2 uncertainties x distribution type parametrization hydrograph s duration for the po river continuous uniform lower bound 24 h upper bound 192 h hydrograph s shape class for the po river discrete probability see table 1 in supplementary material hydrograph s shape for the po river discrete uniform lower bound 1 upper bound number of hydrographs in the selected shape class hydrograph s peak discharge and volume for the secchia and panaro rivers conditioned gaussian copula see tables 3 and 4 in supplementary material breaching water level at the levee stretches wl stretch deep uncertainty continuous uniform lower bound 2 m below the levee s height upper bound levee s height damage model parameter dmrp truncated normal mean 0 113 standard deviation 0 131 min 0 max table 3 results from scenario discovery for each compartment results show which parameter and what range of values lead to flood damages higher than the 3rd quartile under the status quo dmrp indicates the damage model regression parameter wl stretch m a s l indicates the water level at a given stretch and vol po is the flood volume of the po river m 3 table 3 cross seccb cross seccc ogli mincc secc panaa ogli mincb params range params range params range params range params range dmrp 0 15 0 58 dmrp 0 12 0 58 dmrp 0 06 0 58 dmrp 0 06 0 58 dmrp 0 12 0 58 wl stretch 3 23 7 25 4 wl stretch 15 21 8 22 9 wl stretch 6 23 3 23 9 wl stretch 25 20 5 21 2 wl stretch 1 23 8 25 6 wl stretch 5 23 0 24 6 vol po 7 4e9 9 2e9 table 4 ranking of measures under the assumption of a deeply uncertain fragility curve the number in parenthesis indicate the probability of non exceeding the performance threshold status quo raising and strengthening are indicated as sq r and s respectively the number in brackets indicates the probability of non exceeding the performance threshold table 4 compartment ranking first ranked measures second ranked measures third ranked measures cros seccb s 1 0 r 0 80 sq 0 74 ogli mincb s 0 81 r sq 0 75 r sq 0 75 cros seccc r 0 79 sq 0 74 s 0 69 ogli mincc r 0 76 sq 0 74 s 0 62 secc panaa sq 0 74 r 0 72 s 0 66 belief informed robust decision making birdm assessing changes in decision robustness due to changing distributions of deep uncertainties a ciullo a a domeneghetti b j h kwakkel c k m de bruijn d f klijn c d a castellarin b a institute for environmental decisions eth zurich switzerland institute for environmental decisions eth zurich switzerland institute for environmental decisions eth zurich switzerland b university of bologna department of civil chemical environmental and materials engineering italy university of bologna department of civil chemical environmental and materials engineering italy university of bologna department of civil chemical environmental and materials engineering italy c delft university of technology department of technology policy and management the netherlands delft university of technology department of technology policy and management the netherlands delft university of technology department of technology policy and management the netherlands d deltares department of flood risk management the netherlands deltares department of flood risk management the netherlands deltares department of flood risk management the netherlands corresponding author robust decision making rdm is an established framework for decision making under deep uncertainty rdm relies on the idea of scenario neutrality namely that decision robustness is not affected by how scenarios are generated if these are uniformly distributed and span a sufficiently large range of future states of the world several authors have shown that scenario neutrality may not hold but they did so by adopting either new or computationally expensive modeling we introduce the belief informed robust decision making birdm framework to assess how robustness might change under an arbitrary large number of non uniform distributions at virtually no additional costs with respect to rdm we apply birdm to a flood management problem and find that alternative distributions change the robustness and ranking of measures birdm allows identifying what distributions lead to these changes and under what set of distributions a measure has a specific robustness and rank keywords robust decision making uncertain probabilistic information flood risk management planning 1 introduction long term infrastructure planning requires making decisions amid climatic and socio economic uncertainties many of these uncertainties are epistemic in nature as they are due to a lack of knowledge and they might become better characterized as one acquires new information over time however decisions about future infrastructures are urgent and need to be made today such that chances that future infrastructures perform adequately are maximized regardless of how exactly the future unfolds lempert et al 2003 defined this challenge as a problem of decision making under deep uncertainty that is a situation where experts do not know or cannot agree upon the probability distributions of the uncertain factors that influence decision outcomes in a context of deep uncertainty decisions must be made based on their robustness namely on their capability to perform satisfactorily under a plausible range of assumptions regarding deep uncertainties various authors have proposed decision support frameworks for decision making under deep uncertainty dmdu such as robust decision making lempert et al 2006 many objective robust decision making kasprzyk et al 2013 info gap decision theory ben haim 2006 decision scaling brown et al 2012 and dynamic adaptive policy pathways haasnoot et al 2013 these frameworks differ substantially but they all rely on the idea of scenario neutrality quinn et al 2020 as they generate plausible scenarios by sampling independently and uniformly over the widest range of physically plausible climatic and socio economic states of the world scenarios are latter explored via factor mapping techniques to discover which ones are critical and lead to poor system performances robust decisions are then taken to allow maintaining satisfactory system performances if these critical scenarios manifest it follows from scenario neutrality that decision robustness should be insensitive to how scenarios are sampled however quinn et al 2020 and mcphail et al 2020 challenged this idea and showed that the selected experimental design namely the way scenarios are constructed does affect decision robustness quinn et al 2020 conducted a robustness study of the upper basin of the colorado river by reconstructing discharge time series using four alternative information sources namely historic data paleo data future projections and all three together they found that the choice of which information is used for setting up the experimental design dramatically changed the range used to generate scenarios the model parameters deemed critical for the system and the resulting robustness values mcphail et al 2020 addressed a fictitious lake management problem using different scenario generation techniques these techniques varied in the way the space of input parameters is covered during sampling and the number of considered scenarios this resulted in different robustness values for the various decision criteria although the final ranking of decision alternatives remained the same at least for their case study other authors e g shortridge and zaitchik 2018 taner et al 2019 and reis and shortridge 2020 relaxed the main assumption underpinning the idea of scenario neutrality namely that all scenarios are equally likely they did so by exploring alternative approaches for when where and how to integrate probabilistic information into dmdu frameworks shortridge and zaitchik 2018 combined robust decision making with a bayesian statistical model they first conducted a standard robust decision making analysis to identify which subspaces in the model input space were critical next they developed a bayesian statistical model relying on projections from global climate models gcms to estimate the posterior probability of these subspaces taner et al 2019 introduced a framework that integrates decision scaling with bayesian belief networks namely bayesian networks decision scaling they proposed to build a bayesian belief network to estimate the posterior joint probability distribution of the critical future scenarios identified through decision scaling reis and shortridge 2020 carried out four robust decision making analyses using four different probability densities for the input variables next they showed that the parameters identified as critical and their ranges change according to the chosen distribution shortridge and zaitchik 2018 taner et al 2019 and reis and shortridge 2020 successfully integrated probabilistic information into scenario neutral approaches however they do so either by resorting to additional modeling efforts as done by shortridge and zaitchik 2018 or taner et al 2019 or by re running the same analysis multiple times as done by reis and shortridge 2020 this implies that either additional modeling efforts and expertise or high computational costs are required either undermines the practical feasibility and wider adoption of these approaches to address this issue we introduce the belief informed robust decision making birdm framework which allows incorporating probabilistic information into robust decision making rdm without requiring any further modeling nor demanding additional computational costs this is achieved by following the weighing method proposed by beckman and mckay 1987 which allows exploring the influence of various choices about the distribution of inputs on the output statistics at virtually no cost compared to re running the computer models for all desired distributions we showcase birdm on a flood management problem along the lower po river in northern italy with the goal of exploring the effects of relaxing the assumption of scenario neutrality on decision robustness and the final ranking of measures the paper is structured as follows section 2 describes the method section 3 describes the case study and the simulation model section 4 describes the analysis section 5 reports results and discusses them section 6 provides conclusions 2 belief informed robust decision making the belief informed robust decision making birdm framework builds on the robust decision making rdm framework proposed by lempert et al 2006 robust decision making aims at finding intrinsic system vulnerabilities which do not depend on the likelihood of the input factors revealing them yet an a posteriori exploration of how likely these input factors might be is of high relevance for decision making even more so when different assumptions about these likelihoods might change decision robustness and the final ranking of measures birdm expands rdm to account for this aspect birdm is presented in fig 1 in the figure the standard rdm steps are contained in the dotted box while birdm is represented by the full scheme basically birdm adds an additional belief analysis step to rdm each step is described in detail in the following subsections 2 1 problem formulation this step requires formulating the policy problem specifying the scope decision objectives and possible measures a suitable scheme to formulate a policy problem is the xlrm scheme proposed by lempert et al 2003 the xlrm scheme allows the analyst to clearly specify the uncertain factors x the policy levers l the system relationships namely the system model r and the performance metrics m in the present paper we refer to levers i e l as measures and to performance metrics i e m as outcomes 2 2 uncertainty analysis this step involves generating scenarios and evaluating model outcomes for these scenarios typically this step is conducted using a sampling strategy that maximizes the coverage of the input space such as latin hypercube or low discrepancy sequences like the sobol sequence sobol 1967 compared to pseudo random sequences these sampling techniques more evenly distribute the sampled points over the domain of the input factors following the idea of scenario neutrality the sampling of deeply uncertain factors is carried out assuming independence and a uniform distribution over the full range of physically plausible values the outcome of this analysis is a database of model outputs which is investigated further through scenario discovery 2 3 scenario discovery in this step factor mapping techniques are used to identify regions in the input space corresponding to certain output values scenario discovery is typically used to identify regions in the input space that have a high concentration of failure scenarios namely those under which system performances y are below a performance threshold y deemed critical for the functioning of the system finding failure scenarios is instrumental to the identification of robust measures in robust decision making robust measures are defined as those that improve system performances y under the failure scenarios and which therefore increase chances of meeting the critical performance threshold y it follows that in robust decision making robustness relies on the concept of satisficing performances lempert et al 2006 namely on meeting certain performance requirements under the widest range of possible scenarios 2 4 belief analysis this step explores how alternative assumptions regarding the likelihood of the failure scenarios affect the robustness of measures we here refer to these alternative assumptions as beliefs that analysts experts and decision makers may have about the likelihood of the failure scenarios we do so by employing the weighting method proposed by beckman and mckay 1987 the weighting method consists of replacing the original sampling distribution by an alternative one and then assessing through reweighting the output statistics as if the original probability distribution were used more formally assuming a simulation model with input variables x and output variable y and assuming that x has a multivariate distribution function f but it is sampled using an alternative distribution h then the probability of y exceeding a threshold value y can be defined as diermanse et al 2015 1 p y y 1 n i 1 n 1 y i y c i where n is the number of simulations 1 is an indicator function which is equal to 1 if y i y and 0 otherwise and c i is a correction factor weight defined as follows 2 c i f x i h x i with f and h being the density functions of f and h which are called the original and surrogate distributions respectively in the method the original distribution f is unknown and one samples from a known surrogate distribution h to quantify output statistics had f been used it is worth highlighting that the method is equivalent to the well known importance sampling variance reduction technique tokdar and kass 2010 but its intent is different as it reverses the use of the original and surrogate distributions in importance sampling the sampling distribution f is known and one aims to find an unknown surrogate sampling distribution h such that more samples are drawn from important regions in the input space the weighting method of beckman and mckay 1987 has been employed by different authors and for various purposes sparkman et al 2016 used it to calculate global sensitivity indices from an existing sample of simulations and introduced the importance sampling based kernel estimator to calculate the moments of the conditional distributions in the calculation of the sensitivity indices zhang and shields 2018 used the method to deal with the epistemic uncertainties stemming from the inability to reliably specify a unique probability density function due to data scarcity in such cases they suggest to first identify an arbitrary large set of candidate probability densities through bayesian inference from the derived large set of densities a single optimal density is selected which is representative of all candidate probability densities this optimal density is the one used in the monte carlo analysis and then the uncertainty in the selection of the distribution is explored by applying the weighting scheme as if any of the candidates were used zhang et al 2021 expanded this framework within the context of global sensitivity analysis and derived imprecise sobol indices sobol 2001 following these applications birdm applies the weighting method assuming a uniform surrogate distribution h in line with rdm and other scenario neutral frameworks and evaluate robustness of all identified measures under alternative assumptions of the original distribution of deep uncertainties f the resulting outcome consists of as many robustness estimates for each measure as the number of alternative distributions being explored the belief analysis step requires virtually no additional efforts irrespectively of the number of alternative distributions being explored as no new monte carlo experiments are run 3 the case study and the simulation model with the aim of testing birdm we apply it to support flood management planning along the lower reach of the po river in northern italy see fig 2 the goal is to find robust flood damage reduction measures and to assess the effect of alternative distributions regarding deep uncertainties on the ranking of measures the considered river stretch ranges from the stream gauges of borgoforte upstream to pontelagoscuro downstream spanning across a total length of about 115 km and including the confluences of two main tributaries the secchia and panaro rivers the case study includes 17 levee protected floodplains purple areas in fig 2 also referred to as compartments which are protected from the 0 5 annual probability flood event subsection 3 1 provides some background information regarding the choice of the case study and subsection 3 2 introduces the simulation model 3 1 rationale the choice of the case study is driven by recent calls from the flood risk modeling and management community urging for decision support studies on systemic large scale flood risk management planning vorogushyn et al 2017 this systemic perspective was shown to lead to more accurate flood frequency analyses apel et al 2009 and risk estimates courage et al 2013 de bruijn et al 2016 and to ultimately widen the spectrum of flood risk management measures potentially increasing optimality and fairness in the design of the system ciullo et al 2019a ciullo et al 2019b the birdm framework is used to explore beliefs about key epistemic uncertainties in dealing with embanked large scale flood risk systems the probability of levee failure as a function of hydraulic loads beven et al 2018 this uncertainty is typically characterized by the so called fragility curves bachmann et al 2013 curran et al 2019 the generation of such curves however requires extensive knowledge of the geotechnical properties of the flood defenses which in case of large scale systems may not be available or accurate at all locations of interest furthermore even when assuming that fragility curves can be reliably derived unexpected breaching can still occur and this is one of the main causes of disastrous river floods merz et al 2021 in january 2014 for example a levee failure occurred during a minor flood event along the secchia river italy the breach was unforeseen and it was caused by animal burrows and not by one of the failure mechanisms commonly modeled when deriving fragility curves i e overtopping piping or macro instability orlandini et al 2015 for these reasons we treat breaching water levels namely water levels triggering levee failures as deeply uncertain 3 2 simulation model the simulation model follows three steps 1 the generation of hydrological events along the po river and its main tributaries secchia and panaro 2 the propagation of the generated events using a hydrodynamic model 3 the assessment of economic damage in the flooded compartments the following subsections briefly introduce the modeling steps more detailed information is provided in the supplementary material 3 2 1 generation of hydrological events we focus on the generation of alternative 0 2 annual probability flood events at the upstream po river s cross section of borgoforte these events are expected to cause flooding in the compartments as the levee system is designed to withstand the 0 5 annual probability flood events the 0 2 annual probability events along the po river at borgoforte are generated based on the flood duration frequency fdf curve derived by maione et al 2003 and the historical flood hydrographs of the po at borgoforte reported in tanda et al 2001 the aim is to generate 0 2 annual probability events of decreasing peak flow for increasing duration such that the fdf curve is always met after the 0 2 annual probability events are generated for the po river a gaussian copula is used to capture the dependence between the generated events in the po river and in its main tributaries secchia and panaro similarly to what is done by curran et al 2020 the correlation between events in the po river and those in the tributaries is modeled using data from the hypeweb platform of the swedish meteorological and hydrological institute smhi these data consist of 29 years of simulated daily mean discharges for the three rivers and are used to generate a gaussian copula between volumes and discharges of the po secchia and panaro rivers after generating the 0 2 annual probability flood event for the po river as described above events of the tributaries are found by conditional sampling of the gaussian copula 3 2 2 hydrodynamic model flood hazard is simulated with a quasi 2d model implemented through the hec ras software domeneghetti et al 2015 in the model the main river is represented through cross sections retrieved from a lidar digital elevation model with a 2 m spatial resolution the levee protected floodplain is subdivided into 17 compartments which are modeled as storage areas connected to each other and or to the main channel by means of lateral structures or connections that reproduce existing levees the hydrostatic behavior of each storage area is represented through volume level curves thus in case of flooding water levels can be estimated from the water volumes exchanged with the main channel and or adjacent storage areas these curves are built using a 10 m resolution dem available for all italy tinitaly see tarquini et al 2012 3 2 3 economic impact assessment floods are often devastating events and the impact they cause includes loss of life damage to buildings infrastructures and the deterioration of ecosystems moreover flood impacts are larger for communities which are more socially and economically vulnerable however since the focus of this study is primarily methodological for practical reasons we model flood impact only as economic damage to residential buildings to do so we use the hypsometric vulnerability curves approach proposed by domeneghetti et al 2015 and the damage function developed by carisi et al 2018 typically the hypsometric curve of an area provides the percentage of the total area below a certain elevation the hypsometric vulnerability curve combines this information with the land use of the area thus providing the percentage of a land use class below a given elevation therefore if combined with damage functions the hypsometric vulnerability curve is a useful simplified graphical tool to quantify aggregated flood damage of large areas we calculate hypsometric vulnerability curves relative to urban areas in all compartments to do so we use data of residential buildings available from the geodata web platforms of the three italian regions involved in the case study namely emilia romagna lombardia and veneto data about asset values of buildings are retrieved from the italian revenue agency asset value data are provided in terms of euros per square meter m2 for different types of buildings and all italian municipalities every six months we define an asset value per compartment as the weighted average of all asset values where weights are given by the extent of urban area of the municipalities in that compartment for deriving the depth damage curve we used the square root regression model developed by carisi et al 2018 using loss data collected after the secchia river 2014 flood we account for the uncertainty in this relationship by using alternative regression models generated from the distribution of the fitting parameter 4 description of the analysis the analysis aims at applying the birdm framework introduced in section 2 the goal is to explore for each compartment how alternative assumptions regarding the distribution of deep uncertainties affect the robustness of damage reduction measures and their ranking the description follows the steps introduced in section 2 4 1 problem framing the adopted xlrm framework is shown in table 1 exogeneous uncertainties x relate to various hydrological features of the po river and its tributaries the damage function and the breaching water levels at each of the 52 levee stretches identified in the study area the considered structural measures l correspond to either doing nothing hereafter status quo levees raising hereafter raising or levees strengthening hereafter strengthening the simulation model r is the one introduced in subsection 3 2 the outcomes of interest m are the flood damages from the 0 2 annual probability events in each compartment 4 2 uncertainty analysis uncertainty analysis is conducted performing 4500 runs of the simulation model using a sobol sampling sequence sobol 1967 this is also known as quasi monte carlo analysis table 2 reports the selected sampling distribution for each uncertainty we stress that for the reasons outlined in subsection 3 1 uncertainties related to breaching water levels are considered as deeply uncertain it follows from the scenario neutral assumption that these are uniformly distributed with the range of physically plausible breaching water levels going from the levee crest height upper limit to the lowest level at which a breach can physically be triggered lower limit this latter is given by the highest point between the floodplain s height and the height of the levee protected area such that the necessary hydraulic gradient required for a breach to plausibly develop is guaranteed in the case study this is estimated to be approximately 2 m below the initial crest level for all levee stretches this yields a fragility curve as expressed by the thick blue line in fig 3 as indicated in fig 1 this step and scenario discovery are iterated therefore the uncertainty analysis is first carried out for the status quo next system vulnerabilities are identified through scenario discovery and measures that limit such vulnerabilities are defined finally uncertainty analysis is again performed assuming measures are implemented convergence plots are shown in the supplementary material as mean damage against number of simulations 4 3 scenario discovery scenario discovery is performed using the patient rule induction method prim algorithm friedman and fisher 1999 scenario discovery is used to explore the uncertainty space generated by sampling uncertainties as reported in table 2 except for the first three listed uncertainties namely duration shape class and shape of the po river s upstream hydrograph which are used to generate flood waves instead for the sake of interpretability scenario discovery is run on the peak discharges and volumes of the generated flood waves this implies the exploration of 59 variables the peak discharge and hydrographs volumes of the 3 rivers the breaching water levels at the 52 levees stretches and the damage model parameter scenario discovery is used to identify cases where flood damage in each compartment is larger than the 3rd damage quartile under status quo which is the assumed critical performance threshold scenario discovery provides an indication for each compartment of what levee stretches when failing are likely to lead to damages above the critical threshold based on scenario discovery results and a broader consideration of the geometry of the levee system we identify critical levee stretches and define structural measures after these measures are identified their performance is re assessed through uncertainty analysis step 4 2 4 4 belief analysis this step allows exploring how the ranking of structural measures changes under various alternative assumptions about the shape of the fragility curves structural measures are evaluated based on their robustness in providing satisficing performances this is defined as the probability of not exceeding the critical performance threshold which we define for each compartment as the flood damage equal to the 3rd damage quartile under status quo see section 4 3 this probability can be assessed for each measure via the method described in section 2 4 obviously the most robust measure is the one with the highest non exceedance probability of the performance threshold different hypotheses regarding the fragility curve may lead to different robustness values and thus result into a different ranking of measures in practice the choice of the alternative fragility curves should be derived through expert elicitations and reflect the uncertainty around levee breaching for the levee stretch of interest for simplicity we assume all fragility curves follow a normal distribution with the same standard deviation of 0 3 but different mean values these range from 0 6 m upper curve to 1 3 m lower curve below the height of the levee crest with steps of 5 cm each for a total of 15 fragility curves the range number of steps could be made larger finer according to the application as the computational cost of exploring a large number of alternative beliefs is very low all considered fragility curves are shown in fig 3 5 results and discussion this section reports the results of the analysis described in section 4 this section reports results from the uncertainty analysis and scenario discovery steps in subsection 5 1 and those from the belief analysis step in subsection 5 2 5 1 uncertainty analysis and scenario discovery the output of the uncertainty analysis of the current system status quo blue boxplot in fig 4 reveals that five compartments are flooded by the 0 2 annual probability flood events and they are all located upstream this happens because breaching at these compartments results in flood peak attenuation and hence in an unloading effect for the downstream compartments which are consequently not flooded for all the flooded compartments failure scenarios are identified as those where flood damages under status quo are higher than the critical performance threshold table 3 shows the results of scenario discovery for these failure scenarios high values of the damage model parameter are responsible for large flood damage in all compartments flood waves with large volumes along the po river are responsible for large flood damages only to the cross seccb compartment one of the most upstream for each compartment one can identify the critical locations namely those locations where should the levees fail large flood damage would occur identifying these critical locations provides a crucial indication on where structural damage reduction measures should be applied and scenario discovery supports their identification in the case of strengthening this information suffices as we simply assume that these stretches are strengthened in such a way that levee overtopping produces flooding without breaching the levee in the case of raising an additional choice needs to be made based on the height of the nearby levees to make the two structural measures comparable both raising and strengthening are applied to the same set of stretches in compartment cross seccb two stretches are identified as relevant for investing in flood protection namely stretches number 3 and 5 the levee at stretch 5 has an average crest level of 25 m a s l while the levee at stretch 3 is higher with an average crest level of 25 65 m a s l stretch 4 located between stretches 3 and 5 with an average levee crest level of 25 5 m a s l is also considered part of the intervention as the lowest average levee height along the compartment on the other side of the po river is about 25 8 m a s l levees of stretches 3 4 and 5 are raised up to this level following similar reasoning levee raising of 1 0 0 7 0 5 and 0 3 m is applied to stretches in compartments cross seccc ogli minc ogli minb secc panaa respectively in total the procedure leads to the implementation of structural interventions at 7 stretches as shown in fig 5 the performance of both raising and strengthening is assessed by performing two uncertainty analyses and results are shown in fig 4 looking at the total damage raising and strengthening both improve the current situation as they result in lower damage than status quo in the case of strengthening this benefit is much more significant similar results are found for cros seccb the compartment where most damage occurs and the one where the most upstream stretches subject to intervention are located although with a less remarkable difference strengthening is the best option also for compartment ogli minb for all other compartments it is found that either raising or strengthening may deteriorate performance with respect to status quo as the number of cases above the performance threshold increases this is due to right left and upstream downstream hydraulic interactions due to which structural measures reducing damage on a given river stretch may increase damage at the other side of the river or downstream ciullo et al 2020 for example for ogli mincb which is located opposite to cros seccb raising does not seem to bring much damage reduction as it performs similarly to status quo for the more downstream compartments i e cros seccc ogli mincc secc panaa and adig poe strengthening may significantly deteriorate performance with respect to status quo to the point that it may cause damage at compartment adig poe where there was none initially interventions were designed via scenario discovery with the goal of reducing damage above the performance threshold at each compartment to limit the effects of hydraulic interactions the uncertainty analysis and scenario discovery steps would need to be run iteratively until a satisfactory set of measures is achieved for the entire system a comprehensive design of the entire system requiring such an extensive iterative procedure of refinement of measures is beyond the scope of this paper 5 2 belief analysis following the description in section 4 4 this section reports how robustness values and the ranking of measures change for each compartment utilizing each of the 15 alternative fragility curves reported in fig 3 the ranking of the three measures is assessed based on their robustness in achieving satisfactory performances namely the probability of non exceeding the performance threshold see section 4 4 the higher this probability the higher a measure is ranked results of the belief analysis are reported in fig 6 in each panel and for each of the 15 alternative fragility curves colors indicate which measure ranks at the position indicated by the respective column for comparison table 4 reports results under the assumption of a deeply uncertain fragility curve compartment cross seccb is the only one for which alternative beliefs do not affect the ranking of measures at all as strengthening always ranks first followed by raising and status quo this is coherent with the overall performance under deep uncertainty for compartment ogli mincb and cross seccc raising is instead the first ranking measure regardless beliefs for both compartments the second and third ranked measures differ across beliefs and can be either strengthening or status quo with the latter being preferred when levees are believed to be weak and prone to failing at low water levels this is logical as structural measures are most effective under circumstances of weak levees for ogli mincb results are surprising since strengthening which is the best measure under deep uncertainty see table 4 never ranks as the first measure under changing beliefs this highlights how a high performance of measures under the scenario neutrality assumption may deteriorate when beliefs are introduced for cross seccc raising is the first ranked measure also under deep uncertainty followed by status quo and strengthening for compartments ogli mincc and secc panaa the first ranked measure likely to be the most relevant one from a practical viewpoint is sensitive to changes in belief as a shift from status quo to raising occurs depending on the belief about the strength of the levee for both compartments raising is preferable when the levee is believed to be weak and status quo takes over when the levee is believed to be stronger the opposite shift takes place as second ranked measure whereas the third ranked measure is always strengthening as this measure increases damages at these compartments due to upstream downstream interactions see previous section 6 conclusions in the present paper we introduced the belief informed robust decision making birdm framework birdm builds upon robust decision making rdm and it enables the assessment of the effects of alternative distributions of deep uncertainties on the robustness values and ranking of measures in principle the same results provided by birdm could be achieved by running a standard rdm analysis multiple times with alternative assumptions regarding the distribution of deep uncertainties this however would demand large computational power and may become unfeasible as soon as many alternative assumptions need to be explored the proposed approach instead requires almost no additional computational costs nor any new modeling efforts compared to rdm and it therefore allows the exploration of an arbitrarily large number of alternative distributions as such birdm extends and improves rdm as it allows assessing whether ambiguities lack of knowledge or disagreement regarding the distribution of deep uncertainties matter in the first place we demonstrated birdm by applying it to a flood management planning problem along the lower po river italy the goal was to identify the most robust structural measures and to assess whether and to what extent alternative assumptions about the levees fragility curves affect robustness values and the ranking of measures we find that birdm s results reveal changes in the ranking of measures because of changing assumptions about the levees fragility curves therefore the ranking emerging from the standard rdm is sensitive to alternative assumptions regarding such uncertainties this can be decision relevant especially when these changes affect which measure ranks first birdm also supports the identification of which belief triggers such a change in the ranking thus enabling the assessment of what beliefs lead a given measure to have a specific rank this may trigger further investigations regarding for example the reliability of such rank changing beliefs finally birdm can be used in a stress test fashion to assess under what assumptions a given a measure no longer performs as desired data and code availability results of the full analysis and the code need to perform the scenario discovery and belief analysis steps can be found at https github com aleeciu birdm paper declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this project has received funding from the european union s horizon 2020 research and innovation programme under the marie skłodowska curie grant agreement no 676027 appendix a supplementary data the following is the supplementary data to this article supplementary data supplementary data appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105560 
25491,robust decision making rdm lempert et al 2006 whose original scheme is reported within the dotted box fig 1 fig 2 the case study area the thick grey lines delimit the flood protected areas i e compartments and the purple lines depict the levee stretches where structural measures can be implemented fig 2 fig 3 the deeply uncertain fragility curve thick blue line and the 15 considered alternative fragility curves grey lines spanning the considered range blue shaded area all 15 alternative fragility curves follow a normal distribution with standard deviation of 0 3 but mean values spanning from 0 6 m to 1 3 m below the height of the levee crest with a step of 5 cm fig 3 fig 4 uncertainty analysis results for each compartment and the overall system each box shows flood damage from the 0 2 annual probability events under status quo blue and after raising orange and strengthening green the critical performance threshold dotted line corresponds to the 3rd quartile of the damages under status quo fig 4 fig 5 location of the stretches identified for the implementation of structural measures fig 5 fig 6 results from the belief analysis step each column indicates a ranking position and each row indicates a compartment for each compartment and ranking position colors indicate which measure rank at the given position across all 15 fragility curves introduced in fig 3 fragility curves at which a rank shift occurs are reported in bold fig 6 table 1 the adopted xlrm framework table 1 uncertainties x measures l model r outcome m duration of the po river s upstream flood hydrograph shape of the po river s upstream flood hydrograph peak and volume of the secchia and panaro rivers flood hydrograph breaching water level at each of the 52 levee stretches deep uncertainty damage model parameter do nothing levee heightening levee strengthening the flood impact modeling chain described in subsection 3 2 the 0 2 annual probability damage at each compartment table 2 description of the considered uncertainties distributions and parametrizations table 2 uncertainties x distribution type parametrization hydrograph s duration for the po river continuous uniform lower bound 24 h upper bound 192 h hydrograph s shape class for the po river discrete probability see table 1 in supplementary material hydrograph s shape for the po river discrete uniform lower bound 1 upper bound number of hydrographs in the selected shape class hydrograph s peak discharge and volume for the secchia and panaro rivers conditioned gaussian copula see tables 3 and 4 in supplementary material breaching water level at the levee stretches wl stretch deep uncertainty continuous uniform lower bound 2 m below the levee s height upper bound levee s height damage model parameter dmrp truncated normal mean 0 113 standard deviation 0 131 min 0 max table 3 results from scenario discovery for each compartment results show which parameter and what range of values lead to flood damages higher than the 3rd quartile under the status quo dmrp indicates the damage model regression parameter wl stretch m a s l indicates the water level at a given stretch and vol po is the flood volume of the po river m 3 table 3 cross seccb cross seccc ogli mincc secc panaa ogli mincb params range params range params range params range params range dmrp 0 15 0 58 dmrp 0 12 0 58 dmrp 0 06 0 58 dmrp 0 06 0 58 dmrp 0 12 0 58 wl stretch 3 23 7 25 4 wl stretch 15 21 8 22 9 wl stretch 6 23 3 23 9 wl stretch 25 20 5 21 2 wl stretch 1 23 8 25 6 wl stretch 5 23 0 24 6 vol po 7 4e9 9 2e9 table 4 ranking of measures under the assumption of a deeply uncertain fragility curve the number in parenthesis indicate the probability of non exceeding the performance threshold status quo raising and strengthening are indicated as sq r and s respectively the number in brackets indicates the probability of non exceeding the performance threshold table 4 compartment ranking first ranked measures second ranked measures third ranked measures cros seccb s 1 0 r 0 80 sq 0 74 ogli mincb s 0 81 r sq 0 75 r sq 0 75 cros seccc r 0 79 sq 0 74 s 0 69 ogli mincc r 0 76 sq 0 74 s 0 62 secc panaa sq 0 74 r 0 72 s 0 66 belief informed robust decision making birdm assessing changes in decision robustness due to changing distributions of deep uncertainties a ciullo a a domeneghetti b j h kwakkel c k m de bruijn d f klijn c d a castellarin b a institute for environmental decisions eth zurich switzerland institute for environmental decisions eth zurich switzerland institute for environmental decisions eth zurich switzerland b university of bologna department of civil chemical environmental and materials engineering italy university of bologna department of civil chemical environmental and materials engineering italy university of bologna department of civil chemical environmental and materials engineering italy c delft university of technology department of technology policy and management the netherlands delft university of technology department of technology policy and management the netherlands delft university of technology department of technology policy and management the netherlands d deltares department of flood risk management the netherlands deltares department of flood risk management the netherlands deltares department of flood risk management the netherlands corresponding author robust decision making rdm is an established framework for decision making under deep uncertainty rdm relies on the idea of scenario neutrality namely that decision robustness is not affected by how scenarios are generated if these are uniformly distributed and span a sufficiently large range of future states of the world several authors have shown that scenario neutrality may not hold but they did so by adopting either new or computationally expensive modeling we introduce the belief informed robust decision making birdm framework to assess how robustness might change under an arbitrary large number of non uniform distributions at virtually no additional costs with respect to rdm we apply birdm to a flood management problem and find that alternative distributions change the robustness and ranking of measures birdm allows identifying what distributions lead to these changes and under what set of distributions a measure has a specific robustness and rank keywords robust decision making uncertain probabilistic information flood risk management planning 1 introduction long term infrastructure planning requires making decisions amid climatic and socio economic uncertainties many of these uncertainties are epistemic in nature as they are due to a lack of knowledge and they might become better characterized as one acquires new information over time however decisions about future infrastructures are urgent and need to be made today such that chances that future infrastructures perform adequately are maximized regardless of how exactly the future unfolds lempert et al 2003 defined this challenge as a problem of decision making under deep uncertainty that is a situation where experts do not know or cannot agree upon the probability distributions of the uncertain factors that influence decision outcomes in a context of deep uncertainty decisions must be made based on their robustness namely on their capability to perform satisfactorily under a plausible range of assumptions regarding deep uncertainties various authors have proposed decision support frameworks for decision making under deep uncertainty dmdu such as robust decision making lempert et al 2006 many objective robust decision making kasprzyk et al 2013 info gap decision theory ben haim 2006 decision scaling brown et al 2012 and dynamic adaptive policy pathways haasnoot et al 2013 these frameworks differ substantially but they all rely on the idea of scenario neutrality quinn et al 2020 as they generate plausible scenarios by sampling independently and uniformly over the widest range of physically plausible climatic and socio economic states of the world scenarios are latter explored via factor mapping techniques to discover which ones are critical and lead to poor system performances robust decisions are then taken to allow maintaining satisfactory system performances if these critical scenarios manifest it follows from scenario neutrality that decision robustness should be insensitive to how scenarios are sampled however quinn et al 2020 and mcphail et al 2020 challenged this idea and showed that the selected experimental design namely the way scenarios are constructed does affect decision robustness quinn et al 2020 conducted a robustness study of the upper basin of the colorado river by reconstructing discharge time series using four alternative information sources namely historic data paleo data future projections and all three together they found that the choice of which information is used for setting up the experimental design dramatically changed the range used to generate scenarios the model parameters deemed critical for the system and the resulting robustness values mcphail et al 2020 addressed a fictitious lake management problem using different scenario generation techniques these techniques varied in the way the space of input parameters is covered during sampling and the number of considered scenarios this resulted in different robustness values for the various decision criteria although the final ranking of decision alternatives remained the same at least for their case study other authors e g shortridge and zaitchik 2018 taner et al 2019 and reis and shortridge 2020 relaxed the main assumption underpinning the idea of scenario neutrality namely that all scenarios are equally likely they did so by exploring alternative approaches for when where and how to integrate probabilistic information into dmdu frameworks shortridge and zaitchik 2018 combined robust decision making with a bayesian statistical model they first conducted a standard robust decision making analysis to identify which subspaces in the model input space were critical next they developed a bayesian statistical model relying on projections from global climate models gcms to estimate the posterior probability of these subspaces taner et al 2019 introduced a framework that integrates decision scaling with bayesian belief networks namely bayesian networks decision scaling they proposed to build a bayesian belief network to estimate the posterior joint probability distribution of the critical future scenarios identified through decision scaling reis and shortridge 2020 carried out four robust decision making analyses using four different probability densities for the input variables next they showed that the parameters identified as critical and their ranges change according to the chosen distribution shortridge and zaitchik 2018 taner et al 2019 and reis and shortridge 2020 successfully integrated probabilistic information into scenario neutral approaches however they do so either by resorting to additional modeling efforts as done by shortridge and zaitchik 2018 or taner et al 2019 or by re running the same analysis multiple times as done by reis and shortridge 2020 this implies that either additional modeling efforts and expertise or high computational costs are required either undermines the practical feasibility and wider adoption of these approaches to address this issue we introduce the belief informed robust decision making birdm framework which allows incorporating probabilistic information into robust decision making rdm without requiring any further modeling nor demanding additional computational costs this is achieved by following the weighing method proposed by beckman and mckay 1987 which allows exploring the influence of various choices about the distribution of inputs on the output statistics at virtually no cost compared to re running the computer models for all desired distributions we showcase birdm on a flood management problem along the lower po river in northern italy with the goal of exploring the effects of relaxing the assumption of scenario neutrality on decision robustness and the final ranking of measures the paper is structured as follows section 2 describes the method section 3 describes the case study and the simulation model section 4 describes the analysis section 5 reports results and discusses them section 6 provides conclusions 2 belief informed robust decision making the belief informed robust decision making birdm framework builds on the robust decision making rdm framework proposed by lempert et al 2006 robust decision making aims at finding intrinsic system vulnerabilities which do not depend on the likelihood of the input factors revealing them yet an a posteriori exploration of how likely these input factors might be is of high relevance for decision making even more so when different assumptions about these likelihoods might change decision robustness and the final ranking of measures birdm expands rdm to account for this aspect birdm is presented in fig 1 in the figure the standard rdm steps are contained in the dotted box while birdm is represented by the full scheme basically birdm adds an additional belief analysis step to rdm each step is described in detail in the following subsections 2 1 problem formulation this step requires formulating the policy problem specifying the scope decision objectives and possible measures a suitable scheme to formulate a policy problem is the xlrm scheme proposed by lempert et al 2003 the xlrm scheme allows the analyst to clearly specify the uncertain factors x the policy levers l the system relationships namely the system model r and the performance metrics m in the present paper we refer to levers i e l as measures and to performance metrics i e m as outcomes 2 2 uncertainty analysis this step involves generating scenarios and evaluating model outcomes for these scenarios typically this step is conducted using a sampling strategy that maximizes the coverage of the input space such as latin hypercube or low discrepancy sequences like the sobol sequence sobol 1967 compared to pseudo random sequences these sampling techniques more evenly distribute the sampled points over the domain of the input factors following the idea of scenario neutrality the sampling of deeply uncertain factors is carried out assuming independence and a uniform distribution over the full range of physically plausible values the outcome of this analysis is a database of model outputs which is investigated further through scenario discovery 2 3 scenario discovery in this step factor mapping techniques are used to identify regions in the input space corresponding to certain output values scenario discovery is typically used to identify regions in the input space that have a high concentration of failure scenarios namely those under which system performances y are below a performance threshold y deemed critical for the functioning of the system finding failure scenarios is instrumental to the identification of robust measures in robust decision making robust measures are defined as those that improve system performances y under the failure scenarios and which therefore increase chances of meeting the critical performance threshold y it follows that in robust decision making robustness relies on the concept of satisficing performances lempert et al 2006 namely on meeting certain performance requirements under the widest range of possible scenarios 2 4 belief analysis this step explores how alternative assumptions regarding the likelihood of the failure scenarios affect the robustness of measures we here refer to these alternative assumptions as beliefs that analysts experts and decision makers may have about the likelihood of the failure scenarios we do so by employing the weighting method proposed by beckman and mckay 1987 the weighting method consists of replacing the original sampling distribution by an alternative one and then assessing through reweighting the output statistics as if the original probability distribution were used more formally assuming a simulation model with input variables x and output variable y and assuming that x has a multivariate distribution function f but it is sampled using an alternative distribution h then the probability of y exceeding a threshold value y can be defined as diermanse et al 2015 1 p y y 1 n i 1 n 1 y i y c i where n is the number of simulations 1 is an indicator function which is equal to 1 if y i y and 0 otherwise and c i is a correction factor weight defined as follows 2 c i f x i h x i with f and h being the density functions of f and h which are called the original and surrogate distributions respectively in the method the original distribution f is unknown and one samples from a known surrogate distribution h to quantify output statistics had f been used it is worth highlighting that the method is equivalent to the well known importance sampling variance reduction technique tokdar and kass 2010 but its intent is different as it reverses the use of the original and surrogate distributions in importance sampling the sampling distribution f is known and one aims to find an unknown surrogate sampling distribution h such that more samples are drawn from important regions in the input space the weighting method of beckman and mckay 1987 has been employed by different authors and for various purposes sparkman et al 2016 used it to calculate global sensitivity indices from an existing sample of simulations and introduced the importance sampling based kernel estimator to calculate the moments of the conditional distributions in the calculation of the sensitivity indices zhang and shields 2018 used the method to deal with the epistemic uncertainties stemming from the inability to reliably specify a unique probability density function due to data scarcity in such cases they suggest to first identify an arbitrary large set of candidate probability densities through bayesian inference from the derived large set of densities a single optimal density is selected which is representative of all candidate probability densities this optimal density is the one used in the monte carlo analysis and then the uncertainty in the selection of the distribution is explored by applying the weighting scheme as if any of the candidates were used zhang et al 2021 expanded this framework within the context of global sensitivity analysis and derived imprecise sobol indices sobol 2001 following these applications birdm applies the weighting method assuming a uniform surrogate distribution h in line with rdm and other scenario neutral frameworks and evaluate robustness of all identified measures under alternative assumptions of the original distribution of deep uncertainties f the resulting outcome consists of as many robustness estimates for each measure as the number of alternative distributions being explored the belief analysis step requires virtually no additional efforts irrespectively of the number of alternative distributions being explored as no new monte carlo experiments are run 3 the case study and the simulation model with the aim of testing birdm we apply it to support flood management planning along the lower reach of the po river in northern italy see fig 2 the goal is to find robust flood damage reduction measures and to assess the effect of alternative distributions regarding deep uncertainties on the ranking of measures the considered river stretch ranges from the stream gauges of borgoforte upstream to pontelagoscuro downstream spanning across a total length of about 115 km and including the confluences of two main tributaries the secchia and panaro rivers the case study includes 17 levee protected floodplains purple areas in fig 2 also referred to as compartments which are protected from the 0 5 annual probability flood event subsection 3 1 provides some background information regarding the choice of the case study and subsection 3 2 introduces the simulation model 3 1 rationale the choice of the case study is driven by recent calls from the flood risk modeling and management community urging for decision support studies on systemic large scale flood risk management planning vorogushyn et al 2017 this systemic perspective was shown to lead to more accurate flood frequency analyses apel et al 2009 and risk estimates courage et al 2013 de bruijn et al 2016 and to ultimately widen the spectrum of flood risk management measures potentially increasing optimality and fairness in the design of the system ciullo et al 2019a ciullo et al 2019b the birdm framework is used to explore beliefs about key epistemic uncertainties in dealing with embanked large scale flood risk systems the probability of levee failure as a function of hydraulic loads beven et al 2018 this uncertainty is typically characterized by the so called fragility curves bachmann et al 2013 curran et al 2019 the generation of such curves however requires extensive knowledge of the geotechnical properties of the flood defenses which in case of large scale systems may not be available or accurate at all locations of interest furthermore even when assuming that fragility curves can be reliably derived unexpected breaching can still occur and this is one of the main causes of disastrous river floods merz et al 2021 in january 2014 for example a levee failure occurred during a minor flood event along the secchia river italy the breach was unforeseen and it was caused by animal burrows and not by one of the failure mechanisms commonly modeled when deriving fragility curves i e overtopping piping or macro instability orlandini et al 2015 for these reasons we treat breaching water levels namely water levels triggering levee failures as deeply uncertain 3 2 simulation model the simulation model follows three steps 1 the generation of hydrological events along the po river and its main tributaries secchia and panaro 2 the propagation of the generated events using a hydrodynamic model 3 the assessment of economic damage in the flooded compartments the following subsections briefly introduce the modeling steps more detailed information is provided in the supplementary material 3 2 1 generation of hydrological events we focus on the generation of alternative 0 2 annual probability flood events at the upstream po river s cross section of borgoforte these events are expected to cause flooding in the compartments as the levee system is designed to withstand the 0 5 annual probability flood events the 0 2 annual probability events along the po river at borgoforte are generated based on the flood duration frequency fdf curve derived by maione et al 2003 and the historical flood hydrographs of the po at borgoforte reported in tanda et al 2001 the aim is to generate 0 2 annual probability events of decreasing peak flow for increasing duration such that the fdf curve is always met after the 0 2 annual probability events are generated for the po river a gaussian copula is used to capture the dependence between the generated events in the po river and in its main tributaries secchia and panaro similarly to what is done by curran et al 2020 the correlation between events in the po river and those in the tributaries is modeled using data from the hypeweb platform of the swedish meteorological and hydrological institute smhi these data consist of 29 years of simulated daily mean discharges for the three rivers and are used to generate a gaussian copula between volumes and discharges of the po secchia and panaro rivers after generating the 0 2 annual probability flood event for the po river as described above events of the tributaries are found by conditional sampling of the gaussian copula 3 2 2 hydrodynamic model flood hazard is simulated with a quasi 2d model implemented through the hec ras software domeneghetti et al 2015 in the model the main river is represented through cross sections retrieved from a lidar digital elevation model with a 2 m spatial resolution the levee protected floodplain is subdivided into 17 compartments which are modeled as storage areas connected to each other and or to the main channel by means of lateral structures or connections that reproduce existing levees the hydrostatic behavior of each storage area is represented through volume level curves thus in case of flooding water levels can be estimated from the water volumes exchanged with the main channel and or adjacent storage areas these curves are built using a 10 m resolution dem available for all italy tinitaly see tarquini et al 2012 3 2 3 economic impact assessment floods are often devastating events and the impact they cause includes loss of life damage to buildings infrastructures and the deterioration of ecosystems moreover flood impacts are larger for communities which are more socially and economically vulnerable however since the focus of this study is primarily methodological for practical reasons we model flood impact only as economic damage to residential buildings to do so we use the hypsometric vulnerability curves approach proposed by domeneghetti et al 2015 and the damage function developed by carisi et al 2018 typically the hypsometric curve of an area provides the percentage of the total area below a certain elevation the hypsometric vulnerability curve combines this information with the land use of the area thus providing the percentage of a land use class below a given elevation therefore if combined with damage functions the hypsometric vulnerability curve is a useful simplified graphical tool to quantify aggregated flood damage of large areas we calculate hypsometric vulnerability curves relative to urban areas in all compartments to do so we use data of residential buildings available from the geodata web platforms of the three italian regions involved in the case study namely emilia romagna lombardia and veneto data about asset values of buildings are retrieved from the italian revenue agency asset value data are provided in terms of euros per square meter m2 for different types of buildings and all italian municipalities every six months we define an asset value per compartment as the weighted average of all asset values where weights are given by the extent of urban area of the municipalities in that compartment for deriving the depth damage curve we used the square root regression model developed by carisi et al 2018 using loss data collected after the secchia river 2014 flood we account for the uncertainty in this relationship by using alternative regression models generated from the distribution of the fitting parameter 4 description of the analysis the analysis aims at applying the birdm framework introduced in section 2 the goal is to explore for each compartment how alternative assumptions regarding the distribution of deep uncertainties affect the robustness of damage reduction measures and their ranking the description follows the steps introduced in section 2 4 1 problem framing the adopted xlrm framework is shown in table 1 exogeneous uncertainties x relate to various hydrological features of the po river and its tributaries the damage function and the breaching water levels at each of the 52 levee stretches identified in the study area the considered structural measures l correspond to either doing nothing hereafter status quo levees raising hereafter raising or levees strengthening hereafter strengthening the simulation model r is the one introduced in subsection 3 2 the outcomes of interest m are the flood damages from the 0 2 annual probability events in each compartment 4 2 uncertainty analysis uncertainty analysis is conducted performing 4500 runs of the simulation model using a sobol sampling sequence sobol 1967 this is also known as quasi monte carlo analysis table 2 reports the selected sampling distribution for each uncertainty we stress that for the reasons outlined in subsection 3 1 uncertainties related to breaching water levels are considered as deeply uncertain it follows from the scenario neutral assumption that these are uniformly distributed with the range of physically plausible breaching water levels going from the levee crest height upper limit to the lowest level at which a breach can physically be triggered lower limit this latter is given by the highest point between the floodplain s height and the height of the levee protected area such that the necessary hydraulic gradient required for a breach to plausibly develop is guaranteed in the case study this is estimated to be approximately 2 m below the initial crest level for all levee stretches this yields a fragility curve as expressed by the thick blue line in fig 3 as indicated in fig 1 this step and scenario discovery are iterated therefore the uncertainty analysis is first carried out for the status quo next system vulnerabilities are identified through scenario discovery and measures that limit such vulnerabilities are defined finally uncertainty analysis is again performed assuming measures are implemented convergence plots are shown in the supplementary material as mean damage against number of simulations 4 3 scenario discovery scenario discovery is performed using the patient rule induction method prim algorithm friedman and fisher 1999 scenario discovery is used to explore the uncertainty space generated by sampling uncertainties as reported in table 2 except for the first three listed uncertainties namely duration shape class and shape of the po river s upstream hydrograph which are used to generate flood waves instead for the sake of interpretability scenario discovery is run on the peak discharges and volumes of the generated flood waves this implies the exploration of 59 variables the peak discharge and hydrographs volumes of the 3 rivers the breaching water levels at the 52 levees stretches and the damage model parameter scenario discovery is used to identify cases where flood damage in each compartment is larger than the 3rd damage quartile under status quo which is the assumed critical performance threshold scenario discovery provides an indication for each compartment of what levee stretches when failing are likely to lead to damages above the critical threshold based on scenario discovery results and a broader consideration of the geometry of the levee system we identify critical levee stretches and define structural measures after these measures are identified their performance is re assessed through uncertainty analysis step 4 2 4 4 belief analysis this step allows exploring how the ranking of structural measures changes under various alternative assumptions about the shape of the fragility curves structural measures are evaluated based on their robustness in providing satisficing performances this is defined as the probability of not exceeding the critical performance threshold which we define for each compartment as the flood damage equal to the 3rd damage quartile under status quo see section 4 3 this probability can be assessed for each measure via the method described in section 2 4 obviously the most robust measure is the one with the highest non exceedance probability of the performance threshold different hypotheses regarding the fragility curve may lead to different robustness values and thus result into a different ranking of measures in practice the choice of the alternative fragility curves should be derived through expert elicitations and reflect the uncertainty around levee breaching for the levee stretch of interest for simplicity we assume all fragility curves follow a normal distribution with the same standard deviation of 0 3 but different mean values these range from 0 6 m upper curve to 1 3 m lower curve below the height of the levee crest with steps of 5 cm each for a total of 15 fragility curves the range number of steps could be made larger finer according to the application as the computational cost of exploring a large number of alternative beliefs is very low all considered fragility curves are shown in fig 3 5 results and discussion this section reports the results of the analysis described in section 4 this section reports results from the uncertainty analysis and scenario discovery steps in subsection 5 1 and those from the belief analysis step in subsection 5 2 5 1 uncertainty analysis and scenario discovery the output of the uncertainty analysis of the current system status quo blue boxplot in fig 4 reveals that five compartments are flooded by the 0 2 annual probability flood events and they are all located upstream this happens because breaching at these compartments results in flood peak attenuation and hence in an unloading effect for the downstream compartments which are consequently not flooded for all the flooded compartments failure scenarios are identified as those where flood damages under status quo are higher than the critical performance threshold table 3 shows the results of scenario discovery for these failure scenarios high values of the damage model parameter are responsible for large flood damage in all compartments flood waves with large volumes along the po river are responsible for large flood damages only to the cross seccb compartment one of the most upstream for each compartment one can identify the critical locations namely those locations where should the levees fail large flood damage would occur identifying these critical locations provides a crucial indication on where structural damage reduction measures should be applied and scenario discovery supports their identification in the case of strengthening this information suffices as we simply assume that these stretches are strengthened in such a way that levee overtopping produces flooding without breaching the levee in the case of raising an additional choice needs to be made based on the height of the nearby levees to make the two structural measures comparable both raising and strengthening are applied to the same set of stretches in compartment cross seccb two stretches are identified as relevant for investing in flood protection namely stretches number 3 and 5 the levee at stretch 5 has an average crest level of 25 m a s l while the levee at stretch 3 is higher with an average crest level of 25 65 m a s l stretch 4 located between stretches 3 and 5 with an average levee crest level of 25 5 m a s l is also considered part of the intervention as the lowest average levee height along the compartment on the other side of the po river is about 25 8 m a s l levees of stretches 3 4 and 5 are raised up to this level following similar reasoning levee raising of 1 0 0 7 0 5 and 0 3 m is applied to stretches in compartments cross seccc ogli minc ogli minb secc panaa respectively in total the procedure leads to the implementation of structural interventions at 7 stretches as shown in fig 5 the performance of both raising and strengthening is assessed by performing two uncertainty analyses and results are shown in fig 4 looking at the total damage raising and strengthening both improve the current situation as they result in lower damage than status quo in the case of strengthening this benefit is much more significant similar results are found for cros seccb the compartment where most damage occurs and the one where the most upstream stretches subject to intervention are located although with a less remarkable difference strengthening is the best option also for compartment ogli minb for all other compartments it is found that either raising or strengthening may deteriorate performance with respect to status quo as the number of cases above the performance threshold increases this is due to right left and upstream downstream hydraulic interactions due to which structural measures reducing damage on a given river stretch may increase damage at the other side of the river or downstream ciullo et al 2020 for example for ogli mincb which is located opposite to cros seccb raising does not seem to bring much damage reduction as it performs similarly to status quo for the more downstream compartments i e cros seccc ogli mincc secc panaa and adig poe strengthening may significantly deteriorate performance with respect to status quo to the point that it may cause damage at compartment adig poe where there was none initially interventions were designed via scenario discovery with the goal of reducing damage above the performance threshold at each compartment to limit the effects of hydraulic interactions the uncertainty analysis and scenario discovery steps would need to be run iteratively until a satisfactory set of measures is achieved for the entire system a comprehensive design of the entire system requiring such an extensive iterative procedure of refinement of measures is beyond the scope of this paper 5 2 belief analysis following the description in section 4 4 this section reports how robustness values and the ranking of measures change for each compartment utilizing each of the 15 alternative fragility curves reported in fig 3 the ranking of the three measures is assessed based on their robustness in achieving satisfactory performances namely the probability of non exceeding the performance threshold see section 4 4 the higher this probability the higher a measure is ranked results of the belief analysis are reported in fig 6 in each panel and for each of the 15 alternative fragility curves colors indicate which measure ranks at the position indicated by the respective column for comparison table 4 reports results under the assumption of a deeply uncertain fragility curve compartment cross seccb is the only one for which alternative beliefs do not affect the ranking of measures at all as strengthening always ranks first followed by raising and status quo this is coherent with the overall performance under deep uncertainty for compartment ogli mincb and cross seccc raising is instead the first ranking measure regardless beliefs for both compartments the second and third ranked measures differ across beliefs and can be either strengthening or status quo with the latter being preferred when levees are believed to be weak and prone to failing at low water levels this is logical as structural measures are most effective under circumstances of weak levees for ogli mincb results are surprising since strengthening which is the best measure under deep uncertainty see table 4 never ranks as the first measure under changing beliefs this highlights how a high performance of measures under the scenario neutrality assumption may deteriorate when beliefs are introduced for cross seccc raising is the first ranked measure also under deep uncertainty followed by status quo and strengthening for compartments ogli mincc and secc panaa the first ranked measure likely to be the most relevant one from a practical viewpoint is sensitive to changes in belief as a shift from status quo to raising occurs depending on the belief about the strength of the levee for both compartments raising is preferable when the levee is believed to be weak and status quo takes over when the levee is believed to be stronger the opposite shift takes place as second ranked measure whereas the third ranked measure is always strengthening as this measure increases damages at these compartments due to upstream downstream interactions see previous section 6 conclusions in the present paper we introduced the belief informed robust decision making birdm framework birdm builds upon robust decision making rdm and it enables the assessment of the effects of alternative distributions of deep uncertainties on the robustness values and ranking of measures in principle the same results provided by birdm could be achieved by running a standard rdm analysis multiple times with alternative assumptions regarding the distribution of deep uncertainties this however would demand large computational power and may become unfeasible as soon as many alternative assumptions need to be explored the proposed approach instead requires almost no additional computational costs nor any new modeling efforts compared to rdm and it therefore allows the exploration of an arbitrarily large number of alternative distributions as such birdm extends and improves rdm as it allows assessing whether ambiguities lack of knowledge or disagreement regarding the distribution of deep uncertainties matter in the first place we demonstrated birdm by applying it to a flood management planning problem along the lower po river italy the goal was to identify the most robust structural measures and to assess whether and to what extent alternative assumptions about the levees fragility curves affect robustness values and the ranking of measures we find that birdm s results reveal changes in the ranking of measures because of changing assumptions about the levees fragility curves therefore the ranking emerging from the standard rdm is sensitive to alternative assumptions regarding such uncertainties this can be decision relevant especially when these changes affect which measure ranks first birdm also supports the identification of which belief triggers such a change in the ranking thus enabling the assessment of what beliefs lead a given measure to have a specific rank this may trigger further investigations regarding for example the reliability of such rank changing beliefs finally birdm can be used in a stress test fashion to assess under what assumptions a given a measure no longer performs as desired data and code availability results of the full analysis and the code need to perform the scenario discovery and belief analysis steps can be found at https github com aleeciu birdm paper declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this project has received funding from the european union s horizon 2020 research and innovation programme under the marie skłodowska curie grant agreement no 676027 appendix a supplementary data the following is the supplementary data to this article supplementary data supplementary data appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105560 
25492,the use of data driven techniques such as artificial neural network ann models for outdoor air pollution forecasting has been popular in the past two decades however research activity on the uncertainty surrounding the development of ann models has been limited therefore this review outlines the approaches for addressing model uncertainty according to the steps for building ann models based on 128 articles published from 2000 to 2022 the review reveals that input uncertainty was predominantly addressed while less focus was given to structure parameter and output uncertainties ensemble approaches have been mostly employed followed by neuro fuzzy networks however the direct measurement of uncertainty received less attention the use of bootstrapping bayesian and monte carlo simulation techniques which can quantify uncertainty was also limited in conclusion this review recommends the development and application of approaches that can both handle and quantify uncertainty surrounding the development of ann models keywords air pollution forecasting artificial neural networks uncertainty quantification bayesian monte carlo simulation fuzzy data availability no data was used for the research described in the article 1 introduction 1 1 data driven models in air pollution forecasting the use of data driven models in outdoor air pollution ap forecasting has been widely reported in the literature in the last two decades shahraiyni and sodoudi 2016 cabaneros et al 2019 masood and ahmad 2021 the attractiveness of data driven models can be explained in two respects 1 better performance over traditional approaches and 2 the emergence of big data and more powerful computing software data driven models have been shown to understand the complex dynamics between environmental variables and outdoor ap without using physics based formulae this allows researchers to bypass the assumptions and strong theoretical requirements to employ traditional approaches such as gaussian dispersion 3 d gridded eulerian photochemical and lagrangian trajectory models as such data driven models for ap forecasting have been shown to be effective alternatives to the traditional deterministic ones gardner and dorling 1998 lv et al 2016 cabaneros et al 2019 zhao et al 2019 big data and more powerful computing resources have also paved the way for research communities to apply data driven models in ap forecasting bishop 2006 montáns et al 2019 in particular the smart city concept that integrates information and communication technology ict and fixed mobile sensors have generated a tremendous amount of data especially outdoor pollution measurements which has enabled decision makers in providing early warnings to citizens in urban cities bekkar et al 2021 finally the past few years have witnessed the development and emergence of accessible and powerful computing tools that support machine learning processes giray 2021 free and open source programming languages such as python r and java have been popularly used for coding many data driven techniques tiobe 2022 popular data driven approaches utilised for ap forecasting include statistical models e g multiple linear regression mlr ng and awang 2018 and autoregressive integrated moving average arima machine learning ml e g artificial neural network ann gardner and dorling 1997 support vector machine svm lu and wang 2005 fuzzy expert system heo and kim 2004 and more recently deep learning models e g deep neural network dnn freeman et al 2018 the approaches clearly have their pros and cons which have been thoroughly discussed in the literature chen et al 2008 masood and ahmad 2021 ann models are among the most employed data driven tools for ap forecasting anns are parallel computing structures that mimic the information processing paradigm of the human brain this makes them capable of learning from any previously unknown information from any given training dataset mcculloch and pitts 1943 basheer and hajmeer 2000 ann model development generally consists of eight steps 1 data collection 2 data preprocessing 3 input variable selection 4 data division 5 model architecture selection 6 model structure optimisation 7 model training and 8 model validation a detailed description of each step can be found elsewhere maier et al 2010 cabaneros et al 2019 however performing each step entails the selection of one or more methods processes which are problem specific this lack of a clean cut approach makes the development of ann models not straightforward consequently the influence of the various approaches in performing each of the steps on the model results has become a well established research area in their review of articles dealing with the use of anns in outdoor ap forecasting cabaneros et al 2019 revealed that novel methods for implementing one or a combination of the steps for building ann models have been proposed to outperform the traditional ones the authors reported that the articles mainly focused on the development of more novel and sophisticated predictor selection techniques model architectures and model training algorithms a similar observation was reported in earlier related works by maier et al 2010 and wu et al 2014 which reviewed case studies using ann models but applied in forecasting hydrological variables finally a recent review by masood and ahmad 2021 on the use of techniques based on artificial intelligence ai in ap forecasting has also shown similar findings the authors revealed that models with deeper architectures e g dnn models have been employed and reported to provide superior predictions of ap levels 1 2 uncertainty incurred from developing data driven models given their black box nature ann models cannot provide explicit insights regarding the influence of several model building choices e g inputs architecture structure and training parameters on their results this ambiguity surrounding the modelling process exists and has been commonly referred to as model uncertainty model uncertainty has been reported to limit the potential of using ann models especially in tasks involving decision making vardoulakis et al 2002 borrego et al 2008 in particular the uncertainty surrounding ann models that were not carefully designed can limit the reproducibility and reliability of model results arhami et al 2013 elshorbagy et al 2010 noori et al 2010 current efforts have mostly focused on the improvement of the point estimates of ap levels while the incorporation of model uncertainty has received less attention maier et al 2010 cabaneros et al 2019 kasiviswanathan and sudheer 2017 has reviewed research articles dealing with hydrological modelling that employed methods addressing model uncertainty they revealed that the methodological issues for building ann models not model uncertainty have been mostly examined among those that tackled uncertainty only a few investigated the mutual interaction among sources of prediction uncertainty e g inputs training parameters and model structure given the increasing popularity of ann models a thorough review of existing research that accounts for uncertainty is significant hence the main objective of this paper is to provide an extensive framework of methods used for addressing the uncertainty surrounding the development of ann models for outdoor ap forecasting through the results of this paper the authors aim to promote good practice in reporting ann model results by accounting for both accuracy and uncertainty to the best of the authors knowledge a comprehensive survey of articles that deal with the uncertainty of ann models for ap forecasting has not yet been undertaken another novelty of this review paper is that it aims to describe the interplay between various sources of model uncertainty this is carried out by relating each uncertainty source to the eight steps of building ann models the remainder of this review paper is organised as follows in section 2 details regarding the methods for selecting the appropriate articles for this review are presented in section 3 the sources of model uncertainty and the approaches for addressing them are described section 4 presents the methods utilised for quantifying the model uncertainty section 5 provides the conclusions of this review while section 6 presents a number of recommendations for future research 2 overview articles that deal with the application of ann models in outdoor ap forecasting were collected from international peer reviewed journals databases such as sciencedirect ieee access plos one acm springer taylor francis online google scholar and mdpi were searched for relevant literature published from january 2000 to august 2022 the selection process of this review consisted of three stages since the main focus of this paper is on the incorporation of uncertainty that arises from ann model development the search items were firstly narrowed down to include the terms uncertainty and or methods that are well known to account for uncertainty the search items for the methods included air pollution forecasting artificial neural networks deep learning machine learning ann deep neural networks bayesian neural networks monte carlo ensemble confidence interval sensitivity analysis anfis and fuzzy neural networks with different combinations a second search query was carried out containing neural networks and air pollution because many of the appropriate articles for this review do not necessarily have to mention the term uncertainty the third selection stage from both identified and unidentified papers was performed in an ad hoc manner based on the subject matter of this review for instance many articles do mention the term uncertainty yet only a subset of them addressed model uncertainty in their model building process this observation can be made for other articles mentioning one or a set of search terms mentioned above another important criterion for inclusion in the review is that the said methods need to be applied in conjunction with anns for instance several articles on plain models solely applying fuzzy inference systems and linear forecasters were removed the results from a recent review article by cabaneros et al 2019 were also utilised to locate the relevant articles for this review lastly articles presented at conference proceedings were manually removed from the initial list of search results the search methodology above has identified 128 relevant articles for this review table 1 provides a list of journals alongside their respective latest impact factors where the selected articles were published the leading position in terms of the number of publications was held by both the atmospheric environment and atmospheric pollution research journals which accounted for approximately 12 of the total number of articles both the environmental modelling software and ieee access journals had the next highest proportions of articles 8 each followed by science of the total environment 7 the rest of the identified journals had fewer proportions of articles e g 5 articles or less details of the selected articles such as the name of the authors year of publication study location air pollutants examined and methods used to address model uncertainty are given in table 2 it should be noted that several articles employed two or more approaches in addressing uncertainty in their model development process the distribution of papers by years of publication is shown in fig 1 there has been a growing number of articles on ap forecasting using anns that address model uncertainty more recently in particular approximately 65 of the identified articles have been published since 2016 alone not much increased activity has been observed between 2006 and 2011 while a sudden growth of activity has occurred post 2015 however it is worth noting that these values are still comparably lesser than the overall number of published articles per year that deploy ann models for ap forecasting cabaneros et al 2019 nonetheless it is still evident that a considerable amount of attention has been aimed towards the point prediction of ap levels using ann models while handling uncertainty 3 sources of model uncertainty to characterise the uncertainty surrounding ann model development several notations need to be described first any regression type ann model generally takes the following form 1 y ˆ f x w σ n o i s e where x r n d represents the input vector e g corresponding to n samples and d predictors y ˆ r n is the vector of model outputs e g estimates of vector y r n containing the actual observations w is the vector of model parameters e g connection weights and biases f is a function describing the dynamics between x and y and σ n o i s e is the irreducible or data noise that directly influences model errors uncertainty surrounding the development of ann models has been characterised in a plethora of ways yet they can be generally categorised as either aleatoric or epistemic kiureghian and ditlevsen 2009 aleatoric uncertainties refer to those inherent to the dynamics of systems under investigation this includes the stochasticity of physical and chemical properties of environmental systems predictor excitations and noisiness or imperfections of the collected data on the other hand epistemic uncertainties occur during the modelling stage from the lack of knowledge about the underlying system being studied lack of data and development of imperfect models epistemic uncertainty is also referred to as model uncertainty as this is caused by the limitations encountered during the model development process the two terms in eq 1 correspond to the sources of model uncertainty and aleatory uncertainty respectively hence the total uncertainty of y ˆ assuming that the two uncertainties are independent can be characterised as follows 2 σ 2 σ e p 2 σ a l 2 where σ e p 2 refers to the model uncertainty and σ a l 2 refers to the aleatory uncertainty it is often a challenge to classify the uncertainties encountered during the modelling stage as real world applications like outdoor ap forecasting involve both forms of uncertainties this review will however limit its scope by only focusing on model uncertainty in particular the authors propose to characterise model uncertainty by identifying its sources during the building stage of models that is each step in the ann model development process entails a combination of several methods and it is reasonable to link uncertainty to all of them as such this review closely highlights the link between sources of model uncertainty and the said steps fig 2 shows the eight general steps for building ann models and their relationship with the sources of uncertainty which will be discussed in the following subsections detailed information on the inner workings of ann models can be found from several references hagan et al 1995 gardner and dorling 1998 maier et al 2010 bishop 2006 3 1 input uncertainty input uncertainty results from the lack of complete knowledge on the use of appropriate input x and y for training ann models given the data intensive nature of ann models input uncertainty is generally influenced by factors such as data density and number of predictors which can lead to varied model structures training parameters and irreproducible results data density refers to the number of samples n needed to train an ann model although model uncertainty is commonly regarded to be inversely proportional to data density lai et al 2022 there is no general formula for identifying the optimal amount of data needed for training without incurring unnecessary computational costs on the other hand certain types of predictors e g air pollutant meteorological and temporal variables have been known to better capture the important system dynamics that ann models attempt to simulate however this results in the inclusion of too few or too many model predictors d which greatly influences input uncertainty 3 2 structure uncertainty structure uncertainty results from the simplification ambiguity and or lack of information of the governing equation s used by ann models to describe a real world process shrestha and solomatine 2008 due to the empirical nature of ann models dealing with structure uncertainty seems inevitable structure uncertainty arises from the selection of the following i model architecture e g feedforward recurrent etc ii transfer function e g log sigmoid hyperbolic tangent linear etc and iii structure e g number of hidden layers and nodes it should be noted that the term structure has been used in the literature in two different ways as a governing formula of a model and as a specific term among ann modellers referring to the dimension of one or more hidden layers model architecture and transfer function both govern the functional relationship f in eq 1 which then determines the model structure e g the dimension of w as a result structure uncertainty directly influences the uncertainty surrounding of model parameters 3 3 parameter uncertainty parameter uncertainty refers to the lack of a general method for identifying the optimal set of network parameters w as well as the selection of non optimum algorithms for training ann models a common practice to address parameter uncertainty is the assignment of a range of training parameters from which values are then initially and randomly selected hagan et al 1995 cabaneros et al 2019 however such parameter values are impossible to replicate due to the stochastic nature of most training algorithms data division also has a great impact on the level of parameter uncertainty the selection of a subset of the input data for model training e g d x n y n n 1 n d given n d training samples also affects how the network connection weights are initialised and optimised various data division schemes e g ad hoc stratified v fold cross validation and random splitting methods bring varying levels of complexity to training parameters since the number of predictors is directly proportional to the number of connection weights that need to be calibrated input uncertainty also directly impacts the magnitude of parameter uncertainty 3 4 output uncertainty output uncertainty pertains to the lack of reliability of ann model results either due to the use of inappropriate validation techniques or the inability to replicate the same accuracy of point predictions directly linked to parameter uncertainty output uncertainty limits the ability of ann models to produce similar quality of results output uncertainty is often referred to as the total model uncertainty which is described as the sum of all uncertainties surrounding all steps in the ann model development process e g the first term in eq 2 however the majority of the identified validation techniques in the literature only deal with the measurement of the accuracy of the prediction outputs using the training and testing sets maier et al 2010 consequently the model accuracy indices presented in most case studies are difficult to replicate which often leads to the difficulty of future modellers to build upon previous results 3 5 results as shown in fig 3 input uncertainty was the most addressed type of uncertainty source e g 112 times compared with 59 52 and 14 occasions on which parameter output and structure uncertainty sources were addressed respectively input uncertainty was addressed alongside parameter and output uncertainties in 43 of the 112 instances furthermore structure and parameter uncertainties were simultaneously dealt with on 5 occasions however the incorporation of all uncertainty sources only occurred twice in this review 4 methods for addressing model uncertainty in this review paper the methods used to address model uncertainty have been classified into eight types namely 1 bootstrapping 2 bayesian 3 fuzzy method 4 monte carlo simulation 5 optimisation based 6 sensitivity analysis 7 ensemble and 8 miscellaneous approaches the classification is similar to those presented in alvisi and franchini 2011 and kasiviswanathan and sudheer 2017 however a separate type was assigned to ensemble approaches since their use has been prevalent in the field of ap forecasting during the covered time period of this review 4 1 bootstrap method the bootstrap method or bootstrapping is an intensive resampling technique with replacement that operates under the assumption that input samples or bootstraps follow the statistical characteristics of the population and mimic the underlying random component of the modelled process efron 1979 kasiviswanathan and sudheer 2017 bootstrapping is carried out by sampling various realisations of input output patterns to estimate statistical characteristics such as bias variance distribution functions and confidence intervals belayneh et al 2016 as such bootstrapping has been applied to estimate the confidence interval of the ap predictions which can be used to quantify output uncertainty see fig 4 in theory the utilisation of more bootstraps should provide a more reliable estimation of the confidence bounds on the model error indices however there is no general formula for determining the optimal number of bootstraps table 3 provides details of the studies that employed the bootstrap method including the different number of bootstrap samples used and the statistical indices computed for each of the bootstrap samples three studies utilised at least 5000 bootstrap samples which is considered a good practice chernick 1999 although the computational costs should always be considered bowden et al 2005 the reported studies also revealed the improved reliability of their model results after adopting the bootstrap method grivas and chaloulakou 2006 calculated the standard error of their calculated performance indices from the test results of their predictive models their findings revealed that the associated standard error from the bootstraps of the best performing models were also the lowest values ibarra berastegi et al 2008 evaluated the overall performance of their models by estimating the 95 confidence levels of the statistical indices obtained from the model results the authors carried out this method if the mean values of the said indices for the two models are quite similar several studies also applied a similar methodology involving the calculation of 95 confidence intervals of their model results and expressed them as error bars other authors also used any confidence intervals lying entirely above zero as indicators that a model performs significantly better than the benchmark ones voukantsis et al 2011 peng et al 2017 han et al 2021 noori et al 2010 accounted for the output uncertainty of their models by showing the plots of the range 95 confidence intervals for their ap level estimates the authors calculated the 95 prediction uncertainty 95 ppu of their developed models by finding the 2 5th and 97 5th percentiles of the cumulative distribution of every simulated ap level result metrics such as the d factor abbaspour et al 2007 e g the average distance between the upper and lower 95 ppu and r 2 helped the authors determine their best performing model 4 2 bayesian method bayesian method is an approach based on bayes theorem which states that any prior beliefs regarding an uncertain quantity are updated on the basis of new information to yield a posterior probability of the unknown quantity in detail the method begins by defining the network weights w as a probability density function pdf a prior pdf p 0 w is assigned to the network parameters which is then updated using the training data d and bayes theorem to yield the posterior pdf p w d as 3 p w d p d w p 0 w p d by means of the posterior distribution the predictive probability distribution of the model output can then be estimated as follow 4 p y ˆ d p y ˆ x w p w d d w the integration of the bayesian method with ann models or bayesian regularised neural network brnn was first employed by mackay 1992 and neal 1992 to overcome model overfitting and complexity fig 5 illustrates how brnn models vary from standard ann models which only utilise a single optimum vector w in summary fig 6 shows the general schematic of how brnn models are employed by case studies to handle parameter uncertainty the use of brnn models also provides predictions with extra information regarding the precision of the outputs in the form of error bars of the confidence intervals which are very important metrics if the reliability of model results is of particular concern bishop 1995 table 4 reveals a few identified studies that employed the bayesian regularisation method all identified studies except the one by solaiman et al 2008 reported superior model performances of brnn models when compared to a range of benchmark models from regression to deterministic models 4 3 fuzzy method fuzzy method is based on fuzzy logic fl that operates by using if then rules fl deals with high level reasoning using linguistic information acquired from domain experts as such fl provides approximate reasoning and explanation abilities which are important attributes of models employed in real life operations especially air pollution forecasting mishra and goyal 2016 an fl based system has three main phases 1 fuzzification 2 inference and 3 defuzzification see fig 7 the fuzzification process is where the numerical values of the predictors are transformed into membership functions mfs various types of mfs include the triangular gaussian and trapezoidal the number and type of mfs per predictor are usually determined empirically and can be decided by experts on the basis of experiment observation and experience mishra and goyal 2016 the gaussian mf which is based is commonly used due to the nonlinear dynamics between predictors inference then follows as the membership grades are processed through a set of if then rules to generate a fuzzy output finally defuzzification takes place as the fuzzy output is transformed into a quantitative or qualitative output yeganeh et al 2018 however the fuzzy method lacks the self learning capabilities exhibited by ann models on the other hand ann models are not capable of interpreting linguistic information nunnari et al 1998 pao 1989 the combination of fuzzy principles and anns e g the adaptive neuro fuzzy inference system anfis can therefore analyse any form of information e g numeric linguistic and logical this makes anfis models capable of addressing input uncertainty by revealing the influence of their model inputs on their outputs according to the presented if then rules table 5 provides a list of studies that employed anfis models in forecasting ap levels alongside the type of mfs they utilised however the identified papers only focused on improving the accuracy of anfis model predictions similar to the findings of a related review by kasiviswanathan and sudheer 2017 4 4 monte carlo simulation monte carlo simulation mcs is a sampling technique used for obtaining a probabilistic approximation to the solution of an optimisation model metropolis and ulam 1949 rubinstein 1981 mcs operates by sampling different realisations of model inputs and or parameters by assigning ranges and pdfs to each predictor kasiviswanathan and sudheer 2017 as illustrated in fig 8 the pdfs of each predictor are then propagated through f in order to yield the pdf of the model predictions y ˆ as such mcs has been performed to handle the input and output uncertainty of ann models the application of mcs for uncertainty analysis in ap forecasting with ann models was only reported three times in this paper firstly ding et al 2016 carried out mcs to address the parameter uncertainty when developing their ann models trained via sparse response back propagation algorithm the authors determined the mean performance of their proposed models based on ten different patterns of weights and biases secondly noori et al 2010 performed mcs to quantify the output uncertainty of their developed ann and anfis models the authors initially generated random samples according to the probability distribution of their model inputs yielding thousands of model outputs the scheme was repeatedly performed until the results of a new run do not affect the probability distribution of the output variable the authors used two metrics in their uncertainty analysis namely the d factor abbaspour et al 2007 and the 95 percent prediction uncertainties 95 ppu the authors also utilised their mcs results to provide plots of the range 95 confidence intervals for their model forecasts during the training stage finally mokhtari et al 2021 incorporated uncertainty quantification methods into the predictions of their proposed cnn lstm models the authors constructed prediction intervals pis for their predictions using mcs dropout and quantile regression methods specifically two metrics for pis e g prediction interval coverage probability picp and mean prediction interval width mpiw were calculated in the study 4 5 genetic algorithm genetic algorithm is an optimisation method based on the idea of the survival of the fittest from the mechanics of genetics it provides robust solutions for highly complex non linear search and optimisation problems holland 1975 fig 9 illustrates the flowchart of the standard processes involved when performing genetic algorithm the algorithm works by initialising a competitive set of possible solution candidates e g chromosomes and then the solutions are set through the process of natural selection the solution candidates are then evaluated through a fitness function or objective function which ranks the chromosomes in the population fitness functions are formulated depending on the problem being solved the selection of parent chromosomes is then performed which entails two parents for the crossover and the mutation crossover involves the exchange of information between two parents in the mutation stage the genes of the chromosomes of the crossed offspring are changed the entire process is carried out until a certain condition is met michalewicz 1996 genetic algorithm has been integrated with ann models for ap forecasting to address model uncertainty for instance the use of the algorithm to optimise model weights and biases was observed 5 times kadiyala et al 2013 de mattos neto et al 2017 zhai and chen 2018 feng et al 2011 ibarra berastegi et al 2008 the said papers reported superior performance of ann models when trained using the method however the use of genetic algorithm does not always guarantee superior model performance although despite being able to reduce model complexity grivas and chaloulakou 2006 the method was also carried out to address input uncertainty via predictor selection on 7 occasions grivas and chaloulakou 2006 hájek and olej 2012 elangasinghe et al 2014 siwek and osowski 2016 dotse et al 2018 liu et al 2019a photphanloet and lipikorn 2020 on the other hand elangasinghe et al 2014 tackled parameter uncertainty by employing the method to optimise the step size momentum rate and processing elements of their proposed ann models finally the use of genetic algorithm to handle both input and parameter uncertainties was observed three times ibarra berastegi et al 2008 employed the method during data division to optimise the combination of training and validation sets in terms of mean standard deviation maximum and minimum values the authors also used the method to identify the most relevant predictors of their proposed models de mattos neto et al 2014 employed the method to optimise several variables of their mlp model such as the number of predictors in terms of time lags number of hidden nodes and training parameters a similar method for addressing both input and parameter uncertainties was also carried out by de mattos neto et al 2015 in their development of a hybrid mlp model 4 6 sensitivity analysis sensitivity analysis is a general method used for assessing the relative importance of variables selected as inputs for an ann model the method is usually performed to reduce network complexity by eliminating unnecessary predictors while keeping the significant ones by examining the variations of the model output by the minor perturbations of the predictors sensitivity analysis can account for input uncertainty in the following the application of sensitivity analysis for predictor selection are presented niska et al 2005 performed a sensitivity analysis alongside moga to identify an optimal set of predictors for their mlp models the authors defined the sensitivity of their predictor subset as the absolute difference between the model performance achieved by using a predictor subset and the model performance achieved when using all predictors solaiman et al 2008 selected the predictors for their models based on linear autocorrelation and partial autocorrelation analysis and nonlinear sensitivity analysis they calculated a metric called the relative sensitivity of a predictor which is the ratio between the standard deviation of the model outputs and the standard deviation of the predictor the authors initially performed partial autocorrelation analysis between past predictor and predictand values to identify significant time lags secondly sensitivity analysis was then applied for the final stage of screening predictors according to the identified significant lags zito et al 2008 carried out a sensitivity analysis by studying the response of their models to small and equal changes of the predictors they found out that if an increase in predictor value causes a significant change to the model output the examined predictor should be retained in the model kadiyala et al 2013 carried out predictor selection by performing analysis of variance anova alongside the regression tree method shaban et al 2016 investigated the influence of incorporating multiple types of predictors e g temporal meteorological and gaseous on the performance of their models they used metrics such as prediction trend accuracy pta and rmse to assess the results of the models trained using multiple combinations of predictors stamenković et al 2017 performed sensitivity analyses of their model predictors through correlation analysis in conjunction with calculating the variance inflation factor vif which is based on the linear relationship between predictors the authors sequentially removed predictors with highest the vif values which indicate multicollinearity between predictors shams et al 2021 performed a sensitivity analysis by setting the value of one predictor within the range of the standard deviation while the rest were fixed at their mean values then the standard deviation of the model outputs for each predictor changes was measured as model sensitivity for that predictor the authors then selected the variables with high values in the output standard deviation on the other hand sensitivity analysis is usually performed in conjunction with other methods 4 7 ensemble approach ensembling is a modelling approach that integrates the prediction results of multiple models into one final output one main advantage of the approach is the ability of the ensemble model or meta learner to possess the individual strengths and simultaneously overcome the limitations of the single models base learners chen et al 2008 a common key limitation exhibited by single models is the inability to accurately predict peak ap levels kolehmainen et al 2001 niska et al 2005 grivas and chaloulakou 2006 however the independence of the base learners and their comparable performance have been pointed out as two important conditions for any ensemble model to perform well haykin 1999 kuncheva 2004 ensembling creates multiple input output realisations of the examined ap system which could be used to account for the input parameter and output uncertainties when developing the models in particular the resulting ensemble model will contain some diversity and the variance of its predictions can be interpreted as an estimation of model uncertainty lai et al 2022 this review classifies ensemble ann models into two types namely the model and data intensive ensemble models see fig 10 under the model intensive type the input data are being fed to train multiple base learners and the results are used as inputs to the meta learner on the other hand the data intensive type initially extracts important features of the input data to train the base learners and then integrates the multiple outputs based on the feature extractor technique used it is worth noting that the base learners need not consist of entirely ann or ml based models in general as shown in the discussion below many ensemble models are comprised of linear statistical and non linear ml base learners table 6 shows the identified works that adapted the ensemble modelling approach the review has identified a variety of meta learners employed to develop ensemble models the majority of the said meta learners are ann based such as mlp svr elm and lstm models other non ann meta learners have been identified including rf fusion and weighted averaging siwek and osowski 2016 and linear regression and normal copula based models de mattos neto et al 2021 other forms of base learners to create ensemble models were also identified in this review catalano et al 2016 employed an ensembling approach by choosing the maximum of the predictions of their base learners since they had the tendency to underestimate pollution peaks gong and ordieres meré 2016 employed a stacking ensemble algorithm that linearly combined the results of their base learners the authors used the cross validation data and least squares under non negativity constraints to determine the coefficients of the linear combination di et al 2019 utilised a generalised additive model that accounted for geographical differences to predict daily pm2 5 levels at a resolution of 1 km 1 km across a certain geographical area through ensembling in particular the authors developed an ensemble model which incorporated the level of ap concentration against thin plate splines of latitude and longitude as such the results of their base learners were geographically weighted instead of the traditional approach of using constant weights for each base learner similarly valput et al 2019 adopted an ensemble approach to provide local predictions using regional numerical ap predictions their proposed model utilised the forecasts of seven neighbouring air quality models and combined them by calculating their average and weighted average values finally sharma et al 2020 adopted a method that involved entirely non ann base learners e g rf volterra m5 tree and mlr models and an lstm meta learner a few special cases of model intensive ensemble methods have also been found in this review for instance mahajan et al 2018 proposed a hybrid ensemble approach that utilises both linear and nonlinear base learners in particular the authors initially used an arima model to capture the linear tendencies of their pm2 5 time series the model residuals e g difference between actual time series and model results were then fed to a lagged input ann model the results of both base learners were then given equal weights before they were combined gu et al 2019 employed several svr models as base learners and then applied pruning techniques to eliminate the undesirable learners the results of the selected base learners were then fed to an svr based regression model for stacking finally han et al 2020 employed an approach of integrating the results of two bayesian rnn base learners according to their uncertainty measures in particular the authors employed two weighting methods to fuse the results of the base learners 1 uncertainty averaged outputs and 2 selection of a base learner output with the lowest uncertainty measure under the data intensive category the most commonly used feature extractors were wavelet based techniques see table 6 for instance many authors employed wavelet transformation to decompose their original data into several coefficients which were then fed to their base learners or feature learners wavelet decomposition techniques were especially applied to improve the performance of plain ann models when dealing with peak ap concentration levels shekarrizfard and hadad 2012 feng et al 2015 other similar feature extractors were utilised such as empirical mode decomposition emd bai et al 2019 and its variants such as the noise assisted emd dong et al 2021 and clustering algorithm macia g et al 2019 in general the identified papers suggested the superiority of their proposed ensemble models to the involved base learners however the trade off between the overall complexity of ensemble models and performance should be carefully accounted for especially in real world scenarios where computational cost is a major constraint cabaneros et al 2019 a few special cases of data intensive ensemble methods have also been identified for instance de mattos neto et al 2017 employed an approach that involved forecasting both the ap time series and model residuals the authors only conducted the latter if the residual is not white noise e g a series of independent and identically distributed random values with zero mean and constant variance a nonlinear base learner e g mlp model was utilised for the initial forecasting while both linear and nonlinear ones e g arima mlp and svr models for the residual the approach falls under the ensemble type since both forecasting results were all combined using an mlp model an earlier work by de mattos neto et al 2015 applied the same method which included the forecasting of the model residuals however the work was model intensive in that the residuals of the final ensemble model output were further fed to a series of mlp models until the scheme generates residuals having a white noise behaviour finally de mattos neto et al 2020 addressed the seasonality of pm2 5 and pm10 pollutants by decomposing the time series into non overlapping monthly partitions and then applied several models e g as meta learners their decomposition method involved creating n subseries from the original time series according to the coefficient of variation of each subseries 4 8 miscellaneous approaches other methods that address model uncertainty have also been identified in this review and are briefly described below several techniques have been adapted to identify the most significant model predictors hence tackling input uncertainty address method yeganeh et al 2018 boruta algorithm alkabbani et al 2022 correlation analysis perez and salini 2008 he et al 2014 mishra et al 2015 bai et al 2016 stamenković et al 2017 alimissis et al 2018 tao et al 2019 dong et al 2021 menares et al 2021 zhang et al 2021 kristiani et al 2022 tian et al 2022 fourier analysis hrust et al 2009 grey correlation analysis zhu et al 2018 individual smoothing factors antanasijević et al 2013 lasso yeganeh et al 2017 mdsf algorithm photphanloet and lipikorn 2020 and stepwise regression ordieres et al 2005 agirre basurko et al 2006 singh et al 2012 russo et al 2013 russo and soares 2014 siwek and osowski 2016 many approaches have also been employed to address structure uncertainty by determining the optimal number of nodes in the hidden layer and consequently reducing overall model complexity dutot et al 2007 employed a stepwise method using bic like information criterion to tackle structure uncertainty by determining the optimal number of nodes in the hidden layer chelani et al 2002 used a convergence criteria according to the error minimisation criterion and a formula by kinnebrock 1995 catalano et al 2016 applied the weight generalisation formula by bishop 1995 which utilises an extra error term that penalises small weights during model training finally agirre basurko et al 2006 employed a generalisation rule by amari et al 1997 in which the model is considered optimally trained when the ratio of the number of training samples to the number of connection weights exceeds 30 hasham et al 2004 adopted an approach based on factorial design concepts to assess the influence of model predictors structure and parameters on the model output box et al 1978 the approach operates by moving various input factors between high and low settings in combination with other input factors hence investigating the interaction of uncertainties between the said factors empirical formulae which provide either or both the upper and lower bounds of the optimal number of nodes in the hidden layer were also used formula proposed by fletcher and goss 1993 he et al 2014 abdullah et al 2019 empirical formula by shen et al 2008 with trial and error bai et al 2016 empirical rule by kalogirou 2003 radojević et al 2018 a formula by kotu and deshpande 2018 and roiger 2017 photphanloet and lipikorn 2020 and a method by tian et al 2022 dutot et al 2007 used a metric called leverage which provides a confidence interval of the predicted values of their models to address output uncertainty leverage is a metric used to assess the effect of a particular observation on the fitted regression according to the position of the observation in the predictor space monari and dreyfus 2002 powerful swarm intelligence algorithms have also been used to optimise the training parameters of ann models for instance mo et al 2019 utilised the whale optimisation meta heuristic algorithm to obtain the best parameters of their proposed ensemble elm models caraka et al 2019 employed both pso and backpropagation algorithms to train their ann models shahid et al 2021 employed a boosting algorithm to improve the results of their initial model e g svr model to address input uncertainty the boosting technique works by assigning weights to each instance of the input data and using them to train the svr model then identifying and updating the weights of the misclassified instances the weighted instances are finally passed to several models including mlp rf decision tree mlr ridge regression gradient boosting and svr models for the final training and prediction tasks wang et al 2022a applied a probabilistic approach by calculating the distribution of their model prediction errors and comparing them to create confidence intervals of results the approach was based on the gaussian and t location scale distributions which were determined according to the r 2 pe and rmse indices the authors also calculated three metrics such as average coverage error ace prediction interval normalised average width pinaw and prediction interval coverage probability picp to measure the interval predictions zheng et al 2022 decomposed their original pm2 5 time series into several subseries using wavelet transforms and employed the reinforcement learning algorithm e g q learning for the predictor selection of each subseries 4 9 results the distribution of papers by the methods used to handle model uncertainty is shown in fig 11 the majority of the identified papers e g 45 occasions adapted the ensemble modelling approach to address uncertainty among these approximately 69 have been recently published since 2019 indicating the emergence of more sophisticated approaches as ann computing technologies also become more powerful fuzzy systems were applied alongside ann models 20 times while correlation analysis for predictor selection was applied 18 times the utilisation of the global search procedure e g genetic algorithm occurred 15 times followed by sensitivity analysis of trained models 11 times the number of papers in which bootstrapping bayesian and empirical methods were applied was uniform varying between 6 to 7 compared with only three instances where the mcs was applied finally the use of alternative methods for addressing uncertainty was reported 28 times table 7 presents the pros and cons of the identified methods that can address model uncertainty note that the methods falling under miscellaneous approaches were excluded from the summary the use of the said methods has been reported to improve the forecasting accuracy of ann models which is usually the primary motivation of most studies this is especially true for those methods handling input and parameter uncertainties to reduce model complexity however it can be seen that not all methods can directly quantify model uncertainty for instance fuzzy systems ga and sensitivity analysis can only account for uncertainty by addressing the ambiguity arising from predictor selection and parameter optimisation nonetheless the use of such methods is still considered a good practice when compared to the sole adoption of ad hoc or knowledge based methods cabaneros et al 2019 maier et al 2010 another commonly identified drawback is the lack of a one size fits all approach for implementing the methods this could be a potential stumbling block to future researchers from addressing uncertainty when building ann forecasting models finally the majority of the methods tend to demand higher computational costs when implemented for instance bootstrapping monte carlo simulation ga and ensembling involve the training of multiple ann models however the adoption of metrics that directly quantified model uncertainty was limited in this review e g 11 of the 128 identified articles table 8 summarises the metrics which are predominantly based on the confidence intervals of model outputs among the interval based metrics output uncertainty was visually inspected 5 times either as confidence interval plots or error bars on the other hand output uncertainty metrics were treated as separate model performance evaluators e g d factor 95 ppu picp mpiw ace and pinaw in 3 occasions an article was also found in which uncertainty metrics based on the posterior distribution of network weights were used to guide the merging of two base learner results 5 summary and conclusions data driven approaches especially ann models and their application to outdoor ap forecasting have received a lot of attention in the past two decades their development has allowed researchers to provide accurate ap forecasts without the theoretical understanding required by traditional physics based models however ann models are empirical and their development inevitably possesses an intrinsic level of uncertainty that can restrict the reliability of their results hence this review was performed to investigate the methods employed for addressing model uncertainty in the context of ap forecasting using ann models since the period january 2000 august 2022 research activity in the incorporation of model uncertainty when developing ann models has increased rapidly the average number of journal articles published during the said period was around 5 5 per year it is also worth noting that 65 of the identified articles have been published in the past six years alone this is a significant development given the huge adoption of ann models in many decision making tasks despite their black box nature however there still is a huge gap between the number of papers published per year that address model uncertainty and those that do not this is consistent with previous reported results by related reviews on the use of data driven forecasting models see maier et al 2010 kasiviswanathan and sudheer 2017 cabaneros et al 2019 masood and ahmad 2021 in relation to sources of model uncertainty there was a huge amount of research activity that covered input uncertainty for instance approximately 89 of the identified articles employed various methods to handle input uncertainty and almost a third of those articles also addressed both parameter and output uncertainties methods dealing with input uncertainty seem ubiquitous since ann models are essentially as good as their input data used there was also a significant number of methods used to address parameter and output uncertainties however methods handling structure uncertainty received less attention as was in the findings of cabaneros et al 2019 in particular optimal model structure was still mostly determined using ad hoc e g trial and error and or knowledge based approaches which were excluded from this review consequently there is a need to consider more analytical approaches for dealing with structure uncertainty efforts examining the interaction among four model uncertainty types were still not present in the identified papers total model uncertainty was attributed conceptually as a sum of several components related to the sources of uncertainty presented in this work arhami et al 2013 however none of the papers provided any form of metric that attempts to quantify the overall uncertainty when building ann models given the black box nature of ann models future modellers may never be able to quantify total model uncertainty in terms of the individual sources of uncertainty hence this field of research still demands further attention the majority of the identified papers have been found to adopt the ensemble approach in handling model uncertainty this comes as no surprise given the availability of exceedingly more powerful computing tools capable of handling complex model architectures such as deep learning it is also worth noting that the use of ensemble models has only emerged more recently e g a large number of the identified papers have only been published since 2019 an improvement in accuracy in the model results was also reported when ensemble models were compared to their benchmark base learners this is especially the case when dealing with the accurate predictions of peak ap levels shekarrizfard and hadad 2012 feng et al 2015 masood and ahmad 2021 however the use of ensemble models has its drawbacks as pointed out by masood and ahmad 2021 ensembling techniques demand longer computational time which makes them unsuitable for rapid forecasting ap forecasting applications another prevailing issue is the uncertainty surrounding the selection of the base learners and meta learners since the selection is still mostly carried out in an ad hoc or knowledge based manner there is a need to examine this further in the future the use of methods such as fuzzy expert systems correlation analysis genetic algorithm and sensitivity analysis also received a significant amount of attention the application of genetic algorithm for addressing not just parameter but also input and structure uncertainties is significant progress global search optimisation techniques such as genetic algorithm are analytical model based approaches that have previously received less attention because they are computationally expensive to implement cabaneros et al 2019 this review has also identified a wide array of miscellaneous procedures for handling model uncertainty ranging from correlation and mutual information techniques to meta heuristic search algorithms however the use of bootstrapping bayesian regularisation and monte carlo simulation which are especially capable of quantifying output certainty remains limited and therefore requires further investigation wu et al 2014 kasiviswanathan and sudheer 2017 cabaneros et al 2019 in general the majority of the identified methods have only attempted to handle but failed to quantify model uncertainty there are only 11 instances in which uncertainty was directly measured most of those efforts expressed output uncertainty in terms of confidence interval plots error bars or separate metrics for evaluating model performance this could raise some issues especially when both accuracy and reliability of prediction results are required as such the quantification of model uncertainty especially in the context of ap forecasting needs to receive increased attention to ensure the reliability and transparency of ann model results in particular the standardisation of reporting ann model results which include both accuracy and uncertainty metrics should be encouraged such a practice not only enables a better comparison of proposed model development methods but it further increases the confidence in using ann model results in real world applications especially ap forecasting 6 recommendations for future research based on the review of 128 papers on the use of techniques for addressing model uncertainty in the field of outdoor ap forecasting using ann models conducted in this paper the following recommendations for future work are made 1 more research on the improved reporting of both accuracy and uncertainty of ann model results needs to be undertaken this is to further validate the use of data driven models in real world tasks involving decision making where both accuracy and reliability of the results are essential although the primary aim of most studies is towards increasing the accuracy of model results future attempts should also focus on the characterisation of confidence intervals as well as the development of metrics for directly quantifying model uncertainty 2 greater attention should be given to the application of techniques for addressing structure parameter and output uncertainties the majority of identified papers in the review have placed more emphasis on input uncertainty which is noteworthy given that ann models are data driven however the ambiguity surrounding the structure parameter and output of ann models are ubiquitous which may hinder future researchers from adapting black box approaches 3 more work should continue on the use of model based methods such as bootstrapping bayesian regularisation and monte carlo simulation these methods have received less attention despite their ability to address and quantify model uncertainty 4 the relationship between model uncertainty and complexity of ensemble modelling frameworks should be examined in the future there clearly is a growing trend in the implementation of ensemble frameworks in the field of ap forecasting using ann models especially deep learning however almost all of the identified works have not attempted to measure model uncertainty furthermore ensemble approaches are computationally expensive which could hinder their application in real world tasks 5 a new area of research that deals with the interplay of single or multiple model uncertainty sources should be carried out by future modellers to the best of the authors knowledge no work has been done to examine this aspect of model uncertainty in the context of building ann models 6 more software that provides a less difficult and computationally efficient platform for handling and quantifying uncertainty in ann models should be developed to ensure that metrics assessing model uncertainty become an essential requirement in reporting new ann based methods declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
25492,the use of data driven techniques such as artificial neural network ann models for outdoor air pollution forecasting has been popular in the past two decades however research activity on the uncertainty surrounding the development of ann models has been limited therefore this review outlines the approaches for addressing model uncertainty according to the steps for building ann models based on 128 articles published from 2000 to 2022 the review reveals that input uncertainty was predominantly addressed while less focus was given to structure parameter and output uncertainties ensemble approaches have been mostly employed followed by neuro fuzzy networks however the direct measurement of uncertainty received less attention the use of bootstrapping bayesian and monte carlo simulation techniques which can quantify uncertainty was also limited in conclusion this review recommends the development and application of approaches that can both handle and quantify uncertainty surrounding the development of ann models keywords air pollution forecasting artificial neural networks uncertainty quantification bayesian monte carlo simulation fuzzy data availability no data was used for the research described in the article 1 introduction 1 1 data driven models in air pollution forecasting the use of data driven models in outdoor air pollution ap forecasting has been widely reported in the literature in the last two decades shahraiyni and sodoudi 2016 cabaneros et al 2019 masood and ahmad 2021 the attractiveness of data driven models can be explained in two respects 1 better performance over traditional approaches and 2 the emergence of big data and more powerful computing software data driven models have been shown to understand the complex dynamics between environmental variables and outdoor ap without using physics based formulae this allows researchers to bypass the assumptions and strong theoretical requirements to employ traditional approaches such as gaussian dispersion 3 d gridded eulerian photochemical and lagrangian trajectory models as such data driven models for ap forecasting have been shown to be effective alternatives to the traditional deterministic ones gardner and dorling 1998 lv et al 2016 cabaneros et al 2019 zhao et al 2019 big data and more powerful computing resources have also paved the way for research communities to apply data driven models in ap forecasting bishop 2006 montáns et al 2019 in particular the smart city concept that integrates information and communication technology ict and fixed mobile sensors have generated a tremendous amount of data especially outdoor pollution measurements which has enabled decision makers in providing early warnings to citizens in urban cities bekkar et al 2021 finally the past few years have witnessed the development and emergence of accessible and powerful computing tools that support machine learning processes giray 2021 free and open source programming languages such as python r and java have been popularly used for coding many data driven techniques tiobe 2022 popular data driven approaches utilised for ap forecasting include statistical models e g multiple linear regression mlr ng and awang 2018 and autoregressive integrated moving average arima machine learning ml e g artificial neural network ann gardner and dorling 1997 support vector machine svm lu and wang 2005 fuzzy expert system heo and kim 2004 and more recently deep learning models e g deep neural network dnn freeman et al 2018 the approaches clearly have their pros and cons which have been thoroughly discussed in the literature chen et al 2008 masood and ahmad 2021 ann models are among the most employed data driven tools for ap forecasting anns are parallel computing structures that mimic the information processing paradigm of the human brain this makes them capable of learning from any previously unknown information from any given training dataset mcculloch and pitts 1943 basheer and hajmeer 2000 ann model development generally consists of eight steps 1 data collection 2 data preprocessing 3 input variable selection 4 data division 5 model architecture selection 6 model structure optimisation 7 model training and 8 model validation a detailed description of each step can be found elsewhere maier et al 2010 cabaneros et al 2019 however performing each step entails the selection of one or more methods processes which are problem specific this lack of a clean cut approach makes the development of ann models not straightforward consequently the influence of the various approaches in performing each of the steps on the model results has become a well established research area in their review of articles dealing with the use of anns in outdoor ap forecasting cabaneros et al 2019 revealed that novel methods for implementing one or a combination of the steps for building ann models have been proposed to outperform the traditional ones the authors reported that the articles mainly focused on the development of more novel and sophisticated predictor selection techniques model architectures and model training algorithms a similar observation was reported in earlier related works by maier et al 2010 and wu et al 2014 which reviewed case studies using ann models but applied in forecasting hydrological variables finally a recent review by masood and ahmad 2021 on the use of techniques based on artificial intelligence ai in ap forecasting has also shown similar findings the authors revealed that models with deeper architectures e g dnn models have been employed and reported to provide superior predictions of ap levels 1 2 uncertainty incurred from developing data driven models given their black box nature ann models cannot provide explicit insights regarding the influence of several model building choices e g inputs architecture structure and training parameters on their results this ambiguity surrounding the modelling process exists and has been commonly referred to as model uncertainty model uncertainty has been reported to limit the potential of using ann models especially in tasks involving decision making vardoulakis et al 2002 borrego et al 2008 in particular the uncertainty surrounding ann models that were not carefully designed can limit the reproducibility and reliability of model results arhami et al 2013 elshorbagy et al 2010 noori et al 2010 current efforts have mostly focused on the improvement of the point estimates of ap levels while the incorporation of model uncertainty has received less attention maier et al 2010 cabaneros et al 2019 kasiviswanathan and sudheer 2017 has reviewed research articles dealing with hydrological modelling that employed methods addressing model uncertainty they revealed that the methodological issues for building ann models not model uncertainty have been mostly examined among those that tackled uncertainty only a few investigated the mutual interaction among sources of prediction uncertainty e g inputs training parameters and model structure given the increasing popularity of ann models a thorough review of existing research that accounts for uncertainty is significant hence the main objective of this paper is to provide an extensive framework of methods used for addressing the uncertainty surrounding the development of ann models for outdoor ap forecasting through the results of this paper the authors aim to promote good practice in reporting ann model results by accounting for both accuracy and uncertainty to the best of the authors knowledge a comprehensive survey of articles that deal with the uncertainty of ann models for ap forecasting has not yet been undertaken another novelty of this review paper is that it aims to describe the interplay between various sources of model uncertainty this is carried out by relating each uncertainty source to the eight steps of building ann models the remainder of this review paper is organised as follows in section 2 details regarding the methods for selecting the appropriate articles for this review are presented in section 3 the sources of model uncertainty and the approaches for addressing them are described section 4 presents the methods utilised for quantifying the model uncertainty section 5 provides the conclusions of this review while section 6 presents a number of recommendations for future research 2 overview articles that deal with the application of ann models in outdoor ap forecasting were collected from international peer reviewed journals databases such as sciencedirect ieee access plos one acm springer taylor francis online google scholar and mdpi were searched for relevant literature published from january 2000 to august 2022 the selection process of this review consisted of three stages since the main focus of this paper is on the incorporation of uncertainty that arises from ann model development the search items were firstly narrowed down to include the terms uncertainty and or methods that are well known to account for uncertainty the search items for the methods included air pollution forecasting artificial neural networks deep learning machine learning ann deep neural networks bayesian neural networks monte carlo ensemble confidence interval sensitivity analysis anfis and fuzzy neural networks with different combinations a second search query was carried out containing neural networks and air pollution because many of the appropriate articles for this review do not necessarily have to mention the term uncertainty the third selection stage from both identified and unidentified papers was performed in an ad hoc manner based on the subject matter of this review for instance many articles do mention the term uncertainty yet only a subset of them addressed model uncertainty in their model building process this observation can be made for other articles mentioning one or a set of search terms mentioned above another important criterion for inclusion in the review is that the said methods need to be applied in conjunction with anns for instance several articles on plain models solely applying fuzzy inference systems and linear forecasters were removed the results from a recent review article by cabaneros et al 2019 were also utilised to locate the relevant articles for this review lastly articles presented at conference proceedings were manually removed from the initial list of search results the search methodology above has identified 128 relevant articles for this review table 1 provides a list of journals alongside their respective latest impact factors where the selected articles were published the leading position in terms of the number of publications was held by both the atmospheric environment and atmospheric pollution research journals which accounted for approximately 12 of the total number of articles both the environmental modelling software and ieee access journals had the next highest proportions of articles 8 each followed by science of the total environment 7 the rest of the identified journals had fewer proportions of articles e g 5 articles or less details of the selected articles such as the name of the authors year of publication study location air pollutants examined and methods used to address model uncertainty are given in table 2 it should be noted that several articles employed two or more approaches in addressing uncertainty in their model development process the distribution of papers by years of publication is shown in fig 1 there has been a growing number of articles on ap forecasting using anns that address model uncertainty more recently in particular approximately 65 of the identified articles have been published since 2016 alone not much increased activity has been observed between 2006 and 2011 while a sudden growth of activity has occurred post 2015 however it is worth noting that these values are still comparably lesser than the overall number of published articles per year that deploy ann models for ap forecasting cabaneros et al 2019 nonetheless it is still evident that a considerable amount of attention has been aimed towards the point prediction of ap levels using ann models while handling uncertainty 3 sources of model uncertainty to characterise the uncertainty surrounding ann model development several notations need to be described first any regression type ann model generally takes the following form 1 y ˆ f x w σ n o i s e where x r n d represents the input vector e g corresponding to n samples and d predictors y ˆ r n is the vector of model outputs e g estimates of vector y r n containing the actual observations w is the vector of model parameters e g connection weights and biases f is a function describing the dynamics between x and y and σ n o i s e is the irreducible or data noise that directly influences model errors uncertainty surrounding the development of ann models has been characterised in a plethora of ways yet they can be generally categorised as either aleatoric or epistemic kiureghian and ditlevsen 2009 aleatoric uncertainties refer to those inherent to the dynamics of systems under investigation this includes the stochasticity of physical and chemical properties of environmental systems predictor excitations and noisiness or imperfections of the collected data on the other hand epistemic uncertainties occur during the modelling stage from the lack of knowledge about the underlying system being studied lack of data and development of imperfect models epistemic uncertainty is also referred to as model uncertainty as this is caused by the limitations encountered during the model development process the two terms in eq 1 correspond to the sources of model uncertainty and aleatory uncertainty respectively hence the total uncertainty of y ˆ assuming that the two uncertainties are independent can be characterised as follows 2 σ 2 σ e p 2 σ a l 2 where σ e p 2 refers to the model uncertainty and σ a l 2 refers to the aleatory uncertainty it is often a challenge to classify the uncertainties encountered during the modelling stage as real world applications like outdoor ap forecasting involve both forms of uncertainties this review will however limit its scope by only focusing on model uncertainty in particular the authors propose to characterise model uncertainty by identifying its sources during the building stage of models that is each step in the ann model development process entails a combination of several methods and it is reasonable to link uncertainty to all of them as such this review closely highlights the link between sources of model uncertainty and the said steps fig 2 shows the eight general steps for building ann models and their relationship with the sources of uncertainty which will be discussed in the following subsections detailed information on the inner workings of ann models can be found from several references hagan et al 1995 gardner and dorling 1998 maier et al 2010 bishop 2006 3 1 input uncertainty input uncertainty results from the lack of complete knowledge on the use of appropriate input x and y for training ann models given the data intensive nature of ann models input uncertainty is generally influenced by factors such as data density and number of predictors which can lead to varied model structures training parameters and irreproducible results data density refers to the number of samples n needed to train an ann model although model uncertainty is commonly regarded to be inversely proportional to data density lai et al 2022 there is no general formula for identifying the optimal amount of data needed for training without incurring unnecessary computational costs on the other hand certain types of predictors e g air pollutant meteorological and temporal variables have been known to better capture the important system dynamics that ann models attempt to simulate however this results in the inclusion of too few or too many model predictors d which greatly influences input uncertainty 3 2 structure uncertainty structure uncertainty results from the simplification ambiguity and or lack of information of the governing equation s used by ann models to describe a real world process shrestha and solomatine 2008 due to the empirical nature of ann models dealing with structure uncertainty seems inevitable structure uncertainty arises from the selection of the following i model architecture e g feedforward recurrent etc ii transfer function e g log sigmoid hyperbolic tangent linear etc and iii structure e g number of hidden layers and nodes it should be noted that the term structure has been used in the literature in two different ways as a governing formula of a model and as a specific term among ann modellers referring to the dimension of one or more hidden layers model architecture and transfer function both govern the functional relationship f in eq 1 which then determines the model structure e g the dimension of w as a result structure uncertainty directly influences the uncertainty surrounding of model parameters 3 3 parameter uncertainty parameter uncertainty refers to the lack of a general method for identifying the optimal set of network parameters w as well as the selection of non optimum algorithms for training ann models a common practice to address parameter uncertainty is the assignment of a range of training parameters from which values are then initially and randomly selected hagan et al 1995 cabaneros et al 2019 however such parameter values are impossible to replicate due to the stochastic nature of most training algorithms data division also has a great impact on the level of parameter uncertainty the selection of a subset of the input data for model training e g d x n y n n 1 n d given n d training samples also affects how the network connection weights are initialised and optimised various data division schemes e g ad hoc stratified v fold cross validation and random splitting methods bring varying levels of complexity to training parameters since the number of predictors is directly proportional to the number of connection weights that need to be calibrated input uncertainty also directly impacts the magnitude of parameter uncertainty 3 4 output uncertainty output uncertainty pertains to the lack of reliability of ann model results either due to the use of inappropriate validation techniques or the inability to replicate the same accuracy of point predictions directly linked to parameter uncertainty output uncertainty limits the ability of ann models to produce similar quality of results output uncertainty is often referred to as the total model uncertainty which is described as the sum of all uncertainties surrounding all steps in the ann model development process e g the first term in eq 2 however the majority of the identified validation techniques in the literature only deal with the measurement of the accuracy of the prediction outputs using the training and testing sets maier et al 2010 consequently the model accuracy indices presented in most case studies are difficult to replicate which often leads to the difficulty of future modellers to build upon previous results 3 5 results as shown in fig 3 input uncertainty was the most addressed type of uncertainty source e g 112 times compared with 59 52 and 14 occasions on which parameter output and structure uncertainty sources were addressed respectively input uncertainty was addressed alongside parameter and output uncertainties in 43 of the 112 instances furthermore structure and parameter uncertainties were simultaneously dealt with on 5 occasions however the incorporation of all uncertainty sources only occurred twice in this review 4 methods for addressing model uncertainty in this review paper the methods used to address model uncertainty have been classified into eight types namely 1 bootstrapping 2 bayesian 3 fuzzy method 4 monte carlo simulation 5 optimisation based 6 sensitivity analysis 7 ensemble and 8 miscellaneous approaches the classification is similar to those presented in alvisi and franchini 2011 and kasiviswanathan and sudheer 2017 however a separate type was assigned to ensemble approaches since their use has been prevalent in the field of ap forecasting during the covered time period of this review 4 1 bootstrap method the bootstrap method or bootstrapping is an intensive resampling technique with replacement that operates under the assumption that input samples or bootstraps follow the statistical characteristics of the population and mimic the underlying random component of the modelled process efron 1979 kasiviswanathan and sudheer 2017 bootstrapping is carried out by sampling various realisations of input output patterns to estimate statistical characteristics such as bias variance distribution functions and confidence intervals belayneh et al 2016 as such bootstrapping has been applied to estimate the confidence interval of the ap predictions which can be used to quantify output uncertainty see fig 4 in theory the utilisation of more bootstraps should provide a more reliable estimation of the confidence bounds on the model error indices however there is no general formula for determining the optimal number of bootstraps table 3 provides details of the studies that employed the bootstrap method including the different number of bootstrap samples used and the statistical indices computed for each of the bootstrap samples three studies utilised at least 5000 bootstrap samples which is considered a good practice chernick 1999 although the computational costs should always be considered bowden et al 2005 the reported studies also revealed the improved reliability of their model results after adopting the bootstrap method grivas and chaloulakou 2006 calculated the standard error of their calculated performance indices from the test results of their predictive models their findings revealed that the associated standard error from the bootstraps of the best performing models were also the lowest values ibarra berastegi et al 2008 evaluated the overall performance of their models by estimating the 95 confidence levels of the statistical indices obtained from the model results the authors carried out this method if the mean values of the said indices for the two models are quite similar several studies also applied a similar methodology involving the calculation of 95 confidence intervals of their model results and expressed them as error bars other authors also used any confidence intervals lying entirely above zero as indicators that a model performs significantly better than the benchmark ones voukantsis et al 2011 peng et al 2017 han et al 2021 noori et al 2010 accounted for the output uncertainty of their models by showing the plots of the range 95 confidence intervals for their ap level estimates the authors calculated the 95 prediction uncertainty 95 ppu of their developed models by finding the 2 5th and 97 5th percentiles of the cumulative distribution of every simulated ap level result metrics such as the d factor abbaspour et al 2007 e g the average distance between the upper and lower 95 ppu and r 2 helped the authors determine their best performing model 4 2 bayesian method bayesian method is an approach based on bayes theorem which states that any prior beliefs regarding an uncertain quantity are updated on the basis of new information to yield a posterior probability of the unknown quantity in detail the method begins by defining the network weights w as a probability density function pdf a prior pdf p 0 w is assigned to the network parameters which is then updated using the training data d and bayes theorem to yield the posterior pdf p w d as 3 p w d p d w p 0 w p d by means of the posterior distribution the predictive probability distribution of the model output can then be estimated as follow 4 p y ˆ d p y ˆ x w p w d d w the integration of the bayesian method with ann models or bayesian regularised neural network brnn was first employed by mackay 1992 and neal 1992 to overcome model overfitting and complexity fig 5 illustrates how brnn models vary from standard ann models which only utilise a single optimum vector w in summary fig 6 shows the general schematic of how brnn models are employed by case studies to handle parameter uncertainty the use of brnn models also provides predictions with extra information regarding the precision of the outputs in the form of error bars of the confidence intervals which are very important metrics if the reliability of model results is of particular concern bishop 1995 table 4 reveals a few identified studies that employed the bayesian regularisation method all identified studies except the one by solaiman et al 2008 reported superior model performances of brnn models when compared to a range of benchmark models from regression to deterministic models 4 3 fuzzy method fuzzy method is based on fuzzy logic fl that operates by using if then rules fl deals with high level reasoning using linguistic information acquired from domain experts as such fl provides approximate reasoning and explanation abilities which are important attributes of models employed in real life operations especially air pollution forecasting mishra and goyal 2016 an fl based system has three main phases 1 fuzzification 2 inference and 3 defuzzification see fig 7 the fuzzification process is where the numerical values of the predictors are transformed into membership functions mfs various types of mfs include the triangular gaussian and trapezoidal the number and type of mfs per predictor are usually determined empirically and can be decided by experts on the basis of experiment observation and experience mishra and goyal 2016 the gaussian mf which is based is commonly used due to the nonlinear dynamics between predictors inference then follows as the membership grades are processed through a set of if then rules to generate a fuzzy output finally defuzzification takes place as the fuzzy output is transformed into a quantitative or qualitative output yeganeh et al 2018 however the fuzzy method lacks the self learning capabilities exhibited by ann models on the other hand ann models are not capable of interpreting linguistic information nunnari et al 1998 pao 1989 the combination of fuzzy principles and anns e g the adaptive neuro fuzzy inference system anfis can therefore analyse any form of information e g numeric linguistic and logical this makes anfis models capable of addressing input uncertainty by revealing the influence of their model inputs on their outputs according to the presented if then rules table 5 provides a list of studies that employed anfis models in forecasting ap levels alongside the type of mfs they utilised however the identified papers only focused on improving the accuracy of anfis model predictions similar to the findings of a related review by kasiviswanathan and sudheer 2017 4 4 monte carlo simulation monte carlo simulation mcs is a sampling technique used for obtaining a probabilistic approximation to the solution of an optimisation model metropolis and ulam 1949 rubinstein 1981 mcs operates by sampling different realisations of model inputs and or parameters by assigning ranges and pdfs to each predictor kasiviswanathan and sudheer 2017 as illustrated in fig 8 the pdfs of each predictor are then propagated through f in order to yield the pdf of the model predictions y ˆ as such mcs has been performed to handle the input and output uncertainty of ann models the application of mcs for uncertainty analysis in ap forecasting with ann models was only reported three times in this paper firstly ding et al 2016 carried out mcs to address the parameter uncertainty when developing their ann models trained via sparse response back propagation algorithm the authors determined the mean performance of their proposed models based on ten different patterns of weights and biases secondly noori et al 2010 performed mcs to quantify the output uncertainty of their developed ann and anfis models the authors initially generated random samples according to the probability distribution of their model inputs yielding thousands of model outputs the scheme was repeatedly performed until the results of a new run do not affect the probability distribution of the output variable the authors used two metrics in their uncertainty analysis namely the d factor abbaspour et al 2007 and the 95 percent prediction uncertainties 95 ppu the authors also utilised their mcs results to provide plots of the range 95 confidence intervals for their model forecasts during the training stage finally mokhtari et al 2021 incorporated uncertainty quantification methods into the predictions of their proposed cnn lstm models the authors constructed prediction intervals pis for their predictions using mcs dropout and quantile regression methods specifically two metrics for pis e g prediction interval coverage probability picp and mean prediction interval width mpiw were calculated in the study 4 5 genetic algorithm genetic algorithm is an optimisation method based on the idea of the survival of the fittest from the mechanics of genetics it provides robust solutions for highly complex non linear search and optimisation problems holland 1975 fig 9 illustrates the flowchart of the standard processes involved when performing genetic algorithm the algorithm works by initialising a competitive set of possible solution candidates e g chromosomes and then the solutions are set through the process of natural selection the solution candidates are then evaluated through a fitness function or objective function which ranks the chromosomes in the population fitness functions are formulated depending on the problem being solved the selection of parent chromosomes is then performed which entails two parents for the crossover and the mutation crossover involves the exchange of information between two parents in the mutation stage the genes of the chromosomes of the crossed offspring are changed the entire process is carried out until a certain condition is met michalewicz 1996 genetic algorithm has been integrated with ann models for ap forecasting to address model uncertainty for instance the use of the algorithm to optimise model weights and biases was observed 5 times kadiyala et al 2013 de mattos neto et al 2017 zhai and chen 2018 feng et al 2011 ibarra berastegi et al 2008 the said papers reported superior performance of ann models when trained using the method however the use of genetic algorithm does not always guarantee superior model performance although despite being able to reduce model complexity grivas and chaloulakou 2006 the method was also carried out to address input uncertainty via predictor selection on 7 occasions grivas and chaloulakou 2006 hájek and olej 2012 elangasinghe et al 2014 siwek and osowski 2016 dotse et al 2018 liu et al 2019a photphanloet and lipikorn 2020 on the other hand elangasinghe et al 2014 tackled parameter uncertainty by employing the method to optimise the step size momentum rate and processing elements of their proposed ann models finally the use of genetic algorithm to handle both input and parameter uncertainties was observed three times ibarra berastegi et al 2008 employed the method during data division to optimise the combination of training and validation sets in terms of mean standard deviation maximum and minimum values the authors also used the method to identify the most relevant predictors of their proposed models de mattos neto et al 2014 employed the method to optimise several variables of their mlp model such as the number of predictors in terms of time lags number of hidden nodes and training parameters a similar method for addressing both input and parameter uncertainties was also carried out by de mattos neto et al 2015 in their development of a hybrid mlp model 4 6 sensitivity analysis sensitivity analysis is a general method used for assessing the relative importance of variables selected as inputs for an ann model the method is usually performed to reduce network complexity by eliminating unnecessary predictors while keeping the significant ones by examining the variations of the model output by the minor perturbations of the predictors sensitivity analysis can account for input uncertainty in the following the application of sensitivity analysis for predictor selection are presented niska et al 2005 performed a sensitivity analysis alongside moga to identify an optimal set of predictors for their mlp models the authors defined the sensitivity of their predictor subset as the absolute difference between the model performance achieved by using a predictor subset and the model performance achieved when using all predictors solaiman et al 2008 selected the predictors for their models based on linear autocorrelation and partial autocorrelation analysis and nonlinear sensitivity analysis they calculated a metric called the relative sensitivity of a predictor which is the ratio between the standard deviation of the model outputs and the standard deviation of the predictor the authors initially performed partial autocorrelation analysis between past predictor and predictand values to identify significant time lags secondly sensitivity analysis was then applied for the final stage of screening predictors according to the identified significant lags zito et al 2008 carried out a sensitivity analysis by studying the response of their models to small and equal changes of the predictors they found out that if an increase in predictor value causes a significant change to the model output the examined predictor should be retained in the model kadiyala et al 2013 carried out predictor selection by performing analysis of variance anova alongside the regression tree method shaban et al 2016 investigated the influence of incorporating multiple types of predictors e g temporal meteorological and gaseous on the performance of their models they used metrics such as prediction trend accuracy pta and rmse to assess the results of the models trained using multiple combinations of predictors stamenković et al 2017 performed sensitivity analyses of their model predictors through correlation analysis in conjunction with calculating the variance inflation factor vif which is based on the linear relationship between predictors the authors sequentially removed predictors with highest the vif values which indicate multicollinearity between predictors shams et al 2021 performed a sensitivity analysis by setting the value of one predictor within the range of the standard deviation while the rest were fixed at their mean values then the standard deviation of the model outputs for each predictor changes was measured as model sensitivity for that predictor the authors then selected the variables with high values in the output standard deviation on the other hand sensitivity analysis is usually performed in conjunction with other methods 4 7 ensemble approach ensembling is a modelling approach that integrates the prediction results of multiple models into one final output one main advantage of the approach is the ability of the ensemble model or meta learner to possess the individual strengths and simultaneously overcome the limitations of the single models base learners chen et al 2008 a common key limitation exhibited by single models is the inability to accurately predict peak ap levels kolehmainen et al 2001 niska et al 2005 grivas and chaloulakou 2006 however the independence of the base learners and their comparable performance have been pointed out as two important conditions for any ensemble model to perform well haykin 1999 kuncheva 2004 ensembling creates multiple input output realisations of the examined ap system which could be used to account for the input parameter and output uncertainties when developing the models in particular the resulting ensemble model will contain some diversity and the variance of its predictions can be interpreted as an estimation of model uncertainty lai et al 2022 this review classifies ensemble ann models into two types namely the model and data intensive ensemble models see fig 10 under the model intensive type the input data are being fed to train multiple base learners and the results are used as inputs to the meta learner on the other hand the data intensive type initially extracts important features of the input data to train the base learners and then integrates the multiple outputs based on the feature extractor technique used it is worth noting that the base learners need not consist of entirely ann or ml based models in general as shown in the discussion below many ensemble models are comprised of linear statistical and non linear ml base learners table 6 shows the identified works that adapted the ensemble modelling approach the review has identified a variety of meta learners employed to develop ensemble models the majority of the said meta learners are ann based such as mlp svr elm and lstm models other non ann meta learners have been identified including rf fusion and weighted averaging siwek and osowski 2016 and linear regression and normal copula based models de mattos neto et al 2021 other forms of base learners to create ensemble models were also identified in this review catalano et al 2016 employed an ensembling approach by choosing the maximum of the predictions of their base learners since they had the tendency to underestimate pollution peaks gong and ordieres meré 2016 employed a stacking ensemble algorithm that linearly combined the results of their base learners the authors used the cross validation data and least squares under non negativity constraints to determine the coefficients of the linear combination di et al 2019 utilised a generalised additive model that accounted for geographical differences to predict daily pm2 5 levels at a resolution of 1 km 1 km across a certain geographical area through ensembling in particular the authors developed an ensemble model which incorporated the level of ap concentration against thin plate splines of latitude and longitude as such the results of their base learners were geographically weighted instead of the traditional approach of using constant weights for each base learner similarly valput et al 2019 adopted an ensemble approach to provide local predictions using regional numerical ap predictions their proposed model utilised the forecasts of seven neighbouring air quality models and combined them by calculating their average and weighted average values finally sharma et al 2020 adopted a method that involved entirely non ann base learners e g rf volterra m5 tree and mlr models and an lstm meta learner a few special cases of model intensive ensemble methods have also been found in this review for instance mahajan et al 2018 proposed a hybrid ensemble approach that utilises both linear and nonlinear base learners in particular the authors initially used an arima model to capture the linear tendencies of their pm2 5 time series the model residuals e g difference between actual time series and model results were then fed to a lagged input ann model the results of both base learners were then given equal weights before they were combined gu et al 2019 employed several svr models as base learners and then applied pruning techniques to eliminate the undesirable learners the results of the selected base learners were then fed to an svr based regression model for stacking finally han et al 2020 employed an approach of integrating the results of two bayesian rnn base learners according to their uncertainty measures in particular the authors employed two weighting methods to fuse the results of the base learners 1 uncertainty averaged outputs and 2 selection of a base learner output with the lowest uncertainty measure under the data intensive category the most commonly used feature extractors were wavelet based techniques see table 6 for instance many authors employed wavelet transformation to decompose their original data into several coefficients which were then fed to their base learners or feature learners wavelet decomposition techniques were especially applied to improve the performance of plain ann models when dealing with peak ap concentration levels shekarrizfard and hadad 2012 feng et al 2015 other similar feature extractors were utilised such as empirical mode decomposition emd bai et al 2019 and its variants such as the noise assisted emd dong et al 2021 and clustering algorithm macia g et al 2019 in general the identified papers suggested the superiority of their proposed ensemble models to the involved base learners however the trade off between the overall complexity of ensemble models and performance should be carefully accounted for especially in real world scenarios where computational cost is a major constraint cabaneros et al 2019 a few special cases of data intensive ensemble methods have also been identified for instance de mattos neto et al 2017 employed an approach that involved forecasting both the ap time series and model residuals the authors only conducted the latter if the residual is not white noise e g a series of independent and identically distributed random values with zero mean and constant variance a nonlinear base learner e g mlp model was utilised for the initial forecasting while both linear and nonlinear ones e g arima mlp and svr models for the residual the approach falls under the ensemble type since both forecasting results were all combined using an mlp model an earlier work by de mattos neto et al 2015 applied the same method which included the forecasting of the model residuals however the work was model intensive in that the residuals of the final ensemble model output were further fed to a series of mlp models until the scheme generates residuals having a white noise behaviour finally de mattos neto et al 2020 addressed the seasonality of pm2 5 and pm10 pollutants by decomposing the time series into non overlapping monthly partitions and then applied several models e g as meta learners their decomposition method involved creating n subseries from the original time series according to the coefficient of variation of each subseries 4 8 miscellaneous approaches other methods that address model uncertainty have also been identified in this review and are briefly described below several techniques have been adapted to identify the most significant model predictors hence tackling input uncertainty address method yeganeh et al 2018 boruta algorithm alkabbani et al 2022 correlation analysis perez and salini 2008 he et al 2014 mishra et al 2015 bai et al 2016 stamenković et al 2017 alimissis et al 2018 tao et al 2019 dong et al 2021 menares et al 2021 zhang et al 2021 kristiani et al 2022 tian et al 2022 fourier analysis hrust et al 2009 grey correlation analysis zhu et al 2018 individual smoothing factors antanasijević et al 2013 lasso yeganeh et al 2017 mdsf algorithm photphanloet and lipikorn 2020 and stepwise regression ordieres et al 2005 agirre basurko et al 2006 singh et al 2012 russo et al 2013 russo and soares 2014 siwek and osowski 2016 many approaches have also been employed to address structure uncertainty by determining the optimal number of nodes in the hidden layer and consequently reducing overall model complexity dutot et al 2007 employed a stepwise method using bic like information criterion to tackle structure uncertainty by determining the optimal number of nodes in the hidden layer chelani et al 2002 used a convergence criteria according to the error minimisation criterion and a formula by kinnebrock 1995 catalano et al 2016 applied the weight generalisation formula by bishop 1995 which utilises an extra error term that penalises small weights during model training finally agirre basurko et al 2006 employed a generalisation rule by amari et al 1997 in which the model is considered optimally trained when the ratio of the number of training samples to the number of connection weights exceeds 30 hasham et al 2004 adopted an approach based on factorial design concepts to assess the influence of model predictors structure and parameters on the model output box et al 1978 the approach operates by moving various input factors between high and low settings in combination with other input factors hence investigating the interaction of uncertainties between the said factors empirical formulae which provide either or both the upper and lower bounds of the optimal number of nodes in the hidden layer were also used formula proposed by fletcher and goss 1993 he et al 2014 abdullah et al 2019 empirical formula by shen et al 2008 with trial and error bai et al 2016 empirical rule by kalogirou 2003 radojević et al 2018 a formula by kotu and deshpande 2018 and roiger 2017 photphanloet and lipikorn 2020 and a method by tian et al 2022 dutot et al 2007 used a metric called leverage which provides a confidence interval of the predicted values of their models to address output uncertainty leverage is a metric used to assess the effect of a particular observation on the fitted regression according to the position of the observation in the predictor space monari and dreyfus 2002 powerful swarm intelligence algorithms have also been used to optimise the training parameters of ann models for instance mo et al 2019 utilised the whale optimisation meta heuristic algorithm to obtain the best parameters of their proposed ensemble elm models caraka et al 2019 employed both pso and backpropagation algorithms to train their ann models shahid et al 2021 employed a boosting algorithm to improve the results of their initial model e g svr model to address input uncertainty the boosting technique works by assigning weights to each instance of the input data and using them to train the svr model then identifying and updating the weights of the misclassified instances the weighted instances are finally passed to several models including mlp rf decision tree mlr ridge regression gradient boosting and svr models for the final training and prediction tasks wang et al 2022a applied a probabilistic approach by calculating the distribution of their model prediction errors and comparing them to create confidence intervals of results the approach was based on the gaussian and t location scale distributions which were determined according to the r 2 pe and rmse indices the authors also calculated three metrics such as average coverage error ace prediction interval normalised average width pinaw and prediction interval coverage probability picp to measure the interval predictions zheng et al 2022 decomposed their original pm2 5 time series into several subseries using wavelet transforms and employed the reinforcement learning algorithm e g q learning for the predictor selection of each subseries 4 9 results the distribution of papers by the methods used to handle model uncertainty is shown in fig 11 the majority of the identified papers e g 45 occasions adapted the ensemble modelling approach to address uncertainty among these approximately 69 have been recently published since 2019 indicating the emergence of more sophisticated approaches as ann computing technologies also become more powerful fuzzy systems were applied alongside ann models 20 times while correlation analysis for predictor selection was applied 18 times the utilisation of the global search procedure e g genetic algorithm occurred 15 times followed by sensitivity analysis of trained models 11 times the number of papers in which bootstrapping bayesian and empirical methods were applied was uniform varying between 6 to 7 compared with only three instances where the mcs was applied finally the use of alternative methods for addressing uncertainty was reported 28 times table 7 presents the pros and cons of the identified methods that can address model uncertainty note that the methods falling under miscellaneous approaches were excluded from the summary the use of the said methods has been reported to improve the forecasting accuracy of ann models which is usually the primary motivation of most studies this is especially true for those methods handling input and parameter uncertainties to reduce model complexity however it can be seen that not all methods can directly quantify model uncertainty for instance fuzzy systems ga and sensitivity analysis can only account for uncertainty by addressing the ambiguity arising from predictor selection and parameter optimisation nonetheless the use of such methods is still considered a good practice when compared to the sole adoption of ad hoc or knowledge based methods cabaneros et al 2019 maier et al 2010 another commonly identified drawback is the lack of a one size fits all approach for implementing the methods this could be a potential stumbling block to future researchers from addressing uncertainty when building ann forecasting models finally the majority of the methods tend to demand higher computational costs when implemented for instance bootstrapping monte carlo simulation ga and ensembling involve the training of multiple ann models however the adoption of metrics that directly quantified model uncertainty was limited in this review e g 11 of the 128 identified articles table 8 summarises the metrics which are predominantly based on the confidence intervals of model outputs among the interval based metrics output uncertainty was visually inspected 5 times either as confidence interval plots or error bars on the other hand output uncertainty metrics were treated as separate model performance evaluators e g d factor 95 ppu picp mpiw ace and pinaw in 3 occasions an article was also found in which uncertainty metrics based on the posterior distribution of network weights were used to guide the merging of two base learner results 5 summary and conclusions data driven approaches especially ann models and their application to outdoor ap forecasting have received a lot of attention in the past two decades their development has allowed researchers to provide accurate ap forecasts without the theoretical understanding required by traditional physics based models however ann models are empirical and their development inevitably possesses an intrinsic level of uncertainty that can restrict the reliability of their results hence this review was performed to investigate the methods employed for addressing model uncertainty in the context of ap forecasting using ann models since the period january 2000 august 2022 research activity in the incorporation of model uncertainty when developing ann models has increased rapidly the average number of journal articles published during the said period was around 5 5 per year it is also worth noting that 65 of the identified articles have been published in the past six years alone this is a significant development given the huge adoption of ann models in many decision making tasks despite their black box nature however there still is a huge gap between the number of papers published per year that address model uncertainty and those that do not this is consistent with previous reported results by related reviews on the use of data driven forecasting models see maier et al 2010 kasiviswanathan and sudheer 2017 cabaneros et al 2019 masood and ahmad 2021 in relation to sources of model uncertainty there was a huge amount of research activity that covered input uncertainty for instance approximately 89 of the identified articles employed various methods to handle input uncertainty and almost a third of those articles also addressed both parameter and output uncertainties methods dealing with input uncertainty seem ubiquitous since ann models are essentially as good as their input data used there was also a significant number of methods used to address parameter and output uncertainties however methods handling structure uncertainty received less attention as was in the findings of cabaneros et al 2019 in particular optimal model structure was still mostly determined using ad hoc e g trial and error and or knowledge based approaches which were excluded from this review consequently there is a need to consider more analytical approaches for dealing with structure uncertainty efforts examining the interaction among four model uncertainty types were still not present in the identified papers total model uncertainty was attributed conceptually as a sum of several components related to the sources of uncertainty presented in this work arhami et al 2013 however none of the papers provided any form of metric that attempts to quantify the overall uncertainty when building ann models given the black box nature of ann models future modellers may never be able to quantify total model uncertainty in terms of the individual sources of uncertainty hence this field of research still demands further attention the majority of the identified papers have been found to adopt the ensemble approach in handling model uncertainty this comes as no surprise given the availability of exceedingly more powerful computing tools capable of handling complex model architectures such as deep learning it is also worth noting that the use of ensemble models has only emerged more recently e g a large number of the identified papers have only been published since 2019 an improvement in accuracy in the model results was also reported when ensemble models were compared to their benchmark base learners this is especially the case when dealing with the accurate predictions of peak ap levels shekarrizfard and hadad 2012 feng et al 2015 masood and ahmad 2021 however the use of ensemble models has its drawbacks as pointed out by masood and ahmad 2021 ensembling techniques demand longer computational time which makes them unsuitable for rapid forecasting ap forecasting applications another prevailing issue is the uncertainty surrounding the selection of the base learners and meta learners since the selection is still mostly carried out in an ad hoc or knowledge based manner there is a need to examine this further in the future the use of methods such as fuzzy expert systems correlation analysis genetic algorithm and sensitivity analysis also received a significant amount of attention the application of genetic algorithm for addressing not just parameter but also input and structure uncertainties is significant progress global search optimisation techniques such as genetic algorithm are analytical model based approaches that have previously received less attention because they are computationally expensive to implement cabaneros et al 2019 this review has also identified a wide array of miscellaneous procedures for handling model uncertainty ranging from correlation and mutual information techniques to meta heuristic search algorithms however the use of bootstrapping bayesian regularisation and monte carlo simulation which are especially capable of quantifying output certainty remains limited and therefore requires further investigation wu et al 2014 kasiviswanathan and sudheer 2017 cabaneros et al 2019 in general the majority of the identified methods have only attempted to handle but failed to quantify model uncertainty there are only 11 instances in which uncertainty was directly measured most of those efforts expressed output uncertainty in terms of confidence interval plots error bars or separate metrics for evaluating model performance this could raise some issues especially when both accuracy and reliability of prediction results are required as such the quantification of model uncertainty especially in the context of ap forecasting needs to receive increased attention to ensure the reliability and transparency of ann model results in particular the standardisation of reporting ann model results which include both accuracy and uncertainty metrics should be encouraged such a practice not only enables a better comparison of proposed model development methods but it further increases the confidence in using ann model results in real world applications especially ap forecasting 6 recommendations for future research based on the review of 128 papers on the use of techniques for addressing model uncertainty in the field of outdoor ap forecasting using ann models conducted in this paper the following recommendations for future work are made 1 more research on the improved reporting of both accuracy and uncertainty of ann model results needs to be undertaken this is to further validate the use of data driven models in real world tasks involving decision making where both accuracy and reliability of the results are essential although the primary aim of most studies is towards increasing the accuracy of model results future attempts should also focus on the characterisation of confidence intervals as well as the development of metrics for directly quantifying model uncertainty 2 greater attention should be given to the application of techniques for addressing structure parameter and output uncertainties the majority of identified papers in the review have placed more emphasis on input uncertainty which is noteworthy given that ann models are data driven however the ambiguity surrounding the structure parameter and output of ann models are ubiquitous which may hinder future researchers from adapting black box approaches 3 more work should continue on the use of model based methods such as bootstrapping bayesian regularisation and monte carlo simulation these methods have received less attention despite their ability to address and quantify model uncertainty 4 the relationship between model uncertainty and complexity of ensemble modelling frameworks should be examined in the future there clearly is a growing trend in the implementation of ensemble frameworks in the field of ap forecasting using ann models especially deep learning however almost all of the identified works have not attempted to measure model uncertainty furthermore ensemble approaches are computationally expensive which could hinder their application in real world tasks 5 a new area of research that deals with the interplay of single or multiple model uncertainty sources should be carried out by future modellers to the best of the authors knowledge no work has been done to examine this aspect of model uncertainty in the context of building ann models 6 more software that provides a less difficult and computationally efficient platform for handling and quantifying uncertainty in ann models should be developed to ensure that metrics assessing model uncertainty become an essential requirement in reporting new ann based methods declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
25493,the brazilian amazon is suffering from serious deforestation that may lead to catastrophic consequences for the world s climate and biodiversity preventing illegal deforestation activities is an emergent and crucial mission for the world recently automated monitoring systems using uncrewed aerial vehicles uavs appear as a promising solution to combat deforestation however the quality design of such systems considering the deforestation prevention impacts remains under explored in this paper we propose a socio ict information communication technology model that enables to analyze the deforestation prevention impacts of a uav based monitoring system quantitatively the socio ict model uses a causal loop diagram to formulate the relations among system quality measures and key performance indicators kpis associated with deforestation prevention through the numerical study we estimate the effectiveness of the uav monitoring system by annual deforestation rate and co2 gas emissions our analysis shows important design trade offs need to be considered when planning uav based monitoring systems keywords causal loop diagram deforestation monitoring socio ict model system dynamics uav data availability the parameter data used in our analysis is summarized in the table in the paper 1 introduction brazil has the largest tropical forest in the world with an area of approximately 5 million square kilometers this forest is home to the richest biodiversity of any ecosystem on the planet nevertheless the rate of deforestation in brazilian tropical forests is among the highest globally according to brazilian s national institute for space research inpe 4688 square kilometers of tropical forest were removed from august 2019 to january 2020 an increase of 113 over the same period last year butler 2020 deforestation is a global problem with extensive environmental and economic consequences since it reduces biodiversity impacts climate change by co2 emission and breaks up indigenous communities kanninen et al 2007 the most common reasons for deforestation are illegal mining logging and the creation of more space for agriculture and livestock especially beef and soybean remote monitoring using satellite images has been commonly used to measure deforestation in brazil more specifically the monitoring of the amazon forest is carried out through satellites and has four main systems prodes deter degrad and detex rajão et al 2017 these systems were developed to support the inspection and control of deforestation and forest degradation although they were conceived to monitor different stages of deforestation these systems are complementary to the objective of mapping and monitoring deforestation in the amazon basin as stated in guerreiro diniz et al 2015 the data obtained by these systems are among the main sources of information for decision making regarding policies to combat and control deforestation in the amazon however the costs to maintain long term monitoring systems are enormous and they depend on satellite images that are influenced by cloud cover mohan et al 2021 ortega adarme et al 2020 as the brazilian amazon is vast and illegal deforestation has intensified in recent years the use of tools capable of responding quickly to these illegal activities is urgent recently uav based surveillance systems are emerging as a promising solution for monitoring and preventing deforestation paneque gálvez et al 2014 as stated in wright et al 2018 data from these systems is a viable alternative for planning operations to prevent or combat illegal deforestation because they can cover areas that are remote or difficult to reach it can also identify where mining new agriculture fields or other deforestation have replaced native forests and serve as evidence for authorities to take action against violators unlike traditional satellite imagery uav surveillance systems can obtain very high resolution images from cloudiness areas and the cost of these environments is relatively low a uav can automatically fly the target area and monitor changes in forest cover detecting deforestation activities like logging or mining once deforestation activities are identified warning signals can be issued to authorities responsible for enforcing environmental laws in the brazilian amazon such uav based monitoring is more efficient than the traditional remote monitoring approach if uav functions are properly implemented and work with the specified quality level however the impacts of a uav based monitoring system for deforestation prevention associated with system qualities are still little explored to analyze the deforestation prevention impacts of a uav based monitoring system quantitatively we propose a socio ict information communication technology model to capture the factors contributing to deforestation prevention including system quality measures the socio ict model is based on a causal loop diagram that formulates the relations among system quality measures operational constraints environmental variables and key performance indicators kpis associated with deforestation prevention the proposed model allows us to estimate the impact of a uav based monitoring system on the kpis of the solution such as annual deforestation rate and co2 gas emissions based on the model we conducted a numerical analysis with a case study of the amazon rainforest protection the results of the numerical analysis show that 4347 km2 of rainforest can be mitigated from deforestation by the introduction of uav based monitoring systems with a hundred uavs which corresponds to a reduction of 102 4 megatons in co2 gas emissions moreover our simulation analysis clarifies design trade offs related to the quality measures of uav based surveillance systems for instance the results reveal that additional drone flights are not effective in terms of deforestation prevention when it increases up to a certain number 15 this work can help communities governments or deforestation data end users to find low cost drone based surveillance solutions that can be used in developing regions with little access to capital e g indigenous communities the rest of the paper is organized as follows section 2 describes the related work section 3 details the adopted methodology section 4 presents the socio ict model section 5 shows our results and key observations finally section 6 concludes the paper and briefly introduces our future works 2 related work monitoring deforestation is a complex process the investigations around this theme are wide and span many different areas mohan et al 2021 tucker and townshend 2000 however it is usually based on a surveillance system which uses satellite or uav technologies to monitor and control deforestation to position our paper and indicate its contributions we first summarize related work that has been done to analyze deforestation quantitatively via satellite imagery after that we discuss studies using uavs lastly we provide a comparison of our work in relation to these works in terms of modeling and evaluation in the last few years many approaches have been developed to quantitatively analyze changes in forest cover via satellite imagery in edemir shimabukuro et al 2012 the authors introduced the prodes and deter systems which were developed by inpe and are used for annual deforestation mapping and near real time deforestation monitoring respectively using satellite images prodes has been used to monitor and quantify primary forest removal in the brazilian legal amazon bla since 1988 it employs remote sensing images and digital image processing techniques to estimate the annual rate of deforestation by clear cuts of primary forests in bla however the time required for prodes to produce such information prevented the rapid identification of areas in the initial or intermediate stages of deforestation as a consequence deter system was created to provide a near real time monitoring procedure to support brazilian authorities it is worth mentioning that deter system detects deforestation activities in its initial stage without providing an area estimate on the other hand prodes estimates the total annual deforested area in xu and zeng 2017 xu and orien investigated the impacts of using neural networks to analyze satellite images of deforestation in the amazon basin brazil peru uruguay colombia venezuela guyana bolivia and ecuador specifically they used data from kaggle which provided more than 40 000 images of this area each image was labeled with a subset of seventeen different labels those seventeen labels were organized into three groups atmospheric conditions common land cover land use phenomena and rare land cover land use phenomena they adopted multiple approaches to classifying the images from simple convolution neural nets to more complex models that utilized properties of the data to evaluate each approach they used the f2 score the results revealed that the vggnet performed the best in analyzing the satellite images vggnet consists of 16 convolution layers and it is widely used because of its uniform architecture mayfield et al 2017 evaluated the usefulness of various free satellite datasets for predicting deforestation by comparing machine learning methods such as artificial neural networks bayesian networks and gaussian processes for the analysis the authors chose two areas one in mexico and the other in madagascar the usefulness of individual datasets was evaluated based on the analysis of deforestation predictors that were most influential for the adopted methods these predictors could range from proximity to the road to population pressure the results revealed that freely available datasets could indeed be used to predict the probability of deforestation in fact even basic models generalized linear models and generalized linear mixed models could be used for that however the factors affecting deforestation differ from area to area and consequently the success of models trained on freely available datasets will depend on whether these datasets contain data relevant to the area under analysis the main drawbacks of these approaches for detection of deforestation based on satellite data are i they depend on satellite images which are influenced by cloud cover ii high resolution satellite imagery can be costly especially for systems in developing countries and iii freely available datasets do not always provide enough information for detecting deforestation in specific areas recently uav based approaches have been proposed to overcome the limitations of studies based on satellite data in zhang et al 2016 the authors explored the utility of drones for long term ecological monitoring the study was conducted in a plot in southern china using an md4 1000 drone equipped with sony nex 5 camera the results showed that the mapping and monitoring of forest dynamics could be effectively done by drones with high resolution data almeida et al 2019 explored the potential of a drone borne lidar system to monitor forest restoration projects which are otherwise difficult to measure with traditional forest inventories the experiment was conducted in brazil s atlantic forest and evaluated the results of a forest restoration experiment with the planting of mixed species the adopted system demonstrated a significant potential for monitoring large scale forest restoration in paneque gálvez et al 2014 paneque gálvez et al assessed the prospects challenges and opportunities of using drones for community based forest monitoring cbfm cbfm is usually carried out through conventional ground surveys to gather forest data such as forest cover loss number of trees tree species etc however these surveys cover a very small area and are often expensive time consuming and tedious thus the authors proposed an effective drone based approach to cbfm they discussed not only the main advantages of using drones for cbfm but also the disadvantages in terms of technical social and environmental issues the conclusion is that drones can enhance cbfm but for many locations some degree of external assistance and funding is needed ex redd program although these uav based approaches deal with forest monitoring they do not focus on deforestation still there is a wide range of different uav systems with respect to size and payload control systems flight range altitude and endurance the classification of these systems can be found in everaerts et al 2008 hardin and hardin 2010 watts et al 2012 however there is no one size fits all solution applied to deforestation and the effectiveness and practicability of such solutions are still under explored in yang et al 2021 the authors proposed an optimized layout of a uav system considering cost security and capacity however the system focused on improving the efficiency of forest wildfire monitoring and prevention in goraj et al 2008 goraj et al presented the design and optimization of a uav system taking into account reliability safety and cost nevertheless the system was built for long endurance border surveillance and monitoring compared with a long term monitoring system based on satellite imagery a uav based monitoring system considered for our analysis is much more cost effective and easier for deployment and operation however the impact of deforestation prevention is not simply determined by the uav s capabilities but also affected by system qualities such as service availability and detection accuracy as well as other environmental factors this leads to the challenge in the analysis of deforestation prevention impact by a uav based monitoring system this paper first addresses this issue and proposes a socio ict model to quantitatively estimate the effects of such a uav based monitoring system the proposed methodology is detailed in the next section 3 methodology in order to evaluate the effectiveness of uav based monitoring systems which are still not largely employed for deforestation prevention purposes we have to rely on model based estimation methods however most existing studies on environmental model based simulation focus on predicting future deforestation without considering ict system based solutions meanwhile existing studies from the ict system side especially about system designs and applications of uav based monitoring do not evaluate the effectiveness in terms of deforestation prevention as the performance availability and accuracy of uav based monitoring systems can significantly affect the effectiveness of preventive actions simulation models need to incorporate such factors to this end we need a vertically integrated model that can capture the relations among the qualities of ict systems e g performance availability and accuracy the operational condition information the impacts on the environment and society and the kpis concerned by owners or users of the system e g annual deforestation rate and co2 gas emission such a vertical model can give us a reasonable estimation of kpi values achieved by the introduction of the uav based monitoring solution on the other hand the model also allows us to derive the system s quality requirements to achieve the goal of the solution under given constraints like budget and or environment 3 1 socio ict modeling we introduce a socio ict model that can combine systems quality evaluation models with higher level social impact models the system quality evaluation model can be stochastic models representing system configurations and operations under uncertainty factors such as workload variation and component failures several modeling methodologies like markov models queueing networks stochastic petri nets trivedi 2008 and bayesian networks hamada et al 2008 can be used for this purpose for higher level social impact models we employ causal loop diagrams and system dynamics john sterman 2002 that can capture higher level associations related to system qualities by assigning a variable to each system quality measure we may incorporate the causal relations from systems quality into the social impact analysis 3 2 modeling scope while a socio ict model can incorporate diverse aspects of a target solution due to the high flexibility of modeling it is impractical and infeasible to model everything in the real world comprehensively in one representation therefore in this study we consider the kpis concerned in a real deployment and attempt to model major causal relations that significantly impact the kpis there are potentially several risk factors that can inhibit the expected outcomes of systems for example considering a uav based deforestation monitoring scenario the weather condition may harm the system performance and quality resulting in a temporarily lower benefit for the kpis those risk factors can also be incorporated in the socio ict model but we exclude them from our modeling scope in this study and focuses on major causal relations in particular we look into the causal relations of system quality measures affecting kpis in order to have reasonable estimations of system qualities we need comprehensive system quality evaluation models with empirical data for parameter values some existing studies have presented such comprehensive models for estimating the performance and availability of uav based monitoring systems machida and andrade 2021 kharchenko et al 2016 in this paper we do not look into the details of system quality models and rather use the outputs of these models in the causal loop to quantify the impacts for a uav based deforestation monitoring scenario we assume that the key quality metrics such as service availability the length of automated flight and the accuracy of image recognition are estimated in advance by the quality evaluation models machida and andrade 2021 kharchenko et al 2016 and some empirical data from the prototype system araújo et al 2021 4 socio ict model for deforestation monitoring in this section we develop a causal loop diagram and associated component models for a socio ict model to estimate the impacts of a uav based deforestation monitoring system the monitoring system aims to detect any illegal deforestation activities and protect the rainforest by preventing illegal activities with the help of the monitoring system we can expect a reduction in the annual deforestation rate adr and the associated co2 gas emission co2e that are regarded as the kpis of the solution the causal loop diagram is used to capture the relations among system quality measures operational factors environmental variables and kpis associated with deforestation prevention the considered system quality measures are the service availability the detection accuracy and the autonomy that are given by a system quality design in this study we do not consider risk factors such as weather conditions cyber physical attacks on uavs and the capacity of a deforestation monitoring agency such risk factors can be incorporated in the extension of the model in future study the variables used in the proposed model are summarized in table 1 4 1 causal loop diagram first we explain the overview of the developed social impact model in a cld with system dynamics as shown in fig 1 the model consists of the top layer system dynamics representing the quantity of deforestation and the bottom layer representing the deforestation prevention by the deforestation monitoring system the boxes in the system dynamics are the stocks representing the area of preserved forest and deforested area due to the illegal deforestation activities the preserved forest area changes to a deforested area the valve on the flow between the two boxes represents the rate to change the forest area co2 gas emission is associated with the deforested area which is explained in the following section in the bottom layer of the cld each arc among the property nodes represents a causal relation between the two properties the arc signed with indicates a positive relation while the arc signed with indicates a negative relation for instance the arc between deforestation rate and deforestation prevention is negative meaning that the deforestation rate slows down when the amount of deforestation prevention increases the cld clarifies the factors to prevent deforestation events in association with the monitoring service as the cld shows deforestation prevention can be enhanced by a larger monitoring area a better deforestation detection capability and a better probability of preventive action after the alert in a uav based monitoring system the deforestation detection capability depends on the visiting frequency for the target area the detection accuracy and the service availability of the detection function note that the detection accuracy and the service availability are the systems quality metrics that can be estimated from systems models and experiments e g ortega adarme et al 2020 machida and andrade 2021 kharchenko et al 2016 another critical system quality metric is autonomy which is the time length of continuous autonomous operation of the system autonomy is constrained by the battery life but it can contribute to enlarging the monitoring area an important trade off found in the cld is the relation between the monitoring area and the visiting frequency another concerned trade off is the relation between the detection accuracy and the battery life we explain the details of these trade offs in the following subsections 4 2 co2 gas emission as deforestation is regarded as one of the major factors of climate change estimating the co2 gas emission due to deforestation is essential we consider the potential increase of co2 gas emission in three categories i burning ii decaying and iii lost absorption co2 gas emission due to burning occurs in the short term by natural or commercial purpose fires burning also causes the emission of other greenhouse gas such as methane ch4 and nitrous oxide n2o the biomass cut down and burnt then starts emitting co2 by decaying the decay occurs in the long term which is usually within 10 years bellassenrenaud et al 2008 besides these direct impacts the lost forest can cause the reduction of co2 absorption by the forest area it is estimated that 1 ha of the amazon rainforest can sequester ten tons of co2 craig 2019 when a hectare of the rainforest is deforested ten tons co2 gas is left in the air without being absorbed the following formula represents the expected amount of co2 gas emission as a function of deforestation area a d f in consideration of these factors c o 2 e a d f e b u r n e d e c a y e a b s o r b where e b u r n e d e c a y and e a b s o r b represents the amount of co2 gas emission caused by burning decay and lost absorption for a hectare of deforestation respectively 4 3 annual deforestation rate brazil s satellite agency inpe estimates the forest loss annually using the satellite images taken during the dry season in the amazon according to the latest report 10 851 square kilometers of the forest was lost in the brazilian amazon in 2020 portal terrabrasilis instituto nacional de pesquisas espaciais inpe 2020 one of the deforestation monitoring goals is to decelerate the annual deforestation rate by detecting and preventing illegal activities note that deforestation may not be prevented even if illegal activities are detected since the landsat satellite takes the images in 5 16 days revisit rate de almeida et al 2021 illegal deforestation events are not detected in real time moreover the authorities cannot take any actions to prevent the detected illegal activity simply due to the limitation of their capacity although 842 983 cases of deforestation were confirmed in 2020 only 3 were checked by the authorities relatório anual do desmatamento no 2021 therefore the impact of deforestation monitoring needs to be estimated with the net rate of deforestation prevention subtracted from the expected deforestation rate without using the deforestation monitoring solution let a a d f be the expected annual deforestation rate when deforestation monitoring is not adopted and denote a d p v as the net deforestation prevention area per year the deforested area is estimated by a d f a a d f a d p v d t where t is the time scale in a year a a d f corresponds to the flow rate from the preserved forest to the deforested area meanwhile a d p v corresponds to the variable assigned for the valve on the flow 4 4 deforestation prevention as the cld shows deforestation prevention is the key factor to deaccelerate deforestation according to our causality analysis the deforestation prevention area can be characterized by three contributing factors i monitoring area ii deforestation detection capability and iii preventive action probability deforestation prevention is possible only where a uav monitors the land periodically and hence the monitoring area is the upper limit of the deforestation prevention area deforestation can be prevented when illegal activity is detected at the right time and the counteracting measure is applied successfully therefore these factors are essential to computing the net deforestation prevention area per year as given by a d p v a m o n p d d e p d p v where a m o n is the monitoring area p d d e is the deforestation detection probability and p d p v is the deforestation prevention success probability the computation of p d d e is detailed in the following subsection while we assume p d p v is the parameter whose value needs to be assigned in the evaluation time 4 5 exposure time and scan probability illegal deforestation activities can be detected by monitoring uavs when the activities are exposed to the uav flying over the site to represent the probability that the uav can visit and scan the place under deforestation event we define it as the scan probability p s c a we assume that a monitoring uav runs an image recognition process to detect suspicious activities from real time images taken by the equipped camera hence the service availability and detection accuracy of the image recognition function directly impact the deforestation detection probability denoting p s a v and p a c c as the service availability and the detection accuracy respectively the deforestation detection probability is formulated as p d d e p s c a p s a v p a c c in the socio ict model we consider p s a v and p a c c are associated with the estimated values from system models with its design parameter values in order to compute the scan probability we need to analyze the overlap period between the deforestation exposure time and the monitoring period for the site if the exposure time is very long as the uav s revisit interval is smaller than the exposure time the scan probability becomes one otherwise the scan probability is given by p s c a t exp t m o n t o u t t m o n where t exp t m o n and t o u t represent the exposure time the monitoring period for the target place and the period out of monitoring i e the time to next monitoring period respectively fig 2 shows an example of the relation between the exposure time and the monitoring period if the exposure time falls in the out of monitoring period the deforestation activities cannot be scanned on the other hand if the exposure time overlaps with the monitoring period the deforestation activities are scanned properly in consideration of the case with a very long exposure time the scan probability can be generalized to p s c a min t exp t o u t t m o n t o u t t m o n in fact the exposure time varies depending on the types and scales of deforestation events we assume the exposure time follows an exponential distribution with the mean t exp 1 γ and compute the expected scan probability as follows p s c a e min t exp t o u t t m o n t o u t t m o n 0 t o u t t γ e γ t d t t o u t t o u t γ e γ t t m o n t o u t t m o n 1 γ 1 e γ t o u t t o u t e γ t o u t t o u t e γ t o u t t m o n t o u t t m o n 1 γ 1 e γ t o u t t m o n t o u t t m o n t exp 1 e t o u t t exp t m o n t o u t t m o n note that t o u t is the time to the next monitoring period the reciprocal of t o u t represents the visiting frequency of the monitoring uav as the visiting frequency increases the out of monitoring period is reduced resulting in a higher scan probability 4 6 monitoring area and visiting frequency the visiting frequency has a trade off relation with the monitoring area the larger the target area of monitoring is the less frequently a uav visits the same location under the total flight hours the total flight hour is limited by the autonomy the possible automated flight duration per flight and the number of flights thus the total flight hours required to cover the target monitoring area is given by t t f t n f l t t f l t where n f l t is the number of uav flights required to cover the monitoring area t f l t is the autonomy when we consider the waiting time for the next uav monitoring routine denoted by t w t t we can have the following equation t o u t t t f t t m o n t w t t on the other hand when we define a m p f as the monitoring area per one flight hour the total monitoring area can be expressed as a m o n t t f t a m p f as the formulation represents a larger monitoring area can be achieved by increasing the total flight hours and it also causes an increased out of monitoring period t o u t resulting in a decreased visiting frequency we assume that a m p f is a constant determined by the specification of drones employed for monitoring purposes since a uav captures the land image using the camera a view of the flying uav depends on the flight altitude and the camera s field of view fov as fig 3 shows the land image taken by the camera on the drone is in a circle with a radius of h tan θ 2 by denoting θ as fov and h as the flight altitude the covered area by a shot can be approximated as the inscribed square of the circle with 2 h tan θ 2 on each side when we denote d as the flight speed of the drone the monitoring area per one flight hour can be given by a m p f 2 h d tan θ 2 following the specifications of commercially available drones e g food and agriculture organization of the united nations 2018 we assume θ π 2 h 0 5 k m and d 40 k m h and estimate the a m p f 2828 h a 4 7 autonomy and detection accuracy both autonomy and detection accuracy need to be estimated by the system quality evaluation model to increase the detection accuracy one can use higher resolution images and computationally expensive recognition algorithms that may cause increased battery consumption since the remaining battery life directly affects the autonomy there is a performance trade off between the autonomy and the detection accuracy for the simulation purpose in this paper we approximate this trade off relation with a linear function as below p a c c 0 1 t f l t 1 as fig 4 shows the function associates the autonomy in the range of 0 5 4 hours with the accuracy in the range of 0 6 0 95 in a negative correlation although the function can be non linear shape in reality we temporarily use this function for the purpose of trade off impact analysis in section 5 4 8 computation procedure based on the cld with the associated formulations we can compute the estimated values of kpis from the given system quality metrics the computation steps are applied from the bottom of the cld to the top as detailed below 1 from the system quality evaluation models the expected system quality metrics are computed the system quality metrics include the autonomy t f l t service availability p s a v and detection accuracy p a c c 2 applying the autonomy to 8 the total flight hours t t f t is computed t t f t is subsequently used to derive the monitoring area a m o n and the out of monitoring period t o u t by 9 10 11 3 the out of monitoring period is used to compute the scan probability p s c a by 7 4 by 4 multiplying the scan probability with service availability and detection accuracy yields the deforestation detection probability p d d e 5 by 3 the net deforestation prevention area per year is computed from the monitoring area deforestation detection probability and deforestation prevention success probability 6 the estimated annual deforestation area and the corresponding co2 gas emission are computed using 1 2 the system quality metrics can be considered as the parameters that are governed by a system design while the number of uav flights n f l t and the waiting time for the next uav monitoring t w t t are the variables related to the operational design as 7 indicates the effectiveness of the solution also depends on the exposure time of deforestation events consequently we examine the sensitivities to these parameters in the next section 5 analysis we evaluate the effectiveness of uav based deforestation monitoring through the numerical analysis based on the developed socio ict model since the model contains various system parameters we conduct sensitivity analysis by varying the parameter values in the practical ranges while fixing other parameters we use matlab to implement the models and conduct the sensitivity analysis 5 1 base assumptions for the analysis purpose we choose the default parameter values as summarize in table 2 for all the experiments we compare the results with different service availability values i e 0 85 0 9 0 95 we set the default value of autonomy to 2 h in reference to the specification of the uav used for deforestation monitoring in panama food and agriculture organization of the united nations 2018 the detection accuracy is set to 0 9 as it is sufficiently achievable accuracy using recent machine learning techniques pozzobon de bem et al 2020 araújo et al 2021 since a higher accuracy can be achieved at the expense of energy and autonomy we analyze the trade off between accuracy and autonomy in section 5 6 the number of uav flights is the parameter value to be decided during the solution design process we assume a drone flies to monitor 10 different locations in a sequence the waiting time for the next drone monitoring is set to 4 h considering the mid night hours in which drone monitoring is not effective in sections 5 2 and 5 3 we investigate the impacts of these operational parameters as discussed in section 4 3 we assume that the exposure time of deforestation events is exponentially distributed as it depends on the type of illegal activities and their phases de almeida et al 2021 we set the default average exposure time to 10 h for co2 gas emission caused by a hectare of deforestation we refer to the estimated numbers in the literature bellassenrenaud et al 2008 craig 2019 the expected annual deforestation rate without any prevention is estimated from the value in 2020 silva junior et al 2021 we assume the uav based deforestation monitoring system is introduced to the protected areas in the brazilian amazon which is about 44 of brazilian amazon rainforest veríssimo et al 2011 and expect that the same amount of deforestation can occur if we do not take any action the monitoring time per location is set to 3 min by assuming that the drone can fly 2 km in 3 min by 40 km h the monitoring area per one flight hour follows the system assumption described in section 4 6 5 2 the number of uav flights we evaluate the deforestation prevention area a d p v by varying the number of uav flights with different service availability settings the deforestation prevention area is a key indicator for evaluating the effectiveness of the deforestation monitoring solution as we can compute the annual deforestation rate and the expected co2 gas emission from this area fig 5 shows the sensitivity analysis results we can observe that the deforestation prevention area is increased with a higher service availability and a larger number of uav flights however the contribution of additional flights is not so significant when the number of uavs increases up to a certain number 15 as the number of uav flights increases the monitoring area enlarges at the same time however the visiting frequency at each location decreases because the monitored area by each uav flight does not overlap due to the decreased visiting frequency the deforestation prevention effects tend to be saturated at a certain level below 2300 in the case of 0 95 service availability for example the results imply that the number of uav flights needs to be determined in consideration with the trade off between the monitoring area and visiting frequency which influences the deforestation prevention area note that the deforestation prevention area estimated here is due to a sequence of flights from a single uav using multiple uavs to monitor different locations can enlarge the total deforestation prevention area accordingly in section 5 5 we examine the impacts of the number of monitoring uavs on the kpis 5 3 the maintenance time t w t t represents the waiting time for the next uav monitoring during which the uvas can be under maintenance the maintenance time is another operational parameter to be determined we evaluate the deforestation prevention area a d p v by varying t w t t with different service availability settings fig 6 shows the results as can be seen the deforestation prevention area decreases as the maintenance time increases the results look reasonable since the deforestation activities cannot be detected during the out of monitoring period during the maintenance period some parts of uavs may be repaired and software programs can be updated it is essential to make such maintenance periods as short as possible to increase the capability of deforestation prevention 5 4 the average exposure time while the above two operational parameters can be controlled in a solution design the exposure time of deforestation activities is not a controllable factor as it depends on the types of illegal deforestation activities we evaluate the impact of the average exposure time t exp on the deforestation prevention area as shown in fig 7 as the average exposure time increases the deforestation prevention area increases the result is reasonable because the scan probability and associated detection probability are increased by a larger exposure time looking at the shape of the curve the amount of increase gradually becomes insignificant as the average exposure time becomes longer this results from the minimum function in 6 that makes the scan probability to one when the exposure time is larger than the out of monitoring period t o u t the results imply the possibility that the deforestation prevention effects can be limited even using uav based monitoring if illegal deforestation activities occur in a very short time however we assume this is not the case as most deforestation activities are time consuming processes de almeida et al 2021 5 5 annual deforestation rate and co2 gas emission the effectiveness of the solution is eventually evaluated by the kpis concerned by owners and the users of the system in our system dynamics we consider two kpis the reduced annual deforestation rate adr and the associated co2 gas emission co2e the expected values of kpis with a uav based monitoring system can be computed from the deforestation prevention area per year by 1 and 2 table 3 shows the expected adr and co2e in different parameter combinations here we consider additional parameter m representing the number of locations introducing the uav based deforestation monitoring the deforestation prevention area is multiplied m times to derive the total area of deforestation prevention in the protected area when any drone based monitoring solutions are not introduced i e m 0 the expected adr is 4878 72 km2 and co2e is 114 9 megatons the value of adr and co2e decreases with increasing the number of locations m service availability p s a v the number of flights n f l t and the average exposure time t exp although the impact is not significant by monitoring with a single drone we can reduce adr and co2e considerably with a hundred uavs for example when p s a v 0 95 n f l t 15 t exp 30 m 100 the adr is decreased to 531 732 square kilometers meaning that about 4347 square kilometers are protected from deforestation by a hundred uavs in this case co2e is reduced to 12 528 megatons which corresponds to 102 4 megatons of co2 gas reduction in a year 5 6 autonomy and detection accuracy as discussed in section 4 7 there is a trade off between autonomy and detection accuracy when we assume the linear relation 12 and vary t f l t in 0 5 4 the deforestation prevention area changes as shown in fig 8 the deforestation prevention area decreases as the autonomy becomes shorter on the other hand as the autonomy becomes longer sufficiently the deforestation prevention area gradually decreases caused by the decreased accuracy although the assumed linear trade off relation does not completely reflect the reality the above result implies that there is an optimum trade off point at which the deforestation prevention area is maximized note that both autonomy and detection accuracy are quality measures constrained by the battery life it is important that systems engineers determine the system configuration by interacting with solution designers or end users to achieve the optimal trade off in terms of their kpis our socio ict model and analysis can help such a system design process 6 conclusion deforestation prevention is an emergent issue to combat global climate change uav based monitoring considered in our analysis can detect causal illegal activities automatically in real time with good accuracy compared to traditional satellite image remote monitoring this paper first focuses on the quality design of such a uav based monitoring solution in this paper we proposed the socio ict model to analyze the deforestation prevention impacts of a uav based monitoring system and conducted a numerical analysis to show the effectiveness of the model the socio ict model employs a cld to capture the relations among system quality measures operational constraints environmental variables and kpis associated with deforestation prevention we quantitatively estimated the deforestation rate and co2 gas emission through numerical analysis the results of the sensitivity analysis provide the following insights the number of drone flights has positive impacts on deforestation prevention yet the gain of additional flights becomes insignificant as the number of flights increases it is caused by the trade offs between the monitoring area and the visiting frequency of a site the waiting time for the next flights has negative impacts on deforestation prevention hence minimizing the waiting and maintenance period is an issue to be addressed in system operation design the expected deforestation prevention area is significantly affected by the exposure time of illegal activities when the out of monitoring period is smaller enough there is more chance of detecting illegal activities although the impact of a single uav monitoring is not significant we can expect a considerable reduction in annual deforestation rate and co2 gas emissions by introducing multiple uavs to cover different locations there could be an optimal trade off point that maximizes the deforestation prevention area because of the trade off between the uav s autonomy and detection accuracy to explore the best trade off a higher level model like our socio ict model plays an important role in future work we may extend the cld by considering several risk factors such as weather conditions cyber physical attacks on uavs and the capacity of a deforestation monitoring agency as mentioned in section 3 2 the risk factors may reduce the benefit of a uav based monitoring system a quantitative analysis of such negative impacts helps design and prioritize preventive measures note that we can also consider potential contributing factors as well for example monitoring systems might be a deterrent to illegal activities declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported in part by the grant of university of tsukuba basic research support program type s this research was partially supported by japan society for the promotion of science jsps invitational fellowships for research in japan appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105540 
25493,the brazilian amazon is suffering from serious deforestation that may lead to catastrophic consequences for the world s climate and biodiversity preventing illegal deforestation activities is an emergent and crucial mission for the world recently automated monitoring systems using uncrewed aerial vehicles uavs appear as a promising solution to combat deforestation however the quality design of such systems considering the deforestation prevention impacts remains under explored in this paper we propose a socio ict information communication technology model that enables to analyze the deforestation prevention impacts of a uav based monitoring system quantitatively the socio ict model uses a causal loop diagram to formulate the relations among system quality measures and key performance indicators kpis associated with deforestation prevention through the numerical study we estimate the effectiveness of the uav monitoring system by annual deforestation rate and co2 gas emissions our analysis shows important design trade offs need to be considered when planning uav based monitoring systems keywords causal loop diagram deforestation monitoring socio ict model system dynamics uav data availability the parameter data used in our analysis is summarized in the table in the paper 1 introduction brazil has the largest tropical forest in the world with an area of approximately 5 million square kilometers this forest is home to the richest biodiversity of any ecosystem on the planet nevertheless the rate of deforestation in brazilian tropical forests is among the highest globally according to brazilian s national institute for space research inpe 4688 square kilometers of tropical forest were removed from august 2019 to january 2020 an increase of 113 over the same period last year butler 2020 deforestation is a global problem with extensive environmental and economic consequences since it reduces biodiversity impacts climate change by co2 emission and breaks up indigenous communities kanninen et al 2007 the most common reasons for deforestation are illegal mining logging and the creation of more space for agriculture and livestock especially beef and soybean remote monitoring using satellite images has been commonly used to measure deforestation in brazil more specifically the monitoring of the amazon forest is carried out through satellites and has four main systems prodes deter degrad and detex rajão et al 2017 these systems were developed to support the inspection and control of deforestation and forest degradation although they were conceived to monitor different stages of deforestation these systems are complementary to the objective of mapping and monitoring deforestation in the amazon basin as stated in guerreiro diniz et al 2015 the data obtained by these systems are among the main sources of information for decision making regarding policies to combat and control deforestation in the amazon however the costs to maintain long term monitoring systems are enormous and they depend on satellite images that are influenced by cloud cover mohan et al 2021 ortega adarme et al 2020 as the brazilian amazon is vast and illegal deforestation has intensified in recent years the use of tools capable of responding quickly to these illegal activities is urgent recently uav based surveillance systems are emerging as a promising solution for monitoring and preventing deforestation paneque gálvez et al 2014 as stated in wright et al 2018 data from these systems is a viable alternative for planning operations to prevent or combat illegal deforestation because they can cover areas that are remote or difficult to reach it can also identify where mining new agriculture fields or other deforestation have replaced native forests and serve as evidence for authorities to take action against violators unlike traditional satellite imagery uav surveillance systems can obtain very high resolution images from cloudiness areas and the cost of these environments is relatively low a uav can automatically fly the target area and monitor changes in forest cover detecting deforestation activities like logging or mining once deforestation activities are identified warning signals can be issued to authorities responsible for enforcing environmental laws in the brazilian amazon such uav based monitoring is more efficient than the traditional remote monitoring approach if uav functions are properly implemented and work with the specified quality level however the impacts of a uav based monitoring system for deforestation prevention associated with system qualities are still little explored to analyze the deforestation prevention impacts of a uav based monitoring system quantitatively we propose a socio ict information communication technology model to capture the factors contributing to deforestation prevention including system quality measures the socio ict model is based on a causal loop diagram that formulates the relations among system quality measures operational constraints environmental variables and key performance indicators kpis associated with deforestation prevention the proposed model allows us to estimate the impact of a uav based monitoring system on the kpis of the solution such as annual deforestation rate and co2 gas emissions based on the model we conducted a numerical analysis with a case study of the amazon rainforest protection the results of the numerical analysis show that 4347 km2 of rainforest can be mitigated from deforestation by the introduction of uav based monitoring systems with a hundred uavs which corresponds to a reduction of 102 4 megatons in co2 gas emissions moreover our simulation analysis clarifies design trade offs related to the quality measures of uav based surveillance systems for instance the results reveal that additional drone flights are not effective in terms of deforestation prevention when it increases up to a certain number 15 this work can help communities governments or deforestation data end users to find low cost drone based surveillance solutions that can be used in developing regions with little access to capital e g indigenous communities the rest of the paper is organized as follows section 2 describes the related work section 3 details the adopted methodology section 4 presents the socio ict model section 5 shows our results and key observations finally section 6 concludes the paper and briefly introduces our future works 2 related work monitoring deforestation is a complex process the investigations around this theme are wide and span many different areas mohan et al 2021 tucker and townshend 2000 however it is usually based on a surveillance system which uses satellite or uav technologies to monitor and control deforestation to position our paper and indicate its contributions we first summarize related work that has been done to analyze deforestation quantitatively via satellite imagery after that we discuss studies using uavs lastly we provide a comparison of our work in relation to these works in terms of modeling and evaluation in the last few years many approaches have been developed to quantitatively analyze changes in forest cover via satellite imagery in edemir shimabukuro et al 2012 the authors introduced the prodes and deter systems which were developed by inpe and are used for annual deforestation mapping and near real time deforestation monitoring respectively using satellite images prodes has been used to monitor and quantify primary forest removal in the brazilian legal amazon bla since 1988 it employs remote sensing images and digital image processing techniques to estimate the annual rate of deforestation by clear cuts of primary forests in bla however the time required for prodes to produce such information prevented the rapid identification of areas in the initial or intermediate stages of deforestation as a consequence deter system was created to provide a near real time monitoring procedure to support brazilian authorities it is worth mentioning that deter system detects deforestation activities in its initial stage without providing an area estimate on the other hand prodes estimates the total annual deforested area in xu and zeng 2017 xu and orien investigated the impacts of using neural networks to analyze satellite images of deforestation in the amazon basin brazil peru uruguay colombia venezuela guyana bolivia and ecuador specifically they used data from kaggle which provided more than 40 000 images of this area each image was labeled with a subset of seventeen different labels those seventeen labels were organized into three groups atmospheric conditions common land cover land use phenomena and rare land cover land use phenomena they adopted multiple approaches to classifying the images from simple convolution neural nets to more complex models that utilized properties of the data to evaluate each approach they used the f2 score the results revealed that the vggnet performed the best in analyzing the satellite images vggnet consists of 16 convolution layers and it is widely used because of its uniform architecture mayfield et al 2017 evaluated the usefulness of various free satellite datasets for predicting deforestation by comparing machine learning methods such as artificial neural networks bayesian networks and gaussian processes for the analysis the authors chose two areas one in mexico and the other in madagascar the usefulness of individual datasets was evaluated based on the analysis of deforestation predictors that were most influential for the adopted methods these predictors could range from proximity to the road to population pressure the results revealed that freely available datasets could indeed be used to predict the probability of deforestation in fact even basic models generalized linear models and generalized linear mixed models could be used for that however the factors affecting deforestation differ from area to area and consequently the success of models trained on freely available datasets will depend on whether these datasets contain data relevant to the area under analysis the main drawbacks of these approaches for detection of deforestation based on satellite data are i they depend on satellite images which are influenced by cloud cover ii high resolution satellite imagery can be costly especially for systems in developing countries and iii freely available datasets do not always provide enough information for detecting deforestation in specific areas recently uav based approaches have been proposed to overcome the limitations of studies based on satellite data in zhang et al 2016 the authors explored the utility of drones for long term ecological monitoring the study was conducted in a plot in southern china using an md4 1000 drone equipped with sony nex 5 camera the results showed that the mapping and monitoring of forest dynamics could be effectively done by drones with high resolution data almeida et al 2019 explored the potential of a drone borne lidar system to monitor forest restoration projects which are otherwise difficult to measure with traditional forest inventories the experiment was conducted in brazil s atlantic forest and evaluated the results of a forest restoration experiment with the planting of mixed species the adopted system demonstrated a significant potential for monitoring large scale forest restoration in paneque gálvez et al 2014 paneque gálvez et al assessed the prospects challenges and opportunities of using drones for community based forest monitoring cbfm cbfm is usually carried out through conventional ground surveys to gather forest data such as forest cover loss number of trees tree species etc however these surveys cover a very small area and are often expensive time consuming and tedious thus the authors proposed an effective drone based approach to cbfm they discussed not only the main advantages of using drones for cbfm but also the disadvantages in terms of technical social and environmental issues the conclusion is that drones can enhance cbfm but for many locations some degree of external assistance and funding is needed ex redd program although these uav based approaches deal with forest monitoring they do not focus on deforestation still there is a wide range of different uav systems with respect to size and payload control systems flight range altitude and endurance the classification of these systems can be found in everaerts et al 2008 hardin and hardin 2010 watts et al 2012 however there is no one size fits all solution applied to deforestation and the effectiveness and practicability of such solutions are still under explored in yang et al 2021 the authors proposed an optimized layout of a uav system considering cost security and capacity however the system focused on improving the efficiency of forest wildfire monitoring and prevention in goraj et al 2008 goraj et al presented the design and optimization of a uav system taking into account reliability safety and cost nevertheless the system was built for long endurance border surveillance and monitoring compared with a long term monitoring system based on satellite imagery a uav based monitoring system considered for our analysis is much more cost effective and easier for deployment and operation however the impact of deforestation prevention is not simply determined by the uav s capabilities but also affected by system qualities such as service availability and detection accuracy as well as other environmental factors this leads to the challenge in the analysis of deforestation prevention impact by a uav based monitoring system this paper first addresses this issue and proposes a socio ict model to quantitatively estimate the effects of such a uav based monitoring system the proposed methodology is detailed in the next section 3 methodology in order to evaluate the effectiveness of uav based monitoring systems which are still not largely employed for deforestation prevention purposes we have to rely on model based estimation methods however most existing studies on environmental model based simulation focus on predicting future deforestation without considering ict system based solutions meanwhile existing studies from the ict system side especially about system designs and applications of uav based monitoring do not evaluate the effectiveness in terms of deforestation prevention as the performance availability and accuracy of uav based monitoring systems can significantly affect the effectiveness of preventive actions simulation models need to incorporate such factors to this end we need a vertically integrated model that can capture the relations among the qualities of ict systems e g performance availability and accuracy the operational condition information the impacts on the environment and society and the kpis concerned by owners or users of the system e g annual deforestation rate and co2 gas emission such a vertical model can give us a reasonable estimation of kpi values achieved by the introduction of the uav based monitoring solution on the other hand the model also allows us to derive the system s quality requirements to achieve the goal of the solution under given constraints like budget and or environment 3 1 socio ict modeling we introduce a socio ict model that can combine systems quality evaluation models with higher level social impact models the system quality evaluation model can be stochastic models representing system configurations and operations under uncertainty factors such as workload variation and component failures several modeling methodologies like markov models queueing networks stochastic petri nets trivedi 2008 and bayesian networks hamada et al 2008 can be used for this purpose for higher level social impact models we employ causal loop diagrams and system dynamics john sterman 2002 that can capture higher level associations related to system qualities by assigning a variable to each system quality measure we may incorporate the causal relations from systems quality into the social impact analysis 3 2 modeling scope while a socio ict model can incorporate diverse aspects of a target solution due to the high flexibility of modeling it is impractical and infeasible to model everything in the real world comprehensively in one representation therefore in this study we consider the kpis concerned in a real deployment and attempt to model major causal relations that significantly impact the kpis there are potentially several risk factors that can inhibit the expected outcomes of systems for example considering a uav based deforestation monitoring scenario the weather condition may harm the system performance and quality resulting in a temporarily lower benefit for the kpis those risk factors can also be incorporated in the socio ict model but we exclude them from our modeling scope in this study and focuses on major causal relations in particular we look into the causal relations of system quality measures affecting kpis in order to have reasonable estimations of system qualities we need comprehensive system quality evaluation models with empirical data for parameter values some existing studies have presented such comprehensive models for estimating the performance and availability of uav based monitoring systems machida and andrade 2021 kharchenko et al 2016 in this paper we do not look into the details of system quality models and rather use the outputs of these models in the causal loop to quantify the impacts for a uav based deforestation monitoring scenario we assume that the key quality metrics such as service availability the length of automated flight and the accuracy of image recognition are estimated in advance by the quality evaluation models machida and andrade 2021 kharchenko et al 2016 and some empirical data from the prototype system araújo et al 2021 4 socio ict model for deforestation monitoring in this section we develop a causal loop diagram and associated component models for a socio ict model to estimate the impacts of a uav based deforestation monitoring system the monitoring system aims to detect any illegal deforestation activities and protect the rainforest by preventing illegal activities with the help of the monitoring system we can expect a reduction in the annual deforestation rate adr and the associated co2 gas emission co2e that are regarded as the kpis of the solution the causal loop diagram is used to capture the relations among system quality measures operational factors environmental variables and kpis associated with deforestation prevention the considered system quality measures are the service availability the detection accuracy and the autonomy that are given by a system quality design in this study we do not consider risk factors such as weather conditions cyber physical attacks on uavs and the capacity of a deforestation monitoring agency such risk factors can be incorporated in the extension of the model in future study the variables used in the proposed model are summarized in table 1 4 1 causal loop diagram first we explain the overview of the developed social impact model in a cld with system dynamics as shown in fig 1 the model consists of the top layer system dynamics representing the quantity of deforestation and the bottom layer representing the deforestation prevention by the deforestation monitoring system the boxes in the system dynamics are the stocks representing the area of preserved forest and deforested area due to the illegal deforestation activities the preserved forest area changes to a deforested area the valve on the flow between the two boxes represents the rate to change the forest area co2 gas emission is associated with the deforested area which is explained in the following section in the bottom layer of the cld each arc among the property nodes represents a causal relation between the two properties the arc signed with indicates a positive relation while the arc signed with indicates a negative relation for instance the arc between deforestation rate and deforestation prevention is negative meaning that the deforestation rate slows down when the amount of deforestation prevention increases the cld clarifies the factors to prevent deforestation events in association with the monitoring service as the cld shows deforestation prevention can be enhanced by a larger monitoring area a better deforestation detection capability and a better probability of preventive action after the alert in a uav based monitoring system the deforestation detection capability depends on the visiting frequency for the target area the detection accuracy and the service availability of the detection function note that the detection accuracy and the service availability are the systems quality metrics that can be estimated from systems models and experiments e g ortega adarme et al 2020 machida and andrade 2021 kharchenko et al 2016 another critical system quality metric is autonomy which is the time length of continuous autonomous operation of the system autonomy is constrained by the battery life but it can contribute to enlarging the monitoring area an important trade off found in the cld is the relation between the monitoring area and the visiting frequency another concerned trade off is the relation between the detection accuracy and the battery life we explain the details of these trade offs in the following subsections 4 2 co2 gas emission as deforestation is regarded as one of the major factors of climate change estimating the co2 gas emission due to deforestation is essential we consider the potential increase of co2 gas emission in three categories i burning ii decaying and iii lost absorption co2 gas emission due to burning occurs in the short term by natural or commercial purpose fires burning also causes the emission of other greenhouse gas such as methane ch4 and nitrous oxide n2o the biomass cut down and burnt then starts emitting co2 by decaying the decay occurs in the long term which is usually within 10 years bellassenrenaud et al 2008 besides these direct impacts the lost forest can cause the reduction of co2 absorption by the forest area it is estimated that 1 ha of the amazon rainforest can sequester ten tons of co2 craig 2019 when a hectare of the rainforest is deforested ten tons co2 gas is left in the air without being absorbed the following formula represents the expected amount of co2 gas emission as a function of deforestation area a d f in consideration of these factors c o 2 e a d f e b u r n e d e c a y e a b s o r b where e b u r n e d e c a y and e a b s o r b represents the amount of co2 gas emission caused by burning decay and lost absorption for a hectare of deforestation respectively 4 3 annual deforestation rate brazil s satellite agency inpe estimates the forest loss annually using the satellite images taken during the dry season in the amazon according to the latest report 10 851 square kilometers of the forest was lost in the brazilian amazon in 2020 portal terrabrasilis instituto nacional de pesquisas espaciais inpe 2020 one of the deforestation monitoring goals is to decelerate the annual deforestation rate by detecting and preventing illegal activities note that deforestation may not be prevented even if illegal activities are detected since the landsat satellite takes the images in 5 16 days revisit rate de almeida et al 2021 illegal deforestation events are not detected in real time moreover the authorities cannot take any actions to prevent the detected illegal activity simply due to the limitation of their capacity although 842 983 cases of deforestation were confirmed in 2020 only 3 were checked by the authorities relatório anual do desmatamento no 2021 therefore the impact of deforestation monitoring needs to be estimated with the net rate of deforestation prevention subtracted from the expected deforestation rate without using the deforestation monitoring solution let a a d f be the expected annual deforestation rate when deforestation monitoring is not adopted and denote a d p v as the net deforestation prevention area per year the deforested area is estimated by a d f a a d f a d p v d t where t is the time scale in a year a a d f corresponds to the flow rate from the preserved forest to the deforested area meanwhile a d p v corresponds to the variable assigned for the valve on the flow 4 4 deforestation prevention as the cld shows deforestation prevention is the key factor to deaccelerate deforestation according to our causality analysis the deforestation prevention area can be characterized by three contributing factors i monitoring area ii deforestation detection capability and iii preventive action probability deforestation prevention is possible only where a uav monitors the land periodically and hence the monitoring area is the upper limit of the deforestation prevention area deforestation can be prevented when illegal activity is detected at the right time and the counteracting measure is applied successfully therefore these factors are essential to computing the net deforestation prevention area per year as given by a d p v a m o n p d d e p d p v where a m o n is the monitoring area p d d e is the deforestation detection probability and p d p v is the deforestation prevention success probability the computation of p d d e is detailed in the following subsection while we assume p d p v is the parameter whose value needs to be assigned in the evaluation time 4 5 exposure time and scan probability illegal deforestation activities can be detected by monitoring uavs when the activities are exposed to the uav flying over the site to represent the probability that the uav can visit and scan the place under deforestation event we define it as the scan probability p s c a we assume that a monitoring uav runs an image recognition process to detect suspicious activities from real time images taken by the equipped camera hence the service availability and detection accuracy of the image recognition function directly impact the deforestation detection probability denoting p s a v and p a c c as the service availability and the detection accuracy respectively the deforestation detection probability is formulated as p d d e p s c a p s a v p a c c in the socio ict model we consider p s a v and p a c c are associated with the estimated values from system models with its design parameter values in order to compute the scan probability we need to analyze the overlap period between the deforestation exposure time and the monitoring period for the site if the exposure time is very long as the uav s revisit interval is smaller than the exposure time the scan probability becomes one otherwise the scan probability is given by p s c a t exp t m o n t o u t t m o n where t exp t m o n and t o u t represent the exposure time the monitoring period for the target place and the period out of monitoring i e the time to next monitoring period respectively fig 2 shows an example of the relation between the exposure time and the monitoring period if the exposure time falls in the out of monitoring period the deforestation activities cannot be scanned on the other hand if the exposure time overlaps with the monitoring period the deforestation activities are scanned properly in consideration of the case with a very long exposure time the scan probability can be generalized to p s c a min t exp t o u t t m o n t o u t t m o n in fact the exposure time varies depending on the types and scales of deforestation events we assume the exposure time follows an exponential distribution with the mean t exp 1 γ and compute the expected scan probability as follows p s c a e min t exp t o u t t m o n t o u t t m o n 0 t o u t t γ e γ t d t t o u t t o u t γ e γ t t m o n t o u t t m o n 1 γ 1 e γ t o u t t o u t e γ t o u t t o u t e γ t o u t t m o n t o u t t m o n 1 γ 1 e γ t o u t t m o n t o u t t m o n t exp 1 e t o u t t exp t m o n t o u t t m o n note that t o u t is the time to the next monitoring period the reciprocal of t o u t represents the visiting frequency of the monitoring uav as the visiting frequency increases the out of monitoring period is reduced resulting in a higher scan probability 4 6 monitoring area and visiting frequency the visiting frequency has a trade off relation with the monitoring area the larger the target area of monitoring is the less frequently a uav visits the same location under the total flight hours the total flight hour is limited by the autonomy the possible automated flight duration per flight and the number of flights thus the total flight hours required to cover the target monitoring area is given by t t f t n f l t t f l t where n f l t is the number of uav flights required to cover the monitoring area t f l t is the autonomy when we consider the waiting time for the next uav monitoring routine denoted by t w t t we can have the following equation t o u t t t f t t m o n t w t t on the other hand when we define a m p f as the monitoring area per one flight hour the total monitoring area can be expressed as a m o n t t f t a m p f as the formulation represents a larger monitoring area can be achieved by increasing the total flight hours and it also causes an increased out of monitoring period t o u t resulting in a decreased visiting frequency we assume that a m p f is a constant determined by the specification of drones employed for monitoring purposes since a uav captures the land image using the camera a view of the flying uav depends on the flight altitude and the camera s field of view fov as fig 3 shows the land image taken by the camera on the drone is in a circle with a radius of h tan θ 2 by denoting θ as fov and h as the flight altitude the covered area by a shot can be approximated as the inscribed square of the circle with 2 h tan θ 2 on each side when we denote d as the flight speed of the drone the monitoring area per one flight hour can be given by a m p f 2 h d tan θ 2 following the specifications of commercially available drones e g food and agriculture organization of the united nations 2018 we assume θ π 2 h 0 5 k m and d 40 k m h and estimate the a m p f 2828 h a 4 7 autonomy and detection accuracy both autonomy and detection accuracy need to be estimated by the system quality evaluation model to increase the detection accuracy one can use higher resolution images and computationally expensive recognition algorithms that may cause increased battery consumption since the remaining battery life directly affects the autonomy there is a performance trade off between the autonomy and the detection accuracy for the simulation purpose in this paper we approximate this trade off relation with a linear function as below p a c c 0 1 t f l t 1 as fig 4 shows the function associates the autonomy in the range of 0 5 4 hours with the accuracy in the range of 0 6 0 95 in a negative correlation although the function can be non linear shape in reality we temporarily use this function for the purpose of trade off impact analysis in section 5 4 8 computation procedure based on the cld with the associated formulations we can compute the estimated values of kpis from the given system quality metrics the computation steps are applied from the bottom of the cld to the top as detailed below 1 from the system quality evaluation models the expected system quality metrics are computed the system quality metrics include the autonomy t f l t service availability p s a v and detection accuracy p a c c 2 applying the autonomy to 8 the total flight hours t t f t is computed t t f t is subsequently used to derive the monitoring area a m o n and the out of monitoring period t o u t by 9 10 11 3 the out of monitoring period is used to compute the scan probability p s c a by 7 4 by 4 multiplying the scan probability with service availability and detection accuracy yields the deforestation detection probability p d d e 5 by 3 the net deforestation prevention area per year is computed from the monitoring area deforestation detection probability and deforestation prevention success probability 6 the estimated annual deforestation area and the corresponding co2 gas emission are computed using 1 2 the system quality metrics can be considered as the parameters that are governed by a system design while the number of uav flights n f l t and the waiting time for the next uav monitoring t w t t are the variables related to the operational design as 7 indicates the effectiveness of the solution also depends on the exposure time of deforestation events consequently we examine the sensitivities to these parameters in the next section 5 analysis we evaluate the effectiveness of uav based deforestation monitoring through the numerical analysis based on the developed socio ict model since the model contains various system parameters we conduct sensitivity analysis by varying the parameter values in the practical ranges while fixing other parameters we use matlab to implement the models and conduct the sensitivity analysis 5 1 base assumptions for the analysis purpose we choose the default parameter values as summarize in table 2 for all the experiments we compare the results with different service availability values i e 0 85 0 9 0 95 we set the default value of autonomy to 2 h in reference to the specification of the uav used for deforestation monitoring in panama food and agriculture organization of the united nations 2018 the detection accuracy is set to 0 9 as it is sufficiently achievable accuracy using recent machine learning techniques pozzobon de bem et al 2020 araújo et al 2021 since a higher accuracy can be achieved at the expense of energy and autonomy we analyze the trade off between accuracy and autonomy in section 5 6 the number of uav flights is the parameter value to be decided during the solution design process we assume a drone flies to monitor 10 different locations in a sequence the waiting time for the next drone monitoring is set to 4 h considering the mid night hours in which drone monitoring is not effective in sections 5 2 and 5 3 we investigate the impacts of these operational parameters as discussed in section 4 3 we assume that the exposure time of deforestation events is exponentially distributed as it depends on the type of illegal activities and their phases de almeida et al 2021 we set the default average exposure time to 10 h for co2 gas emission caused by a hectare of deforestation we refer to the estimated numbers in the literature bellassenrenaud et al 2008 craig 2019 the expected annual deforestation rate without any prevention is estimated from the value in 2020 silva junior et al 2021 we assume the uav based deforestation monitoring system is introduced to the protected areas in the brazilian amazon which is about 44 of brazilian amazon rainforest veríssimo et al 2011 and expect that the same amount of deforestation can occur if we do not take any action the monitoring time per location is set to 3 min by assuming that the drone can fly 2 km in 3 min by 40 km h the monitoring area per one flight hour follows the system assumption described in section 4 6 5 2 the number of uav flights we evaluate the deforestation prevention area a d p v by varying the number of uav flights with different service availability settings the deforestation prevention area is a key indicator for evaluating the effectiveness of the deforestation monitoring solution as we can compute the annual deforestation rate and the expected co2 gas emission from this area fig 5 shows the sensitivity analysis results we can observe that the deforestation prevention area is increased with a higher service availability and a larger number of uav flights however the contribution of additional flights is not so significant when the number of uavs increases up to a certain number 15 as the number of uav flights increases the monitoring area enlarges at the same time however the visiting frequency at each location decreases because the monitored area by each uav flight does not overlap due to the decreased visiting frequency the deforestation prevention effects tend to be saturated at a certain level below 2300 in the case of 0 95 service availability for example the results imply that the number of uav flights needs to be determined in consideration with the trade off between the monitoring area and visiting frequency which influences the deforestation prevention area note that the deforestation prevention area estimated here is due to a sequence of flights from a single uav using multiple uavs to monitor different locations can enlarge the total deforestation prevention area accordingly in section 5 5 we examine the impacts of the number of monitoring uavs on the kpis 5 3 the maintenance time t w t t represents the waiting time for the next uav monitoring during which the uvas can be under maintenance the maintenance time is another operational parameter to be determined we evaluate the deforestation prevention area a d p v by varying t w t t with different service availability settings fig 6 shows the results as can be seen the deforestation prevention area decreases as the maintenance time increases the results look reasonable since the deforestation activities cannot be detected during the out of monitoring period during the maintenance period some parts of uavs may be repaired and software programs can be updated it is essential to make such maintenance periods as short as possible to increase the capability of deforestation prevention 5 4 the average exposure time while the above two operational parameters can be controlled in a solution design the exposure time of deforestation activities is not a controllable factor as it depends on the types of illegal deforestation activities we evaluate the impact of the average exposure time t exp on the deforestation prevention area as shown in fig 7 as the average exposure time increases the deforestation prevention area increases the result is reasonable because the scan probability and associated detection probability are increased by a larger exposure time looking at the shape of the curve the amount of increase gradually becomes insignificant as the average exposure time becomes longer this results from the minimum function in 6 that makes the scan probability to one when the exposure time is larger than the out of monitoring period t o u t the results imply the possibility that the deforestation prevention effects can be limited even using uav based monitoring if illegal deforestation activities occur in a very short time however we assume this is not the case as most deforestation activities are time consuming processes de almeida et al 2021 5 5 annual deforestation rate and co2 gas emission the effectiveness of the solution is eventually evaluated by the kpis concerned by owners and the users of the system in our system dynamics we consider two kpis the reduced annual deforestation rate adr and the associated co2 gas emission co2e the expected values of kpis with a uav based monitoring system can be computed from the deforestation prevention area per year by 1 and 2 table 3 shows the expected adr and co2e in different parameter combinations here we consider additional parameter m representing the number of locations introducing the uav based deforestation monitoring the deforestation prevention area is multiplied m times to derive the total area of deforestation prevention in the protected area when any drone based monitoring solutions are not introduced i e m 0 the expected adr is 4878 72 km2 and co2e is 114 9 megatons the value of adr and co2e decreases with increasing the number of locations m service availability p s a v the number of flights n f l t and the average exposure time t exp although the impact is not significant by monitoring with a single drone we can reduce adr and co2e considerably with a hundred uavs for example when p s a v 0 95 n f l t 15 t exp 30 m 100 the adr is decreased to 531 732 square kilometers meaning that about 4347 square kilometers are protected from deforestation by a hundred uavs in this case co2e is reduced to 12 528 megatons which corresponds to 102 4 megatons of co2 gas reduction in a year 5 6 autonomy and detection accuracy as discussed in section 4 7 there is a trade off between autonomy and detection accuracy when we assume the linear relation 12 and vary t f l t in 0 5 4 the deforestation prevention area changes as shown in fig 8 the deforestation prevention area decreases as the autonomy becomes shorter on the other hand as the autonomy becomes longer sufficiently the deforestation prevention area gradually decreases caused by the decreased accuracy although the assumed linear trade off relation does not completely reflect the reality the above result implies that there is an optimum trade off point at which the deforestation prevention area is maximized note that both autonomy and detection accuracy are quality measures constrained by the battery life it is important that systems engineers determine the system configuration by interacting with solution designers or end users to achieve the optimal trade off in terms of their kpis our socio ict model and analysis can help such a system design process 6 conclusion deforestation prevention is an emergent issue to combat global climate change uav based monitoring considered in our analysis can detect causal illegal activities automatically in real time with good accuracy compared to traditional satellite image remote monitoring this paper first focuses on the quality design of such a uav based monitoring solution in this paper we proposed the socio ict model to analyze the deforestation prevention impacts of a uav based monitoring system and conducted a numerical analysis to show the effectiveness of the model the socio ict model employs a cld to capture the relations among system quality measures operational constraints environmental variables and kpis associated with deforestation prevention we quantitatively estimated the deforestation rate and co2 gas emission through numerical analysis the results of the sensitivity analysis provide the following insights the number of drone flights has positive impacts on deforestation prevention yet the gain of additional flights becomes insignificant as the number of flights increases it is caused by the trade offs between the monitoring area and the visiting frequency of a site the waiting time for the next flights has negative impacts on deforestation prevention hence minimizing the waiting and maintenance period is an issue to be addressed in system operation design the expected deforestation prevention area is significantly affected by the exposure time of illegal activities when the out of monitoring period is smaller enough there is more chance of detecting illegal activities although the impact of a single uav monitoring is not significant we can expect a considerable reduction in annual deforestation rate and co2 gas emissions by introducing multiple uavs to cover different locations there could be an optimal trade off point that maximizes the deforestation prevention area because of the trade off between the uav s autonomy and detection accuracy to explore the best trade off a higher level model like our socio ict model plays an important role in future work we may extend the cld by considering several risk factors such as weather conditions cyber physical attacks on uavs and the capacity of a deforestation monitoring agency as mentioned in section 3 2 the risk factors may reduce the benefit of a uav based monitoring system a quantitative analysis of such negative impacts helps design and prioritize preventive measures note that we can also consider potential contributing factors as well for example monitoring systems might be a deterrent to illegal activities declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported in part by the grant of university of tsukuba basic research support program type s this research was partially supported by japan society for the promotion of science jsps invitational fellowships for research in japan appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105540 
25494,this study addresses hydrologic engineering center hydrologic modeling system hec hms model simulation issues in web environments and identifies key model parameters to facilitate practical flood forecasting jython scripts serving as an intermedium layer between the web based forecast system and hec hms model were created to enable hec dss database writing and reading and hec hms model execution as a background process empirical relationships between model parameters and time series characteristics were established to mitigate the difficulty in determining event dependent model parameters an open source web based prototype system for flood simulation and forecasting wsff based on the hec hms was built by incorporating these methods and parameter knowledge into the model and knowledge subsystem the wsff performance was evaluated via a case study involving 12 historical flood events in the chuanchang watershed southeastern china among these 12 events nine events served as calibration events to establish wsff parameter knowledge and three events were used to verify the wsff performance a consistent and satisfactory performance in terms of peak flow total flood volume peak flow timing and overall hydrograph fitting effect was found the average nash sutcliffe efficiency nse for the validation events reached 0 81 versus 0 82 for the calibration events the relative error in peak flow rep and relative error in flood volume rev indicated a reasonable flood forecasting accuracy with an rep within 15 and ret within 1 h developed as open source software the wsff could serve as a useful basis for the hydrological community regarding hec hms adoption in flood forecasting keywords decision support system flood forecast system hec hms model parameter identification web solution data availability data will be made available on request 1 introduction flood is most likely the most devastating widespread and frequent natural disaster present in human society this natural hazard created by extreme stormwater developing over a short time or excessive water flowing from upstream has caused massive damage to human life and socioeconomic systems nkwunonwo et al 2020 teng et al 2017 vafakhah et al 2018 since globally floods play a significant role in natural disasters freer et al 2013 many nonstructural measures e g physical hydrological and data driven models have been devised to understand assess and predict flood events and their adverse consequences sousa et al 2021 among these measures flood forecasting systems ffss have become an essential tool providing information that allows authorities to take early and appropriate actions to prevent and or mitigate the impacts of a flood event in general there are two broad types of flood forecasting engines that an ffs can incorporate i e physical hydrological and data driven models hussain et al 2021 compared with physical hydrological models especially semidistributed and distributed models data driven models are relatively lightweight and thus easier to integrate with ffs in addition the emergence and rapid development of deep learning methods have made this method extremely powerful and popular kabir et al 2020 kan et al 2020 luppichini et al 2022 xu et al 2021 nevertheless as its name suggests a data driven model for flood forecasting is built upon long term historical precipitation and discharge time series which are not available in most regions worldwide due to the uncertainty of flood events occurrences and a lack of enough monitoring facilities physical hydrological models on the other hand are less restrained by data availability and thus have more potential application for flood forecasting for example modelers can construct watershed models with one or a few gauged hydrological station s and use this model to forecast stream discharges to other adjacent ungauged hydrological stations chezgi et al 2020 petroselli 2020 petroselli et al 2020 in addition the applicability and suitability of physical hydrological models as flood forecasting engines have been proven by many related applications in the literature for example physical hydrological models have been used in decision support and forecast systems for integrated water resource planning and management hanington et al 2017 khiavi et al 2022 kim et al 2015 qin et al 2019 zeng et al 2012 water and soil conservation abouabdillah et al 2014 khanal et al 2018 lacombe et al 2008 peng et al 2015 flood forecasting achleitner et al 2012 shi et al 2015 etc however physical hydrological models with sophisticated representations of hydrological processes are harder and much more complex to incorporate into ffss moreover it is a desirable and de facto solution for these systems to be operated in a web environment to reduce the decision making complexity and allow public access and use of environmental information for participation in decision making processes guigoz et al 2017 rajib et al 2016 this certainly makes the integration even harder as hydrological models are generally devised as independent research tools without considering the interoperability on the internet among various models or platforms janssen et al 2008 olsson and andersson 2006 rajib et al 2016 therefore a wrap up service is required to incorporate a hydrological model into decision support and forecast systems for operation in a web environment many efforts have been made to employ existing models in web environments guigoz et al 2017 rajib et al 2016 shi et al 2015 these efforts include the development of web platforms for collaborative research and education rajib et al 2016 wen et al 2013 the development of web based decision support systems for watershed management based on the integration of web based geographical information systems and distributed hydrological models zeng et al 2012 zhang et al 2015 the development of web service based platforms for geo analysis model publishing and execution zhang et al 2020 the development of web based participatory planning tools by integrating optimization algorithms process simulation models and interfaces that allow users to spatially optimize the locations and types of conservation practices babbar sebens et al 2015 piemonti et al 2017 and the development of flood forecast systems for operational real time flood forecasting using data assimilation to update hydraulic states mure ravaud et al 2016 however most of these efforts are limited to a handful of hydrological models and in most cases the source codes of these solutions are not publicly available in addition the manner in which the model inputs are edited the model is executed and desirable model results are retrieved differs from model to model which suggests that a model specific solution is required for an individual hydrological model in this study we focused on addressing this issue considering the hydrologic engineering center hydrologic modeling system hec hms the hec hms has been applied in a wide range of geographic watersheds to solve problems regarding water supply bhadoriya et al 2020 ndeketeya and dundu 2021 flood forecasting azam et al 2017 azizi et al 2021 future urbanization impacts du et al 2019 gao et al 2017 ratnayake et al 2022 and floodplain regulation abdessamed and abderrazak 2019 ali et al 2021 to name but a few bhadoriya et al 2020 investigated the climate change impact on the demand supply characteristics of a reservoir in a subtropical region of india by feeding a calibrated and validated hec hms model with eight bias corrected global circulation model projections under two representative concentration pathways 4 5 and 8 5 to simulate the climate change effect on three future time reservoir inflows with respect to the baseline period gonzalez cao et al 2021 reproduced the most catastrophic flood event recorded in europe which occurred in december 1876 in badajoz spain guadiana basin through integration of the hec hms and iber models devi et al 2019 analyzed the impact of urban sprawl on future flooding in chennai city india by chaining an artificial neural network model for the construction of spatially varying urban sprawl scenarios a hydrologic hec hms model for the generation of flood hydrographs and a hydraulic hec ras model for the simulation of possible flood inundation patterns despite many reports on the application of the hec hms model in various fields only a few studies have reported the application of the hec hms model in cyber based environments among these studies horak et al 2008 applied the concepts of web services and open distributed architecture to the development of t dss a prototype model customized for web based hydro information systems in t dss a web service interface wrapped over the hec hms model was created to accommodate data preprocessing model execution and data postprocessing zeng et al 2012 developed a web based decision support system for water resource management in daegu city republic of korea via the integration of the national water management information system wamis a flood forecasting method based on the hec hms model an urban water demand forecasting model using an artificial neural network and a mathematical model for optimal water resource allocation using a genetic algorithm mourato et al 2021 developed the flood forecast and alert system for the agueda river basin located in portugal s center region by linking a rainfall forecasting model wrf a hydrological model hec hms a hydraulic model hec ras 2d and a web gis platform recently mattos et al 2022 developed a flood alert web application for the prosa basin midwestern brazil to send early flood warning messages to ordinary people the flood forecasting information is fed by a decision support system based on hec hms and hec ras while these studies have demonstrated the possibility of integrating the hec hms model into practical use details on how the hec hms model interacts with other components have not been revealed and to the authors knowledge there have been no open source solutions until this project another issue pertaining to the usage of the hec hms model in flood forecasting is that some model parameters are event dependent kamali et al 2013 lin et al 2013 yan et al 2020 for instance the initial loss which is determined by the amount of precipitation loss via interception depression storage soil water content and evaporation is by definition event dependent this poses another barrier to using the hec hms model in flood forecasting as these parameters are very difficult to determine in forecast scenarios studies have suggested an indirect method to determine these parameters that entails establishing empirical relationships between model parameters and measurable boundary conditions for example lin et al 2013 suggested that the initial loss is highly related to the initial discharge and yan et al 2020 argued that the flood wave velocity parameter exhibits a strong relationship with the precipitation intensity these relationships mitigate the difficulty in identifying event based parameters and thus provide a viable solution for hec hms application in flood forecasting thus it is critical to incorporate this knowledge into the knowledge subsystem when building a flood forecast system backed by the hec hms model in this study we proposed solutions to apply the hec hms model in flood forecasting by addressing the two issues recognized in the previous introduction the first issue was solved by applying the jython language to create a communication tunnel between the web based forecast system and model subsystem three jython scripts serving as an intermedium layer between the web based forecast system and hec hms model were created to enable writing and reading to from the hec dss database and hec hms model execution as a background process the second issue pertaining to identifying event based parameters was addressed by establishing empirical relationships between these parameters and easy to retrieve time series characteristics next an open source web based prototype system for flood simulation and forecasting wsff available at https github com djzhang80 wsff was developed according to the aforementioned solutions and evaluated against 12 historical flood events in the chuanchang watershed in southeastern china to assess its capability in simulating and forecasting flood processes it is noted that the applicability in flood forecasting of the wsff is currently tested with only historical flood datasets due to the lack of real time quantitative precipitation forecast data sources 2 methods 2 1 system architecture and implementation the conceptual architecture of the web based prototype system for flood simulation and forecasting is shown in fig 1 and comprises a graphical user interface gui service subsystem ss information subsystem is knowledge subsystem ks and model subsystem ms the gui provides rich and interactive web based interfaces for users to input data perform flood forecasting and present results the ss is the core component of the entire system which is implemented in php language based on the thinkphp framework this subsystem supplies a series of web services for users to perform flood forecasting and system management routines the is can be used for data storage in general there are four types of data in the is i e time series data structured data model input files and intermediate files for chart presentation time series data are stored in the hydrologic engineering center data storage system hec dss database because the hec hms model uses this database for storing and retrieving time series data the driving data for manual flood forecasting i e precipitation data inputs via an online spreadsheet and other systems managing related data are controlled with the open source relation database mysql the is also includes model input and intermediate files extracted from the hec dss database for graphical presentation the ms and ks modules are used to communicate with and identify proper parameters for the hec hms model respectively the implemented details of these subsystems are elucidated in the following subsections 2 1 1 model subsystem the ms module adopts the hec hms as its forecast engine the hec hms was developed by the us army corps of engineers to simulate precipitation runoff processes in dendritic watershed systems it was designed as a generalized modeling system applicable in a wide range of geographic watersheds to solve the widest possible range of problems such as water supply flood forecasting urban drainage future urbanization impact reservoir spillway design flood damage reduction floodplain regulation and wetland hydrological issues there are three primary components of the hec hms including basin models meteorological models and control specifications the basin model is used to represent the physical watershed via the principle of converting atmospheric conditions into streamflow conditions at specific locations in the studied watershed the purpose of the basin model is accomplished through proper coordination of various hydrological elements including sub basins reaches junctions sources sinks reservoirs and diversions the meteorological model component is used to determine the amount of precipitation input for a sub basin element this component can utilize both point and gridded precipitation data and can model frozen and liquid precipitation and evapotranspiration the amount of gaseous liquid and solid water phases is determined by multiple snowmelt and evapotranspiration methods the snowmelt methods include an energy balance approach and temperature index algorithm the evapotranspiration methods incorporate the monthly average penman monteith priestly taylor and user specified time series methods it should be noted that an evapotranspiration method is only incorporated in a meteorological model when the long term hydrologic response in the watershed is simulated the time span and calculation interval are determined by the control specifications the control specifications include simulation starting and ending points and computation time step for more details on the hec hms model readers are encouraged to refer to the technical reference document and manual of the hec hms the ms module enables the ss module to forecast floods based on the hec hms model which in turn relies on the hec dss database to store and retrieve time series data it is impossible to directly employ the hec hms model and hec dss database in a web environment because the default hec hms model and hec dss database operate in an interactive fashion however any processes invoked directly in a web environment must operate as a silent background process the hec hms software provides an internal jython interpreter that allows users to execute jython scripts from the command line to perform model simulations in silent mode as wsff is implemented in php an invoking sequence of php shell jython is adopted to execute the hec hms model from within the wsff fig 2 shows the typical processes for manipulating dss files or hec hms models if users to want to execute certain models a jython script and shell script are generated dynamically with information that pertains to the path of the model and identifies the desired simulation when these files are generated the exec function is invoked with its argument pointing to the shell file the generated jython scripts in the ms module are used for inputting model driven time series data performing hec hms model simulations and extracting the desired simulated results as jython is an implementation of the python programming language that is designed to run on the java platform it can import and use any java class the instructions in the generated jython scripts are actually supported by some java classes provided by the hec hms including the timeseriescontainers hectimeseries and hecdatamanager classes for storing and retrieving time series data and the jythonhms and project classes for performing model simulation fig 3 shows how instances of these classes collaborate to manipulate model and time series the put and get methods of the instance of the hectimeseries class are used to input and retrieve time series respectively the instance of timeseriescontainer as its name suggests is used to hold time series that are input to or retrieved from a dss file the jythonhms class has a static method openproject that can be used to open a model or project and return the opened model a simulation can be performed by invoking the compute method of the instance of the opened model 2 1 2 knowledge subsystem for flood forecasting based on distributed hydrological models such as the hec hms it is crucial to obtain a proper parameter set that can represent the hydrological characteristics of the underlying system and a specific event the ks module can determine a parameter set for the hec hms model by utilizing predefined domain specific knowledge and or experience of adept experts in this initial effort the ks module incorporates two methods to help users identify the proper parameter set for flood forecasting i e empirical and statistical methods for experienced users the parameter set can be identified based on their experience or via comparison to historical flood events and a parameter set associated with certain events exhibiting similar characteristics can then be selected however it is difficult for users with less experience to find a proper parameter set in this way in contrast statistical methods rely on statistical rules of model parameters many studies related to hec hms model parameter identification such as those by lin et al 2013 and yan et al 2020 have indicated that two important parameters of the hec hms model i e the initial abstraction ratio and flood wave velocity are closely related to the initial discharge and rainfall intensity respectively these statistical relationships can provide a viable and easy measure for users to derive model parameters from measurable variables depending on the type of knowledge empirical or statistical the knowledge pertaining to the key parameters of the hec hms is stored in different ways specifically the empirical knowledge is stored with historical flood models and the statistical knowledge is saved in the backend mysql database as stored procedures fig 4 shows the class diagram of the key components of the ks module the empiricalparameterknowledge service allows users to apply parameters from a historical flood event model to a specific model the statisticalparameterknowledge service however provides a function which accepts quantifiable characteristics of precipitation and runoff time series i e the initial discharge or rainfall intensity that enable users to calculate parameter value because it is easier to update stored procedures than the code the statistical relationships for estimating model parameters are presented as stored procedures in the backend database 2 2 visualization the gui was built by integrating multiple open source libraries such as dhtmlx leaflet and apache echarts in this subsystem the dhtmlx leaflet and apache echarts libraries were used to create spreadsheets maps and charts respectively fig 5 shows a map viewer to visualize watershed maps including a base map watershed boundaries sub basins streams and meteorological and hydrological gauges the base map is obtained from openstreetmap osm a volunteered geographical information service that provides free editable maps originating from all over the world other maps are rendered from geojson files which are converted from shapefiles via the geospatial data abstraction library gdal tool in this study apache echarts was used to visualize meteorological and hydrological time series data apache echarts is a charting and data visualization javascript library that provides a powerful interactive charting and data visualization library for web browsers mobile apps and backend usage fig 6 shows how time series data can be visualized with apache echarts in each chart panel there are buttons and legends allowing users to highlight or toggle a particular hyetograph or hydrograph zoom into a region of interest restore the view to the previous status refresh the graph export the graph as an image file and select meteorological gauges to depict in the chart in addition charts can be automatically and dynamically updated when combined with asynchronous javascript and xml ajax technology this can be very useful for the functionality of automatic flood forecasting whenever new weather forecasting data are sent to the wsff this triggers the simulation process of the hec hms model and dynamic updating of hyetographs and hydrographs 2 3 key functionalities this section describes the key functionalities of the web based prototype system for flood simulation and forecasting including manual flood forecasting automatic flood forecasting and historical flood reproduction fig 7 shows a typical interface of the manual flood forecasting functionality this interface comprises three panels the first panel is a spreadsheet that allows users to input and edit precipitation data similarly to a desktop spreadsheet application each column of the spreadsheet contains precipitation time series data retrieved from a meteorological gauge the first cell of each column contains the name of the meteorological gauge at the bottom of the first panel there are three controls the first control i e the input box is used to input the sheet identifier when the load create button is triggered the system creates an empty sheet if the sheet identifier does not match any existing sheets in the database otherwise the system loads the sheet specified by the identifier from the database once the forecast button is triggered the flood forecasting process is initiated which invokes the underlying processes to enter the time series data in the spreadsheet into the hec dss database perform hec hms model simulations and extract the desired simulation results from the hec dss database the second or top right panel is a chart that shows hyetographs of the selected meteorological gauges the third or bottom right panel visualizes the forecast results and allows users to compare current forecast results to previous alternatives automatic flood forecasting uses a polled operation to acquire third party forecasting data or observed meteorological data and drives the flood forecasting processes automatically once there is an update of the forecasting meteorological source the latest information acquired is presented in the left panel of the automatic flood forecasting interface fig 8 and the flood forecast process is triggered to update the acquired information in the hec dss database perform hec hms model simulations extract the desired simulation results from the hec dss database and finally dynamically update the corresponding hyetographs and hydrographs the wsff includes a model repository of historical flood events fig 9 shows the interface for viewing and reproducing historical flood events in this repository users are allowed to reproduce the model results of these historical flood events and compare their results to the observed hydrographs in addition this repository can be of significant help in assisting users in identifying a proper model parameter set by comparing the hydrological characteristics of historical and forecasting events and applying the selected parameter set to the forecasting model 3 case study in this study we used a dataset of the chuanchang watershed located in southeastern fujian province china to evaluate the functionalities and flood forecasting accuracy of the wsff specifically a hydrological model of the chuanchang watershed and empirical equations considering parameters λ and v were first established based on nine historical flood events next these hydrological models and parameter identification knowledge were incorporated into the wsff finally three flood events were used to verify the accuracy of the wsff in terms of flood forecasting the watershed dataset and test results are briefly introduced in the following subsections 3 1 watershed and dataset description the chuanchang river is a main branch upstream of the jiulongjiang watershed which is located on the southeastern coast of china the drainage area of the chuanchang hydrological station was chosen as the study area 782 km2 fig 10 the elevation of this area ranges from 62 to 1380 m fig 10 the study area is characterized by a subtropical monsoon climate with an average annual temperature of 21 c and an annual precipitation range of 1400 1800 mm more than 70 of the annual rainfall occurs from april september with numerous rainstorms caused by the monsoon climate convective storms and typhoons as it is dominated by the typical monsoon climate of southern china the runoff in this study area is primarily generated through the saturation excess mechanism i e runoff is generated when the soil becomes saturated the chuanchang river is prone to flash floods due to the large vertical fall of the riverbed and severe rainstorms these floods are exacerbated by the rapid development of agriculture and urbanization in this area flash flood warning and forecasting systems are urgently needed to alleviate the impact of environmental changes the datasets used to build the hydrological model included a 30 m resolution digital elevation model dem obtained from the geospatial data cloud fig 10 http www gscloud cn a soil map 1 500000 was provided by the soil fertilizer laboratory of fujian province in china fig 11 a the two largest soil types were red earth 50 45 and paddy soil 16 66 the land use was manually interpreted from landsat thematic mapper images for 1986 fig 11 b the dominant land use is forest 74 93 followed by cropland 13 47 mainly rice and peanut land and grass 10 51 soil and land use data were used to calculate the value of the soil conservation service curve number scs cn hourly rainfall and runoff data were available for 6 rain gauges fig 10 with elevations ranging from 117 to 973 m and the chuanchang flow gauge in the chuanchang watershed covering the 1972 1977 period among the six rain gauges three are located in the head sub basins and the rest are in the central part of the study watershed rainfall streamflow and flood peak flow data were obtained from the hydrology and water resources bureau of fujian province flood events ranking in the top two annual peak flows for the studied period were selected resulting in 12 flood events selected to calibrate and validate the hydrological model instead of selecting them according the severity or return period of the flood events this was because flood discharge data beyond the studied period were not published by the authority and due to the fact that the selected flood events with peak flows ranging from 228 2 to 1193 4 m3 s were more suitable for estimating the knowledge for determining parameters and verifying the performance of the wsff with respect to the different flood magnitudes 3 2 construction of the hec hms model the hec hms model is a semidistributed hydrological model that is widely used in flood event simulations the study area was divided into 6 sub basins based on dem via the geospatial hydrologic modeling extension of hec hms hec geohms to reflect spatial variations in a watershed the average rainfall in every sub basin was calculated using the reciprocal distance squared method runoff volume and loss were calculated using the scs curve number scs cn method the scs unit hydrograph scs uh was used to simulate direct runoff the recession method was used to calculate baseflow and the muskingum method was used to perform the reach routing calculations the key parameters of the model were the initial loss ratio λ flood routing wave velocity v muskingum k channel length l lag time t and soil conservation service curve number cn the values of cn were calculated based on soil and land use data and l and t were calculated based on dem via the hec geohms tool muskingum k is a function of parameters l and v and thus can be calculated based on the parameter values of l and v therefore the key parameters requiring calibration are λ and v 3 3 wsff evaluation setup because the empirical equations for λ and v incorporated into the wsff were derived from nine historical flood events during the 1972 1977 period three flood events that occurred during the same period were used for reasonable evaluation of the accuracy of the wsff in terms of flood forecasting the relative error in peak flow rep relative error in flood volume rev error in peak flow time ept nash sutcliffe efficiency nse and coefficient of determination r2 were used as goodness of fit measures to evaluate the wsff the definitions of these measures are given as follows 1 r e p q s q o q o 100 2 r e v v s v o v o 100 3 e p t t s t o 4 n s e 1 i 1 n q o i q s i 2 i 1 n q o i q o a v e 2 5 r 2 i 1 n q o i q o a v e q s i q s a v e i 1 n q o i q o a v e 2 i 1 n q s i q s a v e 2 2 where qs and qo are the simulated and observed flood peak flows respectively vs and vo are the simulated and observed flood volumes respectively ts and to are the simulated and observed timings respectively of the peak flow and qs i and qo i are the simulated and observed flood flows respectively at time i moreover n is the total time series and qo ave is the average observed flow 3 4 results and discussion 3 4 1 model calibration as previously mentioned 12 historical flood events in the studied watershed were divided into calibration and validation sets the calibration set with nine events was used to establish empirical relationships between the antecedent discharge and initial abstraction λ and the maximum hourly precipitation and flood wave velocity v the remaining three events were used to validate the performance of the wsff when incorporating the established empirical relationships as a knowledge base for flood forecasting to this end the flood events in the first set were manually calibrated to find suitable λ and v values that can realize a reasonable fitting effect between the observed and simulated discharge values table 1 lists the calibration results for these events in this table the r2 nse and rev indices reflect the overall goodness of fit of the calibrated models a reasonable overall fitting effect between the simulated and observed discharge values can be observed with respect to r2 ranging from 0 82 to 0 93 nse ranging from 0 72 to 0 90 and rev remaining within 12 the rep and ret indices reflect the model performance in modeling the flood peak volume and time respectively the reps for these events remained within 25 indicating an acceptable performance in flood peak simulation most rets are zero indicating good synchronization of the simulated and observed peak times fig 12 shows the observed and simulated hydrographs of the nine flood events in general the simulated discharge values fit the observed discharge values very well except for some circumstances where there are multiple flood peaks within a flood event e g events 740707 and 741019 3 4 2 identifying the empirical relationship for parameters λ and v it is believed that some hms parameters e g λ and v should have distinct values from event to event to mimic the characteristics of a specific flood event lin et al 2013 yan et al 2020 therefore it is critical to find a suitable parameter set to apply the hms in flood forecasting previous studies have indicated strong relationships between λ and v although these relationships may differ from watershed to watershed these relationships can largely ease the burdens of a modeler in identifying proper model parameters because the antecedent discharge and precipitation intensity can be easily retrieved from observed time series in light of these studies parameter sets identified during calibration were used to establish empirical equations for λ and v for both relationships five potential empirical equations i e power linear polynomial logarithmic and exponential equations were used to fit the relationships between the antecedent discharge and initial abstraction and the precipitation intensity and flood wave velocity with these equations established the antecedent discharge could be represented by the onset of the hydrograph and the precipitation intensity could be determined by the maximum hourly precipitation the potential empirical equations were further evaluated against the remaining three flood events specifically different parameter sets were derived using these potential empirical equations a total of 25 parameter sets were generated and their performance in the representation of event characteristics was evaluated and compared table 1 and fig 13 the logarithmic and exponential equations were identified as the best options for estimating parameters λ and v respectively based on the nse index fig 14 the selected empirical equations were found to achieve a consistent performance across the validation events for instance the average nse for the validation events is 0 81 which is close to the value of 0 82 for the calibration events in addition the reps and rets indicated a reasonable flood forecasting accuracy with reps within 15 and rets within 1 h finally the selected empirical equations were incorporated into the wsff which can be given as follows 6 λ 15 9 log q a n t 77 9 7 v 0 417 i 0 966 where qant is the antecedent discharge which is the discharge at the initial increase point of river flow in the flood hydrograph lin et al 2013 due to the high uncertainty in selecting this point the average flow lasted for 24 h before the starting point was selected as the antecedent discharge moreover e is the base of the natural logarithm and i is the maximum hourly precipitation mm hour 4 discussion the primary differences between the wsff and other ffss stated previously are 1 the model subsystem i e hec hms model is fully coupled into the whole system 2 event dependent parameters can be dynamically estimated and updated and hence enable event based flood forecasting and 3 a detailed description of the integration of different subsystems and open available source codes enables the reproduction of this study and possible extension to other regions and fields see our discussion later for the first noted difference most current ffss did not directly interact with hec hms they usually act as consumers of the hec hms model i e rendering the model results stored in the hec dss database where the hec hms model saves its simulation results secondly it is widely accepted that some model parameters of the hec hms model are event dependent kamali et al 2013 lin et al 2013 yan et al 2020 however most ffss did not account for the dynamic change in the model parameter the case study results indicate that the wsff can produce an overall satisfactory performance in terms of peak flow total flood volume peak flow timing and overall hydrograph fitting effect and hence demonstrate the potential application of this system to flood simulation and forecasting in addition the wsff has some other advantages over traditional flood simulation and forecasting tools as a web based solution wsff can be operated in a web environment which means that users can access this system from anywhere at any time this feature can be of great importance to foster the participation of stakeholders with different goals and perspectives in decision making processes to achieve feasible and successful determinations and to easily disseminate flood forecasting information and alerts to the public in addition wsff was developed based on the sophisticated and semidistributed hec hms model instead of some empirical methods or lumped models this feature lends this system many advantages that may be hard or impossible for other methods to possess such as investigating the response of runoff to changes in natural factors e g chezgi et al 2020 and human activities and having more potential applications to flood forecasting in ungauged watersheds e g petroselli et al 2020 for example modelers can use watershed models with one or a few gauged hydrological station s to forecast stream discharges in adjacent ungauged hydrological stations moreover developed as an open source project the wsff can be adapted to other regions by reusing the source codes provided by this study and incorporating the regional hec hms model of interest it should be noted that the performance of the wsff is mainly dependent on its forecasting engine and knowledge subsystem i e the hydrological model of the case study watershed and knowledge for identifying the proper parameter set needed for the forecasting model to this end for users who want to apply the wsff in their study watershed it is crucial to develop a hydrological model that can reasonably represent the hydrological characteristics of the watershed of interest and incorporate watershed specific knowledge into the knowledge subsystem to properly identify model parameters this suggests that users must rebuild the empirical equations for λ and v we incorporated in this study because these equations are generally watershed specific as a proof of concept or prototype system the wsff has shown great potential in flood forecasting however more work is required to turn the wsff into a fully fledged ffs for example while the wsff enables users to simultaneously perform most tasks it allows only one forecasting task to be executed at a time in other words simultaneous forecast scenarios must be queued another drawback of the wsff is that the hec hms model and other subsystems interact through a handful of jython script files which means that subsystems of the wsff are tightly coupled and cannot deploy over separated machines this certainly affects the expansion of the wsff in this study we used a pseudo meteorological forecasting data source to demonstrate the ability of the wsff to perform automatic flood forecasting and we are currently developing an interface with a meteorological radar system to acquire real time meteorological forecasting data we hope that the effectiveness of the wsff can be sufficiently assessed with more recent flood events when this interface becomes available as mentioned above there are some issues that need to be addressed to facilitate wsff as a fully fledged operational system for flood simulation and forecasting we are currently working on a plugin for the hec hms model that allows users to interact with the hec hms model through a handful of restful webservices representational state transfer style webservices with this plugin developers can deploy subsystems of the wsff in a loose manner this means that we can serve our hec hms model through microservice instance building with virtual technologies such as docker and kubernetes which lends it great opportunity to achieve simultaneous access and boost the performance of the wsff in this study we employed empirical relationships between model parameters and time series characteristics to calculate event specific parameters for the hec hms model and thus enabled wsff to perform flood simulation and forecasting nevertheless the empirical relationships were established manually which requires a large amount of time and effort therefore another ongoing effort for improving the wsff is to incorporate optimization methods to help modelers establish a knowledge subsystem of the wsff i e methods to identify proper model parameters for the hec hms model for their study regions although in this study we only discuss the use of the hec hms model in the web environment to perform flood simulation and forecasting it is possible to adapt this system for decision support systems dedicated to integrated water resource management water and soil conservation reservoir management climate change effects investigation etc to adapt this system to any of these potential applications minimal effort is needed as most functionalities such as model input editing model execution model results retrieving and or modules such as system configuration user management and file storage are readily available in the wsff the primary actions developers need to take are building guis for mapping decision making inputs to model inputs and rendering results to users it is also possible to wrap up key functionalities such as model input editing model execution and model results retrieval as apis application programming interfaces in this case hec hms can easily bridge existing tools and platforms e g swat cup and swatshare for example when linked with swatshare it can bring swatshare as a collaborative environment for hydrology research and education to the community of the hec hms model 5 summary in this study a web based prototype system wsff for flood simulation and forecasting based on the hec hms model was developed by integrating state of the art methods and libraries e g dhtmlx leaflet apache echarts thinkphp and hec dss the performance of the wsff was evaluated against 12 historical flood events occurring in the chuanchang watershed located in southeastern china the test results suggest that the wsff can yield an overall satisfactory performance in terms of peak flow total flood volume peak flow timing and overall hydrograph fitting effect to our knowledge this is the first open source solution based on the hec hms model as a flood forecasting engine in a web environment together with the wide application community and robustness of the hec hms model we believe that the conceptual framework and software proposed and developed in this study could serve as a useful basis for the hydrological community to further develop the wsff into a fully fledged operational system for flood forecasting code availability the source code of wsff is released by mit under license source codes are available on the github repository https github com djzhang80 wsff author contribution qiaoying lin methodology writing original draft software writing review editing funding acquisition bingqing lin methodology writing original draft software writing review editing dejian zhang conceptualization funding acquisition supervision writing review editing jiefeng wu writing original draft software writing review editing validation declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests dejain zhang reports financial support was provided by natural science foundation of fujian province acknowledgments this work was financially supported by the natural science foundation of fujian province grant numbers 2020j01779 and 2021j011189 
25494,this study addresses hydrologic engineering center hydrologic modeling system hec hms model simulation issues in web environments and identifies key model parameters to facilitate practical flood forecasting jython scripts serving as an intermedium layer between the web based forecast system and hec hms model were created to enable hec dss database writing and reading and hec hms model execution as a background process empirical relationships between model parameters and time series characteristics were established to mitigate the difficulty in determining event dependent model parameters an open source web based prototype system for flood simulation and forecasting wsff based on the hec hms was built by incorporating these methods and parameter knowledge into the model and knowledge subsystem the wsff performance was evaluated via a case study involving 12 historical flood events in the chuanchang watershed southeastern china among these 12 events nine events served as calibration events to establish wsff parameter knowledge and three events were used to verify the wsff performance a consistent and satisfactory performance in terms of peak flow total flood volume peak flow timing and overall hydrograph fitting effect was found the average nash sutcliffe efficiency nse for the validation events reached 0 81 versus 0 82 for the calibration events the relative error in peak flow rep and relative error in flood volume rev indicated a reasonable flood forecasting accuracy with an rep within 15 and ret within 1 h developed as open source software the wsff could serve as a useful basis for the hydrological community regarding hec hms adoption in flood forecasting keywords decision support system flood forecast system hec hms model parameter identification web solution data availability data will be made available on request 1 introduction flood is most likely the most devastating widespread and frequent natural disaster present in human society this natural hazard created by extreme stormwater developing over a short time or excessive water flowing from upstream has caused massive damage to human life and socioeconomic systems nkwunonwo et al 2020 teng et al 2017 vafakhah et al 2018 since globally floods play a significant role in natural disasters freer et al 2013 many nonstructural measures e g physical hydrological and data driven models have been devised to understand assess and predict flood events and their adverse consequences sousa et al 2021 among these measures flood forecasting systems ffss have become an essential tool providing information that allows authorities to take early and appropriate actions to prevent and or mitigate the impacts of a flood event in general there are two broad types of flood forecasting engines that an ffs can incorporate i e physical hydrological and data driven models hussain et al 2021 compared with physical hydrological models especially semidistributed and distributed models data driven models are relatively lightweight and thus easier to integrate with ffs in addition the emergence and rapid development of deep learning methods have made this method extremely powerful and popular kabir et al 2020 kan et al 2020 luppichini et al 2022 xu et al 2021 nevertheless as its name suggests a data driven model for flood forecasting is built upon long term historical precipitation and discharge time series which are not available in most regions worldwide due to the uncertainty of flood events occurrences and a lack of enough monitoring facilities physical hydrological models on the other hand are less restrained by data availability and thus have more potential application for flood forecasting for example modelers can construct watershed models with one or a few gauged hydrological station s and use this model to forecast stream discharges to other adjacent ungauged hydrological stations chezgi et al 2020 petroselli 2020 petroselli et al 2020 in addition the applicability and suitability of physical hydrological models as flood forecasting engines have been proven by many related applications in the literature for example physical hydrological models have been used in decision support and forecast systems for integrated water resource planning and management hanington et al 2017 khiavi et al 2022 kim et al 2015 qin et al 2019 zeng et al 2012 water and soil conservation abouabdillah et al 2014 khanal et al 2018 lacombe et al 2008 peng et al 2015 flood forecasting achleitner et al 2012 shi et al 2015 etc however physical hydrological models with sophisticated representations of hydrological processes are harder and much more complex to incorporate into ffss moreover it is a desirable and de facto solution for these systems to be operated in a web environment to reduce the decision making complexity and allow public access and use of environmental information for participation in decision making processes guigoz et al 2017 rajib et al 2016 this certainly makes the integration even harder as hydrological models are generally devised as independent research tools without considering the interoperability on the internet among various models or platforms janssen et al 2008 olsson and andersson 2006 rajib et al 2016 therefore a wrap up service is required to incorporate a hydrological model into decision support and forecast systems for operation in a web environment many efforts have been made to employ existing models in web environments guigoz et al 2017 rajib et al 2016 shi et al 2015 these efforts include the development of web platforms for collaborative research and education rajib et al 2016 wen et al 2013 the development of web based decision support systems for watershed management based on the integration of web based geographical information systems and distributed hydrological models zeng et al 2012 zhang et al 2015 the development of web service based platforms for geo analysis model publishing and execution zhang et al 2020 the development of web based participatory planning tools by integrating optimization algorithms process simulation models and interfaces that allow users to spatially optimize the locations and types of conservation practices babbar sebens et al 2015 piemonti et al 2017 and the development of flood forecast systems for operational real time flood forecasting using data assimilation to update hydraulic states mure ravaud et al 2016 however most of these efforts are limited to a handful of hydrological models and in most cases the source codes of these solutions are not publicly available in addition the manner in which the model inputs are edited the model is executed and desirable model results are retrieved differs from model to model which suggests that a model specific solution is required for an individual hydrological model in this study we focused on addressing this issue considering the hydrologic engineering center hydrologic modeling system hec hms the hec hms has been applied in a wide range of geographic watersheds to solve problems regarding water supply bhadoriya et al 2020 ndeketeya and dundu 2021 flood forecasting azam et al 2017 azizi et al 2021 future urbanization impacts du et al 2019 gao et al 2017 ratnayake et al 2022 and floodplain regulation abdessamed and abderrazak 2019 ali et al 2021 to name but a few bhadoriya et al 2020 investigated the climate change impact on the demand supply characteristics of a reservoir in a subtropical region of india by feeding a calibrated and validated hec hms model with eight bias corrected global circulation model projections under two representative concentration pathways 4 5 and 8 5 to simulate the climate change effect on three future time reservoir inflows with respect to the baseline period gonzalez cao et al 2021 reproduced the most catastrophic flood event recorded in europe which occurred in december 1876 in badajoz spain guadiana basin through integration of the hec hms and iber models devi et al 2019 analyzed the impact of urban sprawl on future flooding in chennai city india by chaining an artificial neural network model for the construction of spatially varying urban sprawl scenarios a hydrologic hec hms model for the generation of flood hydrographs and a hydraulic hec ras model for the simulation of possible flood inundation patterns despite many reports on the application of the hec hms model in various fields only a few studies have reported the application of the hec hms model in cyber based environments among these studies horak et al 2008 applied the concepts of web services and open distributed architecture to the development of t dss a prototype model customized for web based hydro information systems in t dss a web service interface wrapped over the hec hms model was created to accommodate data preprocessing model execution and data postprocessing zeng et al 2012 developed a web based decision support system for water resource management in daegu city republic of korea via the integration of the national water management information system wamis a flood forecasting method based on the hec hms model an urban water demand forecasting model using an artificial neural network and a mathematical model for optimal water resource allocation using a genetic algorithm mourato et al 2021 developed the flood forecast and alert system for the agueda river basin located in portugal s center region by linking a rainfall forecasting model wrf a hydrological model hec hms a hydraulic model hec ras 2d and a web gis platform recently mattos et al 2022 developed a flood alert web application for the prosa basin midwestern brazil to send early flood warning messages to ordinary people the flood forecasting information is fed by a decision support system based on hec hms and hec ras while these studies have demonstrated the possibility of integrating the hec hms model into practical use details on how the hec hms model interacts with other components have not been revealed and to the authors knowledge there have been no open source solutions until this project another issue pertaining to the usage of the hec hms model in flood forecasting is that some model parameters are event dependent kamali et al 2013 lin et al 2013 yan et al 2020 for instance the initial loss which is determined by the amount of precipitation loss via interception depression storage soil water content and evaporation is by definition event dependent this poses another barrier to using the hec hms model in flood forecasting as these parameters are very difficult to determine in forecast scenarios studies have suggested an indirect method to determine these parameters that entails establishing empirical relationships between model parameters and measurable boundary conditions for example lin et al 2013 suggested that the initial loss is highly related to the initial discharge and yan et al 2020 argued that the flood wave velocity parameter exhibits a strong relationship with the precipitation intensity these relationships mitigate the difficulty in identifying event based parameters and thus provide a viable solution for hec hms application in flood forecasting thus it is critical to incorporate this knowledge into the knowledge subsystem when building a flood forecast system backed by the hec hms model in this study we proposed solutions to apply the hec hms model in flood forecasting by addressing the two issues recognized in the previous introduction the first issue was solved by applying the jython language to create a communication tunnel between the web based forecast system and model subsystem three jython scripts serving as an intermedium layer between the web based forecast system and hec hms model were created to enable writing and reading to from the hec dss database and hec hms model execution as a background process the second issue pertaining to identifying event based parameters was addressed by establishing empirical relationships between these parameters and easy to retrieve time series characteristics next an open source web based prototype system for flood simulation and forecasting wsff available at https github com djzhang80 wsff was developed according to the aforementioned solutions and evaluated against 12 historical flood events in the chuanchang watershed in southeastern china to assess its capability in simulating and forecasting flood processes it is noted that the applicability in flood forecasting of the wsff is currently tested with only historical flood datasets due to the lack of real time quantitative precipitation forecast data sources 2 methods 2 1 system architecture and implementation the conceptual architecture of the web based prototype system for flood simulation and forecasting is shown in fig 1 and comprises a graphical user interface gui service subsystem ss information subsystem is knowledge subsystem ks and model subsystem ms the gui provides rich and interactive web based interfaces for users to input data perform flood forecasting and present results the ss is the core component of the entire system which is implemented in php language based on the thinkphp framework this subsystem supplies a series of web services for users to perform flood forecasting and system management routines the is can be used for data storage in general there are four types of data in the is i e time series data structured data model input files and intermediate files for chart presentation time series data are stored in the hydrologic engineering center data storage system hec dss database because the hec hms model uses this database for storing and retrieving time series data the driving data for manual flood forecasting i e precipitation data inputs via an online spreadsheet and other systems managing related data are controlled with the open source relation database mysql the is also includes model input and intermediate files extracted from the hec dss database for graphical presentation the ms and ks modules are used to communicate with and identify proper parameters for the hec hms model respectively the implemented details of these subsystems are elucidated in the following subsections 2 1 1 model subsystem the ms module adopts the hec hms as its forecast engine the hec hms was developed by the us army corps of engineers to simulate precipitation runoff processes in dendritic watershed systems it was designed as a generalized modeling system applicable in a wide range of geographic watersheds to solve the widest possible range of problems such as water supply flood forecasting urban drainage future urbanization impact reservoir spillway design flood damage reduction floodplain regulation and wetland hydrological issues there are three primary components of the hec hms including basin models meteorological models and control specifications the basin model is used to represent the physical watershed via the principle of converting atmospheric conditions into streamflow conditions at specific locations in the studied watershed the purpose of the basin model is accomplished through proper coordination of various hydrological elements including sub basins reaches junctions sources sinks reservoirs and diversions the meteorological model component is used to determine the amount of precipitation input for a sub basin element this component can utilize both point and gridded precipitation data and can model frozen and liquid precipitation and evapotranspiration the amount of gaseous liquid and solid water phases is determined by multiple snowmelt and evapotranspiration methods the snowmelt methods include an energy balance approach and temperature index algorithm the evapotranspiration methods incorporate the monthly average penman monteith priestly taylor and user specified time series methods it should be noted that an evapotranspiration method is only incorporated in a meteorological model when the long term hydrologic response in the watershed is simulated the time span and calculation interval are determined by the control specifications the control specifications include simulation starting and ending points and computation time step for more details on the hec hms model readers are encouraged to refer to the technical reference document and manual of the hec hms the ms module enables the ss module to forecast floods based on the hec hms model which in turn relies on the hec dss database to store and retrieve time series data it is impossible to directly employ the hec hms model and hec dss database in a web environment because the default hec hms model and hec dss database operate in an interactive fashion however any processes invoked directly in a web environment must operate as a silent background process the hec hms software provides an internal jython interpreter that allows users to execute jython scripts from the command line to perform model simulations in silent mode as wsff is implemented in php an invoking sequence of php shell jython is adopted to execute the hec hms model from within the wsff fig 2 shows the typical processes for manipulating dss files or hec hms models if users to want to execute certain models a jython script and shell script are generated dynamically with information that pertains to the path of the model and identifies the desired simulation when these files are generated the exec function is invoked with its argument pointing to the shell file the generated jython scripts in the ms module are used for inputting model driven time series data performing hec hms model simulations and extracting the desired simulated results as jython is an implementation of the python programming language that is designed to run on the java platform it can import and use any java class the instructions in the generated jython scripts are actually supported by some java classes provided by the hec hms including the timeseriescontainers hectimeseries and hecdatamanager classes for storing and retrieving time series data and the jythonhms and project classes for performing model simulation fig 3 shows how instances of these classes collaborate to manipulate model and time series the put and get methods of the instance of the hectimeseries class are used to input and retrieve time series respectively the instance of timeseriescontainer as its name suggests is used to hold time series that are input to or retrieved from a dss file the jythonhms class has a static method openproject that can be used to open a model or project and return the opened model a simulation can be performed by invoking the compute method of the instance of the opened model 2 1 2 knowledge subsystem for flood forecasting based on distributed hydrological models such as the hec hms it is crucial to obtain a proper parameter set that can represent the hydrological characteristics of the underlying system and a specific event the ks module can determine a parameter set for the hec hms model by utilizing predefined domain specific knowledge and or experience of adept experts in this initial effort the ks module incorporates two methods to help users identify the proper parameter set for flood forecasting i e empirical and statistical methods for experienced users the parameter set can be identified based on their experience or via comparison to historical flood events and a parameter set associated with certain events exhibiting similar characteristics can then be selected however it is difficult for users with less experience to find a proper parameter set in this way in contrast statistical methods rely on statistical rules of model parameters many studies related to hec hms model parameter identification such as those by lin et al 2013 and yan et al 2020 have indicated that two important parameters of the hec hms model i e the initial abstraction ratio and flood wave velocity are closely related to the initial discharge and rainfall intensity respectively these statistical relationships can provide a viable and easy measure for users to derive model parameters from measurable variables depending on the type of knowledge empirical or statistical the knowledge pertaining to the key parameters of the hec hms is stored in different ways specifically the empirical knowledge is stored with historical flood models and the statistical knowledge is saved in the backend mysql database as stored procedures fig 4 shows the class diagram of the key components of the ks module the empiricalparameterknowledge service allows users to apply parameters from a historical flood event model to a specific model the statisticalparameterknowledge service however provides a function which accepts quantifiable characteristics of precipitation and runoff time series i e the initial discharge or rainfall intensity that enable users to calculate parameter value because it is easier to update stored procedures than the code the statistical relationships for estimating model parameters are presented as stored procedures in the backend database 2 2 visualization the gui was built by integrating multiple open source libraries such as dhtmlx leaflet and apache echarts in this subsystem the dhtmlx leaflet and apache echarts libraries were used to create spreadsheets maps and charts respectively fig 5 shows a map viewer to visualize watershed maps including a base map watershed boundaries sub basins streams and meteorological and hydrological gauges the base map is obtained from openstreetmap osm a volunteered geographical information service that provides free editable maps originating from all over the world other maps are rendered from geojson files which are converted from shapefiles via the geospatial data abstraction library gdal tool in this study apache echarts was used to visualize meteorological and hydrological time series data apache echarts is a charting and data visualization javascript library that provides a powerful interactive charting and data visualization library for web browsers mobile apps and backend usage fig 6 shows how time series data can be visualized with apache echarts in each chart panel there are buttons and legends allowing users to highlight or toggle a particular hyetograph or hydrograph zoom into a region of interest restore the view to the previous status refresh the graph export the graph as an image file and select meteorological gauges to depict in the chart in addition charts can be automatically and dynamically updated when combined with asynchronous javascript and xml ajax technology this can be very useful for the functionality of automatic flood forecasting whenever new weather forecasting data are sent to the wsff this triggers the simulation process of the hec hms model and dynamic updating of hyetographs and hydrographs 2 3 key functionalities this section describes the key functionalities of the web based prototype system for flood simulation and forecasting including manual flood forecasting automatic flood forecasting and historical flood reproduction fig 7 shows a typical interface of the manual flood forecasting functionality this interface comprises three panels the first panel is a spreadsheet that allows users to input and edit precipitation data similarly to a desktop spreadsheet application each column of the spreadsheet contains precipitation time series data retrieved from a meteorological gauge the first cell of each column contains the name of the meteorological gauge at the bottom of the first panel there are three controls the first control i e the input box is used to input the sheet identifier when the load create button is triggered the system creates an empty sheet if the sheet identifier does not match any existing sheets in the database otherwise the system loads the sheet specified by the identifier from the database once the forecast button is triggered the flood forecasting process is initiated which invokes the underlying processes to enter the time series data in the spreadsheet into the hec dss database perform hec hms model simulations and extract the desired simulation results from the hec dss database the second or top right panel is a chart that shows hyetographs of the selected meteorological gauges the third or bottom right panel visualizes the forecast results and allows users to compare current forecast results to previous alternatives automatic flood forecasting uses a polled operation to acquire third party forecasting data or observed meteorological data and drives the flood forecasting processes automatically once there is an update of the forecasting meteorological source the latest information acquired is presented in the left panel of the automatic flood forecasting interface fig 8 and the flood forecast process is triggered to update the acquired information in the hec dss database perform hec hms model simulations extract the desired simulation results from the hec dss database and finally dynamically update the corresponding hyetographs and hydrographs the wsff includes a model repository of historical flood events fig 9 shows the interface for viewing and reproducing historical flood events in this repository users are allowed to reproduce the model results of these historical flood events and compare their results to the observed hydrographs in addition this repository can be of significant help in assisting users in identifying a proper model parameter set by comparing the hydrological characteristics of historical and forecasting events and applying the selected parameter set to the forecasting model 3 case study in this study we used a dataset of the chuanchang watershed located in southeastern fujian province china to evaluate the functionalities and flood forecasting accuracy of the wsff specifically a hydrological model of the chuanchang watershed and empirical equations considering parameters λ and v were first established based on nine historical flood events next these hydrological models and parameter identification knowledge were incorporated into the wsff finally three flood events were used to verify the accuracy of the wsff in terms of flood forecasting the watershed dataset and test results are briefly introduced in the following subsections 3 1 watershed and dataset description the chuanchang river is a main branch upstream of the jiulongjiang watershed which is located on the southeastern coast of china the drainage area of the chuanchang hydrological station was chosen as the study area 782 km2 fig 10 the elevation of this area ranges from 62 to 1380 m fig 10 the study area is characterized by a subtropical monsoon climate with an average annual temperature of 21 c and an annual precipitation range of 1400 1800 mm more than 70 of the annual rainfall occurs from april september with numerous rainstorms caused by the monsoon climate convective storms and typhoons as it is dominated by the typical monsoon climate of southern china the runoff in this study area is primarily generated through the saturation excess mechanism i e runoff is generated when the soil becomes saturated the chuanchang river is prone to flash floods due to the large vertical fall of the riverbed and severe rainstorms these floods are exacerbated by the rapid development of agriculture and urbanization in this area flash flood warning and forecasting systems are urgently needed to alleviate the impact of environmental changes the datasets used to build the hydrological model included a 30 m resolution digital elevation model dem obtained from the geospatial data cloud fig 10 http www gscloud cn a soil map 1 500000 was provided by the soil fertilizer laboratory of fujian province in china fig 11 a the two largest soil types were red earth 50 45 and paddy soil 16 66 the land use was manually interpreted from landsat thematic mapper images for 1986 fig 11 b the dominant land use is forest 74 93 followed by cropland 13 47 mainly rice and peanut land and grass 10 51 soil and land use data were used to calculate the value of the soil conservation service curve number scs cn hourly rainfall and runoff data were available for 6 rain gauges fig 10 with elevations ranging from 117 to 973 m and the chuanchang flow gauge in the chuanchang watershed covering the 1972 1977 period among the six rain gauges three are located in the head sub basins and the rest are in the central part of the study watershed rainfall streamflow and flood peak flow data were obtained from the hydrology and water resources bureau of fujian province flood events ranking in the top two annual peak flows for the studied period were selected resulting in 12 flood events selected to calibrate and validate the hydrological model instead of selecting them according the severity or return period of the flood events this was because flood discharge data beyond the studied period were not published by the authority and due to the fact that the selected flood events with peak flows ranging from 228 2 to 1193 4 m3 s were more suitable for estimating the knowledge for determining parameters and verifying the performance of the wsff with respect to the different flood magnitudes 3 2 construction of the hec hms model the hec hms model is a semidistributed hydrological model that is widely used in flood event simulations the study area was divided into 6 sub basins based on dem via the geospatial hydrologic modeling extension of hec hms hec geohms to reflect spatial variations in a watershed the average rainfall in every sub basin was calculated using the reciprocal distance squared method runoff volume and loss were calculated using the scs curve number scs cn method the scs unit hydrograph scs uh was used to simulate direct runoff the recession method was used to calculate baseflow and the muskingum method was used to perform the reach routing calculations the key parameters of the model were the initial loss ratio λ flood routing wave velocity v muskingum k channel length l lag time t and soil conservation service curve number cn the values of cn were calculated based on soil and land use data and l and t were calculated based on dem via the hec geohms tool muskingum k is a function of parameters l and v and thus can be calculated based on the parameter values of l and v therefore the key parameters requiring calibration are λ and v 3 3 wsff evaluation setup because the empirical equations for λ and v incorporated into the wsff were derived from nine historical flood events during the 1972 1977 period three flood events that occurred during the same period were used for reasonable evaluation of the accuracy of the wsff in terms of flood forecasting the relative error in peak flow rep relative error in flood volume rev error in peak flow time ept nash sutcliffe efficiency nse and coefficient of determination r2 were used as goodness of fit measures to evaluate the wsff the definitions of these measures are given as follows 1 r e p q s q o q o 100 2 r e v v s v o v o 100 3 e p t t s t o 4 n s e 1 i 1 n q o i q s i 2 i 1 n q o i q o a v e 2 5 r 2 i 1 n q o i q o a v e q s i q s a v e i 1 n q o i q o a v e 2 i 1 n q s i q s a v e 2 2 where qs and qo are the simulated and observed flood peak flows respectively vs and vo are the simulated and observed flood volumes respectively ts and to are the simulated and observed timings respectively of the peak flow and qs i and qo i are the simulated and observed flood flows respectively at time i moreover n is the total time series and qo ave is the average observed flow 3 4 results and discussion 3 4 1 model calibration as previously mentioned 12 historical flood events in the studied watershed were divided into calibration and validation sets the calibration set with nine events was used to establish empirical relationships between the antecedent discharge and initial abstraction λ and the maximum hourly precipitation and flood wave velocity v the remaining three events were used to validate the performance of the wsff when incorporating the established empirical relationships as a knowledge base for flood forecasting to this end the flood events in the first set were manually calibrated to find suitable λ and v values that can realize a reasonable fitting effect between the observed and simulated discharge values table 1 lists the calibration results for these events in this table the r2 nse and rev indices reflect the overall goodness of fit of the calibrated models a reasonable overall fitting effect between the simulated and observed discharge values can be observed with respect to r2 ranging from 0 82 to 0 93 nse ranging from 0 72 to 0 90 and rev remaining within 12 the rep and ret indices reflect the model performance in modeling the flood peak volume and time respectively the reps for these events remained within 25 indicating an acceptable performance in flood peak simulation most rets are zero indicating good synchronization of the simulated and observed peak times fig 12 shows the observed and simulated hydrographs of the nine flood events in general the simulated discharge values fit the observed discharge values very well except for some circumstances where there are multiple flood peaks within a flood event e g events 740707 and 741019 3 4 2 identifying the empirical relationship for parameters λ and v it is believed that some hms parameters e g λ and v should have distinct values from event to event to mimic the characteristics of a specific flood event lin et al 2013 yan et al 2020 therefore it is critical to find a suitable parameter set to apply the hms in flood forecasting previous studies have indicated strong relationships between λ and v although these relationships may differ from watershed to watershed these relationships can largely ease the burdens of a modeler in identifying proper model parameters because the antecedent discharge and precipitation intensity can be easily retrieved from observed time series in light of these studies parameter sets identified during calibration were used to establish empirical equations for λ and v for both relationships five potential empirical equations i e power linear polynomial logarithmic and exponential equations were used to fit the relationships between the antecedent discharge and initial abstraction and the precipitation intensity and flood wave velocity with these equations established the antecedent discharge could be represented by the onset of the hydrograph and the precipitation intensity could be determined by the maximum hourly precipitation the potential empirical equations were further evaluated against the remaining three flood events specifically different parameter sets were derived using these potential empirical equations a total of 25 parameter sets were generated and their performance in the representation of event characteristics was evaluated and compared table 1 and fig 13 the logarithmic and exponential equations were identified as the best options for estimating parameters λ and v respectively based on the nse index fig 14 the selected empirical equations were found to achieve a consistent performance across the validation events for instance the average nse for the validation events is 0 81 which is close to the value of 0 82 for the calibration events in addition the reps and rets indicated a reasonable flood forecasting accuracy with reps within 15 and rets within 1 h finally the selected empirical equations were incorporated into the wsff which can be given as follows 6 λ 15 9 log q a n t 77 9 7 v 0 417 i 0 966 where qant is the antecedent discharge which is the discharge at the initial increase point of river flow in the flood hydrograph lin et al 2013 due to the high uncertainty in selecting this point the average flow lasted for 24 h before the starting point was selected as the antecedent discharge moreover e is the base of the natural logarithm and i is the maximum hourly precipitation mm hour 4 discussion the primary differences between the wsff and other ffss stated previously are 1 the model subsystem i e hec hms model is fully coupled into the whole system 2 event dependent parameters can be dynamically estimated and updated and hence enable event based flood forecasting and 3 a detailed description of the integration of different subsystems and open available source codes enables the reproduction of this study and possible extension to other regions and fields see our discussion later for the first noted difference most current ffss did not directly interact with hec hms they usually act as consumers of the hec hms model i e rendering the model results stored in the hec dss database where the hec hms model saves its simulation results secondly it is widely accepted that some model parameters of the hec hms model are event dependent kamali et al 2013 lin et al 2013 yan et al 2020 however most ffss did not account for the dynamic change in the model parameter the case study results indicate that the wsff can produce an overall satisfactory performance in terms of peak flow total flood volume peak flow timing and overall hydrograph fitting effect and hence demonstrate the potential application of this system to flood simulation and forecasting in addition the wsff has some other advantages over traditional flood simulation and forecasting tools as a web based solution wsff can be operated in a web environment which means that users can access this system from anywhere at any time this feature can be of great importance to foster the participation of stakeholders with different goals and perspectives in decision making processes to achieve feasible and successful determinations and to easily disseminate flood forecasting information and alerts to the public in addition wsff was developed based on the sophisticated and semidistributed hec hms model instead of some empirical methods or lumped models this feature lends this system many advantages that may be hard or impossible for other methods to possess such as investigating the response of runoff to changes in natural factors e g chezgi et al 2020 and human activities and having more potential applications to flood forecasting in ungauged watersheds e g petroselli et al 2020 for example modelers can use watershed models with one or a few gauged hydrological station s to forecast stream discharges in adjacent ungauged hydrological stations moreover developed as an open source project the wsff can be adapted to other regions by reusing the source codes provided by this study and incorporating the regional hec hms model of interest it should be noted that the performance of the wsff is mainly dependent on its forecasting engine and knowledge subsystem i e the hydrological model of the case study watershed and knowledge for identifying the proper parameter set needed for the forecasting model to this end for users who want to apply the wsff in their study watershed it is crucial to develop a hydrological model that can reasonably represent the hydrological characteristics of the watershed of interest and incorporate watershed specific knowledge into the knowledge subsystem to properly identify model parameters this suggests that users must rebuild the empirical equations for λ and v we incorporated in this study because these equations are generally watershed specific as a proof of concept or prototype system the wsff has shown great potential in flood forecasting however more work is required to turn the wsff into a fully fledged ffs for example while the wsff enables users to simultaneously perform most tasks it allows only one forecasting task to be executed at a time in other words simultaneous forecast scenarios must be queued another drawback of the wsff is that the hec hms model and other subsystems interact through a handful of jython script files which means that subsystems of the wsff are tightly coupled and cannot deploy over separated machines this certainly affects the expansion of the wsff in this study we used a pseudo meteorological forecasting data source to demonstrate the ability of the wsff to perform automatic flood forecasting and we are currently developing an interface with a meteorological radar system to acquire real time meteorological forecasting data we hope that the effectiveness of the wsff can be sufficiently assessed with more recent flood events when this interface becomes available as mentioned above there are some issues that need to be addressed to facilitate wsff as a fully fledged operational system for flood simulation and forecasting we are currently working on a plugin for the hec hms model that allows users to interact with the hec hms model through a handful of restful webservices representational state transfer style webservices with this plugin developers can deploy subsystems of the wsff in a loose manner this means that we can serve our hec hms model through microservice instance building with virtual technologies such as docker and kubernetes which lends it great opportunity to achieve simultaneous access and boost the performance of the wsff in this study we employed empirical relationships between model parameters and time series characteristics to calculate event specific parameters for the hec hms model and thus enabled wsff to perform flood simulation and forecasting nevertheless the empirical relationships were established manually which requires a large amount of time and effort therefore another ongoing effort for improving the wsff is to incorporate optimization methods to help modelers establish a knowledge subsystem of the wsff i e methods to identify proper model parameters for the hec hms model for their study regions although in this study we only discuss the use of the hec hms model in the web environment to perform flood simulation and forecasting it is possible to adapt this system for decision support systems dedicated to integrated water resource management water and soil conservation reservoir management climate change effects investigation etc to adapt this system to any of these potential applications minimal effort is needed as most functionalities such as model input editing model execution model results retrieving and or modules such as system configuration user management and file storage are readily available in the wsff the primary actions developers need to take are building guis for mapping decision making inputs to model inputs and rendering results to users it is also possible to wrap up key functionalities such as model input editing model execution and model results retrieval as apis application programming interfaces in this case hec hms can easily bridge existing tools and platforms e g swat cup and swatshare for example when linked with swatshare it can bring swatshare as a collaborative environment for hydrology research and education to the community of the hec hms model 5 summary in this study a web based prototype system wsff for flood simulation and forecasting based on the hec hms model was developed by integrating state of the art methods and libraries e g dhtmlx leaflet apache echarts thinkphp and hec dss the performance of the wsff was evaluated against 12 historical flood events occurring in the chuanchang watershed located in southeastern china the test results suggest that the wsff can yield an overall satisfactory performance in terms of peak flow total flood volume peak flow timing and overall hydrograph fitting effect to our knowledge this is the first open source solution based on the hec hms model as a flood forecasting engine in a web environment together with the wide application community and robustness of the hec hms model we believe that the conceptual framework and software proposed and developed in this study could serve as a useful basis for the hydrological community to further develop the wsff into a fully fledged operational system for flood forecasting code availability the source code of wsff is released by mit under license source codes are available on the github repository https github com djzhang80 wsff author contribution qiaoying lin methodology writing original draft software writing review editing funding acquisition bingqing lin methodology writing original draft software writing review editing dejian zhang conceptualization funding acquisition supervision writing review editing jiefeng wu writing original draft software writing review editing validation declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests dejain zhang reports financial support was provided by natural science foundation of fujian province acknowledgments this work was financially supported by the natural science foundation of fujian province grant numbers 2020j01779 and 2021j011189 
