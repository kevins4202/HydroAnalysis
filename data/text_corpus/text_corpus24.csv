index,text
120,salt marshes and mangrove forests provide critical ecosystem services such as reduced sediment erosion and increased hydrodynamic buffering sediment transport and hydrodynamics can be influenced by specific functional traits of the plants for example flexibility vs rigidity and community traits for example spatial density while there is a growing body of literature on plant trait and hydrodynamic interactions direct comparative studies of sediment transport and scour development in and around intertidal wetland edges are scarce in this study we systematically compared the effects of plant traits on sediment budgets around the lateral edges of intertidal wetlands under controlled hydrodynamic and sedimentary conditions using full scale vegetation mimics with contrasting flexibilities and densities experiments were carried out in a large scale flume using two spatial densities each of flexible and rigid vegetation mimics we measured unconsolidated sedimentary bed level changes in experimental runs using waves only currents only and waves combined with currents both mimic types dampened the energy of the incoming flow highlighting the role of rigid and flexible aquatic vegetation in providing coastal protection the rigid vegetation mimics lateral edge experienced larger velocities more energetic turbulence and local scour around individual stems scour around stems could influence the lateral expansion of the rigid vegetation ecosystem by reducing sediment stability and thus decreasing seedling establishment success the flexible plant mimics produced lower turbulence at their leading edge which resulted in sediment being deposited over a shorter distance into the patch than in the rigid mimics decreased vegetation density caused reduced sediment erosion at the leading edge and less sediment accumulation within the vegetation patches for both the rigid and flexible mimics the hydrodynamic and sedimentary processes identified for both ecosystems are linked to different feedbacks a positive feedback was identified in which vegetation attenuates hydrodynamic energy allowing sediment accumulation within the patch a negative feedback was identified where large velocities caused flow divergence and erosion outside of the vegetation and would therefore compromise its lateral expansion high densities of rigid vegetation enhance this negative feedback lower density flexible vegetation however combined with less energetic hydrodynamic conditions facilitate the expansion of vegetation patches as they cause less flow divergence and therefore less erosion the strong flow divergence observed in the rigid vegetation cases highlight their importance for buffering hydrodynamics but at the cost of increased erosion within the front end of patches and along their lateral edges keywords waves currents sediment transport mangrove forest salt marsh positive and negative feedbacks scale dependant feedbacks 1 introduction coastal intertidal wetlands are highly productive ecosystems that provide multiple ecosystem services such as coastal protection and erosion control barbier 2007 himes cornell et al 2018 mitsch and gosselink 2015 ecosystem engineering species i e species that physically modify the abiotic environment sensu jones et al 1994 growing in coastal wetlands such as mangrove trees and salt marsh grasses form the final semi terrestrial frontier facing the sea fig 1 plant and community traits of the dominant plant species in such ecosystems drive processes such as wave attenuation current speed reduction and sediment trapping moeller et al 2014 quartel et al 2007 temmerman et al 2013 wetlands ability to adapt to sea level rise through aggradation vertical growth and progradation horizontal seaward growth balke et al 2014 wang and temmerman 2013 xi has led to increased interest in their use against coastal flooding erosion control and as a carbon sink barbier 2007 cheong et al 2013 narayan et al 2016 vuik et al 2016 hydrodynamic buffering and sediment transport is controlled by plant and community traits of the key ecosystem engineering species fromard et al 1998 jones et al 1994 pinsky et al 2013 quang bao 2011 shepard et al 2011 plant traits e g flexibility or rigidity and community traits e g stem or root spatial density alter wave energy attenuation flow diversion and turbulence which in turn modifies sediment transport deposition and erosion although salt marshes and mangrove forests occupy the same physical environment the intertidal zone the density and flexibility of their vegetation fundamentally differ for example flexible vegetation could change its frontal area with the intensity of the flow and sway with wave orbital velocities causing scale dependant feedbacks such as hydrodynamic energy attenuation which enhances sediment accretion through or the formation of troughs restricting lateral expansion bouma et al 2007 in contrast rigid vegetation does not deform significantly even in fast currents therefore hydrodynamic forcing waves and currents and the plant and community traits especially flexibility and density can have implications for the geomorphological development of coastal wetlands sediment dynamics around aquatic vegetation can impact the lateral development of these ecosystems the position of a wetland s seaward edge is determined by the ecosystem s tolerance to tidal inundation and hydrodynamic forcing flow reduction causes sediment accumulation in intertidal wetlands can provide their plants with nutrients at the plant scale and be a carbon sink mechanism at the ecosystem scale however high rates of sediment accumulation may bury the plants or increase the elevation of the ecosystem so that it is outside the range suitable for optimum growth bouma et al 2005 sediment trapping within a patch can be facilitated by plant density plant flexibility and patch size bouma et al 2009 however bouma et al 2009 found that there is a density threshold below which erosion around individual stems occurs high sediment erosion can destabilize plants and constrain lateral ecosystem expansion widdows et al 2008 the distribution of plant biomass amongst individual plants plays an important role in this respect bouma at al 2007 in a field experiment found that rigid bamboo sticks had higher erosion within their patches compared to flexible spartina anglica which displayed a dome shaped elevation pattern wider and deeper scour holes have also been observed around rigid cylinders in comparison to flexible plants which by bending reduce the projected area of the plant and thus cause less scour yagci et al 2016 mangrove trees and salt marsh plants occupy different volumes of the water column due to their varying physical structures this difference will impact on their interactions with hydrodynamics and sediment dynamics despite the importance of wetlands little is known about the feedbacks between ecosystem traits hydrodynamic buffering or sediment transport hu et al 2014 morris et al 2002 feedbacks between ecosystem traits and their physical environment the expansion of the ecosystem channel configuration and ecosystem stability fagherazzi et al 2017 liu et al 2020 temmink et al 2020 hydrodynamic effects on wetlands varies with the traits of the ecosystem engineers for instance rigid vegetation which extends through the entire water column will more strongly deflect flow velocity compared to flexible vegetation this stronger deflection can cause a negative feedback such as larger channel erosion which restricts lateral growth bouma et al 2009 flexible wetland plant s ability to reduce wave energy and flow velocity combined with their flexible leaves which trap suspended particulate material can form a positive feedback which increases sediment accumulation potential therefore nutrient availability or ecosystem expansion bouma et al 2009 another critical feedback is scale dependant feedbacks which combines both positive and negative feedbacks and is driven by wave and currents community and plant traits of the vegetation bouma et al 2015 the combination of negative increased erosion and positive feedbacks sediment accumulation are important for organization and distribution of salt marsh and mangrove patches understanding processes at the lateral edge of intertidal wetlands offers improved predictability of edge dynamics and hence the potential future seaward extent of mangrove forests and salt marshes under changing hydrodynamic forcing here we provide a direct comparative study of sediment transport and scour development at the lateral edge of such contrasting habitat types systematic comparisons of flexible vegetation i e marsh plants with rigid vegetation i e mangrove trees can offer new insights into sediment transport coastal management and the evolution of coastal wetland habitats this study aims to contribute to knowledge by systematically comparing the effects of different plant traits on sediment dynamics under controlled hydrodynamic and sedimentary conditions using full scale vegetation mimics of simplified geometry to control plant flexibility and density in a large scale laboratory flume setting a particular focus is placed on scour development and turbulence generation near the ocean facing and lateral edges of continuous patches bordering channels within the vegetation stand fig 1 the study includes waves only currents only and combined wave current scenarios to explore the context dependency of bio physical interactions in the intertidal zone the plants traits will influence the intensity and scale of their turbulent interactions with their physical environment to further understand the ability of wetland ecosystems to buffer hydrodynamics and their lateral expansion information is required on wetland feedback processes this knowledge will allow us to predict the response of coastal wetlands to changing hydrodynamics and therefore use them further in more climate resilient coastal defences 2 material and methods the experiments were conducted in a customized wave basin at the danish hydraulic institute dhi in hørsholm denmark the basin was 25 m wide and 35 m long had a 5 5 m wide piston type paddle for wave generation and a recirculation system for current generation based on the facility s capabilities along with the desired hydrodynamic conditions of an effective water depth h of 0 30 m and a unidirectional flow speed u of 0 30 ms 1 the test zone was limited to a width of 5 5 m by two side walls the area was divided into two 2 75 m wide channels with one of the ecosystem mimics placed in each one each of these channels was divided longitudinally into a section 1 25 m wide in which vegetation mimics were placed and an unvegetated section 1 50 m wide fig 2 this setup mimics the geometry of a channel in the vegetation stand given the confinement of the flume only half a channel side was mimicked while the other side was bordered by a vertical wall this setup allowed clear identification of the effects of the lateral and longitudinal edge of one mimic patch while free flow was able to develop in the open channel as would be the case in the center of natural channels at the end of the test zone a dissipation beach was installed consisting of several perforated parabolic steel plates for the analysis of the results a coordinate system was adopted in which x denotes the along channel direction with x 0 at the upstream edge of the vegetation patches and y denotes the across channel direction with y 0 at the wall next to the flexible mimic section the boundaries between the vegetated and unvegetated sections aligned in the x direction are referred to as lateral edges fig 2 purple lines the edges of the vegetation sections aligned in the y direction and located at x 0 are referred to as leading edges fig 2 yellow lines a schematic representation of the experimental set up is given in fig 2 2 1 hydrodynamic conditions and sediment characteristics within the constraints of the facility s capabilities the hydrodynamic conditions were defined to represent realistic inter tidal flow conditions under mild conditions brinkman 2007 garzon et al 2019 a water depth of 0 30 m was set and regular waves with height h 0 08 m and periods t ranging from 0 8 to 1 4 s were studied for the experimental runs using unidirectional currents the mean flow speed was set at 0 30 ms 1 table 1 presents the hydrodynamic conditions assigned for each run the sediment was selected based on these hydrodynamic conditions a 0 18 mm nominal diameter sand was chosen for the experiment to achieve moderate sediment transport under the test conditions the maximum scour around the mimics was estimated based on eadie and herbich 1986 and to prevent the scour from reaching the concrete bottom of the basin the whole setup was placed in a 20 cm pit that was filled with sediment fig 2 2 2 vegetation mimics given the chosen hydrodynamic conditions it was estimated that a 10 m length of vegetation mimic patch would be sufficient for a fully developed flow to develop within it based on zhang et al 2015 and maza et al 2017 to exclude effects caused by traits other than the ones under investigation flexibility and density a simple plant mimic geometry was chosen it consisted of circular cylinders either rigid or flexible with uniform vertical cross sections while the mimics do not resemble any specific plant species individual i e diameter length as well as community i e density traits were chosen similar to salt marsh vegetation for the flexible mimics and mangrove aerial roots for the rigid mimics respectively table 2 the flexible mimics were defined based on the properties of salt marsh vegetation e g salicornia sp reported in the literature to simulate basic salt marsh morphology transparent pvc tubes of 6 mm in diameter and 50 cm in length with a young s modulus equal to 2 9 gpa were selected based on reports such as zhu et al 2020 two densities equal to 420 and 210 stems m2 are selected for the experiments which fall within the range of natural observations e g knutson et al 1982 rigid mimics were defined based on mangrove prop roots properties mangrove roots were idealized using uniform cylinders of 3 cm diameter and long enough to be emergent under all tested conditions based on data reported in the literature e g ohira et al 2013 as a point of comparison it is considered that the frontal area in both species will remain constant then a density equal to 84 and 42 mimics m2 is considered in the rigid canopy table 2 shoot densities were halved for additional test runs to cover densities relevant for stages of patch development this resulted in a higher total submerged volume for the rigid vegetation patch compared to the flexible vegetation patch because of the former s greater volume to frontal area ratio for current and wave only runs only the higher vegetation densities were tested for the combined wave and current runs both the higher and lower vegetation densities were tested 2 3 measurements and post processing hydrodynamic measurements were begun 15 min after the flow and or waves were turned on to avoid transient start up effects velocities were recorded with 6 acoustic doppler velocimeters advs and free surface height with 24 resistive free surface gauges all the sensors were synchronized and measured at 25 hz for 90 s at each location the incident hydrodynamic conditions were measured 0 75 m in front of the patch s leading edges velocity data were also obtained inside the vegetated sections at 0 5 m and 2 5 m downstream of the patches edges the free surface height was measured at approximately the same positions as well as at 5 m downstream of the leading edge within the vegetation at each of these along channel positions measurements were taken at three across channel locations in the center of the vegetation patches at their lateral edges and in the unvegetated channel fig 2 in the vertical direction velocities were measured every 2 cm for the closest position to the edge and every 5 cm for the other two longitudinal positions starting at 2 and 5 cm from the initial bed level respectively the measured velocity components were processed by applying four filtering strategies firstly a filter based on signal quality was applied in which measurements with correlation values less than 50 were discarded secondly a cut off filter was applied to remove spikes in the velocity record over a predetermined threshold 0 55 ms 1 for waves 0 7 ms 1 for currents and 1 2 ms 1 for waves and currents these threshold values were determined by observing the time series thirdly an acceleration filter was used to suppress oscillations with accelerations larger than gravitational acceleration finally a standard deviation filter was applied removing velocity values which were more than three standard deviations from the mean velocity the filtered values were left empty and not considered in further calculations the mean velocity values were obtained by performing time averaging for the current only runs and phase averaging for the waves only and waves and current runs the time averages were applied over the whole of each recording the phase averages were calculated by dividing the time series in intervals of the length of the wave period and then performing an ensemble averaging over them 70 wave periods were used to obtain each of the phase averages values of the turbulent kinetic energy tke were obtained from the variation of the velocity signal around the mean or phase averaged velocity for the current only cases and the cases with waves respectively for wave only runs ww wave height evolution through the two high density patches was analysed by measuring the wave heights h along their centerlines y 0 625 m and normalizing them by the incident value hi obtained from the wave gage located offshore of the patch to give a relative wave height h hi the submerged solid volume fraction φ tanino and nepf 2008 was calculated as 1 ϕ s u b m e r g e d v e g e t a t i o n v o l u m e w a t e r v o l u m e the length scale of vortex penetration lv zong and nepf 2011 was calculated as being inversely proportional to the frontal area per volume of the patch 2 l v 0 5 c a 1 where a is equal to d n d being the stem diameter and n the number of stems per unit area the scale dependent feedback bouma et al 2015 was derived from the mean velocities it is representative of the flow due to the presence of the vegetation patches and can be obtained as the difference between the mean velocities in the channel uchannel and inside the vegetation patches uveg divided by the incident velocity ui 3 s c a l e d e p e n d e n t f e e d b a c k u c h a n n e l u i u v e g u i to measure bed elevation changes after each test 60 measurements were taken across a transect at each of 10 longitudinal positions along the experimental area shown in fig 2 the measurements were conducted using two sedimentation erosion bars seb with 30 measurement points on each of them at each point along the bar a vertical pin was positioned that could be dropped onto the sediment surface allowing its height and thus the height of the sediment at that location to be measured against the fixed height of the horizontal seb the distance between the measurement points was 5 cm these measurements were made after each of the test runs which lasted for approximately 1 5 h when the sediment bed had acquired a stable configuration after each experiment the sediment was re leveled by moving it from the accumulation to the erosion zones to restore the initial flat configuration to acquire a better representation of the bed level the data were smoothed using a centered 3 point moving average in the transversal direction 3 results 3 1 uni directional flow current velocity patterns and sediment elevation were analysed for both types of high density simulated vegetation table 2 the flexible vegetation slows the incoming flow reducing its speed by almost 50 by x 2 5 m inside the patch fig 3 a top panel the decrease in flow speed leads to sediment deposition within the patch starting at around x 1 0 m simultaneously in the lateral channel the along channel speed increases by more than 30 at x 2 5 m with respect to the incoming value fig 3 a bottom panel leading to sediment erosion fig 3 b shows that the largest lateral y component speeds are measured at the patch lateral edge at x 0 5 m these lateral speeds decrease further into the patch at x 2 5 m this decrease is the result of the flow divergence region at the leading edge of the patch where the flow in line with the patch decelerates and a large part of the flow is diverted towards the open channel the flow divergence produces sediment erosion at the leading edge where velocities are not yet significantly attenuated top and central panel in fig 3 a the divergence flow extends over a distance ld defined as the length necessary for the velocities to be less than the incident values from the velocity measurements the ld should be between x 0 5 and 2 5 m taking the position of maximum sediment deposition within the patch as the location where velocities have been significantly attenuated to allow sediment accumulation ld 1 5 m is estimated for the flexible vegetation which falls well within the range implied by the velocity measurements in the rigid vegetation patch fig 4 a shows that the along channel speed decreases by almost 70 by x 2 5 m while it increases by almost 60 within the open channel at the same x position compared to the incoming value thus the rigid vegetation produces stronger flow divergence than that observed in the flexible vegetation maximum lateral speeds are observed at the patch lateral edge at x 0 5 m fig 4 b these strong lateral speeds implies that rigid vegetation causes a major deviation of the flow around the vegetation patch this process is also observed when analysing the free surface gradient h x produced by both patches fig s1 h x for rigid vegetation is 0 0063 0 0008 while for flexible vegetation it is 0 0046 0 0001 the largest free surface gradient obtained for rigid vegetation indicates a larger drag force exerted by the rigid elements on the flow in comparison to the flexible ones which reconfigure under the flow action as shown in fig s2 consequently a larger drag length scale is found for the rigid canopy which also presents a greater submerged volume fraction φr 0 060 in comparison to the flexible canopy φf 0 008 leading to a larger flow energy attenuation mazda et al 1997 maza et al 2019 the conceptual sketch fig 5 for both vegetation patches shows the main differences in flow divergence and sediment movement stronger flow divergence by the rigid vegetation panel b produces a longer diverging flow region at the leading edge ld and greater erosion in the channel due to the larger velocities observed for both patches at the leading edge after the diverging flow region there is a fully developed region within the rigid vegetation where the velocity is uniform across the patch width and length a shear layer forms at the interface between the patch and the lateral channel where shear layer vortices develop since both patches present the same frontal area per volume the length scale of vortex penetration lv is expected to be similar for both cases being slightly bigger for the rigid vegetation due to its larger cd value as discussed above taking cd 1 gives a value for lv of approximately 0 20 m tanino and nepf 2008 s formula for turbulence generated by element wakes considered a balance between the production of tke and its viscous dissipation the formula shows that larger tke values are obtained in cases with greater in canopy velocities at x 2 5 m tke in the flexible vegetation has a mean value of 0 001 m2 s2 while for rigid vegetation has a mean tke of 0 006 m2 s2 whilst φ is equal to 0 012 and 0 060 for flexible and rigid vegetation respectively whereas the depth averaged velocity at x 2 5 m is equal to 0 155 m s 0 0016 for flexible and 0 097 m s 4e 06 for rigid vegetation values in brackets show the standard deviation thus the greatest tke produced by element wakes is found in the rigid vegetation fig 4 a top panel despite the smaller in canopy velocity following the formular from tanino and nepf 2008 this indicates larger tke for the rigid vegetation than for the flexible one confirming the greater influence of φ versus the velocity reduction in terms of tke produced by element wakes turbulence can also be generated within shear regions such as at patches lateral edges this process is observed in the flexible vegetation case where the largest tke values are recorded at the lateral edge of the patch fig 3 a at the end of the diverging flow region in the rigid vegetation sediment deposition increases in the along channel direction as the local velocity decreases the opposite occurs in the fully developed flow region where deposition decreases longitudinally through the patch as the sediment concentration in the water column is depleted in this region sediment elevation is almost uniform across the patch width except within the shear layer region turbulent transport is enhanced by shear layer vortices which transport resuspended sediment along the lateral edge of the patch this transportation results in an accumulation of sediment at the downstream side edge of the flexible vegetation patch and causes some of the sediment that has been suspended in the diverging flow region of the rigid vegetation to be deposited at the lateral edge fig 5 3 2 wave only runs wave height evolution was measured in the high density vegetation patches and is shown in fig 6 combined with the sediment elevation measured along the centerlines free surface measurements at x 5 m were not recorded for the rigid vegetation run however the results show that the flexible vegetation did not reduce the wave height as much as the rigid vegetation which reduced the wave height up to 23 more than the flexible vegetation at x 2 m fig 6 this difference is also shown by greater velocities within the patch at x 2 5 m for flexible vegetation compared to rigid vegetation figs s3 s4 and s5 there is similar wave height attenuation for the three tested wave conditions with a small decrease in the amount of attenuation with increasing wave period fig 6 the mean values and standard deviations in brackets of the relative wave heights at x 2 m are 0 77 m 0 06 m and 0 67 m 0 02 m for the flexible and rigid vegetation respectively and 0 62 m 0 07 m for the flexible vegetation at x 5 m thus it is observed that the flexible vegetation needs more than twice the distance from the leading edge than the rigid vegetation to obtain similar wave height attenuation although it should be noted that the largest rate is observed for the first 2 m of vegetation in both cases this is further confirmed by the velocities recorded in both patches which are reduced by more than 30 after 2 5 m in both cases figs s3 s4 and s5 the greatest tke values are observed for the rigid vegetation but tke is reduced by up to 40 from 0 5 to 2 5 m in all wave cases in both patches at the leading edge where velocities have not yet been attenuated sediment is transported into the patch the decrease in velocity and tke leads to sediment deposition downstream of the leading edge in both vegetation mimics at 1 5 2 5 m bottom panels in fig 6 sediment deposition and consequently sediment accumulation occurs over a shorter distance in the flexible vegetation patch at 1 5 m compared to the rigid vegetation patch at 2 5 m fig 6 within the patch the largest accumulation for flexible vegetation occurs at 1 5 m while for rigid vegetation it is observed at 2 5 m fig 6 sediment erosion at the leading edge is milder for the flexible vegetation which may be due to the greater turbulence produced around the rigid elements which resuspends a larger amount of sediment compared to the flexible vegetation case fig 6 3 3 waves and currents the interactions of waves and currents with both types of vegetation at both densities high and low is presented in this section for low densities the flow is diverted less compared to high densities so it penetrates further into the patch with fewer stems and greater velocities are recorded at x 0 5 2 5 m however high densities reduce velocity within the patch more strongly than low densities for flexible vegetation the velocity decreases from 20 to 24 low density cases and 20 to 40 high density cases with the smallest reduction for the longest wave period and the largest for the shortest figs 6 s6 a and s7 a in the case of rigid vegetation the decrease in velocity between x 0 5 and 2 5 m is smaller from 2 to 22 with the smaller values observed for low density cases the velocities at x 0 5 m are already lower than those recorded for the flexible vegetation due to the greater divergence of the flow at the leading edge the velocities for the low density cases are greater than those for the high density ones fig 7 longer wave periods lead to greater tke values within the patch for both vegetation types and densities for the flexible vegetation tke does not decrease in the first 2 5 m for the low density cases especially for longer wave periods where tke increases from x 0 5 to 2 5 m by up to 22 fig 7 at high density cases tke decreases from x 0 5 to x 2 5 m for all three waves and current conditions figs 7 s6 a and s7 a tke values recorded for rigid vegetation at both densities are greater than values recorded for flexible vegetation for rigid vegetation at x 0 5 m within the canopy tke values found for high density fig 7 b are larger than the values for low density especially near the bottom i e from the bottom to about half the water depth this larger turbulence near to the bottom for the high density cases increases sediment resuspension at the leading edge which leads to greater levels of erosion figs 7 and 8 at x 2 5 m tke decreases for the high density cases and sediment deposition is observed however for the low density cases the tke does not significantly decrease within the patch from x 0 5 to 2 5 m figs 7 b s5 b and s6 b leading to increased sediment transport resulting in a sediment deposition zone further into the patch around x 5 m thus a change in plant density leads to a change in the longitudinal deposition of sediment within the rigid vegetation patch with sediment deposition at a greater distance from the leading edge for lower densities therefore for rigid vegetation both turbulence intensity and flow velocities determine sediment transport in and around the patch it is also shown that a decrease in vegetation density leads to less erosion along the channel due to the gentler divergence of the flow and to a larger area within the patch from the leading edge where sediment is eroded 3 4 scale dependent feedback following bouma et al 2015 the scale dependent feedback is analysed in this section by using flow deceleration within the patch as a proxy for the positive feedback and flow acceleration around the patch as proxy for the negative feedback discussed in the next section in detail the scale dependent feedback is evaluated for the current only and waves and currents cases at two longitudinal positions x 0 5 and 2 5 m fig 9 shows the scale dependent feedback strengths for both positions rigid and flexible vegetation and the different hydrodynamic conditions fig 9 shows that the scale dependent feedback increases when moving towards the patch from the leading edge in agreement with the velocity decrease produced by the canopy fig 9 b x 2 5 m shows larger values than fig 9 a x 0 5 m for all hydrodynamic and vegetation conditions especially for cc at least for flexible vegetation since no measurements at x 0 5 m are available for the rigid one at the leading edge fig 9 a and for high density cases the scale dependent feedback is almost constant for both vegetation types being equal to 0 24 0 04 for flexible vegetation and equal to 0 39 0 02 for rigid vegetation this difference is also observed for the low density cases where the mean value for the flexible vegetation is equal to 0 10 0 06 while it is 0 22 0 05 for the rigid vegetation for flexible vegetation a decrease in the scale dependent feedback is observed with increasing wave periods especially for low density cases this decrease may be linked to the ability of flexible vegetation to move and reconfigure under the flow action finally it is observed that the scale dependent feedbacks for low density cases are 42 and 57 smaller than those for high density cases for flexible and rigid vegetation respectively thus the strong flow divergence produced by high density cases with 0 012 and 0 060 for flexible and rigid vegetation is significantly reduced by half at x 2 5 m fig 9 b for low density cases the scale dependent feedback is greater than the value for high density at x 0 5 m confirming that for low density cases a longer distance inside the patch is needed to achieve the same velocity attenuation in contrast with what is observed at x 0 5 m for wave and currents conditions and high density cases similar scale dependent feedbacks are found for rigid and flexible vegetation being equal to 0 55 0 05 and 0 53 0 06 respectively for low densities with smaller φ a significant difference in the value of the scale dependent feedback is still observed between rigid and flexible vegetation at x 2 5 m with values equal to 0 43 0 03 and 0 32 0 05 respectively thus it is found that dense flexible canopies may be very effective at attenuating flow velocity over short distances but once the density of the canopy is significantly reduced by half in this case the attenuation of the meadow is more limited however for rigid vegetation the same density change i e a φ change does not result in such a large loss of attenuation capacity of the canopy 4 discussion ecosystem engineers traits affect hydrodynamics and sediment dynamics our results show that flexible and rigid vegetation reduced flow speed by 50 and 70 respectively for unidirectional flow figs 3 and 4 within this unidirectional flow turbulent kinetic energy was generally lower in the flexible vegetation 0 001 m2s 2 compared to the rigid vegetation 0 006 m2s 2 which corresponds to greater erosion figs 3 and 4 under wave only conditions the flexible vegetation attenuated waves slower initially than rigid vegetation the flexible vegetation required more than twice the distance from the leading edge than the rigid vegetation to obtain similar attenuation rates fig 6 however by 2 5 m both vegetation types reduced turbulent kinetic energy by 40 under only wave conditions which led to sediment deposition after the leading edge for both vegetation mimics at 1 5 2 5 m fig 6 for flexible low density vegetation under combined waves and currents the flow speed reduction within the patch ranged from 20 to 24 whilst for high density it was 20 40 the change in flow speed corresponded to larger sediment accumulation in high density flexible plants which occurred closer to the leading edge fig 7 in the case of the rigid vegetation the decrease of flow speed within the patch ranged from 2 to 22 with the smaller values observed for low density vegetation combining waves and currents resulted in increased channel speeds when the current first encountered the vegetation across all conditions for both patches at high density tke values recorded for flexible vegetation at both densities were lower than those recorded for rigid vegetation as shown for current only tests larger tke values indicated greater sediment erosion at the front of the rigid vegetation fig 8 the biggest increase in scale dependent feedback was found between 0 5 and 2 5 m in flexible vegetation marsh patches under all wave and current conditions however generally scale dependent feedback was greater in the rigid vegetation especially at lower densities 4 1 effects of flexible vegetation in terms of the ability to reduce erosion and accumulate sediment flexible vegetation clearly has advantages in flexible vegetation the formation of a sharp precipice does not occur due to the strong erosion at the front of the patch which forms in the rigid vegetation figs 3 4 and 7 previous research has found most of the suspended particles transported within flexible canopies collide with the moving flexible stems leading to particle capture by loss of momentum ganthy et al 2015 the current study confirms this assessment for example under wave only conditions where the flexible vegetation caused no erosion only accumulation in contrast to the rigid vegetation fig 6 as noted in other studies king et al 2012 nepf and vivoni 2000 turbulence was generated within the shear regions along the lateral edge of the flexible vegetation patch 4 2 effects of rigid vegetation the rigid vegetation mimics had a greater effect in attenuating hydrodynamic energy compared to flexible vegetation for two reasons firstly the rigid vegetation does not bend and therefore has a greater flow resistance bouma et al 2005 2015 secondly there is larger drag caused by the rigid vegetation due to its rigidity and submerged solid volume fraction maza et al 2015 2019 paul et al 2012 as shown by the larger free surface gradient in the rigid vegetation under currents only in agreement with bouma et al 2009 tanino and nepf 2008 the rigid vegetation s ability to quickly reduce momentum within the patch under all hydrodynamic conditions reduces its ability to accumulate sediment this is because the reduction increases turbulence which these results suggest is the dominant hydrodynamic process for erosion in rigid vegetation tinoco and coco 2018 under unidirectional flow and waves combined with currents tke was generated around the individual vegetation mimics which was stronger for rigid vegetation compared to flexible vegetation tanino and nepf 2008 this can be linked to rigid mimic s traits such as rigidity and diameter when exposed to strong hydrodynamic conditions they generate intense near bed turbulence which causes local scouring around the base of the roots norris et al 2021 bouma et al 2009 found that for rigid vegetation this process was mainly due to basal diameter which determines the degree of scouring therefore the larger diameter of rigid mimics leads to greater scouring which is linked to the overall larger erosion produced within the rigid vegetation additionally near bed coherent structures generated by rigid stem bed flow interactions such as horseshoe vortices can significantly alter the near bed stresses locally in front of the rigid elements resulting in additional sediment resuspension schanderl et al 2017a 2017b previous research has established there is a relatively sheltered interior in rigid vegetation i e mangroves folkard 2019 norris 2019 where deposition occurs this study also located a sheltered interior where the lowest turbulence resulted in the greatest deposition of sediment within rigid patches deposition generally occurred much further ca 2 m into the rigid vegetation patch compared to the flexible vegetation patch ca 1 m the larger flow speed inside the lower density patches of rigid vegetation resulted in sediment being transported further into the patch this is facilitated by the decrease in tke in the first few meters of the patch for high density but not for low density similar to the flexible vegetation cases fig 7 b this was shown by the sediment accumulation patterns where accumulation was produced closer to the leading edge in the higher density patches fig 8 4 3 bio physical feedbacks in flexible and rigid vegetation three possible feedback processes have been identified in this study a positive feedback a negative feedback and the scale dependent feedback the positive feedback occurs where mimic vegetation attenuates flow energy to allow for sediment accumulation bouma et al 2009 2005 gourgue et al 2021 from this we deduce that flexible vegetation will have stronger positive feedback compared to rigid vegetation as flexible vegetation will accumulate more sediment in their patch compared to rigid vegetation under all the hydrodynamic conditions the negative feedback occurs where mimic vegetation deflects flow causing greater channel erosion in situ this could reduce the vegetation s lateral expansion but is also a key element of channel formation and hence wetland drainage temmerman et al 2007 zong and nepf 2011 the rigid vegetation cases had greater channel erosion under unidirectional flow due to stronger flow diversion compared to the flexible vegetation case when considering combined waves and currents this study shows that rigid vegetation at both densities studied was also more likely to develop a negative feedback of channel formation as diversion of the current was greater the stronger flow divergence relates to the fact that although both patches present the same frontal area rigid emergent elements produce greater flow resistance than flexible nearly emergent ones as reported previously bouma et al 2015 2005 previous studies have also found that higher densities cause greater flow deflection which results in more pronounced channel erosion whilst lower vegetation density allows the flow to dissipate within the vegetation due to there being more space around the vegetation roots or stems therefore less channel erosion gourgue et al 2021 the front of the lower density flexible vegetation had less channel erosion compared to the higher density case in agreement with previous studies where flow passed through flexible vegetation and was not deflected as strongly bouma et al 2015 2009 this deflection indicated a weaker manifestation of negative feedback i e channel erosion as decreasing density reduces deflection by flexible vegetation at this point fig 8 additionally along the leading edge of the high density flexible vegetation patch erosion from deflection is reduced with decreasing flow speeds the depth of erosion in the channel was dependent on the density of the plants and the hydrodynamic regime folkard 2019 the absence of these negative feedbacks in low flow environments may facilitate formation of flexible patches without erosion troughs and could facilitate expansion of such vegetation as is commonly observed in calmer hydrodynamic areas bouma et al 2009 conversely at larger velocities and higher densities increased channel depth could in situ reduce the plants success in expanding laterally into the channel the scale dependent feedback concerns the ratio of the flow speed within the vegetation to that in the channel changes in ratios between the rigid and flexible patch indicates flexible dense vegetation was effective in attenuating flow speed but once patch density was significantly reduced the attenuation capacity of the patch was limited however for rigid vegetation the same density change does not result in such a large loss of attenuation capacity of the meadow the values obtained at x 2 5 m in the currents only cases agree with values reported by bouma et al 2015 despite the differences between the tested vegetation real vegetation in the case of bouma et al 2015 while mimics are used here the value for rigid vegetation 1 23 is close to that obtained for spartina anglica 1 20 fig 5 in bouma et al 2015 a rather rigid species while the value obtained for flexible vegetation 1 03 is close to that obtained for salicornia procumbens 0 90 fig 5 in bouma et al 2015 a more flexible species additionally this study found that multi scale feedbacks are also associated with community and plant traits such as density of vegetation and rigidity flexibility of the plant recent studies have also shown the importance of traits temmink et al 2020 for plant establishment however there are a variety of traits associated with salt marsh and mangrove plants and their ecosystems and further research should concentrate on identifying potential abiotic boundary conditions determining hydrodynamic buffering and sediment transport capacity sediment accumulation is crucial for salt marsh and mangrove survival this research increases our understanding of bio physical limitations to lateral expansion of these ecosystems however for these ecosystems to maintain their role as a coastal defense in the context of climate change induced sea level rise we require further information i e community and plant traits to allow prediction of expansion with changing external forcing this information will become more important for designing ecosystem based coastal protection measures that are climate resilient 4 4 application to natural environments our results present insights into the effects of vegetation flexibility and density on sediment dynamics however their transfer to natural conditions may be partially limited by the simplifications introduced while constructing the vegetation mimics while the rigid mimics model mangrove prop roots reasonably well with respect to individual diameter length and community density traits as well as rigidity the flexible mimics neglect the effect of leaves on vegetation drag previous work has highlighted the importance of leaves and their reconfiguration under hydrodynamic loading for vegetation drag san juan et al 2019 whittaker et al 2015 schoneboom and aberle 2009 and instantaneous flow fields tinoco et al 2020 however resulting turbulent flow is most prominent in the upper part of the canopy while the region close to the bed is less affected tinoco et al 2020 especially for flexible coastal wetland vegetation e g salt marshes this pattern coincides with standing biomass distribution close to the bed most salt marsh plants exhibit individual stems growing from the roots with branches and leaves only occurring higher up along the plant as sediment dynamics will be governed by the local flow near the bed rather than fluctuations at the top of the canopy leaves play a minor role in this context they do however complicate calculations of the frontal area due to their constant changes in position such as streamlining under approaching flow aberle and järvelä 2015 we thus decided on a flexible vegetation model which models vegetation shape close to the bed and at the same time allows for a constant frontal area per canopy volume for comparison with the stiff mimics consequently our results based on mimics provide insights into the general sediment dynamics at leading and lateral edges of coastal wetland with flexible salt marsh and rigid mangrove vegetation while the application of results on the flow field higher up in the water column may be limited 5 conclusions this paper highlights the differences observed between two mimic ecosystem engineers mangrove trees and salt marsh plants with different characteristics which are subjected to the same hydrodynamic conditions the hydrodynamic and sedimentation processes identified for both ecosystems are linked to different feedbacks a positive feedback was identified in which vegetation attenuates hydrodynamic energy allowing sediment accumulation within the patch and a negative feedback associated with the high velocities produced from flow divergence causing channel erosion greater channel erosion could compromise the lateral expansion of the vegetation high rigid vegetation densities enhance this negative feedback lower flexible vegetation densities combined with calmer hydrodynamic conditions could facilitate the expansion of flexible patches as these patches have less flow divergence and therefore less channel erosion the strong flow divergence from rigid vegetation highlights their importance for buffering hydrodynamics but at the cost of potentially increased erosion within the frontal patch and lateral edges these findings illustrate the spatial dimensions of the ecosystem engineering outcome of two contrasting intertidal wetland species declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study h dhi 10 hywedges received funding from european union hydralab the authors would like to thank dhi for the use of their total environmental stimulator flume and the support staff m maza is sincerely grateful to the spanish ministry of science and innovation for the funding provided in the grant rti2018 097014 b i00 of proyectos de i d i retos investigación 2018 funded by mcin aei 10 13039 501100011033 and by erdf a way of making europe supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2022 104257 appendix supplementary materials image video 1 image application 2 image video 3 
120,salt marshes and mangrove forests provide critical ecosystem services such as reduced sediment erosion and increased hydrodynamic buffering sediment transport and hydrodynamics can be influenced by specific functional traits of the plants for example flexibility vs rigidity and community traits for example spatial density while there is a growing body of literature on plant trait and hydrodynamic interactions direct comparative studies of sediment transport and scour development in and around intertidal wetland edges are scarce in this study we systematically compared the effects of plant traits on sediment budgets around the lateral edges of intertidal wetlands under controlled hydrodynamic and sedimentary conditions using full scale vegetation mimics with contrasting flexibilities and densities experiments were carried out in a large scale flume using two spatial densities each of flexible and rigid vegetation mimics we measured unconsolidated sedimentary bed level changes in experimental runs using waves only currents only and waves combined with currents both mimic types dampened the energy of the incoming flow highlighting the role of rigid and flexible aquatic vegetation in providing coastal protection the rigid vegetation mimics lateral edge experienced larger velocities more energetic turbulence and local scour around individual stems scour around stems could influence the lateral expansion of the rigid vegetation ecosystem by reducing sediment stability and thus decreasing seedling establishment success the flexible plant mimics produced lower turbulence at their leading edge which resulted in sediment being deposited over a shorter distance into the patch than in the rigid mimics decreased vegetation density caused reduced sediment erosion at the leading edge and less sediment accumulation within the vegetation patches for both the rigid and flexible mimics the hydrodynamic and sedimentary processes identified for both ecosystems are linked to different feedbacks a positive feedback was identified in which vegetation attenuates hydrodynamic energy allowing sediment accumulation within the patch a negative feedback was identified where large velocities caused flow divergence and erosion outside of the vegetation and would therefore compromise its lateral expansion high densities of rigid vegetation enhance this negative feedback lower density flexible vegetation however combined with less energetic hydrodynamic conditions facilitate the expansion of vegetation patches as they cause less flow divergence and therefore less erosion the strong flow divergence observed in the rigid vegetation cases highlight their importance for buffering hydrodynamics but at the cost of increased erosion within the front end of patches and along their lateral edges keywords waves currents sediment transport mangrove forest salt marsh positive and negative feedbacks scale dependant feedbacks 1 introduction coastal intertidal wetlands are highly productive ecosystems that provide multiple ecosystem services such as coastal protection and erosion control barbier 2007 himes cornell et al 2018 mitsch and gosselink 2015 ecosystem engineering species i e species that physically modify the abiotic environment sensu jones et al 1994 growing in coastal wetlands such as mangrove trees and salt marsh grasses form the final semi terrestrial frontier facing the sea fig 1 plant and community traits of the dominant plant species in such ecosystems drive processes such as wave attenuation current speed reduction and sediment trapping moeller et al 2014 quartel et al 2007 temmerman et al 2013 wetlands ability to adapt to sea level rise through aggradation vertical growth and progradation horizontal seaward growth balke et al 2014 wang and temmerman 2013 xi has led to increased interest in their use against coastal flooding erosion control and as a carbon sink barbier 2007 cheong et al 2013 narayan et al 2016 vuik et al 2016 hydrodynamic buffering and sediment transport is controlled by plant and community traits of the key ecosystem engineering species fromard et al 1998 jones et al 1994 pinsky et al 2013 quang bao 2011 shepard et al 2011 plant traits e g flexibility or rigidity and community traits e g stem or root spatial density alter wave energy attenuation flow diversion and turbulence which in turn modifies sediment transport deposition and erosion although salt marshes and mangrove forests occupy the same physical environment the intertidal zone the density and flexibility of their vegetation fundamentally differ for example flexible vegetation could change its frontal area with the intensity of the flow and sway with wave orbital velocities causing scale dependant feedbacks such as hydrodynamic energy attenuation which enhances sediment accretion through or the formation of troughs restricting lateral expansion bouma et al 2007 in contrast rigid vegetation does not deform significantly even in fast currents therefore hydrodynamic forcing waves and currents and the plant and community traits especially flexibility and density can have implications for the geomorphological development of coastal wetlands sediment dynamics around aquatic vegetation can impact the lateral development of these ecosystems the position of a wetland s seaward edge is determined by the ecosystem s tolerance to tidal inundation and hydrodynamic forcing flow reduction causes sediment accumulation in intertidal wetlands can provide their plants with nutrients at the plant scale and be a carbon sink mechanism at the ecosystem scale however high rates of sediment accumulation may bury the plants or increase the elevation of the ecosystem so that it is outside the range suitable for optimum growth bouma et al 2005 sediment trapping within a patch can be facilitated by plant density plant flexibility and patch size bouma et al 2009 however bouma et al 2009 found that there is a density threshold below which erosion around individual stems occurs high sediment erosion can destabilize plants and constrain lateral ecosystem expansion widdows et al 2008 the distribution of plant biomass amongst individual plants plays an important role in this respect bouma at al 2007 in a field experiment found that rigid bamboo sticks had higher erosion within their patches compared to flexible spartina anglica which displayed a dome shaped elevation pattern wider and deeper scour holes have also been observed around rigid cylinders in comparison to flexible plants which by bending reduce the projected area of the plant and thus cause less scour yagci et al 2016 mangrove trees and salt marsh plants occupy different volumes of the water column due to their varying physical structures this difference will impact on their interactions with hydrodynamics and sediment dynamics despite the importance of wetlands little is known about the feedbacks between ecosystem traits hydrodynamic buffering or sediment transport hu et al 2014 morris et al 2002 feedbacks between ecosystem traits and their physical environment the expansion of the ecosystem channel configuration and ecosystem stability fagherazzi et al 2017 liu et al 2020 temmink et al 2020 hydrodynamic effects on wetlands varies with the traits of the ecosystem engineers for instance rigid vegetation which extends through the entire water column will more strongly deflect flow velocity compared to flexible vegetation this stronger deflection can cause a negative feedback such as larger channel erosion which restricts lateral growth bouma et al 2009 flexible wetland plant s ability to reduce wave energy and flow velocity combined with their flexible leaves which trap suspended particulate material can form a positive feedback which increases sediment accumulation potential therefore nutrient availability or ecosystem expansion bouma et al 2009 another critical feedback is scale dependant feedbacks which combines both positive and negative feedbacks and is driven by wave and currents community and plant traits of the vegetation bouma et al 2015 the combination of negative increased erosion and positive feedbacks sediment accumulation are important for organization and distribution of salt marsh and mangrove patches understanding processes at the lateral edge of intertidal wetlands offers improved predictability of edge dynamics and hence the potential future seaward extent of mangrove forests and salt marshes under changing hydrodynamic forcing here we provide a direct comparative study of sediment transport and scour development at the lateral edge of such contrasting habitat types systematic comparisons of flexible vegetation i e marsh plants with rigid vegetation i e mangrove trees can offer new insights into sediment transport coastal management and the evolution of coastal wetland habitats this study aims to contribute to knowledge by systematically comparing the effects of different plant traits on sediment dynamics under controlled hydrodynamic and sedimentary conditions using full scale vegetation mimics of simplified geometry to control plant flexibility and density in a large scale laboratory flume setting a particular focus is placed on scour development and turbulence generation near the ocean facing and lateral edges of continuous patches bordering channels within the vegetation stand fig 1 the study includes waves only currents only and combined wave current scenarios to explore the context dependency of bio physical interactions in the intertidal zone the plants traits will influence the intensity and scale of their turbulent interactions with their physical environment to further understand the ability of wetland ecosystems to buffer hydrodynamics and their lateral expansion information is required on wetland feedback processes this knowledge will allow us to predict the response of coastal wetlands to changing hydrodynamics and therefore use them further in more climate resilient coastal defences 2 material and methods the experiments were conducted in a customized wave basin at the danish hydraulic institute dhi in hørsholm denmark the basin was 25 m wide and 35 m long had a 5 5 m wide piston type paddle for wave generation and a recirculation system for current generation based on the facility s capabilities along with the desired hydrodynamic conditions of an effective water depth h of 0 30 m and a unidirectional flow speed u of 0 30 ms 1 the test zone was limited to a width of 5 5 m by two side walls the area was divided into two 2 75 m wide channels with one of the ecosystem mimics placed in each one each of these channels was divided longitudinally into a section 1 25 m wide in which vegetation mimics were placed and an unvegetated section 1 50 m wide fig 2 this setup mimics the geometry of a channel in the vegetation stand given the confinement of the flume only half a channel side was mimicked while the other side was bordered by a vertical wall this setup allowed clear identification of the effects of the lateral and longitudinal edge of one mimic patch while free flow was able to develop in the open channel as would be the case in the center of natural channels at the end of the test zone a dissipation beach was installed consisting of several perforated parabolic steel plates for the analysis of the results a coordinate system was adopted in which x denotes the along channel direction with x 0 at the upstream edge of the vegetation patches and y denotes the across channel direction with y 0 at the wall next to the flexible mimic section the boundaries between the vegetated and unvegetated sections aligned in the x direction are referred to as lateral edges fig 2 purple lines the edges of the vegetation sections aligned in the y direction and located at x 0 are referred to as leading edges fig 2 yellow lines a schematic representation of the experimental set up is given in fig 2 2 1 hydrodynamic conditions and sediment characteristics within the constraints of the facility s capabilities the hydrodynamic conditions were defined to represent realistic inter tidal flow conditions under mild conditions brinkman 2007 garzon et al 2019 a water depth of 0 30 m was set and regular waves with height h 0 08 m and periods t ranging from 0 8 to 1 4 s were studied for the experimental runs using unidirectional currents the mean flow speed was set at 0 30 ms 1 table 1 presents the hydrodynamic conditions assigned for each run the sediment was selected based on these hydrodynamic conditions a 0 18 mm nominal diameter sand was chosen for the experiment to achieve moderate sediment transport under the test conditions the maximum scour around the mimics was estimated based on eadie and herbich 1986 and to prevent the scour from reaching the concrete bottom of the basin the whole setup was placed in a 20 cm pit that was filled with sediment fig 2 2 2 vegetation mimics given the chosen hydrodynamic conditions it was estimated that a 10 m length of vegetation mimic patch would be sufficient for a fully developed flow to develop within it based on zhang et al 2015 and maza et al 2017 to exclude effects caused by traits other than the ones under investigation flexibility and density a simple plant mimic geometry was chosen it consisted of circular cylinders either rigid or flexible with uniform vertical cross sections while the mimics do not resemble any specific plant species individual i e diameter length as well as community i e density traits were chosen similar to salt marsh vegetation for the flexible mimics and mangrove aerial roots for the rigid mimics respectively table 2 the flexible mimics were defined based on the properties of salt marsh vegetation e g salicornia sp reported in the literature to simulate basic salt marsh morphology transparent pvc tubes of 6 mm in diameter and 50 cm in length with a young s modulus equal to 2 9 gpa were selected based on reports such as zhu et al 2020 two densities equal to 420 and 210 stems m2 are selected for the experiments which fall within the range of natural observations e g knutson et al 1982 rigid mimics were defined based on mangrove prop roots properties mangrove roots were idealized using uniform cylinders of 3 cm diameter and long enough to be emergent under all tested conditions based on data reported in the literature e g ohira et al 2013 as a point of comparison it is considered that the frontal area in both species will remain constant then a density equal to 84 and 42 mimics m2 is considered in the rigid canopy table 2 shoot densities were halved for additional test runs to cover densities relevant for stages of patch development this resulted in a higher total submerged volume for the rigid vegetation patch compared to the flexible vegetation patch because of the former s greater volume to frontal area ratio for current and wave only runs only the higher vegetation densities were tested for the combined wave and current runs both the higher and lower vegetation densities were tested 2 3 measurements and post processing hydrodynamic measurements were begun 15 min after the flow and or waves were turned on to avoid transient start up effects velocities were recorded with 6 acoustic doppler velocimeters advs and free surface height with 24 resistive free surface gauges all the sensors were synchronized and measured at 25 hz for 90 s at each location the incident hydrodynamic conditions were measured 0 75 m in front of the patch s leading edges velocity data were also obtained inside the vegetated sections at 0 5 m and 2 5 m downstream of the patches edges the free surface height was measured at approximately the same positions as well as at 5 m downstream of the leading edge within the vegetation at each of these along channel positions measurements were taken at three across channel locations in the center of the vegetation patches at their lateral edges and in the unvegetated channel fig 2 in the vertical direction velocities were measured every 2 cm for the closest position to the edge and every 5 cm for the other two longitudinal positions starting at 2 and 5 cm from the initial bed level respectively the measured velocity components were processed by applying four filtering strategies firstly a filter based on signal quality was applied in which measurements with correlation values less than 50 were discarded secondly a cut off filter was applied to remove spikes in the velocity record over a predetermined threshold 0 55 ms 1 for waves 0 7 ms 1 for currents and 1 2 ms 1 for waves and currents these threshold values were determined by observing the time series thirdly an acceleration filter was used to suppress oscillations with accelerations larger than gravitational acceleration finally a standard deviation filter was applied removing velocity values which were more than three standard deviations from the mean velocity the filtered values were left empty and not considered in further calculations the mean velocity values were obtained by performing time averaging for the current only runs and phase averaging for the waves only and waves and current runs the time averages were applied over the whole of each recording the phase averages were calculated by dividing the time series in intervals of the length of the wave period and then performing an ensemble averaging over them 70 wave periods were used to obtain each of the phase averages values of the turbulent kinetic energy tke were obtained from the variation of the velocity signal around the mean or phase averaged velocity for the current only cases and the cases with waves respectively for wave only runs ww wave height evolution through the two high density patches was analysed by measuring the wave heights h along their centerlines y 0 625 m and normalizing them by the incident value hi obtained from the wave gage located offshore of the patch to give a relative wave height h hi the submerged solid volume fraction φ tanino and nepf 2008 was calculated as 1 ϕ s u b m e r g e d v e g e t a t i o n v o l u m e w a t e r v o l u m e the length scale of vortex penetration lv zong and nepf 2011 was calculated as being inversely proportional to the frontal area per volume of the patch 2 l v 0 5 c a 1 where a is equal to d n d being the stem diameter and n the number of stems per unit area the scale dependent feedback bouma et al 2015 was derived from the mean velocities it is representative of the flow due to the presence of the vegetation patches and can be obtained as the difference between the mean velocities in the channel uchannel and inside the vegetation patches uveg divided by the incident velocity ui 3 s c a l e d e p e n d e n t f e e d b a c k u c h a n n e l u i u v e g u i to measure bed elevation changes after each test 60 measurements were taken across a transect at each of 10 longitudinal positions along the experimental area shown in fig 2 the measurements were conducted using two sedimentation erosion bars seb with 30 measurement points on each of them at each point along the bar a vertical pin was positioned that could be dropped onto the sediment surface allowing its height and thus the height of the sediment at that location to be measured against the fixed height of the horizontal seb the distance between the measurement points was 5 cm these measurements were made after each of the test runs which lasted for approximately 1 5 h when the sediment bed had acquired a stable configuration after each experiment the sediment was re leveled by moving it from the accumulation to the erosion zones to restore the initial flat configuration to acquire a better representation of the bed level the data were smoothed using a centered 3 point moving average in the transversal direction 3 results 3 1 uni directional flow current velocity patterns and sediment elevation were analysed for both types of high density simulated vegetation table 2 the flexible vegetation slows the incoming flow reducing its speed by almost 50 by x 2 5 m inside the patch fig 3 a top panel the decrease in flow speed leads to sediment deposition within the patch starting at around x 1 0 m simultaneously in the lateral channel the along channel speed increases by more than 30 at x 2 5 m with respect to the incoming value fig 3 a bottom panel leading to sediment erosion fig 3 b shows that the largest lateral y component speeds are measured at the patch lateral edge at x 0 5 m these lateral speeds decrease further into the patch at x 2 5 m this decrease is the result of the flow divergence region at the leading edge of the patch where the flow in line with the patch decelerates and a large part of the flow is diverted towards the open channel the flow divergence produces sediment erosion at the leading edge where velocities are not yet significantly attenuated top and central panel in fig 3 a the divergence flow extends over a distance ld defined as the length necessary for the velocities to be less than the incident values from the velocity measurements the ld should be between x 0 5 and 2 5 m taking the position of maximum sediment deposition within the patch as the location where velocities have been significantly attenuated to allow sediment accumulation ld 1 5 m is estimated for the flexible vegetation which falls well within the range implied by the velocity measurements in the rigid vegetation patch fig 4 a shows that the along channel speed decreases by almost 70 by x 2 5 m while it increases by almost 60 within the open channel at the same x position compared to the incoming value thus the rigid vegetation produces stronger flow divergence than that observed in the flexible vegetation maximum lateral speeds are observed at the patch lateral edge at x 0 5 m fig 4 b these strong lateral speeds implies that rigid vegetation causes a major deviation of the flow around the vegetation patch this process is also observed when analysing the free surface gradient h x produced by both patches fig s1 h x for rigid vegetation is 0 0063 0 0008 while for flexible vegetation it is 0 0046 0 0001 the largest free surface gradient obtained for rigid vegetation indicates a larger drag force exerted by the rigid elements on the flow in comparison to the flexible ones which reconfigure under the flow action as shown in fig s2 consequently a larger drag length scale is found for the rigid canopy which also presents a greater submerged volume fraction φr 0 060 in comparison to the flexible canopy φf 0 008 leading to a larger flow energy attenuation mazda et al 1997 maza et al 2019 the conceptual sketch fig 5 for both vegetation patches shows the main differences in flow divergence and sediment movement stronger flow divergence by the rigid vegetation panel b produces a longer diverging flow region at the leading edge ld and greater erosion in the channel due to the larger velocities observed for both patches at the leading edge after the diverging flow region there is a fully developed region within the rigid vegetation where the velocity is uniform across the patch width and length a shear layer forms at the interface between the patch and the lateral channel where shear layer vortices develop since both patches present the same frontal area per volume the length scale of vortex penetration lv is expected to be similar for both cases being slightly bigger for the rigid vegetation due to its larger cd value as discussed above taking cd 1 gives a value for lv of approximately 0 20 m tanino and nepf 2008 s formula for turbulence generated by element wakes considered a balance between the production of tke and its viscous dissipation the formula shows that larger tke values are obtained in cases with greater in canopy velocities at x 2 5 m tke in the flexible vegetation has a mean value of 0 001 m2 s2 while for rigid vegetation has a mean tke of 0 006 m2 s2 whilst φ is equal to 0 012 and 0 060 for flexible and rigid vegetation respectively whereas the depth averaged velocity at x 2 5 m is equal to 0 155 m s 0 0016 for flexible and 0 097 m s 4e 06 for rigid vegetation values in brackets show the standard deviation thus the greatest tke produced by element wakes is found in the rigid vegetation fig 4 a top panel despite the smaller in canopy velocity following the formular from tanino and nepf 2008 this indicates larger tke for the rigid vegetation than for the flexible one confirming the greater influence of φ versus the velocity reduction in terms of tke produced by element wakes turbulence can also be generated within shear regions such as at patches lateral edges this process is observed in the flexible vegetation case where the largest tke values are recorded at the lateral edge of the patch fig 3 a at the end of the diverging flow region in the rigid vegetation sediment deposition increases in the along channel direction as the local velocity decreases the opposite occurs in the fully developed flow region where deposition decreases longitudinally through the patch as the sediment concentration in the water column is depleted in this region sediment elevation is almost uniform across the patch width except within the shear layer region turbulent transport is enhanced by shear layer vortices which transport resuspended sediment along the lateral edge of the patch this transportation results in an accumulation of sediment at the downstream side edge of the flexible vegetation patch and causes some of the sediment that has been suspended in the diverging flow region of the rigid vegetation to be deposited at the lateral edge fig 5 3 2 wave only runs wave height evolution was measured in the high density vegetation patches and is shown in fig 6 combined with the sediment elevation measured along the centerlines free surface measurements at x 5 m were not recorded for the rigid vegetation run however the results show that the flexible vegetation did not reduce the wave height as much as the rigid vegetation which reduced the wave height up to 23 more than the flexible vegetation at x 2 m fig 6 this difference is also shown by greater velocities within the patch at x 2 5 m for flexible vegetation compared to rigid vegetation figs s3 s4 and s5 there is similar wave height attenuation for the three tested wave conditions with a small decrease in the amount of attenuation with increasing wave period fig 6 the mean values and standard deviations in brackets of the relative wave heights at x 2 m are 0 77 m 0 06 m and 0 67 m 0 02 m for the flexible and rigid vegetation respectively and 0 62 m 0 07 m for the flexible vegetation at x 5 m thus it is observed that the flexible vegetation needs more than twice the distance from the leading edge than the rigid vegetation to obtain similar wave height attenuation although it should be noted that the largest rate is observed for the first 2 m of vegetation in both cases this is further confirmed by the velocities recorded in both patches which are reduced by more than 30 after 2 5 m in both cases figs s3 s4 and s5 the greatest tke values are observed for the rigid vegetation but tke is reduced by up to 40 from 0 5 to 2 5 m in all wave cases in both patches at the leading edge where velocities have not yet been attenuated sediment is transported into the patch the decrease in velocity and tke leads to sediment deposition downstream of the leading edge in both vegetation mimics at 1 5 2 5 m bottom panels in fig 6 sediment deposition and consequently sediment accumulation occurs over a shorter distance in the flexible vegetation patch at 1 5 m compared to the rigid vegetation patch at 2 5 m fig 6 within the patch the largest accumulation for flexible vegetation occurs at 1 5 m while for rigid vegetation it is observed at 2 5 m fig 6 sediment erosion at the leading edge is milder for the flexible vegetation which may be due to the greater turbulence produced around the rigid elements which resuspends a larger amount of sediment compared to the flexible vegetation case fig 6 3 3 waves and currents the interactions of waves and currents with both types of vegetation at both densities high and low is presented in this section for low densities the flow is diverted less compared to high densities so it penetrates further into the patch with fewer stems and greater velocities are recorded at x 0 5 2 5 m however high densities reduce velocity within the patch more strongly than low densities for flexible vegetation the velocity decreases from 20 to 24 low density cases and 20 to 40 high density cases with the smallest reduction for the longest wave period and the largest for the shortest figs 6 s6 a and s7 a in the case of rigid vegetation the decrease in velocity between x 0 5 and 2 5 m is smaller from 2 to 22 with the smaller values observed for low density cases the velocities at x 0 5 m are already lower than those recorded for the flexible vegetation due to the greater divergence of the flow at the leading edge the velocities for the low density cases are greater than those for the high density ones fig 7 longer wave periods lead to greater tke values within the patch for both vegetation types and densities for the flexible vegetation tke does not decrease in the first 2 5 m for the low density cases especially for longer wave periods where tke increases from x 0 5 to 2 5 m by up to 22 fig 7 at high density cases tke decreases from x 0 5 to x 2 5 m for all three waves and current conditions figs 7 s6 a and s7 a tke values recorded for rigid vegetation at both densities are greater than values recorded for flexible vegetation for rigid vegetation at x 0 5 m within the canopy tke values found for high density fig 7 b are larger than the values for low density especially near the bottom i e from the bottom to about half the water depth this larger turbulence near to the bottom for the high density cases increases sediment resuspension at the leading edge which leads to greater levels of erosion figs 7 and 8 at x 2 5 m tke decreases for the high density cases and sediment deposition is observed however for the low density cases the tke does not significantly decrease within the patch from x 0 5 to 2 5 m figs 7 b s5 b and s6 b leading to increased sediment transport resulting in a sediment deposition zone further into the patch around x 5 m thus a change in plant density leads to a change in the longitudinal deposition of sediment within the rigid vegetation patch with sediment deposition at a greater distance from the leading edge for lower densities therefore for rigid vegetation both turbulence intensity and flow velocities determine sediment transport in and around the patch it is also shown that a decrease in vegetation density leads to less erosion along the channel due to the gentler divergence of the flow and to a larger area within the patch from the leading edge where sediment is eroded 3 4 scale dependent feedback following bouma et al 2015 the scale dependent feedback is analysed in this section by using flow deceleration within the patch as a proxy for the positive feedback and flow acceleration around the patch as proxy for the negative feedback discussed in the next section in detail the scale dependent feedback is evaluated for the current only and waves and currents cases at two longitudinal positions x 0 5 and 2 5 m fig 9 shows the scale dependent feedback strengths for both positions rigid and flexible vegetation and the different hydrodynamic conditions fig 9 shows that the scale dependent feedback increases when moving towards the patch from the leading edge in agreement with the velocity decrease produced by the canopy fig 9 b x 2 5 m shows larger values than fig 9 a x 0 5 m for all hydrodynamic and vegetation conditions especially for cc at least for flexible vegetation since no measurements at x 0 5 m are available for the rigid one at the leading edge fig 9 a and for high density cases the scale dependent feedback is almost constant for both vegetation types being equal to 0 24 0 04 for flexible vegetation and equal to 0 39 0 02 for rigid vegetation this difference is also observed for the low density cases where the mean value for the flexible vegetation is equal to 0 10 0 06 while it is 0 22 0 05 for the rigid vegetation for flexible vegetation a decrease in the scale dependent feedback is observed with increasing wave periods especially for low density cases this decrease may be linked to the ability of flexible vegetation to move and reconfigure under the flow action finally it is observed that the scale dependent feedbacks for low density cases are 42 and 57 smaller than those for high density cases for flexible and rigid vegetation respectively thus the strong flow divergence produced by high density cases with 0 012 and 0 060 for flexible and rigid vegetation is significantly reduced by half at x 2 5 m fig 9 b for low density cases the scale dependent feedback is greater than the value for high density at x 0 5 m confirming that for low density cases a longer distance inside the patch is needed to achieve the same velocity attenuation in contrast with what is observed at x 0 5 m for wave and currents conditions and high density cases similar scale dependent feedbacks are found for rigid and flexible vegetation being equal to 0 55 0 05 and 0 53 0 06 respectively for low densities with smaller φ a significant difference in the value of the scale dependent feedback is still observed between rigid and flexible vegetation at x 2 5 m with values equal to 0 43 0 03 and 0 32 0 05 respectively thus it is found that dense flexible canopies may be very effective at attenuating flow velocity over short distances but once the density of the canopy is significantly reduced by half in this case the attenuation of the meadow is more limited however for rigid vegetation the same density change i e a φ change does not result in such a large loss of attenuation capacity of the canopy 4 discussion ecosystem engineers traits affect hydrodynamics and sediment dynamics our results show that flexible and rigid vegetation reduced flow speed by 50 and 70 respectively for unidirectional flow figs 3 and 4 within this unidirectional flow turbulent kinetic energy was generally lower in the flexible vegetation 0 001 m2s 2 compared to the rigid vegetation 0 006 m2s 2 which corresponds to greater erosion figs 3 and 4 under wave only conditions the flexible vegetation attenuated waves slower initially than rigid vegetation the flexible vegetation required more than twice the distance from the leading edge than the rigid vegetation to obtain similar attenuation rates fig 6 however by 2 5 m both vegetation types reduced turbulent kinetic energy by 40 under only wave conditions which led to sediment deposition after the leading edge for both vegetation mimics at 1 5 2 5 m fig 6 for flexible low density vegetation under combined waves and currents the flow speed reduction within the patch ranged from 20 to 24 whilst for high density it was 20 40 the change in flow speed corresponded to larger sediment accumulation in high density flexible plants which occurred closer to the leading edge fig 7 in the case of the rigid vegetation the decrease of flow speed within the patch ranged from 2 to 22 with the smaller values observed for low density vegetation combining waves and currents resulted in increased channel speeds when the current first encountered the vegetation across all conditions for both patches at high density tke values recorded for flexible vegetation at both densities were lower than those recorded for rigid vegetation as shown for current only tests larger tke values indicated greater sediment erosion at the front of the rigid vegetation fig 8 the biggest increase in scale dependent feedback was found between 0 5 and 2 5 m in flexible vegetation marsh patches under all wave and current conditions however generally scale dependent feedback was greater in the rigid vegetation especially at lower densities 4 1 effects of flexible vegetation in terms of the ability to reduce erosion and accumulate sediment flexible vegetation clearly has advantages in flexible vegetation the formation of a sharp precipice does not occur due to the strong erosion at the front of the patch which forms in the rigid vegetation figs 3 4 and 7 previous research has found most of the suspended particles transported within flexible canopies collide with the moving flexible stems leading to particle capture by loss of momentum ganthy et al 2015 the current study confirms this assessment for example under wave only conditions where the flexible vegetation caused no erosion only accumulation in contrast to the rigid vegetation fig 6 as noted in other studies king et al 2012 nepf and vivoni 2000 turbulence was generated within the shear regions along the lateral edge of the flexible vegetation patch 4 2 effects of rigid vegetation the rigid vegetation mimics had a greater effect in attenuating hydrodynamic energy compared to flexible vegetation for two reasons firstly the rigid vegetation does not bend and therefore has a greater flow resistance bouma et al 2005 2015 secondly there is larger drag caused by the rigid vegetation due to its rigidity and submerged solid volume fraction maza et al 2015 2019 paul et al 2012 as shown by the larger free surface gradient in the rigid vegetation under currents only in agreement with bouma et al 2009 tanino and nepf 2008 the rigid vegetation s ability to quickly reduce momentum within the patch under all hydrodynamic conditions reduces its ability to accumulate sediment this is because the reduction increases turbulence which these results suggest is the dominant hydrodynamic process for erosion in rigid vegetation tinoco and coco 2018 under unidirectional flow and waves combined with currents tke was generated around the individual vegetation mimics which was stronger for rigid vegetation compared to flexible vegetation tanino and nepf 2008 this can be linked to rigid mimic s traits such as rigidity and diameter when exposed to strong hydrodynamic conditions they generate intense near bed turbulence which causes local scouring around the base of the roots norris et al 2021 bouma et al 2009 found that for rigid vegetation this process was mainly due to basal diameter which determines the degree of scouring therefore the larger diameter of rigid mimics leads to greater scouring which is linked to the overall larger erosion produced within the rigid vegetation additionally near bed coherent structures generated by rigid stem bed flow interactions such as horseshoe vortices can significantly alter the near bed stresses locally in front of the rigid elements resulting in additional sediment resuspension schanderl et al 2017a 2017b previous research has established there is a relatively sheltered interior in rigid vegetation i e mangroves folkard 2019 norris 2019 where deposition occurs this study also located a sheltered interior where the lowest turbulence resulted in the greatest deposition of sediment within rigid patches deposition generally occurred much further ca 2 m into the rigid vegetation patch compared to the flexible vegetation patch ca 1 m the larger flow speed inside the lower density patches of rigid vegetation resulted in sediment being transported further into the patch this is facilitated by the decrease in tke in the first few meters of the patch for high density but not for low density similar to the flexible vegetation cases fig 7 b this was shown by the sediment accumulation patterns where accumulation was produced closer to the leading edge in the higher density patches fig 8 4 3 bio physical feedbacks in flexible and rigid vegetation three possible feedback processes have been identified in this study a positive feedback a negative feedback and the scale dependent feedback the positive feedback occurs where mimic vegetation attenuates flow energy to allow for sediment accumulation bouma et al 2009 2005 gourgue et al 2021 from this we deduce that flexible vegetation will have stronger positive feedback compared to rigid vegetation as flexible vegetation will accumulate more sediment in their patch compared to rigid vegetation under all the hydrodynamic conditions the negative feedback occurs where mimic vegetation deflects flow causing greater channel erosion in situ this could reduce the vegetation s lateral expansion but is also a key element of channel formation and hence wetland drainage temmerman et al 2007 zong and nepf 2011 the rigid vegetation cases had greater channel erosion under unidirectional flow due to stronger flow diversion compared to the flexible vegetation case when considering combined waves and currents this study shows that rigid vegetation at both densities studied was also more likely to develop a negative feedback of channel formation as diversion of the current was greater the stronger flow divergence relates to the fact that although both patches present the same frontal area rigid emergent elements produce greater flow resistance than flexible nearly emergent ones as reported previously bouma et al 2015 2005 previous studies have also found that higher densities cause greater flow deflection which results in more pronounced channel erosion whilst lower vegetation density allows the flow to dissipate within the vegetation due to there being more space around the vegetation roots or stems therefore less channel erosion gourgue et al 2021 the front of the lower density flexible vegetation had less channel erosion compared to the higher density case in agreement with previous studies where flow passed through flexible vegetation and was not deflected as strongly bouma et al 2015 2009 this deflection indicated a weaker manifestation of negative feedback i e channel erosion as decreasing density reduces deflection by flexible vegetation at this point fig 8 additionally along the leading edge of the high density flexible vegetation patch erosion from deflection is reduced with decreasing flow speeds the depth of erosion in the channel was dependent on the density of the plants and the hydrodynamic regime folkard 2019 the absence of these negative feedbacks in low flow environments may facilitate formation of flexible patches without erosion troughs and could facilitate expansion of such vegetation as is commonly observed in calmer hydrodynamic areas bouma et al 2009 conversely at larger velocities and higher densities increased channel depth could in situ reduce the plants success in expanding laterally into the channel the scale dependent feedback concerns the ratio of the flow speed within the vegetation to that in the channel changes in ratios between the rigid and flexible patch indicates flexible dense vegetation was effective in attenuating flow speed but once patch density was significantly reduced the attenuation capacity of the patch was limited however for rigid vegetation the same density change does not result in such a large loss of attenuation capacity of the meadow the values obtained at x 2 5 m in the currents only cases agree with values reported by bouma et al 2015 despite the differences between the tested vegetation real vegetation in the case of bouma et al 2015 while mimics are used here the value for rigid vegetation 1 23 is close to that obtained for spartina anglica 1 20 fig 5 in bouma et al 2015 a rather rigid species while the value obtained for flexible vegetation 1 03 is close to that obtained for salicornia procumbens 0 90 fig 5 in bouma et al 2015 a more flexible species additionally this study found that multi scale feedbacks are also associated with community and plant traits such as density of vegetation and rigidity flexibility of the plant recent studies have also shown the importance of traits temmink et al 2020 for plant establishment however there are a variety of traits associated with salt marsh and mangrove plants and their ecosystems and further research should concentrate on identifying potential abiotic boundary conditions determining hydrodynamic buffering and sediment transport capacity sediment accumulation is crucial for salt marsh and mangrove survival this research increases our understanding of bio physical limitations to lateral expansion of these ecosystems however for these ecosystems to maintain their role as a coastal defense in the context of climate change induced sea level rise we require further information i e community and plant traits to allow prediction of expansion with changing external forcing this information will become more important for designing ecosystem based coastal protection measures that are climate resilient 4 4 application to natural environments our results present insights into the effects of vegetation flexibility and density on sediment dynamics however their transfer to natural conditions may be partially limited by the simplifications introduced while constructing the vegetation mimics while the rigid mimics model mangrove prop roots reasonably well with respect to individual diameter length and community density traits as well as rigidity the flexible mimics neglect the effect of leaves on vegetation drag previous work has highlighted the importance of leaves and their reconfiguration under hydrodynamic loading for vegetation drag san juan et al 2019 whittaker et al 2015 schoneboom and aberle 2009 and instantaneous flow fields tinoco et al 2020 however resulting turbulent flow is most prominent in the upper part of the canopy while the region close to the bed is less affected tinoco et al 2020 especially for flexible coastal wetland vegetation e g salt marshes this pattern coincides with standing biomass distribution close to the bed most salt marsh plants exhibit individual stems growing from the roots with branches and leaves only occurring higher up along the plant as sediment dynamics will be governed by the local flow near the bed rather than fluctuations at the top of the canopy leaves play a minor role in this context they do however complicate calculations of the frontal area due to their constant changes in position such as streamlining under approaching flow aberle and järvelä 2015 we thus decided on a flexible vegetation model which models vegetation shape close to the bed and at the same time allows for a constant frontal area per canopy volume for comparison with the stiff mimics consequently our results based on mimics provide insights into the general sediment dynamics at leading and lateral edges of coastal wetland with flexible salt marsh and rigid mangrove vegetation while the application of results on the flow field higher up in the water column may be limited 5 conclusions this paper highlights the differences observed between two mimic ecosystem engineers mangrove trees and salt marsh plants with different characteristics which are subjected to the same hydrodynamic conditions the hydrodynamic and sedimentation processes identified for both ecosystems are linked to different feedbacks a positive feedback was identified in which vegetation attenuates hydrodynamic energy allowing sediment accumulation within the patch and a negative feedback associated with the high velocities produced from flow divergence causing channel erosion greater channel erosion could compromise the lateral expansion of the vegetation high rigid vegetation densities enhance this negative feedback lower flexible vegetation densities combined with calmer hydrodynamic conditions could facilitate the expansion of flexible patches as these patches have less flow divergence and therefore less channel erosion the strong flow divergence from rigid vegetation highlights their importance for buffering hydrodynamics but at the cost of potentially increased erosion within the frontal patch and lateral edges these findings illustrate the spatial dimensions of the ecosystem engineering outcome of two contrasting intertidal wetland species declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study h dhi 10 hywedges received funding from european union hydralab the authors would like to thank dhi for the use of their total environmental stimulator flume and the support staff m maza is sincerely grateful to the spanish ministry of science and innovation for the funding provided in the grant rti2018 097014 b i00 of proyectos de i d i retos investigación 2018 funded by mcin aei 10 13039 501100011033 and by erdf a way of making europe supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2022 104257 appendix supplementary materials image video 1 image application 2 image video 3 
121,dynamic programming dp is an effective method for solving multi stage decision making problems and has been extensively applied to the optimization of reservoir operations however the method is limited by the dimensionality problem hence it cannot be directly applied to optimizing the operations of a reservoir system with more than three reservoirs at current computing power the progressive optimality algorithm poa is a classic variant of dp and has been widely used for optimizing multi reservoir operations despite the use of a static variable decoupling strategy to ease dp s dimensionality problem the poa s performance is reduced by an inherent drawback of the blind search of the static variable decoupling strategy and the dimensionality problem in two stage solutions to enhance the poa s performance we propose an improved version of the poa known as the dipole optimization procedure dop for optimizing cascade reservoir operations in the improved algorithm we use a dynamic variable decoupling strategy to obtain search directions that are more targeted than those of the poa so as to improve the quality of the solutions the dynamic decoupling of variables was achieved by constructing and solving a dipole optimization problem using a dp model developed in this study moreover a perturbation mechanism was introduced to address the poa s dimensionality problem so that the algorithm can be extended to larger reservoir systems the results of the simulation of a hypothetical four reservoir system and a real world cascade reservoir system showed the superiority of the dop over the poa and other five available alternatives the comparison was based on the quality of solutions and solving efficiency the results indicate that the dop is rational and feasible and has the potential to be applied to optimizing the operation of large scale multi reservoir systems with numerous reservoirs keyword cascade reservoirs operation optimization dimension reduction variable decoupling dipole optimization perturbation mechanism 1 introduction hydropower is a clean cheap and renewable energy source which is vital in the global energy structure and power system hydropower accounts for 15 8 of the global electricity production azad et al 2020 with the rapid growth of the global economy and population energy shortage and environmental pollution are becoming increasingly severe thus it is crucial to utilize hydropower resources rationally and scientifically for improving economic benefits alleviating energy crises and reducing greenhouse gas emissions sibtain et al 2021 as hydropower systems on rivers are typically cascaded jointly operating cascade reservoirs has become an important way of improving the utilization of hydropower resources in river basins the interactions of water heads and discharge between upstream and downstream reservoirs require attention because of the strong hydraulic coupling among various reservoirs in a cascade reservoir system crs the storage and discharge order of reservoirs should be determined with the aim of making full use of water head and avoiding abandoning water as far as possible the joint operation of cascade reservoirs is a complex optimization problem characterized by high dimensionality multiple stages and numerous constraint additionally climate warming alters the terrestrial energy budget and aggravates the uneven redistribution of water resources yin et al 2021a all these factors make the joint operation problem of cascade reservoirs difficult to obtain the global optimal solution therefore researchers are actively investigating an efficient optimization method for optimizing the joint operation of cascade reservoirs optimization methods are the key to both finding the optimal release trajectories of reservoir systems under deterministic inflow scenarios and deriving operational policies e g rule curves of reservoir systems against stochastic reservoir inflows over the past few decades a series of optimization methods have been developed for optimizing reservoir operations they can be categorized into two main classes traditional optimization methods represented by dynamic programming dp and modern intelligence algorithms represented by the genetic algorithm ga the former mainly includes linear programming shamloo et al 2021 nonlinear programming dogan et al 2021 dp danso et al 2021 and its various improved versions feng et al 2017 he et al 2021 ji et al 2021 kumar and baliarsingh 2003 network flow algorithm braga and barbosa 2001 and large scale system decomposition coordination method hou et al 2022 the latter covers a large variety of algorithms ga seetharam 2021 particle swarm optimization chen et al 2020 gravitational search algorithm niu et al 2021 water cycle algorithm yavari and robati 2021 and so on the advantages and drawbacks of these methods have been discussed by azad et al 2020 dobson et al 2019 giuliani et al 2021 and zhang et al 2015 dp is outstanding because of its attribute of providing the global optimal solution of an optimization problem without imposing special requirements e g analyticity linearity differentiability and convexity on objective and constraint functions however the dimensionality problem prevents the application of dp in multi reservoir operation optimizations mroos to extend the scope of applicability of dp researchers have proposed several dimension reduction techniques these techniques can be classified into two main categories parallel computing techniques for improving the computational efficiency of dp and improved versions of dp for reducing the computational effort of dp parallel computing cheng et al 2014 he et al 2019 ma et al 2021 zhang et al 2015 is an effective way of improving the computational efficiency of algorithms unlike the traditional serial computing mode parallel computing allocates various computing cells of an entire computing task that can be executed independently to multiple computing resources e g cpu cores or processors which simultaneously perform computations to improve computational efficiency a prominent benefit of parallel computing is that it improves the utilization of computing resources and helps to realize the full potential of multi core processors however the memory and computational requirements of dp do not decrease as parallel computing only exchanges space resources for time the computational cost of dp becomes unbearable as the problem size increases precisely the parallel dynamic programming will lose its applicability to daily impoundment operation when the number of reservoirs is more than three he et al 2019 the improved versions of dp are mainly typified by dynamic programming successive approximation dpsa larson and korsak 1970 discrete differential dynamic programming dddp heidari et al 1971 and progressive optimality algorithm poa howson and sancho 1975 these are successive approximation algorithms but differ in their dimension reduction mechanisms dddp adopts a neighborhood search strategy to reduce the number of states in dp models by searching in constantly changing neighborhoods better known as corridors around the current state trajectory which contain only a few of states usually three per stage the dpsa and poa adopt a static variable decoupling strategy svds to divide variables of the problem into several fixed groups and then optimize alternately the values of the variables of each group while fixing those of the remaining groups the strategy reduces the number of variables concerned per time while these algorithms have effectively eased dp s dimensionality problem they possess certain downsides that limit their performance in mroos for example dddp is an exponential time algorithm which is not applicable to large scale mroos feng et al 2017 additionally dpsa struggles to achieve global convergence when dealing with a non convex problem cheng et al 2014 the poa is a classic variant of dp proposed by howson and sancho 1975 and has been widely applied to mroos it reduces the dimension of problems through successive approximations using a general two stage solution which is essentially the svds owing to the inherent defects of the svds discussed in section 3 2 the poa s performance is significantly relegated in mroos the poa has two important limitations the dimensionality problem in two stage solutions and the blind search improvements to the poa have been proposed recently for example cheng et al 2012 developed the multi step poa to improve the convergence speed of the algorithm by reducing the number of computational stages zhang et al 2016 used a linear approximation technique to reduce the computational effort of the two stage solution of the poa feng et al 2018 contributed the uniform poa to relieve the poa s dimensionality problem through sampling in the state space of the system with a uniform design table jiang et al 2018 advanced the multi stage poa to improve the solution quality of the poa by expanding the two stage mode of the poa to a multi stage mode chen 2021 proposed an enhanced version of the poa to improve the solution quality and computing efficiency of the poa by adopting dp to solve the two stage poa problem ji et al 2021 introduced the nested poa to alleviate the poa s dimensionality problem in the context of water propagation impact between cascade reservoirs although the poa s performance has been considerably improved through these enhanced variants the algorithm still has much room for enhancement therefore we propose a dynamic variable decoupling strategy dvds discussed in section 3 3 to improve the solution quality of the poa we also discuss relevant research directions for improving the solution quality of the algorithm additionally we introduce a perturbation mechanism to address the poa s dimensionality problem in two stage solutions owing to our focus on improving the poa s performance so as to obtain better solutions under shorter time for simplifying the calculation the uncertainty of reservoir inflows was not accounted for in the cascade reservoir operation optimization problem formulation the rest of the paper is organized as follows section 2 describes the formulation of the long term power generation operation problem of a crs section 3 introduces the basic principle and major limitations of the poa and describes the proposed improved version of the poa section 4 presents two case studies that test the performance of the proposed algorithm finally section 5 outlines the main conclusions of the study 2 problem formulation 2 1 objective function the power generation operation of cascade reservoirs is typically aimed at maximizing the total power generation benefit of the system given the total available hydro energy e g ma et al 2021 or minimizing the total operation cost of the system toward supplying the required amount of electricity e g liao et al 2021 the former goal was prioritized in this study we consider a crs consisting of m reservoirs which are sequentially numbered 1 2 m from upstream to downstream assuming the operation period is divided into t identical time periods the objective function can be written as 1 max i 1 m t 1 t λ i t p i t δ t i 1 m t 1 t 9 81 3600 λ i t η i q i t 0 5 h i t 1 f h i t f h i t b where λi t energy price of reservoir i at time period t cny kwh pi t output of reservoir i at time period t kw δt length of a time period h qi t release through the turbine of reservoir i at time period t m3 i reservoir index t time period index ηi average efficiency of reservoir i h i t 1 f h i t f forebay elevations of reservoir i at the beginning and end of time period t respectively m h i t b average tailrace elevation of reservoir i at time period t m 2 2 constraints 2 2 1 continuity equation 2 v i t v i t 1 q i t r i 1 t r i t l i t r i t q i t s i t where vi t 1 vi t storages in reservoir i at the beginning and the end of time period t respectively m3 qi t and ri 1 t independent and dependent inflows to reservoir i at time period t respectively m3 ri t release of reservoir i at time period t m3 and r 0 t 0 li t evaporation loss of reservoir i at time period t m3 si t spillage of reservoir i at time period t m3 2 2 2 limits on reservoir storage 3 v i t min v i t v i t max where v i t min v i t max lower and upper bounds on the storage in reservoir i at time period t m3 2 2 3 limits on reservoir release 4 r i t min r i t r i t max where r i t min r i t max lower and upper bounds on the release of reservoir i at time period t m3 2 2 4 turbine discharge capacity 5 q i t min q i t q i t max where q i t min q i t max lower and upper bounds on the release through the turbine of reservoir i at time period t m3 2 2 5 power output bounds 6 p i t min p i t p i t max where p i t min p i t max lower and upper bounds on the power output of reservoir i at time period t kw 2 2 6 total power output bounds 7 p t min i 1 m p i t p t max where p t min p t max lower and upper bounds on the total power output from the cascade reservoir system at time period t kw 2 2 7 elevation storage curve 8 h i t f f v i t where f vi t forebay elevation function of reservoir i which varies with reservoir storage vi t m 2 2 8 elevation release curve 9 h i t b h r i t where h ri t tailrace elevation function of reservoir i which varies with reservoir release ri t m 2 2 9 limits on initial and final storage 10 v i 0 v i i v i t v i f where v i i v i f initial and final storages in reservoir i which are typically specified at the beginning of the operation period m3 3 improved poa dop 3 1 basic principle of poa the poa is an effective optimization technique developed by howson and sancho 1975 to alleviate dp s dimensionality problem the alleviation is based on a prior deduction from bellman s principle of optimality which states that the optimal path has the property that each pair of decision sets is optimal in relation to its initial and terminal values according to this theory the poa decomposes a t stage m reservoir problem into t 1 two stage m reservoir sub problems and then successively and iteratively solves the two stage sub problems to approximate to the solution of the entire problem the poa is detailed in the work of howson and sancho 1975 the poa s dimension reduction is essentially a svds as the state variables of the problem are divided into t 1 fixed groups that is the members of each group remain unchanged throughout the entire optimization process and then the algorithm alternately optimizes the values of the variables of each group while fixing those of the remaining t 2 groups as only the values of a few variables are optimized while those of the remainders are fixed in each sub problem solution the computational burden is reduced the time and space complexities of the poa are o t nm and o nm respectively where n number of the discrete states of a reservoir 3 2 major challenges of poa the poa has a good convergence and is robust to various problems which makes it a popular mroo technique however some drawbacks limit the performance of the technique in mroos a critical drawback is the blind search due to the svds from the poa search mechanism the algorithm only searches in the t 1 fixed m dimensional subspaces produced from the t 1 variable groups and does not compare and evaluate the variables throughout the entire optimization process this considerably limits the algorithm search space and inevitably affects the quality of the solutions another drawback of the poa is the dimensionality problem in two stage solutions since no universal solution method is available for different types of two stage poa problems a reliable and commonly used approach is to discretize the continuous state space of each reservoir into a finite set of state values and then evaluate the objective function of the problem against all possible state combinations of various reservoirs i e enumeration this inevitably increases the computational requirements exponentially with the problem size as with dp this drawback severely limits the applicability of the poa to large reservoir systems with numerous reservoirs 3 3 dvds 3 3 1 basic idea of dvds the above analysis shows that the key idea of the svds is to decompose the m t 1 dimensional solution space into t 1 fixed m dimensional subspaces and then successively approximate to the solution of the problem by searching alternately in the t 1 subspaces actually such m dimensional subspaces are generally much more than the t 1 subspaces because the subspace can be obtained from a combination of arbitrary m variables of the problem the number of combinations is m t 1 m therefore using the svds to decompose the solution space will inevitably result in a loss of numerous search directions which will ultimately affect the solution quality of the poa to obtain more search directions we suggest a dvds for the poa for clarity we formulize the relevant problems discussed in the rest of this paper in terms of decision variables i e reservoir releases in the dvds 2 m decision variables two for each reservoir are dynamically extracted from the m t decision variables of the problem to build an m dimensional subspace of the solution space the algorithm then searches in the subspace to continually improve the power generation benefit of the system the process is repeated until the release trajectory of the system reaches a steady state the variable extraction is achieved by constructing and solving a dipole optimization problem aimed at the maximal increase in the power generation benefit of the system the increase in power generation benefit is achieved by disturbing the values of the extracted variables by a small increment while fixing those of the remaining variables i e the combination of variables extracted to build the subspace has the most considerable influence on the power generation benefit of the system although the dop s dimension reduction idea originates from the poa the techniques are distinct the main difference between the dop and poa is that the latter divides the variables of the problem into t 1 fixed groups beforehand and then selects a variable group including 2 m decision variables to generate a search subspace at a time however the former does not group the variables but dynamically extracts 2 m variables from the variable set of the problem after evaluating the sensibility of the objective function to various combinations of 2 m variables to produce a search subspace at a time by contrast the dop has more as well as more targeted search directions than the poa which can compensate for the latter s defect of blind search we now introduce the detailed design of the dop 3 3 2 dipole optimization problem 1 definition of dipole pair according to the maximum power generation benefit model of cascade reservoirs described in section 2 the total release of reservoir i over the entire operation period can be expressed as the following recursive relationship 11 w i t 1 t r i t v i i v i f t 1 t q i t l i t w i 1 w 0 0 where v i i v i f qi t and li t are known thus wi can be calculated implying that the total release of each reservoir over the entire operation period stays constant in a crs thus for any reservoir i i 1 2 m an increase in release at a time period causes a decrease in release at another time period and the increment is equal to the decrement according to the above principle we introduce a disturbance on the release trajectory of reservoir i for example by increasing the release at time period j while reducing the release at time period k by an equal magnitude of ri where ri is the disturbance magnitude corresponding to reservoir i and takes a small positive value for other reservoirs in the system we also perform an operation similar to that of reservoir i owing to the disturbance superimposed on the release trajectory of the crs the power generation benefit of the system changes the disturbance is referred to as an effective disturbance when the power generation benefit increases otherwise it is an invalid disturbance we define the disturbances as a dipole pair of reservoir i expressed as ji ki ri or more succinctly expressed as ji ki because the disturbances acting on the releases of reservoir i at time periods j and k are equal in magnitude but opposite in direction 2 objective function exerting an effective disturbance on the release trajectory of a crs can typically improve the power generation benefit of the system and the more effective the disturbance the larger the power generation benefit as ji and ki in ji ki ri can take any value from the integer set 1 2 t the total number of dipole pairs for reservoir i is t t 1 1 all the cases of ji ki correspond to one dipole pair considering all possible dipole pairs of every reservoir we obtain the total number of combinations of dipole pairs for a crs consisting of m reservoirs as t t 1 1 m one of these combinations must produce the largest power generation benefit by acting on the release trajectory of the system called the optimal dipole combination we intend to find the optimal dipole combination thus the dipole optimization problem can be written as follows taking the m 1 th iteration of the algorithm as an example 12 max 1 j i k i t e i 1 m t 1 t 9 81 3600 λ i t η i r i t m sign t j i k i δ r i s i t 0 5 h i t 1 f h i t f h i t b sign t j i k i 1 if t j i 1 if t k i 0 otherwise where ji ki variables of the problem namely the time period numbers in the dipole pair of reservoir i e power generation benefit of the system over the entire operation period cny r i t m optimal release of reservoir i at time period t at the m th iteration sign t ji ki sign function which takes 1 1 or 0 3 constraints the constraints that must be considered for the dipole optimization problem are the same as those in eqs 2 10 the essence of the dop is to find the optimal dipole combination by maximizing the m t terms in summation as shown in eq 12 and then repeatedly exert the disturbance i e the optimal dipole combination on the continually updated release trajectory of the crs to improve the quality of solutions the release trajectory of the crs reaches a steady state after the dipole optimization problems are repeatedly constructed and solved while different subspaces determined by the optimal dipole combinations are continually searched 3 3 3 dynamic programming model for dipole optimization problem the main advantage of dp is that it establishes a recursive relationship according to bellman s principle of optimality that is any sub strategy of the optimal strategy is always the optimal to decompose the multi stage problem into multiple one stage sub problems and then obtains the optimal solution of the entire problem by determining that of each sub problem owing to the overlaps among various sub problems the calculation results of the previous sub problem can be directly called in solving the current one which prevents repeated calculations and improves solving efficiency dp has been widely used in different areas such as economic management production scheduling engineering technology and optimization control dp contains three major elements stage variables state variables and a recursive equation we used dp to solve the dipole optimization problem and the basic elements of dp for the problem are as follows 1 stage variables the operation period is typically divided into several stages a time period corresponds to a stage when using dp to solve the operation optimization problem of reservoir systems this division is performed because the operation decisions of the problem are typically made in a temporal order the solution of the dipole optimization problem comprises the optimal dipole pair of each reservoir which shows little relevance to time moreover in a crs the optimal release trajectory of a reservoir cannot be calculated until the release trajectory of its immediately upstream reservoir is determined thus the decisions corresponding to the dipole optimization problem are made in a spatial order therefore we consider a reservoir as a stage for the dipole optimization problem hence the stage variable is the reservoir index i for a crs consisting of m reservoirs the total number of stages for the dipole optimization problem is m 2 state variables a state variable is a set of variables that can completely describe all possible states of a system thus we take ji ki the dipole pair of reservoir i as the state variable of the dipole optimization problem analyzing the characteristics of crss shows that the optimal states of reservoirs i 1 i 2 m only depend on the state of reservoir i and are irrelevant to those of reservoirs 1 2 i 1 that is j i 1 k i 1 j i 2 k i 2 j m k m the optimal state sequence after stage i only depends on ji ki the state at stage i and has nothing to do with j 1 k 1 j 2 k 2 ji 1 ki 1 the path to ji ki thus taking ji ki as the state variable of the problem follows the markov property according to the previous analysis the number of states for each stage of the dipole optimization problem is t t 1 1 3 recursive equation we now use a forward recursion procedure of dp to solve the problem after defining the stage and state variables the recursive equation is as follows 13 g i j i k i max 1 j i 1 k i 1 t t 1 t 9 81 3600 λ i t η i r i t m sign t j i k i δ r i s i t 0 5 h i t 1 f h i t f h i t b g i 1 j i 1 k i 1 g 0 j 0 k 0 0 for each ji ki where g i 1 j i 1 k i 1 optimal cumulative power generation benefit over stage 1 to stage i 1 for state ji 1 ki 1 cny the detailed process of solving the dipole optimization problem using dp is shown in fig 1 as illustrated in fig 1 dp decomposes the original m stage problem into m overlapping one stage sub problems using eq 13 which prevents repeated calculations and improves solving efficiency because each overlap among the sub problems is calculated only once since each reservoir has n n t 2 t 1 states and at each stage the power generation benefit is evaluated at most n 2 times the time and space complexities of dp for solving the dipole optimization problem are o m t 4 and o m t 2 respectively 3 4 searching in m dimensional subspaces using perturbation mechanism we determine the combination of 2 m variables that has the most considerable influence on the power generation benefit of the system the final task of the dop is to search in the m dimensional subspace generated from the extracted variable combination to improve the power generation benefit of the system to avoid the dimensionality problem of the enumeration method we introduce a perturbation mechanism into the algorithm for searching in the subspaces assuming the optimal dipole combination can be expressed as j i k i i 1 2 m the update equation for reservoir releases can be written as taking the m 1 th iteration of the algorithm as an example 14 k 1 r i t k r i t sign t j i k i δ r i 0 r i t r i t m for any combination of i and t i 1 2 m t 1 2 t where k number of exerting disturbances set to 0 at the beginning of the search typically the power generation benefit of the system increases remarkably in the early stage of the search as the search progresses i e the increase in k the increase in the power generation benefit tends to slow down when k reaches a critical value e g k the power generation benefit stops increasing i e once the value of k exceeds k the optimal dipole combination starts becoming an invalid disturbance hence the final value of k is k and is typically determined by trials thus the release trajectory of the reservoir system is continuously updated using eq 14 recursively from k 0 to k k 3 5 implementation steps and flowchart of dop the implementation details of the dop are introduced step by step as follows step 1 obtain an initial release trajectory of the crs using a traditional method such as the equal flow regulation method step 2 construct a dipole optimization problem based on the current release trajectory of the system and use dp to solve it to determine the optimal dipole combination step 3 use the perturbation mechanism introduced in section 3 4 to search in the m dimensional subspace determined by the optimal dipole combination and update the release trajectory of the system using eq 14 step 4 perform step 2 and step 3 iteratively until the specified accuracy is attained and then output the solution to start the dop it is necessary to initialize the parameters and variables used by the procedure parameter initialization refers to assigning values to ri i 1 2 m the convergence precision limit lim and the maximum number of iterations num similarly variable initialization refers to specifying a feasible release trajectory for the crs after the initialization the dop proceeds as illustrated in fig 2 as the computational and memory requirements of the dop are mainly from the solution of the dipole optimization problem the time and space complexities of the dop are the same as those of dp namely o m t 4 and o m t 2 according to the previous analysis 4 case study 4 1 hypothetical four reservoir problem a hypothetical four reservoir problem was used as the basis for testing the performance of the dop this problem was introduced by larson 1968 as an illustrative example for the state increment dynamic programming since its introduction the problem has attracted the attention of researchers and has been used frequently to test the performance of algorithms such as constrained differential dynamic programming murray and yakowitz 1979 ga wardlaw and sharif 1999 folded dynamic programming fdp kumar and baliarsingh 2003 harmony search optimization algorithm kougias and theodossiou 2013 and constrained coral reefs optimization algorithm with machine learning emami et al 2021 the problem repeatedly served as a test case because its optimal solution is accessible and can be obtained using the simplex method the value of the objective function corresponding to the optimal solution is 401 3 the hypothetical four reservoir problem involves four adjustable reservoirs whose topological relationships are illustrated in fig 3 as presented in fig 3 the four reservoirs constitute a complex series parallel system which is representative of typical reservoir systems complex runoff compensation relationships exist among the reservoirs in any series parallel system because a reservoir release is used both by itself and by its downstream reservoirs the four reservoirs are responsible for power generation and reservoir 4 is additionally responsible for irrigation the operation period is a year consisting of twelve time periods i e months the objective function that maximizes the total benefits of the system can be expressed as 15 max r i t i 1 4 t 1 12 c i t r i t where ci t benefit function of reservoir i at time period t whose value has been reported by murray and yakowitz 1979 inflows are defined for reservoirs 1 and 3 at each time period the inflow to reservoir 1 is 3 units and that to reservoir 3 is 2 units the local inflows of the river are negligible the storages in reservoirs 1 2 3 and 4 are 5 units at the start of the operation period and the final storages are 5 5 5 and 7 units respectively the lower bounds on the reservoir storage and release are 0 at each time period the upper bounds on the storages in reservoirs 1 2 3 and 4 are 10 10 10 and 15 units respectively for the entire operation period and those on the releases are 4 4 3 and 7 units respectively the water balance equation can be expressed as 16 v 1 t v 1 t 1 q 1 t r 1 t v 2 t v 2 t 1 r 1 t r 2 t v 3 t v 3 t 1 q 3 t r 3 t v 4 t v 4 t 1 r 2 t r 3 t r 4 t this completes the problem specification the dddp enhanced poa and dp hybrid approach epoa dp chen 2021 poa dpsa fdp and real coded ga rga were analyzed and used to demonstrate the improvement in the poa performance a simple treatment of the series parallel system is necessary before applying the dop given the natural inflow process of the system the optimal release trajectories of reservoirs 2 3 and 4 are determined by the release trajectory of reservoir 1 and the optimal release trajectory of reservoir 4 is determined by the release trajectories of reservoirs 2 and 3 and has nothing to do with the release trajectory of reservoir 1 hence reservoirs 2 and 3 can become an aggregated whole i e an imaginary aggregate reservoir thus the series parallel system can be transformed into a crs expressed as 1 2 3 4 after this transformation the number of stages for the dipole optimization problem becomes three in this study the initial solution for the dop dddp epoa dp poa and dpsa was obtained from murray and yakowitz 1979 and the corresponding value of the objective function was 362 5 which is 90 33 of the known optimum the rga was set up with tournament selection elitism uniform crossover and uniform mutation the population size and maximum number of iterations were 100 and 1000 respectively the crossover and mutation probabilities were set to 0 7 and 0 02 respectively according to a parameter sensitivity analysis the dop dddp epoa dp poa dpsa fdp and rga obtained their optima after 33 72 38 3 2 10 and 1000 iterations respectively to eliminate the effect of uncertainties we tested each algorithm 10 times independently table 1 presents the optimal indexes of each algorithm ri 0 1 units for each reservoir performed using a personal computer equipped with an 8 gb ram and a 3 40 ghz intel core i5 cpu table 1 shows that the dop and dddp provided optimal solutions to the problem and the execution time of the former was acceptable and slightly longer than that of the latter the long execution time of the dop was caused by the aggregation operation performed on reservoirs 2 and 3 increasing the number of states at stage 2 in this case study the number of states for reservoir 2 or reservoir 3 was 133 but that for the aggregate reservoir was 1332 causing the computational effort of the dop to increase as shown in table 1 none of the epoa dp poa dpsa fdp and rga obtained the optimal solution of the problem and their returns were 99 51 96 20 98 82 99 33 and 96 72 of the known optimum respectively the execution time of dpsa was shorter than that of the other six algorithms and that of the poa was above 0 5 h which was much longer than that of the other six algorithms the execution times of the epoa dp fdp and rga were shorter than that of the dop but longer than that of dpsa the poa had a long execution time mainly because its computational effort was proportional to the fourth power of the number of discrete states of the reservoirs the storage trajectories related to the dop and dddp returns are plotted in fig 4 a same storage trajectory was obtained in 10 independent runs of the dop or dddp which shows that the storage trajectories produced from the two algorithms did not coincide completely the trajectories of reservoirs 3 and 4 were well matched while those of reservoirs 1 and 2 had minor differences at time periods 3 10 and 11 indicating that the hypothetical four reservoir problem is a multi solution problem fig 5 illustrates an analysis of the returns as a function of the number of iterations of each algorithm a same return sequence was obtained in 10 independent runs of the dop or dddp 4 2 real world four reservoir problem to further verify the performance of the dop we conducted a simulation on a real world crs on the yuan river china the yuan river is an important tributary of dongting lake in the yangtze river basin and is the second largest river in hunan province china with a main stream length of 1033 km total drop of 1 46 km basin area of 8 9 104 km2 and mean annual runoff of 6 68 1010 m3 as an important part of west hunan hydropower base one of the 14 largest hydropower bases in china the yuan river has the gross hydropower potential of 7 94 gw exploitable installed capacity of 5 94 gw and mean annual power generation of 2 76 1010 kwh in this study four dominated hydroelectric reservoirs under operation named sanbanxi baishi tuokou and wuqiangxi were selected to form a cascade system and the system structure is illustrated in fig 6 the four hydroelectric reservoirs are used for power generation while other functions include flood control and navigation sanbanxi is the leading hydroelectric reservoir located in the upper reaches of the main stream of the yuan river with multi yearly regulating ability it controls a catchment area of 11 100 km2 with a total storage capacity of 4 09 109 m3 and normal capacity of 3 75 109 m3 baishi is the fourth cascade hydroelectric reservoir planned on the main stream of the yuan river it controls a catchment area of 16 530 km2 with the mean annual runoff of 1 15 1010 m3 and annual generating capacity of 1 24 109 kwh tuokou is the lastly developed large scale hydroelectric reservoir in the yuan river basin with an installed capacity of 0 83 gw and annual generating capacity of 2 13 109 kwh wuqiangxi is located in the lower reaches of the main stream of the yuan river and controls 93 of the catchment area of the yuan river it is a key project for the cascade development of the yuan river basin with an installed capacity of 1 2 gw and annual generating capacity of 5 37 109 kwh other characteristic information for the four hydroelectric reservoirs is presented in table 2 the operation period was for a year with twelve time periods i e months the objective is to maximize the total power generation from the system which can be expressed as 17 max i 1 4 t 1 12 9 81 3600 η i q i t 0 5 h i t 1 f h i t f h i t b the constraints are the same as those in eqs 2 10 the simulation was performed under five inflow scenarios using the dop dddp epoa dp poa dpsa fdp and rga independently the initial solution for the dop dddp epoa dp dpsa and poa was obtained by maintaining a constant storage in each reservoir throughout the operation period we adopted the same rga parameters population size and maximum number of generations used for the hypothetical four reservoir problem the procedures were run on the same computer used for the hypothetical four reservoir problem and each algorithm was tested in 10 independent runs the optimal indexes of each algorithm are listed in table 3 ri 1 054 107 m3 for each reservoir corresponding to a flow step of 4 m3 s table 3 shows that the power generations derived from the dop and dddp were approximately equal while the relative deviation of the power generations derived from them was below 0 1 under each inflow scenario the execution time of the dop under each inflow scenario was shorter than that of dddp approximately 33 85 of that of the latter on average the exponential growth of the computational effort of dddp with m the time complexity of dddp is o t 32 m demonstrates that as new reservoirs are added to the system for optimization the execution time of dddp extends much longer than that of the dop the relative deviations of the power generations derived from the dop and epoa dp were below 0 1 under the 1st 2nd 4th and 5th inflow scenarios the execution time of the dop was shorter than that of the epoa dp under each inflow scenario approximately 35 81 of that of the latter on average although the power generation derived from the dop was 0 19 smaller than that from the epoa dp under the 3rd inflow scenario the execution time of the former was shorter than half of that of the latter furthermore when ri was 2 635 106 m3 the power generation derived from the dop under the 3rd inflow scenario became 8 3053 109 kwh and the execution time was 5 8 s thus under a shorter execution time the dop derived a power generation nearly equal to that derived from the epoa dp whose relative deviation was below 0 1 a major benefit of the dop over the epoa dp is that the time complexity of the former is irrelevant to n n number of discrete states of the reservoir whereas that of the latter grows proportionally with n 2 the time complexity of the epoa dp is o m t 2 n 2 with the increasing need for accuracy the execution time of the epoa dp will increase much longer than that of the dop the execution time of the poa was above 1 h under each inflow scenario which was much longer than that of the other six algorithms the longest execution time extended above 5 h under the 2nd inflow scenario the long execution time of the poa was caused by the power generation evaluations against all possible state combinations of the reservoirs in this case study the computational effort of the two stage solution of the poa was proportional to the fourth power of the number of discrete states of the reservoir overall the power generations derived from the poa were larger than those from the fdp and rga but smaller than those from the dop dddp epoa dp and dpsa approximately 1 14 smaller than those from the dop on average and the maximum reduction was 2 20 under the 3rd inflow scenario the execution time of dpsa was shorter than that of the other six algorithms under each inflow scenario overall the power generations derived from dpsa were larger than those from the poa fdp and rga but smaller than those from the dop dddp and epoa dp approximately 0 26 smaller than those from the dop on average and the maximum reduction was 0 33 under the 4th inflow scenario the fdp derived the minimum power generation among the seven algorithms under each inflow scenario approximately 7 86 smaller than that derived from the dop on average and the maximum reduction was 10 40 under the 3rd inflow scenario the execution time of the fdp was shorter than that of the poa but longer than that of the other five algorithms under each inflow scenario while the rga obtained the maximum power generation under the 1st inflow scenario its derived power generations under the other four inflow scenarios were only larger than those from the fdp but smaller than those from the other five algorithms approximately 5 08 smaller than those from the dop on average and the maximum reduction was 6 60 under the 3rd inflow scenario hence the solution quality of the rga was unstable the work progress of each algorithm under the 2nd inflow scenario is illustrated in fig 7 a same return sequence was obtained in 10 independent runs of each algorithm except the rga and the return sequence of the rga was taken the one that included the maximum objective value of 10 independent runs 5 conclusions two major problems limit the poa s performance in mroos and they are the dimensionality problem and blind search to enhance the poa s computing efficiency and solution quality we proposed an improved version of the poa referred to as the dop for the first problem a perturbation mechanism was introduced to replace the commonly used enumeration method in the two stage solution of the poa to prevent the numerical resolution of the objective function hence the computing efficiency was improved for the second problem a dvds was introduced to replace the svds used in the poa for obtaining more and better search directions so as to improve the solution quality of the poa two test cases were used to evaluate the dop s performance according to the results we developed the following conclusions 1 by using the dvds the benefit of the hypothetical four reservoir system and the power generation of the real world cascade reservoir system were improved by 3 95 and 1 16 on average respectively indicating that the dvds can effectively improve the solution quality of the poa 2 by using the perturbation mechanism the execution times of the poa for the hypothetical four reservoir problem and the real world cascade reservoir problem were shortened by 98 39 and 99 92 on average respectively indicating that the perturbation mechanism can effectively overcome the poa s dimensionality problem in two stage solutions and greatly improve the computing efficiency of the algorithm 3 the dop outperformed the available alternatives in terms of the quality of solutions and computing efficiency in the case studies the most promising advantage of the dop over existing available alternatives is that its computational effort is proportional to the number of reservoirs in the crs indicating that the dop can be applied to the operation optimization of a large scale crs with numerous reservoirs and has the potential to be applied to optimizing the operation of a large scale multi reservoir system with a more complex structure and properties although the dop outperformed the poa and five available alternatives in applications to a simple series parallel reservoir system and a real world crs it has not been tested on more complex reservoir systems therefore subsequent research should focus on extending the algorithm to a generic reservoir system with a more complex structure and properties another research direction is to derive the rule curves of real world reservoir systems to handle the uncertainty of runoff based on the simulation calculations with long series of historical runoff using the dop with the advances in remote sensing technologies climate system modeling yin et al 2021b and hydrological modeling he et al 2022 the uncertainty of runoff will be constantly reduced which will greatly benefit the power generation operation of reservoir systems in the future data availability statement the data and code generated or used during the study are available from the corresponding author upon reasonable request credit authorship contribution statement jia chen conceptualization methodology software writing original draft xinlong qi data curation formal analysis writing review editing gengfeng qiu supervision validation lei chen visualization investigation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
121,dynamic programming dp is an effective method for solving multi stage decision making problems and has been extensively applied to the optimization of reservoir operations however the method is limited by the dimensionality problem hence it cannot be directly applied to optimizing the operations of a reservoir system with more than three reservoirs at current computing power the progressive optimality algorithm poa is a classic variant of dp and has been widely used for optimizing multi reservoir operations despite the use of a static variable decoupling strategy to ease dp s dimensionality problem the poa s performance is reduced by an inherent drawback of the blind search of the static variable decoupling strategy and the dimensionality problem in two stage solutions to enhance the poa s performance we propose an improved version of the poa known as the dipole optimization procedure dop for optimizing cascade reservoir operations in the improved algorithm we use a dynamic variable decoupling strategy to obtain search directions that are more targeted than those of the poa so as to improve the quality of the solutions the dynamic decoupling of variables was achieved by constructing and solving a dipole optimization problem using a dp model developed in this study moreover a perturbation mechanism was introduced to address the poa s dimensionality problem so that the algorithm can be extended to larger reservoir systems the results of the simulation of a hypothetical four reservoir system and a real world cascade reservoir system showed the superiority of the dop over the poa and other five available alternatives the comparison was based on the quality of solutions and solving efficiency the results indicate that the dop is rational and feasible and has the potential to be applied to optimizing the operation of large scale multi reservoir systems with numerous reservoirs keyword cascade reservoirs operation optimization dimension reduction variable decoupling dipole optimization perturbation mechanism 1 introduction hydropower is a clean cheap and renewable energy source which is vital in the global energy structure and power system hydropower accounts for 15 8 of the global electricity production azad et al 2020 with the rapid growth of the global economy and population energy shortage and environmental pollution are becoming increasingly severe thus it is crucial to utilize hydropower resources rationally and scientifically for improving economic benefits alleviating energy crises and reducing greenhouse gas emissions sibtain et al 2021 as hydropower systems on rivers are typically cascaded jointly operating cascade reservoirs has become an important way of improving the utilization of hydropower resources in river basins the interactions of water heads and discharge between upstream and downstream reservoirs require attention because of the strong hydraulic coupling among various reservoirs in a cascade reservoir system crs the storage and discharge order of reservoirs should be determined with the aim of making full use of water head and avoiding abandoning water as far as possible the joint operation of cascade reservoirs is a complex optimization problem characterized by high dimensionality multiple stages and numerous constraint additionally climate warming alters the terrestrial energy budget and aggravates the uneven redistribution of water resources yin et al 2021a all these factors make the joint operation problem of cascade reservoirs difficult to obtain the global optimal solution therefore researchers are actively investigating an efficient optimization method for optimizing the joint operation of cascade reservoirs optimization methods are the key to both finding the optimal release trajectories of reservoir systems under deterministic inflow scenarios and deriving operational policies e g rule curves of reservoir systems against stochastic reservoir inflows over the past few decades a series of optimization methods have been developed for optimizing reservoir operations they can be categorized into two main classes traditional optimization methods represented by dynamic programming dp and modern intelligence algorithms represented by the genetic algorithm ga the former mainly includes linear programming shamloo et al 2021 nonlinear programming dogan et al 2021 dp danso et al 2021 and its various improved versions feng et al 2017 he et al 2021 ji et al 2021 kumar and baliarsingh 2003 network flow algorithm braga and barbosa 2001 and large scale system decomposition coordination method hou et al 2022 the latter covers a large variety of algorithms ga seetharam 2021 particle swarm optimization chen et al 2020 gravitational search algorithm niu et al 2021 water cycle algorithm yavari and robati 2021 and so on the advantages and drawbacks of these methods have been discussed by azad et al 2020 dobson et al 2019 giuliani et al 2021 and zhang et al 2015 dp is outstanding because of its attribute of providing the global optimal solution of an optimization problem without imposing special requirements e g analyticity linearity differentiability and convexity on objective and constraint functions however the dimensionality problem prevents the application of dp in multi reservoir operation optimizations mroos to extend the scope of applicability of dp researchers have proposed several dimension reduction techniques these techniques can be classified into two main categories parallel computing techniques for improving the computational efficiency of dp and improved versions of dp for reducing the computational effort of dp parallel computing cheng et al 2014 he et al 2019 ma et al 2021 zhang et al 2015 is an effective way of improving the computational efficiency of algorithms unlike the traditional serial computing mode parallel computing allocates various computing cells of an entire computing task that can be executed independently to multiple computing resources e g cpu cores or processors which simultaneously perform computations to improve computational efficiency a prominent benefit of parallel computing is that it improves the utilization of computing resources and helps to realize the full potential of multi core processors however the memory and computational requirements of dp do not decrease as parallel computing only exchanges space resources for time the computational cost of dp becomes unbearable as the problem size increases precisely the parallel dynamic programming will lose its applicability to daily impoundment operation when the number of reservoirs is more than three he et al 2019 the improved versions of dp are mainly typified by dynamic programming successive approximation dpsa larson and korsak 1970 discrete differential dynamic programming dddp heidari et al 1971 and progressive optimality algorithm poa howson and sancho 1975 these are successive approximation algorithms but differ in their dimension reduction mechanisms dddp adopts a neighborhood search strategy to reduce the number of states in dp models by searching in constantly changing neighborhoods better known as corridors around the current state trajectory which contain only a few of states usually three per stage the dpsa and poa adopt a static variable decoupling strategy svds to divide variables of the problem into several fixed groups and then optimize alternately the values of the variables of each group while fixing those of the remaining groups the strategy reduces the number of variables concerned per time while these algorithms have effectively eased dp s dimensionality problem they possess certain downsides that limit their performance in mroos for example dddp is an exponential time algorithm which is not applicable to large scale mroos feng et al 2017 additionally dpsa struggles to achieve global convergence when dealing with a non convex problem cheng et al 2014 the poa is a classic variant of dp proposed by howson and sancho 1975 and has been widely applied to mroos it reduces the dimension of problems through successive approximations using a general two stage solution which is essentially the svds owing to the inherent defects of the svds discussed in section 3 2 the poa s performance is significantly relegated in mroos the poa has two important limitations the dimensionality problem in two stage solutions and the blind search improvements to the poa have been proposed recently for example cheng et al 2012 developed the multi step poa to improve the convergence speed of the algorithm by reducing the number of computational stages zhang et al 2016 used a linear approximation technique to reduce the computational effort of the two stage solution of the poa feng et al 2018 contributed the uniform poa to relieve the poa s dimensionality problem through sampling in the state space of the system with a uniform design table jiang et al 2018 advanced the multi stage poa to improve the solution quality of the poa by expanding the two stage mode of the poa to a multi stage mode chen 2021 proposed an enhanced version of the poa to improve the solution quality and computing efficiency of the poa by adopting dp to solve the two stage poa problem ji et al 2021 introduced the nested poa to alleviate the poa s dimensionality problem in the context of water propagation impact between cascade reservoirs although the poa s performance has been considerably improved through these enhanced variants the algorithm still has much room for enhancement therefore we propose a dynamic variable decoupling strategy dvds discussed in section 3 3 to improve the solution quality of the poa we also discuss relevant research directions for improving the solution quality of the algorithm additionally we introduce a perturbation mechanism to address the poa s dimensionality problem in two stage solutions owing to our focus on improving the poa s performance so as to obtain better solutions under shorter time for simplifying the calculation the uncertainty of reservoir inflows was not accounted for in the cascade reservoir operation optimization problem formulation the rest of the paper is organized as follows section 2 describes the formulation of the long term power generation operation problem of a crs section 3 introduces the basic principle and major limitations of the poa and describes the proposed improved version of the poa section 4 presents two case studies that test the performance of the proposed algorithm finally section 5 outlines the main conclusions of the study 2 problem formulation 2 1 objective function the power generation operation of cascade reservoirs is typically aimed at maximizing the total power generation benefit of the system given the total available hydro energy e g ma et al 2021 or minimizing the total operation cost of the system toward supplying the required amount of electricity e g liao et al 2021 the former goal was prioritized in this study we consider a crs consisting of m reservoirs which are sequentially numbered 1 2 m from upstream to downstream assuming the operation period is divided into t identical time periods the objective function can be written as 1 max i 1 m t 1 t λ i t p i t δ t i 1 m t 1 t 9 81 3600 λ i t η i q i t 0 5 h i t 1 f h i t f h i t b where λi t energy price of reservoir i at time period t cny kwh pi t output of reservoir i at time period t kw δt length of a time period h qi t release through the turbine of reservoir i at time period t m3 i reservoir index t time period index ηi average efficiency of reservoir i h i t 1 f h i t f forebay elevations of reservoir i at the beginning and end of time period t respectively m h i t b average tailrace elevation of reservoir i at time period t m 2 2 constraints 2 2 1 continuity equation 2 v i t v i t 1 q i t r i 1 t r i t l i t r i t q i t s i t where vi t 1 vi t storages in reservoir i at the beginning and the end of time period t respectively m3 qi t and ri 1 t independent and dependent inflows to reservoir i at time period t respectively m3 ri t release of reservoir i at time period t m3 and r 0 t 0 li t evaporation loss of reservoir i at time period t m3 si t spillage of reservoir i at time period t m3 2 2 2 limits on reservoir storage 3 v i t min v i t v i t max where v i t min v i t max lower and upper bounds on the storage in reservoir i at time period t m3 2 2 3 limits on reservoir release 4 r i t min r i t r i t max where r i t min r i t max lower and upper bounds on the release of reservoir i at time period t m3 2 2 4 turbine discharge capacity 5 q i t min q i t q i t max where q i t min q i t max lower and upper bounds on the release through the turbine of reservoir i at time period t m3 2 2 5 power output bounds 6 p i t min p i t p i t max where p i t min p i t max lower and upper bounds on the power output of reservoir i at time period t kw 2 2 6 total power output bounds 7 p t min i 1 m p i t p t max where p t min p t max lower and upper bounds on the total power output from the cascade reservoir system at time period t kw 2 2 7 elevation storage curve 8 h i t f f v i t where f vi t forebay elevation function of reservoir i which varies with reservoir storage vi t m 2 2 8 elevation release curve 9 h i t b h r i t where h ri t tailrace elevation function of reservoir i which varies with reservoir release ri t m 2 2 9 limits on initial and final storage 10 v i 0 v i i v i t v i f where v i i v i f initial and final storages in reservoir i which are typically specified at the beginning of the operation period m3 3 improved poa dop 3 1 basic principle of poa the poa is an effective optimization technique developed by howson and sancho 1975 to alleviate dp s dimensionality problem the alleviation is based on a prior deduction from bellman s principle of optimality which states that the optimal path has the property that each pair of decision sets is optimal in relation to its initial and terminal values according to this theory the poa decomposes a t stage m reservoir problem into t 1 two stage m reservoir sub problems and then successively and iteratively solves the two stage sub problems to approximate to the solution of the entire problem the poa is detailed in the work of howson and sancho 1975 the poa s dimension reduction is essentially a svds as the state variables of the problem are divided into t 1 fixed groups that is the members of each group remain unchanged throughout the entire optimization process and then the algorithm alternately optimizes the values of the variables of each group while fixing those of the remaining t 2 groups as only the values of a few variables are optimized while those of the remainders are fixed in each sub problem solution the computational burden is reduced the time and space complexities of the poa are o t nm and o nm respectively where n number of the discrete states of a reservoir 3 2 major challenges of poa the poa has a good convergence and is robust to various problems which makes it a popular mroo technique however some drawbacks limit the performance of the technique in mroos a critical drawback is the blind search due to the svds from the poa search mechanism the algorithm only searches in the t 1 fixed m dimensional subspaces produced from the t 1 variable groups and does not compare and evaluate the variables throughout the entire optimization process this considerably limits the algorithm search space and inevitably affects the quality of the solutions another drawback of the poa is the dimensionality problem in two stage solutions since no universal solution method is available for different types of two stage poa problems a reliable and commonly used approach is to discretize the continuous state space of each reservoir into a finite set of state values and then evaluate the objective function of the problem against all possible state combinations of various reservoirs i e enumeration this inevitably increases the computational requirements exponentially with the problem size as with dp this drawback severely limits the applicability of the poa to large reservoir systems with numerous reservoirs 3 3 dvds 3 3 1 basic idea of dvds the above analysis shows that the key idea of the svds is to decompose the m t 1 dimensional solution space into t 1 fixed m dimensional subspaces and then successively approximate to the solution of the problem by searching alternately in the t 1 subspaces actually such m dimensional subspaces are generally much more than the t 1 subspaces because the subspace can be obtained from a combination of arbitrary m variables of the problem the number of combinations is m t 1 m therefore using the svds to decompose the solution space will inevitably result in a loss of numerous search directions which will ultimately affect the solution quality of the poa to obtain more search directions we suggest a dvds for the poa for clarity we formulize the relevant problems discussed in the rest of this paper in terms of decision variables i e reservoir releases in the dvds 2 m decision variables two for each reservoir are dynamically extracted from the m t decision variables of the problem to build an m dimensional subspace of the solution space the algorithm then searches in the subspace to continually improve the power generation benefit of the system the process is repeated until the release trajectory of the system reaches a steady state the variable extraction is achieved by constructing and solving a dipole optimization problem aimed at the maximal increase in the power generation benefit of the system the increase in power generation benefit is achieved by disturbing the values of the extracted variables by a small increment while fixing those of the remaining variables i e the combination of variables extracted to build the subspace has the most considerable influence on the power generation benefit of the system although the dop s dimension reduction idea originates from the poa the techniques are distinct the main difference between the dop and poa is that the latter divides the variables of the problem into t 1 fixed groups beforehand and then selects a variable group including 2 m decision variables to generate a search subspace at a time however the former does not group the variables but dynamically extracts 2 m variables from the variable set of the problem after evaluating the sensibility of the objective function to various combinations of 2 m variables to produce a search subspace at a time by contrast the dop has more as well as more targeted search directions than the poa which can compensate for the latter s defect of blind search we now introduce the detailed design of the dop 3 3 2 dipole optimization problem 1 definition of dipole pair according to the maximum power generation benefit model of cascade reservoirs described in section 2 the total release of reservoir i over the entire operation period can be expressed as the following recursive relationship 11 w i t 1 t r i t v i i v i f t 1 t q i t l i t w i 1 w 0 0 where v i i v i f qi t and li t are known thus wi can be calculated implying that the total release of each reservoir over the entire operation period stays constant in a crs thus for any reservoir i i 1 2 m an increase in release at a time period causes a decrease in release at another time period and the increment is equal to the decrement according to the above principle we introduce a disturbance on the release trajectory of reservoir i for example by increasing the release at time period j while reducing the release at time period k by an equal magnitude of ri where ri is the disturbance magnitude corresponding to reservoir i and takes a small positive value for other reservoirs in the system we also perform an operation similar to that of reservoir i owing to the disturbance superimposed on the release trajectory of the crs the power generation benefit of the system changes the disturbance is referred to as an effective disturbance when the power generation benefit increases otherwise it is an invalid disturbance we define the disturbances as a dipole pair of reservoir i expressed as ji ki ri or more succinctly expressed as ji ki because the disturbances acting on the releases of reservoir i at time periods j and k are equal in magnitude but opposite in direction 2 objective function exerting an effective disturbance on the release trajectory of a crs can typically improve the power generation benefit of the system and the more effective the disturbance the larger the power generation benefit as ji and ki in ji ki ri can take any value from the integer set 1 2 t the total number of dipole pairs for reservoir i is t t 1 1 all the cases of ji ki correspond to one dipole pair considering all possible dipole pairs of every reservoir we obtain the total number of combinations of dipole pairs for a crs consisting of m reservoirs as t t 1 1 m one of these combinations must produce the largest power generation benefit by acting on the release trajectory of the system called the optimal dipole combination we intend to find the optimal dipole combination thus the dipole optimization problem can be written as follows taking the m 1 th iteration of the algorithm as an example 12 max 1 j i k i t e i 1 m t 1 t 9 81 3600 λ i t η i r i t m sign t j i k i δ r i s i t 0 5 h i t 1 f h i t f h i t b sign t j i k i 1 if t j i 1 if t k i 0 otherwise where ji ki variables of the problem namely the time period numbers in the dipole pair of reservoir i e power generation benefit of the system over the entire operation period cny r i t m optimal release of reservoir i at time period t at the m th iteration sign t ji ki sign function which takes 1 1 or 0 3 constraints the constraints that must be considered for the dipole optimization problem are the same as those in eqs 2 10 the essence of the dop is to find the optimal dipole combination by maximizing the m t terms in summation as shown in eq 12 and then repeatedly exert the disturbance i e the optimal dipole combination on the continually updated release trajectory of the crs to improve the quality of solutions the release trajectory of the crs reaches a steady state after the dipole optimization problems are repeatedly constructed and solved while different subspaces determined by the optimal dipole combinations are continually searched 3 3 3 dynamic programming model for dipole optimization problem the main advantage of dp is that it establishes a recursive relationship according to bellman s principle of optimality that is any sub strategy of the optimal strategy is always the optimal to decompose the multi stage problem into multiple one stage sub problems and then obtains the optimal solution of the entire problem by determining that of each sub problem owing to the overlaps among various sub problems the calculation results of the previous sub problem can be directly called in solving the current one which prevents repeated calculations and improves solving efficiency dp has been widely used in different areas such as economic management production scheduling engineering technology and optimization control dp contains three major elements stage variables state variables and a recursive equation we used dp to solve the dipole optimization problem and the basic elements of dp for the problem are as follows 1 stage variables the operation period is typically divided into several stages a time period corresponds to a stage when using dp to solve the operation optimization problem of reservoir systems this division is performed because the operation decisions of the problem are typically made in a temporal order the solution of the dipole optimization problem comprises the optimal dipole pair of each reservoir which shows little relevance to time moreover in a crs the optimal release trajectory of a reservoir cannot be calculated until the release trajectory of its immediately upstream reservoir is determined thus the decisions corresponding to the dipole optimization problem are made in a spatial order therefore we consider a reservoir as a stage for the dipole optimization problem hence the stage variable is the reservoir index i for a crs consisting of m reservoirs the total number of stages for the dipole optimization problem is m 2 state variables a state variable is a set of variables that can completely describe all possible states of a system thus we take ji ki the dipole pair of reservoir i as the state variable of the dipole optimization problem analyzing the characteristics of crss shows that the optimal states of reservoirs i 1 i 2 m only depend on the state of reservoir i and are irrelevant to those of reservoirs 1 2 i 1 that is j i 1 k i 1 j i 2 k i 2 j m k m the optimal state sequence after stage i only depends on ji ki the state at stage i and has nothing to do with j 1 k 1 j 2 k 2 ji 1 ki 1 the path to ji ki thus taking ji ki as the state variable of the problem follows the markov property according to the previous analysis the number of states for each stage of the dipole optimization problem is t t 1 1 3 recursive equation we now use a forward recursion procedure of dp to solve the problem after defining the stage and state variables the recursive equation is as follows 13 g i j i k i max 1 j i 1 k i 1 t t 1 t 9 81 3600 λ i t η i r i t m sign t j i k i δ r i s i t 0 5 h i t 1 f h i t f h i t b g i 1 j i 1 k i 1 g 0 j 0 k 0 0 for each ji ki where g i 1 j i 1 k i 1 optimal cumulative power generation benefit over stage 1 to stage i 1 for state ji 1 ki 1 cny the detailed process of solving the dipole optimization problem using dp is shown in fig 1 as illustrated in fig 1 dp decomposes the original m stage problem into m overlapping one stage sub problems using eq 13 which prevents repeated calculations and improves solving efficiency because each overlap among the sub problems is calculated only once since each reservoir has n n t 2 t 1 states and at each stage the power generation benefit is evaluated at most n 2 times the time and space complexities of dp for solving the dipole optimization problem are o m t 4 and o m t 2 respectively 3 4 searching in m dimensional subspaces using perturbation mechanism we determine the combination of 2 m variables that has the most considerable influence on the power generation benefit of the system the final task of the dop is to search in the m dimensional subspace generated from the extracted variable combination to improve the power generation benefit of the system to avoid the dimensionality problem of the enumeration method we introduce a perturbation mechanism into the algorithm for searching in the subspaces assuming the optimal dipole combination can be expressed as j i k i i 1 2 m the update equation for reservoir releases can be written as taking the m 1 th iteration of the algorithm as an example 14 k 1 r i t k r i t sign t j i k i δ r i 0 r i t r i t m for any combination of i and t i 1 2 m t 1 2 t where k number of exerting disturbances set to 0 at the beginning of the search typically the power generation benefit of the system increases remarkably in the early stage of the search as the search progresses i e the increase in k the increase in the power generation benefit tends to slow down when k reaches a critical value e g k the power generation benefit stops increasing i e once the value of k exceeds k the optimal dipole combination starts becoming an invalid disturbance hence the final value of k is k and is typically determined by trials thus the release trajectory of the reservoir system is continuously updated using eq 14 recursively from k 0 to k k 3 5 implementation steps and flowchart of dop the implementation details of the dop are introduced step by step as follows step 1 obtain an initial release trajectory of the crs using a traditional method such as the equal flow regulation method step 2 construct a dipole optimization problem based on the current release trajectory of the system and use dp to solve it to determine the optimal dipole combination step 3 use the perturbation mechanism introduced in section 3 4 to search in the m dimensional subspace determined by the optimal dipole combination and update the release trajectory of the system using eq 14 step 4 perform step 2 and step 3 iteratively until the specified accuracy is attained and then output the solution to start the dop it is necessary to initialize the parameters and variables used by the procedure parameter initialization refers to assigning values to ri i 1 2 m the convergence precision limit lim and the maximum number of iterations num similarly variable initialization refers to specifying a feasible release trajectory for the crs after the initialization the dop proceeds as illustrated in fig 2 as the computational and memory requirements of the dop are mainly from the solution of the dipole optimization problem the time and space complexities of the dop are the same as those of dp namely o m t 4 and o m t 2 according to the previous analysis 4 case study 4 1 hypothetical four reservoir problem a hypothetical four reservoir problem was used as the basis for testing the performance of the dop this problem was introduced by larson 1968 as an illustrative example for the state increment dynamic programming since its introduction the problem has attracted the attention of researchers and has been used frequently to test the performance of algorithms such as constrained differential dynamic programming murray and yakowitz 1979 ga wardlaw and sharif 1999 folded dynamic programming fdp kumar and baliarsingh 2003 harmony search optimization algorithm kougias and theodossiou 2013 and constrained coral reefs optimization algorithm with machine learning emami et al 2021 the problem repeatedly served as a test case because its optimal solution is accessible and can be obtained using the simplex method the value of the objective function corresponding to the optimal solution is 401 3 the hypothetical four reservoir problem involves four adjustable reservoirs whose topological relationships are illustrated in fig 3 as presented in fig 3 the four reservoirs constitute a complex series parallel system which is representative of typical reservoir systems complex runoff compensation relationships exist among the reservoirs in any series parallel system because a reservoir release is used both by itself and by its downstream reservoirs the four reservoirs are responsible for power generation and reservoir 4 is additionally responsible for irrigation the operation period is a year consisting of twelve time periods i e months the objective function that maximizes the total benefits of the system can be expressed as 15 max r i t i 1 4 t 1 12 c i t r i t where ci t benefit function of reservoir i at time period t whose value has been reported by murray and yakowitz 1979 inflows are defined for reservoirs 1 and 3 at each time period the inflow to reservoir 1 is 3 units and that to reservoir 3 is 2 units the local inflows of the river are negligible the storages in reservoirs 1 2 3 and 4 are 5 units at the start of the operation period and the final storages are 5 5 5 and 7 units respectively the lower bounds on the reservoir storage and release are 0 at each time period the upper bounds on the storages in reservoirs 1 2 3 and 4 are 10 10 10 and 15 units respectively for the entire operation period and those on the releases are 4 4 3 and 7 units respectively the water balance equation can be expressed as 16 v 1 t v 1 t 1 q 1 t r 1 t v 2 t v 2 t 1 r 1 t r 2 t v 3 t v 3 t 1 q 3 t r 3 t v 4 t v 4 t 1 r 2 t r 3 t r 4 t this completes the problem specification the dddp enhanced poa and dp hybrid approach epoa dp chen 2021 poa dpsa fdp and real coded ga rga were analyzed and used to demonstrate the improvement in the poa performance a simple treatment of the series parallel system is necessary before applying the dop given the natural inflow process of the system the optimal release trajectories of reservoirs 2 3 and 4 are determined by the release trajectory of reservoir 1 and the optimal release trajectory of reservoir 4 is determined by the release trajectories of reservoirs 2 and 3 and has nothing to do with the release trajectory of reservoir 1 hence reservoirs 2 and 3 can become an aggregated whole i e an imaginary aggregate reservoir thus the series parallel system can be transformed into a crs expressed as 1 2 3 4 after this transformation the number of stages for the dipole optimization problem becomes three in this study the initial solution for the dop dddp epoa dp poa and dpsa was obtained from murray and yakowitz 1979 and the corresponding value of the objective function was 362 5 which is 90 33 of the known optimum the rga was set up with tournament selection elitism uniform crossover and uniform mutation the population size and maximum number of iterations were 100 and 1000 respectively the crossover and mutation probabilities were set to 0 7 and 0 02 respectively according to a parameter sensitivity analysis the dop dddp epoa dp poa dpsa fdp and rga obtained their optima after 33 72 38 3 2 10 and 1000 iterations respectively to eliminate the effect of uncertainties we tested each algorithm 10 times independently table 1 presents the optimal indexes of each algorithm ri 0 1 units for each reservoir performed using a personal computer equipped with an 8 gb ram and a 3 40 ghz intel core i5 cpu table 1 shows that the dop and dddp provided optimal solutions to the problem and the execution time of the former was acceptable and slightly longer than that of the latter the long execution time of the dop was caused by the aggregation operation performed on reservoirs 2 and 3 increasing the number of states at stage 2 in this case study the number of states for reservoir 2 or reservoir 3 was 133 but that for the aggregate reservoir was 1332 causing the computational effort of the dop to increase as shown in table 1 none of the epoa dp poa dpsa fdp and rga obtained the optimal solution of the problem and their returns were 99 51 96 20 98 82 99 33 and 96 72 of the known optimum respectively the execution time of dpsa was shorter than that of the other six algorithms and that of the poa was above 0 5 h which was much longer than that of the other six algorithms the execution times of the epoa dp fdp and rga were shorter than that of the dop but longer than that of dpsa the poa had a long execution time mainly because its computational effort was proportional to the fourth power of the number of discrete states of the reservoirs the storage trajectories related to the dop and dddp returns are plotted in fig 4 a same storage trajectory was obtained in 10 independent runs of the dop or dddp which shows that the storage trajectories produced from the two algorithms did not coincide completely the trajectories of reservoirs 3 and 4 were well matched while those of reservoirs 1 and 2 had minor differences at time periods 3 10 and 11 indicating that the hypothetical four reservoir problem is a multi solution problem fig 5 illustrates an analysis of the returns as a function of the number of iterations of each algorithm a same return sequence was obtained in 10 independent runs of the dop or dddp 4 2 real world four reservoir problem to further verify the performance of the dop we conducted a simulation on a real world crs on the yuan river china the yuan river is an important tributary of dongting lake in the yangtze river basin and is the second largest river in hunan province china with a main stream length of 1033 km total drop of 1 46 km basin area of 8 9 104 km2 and mean annual runoff of 6 68 1010 m3 as an important part of west hunan hydropower base one of the 14 largest hydropower bases in china the yuan river has the gross hydropower potential of 7 94 gw exploitable installed capacity of 5 94 gw and mean annual power generation of 2 76 1010 kwh in this study four dominated hydroelectric reservoirs under operation named sanbanxi baishi tuokou and wuqiangxi were selected to form a cascade system and the system structure is illustrated in fig 6 the four hydroelectric reservoirs are used for power generation while other functions include flood control and navigation sanbanxi is the leading hydroelectric reservoir located in the upper reaches of the main stream of the yuan river with multi yearly regulating ability it controls a catchment area of 11 100 km2 with a total storage capacity of 4 09 109 m3 and normal capacity of 3 75 109 m3 baishi is the fourth cascade hydroelectric reservoir planned on the main stream of the yuan river it controls a catchment area of 16 530 km2 with the mean annual runoff of 1 15 1010 m3 and annual generating capacity of 1 24 109 kwh tuokou is the lastly developed large scale hydroelectric reservoir in the yuan river basin with an installed capacity of 0 83 gw and annual generating capacity of 2 13 109 kwh wuqiangxi is located in the lower reaches of the main stream of the yuan river and controls 93 of the catchment area of the yuan river it is a key project for the cascade development of the yuan river basin with an installed capacity of 1 2 gw and annual generating capacity of 5 37 109 kwh other characteristic information for the four hydroelectric reservoirs is presented in table 2 the operation period was for a year with twelve time periods i e months the objective is to maximize the total power generation from the system which can be expressed as 17 max i 1 4 t 1 12 9 81 3600 η i q i t 0 5 h i t 1 f h i t f h i t b the constraints are the same as those in eqs 2 10 the simulation was performed under five inflow scenarios using the dop dddp epoa dp poa dpsa fdp and rga independently the initial solution for the dop dddp epoa dp dpsa and poa was obtained by maintaining a constant storage in each reservoir throughout the operation period we adopted the same rga parameters population size and maximum number of generations used for the hypothetical four reservoir problem the procedures were run on the same computer used for the hypothetical four reservoir problem and each algorithm was tested in 10 independent runs the optimal indexes of each algorithm are listed in table 3 ri 1 054 107 m3 for each reservoir corresponding to a flow step of 4 m3 s table 3 shows that the power generations derived from the dop and dddp were approximately equal while the relative deviation of the power generations derived from them was below 0 1 under each inflow scenario the execution time of the dop under each inflow scenario was shorter than that of dddp approximately 33 85 of that of the latter on average the exponential growth of the computational effort of dddp with m the time complexity of dddp is o t 32 m demonstrates that as new reservoirs are added to the system for optimization the execution time of dddp extends much longer than that of the dop the relative deviations of the power generations derived from the dop and epoa dp were below 0 1 under the 1st 2nd 4th and 5th inflow scenarios the execution time of the dop was shorter than that of the epoa dp under each inflow scenario approximately 35 81 of that of the latter on average although the power generation derived from the dop was 0 19 smaller than that from the epoa dp under the 3rd inflow scenario the execution time of the former was shorter than half of that of the latter furthermore when ri was 2 635 106 m3 the power generation derived from the dop under the 3rd inflow scenario became 8 3053 109 kwh and the execution time was 5 8 s thus under a shorter execution time the dop derived a power generation nearly equal to that derived from the epoa dp whose relative deviation was below 0 1 a major benefit of the dop over the epoa dp is that the time complexity of the former is irrelevant to n n number of discrete states of the reservoir whereas that of the latter grows proportionally with n 2 the time complexity of the epoa dp is o m t 2 n 2 with the increasing need for accuracy the execution time of the epoa dp will increase much longer than that of the dop the execution time of the poa was above 1 h under each inflow scenario which was much longer than that of the other six algorithms the longest execution time extended above 5 h under the 2nd inflow scenario the long execution time of the poa was caused by the power generation evaluations against all possible state combinations of the reservoirs in this case study the computational effort of the two stage solution of the poa was proportional to the fourth power of the number of discrete states of the reservoir overall the power generations derived from the poa were larger than those from the fdp and rga but smaller than those from the dop dddp epoa dp and dpsa approximately 1 14 smaller than those from the dop on average and the maximum reduction was 2 20 under the 3rd inflow scenario the execution time of dpsa was shorter than that of the other six algorithms under each inflow scenario overall the power generations derived from dpsa were larger than those from the poa fdp and rga but smaller than those from the dop dddp and epoa dp approximately 0 26 smaller than those from the dop on average and the maximum reduction was 0 33 under the 4th inflow scenario the fdp derived the minimum power generation among the seven algorithms under each inflow scenario approximately 7 86 smaller than that derived from the dop on average and the maximum reduction was 10 40 under the 3rd inflow scenario the execution time of the fdp was shorter than that of the poa but longer than that of the other five algorithms under each inflow scenario while the rga obtained the maximum power generation under the 1st inflow scenario its derived power generations under the other four inflow scenarios were only larger than those from the fdp but smaller than those from the other five algorithms approximately 5 08 smaller than those from the dop on average and the maximum reduction was 6 60 under the 3rd inflow scenario hence the solution quality of the rga was unstable the work progress of each algorithm under the 2nd inflow scenario is illustrated in fig 7 a same return sequence was obtained in 10 independent runs of each algorithm except the rga and the return sequence of the rga was taken the one that included the maximum objective value of 10 independent runs 5 conclusions two major problems limit the poa s performance in mroos and they are the dimensionality problem and blind search to enhance the poa s computing efficiency and solution quality we proposed an improved version of the poa referred to as the dop for the first problem a perturbation mechanism was introduced to replace the commonly used enumeration method in the two stage solution of the poa to prevent the numerical resolution of the objective function hence the computing efficiency was improved for the second problem a dvds was introduced to replace the svds used in the poa for obtaining more and better search directions so as to improve the solution quality of the poa two test cases were used to evaluate the dop s performance according to the results we developed the following conclusions 1 by using the dvds the benefit of the hypothetical four reservoir system and the power generation of the real world cascade reservoir system were improved by 3 95 and 1 16 on average respectively indicating that the dvds can effectively improve the solution quality of the poa 2 by using the perturbation mechanism the execution times of the poa for the hypothetical four reservoir problem and the real world cascade reservoir problem were shortened by 98 39 and 99 92 on average respectively indicating that the perturbation mechanism can effectively overcome the poa s dimensionality problem in two stage solutions and greatly improve the computing efficiency of the algorithm 3 the dop outperformed the available alternatives in terms of the quality of solutions and computing efficiency in the case studies the most promising advantage of the dop over existing available alternatives is that its computational effort is proportional to the number of reservoirs in the crs indicating that the dop can be applied to the operation optimization of a large scale crs with numerous reservoirs and has the potential to be applied to optimizing the operation of a large scale multi reservoir system with a more complex structure and properties although the dop outperformed the poa and five available alternatives in applications to a simple series parallel reservoir system and a real world crs it has not been tested on more complex reservoir systems therefore subsequent research should focus on extending the algorithm to a generic reservoir system with a more complex structure and properties another research direction is to derive the rule curves of real world reservoir systems to handle the uncertainty of runoff based on the simulation calculations with long series of historical runoff using the dop with the advances in remote sensing technologies climate system modeling yin et al 2021b and hydrological modeling he et al 2022 the uncertainty of runoff will be constantly reduced which will greatly benefit the power generation operation of reservoir systems in the future data availability statement the data and code generated or used during the study are available from the corresponding author upon reasonable request credit authorship contribution statement jia chen conceptualization methodology software writing original draft xinlong qi data curation formal analysis writing review editing gengfeng qiu supervision validation lei chen visualization investigation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
122,two dimensional 2d depth averaged shallow water equations swe are widely used to model unsteady free surface flows such as flooding processes including those due to dam break or levee breach however the basic hypothesis of small bottom slopes may be far from satisfied in certain practical circumstances both locally at geometric singularities and even in wide portions of the floodable area such as in mountain regions in these cases the classic 2d swe might provide inaccurate results and the steep slope shallow water equations ssswe in which the restriction of small bottom slopes is relaxed are a valid alternative modeling option however different 2d formulations of this set of equations can be found in the geophysical flow literature in both global horizontally oriented and local bottom oriented coordinate systems in this paper a new ssswe model is presented in which water depth is defined along the vertical direction and flow velocity is assumed parallel to the bottom surface this choice of the dependent variables combines the advantages of considering the flow velocity parallel to the bottom as can be expected in gradually varied shallow flow and handling vertical water depths consistent with elevation data usually available as digital terrain models the pressure distribution is assumed linear along the vertical direction and flow curvature effects are neglected a new formulation of the 2d depth averaged ssswe is derived in which the two dynamic equations represent momentum balances along two spatial directions parallel to the bottom whose horizontal projections are parallel to two fixed orthogonal coordinate directions the analysis of the mathematical properties of the new ssswe equations shows that they are strictly hyperbolic for wet bed conditions and reduce to the conventional 2d swe when bottom slopes are small finally it is shown that the ssswe predict a slower flow compared with the conventional swe in the theoretical case of a 1d dam break on a frictionless channel with fixed slope the capabilities of the proposed model are demonstrated in a companion paper on the basis of numerical and experimental tests keywords basic flow equations free surface flow shallow water equations steep bottom slopes two dimensional depth averaged model 1 introduction the two dimensional 2d depth averaged free surface flow equations under the shallow water approximation are a standard mathematical model widely used to describe a variety of gravity driven geophysical flow phenomena including flooding due to dam break e g aureli et al 2008 wang et al 2011 petaccia and natale 2020 pilotti et al 2020 or levee breach e g viero et al 2013 dazzi et al 2019 d oria et al 2019 overland flows e g singh et al 2015 cea and bladé 2015 costabile et al 2020 mixed free surface pressurized flows maranzoni et al 2015 maranzoni and mignosa 2018 cea and lópez núñez 2021 tsunami propagation and inundation of coastal regions e g segur 2007 leveque et al 2011 tidal bore propagation in river estuaries e g pan et al 2007 atmospheric air currents over non flat terrain sivakumaran and dressler 1989 and based on the continuum mechanics approach even granular flows e g denlinger and iverson 2004 mangeney castelnau et al 2005 juez et al 2013 castro orgaz et al 2015 and snow avalanches e g christen et al 2010 barbolini et al 2000 as well as flows of fluid sediment mixtures such as debris or mud flows e g han and wang 1996 laigle and coussot 1997 rickenmann et al 2006 in all these application fields despite the limitations connected to the basic restrictive assumptions e g basco 1989 hu and meyer 2005 van emelen et al 2014 and the difficulties related to the calibration of the model parameters e g barbolini et al 2000 guinot and cappelaere 2009 the 2d depth averaged shallow water model is generally accepted to predict the main flow features for flood hazard assessment and flood risk management e g o brien et al 1993 nakagawa and takahashi 1997 aureli et al 2006 rickenmann et al 2006 gruber and bartelt 2007 xia et al 2011 d oria et al 2019 furthermore the use of 2d depth averaged numerical models for free surface flow simulations is currently facilitated by the increasing availability of efficient user oriented software teng et al 2017 sometimes freeware such as basement hec ras 2d and telemac 2d e g horritt and bates 2002 zischg et al 2018 pilotti et al 2020 sharma and regonda 2021 among the key assumptions of the classic 2d depth averaged shallow water equations swe there is the hypothesis of small bottom slopes e g chow 1959 henderson 1966 toro 2001 bottom slopes are usually considered small when less than 1 10 which corresponds to inclination angles less than approximately 6 chow 1959 p 33 according to this hypothesis the bottom surface can be considered practically horizontal and the flow depth can be indifferently measured along the vertical direction or the direction normal to the bottom moreover the flow velocity which is assumed uniform on the vertical depth is represented by two orthogonal horizontal components since the vertical component of the fluid acceleration can be neglected in comparison with gravity according to the shallow water approximation the pressure distribution is essentially hydrostatic in the vertical direction acheson 1990 toro 2001 castro orgaz and hager 2017 however the topography on which free surface flows occur is sometimes very steep and irregular with slopes greater than 1 10 such as in mountain regions or locally near geometric singularities in these contexts the small bottom slope assumption is violated and the classic depth averaged swe are no longer strictly valid firstly the vertical component of the fluid acceleration can be non negligible thereby significantly affecting the physical process and inducing three dimensional effects e g aureli et al 2015 horna munoz and constantinescu 2020 then the pressure distribution is non hydrostatic along the vertical juez et al 2013 castro orgaz and hager 2017 nevertheless 1d and 2d swe are commonly used in flood hazard analysis even in the presence of steep and deeply irregular topographies e g han and wang 1996 valiani et al 2002 begnudelli and sanders 2007 aureli et al 2008 pilotti et al 2011 wang et al 2011 de almeida et al 2012 aureli et al 2014 pilotti et al 2014 touma and kanbar 2018 the vertical pressure distribution is non hydrostatic also where flow curvature is significant in this case a depth averaged boussinesq type model could be used e g castro orgaz et al 2015 cantero chinchilla et al 2016 castro orgaz and hager 2017 cap 2 fabiani and ota 2019 cantero chincilla et al 2020 however these higher order effects are not considered in this paper for flows on steep topographies the steep slope shallow water equations ssswe could be considered as a valid modeling option because the effect of steep bottom slopes is included i e the restrictive hypothesis of small bottom slopes is removed maintaining the assumption of negligible fluid acceleration normal to the bottom in the one dimensional 1d framework there is general agreement on the formulation of the ssswe based on a local bottom oriented coordinate system following the fixed shape of the channel bottom in this local coordinate system the longitudinal axis is parallel to the bottom and the other axis is normal to it consistently flow velocity is assumed parallel to the channel bottom and flow depth is defined orthogonally to it in the resulting equations representing mass conservation and momentum balance along the sloping flow direction the effect of the bottom slope is included in both pressure and bottom terms in which trigonometric functions of the bed inclination angle appear e g berger 1994 fernandez feria 2006 takahashi 2007 ancey et al 2008 van emelen et al 2014 this approach is systematically adopted in 1d modeling of both flows in steep channels with fixed bottom slope e g savage and hutter 1989 berger 1994 keller 2003 fernandez feria 2006 ancey et al 2008 antuono and hogg 2009 and curvilinear flows in channels with curved bottom e g dressler 1978 castro orgaz and hager 2014 castro orgaz and hager 2016 conversely various 2d formulations of the ssswe have been proposed in the wide gravity driven flow literature although such equations always represent the basic principles of mass conservation and linear momentum balance in the 2d context the discrepancies between the existing 2d formulations are mainly due to the different approaches adopted in deriving the equations and the different ways of introducing the bottom slope effects in particular some approaches assume a local rectangular coordinate system with bottom oriented axes i e a local coordinate system following the terrain with two axes parallel to the bottom and the third axis locally orthogonal to it iverson and denlinger 2001 mcdougall and hungr 2004 medina et al 2008 juez et al 2013 whereas other approaches are based on a horizontally oriented coordinate system i e a classic fixed cartesian coordinate system in which two axes are horizontal and the third axis is vertical and aligned with the gravity vector chaudhry 1993 laigle 1997 denlinger and iverson 2004 rickenmann et al 2006 denlinger and o connel 2008 ni et al 2019 juez et al 2013 xia and liang 2018 in the models based on local bottom oriented coordinates flow depth is measured along the direction normal to the bottom and flow velocity is assumed parallel to the local terrain surface whereas in the models that use global horizontally oriented coordinates the water surface is identified by its vertical elevation above the bottom and flow velocity is described by two orthogonal horizontal components in the presence of steep bottom slopes the choice of a local bottom oriented coordinate system seems at a first glance better in shallow water flow analysis indeed in quasi parallel flows on sloping planar bottom surfaces it is natural to assume the flow velocity parallel to the bottom surface and the flow depth normal to it in this case according to the shallow water approximation the fluid acceleration component normal to the bottom is negligible and consequently the pressure distribution is hydrostatic on each cross section of the flow however the use of a local coordinate system following the topography can cause practical problems in defining the initial condition handling the topographic data juez et al 2013 p 203 or returning the results since topographic information is typically available through a digital elevation model which provides terrain elevation data with reference to a horizontal geodetic datum denlinger and iverson 2004 p 2 for this reason various formulations of the 2d ssswe in global horizontally oriented coordinates can be found in the literature which mainly differ in how the effect of bottom slope on pressure distribution is included in the model a comprehensive review of the 2d depth averaged ssswe models proposed in the literature is presented in appendix a it is worth noting that some of these models present serious critical issues in particular in some proposed formulations the pressure correction coefficient that appears in the equations to take into account the effect of steep bottom slope on vertical pressure distribution is not symmetric with respect to the exchange of the spatial coordinates see eqs a1 and a3 in appendix a contrary to what expected to combine the advantages of the two alternative approaches this paper proposes a new formulation of the 2d ssswe which uses the vertical water depth defined as the vertical distance of the water surface above the bottom and two flow velocity components parallel to the bottom as depending variables the independent variables are two orthogonal horizontal coordinates and time using spatial dependent or independent variables defined with respect to a conventional horizontally oriented cartesian reference frame allows for efficient and direct manipulation of the data of a digital terrain model the depth averaged governing equations are derived by applying the basic principles of mass conservation and linear momentum balance to a vertical column of incompressible water extending from the bottom to the free surface the pressure distribution is assumed linear non hydrostatic in the vertical direction neglecting the effects of flow curvature the eigenstructure and some special cases of the new set of equations are discussed in detail moreover the simple case of a 1d dam break on a frictionless sloping straight channel with fixed positive bottom slope is considered to perform a first comparison of the analytical results provided by the proposed ssswe model and the conventional swe model in the presence of high bottom slopes a more extensive comparison on the basis of validation and numerical test cases will be performed in a companion paper in which a finite volume numerical scheme will be used to solve the equations this paper is organized as follows section 2 presents the conceptual bases of the model the new formulation of the equations is presented in section 3 the mathematical properties of the equations are discussed in section 4 along with some special cases the 1d dam break problem on a frictionless sloping channel with fixed slope is analyzed in section 5 concluding remarks are drawn in section 6 finally a review of the existing formulations of the 2d ssswe is provided in appendix a while the detailed derivation of the equations and their eigenstructure are presented in appendixes b and c respectively 2 conceptual bases of the model using a local bottom oriented coordinate system e g gray et al 1999 iverson and denlinger 2001 the shallow water equations are expressed in terms of flow depth measured normally to the bottom and depth averaged velocity components locally parallel to the bottom however defining the flow depth orthogonally to the bottom can cause some practical difficulties for example in fig 1 which sketches the initial condition of the dam break problem in a sloping channel straight line aa traced orthogonally from the bottom immediately upstream of the dam intersects the dam and does not reach the water surface in this case the depth of the water column does not represent correctly the water depth appearing in the equations to overcome this problem which for fixed dam height affects a longer channel stretch upstream of the dam as the bottom slope increases and avoid the ensuing mathematical complications in the analysis of the dam break problem fernandez feria 2006 the dam was assumed perpendicular to the bottom in some applications e g ancey et al 2008 although this configuration is unrealistic as a further example of such practical problems fig 2 shows that when the flow has concave curvature in the longitudinal vertical plane due to the curved bottom straight lines traced orthogonally from the bottom can intersect before reaching the free surface lines aa and bb in fig 2 causing an erroneous representation of the flow volume additionally since topographic information is commonly available as digital elevation data adopting flow depths measured perpendicularly from the terrain potentially requires laborious pre and post processing calculations to transform vertical depths into normal bottom depths and vice versa especially on irregular topography denlinger and iverson 2004 medina et al 2008 given these problems it seems reasonable to explore the possibility of modeling a shallow water flow on steep topography considering the flow depth defined along the vertical direction and the flow velocity parallel to the bottom surface accordingly the water depth h is defined with reference to the z axis aligned with the vertical direction and oriented upwards of a global fixed cartesian frame of reference oxyz while the flow velocity components u and v are defined with reference to the ξ and η directions of a local bottom oriented reference frame pξηζ in which axes ξ and η identify the tangent plane that approximates locally the bottom surface at each point p and axis ζ is locally normal to the bottom surface fig 3 axes ξ and η are characterized by the property that their horizontal projections have the same directions of orthogonal horizontal axes x and y of the fixed global frame of reference oxyz respectively i e ξ and η directions are locally tangent to the coordinate lines corresponding to directions x and y on the bottom surface respectively fig 3b denoting with i j and k the unit vectors of the x y and z axes of the global reference frame respectively the unit vectors of the ξ and η axes of the local reference frame with respect to the global one are respectively 1 ξ cos θ x i sin θ x k and η cos θ y j sin θ y k where θx and θy are the inclination angles of the bottom surface in the x and y directions respectively both these angles range between 90 and 90 excluding the extreme values and can be easily computed from the topographic data unit vectors ξ and η constitute a basis for the 2d vector space on the plane locally tangent to the surface bottom in p moreover axes ξ and η are in general non orthogonal because ξ η sin θ x sin θ y cos φ where φ denotes the angle between local directions ξ and η fig 3a they are orthogonal only when either θx 0 or θy 0 or both the unit vector of the ζ direction locally normal to the bottom surface at each point p can be represented in the global reference frame as 2 ζ tan θ x 1 ta n 2 θ x ta n 2 θ y i tan θ y 1 ta n 2 θ x ta n 2 θ y j 1 1 ta n 2 θ x ta n 2 θ y k accordingly the vertical component of the ζ unit vector i e ζ k is equal to the cosine of ψ which is the angle between the direction normal to the bottom and the vertical fig 3a based on this approach flow velocity v can be expressed as the sum of the two vector components u ξ and v η along the ξ and η directions respectively as 3 v u ξ v η or in the global reference frame 4 v u cos θ x i v cos θ y j u sin θ x v sin θ y k consequently the flow velocity magnitude is 5 v v v u 2 v 2 2 uv ξ η u 2 v 2 2 uv sin θ x sin θ y the two velocity components are assumed uniform on the vertical column of fluid as in the 2d depth averaged models the u and v orthogonal projections of flow velocity v in the ξ and η directions respectively can be expressed as a function of u and v as 6 u v ξ u v sin θ x sin θ y and v v η v u sin θ x sin θ y eq 4 shows that the horizontal velocity components along the orthogonal x and y directions are respectively 7 v x v i u cos θ x and v y v j v cos θ y and that the vertical component of the flow velocity is negligible only if both θx and θy are small i e the bottom is locally nearly horizontal flow variables h u and v are considered to be functions of horizontal spatial coordinates x and y as well as of time t as in the conventional 2d depth averaged swe 3 governing equations the new 2d ssswe are derived by applying the basic principles of mass conservation and linear momentum balance to a fixed infinitesimal parallelepiped control volume bounded by vertical faces and extending from the slanted bottom surface assumed planar and non erodible up to the free surface fig 4 the area of the parallelogram bottom of this infinitesimal control volume is 8 da d ξ ξ d η η d ξ d η sin φ d ξ d η 1 si n 2 θ x si n 2 θ y d ξ d η cos θ x cos θ y 1 ta n 2 θ x ta n 2 θ y where dξ and dη are the lengths of the sides of the bottom face and symbol denotes the vector product hence the area of the horizontal projection on the xy plane of the bottom of the control volume is 9 d ω da ζ k d ξ d η sin φ cos ψ d ξ d η cos θ x cos θ y dx dy in view of the geometric relations 10 d x d ξ cos θ x and d y d η cos θ y the vertical orientation of the control volume prevents superimpositions or distortions mancarella and hungr 2010 especially in curvilinear flows in the vertical plane fig 2 allowing the lateral faces of adjacent control volumes to match denlinger and iverson 2004 the momentum equations represent linear momentum balance along the inclined ξ and η directions consistently with the hypothesis of flow velocity locally parallel to the bottom surface the model is applied to incompressible inviscid water flow thus considering an isotropic stress tensor and neglecting internal viscous and turbulent stresses according to the assumption of gradually varied flow the pressure distribution is assumed linear on the lateral faces of the control volume neglecting the effect of flow curvature details on the derivation of the equations are reported in appendix b the resulting vertically averaged governing equations can be written in conservative form as 11 u t f x g y s with 12 u h uh vh f uh cos θ x uuh 1 2 kg h 2 cos θ x uvh cos θ x g vh cos θ y uvh cos θ y vvh 1 2 kg h 2 cos θ y s 0 gh s 0 ξ s f ξ gh s 0 η s f η in which u is the vector of conserved variables i e the vertical depth h and the uh and vh unit discharges in the ξ and η direction respectively f and g are the vectors of physical fluxes and s is the source term g is the gravity acceleration and k 1 is a pressure correction factor the bottom slopes are 13 s 0 ξ sin θ x and s 0 η sin θ y in the ξ and η direction respectively and the friction slopes along the same directions are 14 s f ξ n 2 u v h 4 3 1 ta n 2 θ x ta n 2 θ y and s f η n 2 v v h 4 3 1 ta n 2 θ x ta n 2 θ y where n is the manning roughness coefficient in eq 12 k describes the effect of the bottom slope on vertical pressure distribution actually for a parallel flow in a straight open channel of slope angle θ this distribution is no longer hydrostatic and pressure head at a vertical transverse section is equal to the vertical depth multiplied by the corrector factor cos2 θ as shown by chow 1959 p 33 and henderson 1966 p 28 this result also applies approximately to gradually varied flows and can be generalized to the 2d case setting 15 k 1 if v 0 cos 2 ψ 1 1 tan 2 θ x tan 2 θ y otherwise according to eq 15 the pressure correction factor is set to unity in static conditions when the pressure distribution is hydrostatic whereas in dynamic conditions k is assumed equal to the square cosine of the angle between the direction locally normal to the bottom and the vertical juez et al 2013 xia and liang 2018 ni et al 2019 over non horizontal bottom i e θx 0 and θy 0 k is discontinuous for v 0 actually according to the assumption that flow velocity is parallel to the bottom surface the vertical pressure distribution changes abruptly from hydrostatic as soon as the flow motion starts in the special case of a unidirectional uniform flow in a sloping channel with fixed slope in the longitudinal ξ direction i e θx const θy 0 ψ θx and k reduces to cos2 θx as expected in particular for θx tending to 90 k tends to 0 which is representative of a free falling fluid with a vertical pressure gradient equal to zero denlinger and iverson 2004 p 4 in a 2d flow on small bottom slopes i e θx 0 and θy 0 the value of k does not differ appreciably from unity and the pressure terms in eq 12 become practically hydrostatic as in the classic swe finally it is worth noting that based on the definition of eq 15 k is symmetric with respect to the two horizontal directions x and y because θx and θy are interchangeable to take into account the effect of streamline curvature on pressure distribution a further correction factor could be introduced in the pressure term chow 1959 p 33 however when the effect of curvature is significant such as in flows over curved bottom or flows with strongly curved water surface on even topography boussinesq type equations provide more accurate predictions since non linear terms are added to the conventional hydrostatic pressure term e g basco 1989 mohapatra and chaudhry 2004 kim and lynett 2011 cantero chinchilla et al 2016 castro orgaz and hager 2017 in eqs 11 12 dependent flow variables uh and vh are defined in terms of vertically averaged velocity components u and v however velocity components u and v also appear in the flux vectors inverting eq 6 u and v can be calculated from u and v as 16 u u v sin θ x sin θ y 1 sin 2 θ x sin 2 θ y u v cos φ sin 2 φ v v u sin θ x sin θ y 1 sin 2 θ x sin 2 θ y v u cos φ sin 2 φ the new ssswe eqs 11 12 present four main differences compared with the conventional swe i the flux terms depend on the local bottom slope through inclination angles θx and θy ii a correction factor which reduces the gravity effect appears in the pressure term this factor depends only on the local bottom slope iii the bottom source term is expressed as a function of the sine and not of the tangent of bottom inclination angles θx and θy actually the sine of an angle cannot be approximated with its tangent when the angle is not small iv friction stresses act on a bottom surface that is significantly larger than its horizontal projection for steep topographies and the friction term depends on the bottom slope 4 properties of the equations 4 1 hyperbolicity the character of the new equations can be determined by analyzing the eigenstructure of their homogeneous form in terms of the conserved variables toro 2001 this analysis is reported in appendix c which shows that the equations are strictly hyperbolic for a wet bed wave celerity is different in the ξ and η directions and varies with the local bottom inclination and flow velocity as well as the vertical flow depth eqs c4 and c5 in appendix c single elementary waves of small amplitude propagating in quiescent water k 1 over steep topography move with speed 17 c ξ g h cos θ x sin φ and c η g h cos θ y sin φ in the ξ and η direction respectively for a 1d flow in the ξ direction in a sloping channel of fixed slope i e θx const θy 0 and φ 90 the wave celerity is 18 c ξ gh cos θ x in which the gravity acceleration is corrected by the reduction coefficient cos2 θx dependent only on the channel slope van emelen et al 2014 in the limit case of small bottom slopes both c ξ and c η in eq 17 reduce to gh 1 2 which is the wave celerity in the conventional 2d swe 4 2 special cases the new set of equations reduces to the conventional 2d swe if bottom slopes are small i e θx θy 0 and consequently cosθx cosθy 1 sinθx tanθx sinθy tanθy and k 1 this simplification is obtained automatically in the regions of the flow domain where the topography is flat with gentle slopes in the special case of a 2d flow spreading on an inclined plane in which ξ is the coordinate along the direction of maximum slope i e θx const and θy 0 consequently φ 90 and ψ θx u and v are respectively equal to u and v in view of eq 6 and the system of eqs 11 12 becomes 19 h t u h cos θ x x v h y 0 u h t x u 2 h 1 2 k g h 2 cos θ x y u v h g h sin θ x n 2 u v h 4 3 1 ta n 2 θ x v h t x u v h cos θ x y v 2 h 1 2 k g h 2 g h n 2 v v h 4 3 1 ta n 2 θ x with 20 k 1 if v 0 cos 2 θ x otherwise the associated 1d ξ split equations i e v 0 of unsteady flow in a wide rectangular channel of fixed slope θx are 21 h t u h cos θ x x 0 u h t x u 2 h 1 2 k g h 2 cos θ x g h sin θ x n 2 u 2 h 4 3 1 ta n 2 θ x with k again expressed by eq 20 these equations are equivalent to the classic 1d ssswe in bottom oriented coordinates e g berger 1994 ancey et al 2008 van emelen et al 2014 as can be easily verified using elementary trigonometric transformations finally the new ssswe correctly represent the static condition indeed in this case u v 0 and k 1 then eqs 11 12 reduce to 22 h t 0 h x tan θ x h y tan θ y as expected in static conditions because the water surface is stationary and horizontal 5 1d dam break problem in a sloping channel let us consider the dam break problem in a frictionless rectangular channel of fixed bottom slope fig 5 in this case the proposed ssswe appear in the form of eq 21 without the friction term n 0 these equations can be recast in a convenient dimensionless form to obtain general solutions regardless of the value of the scaling variables e g ancey et al 2008 aureli et al 2014 maranzoni and mignosa 2019 using the dimensionless variables denoted by an upper tilde 23 x x h 0 tan θ x t t g h 0 sin θ x h h h 0 cos 2 θ x u u g h 0 where h 0 is the initial water depth at the dam location the dimensionless equations reads 24 h t u h x 0 u h t x u 2 h 1 2 h 2 h eq 24 is identical to that obtained by writing the conventional 1d x split swe in dimensionless form using the variables aureli et al 2014 25 x x h 0 tan θ x t t g h 0 tan θ x h h h 0 u u g h 0 accordingly the analysis of the dam break problem in a sloping channel of fixed slope performed on the basis of the ssswe can be transformed by proper nondimensionalization into the analysis of a dam break problem in the conventional swe framework hunt 1987 analyzed this problem using the characteristic form of the classic swe and obtained analytically the solution domain boundaries in the dimensional x t plane these boundaries were obtained in dimensionless form by dressler 1958 and later by ancey et al 2008 and aureli et al 2014 exploiting the dimensionless characteristic formulation of the equations 26 u 2 c t const along the c characteristic curve d x d t u c u 2 c t const along the c characteristic curve d x d t u c where c h 1 2 is the dimensionless wave celerity in the dimensionless x t plane the quiet front which identify the position as a function of time of the tail of the rarefaction wave generated by the dam removal is given by 27 x q t t 2 4 t for 0 t 2 the drying front i e the trailing edge of the dam break wave is given by 28 x d t 1 2 t 2 2 1 for t 2 and the wetting front i e the leading edge of the dam break wave is given by 29 x w t 1 2 t 2 2 2 for t 0 these boundaries are shown in fig 6 a in the dimensionless x t plane dimensionless eqs 27 29 can then be converted into different dimensional equations for the ssswe and swe models by applying the corresponding variable transformations eq 23 or eq 25 respectively the ssswe and swe solutions substantially coincide when the bottom slope is small i e cosθx 1 and sinθx tanθx for example fig 6b compares the boundaries of the two solution domains in the x t plane for θx 45 and h 0 1 m on the whole the standard swe predict a faster propagation of the dam break wave compared to the ssswe indeed the scaling time of the swe model eq 25 is less than that of the ssswe model eq 23 by a factor of cosθx accordingly the times predicted by the conventional swe model e g the arrival time of the rarefaction wave at a selected section upstream of the dam the emptying time of the reservoir the arrival time of the dam break wave at a selected downstream location are shorter than the ones predicted by the ssswe model and the difference increases with the channel slope the absolute percentage deviation between the ssswe and swe predictions with reference to the ssswe one is 1 cosθx 100 for θx 45 it reaches approximately 30 6 conclusions in this paper a new formulation of the 2d ssswe for unsteady water flow on steep topography has been proposed in which flow depth is defined along the vertical direction and flow velocity is assumed locally parallel to the bottom surface this novel approach combines the advantages of considering two velocity components parallel to the bottom which is consistent with the 2d modeling of a quasi parallel shallow flow and a vertical water depth which avoids laborious pre and post processing computations on elevation data the equations were derived from differential analysis by applying the mass conservation and linear momentum principles to an infinitesimal vertical column of water and were written in conservative vector form according to the gradually varied flow assumption the pressure was assumed to vary linearly on the vertical depth and a correction factor was introduced to take into account the effect of the bottom slope on pressure distribution the effects of flow curvature were neglected the new ssswe appear structurally similar to the conventional swe but differ from them in four main respects i the flux terms depend on the local bottom slope ii a correction factor reducing the gravity effect and depending only on the local bottom topography is applied to the pressure term iii the bottom source term is a function of the sine of the bottom inclination angles iv the friction term depends on the local bottom slope the analysis of the eigenstructure of the new system of equations has shown that it is strictly hyperbolic for wet bed conditions and therefore it constitutes a nonlinear hyperbolic system of conservation laws with source term as the conventional swe hence the wide variety of numerical methods proposed in the literature to solve such equations can be usefully employed in numerical applications moreover when bottom slopes are small the new set of equations reduces to the conventional system of 2d swe for a 1d dam break flow in a frictionless sloping channel of fixed slope both ssswe and conventional swe can be recast in the same dimensionless form by using different sets of scaling variables this allows the analytical solutions obtained in dimensionless form from the conventional swe model to be transferred to the ssswe one the comparison between the solution domains bounded by the quiet drying and wetting fronts has shown that the ssswe predict a slower dam break wave propagation compared with the conventional swe this difference becomes more significant as the channel slope increases ultimately the new system of equations constitutes a shallow water model that involving slightly more complex mathematical expressions than the conventional 2d swe can be considered for the 2d numerical simulation of unsteady free surface flows on steep topography and for assessing the effect of bottom slope on the flow in the companion paper this model will be solved by a finite volume numerical scheme and validated against experimental data available in the literature the effect of steep bottom slopes will be assessed by comparing the numerical results of the ssswe with those provided by the conventional swe declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a existing formulations of the 2d ssswe various formulations of the 2d ssswe have been proposed in the literature for geophysical flows on steep bottom slopes ranging from water flows to flows of sediment fluid mixtures up to granular avalanches to compare different formulations used in different contexts the friction term is disregarded in this review and the velocity distribution correction coefficients are assumed equal to unity moreover the terms representing the effects of tangential or solid internal stresses in granular flow modeling are omitted one of the first formulations of the 2d ssswe can be found in chaudhry 1993 p 352 starting from equations expressed in a local bottom oriented rectangular coordinate system the author derived the following equations in global horizontal oriented coordinates by applying a rigid rotation of the axes and introducing simplifying assumptions a1 h t u h x v h y 0 u h t x u 2 h g h cos α x cos α z 2 h x y u v h g h cos α x sin α x v h t x u v h y v 2 h g h cos α y cos α z 2 h y g h cos α y sin α y where h is the vertical water depth u and v are the horizontal depth averaged velocity components along orthogonal directions x and y respectively g is the acceleration due to gravity t is time and αx αy and αz are the angles between the corresponding axes of the local and global coordinate systems in the field of debris and mud flow modeling assuming the two phase mixture as an equivalent homogeneous fluid laigle 1997 proposed the following conservative formulation of the 2d depth averaged equations a2 h t u h x v h y 0 u h t x u 2 h 1 2 β g h 2 y u v h g h sin θ x v h t x u v h y v 2 h 1 2 β g h 2 g h sin θ y in which h u and v are again the vertical flow depth and the depth averaged velocity components in the x and y horizontal directions respectively θx and θy are the inclination angles of the bottom surface in the x and y direction respectively β is a reduction correction factor of the pressure term set to cosθx cosθy without any physical explanation rickenmann et al 2006 adopted the same set of equations in their 2d depth averaged debris flow hb model in which the moving mass is assumed to behave rheologically as a herschel bulkley viscoplastic one phase fluid however they assumed β cosθxy being θxy the steepest slope angle rickenmann et al 2006 p 247 which is presumably the local maximum bottom inclination angle nakagawa and takahashi 1997 modeled debris flow on steep bottom slopes using 2d depth averaged equations that for an unerodible bottom surface reduce to takahashi 2007 pp 158 159 a3 h t u h x v h y 0 u h t x u 2 h g h cos θ x h x y u v h g h sin θ x v h t x u v h y v 2 h g h cos θ y h y g h sin θ y where different pressure correction factors are applied along the x and y directions in the momentum equations even though it would be reasonable to expect that the same pressure correction factor appears in the two equations and that it is a symmetric function of the bottom inclination angles θx and θy i e it should remains unchanged as a result of the exchange of coordinates x and y another formulation of the 2d ssswe was proposed by denlinger and iverson 2004 and denlinger and o connel 2008 to study granular flows over irregular topographies adapting this model to a homogeneous fluid and assuming an isotropic stress tensor the equations read a4 h t uh x vh y 0 uh t x u 2 h 1 2 g h 2 y uvh g h b x vh t x uvh y v 2 h 1 2 g h 2 g h b y where b denotes the elevation of the bottom surface above a horizontal datum and g is a total enhanced vertical acceleration defined as the sum of the acceleration due to gravity with the vertical acceleration of the fluid see also castro orgaz et al 2015 p 10 castro orgaz and hager 2017 p 41 the authors suggested assessing this latter quantity by calculating the total derivative of the depth averaged vertical velocity which is estimated as the arithmetic average of the vertical velocities computed at the free surface and the bottom by imposing the corresponding kinematic constraints denlinger and iverson 2004 p 4 however numerical problems could arise at free surface discontinuities such as bores or the initial discontinuity of a dam break problem where the vertical velocity is difficult to estimate because the water elevation gradient tends to be infinite similarly juez et al 2013 xia and liang 2018 and ni et al 2019 introduced in their 2d depth averaged models in global coordinates a correction factor ϕ which reduces the gravity effect this factor is assumed equal to the square cosine of the angle between the local normal to the bottom and the vertical direction or the maximum bottom inclination angle i e ϕ 1 1 tan2 θx tan2 θy in all previous formulations the flow velocity components are assumed horizontal and the water depth is measured along the vertical direction consistently with the adoption of a global horizontally oriented coordinate system conversely iverson and denlinger s 2001 formulation of the 2d depth averaged equations for the description of the flow of grain fluid mixtures on irregular topographies is based on the assumption that the water depth is measured perpendicularly to the bottom surface and the orthogonal velocity components are locally parallel to the bottom considering only normal fluid stresses as in fully liquefied inviscid masses the equations proposed by iverson and denlinger 2001 become a5 h t u h ξ v h η 0 u h t ξ u 2 h g ζ h h ξ η u v h g ξ h v h t ξ u v h η v 2 h g ζ h h η g η h in which ξ and η denote orthogonal directions locally parallel to the bottom surface and ζ is the axis normal to the bottom while g ξ g η and g ζ are the gravity acceleration components along the ξ η and ζ directions respectively in eq a5 the upper bar recalls that the flow variables are defined with respect to a bottom oriented coordinate system the same formulation of the equations in local coordinates was used by juez et al 2013 for granular flow modeling based on the same approach mcdougall and hungr 2004 derived a set of governing equations written in non conservative form to study the motion of landslides assumed to behave as a homogeneous equivalent fluid of specified rheology neglecting entrainment of solid material due to erosion and the effects of the internal shear stresses as well as bottom friction these equations can be rewritten in conservative form as in eq a5 however the choice of defining the flow depth along the direction orthogonal to the bottom can cause modeling difficulties especially on real topographies as explained in section 2 finally describing a real field application of the flatmodel numerical code medina et al 2008 presented the basic 2d depth averaged equations of the model which neglecting the effect of flow curvature are a6 h t u h ξ v h η 0 u h t ξ u 2 h 1 2 g p h 2 η u v h g p h tan θ x v h t ξ u v h η v 2 h 1 2 g p h 2 g p h tan θ y where g p g cosθ denotes a corrected gravity acceleration with θ being the angle defined by the horizontal plane and the velocity direction medina et al 2008 p 128 however it seems unphysical that the correction factor depends on the local flow direction moreover since flow depths are calculated along the vertical direction in the numerical code geometrical transformations must be carried out at each time step in each computational cell with a negative impact on computational efficiency appendix b derivation of the ssswe b 1 continuity equation assuming incompressible flow the rate of change of mass in the fixed control volume of fig 4 is b1 ρ h t d ω where ρ denotes the mass density the flux of mass through the vertical trapezoidal face normal to x is b2 ρ v i h d y ρ u h cos θ x d y hence the net influx of volume across the two opposite vertical faces normal to x is b3 ρ u h cos θ x x d ω similarly the net influx of volume across the two opposite vertical faces normal to y is b4 ρ v h cos θ y y d ω therefore equating the term in eq b1 with the sum of the terms in eqs b3 b4 according to the mass conservation principle and dividing by ρdω the following vertically averaged continuity equation can be obtained b5 h t u h cos θ x x v h cos θ y y 0 b 2 linear momentum equations the linear momentum principle is applied to the fixed control volume shown in fig 4 along directions ξ and η only the derivation of the ξ component equation is presented here the equation in the η direction can be obtained similarly assuming incompressible flow the rate of change of the ξ momentum within the control volume is b6 ρ t v ξ h d ω ρ uh t d ω where u is defined according to eq 6 the ξ component of the net momentum flux entering the control volume through the opposite trapezoidal faces normal to x is b7 ρ x v ξ v i h d ω ρ uuh cos θ x x d ω while the same quantity computed for the two opposite vertical faces normal to y is b8 ρ y v ξ v j h d ω ρ uvh cos θ y y d ω the total pressure force acting on the control surface in the ξ direction is b9 ρ x 1 2 kg h 2 i ξ d ω ρ 2 kg h 2 cos θ x d ω where g is the acceleration due to gravity and k is a dimensionless pressure correction factor k 1 the ξ component of the weight of the water enclosed in the control volume is b10 ρ g k ξ h d ω ρ gh sin θ x d ω while based on the 2d extension of the manning formula e g molls et al 1998 the component of the bed friction in the ξ direction is b11 ρ g h n 2 u v h 4 3 d a ρ g h n 2 u v h 4 3 d ω 1 ta n 2 θ x ta n 2 θ y where n is the manning roughness coefficient combining the various terms according to the linear momentum principle and simplifying yield the vertically averaged momentum equation in the ξ direction b12 u h t x u u h 1 2 k g h 2 cos θ x y u v h cos θ y g h sin θ x n 2 u v h 4 3 1 ta n 2 θ x ta n 2 θ y similarly the momentum equation in the η direction is b13 v h t x u v h cos θ x y v v h 1 2 k g h 2 cos θ y g h sin θ y n 2 v v h 4 3 1 ta n 2 θ x ta n 2 θ y appendix c eigenstructure of the equations flux functions f and g defined in eq 12 can be explicitly expressed in terms of the conserved variables as c1 f u h v h cos φ cos θ x sin 2 φ u h 2 u h v h cos φ h sin 2 φ 1 2 k g h 2 cos θ x u h v h v h 2 cos φ h sin 2 φ cos θ x g v h u h cos φ cos θ y sin 2 φ u h v h u h 2 cos φ h sin 2 φ cos θ y v h 2 u h v h cos φ h sin 2 φ 1 2 k g h 2 cos θ y where k does not depend on the conserved variables the jacobian matrices of flux functions f and g are respectively given by c2 j f 0 1 cos φ u 2 uv cos φ kgh sin 2 φ 2 u v cos φ u cos φ uv v 2 cos φ v u 2 v cos φ cos θ x sin 2 φ and c3 j g 0 cos φ 1 u v u 2 cos φ v 2 u cos φ u v 2 u v cos φ k g h sin 2 φ v cos φ 2 v u cos φ cos θ y sin 2 φ the eigenvalues of jf are c4 λ 1 3 f u v cos φ gh k sin 2 φ cos θ x sin 2 φ λ 2 f u v cos φ cos θ x sin 2 φ and the eigenvalues of jg are c5 λ 1 3 g v u cos φ gh k sin 2 φ cos θ y sin 2 φ λ 2 g v u cos φ cos θ y sin 2 φ eigenvalues λ f and λ g are real and distinct for h 0 and the matrix defined as the linear combination of jacobian matrices jf and jg has three distinct eigenvalues for h 0 too therefore the new equations are strictly hyperbolic for a wet bed toro 2001 p 39 eqs c2 c3 and eqs c4 c5 reduce to the well known expressions valid for the conventional swe when bottom slopes are small toro 2001 pp 32 33 
122,two dimensional 2d depth averaged shallow water equations swe are widely used to model unsteady free surface flows such as flooding processes including those due to dam break or levee breach however the basic hypothesis of small bottom slopes may be far from satisfied in certain practical circumstances both locally at geometric singularities and even in wide portions of the floodable area such as in mountain regions in these cases the classic 2d swe might provide inaccurate results and the steep slope shallow water equations ssswe in which the restriction of small bottom slopes is relaxed are a valid alternative modeling option however different 2d formulations of this set of equations can be found in the geophysical flow literature in both global horizontally oriented and local bottom oriented coordinate systems in this paper a new ssswe model is presented in which water depth is defined along the vertical direction and flow velocity is assumed parallel to the bottom surface this choice of the dependent variables combines the advantages of considering the flow velocity parallel to the bottom as can be expected in gradually varied shallow flow and handling vertical water depths consistent with elevation data usually available as digital terrain models the pressure distribution is assumed linear along the vertical direction and flow curvature effects are neglected a new formulation of the 2d depth averaged ssswe is derived in which the two dynamic equations represent momentum balances along two spatial directions parallel to the bottom whose horizontal projections are parallel to two fixed orthogonal coordinate directions the analysis of the mathematical properties of the new ssswe equations shows that they are strictly hyperbolic for wet bed conditions and reduce to the conventional 2d swe when bottom slopes are small finally it is shown that the ssswe predict a slower flow compared with the conventional swe in the theoretical case of a 1d dam break on a frictionless channel with fixed slope the capabilities of the proposed model are demonstrated in a companion paper on the basis of numerical and experimental tests keywords basic flow equations free surface flow shallow water equations steep bottom slopes two dimensional depth averaged model 1 introduction the two dimensional 2d depth averaged free surface flow equations under the shallow water approximation are a standard mathematical model widely used to describe a variety of gravity driven geophysical flow phenomena including flooding due to dam break e g aureli et al 2008 wang et al 2011 petaccia and natale 2020 pilotti et al 2020 or levee breach e g viero et al 2013 dazzi et al 2019 d oria et al 2019 overland flows e g singh et al 2015 cea and bladé 2015 costabile et al 2020 mixed free surface pressurized flows maranzoni et al 2015 maranzoni and mignosa 2018 cea and lópez núñez 2021 tsunami propagation and inundation of coastal regions e g segur 2007 leveque et al 2011 tidal bore propagation in river estuaries e g pan et al 2007 atmospheric air currents over non flat terrain sivakumaran and dressler 1989 and based on the continuum mechanics approach even granular flows e g denlinger and iverson 2004 mangeney castelnau et al 2005 juez et al 2013 castro orgaz et al 2015 and snow avalanches e g christen et al 2010 barbolini et al 2000 as well as flows of fluid sediment mixtures such as debris or mud flows e g han and wang 1996 laigle and coussot 1997 rickenmann et al 2006 in all these application fields despite the limitations connected to the basic restrictive assumptions e g basco 1989 hu and meyer 2005 van emelen et al 2014 and the difficulties related to the calibration of the model parameters e g barbolini et al 2000 guinot and cappelaere 2009 the 2d depth averaged shallow water model is generally accepted to predict the main flow features for flood hazard assessment and flood risk management e g o brien et al 1993 nakagawa and takahashi 1997 aureli et al 2006 rickenmann et al 2006 gruber and bartelt 2007 xia et al 2011 d oria et al 2019 furthermore the use of 2d depth averaged numerical models for free surface flow simulations is currently facilitated by the increasing availability of efficient user oriented software teng et al 2017 sometimes freeware such as basement hec ras 2d and telemac 2d e g horritt and bates 2002 zischg et al 2018 pilotti et al 2020 sharma and regonda 2021 among the key assumptions of the classic 2d depth averaged shallow water equations swe there is the hypothesis of small bottom slopes e g chow 1959 henderson 1966 toro 2001 bottom slopes are usually considered small when less than 1 10 which corresponds to inclination angles less than approximately 6 chow 1959 p 33 according to this hypothesis the bottom surface can be considered practically horizontal and the flow depth can be indifferently measured along the vertical direction or the direction normal to the bottom moreover the flow velocity which is assumed uniform on the vertical depth is represented by two orthogonal horizontal components since the vertical component of the fluid acceleration can be neglected in comparison with gravity according to the shallow water approximation the pressure distribution is essentially hydrostatic in the vertical direction acheson 1990 toro 2001 castro orgaz and hager 2017 however the topography on which free surface flows occur is sometimes very steep and irregular with slopes greater than 1 10 such as in mountain regions or locally near geometric singularities in these contexts the small bottom slope assumption is violated and the classic depth averaged swe are no longer strictly valid firstly the vertical component of the fluid acceleration can be non negligible thereby significantly affecting the physical process and inducing three dimensional effects e g aureli et al 2015 horna munoz and constantinescu 2020 then the pressure distribution is non hydrostatic along the vertical juez et al 2013 castro orgaz and hager 2017 nevertheless 1d and 2d swe are commonly used in flood hazard analysis even in the presence of steep and deeply irregular topographies e g han and wang 1996 valiani et al 2002 begnudelli and sanders 2007 aureli et al 2008 pilotti et al 2011 wang et al 2011 de almeida et al 2012 aureli et al 2014 pilotti et al 2014 touma and kanbar 2018 the vertical pressure distribution is non hydrostatic also where flow curvature is significant in this case a depth averaged boussinesq type model could be used e g castro orgaz et al 2015 cantero chinchilla et al 2016 castro orgaz and hager 2017 cap 2 fabiani and ota 2019 cantero chincilla et al 2020 however these higher order effects are not considered in this paper for flows on steep topographies the steep slope shallow water equations ssswe could be considered as a valid modeling option because the effect of steep bottom slopes is included i e the restrictive hypothesis of small bottom slopes is removed maintaining the assumption of negligible fluid acceleration normal to the bottom in the one dimensional 1d framework there is general agreement on the formulation of the ssswe based on a local bottom oriented coordinate system following the fixed shape of the channel bottom in this local coordinate system the longitudinal axis is parallel to the bottom and the other axis is normal to it consistently flow velocity is assumed parallel to the channel bottom and flow depth is defined orthogonally to it in the resulting equations representing mass conservation and momentum balance along the sloping flow direction the effect of the bottom slope is included in both pressure and bottom terms in which trigonometric functions of the bed inclination angle appear e g berger 1994 fernandez feria 2006 takahashi 2007 ancey et al 2008 van emelen et al 2014 this approach is systematically adopted in 1d modeling of both flows in steep channels with fixed bottom slope e g savage and hutter 1989 berger 1994 keller 2003 fernandez feria 2006 ancey et al 2008 antuono and hogg 2009 and curvilinear flows in channels with curved bottom e g dressler 1978 castro orgaz and hager 2014 castro orgaz and hager 2016 conversely various 2d formulations of the ssswe have been proposed in the wide gravity driven flow literature although such equations always represent the basic principles of mass conservation and linear momentum balance in the 2d context the discrepancies between the existing 2d formulations are mainly due to the different approaches adopted in deriving the equations and the different ways of introducing the bottom slope effects in particular some approaches assume a local rectangular coordinate system with bottom oriented axes i e a local coordinate system following the terrain with two axes parallel to the bottom and the third axis locally orthogonal to it iverson and denlinger 2001 mcdougall and hungr 2004 medina et al 2008 juez et al 2013 whereas other approaches are based on a horizontally oriented coordinate system i e a classic fixed cartesian coordinate system in which two axes are horizontal and the third axis is vertical and aligned with the gravity vector chaudhry 1993 laigle 1997 denlinger and iverson 2004 rickenmann et al 2006 denlinger and o connel 2008 ni et al 2019 juez et al 2013 xia and liang 2018 in the models based on local bottom oriented coordinates flow depth is measured along the direction normal to the bottom and flow velocity is assumed parallel to the local terrain surface whereas in the models that use global horizontally oriented coordinates the water surface is identified by its vertical elevation above the bottom and flow velocity is described by two orthogonal horizontal components in the presence of steep bottom slopes the choice of a local bottom oriented coordinate system seems at a first glance better in shallow water flow analysis indeed in quasi parallel flows on sloping planar bottom surfaces it is natural to assume the flow velocity parallel to the bottom surface and the flow depth normal to it in this case according to the shallow water approximation the fluid acceleration component normal to the bottom is negligible and consequently the pressure distribution is hydrostatic on each cross section of the flow however the use of a local coordinate system following the topography can cause practical problems in defining the initial condition handling the topographic data juez et al 2013 p 203 or returning the results since topographic information is typically available through a digital elevation model which provides terrain elevation data with reference to a horizontal geodetic datum denlinger and iverson 2004 p 2 for this reason various formulations of the 2d ssswe in global horizontally oriented coordinates can be found in the literature which mainly differ in how the effect of bottom slope on pressure distribution is included in the model a comprehensive review of the 2d depth averaged ssswe models proposed in the literature is presented in appendix a it is worth noting that some of these models present serious critical issues in particular in some proposed formulations the pressure correction coefficient that appears in the equations to take into account the effect of steep bottom slope on vertical pressure distribution is not symmetric with respect to the exchange of the spatial coordinates see eqs a1 and a3 in appendix a contrary to what expected to combine the advantages of the two alternative approaches this paper proposes a new formulation of the 2d ssswe which uses the vertical water depth defined as the vertical distance of the water surface above the bottom and two flow velocity components parallel to the bottom as depending variables the independent variables are two orthogonal horizontal coordinates and time using spatial dependent or independent variables defined with respect to a conventional horizontally oriented cartesian reference frame allows for efficient and direct manipulation of the data of a digital terrain model the depth averaged governing equations are derived by applying the basic principles of mass conservation and linear momentum balance to a vertical column of incompressible water extending from the bottom to the free surface the pressure distribution is assumed linear non hydrostatic in the vertical direction neglecting the effects of flow curvature the eigenstructure and some special cases of the new set of equations are discussed in detail moreover the simple case of a 1d dam break on a frictionless sloping straight channel with fixed positive bottom slope is considered to perform a first comparison of the analytical results provided by the proposed ssswe model and the conventional swe model in the presence of high bottom slopes a more extensive comparison on the basis of validation and numerical test cases will be performed in a companion paper in which a finite volume numerical scheme will be used to solve the equations this paper is organized as follows section 2 presents the conceptual bases of the model the new formulation of the equations is presented in section 3 the mathematical properties of the equations are discussed in section 4 along with some special cases the 1d dam break problem on a frictionless sloping channel with fixed slope is analyzed in section 5 concluding remarks are drawn in section 6 finally a review of the existing formulations of the 2d ssswe is provided in appendix a while the detailed derivation of the equations and their eigenstructure are presented in appendixes b and c respectively 2 conceptual bases of the model using a local bottom oriented coordinate system e g gray et al 1999 iverson and denlinger 2001 the shallow water equations are expressed in terms of flow depth measured normally to the bottom and depth averaged velocity components locally parallel to the bottom however defining the flow depth orthogonally to the bottom can cause some practical difficulties for example in fig 1 which sketches the initial condition of the dam break problem in a sloping channel straight line aa traced orthogonally from the bottom immediately upstream of the dam intersects the dam and does not reach the water surface in this case the depth of the water column does not represent correctly the water depth appearing in the equations to overcome this problem which for fixed dam height affects a longer channel stretch upstream of the dam as the bottom slope increases and avoid the ensuing mathematical complications in the analysis of the dam break problem fernandez feria 2006 the dam was assumed perpendicular to the bottom in some applications e g ancey et al 2008 although this configuration is unrealistic as a further example of such practical problems fig 2 shows that when the flow has concave curvature in the longitudinal vertical plane due to the curved bottom straight lines traced orthogonally from the bottom can intersect before reaching the free surface lines aa and bb in fig 2 causing an erroneous representation of the flow volume additionally since topographic information is commonly available as digital elevation data adopting flow depths measured perpendicularly from the terrain potentially requires laborious pre and post processing calculations to transform vertical depths into normal bottom depths and vice versa especially on irregular topography denlinger and iverson 2004 medina et al 2008 given these problems it seems reasonable to explore the possibility of modeling a shallow water flow on steep topography considering the flow depth defined along the vertical direction and the flow velocity parallel to the bottom surface accordingly the water depth h is defined with reference to the z axis aligned with the vertical direction and oriented upwards of a global fixed cartesian frame of reference oxyz while the flow velocity components u and v are defined with reference to the ξ and η directions of a local bottom oriented reference frame pξηζ in which axes ξ and η identify the tangent plane that approximates locally the bottom surface at each point p and axis ζ is locally normal to the bottom surface fig 3 axes ξ and η are characterized by the property that their horizontal projections have the same directions of orthogonal horizontal axes x and y of the fixed global frame of reference oxyz respectively i e ξ and η directions are locally tangent to the coordinate lines corresponding to directions x and y on the bottom surface respectively fig 3b denoting with i j and k the unit vectors of the x y and z axes of the global reference frame respectively the unit vectors of the ξ and η axes of the local reference frame with respect to the global one are respectively 1 ξ cos θ x i sin θ x k and η cos θ y j sin θ y k where θx and θy are the inclination angles of the bottom surface in the x and y directions respectively both these angles range between 90 and 90 excluding the extreme values and can be easily computed from the topographic data unit vectors ξ and η constitute a basis for the 2d vector space on the plane locally tangent to the surface bottom in p moreover axes ξ and η are in general non orthogonal because ξ η sin θ x sin θ y cos φ where φ denotes the angle between local directions ξ and η fig 3a they are orthogonal only when either θx 0 or θy 0 or both the unit vector of the ζ direction locally normal to the bottom surface at each point p can be represented in the global reference frame as 2 ζ tan θ x 1 ta n 2 θ x ta n 2 θ y i tan θ y 1 ta n 2 θ x ta n 2 θ y j 1 1 ta n 2 θ x ta n 2 θ y k accordingly the vertical component of the ζ unit vector i e ζ k is equal to the cosine of ψ which is the angle between the direction normal to the bottom and the vertical fig 3a based on this approach flow velocity v can be expressed as the sum of the two vector components u ξ and v η along the ξ and η directions respectively as 3 v u ξ v η or in the global reference frame 4 v u cos θ x i v cos θ y j u sin θ x v sin θ y k consequently the flow velocity magnitude is 5 v v v u 2 v 2 2 uv ξ η u 2 v 2 2 uv sin θ x sin θ y the two velocity components are assumed uniform on the vertical column of fluid as in the 2d depth averaged models the u and v orthogonal projections of flow velocity v in the ξ and η directions respectively can be expressed as a function of u and v as 6 u v ξ u v sin θ x sin θ y and v v η v u sin θ x sin θ y eq 4 shows that the horizontal velocity components along the orthogonal x and y directions are respectively 7 v x v i u cos θ x and v y v j v cos θ y and that the vertical component of the flow velocity is negligible only if both θx and θy are small i e the bottom is locally nearly horizontal flow variables h u and v are considered to be functions of horizontal spatial coordinates x and y as well as of time t as in the conventional 2d depth averaged swe 3 governing equations the new 2d ssswe are derived by applying the basic principles of mass conservation and linear momentum balance to a fixed infinitesimal parallelepiped control volume bounded by vertical faces and extending from the slanted bottom surface assumed planar and non erodible up to the free surface fig 4 the area of the parallelogram bottom of this infinitesimal control volume is 8 da d ξ ξ d η η d ξ d η sin φ d ξ d η 1 si n 2 θ x si n 2 θ y d ξ d η cos θ x cos θ y 1 ta n 2 θ x ta n 2 θ y where dξ and dη are the lengths of the sides of the bottom face and symbol denotes the vector product hence the area of the horizontal projection on the xy plane of the bottom of the control volume is 9 d ω da ζ k d ξ d η sin φ cos ψ d ξ d η cos θ x cos θ y dx dy in view of the geometric relations 10 d x d ξ cos θ x and d y d η cos θ y the vertical orientation of the control volume prevents superimpositions or distortions mancarella and hungr 2010 especially in curvilinear flows in the vertical plane fig 2 allowing the lateral faces of adjacent control volumes to match denlinger and iverson 2004 the momentum equations represent linear momentum balance along the inclined ξ and η directions consistently with the hypothesis of flow velocity locally parallel to the bottom surface the model is applied to incompressible inviscid water flow thus considering an isotropic stress tensor and neglecting internal viscous and turbulent stresses according to the assumption of gradually varied flow the pressure distribution is assumed linear on the lateral faces of the control volume neglecting the effect of flow curvature details on the derivation of the equations are reported in appendix b the resulting vertically averaged governing equations can be written in conservative form as 11 u t f x g y s with 12 u h uh vh f uh cos θ x uuh 1 2 kg h 2 cos θ x uvh cos θ x g vh cos θ y uvh cos θ y vvh 1 2 kg h 2 cos θ y s 0 gh s 0 ξ s f ξ gh s 0 η s f η in which u is the vector of conserved variables i e the vertical depth h and the uh and vh unit discharges in the ξ and η direction respectively f and g are the vectors of physical fluxes and s is the source term g is the gravity acceleration and k 1 is a pressure correction factor the bottom slopes are 13 s 0 ξ sin θ x and s 0 η sin θ y in the ξ and η direction respectively and the friction slopes along the same directions are 14 s f ξ n 2 u v h 4 3 1 ta n 2 θ x ta n 2 θ y and s f η n 2 v v h 4 3 1 ta n 2 θ x ta n 2 θ y where n is the manning roughness coefficient in eq 12 k describes the effect of the bottom slope on vertical pressure distribution actually for a parallel flow in a straight open channel of slope angle θ this distribution is no longer hydrostatic and pressure head at a vertical transverse section is equal to the vertical depth multiplied by the corrector factor cos2 θ as shown by chow 1959 p 33 and henderson 1966 p 28 this result also applies approximately to gradually varied flows and can be generalized to the 2d case setting 15 k 1 if v 0 cos 2 ψ 1 1 tan 2 θ x tan 2 θ y otherwise according to eq 15 the pressure correction factor is set to unity in static conditions when the pressure distribution is hydrostatic whereas in dynamic conditions k is assumed equal to the square cosine of the angle between the direction locally normal to the bottom and the vertical juez et al 2013 xia and liang 2018 ni et al 2019 over non horizontal bottom i e θx 0 and θy 0 k is discontinuous for v 0 actually according to the assumption that flow velocity is parallel to the bottom surface the vertical pressure distribution changes abruptly from hydrostatic as soon as the flow motion starts in the special case of a unidirectional uniform flow in a sloping channel with fixed slope in the longitudinal ξ direction i e θx const θy 0 ψ θx and k reduces to cos2 θx as expected in particular for θx tending to 90 k tends to 0 which is representative of a free falling fluid with a vertical pressure gradient equal to zero denlinger and iverson 2004 p 4 in a 2d flow on small bottom slopes i e θx 0 and θy 0 the value of k does not differ appreciably from unity and the pressure terms in eq 12 become practically hydrostatic as in the classic swe finally it is worth noting that based on the definition of eq 15 k is symmetric with respect to the two horizontal directions x and y because θx and θy are interchangeable to take into account the effect of streamline curvature on pressure distribution a further correction factor could be introduced in the pressure term chow 1959 p 33 however when the effect of curvature is significant such as in flows over curved bottom or flows with strongly curved water surface on even topography boussinesq type equations provide more accurate predictions since non linear terms are added to the conventional hydrostatic pressure term e g basco 1989 mohapatra and chaudhry 2004 kim and lynett 2011 cantero chinchilla et al 2016 castro orgaz and hager 2017 in eqs 11 12 dependent flow variables uh and vh are defined in terms of vertically averaged velocity components u and v however velocity components u and v also appear in the flux vectors inverting eq 6 u and v can be calculated from u and v as 16 u u v sin θ x sin θ y 1 sin 2 θ x sin 2 θ y u v cos φ sin 2 φ v v u sin θ x sin θ y 1 sin 2 θ x sin 2 θ y v u cos φ sin 2 φ the new ssswe eqs 11 12 present four main differences compared with the conventional swe i the flux terms depend on the local bottom slope through inclination angles θx and θy ii a correction factor which reduces the gravity effect appears in the pressure term this factor depends only on the local bottom slope iii the bottom source term is expressed as a function of the sine and not of the tangent of bottom inclination angles θx and θy actually the sine of an angle cannot be approximated with its tangent when the angle is not small iv friction stresses act on a bottom surface that is significantly larger than its horizontal projection for steep topographies and the friction term depends on the bottom slope 4 properties of the equations 4 1 hyperbolicity the character of the new equations can be determined by analyzing the eigenstructure of their homogeneous form in terms of the conserved variables toro 2001 this analysis is reported in appendix c which shows that the equations are strictly hyperbolic for a wet bed wave celerity is different in the ξ and η directions and varies with the local bottom inclination and flow velocity as well as the vertical flow depth eqs c4 and c5 in appendix c single elementary waves of small amplitude propagating in quiescent water k 1 over steep topography move with speed 17 c ξ g h cos θ x sin φ and c η g h cos θ y sin φ in the ξ and η direction respectively for a 1d flow in the ξ direction in a sloping channel of fixed slope i e θx const θy 0 and φ 90 the wave celerity is 18 c ξ gh cos θ x in which the gravity acceleration is corrected by the reduction coefficient cos2 θx dependent only on the channel slope van emelen et al 2014 in the limit case of small bottom slopes both c ξ and c η in eq 17 reduce to gh 1 2 which is the wave celerity in the conventional 2d swe 4 2 special cases the new set of equations reduces to the conventional 2d swe if bottom slopes are small i e θx θy 0 and consequently cosθx cosθy 1 sinθx tanθx sinθy tanθy and k 1 this simplification is obtained automatically in the regions of the flow domain where the topography is flat with gentle slopes in the special case of a 2d flow spreading on an inclined plane in which ξ is the coordinate along the direction of maximum slope i e θx const and θy 0 consequently φ 90 and ψ θx u and v are respectively equal to u and v in view of eq 6 and the system of eqs 11 12 becomes 19 h t u h cos θ x x v h y 0 u h t x u 2 h 1 2 k g h 2 cos θ x y u v h g h sin θ x n 2 u v h 4 3 1 ta n 2 θ x v h t x u v h cos θ x y v 2 h 1 2 k g h 2 g h n 2 v v h 4 3 1 ta n 2 θ x with 20 k 1 if v 0 cos 2 θ x otherwise the associated 1d ξ split equations i e v 0 of unsteady flow in a wide rectangular channel of fixed slope θx are 21 h t u h cos θ x x 0 u h t x u 2 h 1 2 k g h 2 cos θ x g h sin θ x n 2 u 2 h 4 3 1 ta n 2 θ x with k again expressed by eq 20 these equations are equivalent to the classic 1d ssswe in bottom oriented coordinates e g berger 1994 ancey et al 2008 van emelen et al 2014 as can be easily verified using elementary trigonometric transformations finally the new ssswe correctly represent the static condition indeed in this case u v 0 and k 1 then eqs 11 12 reduce to 22 h t 0 h x tan θ x h y tan θ y as expected in static conditions because the water surface is stationary and horizontal 5 1d dam break problem in a sloping channel let us consider the dam break problem in a frictionless rectangular channel of fixed bottom slope fig 5 in this case the proposed ssswe appear in the form of eq 21 without the friction term n 0 these equations can be recast in a convenient dimensionless form to obtain general solutions regardless of the value of the scaling variables e g ancey et al 2008 aureli et al 2014 maranzoni and mignosa 2019 using the dimensionless variables denoted by an upper tilde 23 x x h 0 tan θ x t t g h 0 sin θ x h h h 0 cos 2 θ x u u g h 0 where h 0 is the initial water depth at the dam location the dimensionless equations reads 24 h t u h x 0 u h t x u 2 h 1 2 h 2 h eq 24 is identical to that obtained by writing the conventional 1d x split swe in dimensionless form using the variables aureli et al 2014 25 x x h 0 tan θ x t t g h 0 tan θ x h h h 0 u u g h 0 accordingly the analysis of the dam break problem in a sloping channel of fixed slope performed on the basis of the ssswe can be transformed by proper nondimensionalization into the analysis of a dam break problem in the conventional swe framework hunt 1987 analyzed this problem using the characteristic form of the classic swe and obtained analytically the solution domain boundaries in the dimensional x t plane these boundaries were obtained in dimensionless form by dressler 1958 and later by ancey et al 2008 and aureli et al 2014 exploiting the dimensionless characteristic formulation of the equations 26 u 2 c t const along the c characteristic curve d x d t u c u 2 c t const along the c characteristic curve d x d t u c where c h 1 2 is the dimensionless wave celerity in the dimensionless x t plane the quiet front which identify the position as a function of time of the tail of the rarefaction wave generated by the dam removal is given by 27 x q t t 2 4 t for 0 t 2 the drying front i e the trailing edge of the dam break wave is given by 28 x d t 1 2 t 2 2 1 for t 2 and the wetting front i e the leading edge of the dam break wave is given by 29 x w t 1 2 t 2 2 2 for t 0 these boundaries are shown in fig 6 a in the dimensionless x t plane dimensionless eqs 27 29 can then be converted into different dimensional equations for the ssswe and swe models by applying the corresponding variable transformations eq 23 or eq 25 respectively the ssswe and swe solutions substantially coincide when the bottom slope is small i e cosθx 1 and sinθx tanθx for example fig 6b compares the boundaries of the two solution domains in the x t plane for θx 45 and h 0 1 m on the whole the standard swe predict a faster propagation of the dam break wave compared to the ssswe indeed the scaling time of the swe model eq 25 is less than that of the ssswe model eq 23 by a factor of cosθx accordingly the times predicted by the conventional swe model e g the arrival time of the rarefaction wave at a selected section upstream of the dam the emptying time of the reservoir the arrival time of the dam break wave at a selected downstream location are shorter than the ones predicted by the ssswe model and the difference increases with the channel slope the absolute percentage deviation between the ssswe and swe predictions with reference to the ssswe one is 1 cosθx 100 for θx 45 it reaches approximately 30 6 conclusions in this paper a new formulation of the 2d ssswe for unsteady water flow on steep topography has been proposed in which flow depth is defined along the vertical direction and flow velocity is assumed locally parallel to the bottom surface this novel approach combines the advantages of considering two velocity components parallel to the bottom which is consistent with the 2d modeling of a quasi parallel shallow flow and a vertical water depth which avoids laborious pre and post processing computations on elevation data the equations were derived from differential analysis by applying the mass conservation and linear momentum principles to an infinitesimal vertical column of water and were written in conservative vector form according to the gradually varied flow assumption the pressure was assumed to vary linearly on the vertical depth and a correction factor was introduced to take into account the effect of the bottom slope on pressure distribution the effects of flow curvature were neglected the new ssswe appear structurally similar to the conventional swe but differ from them in four main respects i the flux terms depend on the local bottom slope ii a correction factor reducing the gravity effect and depending only on the local bottom topography is applied to the pressure term iii the bottom source term is a function of the sine of the bottom inclination angles iv the friction term depends on the local bottom slope the analysis of the eigenstructure of the new system of equations has shown that it is strictly hyperbolic for wet bed conditions and therefore it constitutes a nonlinear hyperbolic system of conservation laws with source term as the conventional swe hence the wide variety of numerical methods proposed in the literature to solve such equations can be usefully employed in numerical applications moreover when bottom slopes are small the new set of equations reduces to the conventional system of 2d swe for a 1d dam break flow in a frictionless sloping channel of fixed slope both ssswe and conventional swe can be recast in the same dimensionless form by using different sets of scaling variables this allows the analytical solutions obtained in dimensionless form from the conventional swe model to be transferred to the ssswe one the comparison between the solution domains bounded by the quiet drying and wetting fronts has shown that the ssswe predict a slower dam break wave propagation compared with the conventional swe this difference becomes more significant as the channel slope increases ultimately the new system of equations constitutes a shallow water model that involving slightly more complex mathematical expressions than the conventional 2d swe can be considered for the 2d numerical simulation of unsteady free surface flows on steep topography and for assessing the effect of bottom slope on the flow in the companion paper this model will be solved by a finite volume numerical scheme and validated against experimental data available in the literature the effect of steep bottom slopes will be assessed by comparing the numerical results of the ssswe with those provided by the conventional swe declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a existing formulations of the 2d ssswe various formulations of the 2d ssswe have been proposed in the literature for geophysical flows on steep bottom slopes ranging from water flows to flows of sediment fluid mixtures up to granular avalanches to compare different formulations used in different contexts the friction term is disregarded in this review and the velocity distribution correction coefficients are assumed equal to unity moreover the terms representing the effects of tangential or solid internal stresses in granular flow modeling are omitted one of the first formulations of the 2d ssswe can be found in chaudhry 1993 p 352 starting from equations expressed in a local bottom oriented rectangular coordinate system the author derived the following equations in global horizontal oriented coordinates by applying a rigid rotation of the axes and introducing simplifying assumptions a1 h t u h x v h y 0 u h t x u 2 h g h cos α x cos α z 2 h x y u v h g h cos α x sin α x v h t x u v h y v 2 h g h cos α y cos α z 2 h y g h cos α y sin α y where h is the vertical water depth u and v are the horizontal depth averaged velocity components along orthogonal directions x and y respectively g is the acceleration due to gravity t is time and αx αy and αz are the angles between the corresponding axes of the local and global coordinate systems in the field of debris and mud flow modeling assuming the two phase mixture as an equivalent homogeneous fluid laigle 1997 proposed the following conservative formulation of the 2d depth averaged equations a2 h t u h x v h y 0 u h t x u 2 h 1 2 β g h 2 y u v h g h sin θ x v h t x u v h y v 2 h 1 2 β g h 2 g h sin θ y in which h u and v are again the vertical flow depth and the depth averaged velocity components in the x and y horizontal directions respectively θx and θy are the inclination angles of the bottom surface in the x and y direction respectively β is a reduction correction factor of the pressure term set to cosθx cosθy without any physical explanation rickenmann et al 2006 adopted the same set of equations in their 2d depth averaged debris flow hb model in which the moving mass is assumed to behave rheologically as a herschel bulkley viscoplastic one phase fluid however they assumed β cosθxy being θxy the steepest slope angle rickenmann et al 2006 p 247 which is presumably the local maximum bottom inclination angle nakagawa and takahashi 1997 modeled debris flow on steep bottom slopes using 2d depth averaged equations that for an unerodible bottom surface reduce to takahashi 2007 pp 158 159 a3 h t u h x v h y 0 u h t x u 2 h g h cos θ x h x y u v h g h sin θ x v h t x u v h y v 2 h g h cos θ y h y g h sin θ y where different pressure correction factors are applied along the x and y directions in the momentum equations even though it would be reasonable to expect that the same pressure correction factor appears in the two equations and that it is a symmetric function of the bottom inclination angles θx and θy i e it should remains unchanged as a result of the exchange of coordinates x and y another formulation of the 2d ssswe was proposed by denlinger and iverson 2004 and denlinger and o connel 2008 to study granular flows over irregular topographies adapting this model to a homogeneous fluid and assuming an isotropic stress tensor the equations read a4 h t uh x vh y 0 uh t x u 2 h 1 2 g h 2 y uvh g h b x vh t x uvh y v 2 h 1 2 g h 2 g h b y where b denotes the elevation of the bottom surface above a horizontal datum and g is a total enhanced vertical acceleration defined as the sum of the acceleration due to gravity with the vertical acceleration of the fluid see also castro orgaz et al 2015 p 10 castro orgaz and hager 2017 p 41 the authors suggested assessing this latter quantity by calculating the total derivative of the depth averaged vertical velocity which is estimated as the arithmetic average of the vertical velocities computed at the free surface and the bottom by imposing the corresponding kinematic constraints denlinger and iverson 2004 p 4 however numerical problems could arise at free surface discontinuities such as bores or the initial discontinuity of a dam break problem where the vertical velocity is difficult to estimate because the water elevation gradient tends to be infinite similarly juez et al 2013 xia and liang 2018 and ni et al 2019 introduced in their 2d depth averaged models in global coordinates a correction factor ϕ which reduces the gravity effect this factor is assumed equal to the square cosine of the angle between the local normal to the bottom and the vertical direction or the maximum bottom inclination angle i e ϕ 1 1 tan2 θx tan2 θy in all previous formulations the flow velocity components are assumed horizontal and the water depth is measured along the vertical direction consistently with the adoption of a global horizontally oriented coordinate system conversely iverson and denlinger s 2001 formulation of the 2d depth averaged equations for the description of the flow of grain fluid mixtures on irregular topographies is based on the assumption that the water depth is measured perpendicularly to the bottom surface and the orthogonal velocity components are locally parallel to the bottom considering only normal fluid stresses as in fully liquefied inviscid masses the equations proposed by iverson and denlinger 2001 become a5 h t u h ξ v h η 0 u h t ξ u 2 h g ζ h h ξ η u v h g ξ h v h t ξ u v h η v 2 h g ζ h h η g η h in which ξ and η denote orthogonal directions locally parallel to the bottom surface and ζ is the axis normal to the bottom while g ξ g η and g ζ are the gravity acceleration components along the ξ η and ζ directions respectively in eq a5 the upper bar recalls that the flow variables are defined with respect to a bottom oriented coordinate system the same formulation of the equations in local coordinates was used by juez et al 2013 for granular flow modeling based on the same approach mcdougall and hungr 2004 derived a set of governing equations written in non conservative form to study the motion of landslides assumed to behave as a homogeneous equivalent fluid of specified rheology neglecting entrainment of solid material due to erosion and the effects of the internal shear stresses as well as bottom friction these equations can be rewritten in conservative form as in eq a5 however the choice of defining the flow depth along the direction orthogonal to the bottom can cause modeling difficulties especially on real topographies as explained in section 2 finally describing a real field application of the flatmodel numerical code medina et al 2008 presented the basic 2d depth averaged equations of the model which neglecting the effect of flow curvature are a6 h t u h ξ v h η 0 u h t ξ u 2 h 1 2 g p h 2 η u v h g p h tan θ x v h t ξ u v h η v 2 h 1 2 g p h 2 g p h tan θ y where g p g cosθ denotes a corrected gravity acceleration with θ being the angle defined by the horizontal plane and the velocity direction medina et al 2008 p 128 however it seems unphysical that the correction factor depends on the local flow direction moreover since flow depths are calculated along the vertical direction in the numerical code geometrical transformations must be carried out at each time step in each computational cell with a negative impact on computational efficiency appendix b derivation of the ssswe b 1 continuity equation assuming incompressible flow the rate of change of mass in the fixed control volume of fig 4 is b1 ρ h t d ω where ρ denotes the mass density the flux of mass through the vertical trapezoidal face normal to x is b2 ρ v i h d y ρ u h cos θ x d y hence the net influx of volume across the two opposite vertical faces normal to x is b3 ρ u h cos θ x x d ω similarly the net influx of volume across the two opposite vertical faces normal to y is b4 ρ v h cos θ y y d ω therefore equating the term in eq b1 with the sum of the terms in eqs b3 b4 according to the mass conservation principle and dividing by ρdω the following vertically averaged continuity equation can be obtained b5 h t u h cos θ x x v h cos θ y y 0 b 2 linear momentum equations the linear momentum principle is applied to the fixed control volume shown in fig 4 along directions ξ and η only the derivation of the ξ component equation is presented here the equation in the η direction can be obtained similarly assuming incompressible flow the rate of change of the ξ momentum within the control volume is b6 ρ t v ξ h d ω ρ uh t d ω where u is defined according to eq 6 the ξ component of the net momentum flux entering the control volume through the opposite trapezoidal faces normal to x is b7 ρ x v ξ v i h d ω ρ uuh cos θ x x d ω while the same quantity computed for the two opposite vertical faces normal to y is b8 ρ y v ξ v j h d ω ρ uvh cos θ y y d ω the total pressure force acting on the control surface in the ξ direction is b9 ρ x 1 2 kg h 2 i ξ d ω ρ 2 kg h 2 cos θ x d ω where g is the acceleration due to gravity and k is a dimensionless pressure correction factor k 1 the ξ component of the weight of the water enclosed in the control volume is b10 ρ g k ξ h d ω ρ gh sin θ x d ω while based on the 2d extension of the manning formula e g molls et al 1998 the component of the bed friction in the ξ direction is b11 ρ g h n 2 u v h 4 3 d a ρ g h n 2 u v h 4 3 d ω 1 ta n 2 θ x ta n 2 θ y where n is the manning roughness coefficient combining the various terms according to the linear momentum principle and simplifying yield the vertically averaged momentum equation in the ξ direction b12 u h t x u u h 1 2 k g h 2 cos θ x y u v h cos θ y g h sin θ x n 2 u v h 4 3 1 ta n 2 θ x ta n 2 θ y similarly the momentum equation in the η direction is b13 v h t x u v h cos θ x y v v h 1 2 k g h 2 cos θ y g h sin θ y n 2 v v h 4 3 1 ta n 2 θ x ta n 2 θ y appendix c eigenstructure of the equations flux functions f and g defined in eq 12 can be explicitly expressed in terms of the conserved variables as c1 f u h v h cos φ cos θ x sin 2 φ u h 2 u h v h cos φ h sin 2 φ 1 2 k g h 2 cos θ x u h v h v h 2 cos φ h sin 2 φ cos θ x g v h u h cos φ cos θ y sin 2 φ u h v h u h 2 cos φ h sin 2 φ cos θ y v h 2 u h v h cos φ h sin 2 φ 1 2 k g h 2 cos θ y where k does not depend on the conserved variables the jacobian matrices of flux functions f and g are respectively given by c2 j f 0 1 cos φ u 2 uv cos φ kgh sin 2 φ 2 u v cos φ u cos φ uv v 2 cos φ v u 2 v cos φ cos θ x sin 2 φ and c3 j g 0 cos φ 1 u v u 2 cos φ v 2 u cos φ u v 2 u v cos φ k g h sin 2 φ v cos φ 2 v u cos φ cos θ y sin 2 φ the eigenvalues of jf are c4 λ 1 3 f u v cos φ gh k sin 2 φ cos θ x sin 2 φ λ 2 f u v cos φ cos θ x sin 2 φ and the eigenvalues of jg are c5 λ 1 3 g v u cos φ gh k sin 2 φ cos θ y sin 2 φ λ 2 g v u cos φ cos θ y sin 2 φ eigenvalues λ f and λ g are real and distinct for h 0 and the matrix defined as the linear combination of jacobian matrices jf and jg has three distinct eigenvalues for h 0 too therefore the new equations are strictly hyperbolic for a wet bed toro 2001 p 39 eqs c2 c3 and eqs c4 c5 reduce to the well known expressions valid for the conventional swe when bottom slopes are small toro 2001 pp 32 33 
123,for strongly non linear and high dimensional inverse problems markov chain monte carlo mcmc methods may fail to properly explore the posterior probability density function pdf given a realistic computational budget and are generally poorly amenable to parallelization particle methods approximate the posterior pdf using the states and weights of a population of evolving particles and they are very well suited to parallelization we focus on adaptive sequential monte carlo asmc an extension of annealed importance sampling ais in ais and asmc importance sampling is performed over a sequence of intermediate distributions known as power posteriors linking the prior to the posterior pdf the ais and asmc algorithms also provide estimates of the evidence marginal likelihood as needed for bayesian model selection at basically no additional cost asmc performs better than ais as it adaptively tunes the tempering schedule and performs resampling of particles when the variance of the particle weights becomes too large we consider a challenging synthetic groundwater transport inverse problem with a categorical channelized 2d hydraulic conductivity field defined such that the posterior facies distribution includes two distinct modes the model proposals are obtained by iteratively re simulating a fraction of the current model using conditional multiple point statistics mps simulations we examine how asmc explores the posterior pdf and compare with results obtained with parallel tempering pt a state of the art mcmc inversion approach that runs multiple interacting chains targeting different power posteriors for a similar computational budget asmc outperforms pt as the asmc derived models fit the data better and recover the reference likelihood moreover we show that asmc partly retrieves both posterior modes while none of them is recovered by pt lastly we demonstrate how the power posteriors obtained by asmc can be used to assess the influence of the assumed data errors on the posterior means and variances as well as on the evidence we suggest that asmc can advantageously replace mcmc for solving many challenging inverse problems arising in the field of water resources keywords adaptive sequential monte carlo particle methods sequential geostatistical resampling multiple point statistics bayesian inference evidence computation 1 introduction markov chain monte carlo mcmc methods are widely used to tackle probabilistic inverse problems arising in hydrology as the dimensionality of the parameter space and the non linearity of the forward problem increases standard mcmc methods often fail to explore the posterior probability density function pdf given realistic computational constraints this happens as the markov chains may be trapped in local minima for long times or they may be unable to move between modes of high posterior probability to circumvent such issues methods exploring a series of so called power posteriors have been developed in a power posterior less weight is given to the likelihood function as it is raised to the inverse of a temperature that is larger than one something that is typically referred to as tempering tempering based methods take advantage of the enhanced freedom of exploration at higher temperatures sampling closer to the prior pdf as popularized by the widely used simulated annealing method for global optimization kirkpatrick et al 1983 parallel tempering earl and deem 2005 is a mcmc method in which many interacting chains target different power posteriors through proposed swaps of the states between chains states sampled at higher temperatures can act as model proposals in the chains targeting the posterior distribution also called unit temperature chains analogous to classical mcmc methods pt approximates the posterior pdf by the states of the unit temperature chains sampled after burn in in the context of geophysical inversion sambridge 2014 demonstrated that pt can drastically improve sampling efficiency leading to an expanded exploration of the parameter space compared to standard mcmc the pt method has lately been applied in a range of geoscientific problems such as landscape evolution chandra et al 2019 groundwater flow and transport laloy et al 2016 reuschen et al 2020 and earthquake source inversion gallovič et al 2019 a highly parallelizable alternative to mcmc is offered by particle methods such as annealed importance sampling ais neal 2001 and sequential monte carlo smc doucet and johansen 2011 where the posterior distribution is approximated using a weighted sample of particle states in these methods importance sampling steps are performed sequentially along a sequence of power posteriors what differentiates these two methods from each other is that smc performs resampling of the particle population when the variance of the importance weights becomes too high one outstanding feature of both methods is that the evidence the normalizing constant in bayes theorem and the key quantity in bayesian model selection is estimated as well compared with its extensive use in science and engineering smc appears poorly explored in the earth sciences linde et al 2017 zhou et al 2016 proposed an adaptive version of smc asmc that automatically tunes the temperature reduction between neighboring power posteriors recently adaptive smc algorithms were introduced and successfully implemented in geophysical applications for posterior pdf and evidence estimations amaya et al 2021 davies et al 2021 realistic geological priors can often not be expressed by two point geostatistical models e g multivariate gaussian for example when connectivity patterns play an essential role in determining the system response gómez hernández and wen 1998 renard and allard 2013 multiple point statistics mps is a sub field of geostatistics aiming at generating conditional model realizations that honor higher order statistics found in a so called training image a gridded 2 d or 3 d representation of the spatial field of interest that is built from generic or previous geological knowledge of the site mariethoz and caers 2014 to generate mps based candidate models within mcmc inversions one popular approach is sequential geostatistical resampling sgr ruggeri et al 2015 in which model proposals are generated by re simulating a random fraction of the current model conditioned to the remaining grid values the sgr framework embraces two end member strategies either a randomly located boxed shaped area is resimulated as in sequential gibbs sampling by hansen et al 2012 and in blocking mcmc by fu and gómez hernández 2009 or random points throughout the model domain are resimulated as in iterative spatial resampling by mariethoz et al 2010a recently hybrid methods determining an optimal combination of these end members have been proposed by reuschen et al 2021 other approaches relying on much faster model proposals are offered for instance by graph cuts zahner et al 2016 or by encoding the complex priors in a much lower dimensional space using deep generative networks thereby reducing the number of inferred parameters from several thousands of unknowns to some tenths of latent variables laloy et al 2017 2018 in this paper we consider the challenging groundwater transport inverse problem introduced by laloy et al 2016 it consists of a 2 d synthetic tracer experiment in which concentration is monitored at pumping wells and the aim is to recover the hydraulic conductivity field assuming a binary geological media their case study 2 this test case is particularly challenging for three reasons i the underlying non linearity caused by long range connectivity of high conductivity zones and a conductivity ratio of 100 between permeable channels and less permeable matrix material ii a large number of observations with a high signal to noise ratio and iii a true field that is designed such that the targeted posterior distribution is bimodal with the modes being located far from each other laloy et al 2016 demonstrated how pt clearly outperforms standard mcmc when used within a sgr framework nevertheless even if pt offered important improvements it did not sample any of the posterior modes and the simulated data of the generated model realizations did not fit the true data to the level of the added noise compared to pt asmc presents the following advantages i adaptive determination of the temperature schedule ii the model proposal scale is tuned adaptively using the acceptance rate at the previously considered temperature and iii the evidence is calculated along the run with updates being made every time the temperature changes in pt the temperature schedule and the proposal scale need to be pre defined the evidence estimation in pt is reduced to a one dimensional integral over the inverse temperatures which can imply large approximation errors if the temperatures are comparatively few or poorly chosen we assess the performance of asmc for this test case and compare the results with the pt results of laloy et al 2016 we further discuss the insights offered by analyzing the results at intermediate temperatures corresponding de facto to assumptions of larger measurement noise with respect to the geophysical asmc study by amaya et al 2021 the present work considers a hydrogeological problem that is much more non linear and the model parameterizations and model proposal schemes are entirely different in asmc sgr we need to consider as many model parameters as there are pixels 7500 in our example while the deep generative network used by amaya et al 2021 only considered a few tenths of unknown latent variables 2 method 2 1 bayes theorem it is often beneficial to pose inverse problems within a probabilistic framework using bayes theorem in which the parameters to infer are treated as random variables if we consider a conceptual model composed by parameters θ the posterior pdf π θ y is given by 1 π θ y π θ p y θ π y the prior pdf π θ represents the a priori information concerning the model parameters this information is then weighted by the likelihood function p y θ that expresses for a given noise model how probable it is that a particular set of parameter values have produced the observations y assuming the noise on the data to be uncorrelated and normally distributed with a constant variance σ 2 the likelihood is expressed as 2 p y θ 2 π σ 2 m d exp 1 2 σ 2 i m d y i f i θ 2 where m d is the number of data points and f θ the simulated data given a set of model parameter values it can be convenient to consider the variable component of the natural logarithm of the likelihood 3 l y θ 1 2 σ 2 i m d y i f i θ 2 which we refer to as the reduced log likelihood as it ignores the constant terms the evidence also known as the marginal likelihood is the normalizing constant in bayes theorem this quantity can be used to rank alternative conceptual models defined by different prior models as it represents how consistent a conceptual model is with the set of observations under consideration kass and raftery 1995 the evidence is a multidimensional integral over the parameter space 4 π y π θ p y θ d θ making it very challenging to calculate for high dimensional models brunetti et al 2019 focus particularly on how to compute the evidence to compare different conceptual models within a mps inversion framework 2 2 sequential geostatistical resampling prior models are often represented by mathematical functions allowing any prior model realization to be evaluated in terms of its probability examples include uniform priors multivariate gaussian priors and latent space distributions learned by deep generative neural networks however such explicit prior model parameterizations are not always suitable or possible when seeking to encode realistic geological spatial heterogeneity linde et al 2015 as an alternative one can instead consider realizations of mps simulation tools strebelle 2002 as samples drawn from the prior model these realizations honor the higher order statistics of training images that can be built based on expected geological structures outcrops geophysical or borehole data the downside of such prior sampling based approaches is that one cannot calculate the prior probabilities of model realizations as needed in most mcmc algorithms sequential geostatistical resampling is a mechanism allowing mcmc inference when model proposals are drawn using mps algorithms that sample proposals proportionally to the prior density it builds on the foundational paper by mosegaard and tarantola 1995 in geophysics however it is noteworthy that the underlying philosophy of such a prior sampling based algorithm has more recently received strong theoretical backing in the context of infinite dimensional inversion problems e g cotter et al 2013 at each mcmc iteration a new model proposal is generated by re simulating a random fraction of the current model realization using an mps algorithm conditioned to the remaining pixel values there are two end member approaches to determine the locations of the pixels that are to be resimulated either a randomly located box shaped area alcolea and renard 2010 hansen et al 2012 or randomly located points mariethoz et al 2010a in this study we use boxes as it provided the best results in laloy et al 2016 we further rely on the deesse mps algorithm http www randlab org research deesse that is in turn based on the direct sampling method by mariethoz et al 2010b to re simulate the value of a certain uninformed pixel the algorithm scans the training image searching for patterns that agree with those found in the vicinity of this pixel if a similar enough pattern is found it assigns the value of the pixel under consideration in the training image to the one in the new proposed model this procedure is repeated for all the pixels that are to be re simulated in mcmc algorithms the metropolis rule is used to accept or reject model proposals obtained from symmetric proposal distributions the acceptance probability γ to move from a current state θ c to a proposed state θ p is 5 γ θ p θ c m i n 1 π θ p p y θ p π θ c p y θ c as mentioned above this rule cannot be used with mps algorithms such as deesse as π θ is unknown instead mps based inversions often rely on the extended metropolis mosegaard and tarantola 1995 method that is applicable if the model proposal mechanism generates samples drawn proportionally to the prior pdf the acceptance probability is then reduced to 6 γ θ p θ c m i n 1 p y θ p p y θ c which involves only likelihood ratios 2 3 adaptive sequential monte carlo asmc 2 3 1 power posteriors tempering consists in introducing a temperature variable flattening the likelihood function in eq 1 the corresponding tempered posterior pdfs are called power posteriors and can in their unnormalized form be expressed as 7 γ t θ t y π θ t p y θ t α t where the likelihood is raised to an inverse temperature α t 0 1 the effect of increasing the temperature decreasing α t is that the likelihood function becomes less peaky that is with less pronounced modes targeting these power posteriors instead of only targeting the posterior pdf at unit temperature as in standard mcmc increases the exploration capacity because the tempering process decreases the probability of getting trapped in local minima a graphical explanation regarding the advantages of tempered exploration can be found in sambridge 2014 fig 1 illustrates the main structural differences between standard mcmc and the methods of pt and ais that both rely on tempering 2 3 2 annealed importance sampling ais importance sampling is a monte carlo method used to estimate properties of a distribution that it is not possible to sample from hammersley and handscomb 1964 it relies on an auxiliary distribution q θ for drawing the samples that must include and should ideally be slightly inflated with respect to the target distribution for most applications sampling from the prior distribution in order to estimate properties of the posterior pdf suffers from the curse of dimensionality meaning that the computational effort needed to draw enough samples with a significant likelihood as needed to enable reliable estimates is unfeasible in contrast sampling from a well chosen importance distribution allows focusing the sampling in regions of high posterior probability the samples drawn are then used to compute the desired property while correcting for the bias resulting from the chosen importance distribution if the target distribution is the unnormalized posterior pdf π θ p y θ the importance weights are given by 8 w π θ p y θ q θ neal 2001 combined tempering and importance sampling to produce the ais method it uses n chains each of them representing evolving particles that target sequentially a sequence of power posteriors at different temperatures ranging from the prior to the unnormalized posterior pdf of interest the sequence is given by γ t θ t y t 0 t and it contains unnormalized power posteriors given by eq 7 with α t ranging from α t 0 0 the prior to α t t 1 the unnormalized posterior pdf the normalized power posteriors are given by 9 π t θ t y γ t θ t y z t where z t is the normalizing constant of the distribution in ais importance sampling steps are performed sequentially between each pair of consecutive power posteriors a subsequent power posterior γ t θ t y is approximated by using the estimation of the previous power posterior γ t 1 θ t y as the importance sampling distribution in contrast to standard importance sampling were the samples are drawn directly from the importance distribution in ais the γ t 1 θ t y samples are obtained by performing k mcmc iterations targeting this power posterior starting from the approximation γ t 2 θ t y by performing multiple intermediate importance sampling steps between the prior and the posterior pdf it is possible to ensure that each importance distribution is of high quality slightly inflated with respect to the target leading to estimates with low uncertainty variance after the importance sampling step represented by the longer arrows in between different colored circles in fig 2 again each of the n chains perform k mcmc steps targeting now γ t θ t y this process is repeated until α t 1 we refer to the importance weights eq 8 resulting from each intermediate importance sampling step as the incremental weights for a particle i at state θ t 1 i the incremental weight w t i that result from using γ t 1 θ t 1 y as an importance distribution for γ t θ t y is 10 w t i γ t θ t 1 i y γ t 1 θ t 1 i y to calculate the total weight of a particle one needs to account for all the intermediate importance sampling steps to achieve this the incremental weight w t i is used to update the normalized weight of particle i by 11 w t i w t 1 i w t i j 1 n w t 1 j w t j where w t 1 i is the normalized weight that is i 1 n w t 1 i 1 of the previous importance sampling step the posterior pdf is then approximated through a particle approximation in which the relative probabilities of the last n states of the particles are determined by the final normalized weights w t i by saving intermediate normalized weights and corresponding particle states the method allows also to approximate intermediate power posteriors that represent the solutions to the equivalent tempered problems 2 3 3 resampling the variance of the particle weights influences strongly the quality of the importance sampling estimator neal 2001 when using ais this variance may grow exponentially resulting in poor estimations of the posterior pdf and the evidence sequential monte carlo smc is a family of particle approaches that as ais rely on sequential importance sampling however smc incorporates also resampling del moral et al 2006 doucet and johansen 2011 in a resampling step the states of the particles are replicated according to a probability that is proportional to their current normalized weights and all the weights are re set to 1 n the replacement of particles with lower weights and increasing those with higher weights results in two advantages i it avoids the variance of the weights to grow indefinitely and ii it orients the exploration towards regions of higher posterior probability nevertheless since the resampling process increases the variance of the estimates douc and cappe 2005 it is often better to perform resampling only when needed the effective sample size e s s kong et al 1994 is expressed as 12 e s s t i 1 n w t 1 i w t i 2 j 1 n w t 1 j 2 w t j 2 it quantifies the number of effective samples in the particle approximation the common approach is to monitor the e s s along the run and perform resampling when it is lower than a specified threshold in this paper we rely on systematic resampling due to its good performance and easy implementation doucet and johansen 2011 fig 2 shows a graphical example of smc with n 5 particles in which the resampling step is indicated with red dashed lines 2 3 4 adaptive tempering schedule one complication of the ais and smc methods is the difficulty to pre define a suitable tempering schedule fig 2 zhou et al 2016 propose an adaptive smc method asmc their algorithm 4 in which an appropriate α step size increment is determined before each importance sampling step to do so they rely on the conditional effective sample size c e s s quantifying the quality of using the particle approximation γ t 1 θ t 1 y as an importance distribution to estimate expectations for the γ t θ t 1 y arising for different choices of α t the c e s s is given by 13 c e s s n i 1 n w t 1 i w t i 2 j 1 n w t 1 j w t j 2 the e s s and c e s s are both obtained by a sample approximation of a taylor expansion of the relative variance of the estimator kong et al 1994 the difference between them is that the e s s embraces the accumulated mismatch between the importance and target distributions whereas the c e s s focuses on the quality of the current importance sampling step if resampling was to be performed at every iteration then the e s s and c e s s quantities would be equal a detailed derivation of the c e s s can be found in the supplementary material of zhou et al 2016 the c e s s depends on the incremental weights w t that in turn depend on α t the strategy consists in finding the α increment between consecutive power posteriors that is the δ α t such that α t α t 1 δ α t giving the c e s s that is the closest to a pre defined quality expressed by c e s s o p to find δ α t we rely on a binary search within a sequence of possible δ α values first the c e s s is computed using the middle value of the δ α sequence and it is compared with c e s s o p depending on if it is higher or lower one of the two δ α half intervals is kept this procedure is repeated until the δ α that gives the c e s s that is the closest to c e s s o p is found if we increase c e s s o p we obtain higher quality estimates as the number l of intermediate power posteriors increases but at the expense of a longer asmc run the total number of iterations per particle is l k with k the number of mcmc steps per intermediate power posterior in practice the ratio c e s s o p n is chosen close to 1 in order to ensure high quality estimates it has been suggested that it should be at least 0 99 to build a smooth α sequence amaya et al 2021 but the optimal value is highly problem dependent the impact of the c e s s o p n value on the resulting l is non linear and not easy to predict 2 3 5 asmc based evidence estimation evidence estimation is essential for bayesian model selection considering two neighboring distributions γ t 1 θ t 1 y and γ t θ t y we can express the ratio of their normalizing constants as 14 z t z t 1 γ t θ t y d θ t γ t 1 θ t 1 y d θ t 1 del moral et al 2006 propose an approximation of this ratio as 15 z t z t 1 i 1 n w t 1 i w t i the evidence π y is the normalizing constant z t of the unnormalized posterior pdf that is the last distribution of the sequence when α t t 1 considering that the prior pdf integrates to one z 0 1 we can express the evidence as the product of the normalizing constant ratios 16 π y z t z t z 0 t 1 t z t z t 1 t 1 t i 1 n w t 1 i w t i consequently the evidence can be updated along the run by accounting for the evolving particle weights 2 4 full asmc sgr algorithm our algorithm combining the sgr method for model proposals with asmc for posterior pdf and evidence estimation is given in algorithm 1 we denote this algorithm as asmc sgr following the nomenclature in laloy et al 2016 in this study the proposal scale ϕ indicates half of the side length in meters of the box that is being re simulated at each iteration in addition to the previously mentioned advantages of adaptive tempering and evidence estimation the algorithm also has the attractive feature that the proposal scale ϕ can be tuned on the go without violating detailed balance conditions as would be the case for mcmc or pt applications this is simply achieved by keeping track of the acceptance rate for the k mcmc steps at the previous α t 1 and then to use this information to adapt the proposal scale for the next number of k mcmc steps such that the acceptance rate remains within a pre defined range this saves a lot of time compared with standard mcmc and pt algorithms that often necessitate tuning using multiple time consuming trial runs 3 results 3 1 test case we consider the second test case from laloy et al 2016 in which the concentration of an injected tracer is measured at regular time intervals the conceptual model is represented by a 250 250 categorical binary training image from strebelle 2002 fig 3 the 2 d reference model is located in the x y plane and has a dimension of 75 m 100 m with a discretization cell size of 1 m the hydraulic conductivity k is 0 01 m s for the channels and 0 0001 m s for the matrix a conservative tracer with a concentration of 1 kg m3 is injected at 8 locations on the top and bottom of the model fig 4 the concentration is measured every 8 h during 10 days at 11 pumping wells a total of 330 observations that extract 0 0005 m2 s of water and the facies at these points are assumed to be known this test exhibits symmetry with respect to the x axis such that any model and its mirrored image produce the same simulated concentration data and therefore the same likelihood consequently the posterior pdf is bi modal with two distinct modes mode 1 of the reference model was obtained as a random realization from the deesse algorithm fig 4a and then mirrored to obtain the mode 2 reference model fig 4b the simulations are performed using maflot a finite volume open source code for transport simulations in porous media künze and lunati 2012 fixed head boundaries of 0 m on the top and bottom of the domain and no flow boundaries on the sides are assumed to simulate steady state groundwater flow for the tracer transport we assume open boundaries an hydraulic dispersivity of 0 1 m and a background concentration of 0 01 kg m3 the simulated data were corrupted with uncorrelated gaussian noise with a standard deviation of σ 0 003 kg m3 approximately 3 of the mean concentration 3 1 1 asmc sgr settings the proposal scale ϕ used to create candidate models is tuned along the run see algorithm 1 by increasing or decreasing it by f 20 to ensure that the acceptance rate stays within the range of a r m i n 15 and a r m a x 35 it is further constrained to be between ϕ m a x 50 m and ϕ m i n 5 m for the deesse simulations we follow laloy et al 2016 and use 75 neighbors which implies that the patterns that are searched by the algorithm are composed of the 75 informed nodes that are the closest to the one being re simulated the fraction of the training image that is scanned is 0 9 and the distance threshold to accept a pattern is 0 01 mariethoz et al 2010b 3 2 asmc sgr results 3 2 1 test 1 asmc sgr with 24 particles we first compare the asmc sgr results with those obtained by laloy et al 2016 for a similar computational budget 24 chains and 25 000 iterations per chain to achieve this we chose n 24 particles and c e s s o p n 0 9997 combined with k 18 which resulted in 25 956 iterations per particle the resampling threshold e s s n was set to 0 3 del moral et al 2006 the user defined parameters and length of the run are summarized in table 1 asmc sgr 24p we first consider the evolution of the tempered log likelihood that is the likelihood raised to the inverse temperature in the natural log scale fig 5a the tempered log likelihood of each particle is seen to evolve according to the reference tempered log likelihood curve calculated using the assumed noise standard deviation σ 0 003 kg m3 if c e s s o p n or k would be too low then the particles would have considerably lower tempered likelihoods than the reference curve thereby indicating that the sampled log likelihoods are too low and that the associated computational budget is insufficient for the problem at hand consequently this type of curve is a useful diagnostic plot allowing the user to terminate an asmc run at an early stage if the tempered log likelihoods fall below the reference curve the automatically tuned proposal scale ϕ fig 5e enables the acceptance rate to stay within the pre defined range fig 5c the resulting α t sequence fig 5b demonstrates that roughly half of the forward simulations are carried out with α t values less than 0 01 corresponding to temperatures above 100 the plot showing the evolution of the normalized weights fig 5d illustrates the divergence of the weights between resampling steps and the re alignment of the weights when the normalized effective sample size e s s n fig 5f reaches below the 0 3 threshold to compare these asmc sgr 24p results with those obtained by laloy et al 2016 we first consider the measure used in their study as an indicator of data fitting 17 δ l y θ l y θ t l y θ r e f l y θ r e f 100 where l y θ r e f is the reduced reference log likelihood eq 3 and l y θ t is the mean sampled reduced log likelihood for mcmc and pt this mean is simply the arithmetic average of the reduced log likelihoods after burn in only considering unit temperature chains for pt whereas for asmc it is the weighted average of the n final reduced log likelihoods laloy et al 2016 demonstrated a drastic improvement when using pt sgr compared with mcmc sgr following hansen et al 2012 the indicator δ l y θ t was 9 an important improvement of 70 on average compared with mcmc sgr still the reference reduced log likelihood was actually not contained in the range of sampled reduced log likelihoods with pt sgr indicating that these samples are not representative of the posterior pdf for our asmc sgr 24p run the indicator δ l y θ t is 3 76 and the log likelihood range contains the reference value table 1 fig 6a d show exemplary pt sgr posterior samples from laloy et al 2016 these samples do not resemble either mode 1 or mode 2 even if fig 6b has some structural similarities with mode 1 fig 4a in contrast the final states obtained by asmc sgr 24p fig 6e h recover models that resemble both reference modes the realizations in fig 6e g resemble mode 2 fig 4b and the one in fig 6h resembles mode 1 fig 4a the reference mean fig 7a is the mean of mode 1 fig 4a and mode 2 fig 4b of the reference model the true posterior mean is unknown and it is likely to be slightly biased towards models resembling one of the modes the reason for this is that even if mode 1 and 2 have the same likelihood they do not have the same prior probability this is a consequence of using the training image in fig 3 that is likely to favor certain orientations of structures when generating prior samples nevertheless fig 7a provides a sensible point of comparison the asmc sgr 24p weights fig 6e h and the posterior mean corresponding to the weighted arithmetic mean of the samples fig 7b suggest that the total weights given to the two modes is unbalanced with mode 2 having a higher total weight than mode 1 still these results show that asmc sgr can sample the two modes of this very challenging inverse problem and that the structures of the reference mean are partly recovered unlike for the pt sgr mean see fig 9b in laloy et al 2016 3 2 2 test 2 asmc sgr with 72 particles asmc provides an approximation not only of the posterior pdf but also of every tempered intermediate power posterior in this section we focus on the evolution of the unnormalized power posteriors as α increases from the prior α 0 to the posterior pdf α 1 one way of interpreting these power posteriors is to consider them as posterior pdfs for different assumptions on the data error level indeed decreasing the α exponent has the same impact on the likelihood variable component as increasing the assumed standard deviation σ of the data noise α 1 σ 2 in eqs 3 and 7 thus the effect of tempering with a given α could also be achieved by considering an assumed standard deviation of σ α σ α where σ is the original standard deviation of 0 003 kg m3 for example α 0 25 is analogous to assuming a standard deviation that is twice as large σ α 0 006 kg m3 since the objective in this section is no longer to compare the results with laloy et al 2016 for a similar computational budget we now consider more particles we increase the number of particles running in parallel from 24 to 72 thereby aiming for improved approximations of the intermediate power posteriors while keeping fixed the other user defined parameters asmc sgr 72p in table 1 the number of power posteriors needed to honor the targeted c e s s o p are slightly higher compared to the asmc sgr 24p test the indicator δ l y θ eq 17 for asmc sgr 72p is 1 5 that is 60 less than for the 24 particles test furthermore the likelihood range of the final particles is also reduced the posterior mean for asmc sgr 72p fig 7c and four samples from the posterior pdf fig 6i l indicate that most of the samples resemble mode 1 instead of mode 2 of the reference model that is the opposite behavior compared with the asmc sgr 24p run the structural similarity index measure ssim wang et al 2004 can be used to quantify the similarity between two images it varies between 1 and 1 the higher the ssim the more similar the two compared images are ssim 1 indicates identical images the ssim of the power posterior mean models with respect to the reference mean model fig 7a initially increases before stagnating when α reaches 0 01 for both asmc sgr 24p fig 8a and asmc sgr 72p fig 8b for asmc sgr 24p the ssim values with respect to mode 2 continue to increase while the ssim values with respect to mode 1 is even decreasing at the end of the run fig 8a for asmc sgr 72p the situation is the opposite with the ssim values with respect to mode 1 being those that continue to increase for larger α values fig 8b for asmc sgr 24p the ssim remains the highest for mode 2 for all α values above 0 001 while the ssim values with respect to mode 2 for asmc sgr 72p only start to dominate for α values above 0 1 this is a consequence of the larger number of particles and the corresponding increased ability to approximate the power posterior the range of ssim values between the particle realizations and the reference models are shown to decrease as the run progresses fig 9 shows the posterior means and standard deviations at five stages of the asmc sgr 72p run the mean of the prior models fig 9a is computed from the initial deesse simulations using the facies at the pumping wells as conditioning data at α 2 0 e 3 fig 9b α 1 7 e 2 fig 9c and α 8 8 e 2 fig 9d the power posterior mean models already resembles patterns of the reference mean fig 7a when α 1 the posterior mean model is dominated by mode 1 fig 9e the standard deviations are initially high except in the vicinity of the conditioning points fig 9f and they decrease as expected with increasing α values fig 9g j as the run evolves towards the posterior pdf four samples from the power posteriors corresponding to α 2 0 e 3 fig 10e h α 1 7 e 2 fig 10i l and α 8 8 e 2 fig 10m p indicate that the variability among the realizations are high at the beginning with large corresponding rmse values as α increases the variability among the realizations and the corresponding rmse values decrease as the samples start resembling the modes and fit the data better 3 2 3 resampling and eve indices resampling has the advantage of reducing the variance of the particle weights and focusing the sampling in regions of high posterior probability however the corresponding decrease in the variability of the sample realizations has also an adverse impact on the asmc estimations a conservative way of estimating the number of independent particles remaining in a run is to trace back the origin of the particles using the eve indices before any resampling is performed the eve indices of the particles are 1 n as resampling implies re organization and replication of particles the eve indices change along the run at time t each particle i has an eve index e t i that denotes the original index of the particle that moved there see lee and whiteley 2018 for a detailed and illustrative explanation the evolution of the eve indices are shown for tests asmc sgr 24p fig 11a and asmc sgr 72p fig 11b the eve indices are modified after each resampling step particles with higher weights are more likely to be replicated and as they bring their eve indices their origin with them these eve indices are replicated as well while other eve indices corresponding to particle states with low weights are lost on the way consequently the number of distinct eve indices is reduced along the run due to resampling the more resampling there is the fewer surviving eve indices at the end of the run in each of our two example runs there is six resampling steps this led to two surviving eve indices out of 24 for asmc sgr 24p and only one surviving eve index out of 72 for asmc sgr 72p of course the particles with the same eve indices are generally not identical as they develop independently after resampling in response to the mcmc proposal steps despite inherent randomness a larger number of eve indices are expected when reducing the number of resampling steps or increasing the number of particles for our two test cases the few surviving eve indices indicate that a higher number of particles n intermediate power posteriors or k steps would be beneficial 3 2 4 evidence estimation the evidence π y eq 4 which can be used for bayesian model selection and ranking is obtained as a byproduct of the asmc algorithm eq 16 the log evidence is shown to evolve similarly for the asmc sgr 24p and asmc sgr 72p fig 12a runs both evidence curves have the same shape as α increases and the final evidence estimates are close π y 1374 14 for asmc sgr 72p and π y 1371 06 for asmc sgr 24p for many model selection studies focused on conceptual model comparison the differences in the evidence between conceptual models are often much larger amaya et al 2021 brunetti et al 2017 2019 than this discrepancy fig 12b thereby suggesting that only 24 particles would probably provide sufficiently accurate results analogous to the power posteriors it is also possible to interpret the intermediate evidences as those corresponding to larger assumed σ values this necessitates a correction nevertheless as the multiplicative term 2 π σ 2 m d in eq 2 does not follow the proportionality α 1 σ 2 the intermediate log evidences l o g π y α can be corrected to l o g π y α c o r r following 18 l o g π y α c o r r l o g π y α α m d l o g 2 π σ m d l o g 2 π σ α where σ is the originally assumed standard deviation of 0 003 kg m3 and σ α σ α is the standard deviation corresponding to that particular α the results highlight that the estimated evidences depend very strongly on the assumed error level fig 12c 4 discussion for a similar computational budget asmc sgr has been shown to outperform pt sgr in terms of data fitting table 1 moreover asmc sgr recovers particle states fig 6e h that resemble both of the reference modes fig 4a b while none of them are recovered when using pt sgr fig 6a d the asmc algorithm adaptively tunes both the proposal scale and the α sequence inverse temperatures along the run which implies much less user effort compared to the tedious testing needed to make pt sgr perform well if conceptual model comparison is intended asmc becomes even more attractive as it provides evidence estimations zhou et al 2016 that are reliable and in agreement with unbiased estimations obtained using brute force monte carlo amaya et al 2021 the intermediate power posterior approximations offered by the asmc algorithm are highly instructive figs 9 10 when the asmc sgr algorithm starts considering α values above a given threshold lower for 24 particles than for 72 particles the sampling tends to become unbalanced in our two example runs and there is one mode that ends up having a higher posterior probability than the other this could be addressed by increasing the computational budget either by considering a much larger number of particles one could imagine using hundreds or thousands of particles or by increasing c e s s o p or k that would reduce the number of resampling steps resampling plays the important role in particle methods of focusing the sampling towards high probability regions by controlling the variance of the particle weights unfortunately this advantage comes at the expense of losing the independence between the particles leading in our case to over prediction of one of the posterior modes in our test example it would be straightforward to facilitate sampling of both modes simply by allowing for model proposals that would mirror the present state however this would not be possible in most realistic settings the test example was primarily designed to ensure that the posterior had two posterior modes located far from each other thereby enabling comparison of different probabilistic methods for a very challenging inverse problem in order to allow a fair comparison between the previously published pt results and the new asmc results the training image and the deesse simulation parameters were the same as in laloy et al 2016 however this implies that the prior probability of sampling modes 1 and 2 are different and consequently that the two posterior modes have unequal posterior probabilities despite that the likelihoods are equivalent to ensure that the true posterior has two modes of equal posterior probability one could use a training image with two layers the first layer would be the original training image and the second layer would be obtained by mirroring the training image similarly to how mode 2 was created at each sgr step the mps algorithm would scan from either layer 1 or 2 nevertheless the fact that asmc sgr 24p primarily sampled mode 2 and asmc sgr 72p primarily sampled mode 1 suggests that the main limitation in the presented runs are the limited computational budgets that prohibit sampling the two posterior modes well during one asmc run one option to reduce the computational time and thereby allow for longer runs would be to use faster algorithms for generating the candidate models either newer versions of deesse quick sampling gravey and mariethoz 2020 graph cuts zahner et al 2016 or by replacing mps based algorithms with deep learning based generators as in the study by amaya et al 2021 also a computational gain could be achieved by replacing the expensive forward solver with a surrogate e g by polynomial chaos expansion laloy et al 2013 meles et al 2022 this should not bias the results if the surrogate is only applied in the intermediate k markov steps while still using the expensive forward solver for the importance sampling steps the power posterior approximations can also be interpreted as posterior pdf approximations for different assumed data error levels fig 9 by raising the likelihood function to an inverse temperature α that is less that 1 the impact on the reduced log likelihood is the same as if increasing the assumed error level that is flattening the likelihood and thereby enhancing the freedom of the exploration a similar effect is obtained by decreasing the number of data points considered m d α m d in eq 3 keeping a subset of the original observations will have a similar impact as reducing α or increasing σ 2 tempering assuming artificially high data errors or reducing the number of data are not uncommon in the literature when addressing challenging bayesian inversions e g juda and renard 2021 this results in an easier to solve but different inverse problem that is conservative in the sense that the posterior mean is less informative and the posterior variance is larger than for the original problem one important advantage of asmc is that it explores all these intermediate problems but also use the information gained to sample the original posterior pdf that is unfeasible for many other methods similarly the evidence computations can be re scaled to correspond to different assumptions of data error levels fig 12 in field applications the data error level is typically poorly known asmc can then be very helpful as one could assume a noise level that is likely too low and then obtain approximations of several power posterior corresponding to different larger error assumptions one could then consider choosing an optimal error level based on the asmc intermediate results using the relationship between α and σ for instance one could perhaps choose the error level and the corresponding posterior and evidence approximations by considering the divergence between the reference target log likelihood and the tempered log likelihoods with increasing α in fig 2a there is no such divergence as the true data error level is assumed this would be much more efficient than running multiple mcmc runs with different assumptions concerning σ an alternative and somewhat related method to solve inverse problems with sgr is population expansion popex introduced by jäggli et al 2017 2018 this method is similar to asmc in the sense that the proposal distribution progressively evolves along the run towards the posterior pdf these evolving distributions provide information maps built to efficiently select conditioning data for new sgr model proposals based on previously sampled high likelihood models the posterior pdf is approximated by iteratively expanding the set of models along the run the corrected popex algorithm by jäggli et al 2018 can be interpreted as an adaptive importance sampling algorithm naylor and smith 1988 in which the evolving proposal distribution is the importance distribution and the posterior pdf is the target distribution this is different from asmc where the importance sampling relies on consecutive power posteriors compared with popex asmc also includes resampling steps thereby avoiding the degeneracy that often seems to plague popex to address this problem jäggli et al 2018 artificially reweigh the weights in order to achieve a lower variance and hence a richer representation of the approximated posterior 5 conclusions tempering of likelihood functions is used in a wide variety of bayesian methods to enhance posterior exploration and for evidence computations particularly when confronted with high dimensional and multimodal posterior pdfs that standard mcmc methods often struggle with we demonstrate that adaptive sequential monte carlo asmc outperforms parallel tempering pt when using sequential geostatistical resampling a multiple point statistics approach as model proposal scheme in the context of a challenging synthetic groundwater transport inverse problem involving 7500 model parameters with a bimodal posterior pdf asmc is found to be considerably more effective in locating the two posterior modes and to sample states with likelihoods that are in agreement with the data noise the algorithm has a simple implementation and demands a minimal user effort in terms of tuning due to its adaptive features furthermore it also estimates the evidence marginal likelihood at almost no additional computational cost the intermediate results of the algorithm can be used to determine the posterior means standard deviations and evidences corresponding to different assumptions of data errors this can be very helpful as it avoids pre defining one standard deviation on the noise or doing many mcmc runs with different assumed errors and it allows assessing how the posterior changes from the prior through a number of intermediate power posteriors to the targeted posterior pdf the method is versatile robust and very well suited for parallelization and could have wide applicability to solve inverse problems arising in the field of water resources using a wide range of model parameterizations forward solvers and model proposal schemes in the future we will seek speed ups through surrogate modeling to enable a larger number of particles or longer runs and thereby improve the posterior estimations further for a given computational cost indeed our examples with 24 and 72 particles could locate the posterior modes but the computational budgets were insufficient to robustly sample the two posterior modes during the same asmc run credit authorship contribution statement macarena amaya conceptualization methodology software validation formal analysis investigation data curation writing original draft visualization niklas linde conceptualization methodology formal analysis writing review editing supervision funding acquisition eric laloy software writing review editing declaration of competing interest niklas linde director of the project number 184574 reports financial support was provided by swiss national science foundation acknowledgments this work was supported by the swiss national science foundation project number 184574 our asmc code and the test examples are available at the following github repository https github com amaya macarena asmc sgr we are grateful for the insightful comments provided by two anonymous reviewers we would also like to thank prof arnaud doucet university of oxford for his helpful suggestions 
123,for strongly non linear and high dimensional inverse problems markov chain monte carlo mcmc methods may fail to properly explore the posterior probability density function pdf given a realistic computational budget and are generally poorly amenable to parallelization particle methods approximate the posterior pdf using the states and weights of a population of evolving particles and they are very well suited to parallelization we focus on adaptive sequential monte carlo asmc an extension of annealed importance sampling ais in ais and asmc importance sampling is performed over a sequence of intermediate distributions known as power posteriors linking the prior to the posterior pdf the ais and asmc algorithms also provide estimates of the evidence marginal likelihood as needed for bayesian model selection at basically no additional cost asmc performs better than ais as it adaptively tunes the tempering schedule and performs resampling of particles when the variance of the particle weights becomes too large we consider a challenging synthetic groundwater transport inverse problem with a categorical channelized 2d hydraulic conductivity field defined such that the posterior facies distribution includes two distinct modes the model proposals are obtained by iteratively re simulating a fraction of the current model using conditional multiple point statistics mps simulations we examine how asmc explores the posterior pdf and compare with results obtained with parallel tempering pt a state of the art mcmc inversion approach that runs multiple interacting chains targeting different power posteriors for a similar computational budget asmc outperforms pt as the asmc derived models fit the data better and recover the reference likelihood moreover we show that asmc partly retrieves both posterior modes while none of them is recovered by pt lastly we demonstrate how the power posteriors obtained by asmc can be used to assess the influence of the assumed data errors on the posterior means and variances as well as on the evidence we suggest that asmc can advantageously replace mcmc for solving many challenging inverse problems arising in the field of water resources keywords adaptive sequential monte carlo particle methods sequential geostatistical resampling multiple point statistics bayesian inference evidence computation 1 introduction markov chain monte carlo mcmc methods are widely used to tackle probabilistic inverse problems arising in hydrology as the dimensionality of the parameter space and the non linearity of the forward problem increases standard mcmc methods often fail to explore the posterior probability density function pdf given realistic computational constraints this happens as the markov chains may be trapped in local minima for long times or they may be unable to move between modes of high posterior probability to circumvent such issues methods exploring a series of so called power posteriors have been developed in a power posterior less weight is given to the likelihood function as it is raised to the inverse of a temperature that is larger than one something that is typically referred to as tempering tempering based methods take advantage of the enhanced freedom of exploration at higher temperatures sampling closer to the prior pdf as popularized by the widely used simulated annealing method for global optimization kirkpatrick et al 1983 parallel tempering earl and deem 2005 is a mcmc method in which many interacting chains target different power posteriors through proposed swaps of the states between chains states sampled at higher temperatures can act as model proposals in the chains targeting the posterior distribution also called unit temperature chains analogous to classical mcmc methods pt approximates the posterior pdf by the states of the unit temperature chains sampled after burn in in the context of geophysical inversion sambridge 2014 demonstrated that pt can drastically improve sampling efficiency leading to an expanded exploration of the parameter space compared to standard mcmc the pt method has lately been applied in a range of geoscientific problems such as landscape evolution chandra et al 2019 groundwater flow and transport laloy et al 2016 reuschen et al 2020 and earthquake source inversion gallovič et al 2019 a highly parallelizable alternative to mcmc is offered by particle methods such as annealed importance sampling ais neal 2001 and sequential monte carlo smc doucet and johansen 2011 where the posterior distribution is approximated using a weighted sample of particle states in these methods importance sampling steps are performed sequentially along a sequence of power posteriors what differentiates these two methods from each other is that smc performs resampling of the particle population when the variance of the importance weights becomes too high one outstanding feature of both methods is that the evidence the normalizing constant in bayes theorem and the key quantity in bayesian model selection is estimated as well compared with its extensive use in science and engineering smc appears poorly explored in the earth sciences linde et al 2017 zhou et al 2016 proposed an adaptive version of smc asmc that automatically tunes the temperature reduction between neighboring power posteriors recently adaptive smc algorithms were introduced and successfully implemented in geophysical applications for posterior pdf and evidence estimations amaya et al 2021 davies et al 2021 realistic geological priors can often not be expressed by two point geostatistical models e g multivariate gaussian for example when connectivity patterns play an essential role in determining the system response gómez hernández and wen 1998 renard and allard 2013 multiple point statistics mps is a sub field of geostatistics aiming at generating conditional model realizations that honor higher order statistics found in a so called training image a gridded 2 d or 3 d representation of the spatial field of interest that is built from generic or previous geological knowledge of the site mariethoz and caers 2014 to generate mps based candidate models within mcmc inversions one popular approach is sequential geostatistical resampling sgr ruggeri et al 2015 in which model proposals are generated by re simulating a random fraction of the current model conditioned to the remaining grid values the sgr framework embraces two end member strategies either a randomly located boxed shaped area is resimulated as in sequential gibbs sampling by hansen et al 2012 and in blocking mcmc by fu and gómez hernández 2009 or random points throughout the model domain are resimulated as in iterative spatial resampling by mariethoz et al 2010a recently hybrid methods determining an optimal combination of these end members have been proposed by reuschen et al 2021 other approaches relying on much faster model proposals are offered for instance by graph cuts zahner et al 2016 or by encoding the complex priors in a much lower dimensional space using deep generative networks thereby reducing the number of inferred parameters from several thousands of unknowns to some tenths of latent variables laloy et al 2017 2018 in this paper we consider the challenging groundwater transport inverse problem introduced by laloy et al 2016 it consists of a 2 d synthetic tracer experiment in which concentration is monitored at pumping wells and the aim is to recover the hydraulic conductivity field assuming a binary geological media their case study 2 this test case is particularly challenging for three reasons i the underlying non linearity caused by long range connectivity of high conductivity zones and a conductivity ratio of 100 between permeable channels and less permeable matrix material ii a large number of observations with a high signal to noise ratio and iii a true field that is designed such that the targeted posterior distribution is bimodal with the modes being located far from each other laloy et al 2016 demonstrated how pt clearly outperforms standard mcmc when used within a sgr framework nevertheless even if pt offered important improvements it did not sample any of the posterior modes and the simulated data of the generated model realizations did not fit the true data to the level of the added noise compared to pt asmc presents the following advantages i adaptive determination of the temperature schedule ii the model proposal scale is tuned adaptively using the acceptance rate at the previously considered temperature and iii the evidence is calculated along the run with updates being made every time the temperature changes in pt the temperature schedule and the proposal scale need to be pre defined the evidence estimation in pt is reduced to a one dimensional integral over the inverse temperatures which can imply large approximation errors if the temperatures are comparatively few or poorly chosen we assess the performance of asmc for this test case and compare the results with the pt results of laloy et al 2016 we further discuss the insights offered by analyzing the results at intermediate temperatures corresponding de facto to assumptions of larger measurement noise with respect to the geophysical asmc study by amaya et al 2021 the present work considers a hydrogeological problem that is much more non linear and the model parameterizations and model proposal schemes are entirely different in asmc sgr we need to consider as many model parameters as there are pixels 7500 in our example while the deep generative network used by amaya et al 2021 only considered a few tenths of unknown latent variables 2 method 2 1 bayes theorem it is often beneficial to pose inverse problems within a probabilistic framework using bayes theorem in which the parameters to infer are treated as random variables if we consider a conceptual model composed by parameters θ the posterior pdf π θ y is given by 1 π θ y π θ p y θ π y the prior pdf π θ represents the a priori information concerning the model parameters this information is then weighted by the likelihood function p y θ that expresses for a given noise model how probable it is that a particular set of parameter values have produced the observations y assuming the noise on the data to be uncorrelated and normally distributed with a constant variance σ 2 the likelihood is expressed as 2 p y θ 2 π σ 2 m d exp 1 2 σ 2 i m d y i f i θ 2 where m d is the number of data points and f θ the simulated data given a set of model parameter values it can be convenient to consider the variable component of the natural logarithm of the likelihood 3 l y θ 1 2 σ 2 i m d y i f i θ 2 which we refer to as the reduced log likelihood as it ignores the constant terms the evidence also known as the marginal likelihood is the normalizing constant in bayes theorem this quantity can be used to rank alternative conceptual models defined by different prior models as it represents how consistent a conceptual model is with the set of observations under consideration kass and raftery 1995 the evidence is a multidimensional integral over the parameter space 4 π y π θ p y θ d θ making it very challenging to calculate for high dimensional models brunetti et al 2019 focus particularly on how to compute the evidence to compare different conceptual models within a mps inversion framework 2 2 sequential geostatistical resampling prior models are often represented by mathematical functions allowing any prior model realization to be evaluated in terms of its probability examples include uniform priors multivariate gaussian priors and latent space distributions learned by deep generative neural networks however such explicit prior model parameterizations are not always suitable or possible when seeking to encode realistic geological spatial heterogeneity linde et al 2015 as an alternative one can instead consider realizations of mps simulation tools strebelle 2002 as samples drawn from the prior model these realizations honor the higher order statistics of training images that can be built based on expected geological structures outcrops geophysical or borehole data the downside of such prior sampling based approaches is that one cannot calculate the prior probabilities of model realizations as needed in most mcmc algorithms sequential geostatistical resampling is a mechanism allowing mcmc inference when model proposals are drawn using mps algorithms that sample proposals proportionally to the prior density it builds on the foundational paper by mosegaard and tarantola 1995 in geophysics however it is noteworthy that the underlying philosophy of such a prior sampling based algorithm has more recently received strong theoretical backing in the context of infinite dimensional inversion problems e g cotter et al 2013 at each mcmc iteration a new model proposal is generated by re simulating a random fraction of the current model realization using an mps algorithm conditioned to the remaining pixel values there are two end member approaches to determine the locations of the pixels that are to be resimulated either a randomly located box shaped area alcolea and renard 2010 hansen et al 2012 or randomly located points mariethoz et al 2010a in this study we use boxes as it provided the best results in laloy et al 2016 we further rely on the deesse mps algorithm http www randlab org research deesse that is in turn based on the direct sampling method by mariethoz et al 2010b to re simulate the value of a certain uninformed pixel the algorithm scans the training image searching for patterns that agree with those found in the vicinity of this pixel if a similar enough pattern is found it assigns the value of the pixel under consideration in the training image to the one in the new proposed model this procedure is repeated for all the pixels that are to be re simulated in mcmc algorithms the metropolis rule is used to accept or reject model proposals obtained from symmetric proposal distributions the acceptance probability γ to move from a current state θ c to a proposed state θ p is 5 γ θ p θ c m i n 1 π θ p p y θ p π θ c p y θ c as mentioned above this rule cannot be used with mps algorithms such as deesse as π θ is unknown instead mps based inversions often rely on the extended metropolis mosegaard and tarantola 1995 method that is applicable if the model proposal mechanism generates samples drawn proportionally to the prior pdf the acceptance probability is then reduced to 6 γ θ p θ c m i n 1 p y θ p p y θ c which involves only likelihood ratios 2 3 adaptive sequential monte carlo asmc 2 3 1 power posteriors tempering consists in introducing a temperature variable flattening the likelihood function in eq 1 the corresponding tempered posterior pdfs are called power posteriors and can in their unnormalized form be expressed as 7 γ t θ t y π θ t p y θ t α t where the likelihood is raised to an inverse temperature α t 0 1 the effect of increasing the temperature decreasing α t is that the likelihood function becomes less peaky that is with less pronounced modes targeting these power posteriors instead of only targeting the posterior pdf at unit temperature as in standard mcmc increases the exploration capacity because the tempering process decreases the probability of getting trapped in local minima a graphical explanation regarding the advantages of tempered exploration can be found in sambridge 2014 fig 1 illustrates the main structural differences between standard mcmc and the methods of pt and ais that both rely on tempering 2 3 2 annealed importance sampling ais importance sampling is a monte carlo method used to estimate properties of a distribution that it is not possible to sample from hammersley and handscomb 1964 it relies on an auxiliary distribution q θ for drawing the samples that must include and should ideally be slightly inflated with respect to the target distribution for most applications sampling from the prior distribution in order to estimate properties of the posterior pdf suffers from the curse of dimensionality meaning that the computational effort needed to draw enough samples with a significant likelihood as needed to enable reliable estimates is unfeasible in contrast sampling from a well chosen importance distribution allows focusing the sampling in regions of high posterior probability the samples drawn are then used to compute the desired property while correcting for the bias resulting from the chosen importance distribution if the target distribution is the unnormalized posterior pdf π θ p y θ the importance weights are given by 8 w π θ p y θ q θ neal 2001 combined tempering and importance sampling to produce the ais method it uses n chains each of them representing evolving particles that target sequentially a sequence of power posteriors at different temperatures ranging from the prior to the unnormalized posterior pdf of interest the sequence is given by γ t θ t y t 0 t and it contains unnormalized power posteriors given by eq 7 with α t ranging from α t 0 0 the prior to α t t 1 the unnormalized posterior pdf the normalized power posteriors are given by 9 π t θ t y γ t θ t y z t where z t is the normalizing constant of the distribution in ais importance sampling steps are performed sequentially between each pair of consecutive power posteriors a subsequent power posterior γ t θ t y is approximated by using the estimation of the previous power posterior γ t 1 θ t y as the importance sampling distribution in contrast to standard importance sampling were the samples are drawn directly from the importance distribution in ais the γ t 1 θ t y samples are obtained by performing k mcmc iterations targeting this power posterior starting from the approximation γ t 2 θ t y by performing multiple intermediate importance sampling steps between the prior and the posterior pdf it is possible to ensure that each importance distribution is of high quality slightly inflated with respect to the target leading to estimates with low uncertainty variance after the importance sampling step represented by the longer arrows in between different colored circles in fig 2 again each of the n chains perform k mcmc steps targeting now γ t θ t y this process is repeated until α t 1 we refer to the importance weights eq 8 resulting from each intermediate importance sampling step as the incremental weights for a particle i at state θ t 1 i the incremental weight w t i that result from using γ t 1 θ t 1 y as an importance distribution for γ t θ t y is 10 w t i γ t θ t 1 i y γ t 1 θ t 1 i y to calculate the total weight of a particle one needs to account for all the intermediate importance sampling steps to achieve this the incremental weight w t i is used to update the normalized weight of particle i by 11 w t i w t 1 i w t i j 1 n w t 1 j w t j where w t 1 i is the normalized weight that is i 1 n w t 1 i 1 of the previous importance sampling step the posterior pdf is then approximated through a particle approximation in which the relative probabilities of the last n states of the particles are determined by the final normalized weights w t i by saving intermediate normalized weights and corresponding particle states the method allows also to approximate intermediate power posteriors that represent the solutions to the equivalent tempered problems 2 3 3 resampling the variance of the particle weights influences strongly the quality of the importance sampling estimator neal 2001 when using ais this variance may grow exponentially resulting in poor estimations of the posterior pdf and the evidence sequential monte carlo smc is a family of particle approaches that as ais rely on sequential importance sampling however smc incorporates also resampling del moral et al 2006 doucet and johansen 2011 in a resampling step the states of the particles are replicated according to a probability that is proportional to their current normalized weights and all the weights are re set to 1 n the replacement of particles with lower weights and increasing those with higher weights results in two advantages i it avoids the variance of the weights to grow indefinitely and ii it orients the exploration towards regions of higher posterior probability nevertheless since the resampling process increases the variance of the estimates douc and cappe 2005 it is often better to perform resampling only when needed the effective sample size e s s kong et al 1994 is expressed as 12 e s s t i 1 n w t 1 i w t i 2 j 1 n w t 1 j 2 w t j 2 it quantifies the number of effective samples in the particle approximation the common approach is to monitor the e s s along the run and perform resampling when it is lower than a specified threshold in this paper we rely on systematic resampling due to its good performance and easy implementation doucet and johansen 2011 fig 2 shows a graphical example of smc with n 5 particles in which the resampling step is indicated with red dashed lines 2 3 4 adaptive tempering schedule one complication of the ais and smc methods is the difficulty to pre define a suitable tempering schedule fig 2 zhou et al 2016 propose an adaptive smc method asmc their algorithm 4 in which an appropriate α step size increment is determined before each importance sampling step to do so they rely on the conditional effective sample size c e s s quantifying the quality of using the particle approximation γ t 1 θ t 1 y as an importance distribution to estimate expectations for the γ t θ t 1 y arising for different choices of α t the c e s s is given by 13 c e s s n i 1 n w t 1 i w t i 2 j 1 n w t 1 j w t j 2 the e s s and c e s s are both obtained by a sample approximation of a taylor expansion of the relative variance of the estimator kong et al 1994 the difference between them is that the e s s embraces the accumulated mismatch between the importance and target distributions whereas the c e s s focuses on the quality of the current importance sampling step if resampling was to be performed at every iteration then the e s s and c e s s quantities would be equal a detailed derivation of the c e s s can be found in the supplementary material of zhou et al 2016 the c e s s depends on the incremental weights w t that in turn depend on α t the strategy consists in finding the α increment between consecutive power posteriors that is the δ α t such that α t α t 1 δ α t giving the c e s s that is the closest to a pre defined quality expressed by c e s s o p to find δ α t we rely on a binary search within a sequence of possible δ α values first the c e s s is computed using the middle value of the δ α sequence and it is compared with c e s s o p depending on if it is higher or lower one of the two δ α half intervals is kept this procedure is repeated until the δ α that gives the c e s s that is the closest to c e s s o p is found if we increase c e s s o p we obtain higher quality estimates as the number l of intermediate power posteriors increases but at the expense of a longer asmc run the total number of iterations per particle is l k with k the number of mcmc steps per intermediate power posterior in practice the ratio c e s s o p n is chosen close to 1 in order to ensure high quality estimates it has been suggested that it should be at least 0 99 to build a smooth α sequence amaya et al 2021 but the optimal value is highly problem dependent the impact of the c e s s o p n value on the resulting l is non linear and not easy to predict 2 3 5 asmc based evidence estimation evidence estimation is essential for bayesian model selection considering two neighboring distributions γ t 1 θ t 1 y and γ t θ t y we can express the ratio of their normalizing constants as 14 z t z t 1 γ t θ t y d θ t γ t 1 θ t 1 y d θ t 1 del moral et al 2006 propose an approximation of this ratio as 15 z t z t 1 i 1 n w t 1 i w t i the evidence π y is the normalizing constant z t of the unnormalized posterior pdf that is the last distribution of the sequence when α t t 1 considering that the prior pdf integrates to one z 0 1 we can express the evidence as the product of the normalizing constant ratios 16 π y z t z t z 0 t 1 t z t z t 1 t 1 t i 1 n w t 1 i w t i consequently the evidence can be updated along the run by accounting for the evolving particle weights 2 4 full asmc sgr algorithm our algorithm combining the sgr method for model proposals with asmc for posterior pdf and evidence estimation is given in algorithm 1 we denote this algorithm as asmc sgr following the nomenclature in laloy et al 2016 in this study the proposal scale ϕ indicates half of the side length in meters of the box that is being re simulated at each iteration in addition to the previously mentioned advantages of adaptive tempering and evidence estimation the algorithm also has the attractive feature that the proposal scale ϕ can be tuned on the go without violating detailed balance conditions as would be the case for mcmc or pt applications this is simply achieved by keeping track of the acceptance rate for the k mcmc steps at the previous α t 1 and then to use this information to adapt the proposal scale for the next number of k mcmc steps such that the acceptance rate remains within a pre defined range this saves a lot of time compared with standard mcmc and pt algorithms that often necessitate tuning using multiple time consuming trial runs 3 results 3 1 test case we consider the second test case from laloy et al 2016 in which the concentration of an injected tracer is measured at regular time intervals the conceptual model is represented by a 250 250 categorical binary training image from strebelle 2002 fig 3 the 2 d reference model is located in the x y plane and has a dimension of 75 m 100 m with a discretization cell size of 1 m the hydraulic conductivity k is 0 01 m s for the channels and 0 0001 m s for the matrix a conservative tracer with a concentration of 1 kg m3 is injected at 8 locations on the top and bottom of the model fig 4 the concentration is measured every 8 h during 10 days at 11 pumping wells a total of 330 observations that extract 0 0005 m2 s of water and the facies at these points are assumed to be known this test exhibits symmetry with respect to the x axis such that any model and its mirrored image produce the same simulated concentration data and therefore the same likelihood consequently the posterior pdf is bi modal with two distinct modes mode 1 of the reference model was obtained as a random realization from the deesse algorithm fig 4a and then mirrored to obtain the mode 2 reference model fig 4b the simulations are performed using maflot a finite volume open source code for transport simulations in porous media künze and lunati 2012 fixed head boundaries of 0 m on the top and bottom of the domain and no flow boundaries on the sides are assumed to simulate steady state groundwater flow for the tracer transport we assume open boundaries an hydraulic dispersivity of 0 1 m and a background concentration of 0 01 kg m3 the simulated data were corrupted with uncorrelated gaussian noise with a standard deviation of σ 0 003 kg m3 approximately 3 of the mean concentration 3 1 1 asmc sgr settings the proposal scale ϕ used to create candidate models is tuned along the run see algorithm 1 by increasing or decreasing it by f 20 to ensure that the acceptance rate stays within the range of a r m i n 15 and a r m a x 35 it is further constrained to be between ϕ m a x 50 m and ϕ m i n 5 m for the deesse simulations we follow laloy et al 2016 and use 75 neighbors which implies that the patterns that are searched by the algorithm are composed of the 75 informed nodes that are the closest to the one being re simulated the fraction of the training image that is scanned is 0 9 and the distance threshold to accept a pattern is 0 01 mariethoz et al 2010b 3 2 asmc sgr results 3 2 1 test 1 asmc sgr with 24 particles we first compare the asmc sgr results with those obtained by laloy et al 2016 for a similar computational budget 24 chains and 25 000 iterations per chain to achieve this we chose n 24 particles and c e s s o p n 0 9997 combined with k 18 which resulted in 25 956 iterations per particle the resampling threshold e s s n was set to 0 3 del moral et al 2006 the user defined parameters and length of the run are summarized in table 1 asmc sgr 24p we first consider the evolution of the tempered log likelihood that is the likelihood raised to the inverse temperature in the natural log scale fig 5a the tempered log likelihood of each particle is seen to evolve according to the reference tempered log likelihood curve calculated using the assumed noise standard deviation σ 0 003 kg m3 if c e s s o p n or k would be too low then the particles would have considerably lower tempered likelihoods than the reference curve thereby indicating that the sampled log likelihoods are too low and that the associated computational budget is insufficient for the problem at hand consequently this type of curve is a useful diagnostic plot allowing the user to terminate an asmc run at an early stage if the tempered log likelihoods fall below the reference curve the automatically tuned proposal scale ϕ fig 5e enables the acceptance rate to stay within the pre defined range fig 5c the resulting α t sequence fig 5b demonstrates that roughly half of the forward simulations are carried out with α t values less than 0 01 corresponding to temperatures above 100 the plot showing the evolution of the normalized weights fig 5d illustrates the divergence of the weights between resampling steps and the re alignment of the weights when the normalized effective sample size e s s n fig 5f reaches below the 0 3 threshold to compare these asmc sgr 24p results with those obtained by laloy et al 2016 we first consider the measure used in their study as an indicator of data fitting 17 δ l y θ l y θ t l y θ r e f l y θ r e f 100 where l y θ r e f is the reduced reference log likelihood eq 3 and l y θ t is the mean sampled reduced log likelihood for mcmc and pt this mean is simply the arithmetic average of the reduced log likelihoods after burn in only considering unit temperature chains for pt whereas for asmc it is the weighted average of the n final reduced log likelihoods laloy et al 2016 demonstrated a drastic improvement when using pt sgr compared with mcmc sgr following hansen et al 2012 the indicator δ l y θ t was 9 an important improvement of 70 on average compared with mcmc sgr still the reference reduced log likelihood was actually not contained in the range of sampled reduced log likelihoods with pt sgr indicating that these samples are not representative of the posterior pdf for our asmc sgr 24p run the indicator δ l y θ t is 3 76 and the log likelihood range contains the reference value table 1 fig 6a d show exemplary pt sgr posterior samples from laloy et al 2016 these samples do not resemble either mode 1 or mode 2 even if fig 6b has some structural similarities with mode 1 fig 4a in contrast the final states obtained by asmc sgr 24p fig 6e h recover models that resemble both reference modes the realizations in fig 6e g resemble mode 2 fig 4b and the one in fig 6h resembles mode 1 fig 4a the reference mean fig 7a is the mean of mode 1 fig 4a and mode 2 fig 4b of the reference model the true posterior mean is unknown and it is likely to be slightly biased towards models resembling one of the modes the reason for this is that even if mode 1 and 2 have the same likelihood they do not have the same prior probability this is a consequence of using the training image in fig 3 that is likely to favor certain orientations of structures when generating prior samples nevertheless fig 7a provides a sensible point of comparison the asmc sgr 24p weights fig 6e h and the posterior mean corresponding to the weighted arithmetic mean of the samples fig 7b suggest that the total weights given to the two modes is unbalanced with mode 2 having a higher total weight than mode 1 still these results show that asmc sgr can sample the two modes of this very challenging inverse problem and that the structures of the reference mean are partly recovered unlike for the pt sgr mean see fig 9b in laloy et al 2016 3 2 2 test 2 asmc sgr with 72 particles asmc provides an approximation not only of the posterior pdf but also of every tempered intermediate power posterior in this section we focus on the evolution of the unnormalized power posteriors as α increases from the prior α 0 to the posterior pdf α 1 one way of interpreting these power posteriors is to consider them as posterior pdfs for different assumptions on the data error level indeed decreasing the α exponent has the same impact on the likelihood variable component as increasing the assumed standard deviation σ of the data noise α 1 σ 2 in eqs 3 and 7 thus the effect of tempering with a given α could also be achieved by considering an assumed standard deviation of σ α σ α where σ is the original standard deviation of 0 003 kg m3 for example α 0 25 is analogous to assuming a standard deviation that is twice as large σ α 0 006 kg m3 since the objective in this section is no longer to compare the results with laloy et al 2016 for a similar computational budget we now consider more particles we increase the number of particles running in parallel from 24 to 72 thereby aiming for improved approximations of the intermediate power posteriors while keeping fixed the other user defined parameters asmc sgr 72p in table 1 the number of power posteriors needed to honor the targeted c e s s o p are slightly higher compared to the asmc sgr 24p test the indicator δ l y θ eq 17 for asmc sgr 72p is 1 5 that is 60 less than for the 24 particles test furthermore the likelihood range of the final particles is also reduced the posterior mean for asmc sgr 72p fig 7c and four samples from the posterior pdf fig 6i l indicate that most of the samples resemble mode 1 instead of mode 2 of the reference model that is the opposite behavior compared with the asmc sgr 24p run the structural similarity index measure ssim wang et al 2004 can be used to quantify the similarity between two images it varies between 1 and 1 the higher the ssim the more similar the two compared images are ssim 1 indicates identical images the ssim of the power posterior mean models with respect to the reference mean model fig 7a initially increases before stagnating when α reaches 0 01 for both asmc sgr 24p fig 8a and asmc sgr 72p fig 8b for asmc sgr 24p the ssim values with respect to mode 2 continue to increase while the ssim values with respect to mode 1 is even decreasing at the end of the run fig 8a for asmc sgr 72p the situation is the opposite with the ssim values with respect to mode 1 being those that continue to increase for larger α values fig 8b for asmc sgr 24p the ssim remains the highest for mode 2 for all α values above 0 001 while the ssim values with respect to mode 2 for asmc sgr 72p only start to dominate for α values above 0 1 this is a consequence of the larger number of particles and the corresponding increased ability to approximate the power posterior the range of ssim values between the particle realizations and the reference models are shown to decrease as the run progresses fig 9 shows the posterior means and standard deviations at five stages of the asmc sgr 72p run the mean of the prior models fig 9a is computed from the initial deesse simulations using the facies at the pumping wells as conditioning data at α 2 0 e 3 fig 9b α 1 7 e 2 fig 9c and α 8 8 e 2 fig 9d the power posterior mean models already resembles patterns of the reference mean fig 7a when α 1 the posterior mean model is dominated by mode 1 fig 9e the standard deviations are initially high except in the vicinity of the conditioning points fig 9f and they decrease as expected with increasing α values fig 9g j as the run evolves towards the posterior pdf four samples from the power posteriors corresponding to α 2 0 e 3 fig 10e h α 1 7 e 2 fig 10i l and α 8 8 e 2 fig 10m p indicate that the variability among the realizations are high at the beginning with large corresponding rmse values as α increases the variability among the realizations and the corresponding rmse values decrease as the samples start resembling the modes and fit the data better 3 2 3 resampling and eve indices resampling has the advantage of reducing the variance of the particle weights and focusing the sampling in regions of high posterior probability however the corresponding decrease in the variability of the sample realizations has also an adverse impact on the asmc estimations a conservative way of estimating the number of independent particles remaining in a run is to trace back the origin of the particles using the eve indices before any resampling is performed the eve indices of the particles are 1 n as resampling implies re organization and replication of particles the eve indices change along the run at time t each particle i has an eve index e t i that denotes the original index of the particle that moved there see lee and whiteley 2018 for a detailed and illustrative explanation the evolution of the eve indices are shown for tests asmc sgr 24p fig 11a and asmc sgr 72p fig 11b the eve indices are modified after each resampling step particles with higher weights are more likely to be replicated and as they bring their eve indices their origin with them these eve indices are replicated as well while other eve indices corresponding to particle states with low weights are lost on the way consequently the number of distinct eve indices is reduced along the run due to resampling the more resampling there is the fewer surviving eve indices at the end of the run in each of our two example runs there is six resampling steps this led to two surviving eve indices out of 24 for asmc sgr 24p and only one surviving eve index out of 72 for asmc sgr 72p of course the particles with the same eve indices are generally not identical as they develop independently after resampling in response to the mcmc proposal steps despite inherent randomness a larger number of eve indices are expected when reducing the number of resampling steps or increasing the number of particles for our two test cases the few surviving eve indices indicate that a higher number of particles n intermediate power posteriors or k steps would be beneficial 3 2 4 evidence estimation the evidence π y eq 4 which can be used for bayesian model selection and ranking is obtained as a byproduct of the asmc algorithm eq 16 the log evidence is shown to evolve similarly for the asmc sgr 24p and asmc sgr 72p fig 12a runs both evidence curves have the same shape as α increases and the final evidence estimates are close π y 1374 14 for asmc sgr 72p and π y 1371 06 for asmc sgr 24p for many model selection studies focused on conceptual model comparison the differences in the evidence between conceptual models are often much larger amaya et al 2021 brunetti et al 2017 2019 than this discrepancy fig 12b thereby suggesting that only 24 particles would probably provide sufficiently accurate results analogous to the power posteriors it is also possible to interpret the intermediate evidences as those corresponding to larger assumed σ values this necessitates a correction nevertheless as the multiplicative term 2 π σ 2 m d in eq 2 does not follow the proportionality α 1 σ 2 the intermediate log evidences l o g π y α can be corrected to l o g π y α c o r r following 18 l o g π y α c o r r l o g π y α α m d l o g 2 π σ m d l o g 2 π σ α where σ is the originally assumed standard deviation of 0 003 kg m3 and σ α σ α is the standard deviation corresponding to that particular α the results highlight that the estimated evidences depend very strongly on the assumed error level fig 12c 4 discussion for a similar computational budget asmc sgr has been shown to outperform pt sgr in terms of data fitting table 1 moreover asmc sgr recovers particle states fig 6e h that resemble both of the reference modes fig 4a b while none of them are recovered when using pt sgr fig 6a d the asmc algorithm adaptively tunes both the proposal scale and the α sequence inverse temperatures along the run which implies much less user effort compared to the tedious testing needed to make pt sgr perform well if conceptual model comparison is intended asmc becomes even more attractive as it provides evidence estimations zhou et al 2016 that are reliable and in agreement with unbiased estimations obtained using brute force monte carlo amaya et al 2021 the intermediate power posterior approximations offered by the asmc algorithm are highly instructive figs 9 10 when the asmc sgr algorithm starts considering α values above a given threshold lower for 24 particles than for 72 particles the sampling tends to become unbalanced in our two example runs and there is one mode that ends up having a higher posterior probability than the other this could be addressed by increasing the computational budget either by considering a much larger number of particles one could imagine using hundreds or thousands of particles or by increasing c e s s o p or k that would reduce the number of resampling steps resampling plays the important role in particle methods of focusing the sampling towards high probability regions by controlling the variance of the particle weights unfortunately this advantage comes at the expense of losing the independence between the particles leading in our case to over prediction of one of the posterior modes in our test example it would be straightforward to facilitate sampling of both modes simply by allowing for model proposals that would mirror the present state however this would not be possible in most realistic settings the test example was primarily designed to ensure that the posterior had two posterior modes located far from each other thereby enabling comparison of different probabilistic methods for a very challenging inverse problem in order to allow a fair comparison between the previously published pt results and the new asmc results the training image and the deesse simulation parameters were the same as in laloy et al 2016 however this implies that the prior probability of sampling modes 1 and 2 are different and consequently that the two posterior modes have unequal posterior probabilities despite that the likelihoods are equivalent to ensure that the true posterior has two modes of equal posterior probability one could use a training image with two layers the first layer would be the original training image and the second layer would be obtained by mirroring the training image similarly to how mode 2 was created at each sgr step the mps algorithm would scan from either layer 1 or 2 nevertheless the fact that asmc sgr 24p primarily sampled mode 2 and asmc sgr 72p primarily sampled mode 1 suggests that the main limitation in the presented runs are the limited computational budgets that prohibit sampling the two posterior modes well during one asmc run one option to reduce the computational time and thereby allow for longer runs would be to use faster algorithms for generating the candidate models either newer versions of deesse quick sampling gravey and mariethoz 2020 graph cuts zahner et al 2016 or by replacing mps based algorithms with deep learning based generators as in the study by amaya et al 2021 also a computational gain could be achieved by replacing the expensive forward solver with a surrogate e g by polynomial chaos expansion laloy et al 2013 meles et al 2022 this should not bias the results if the surrogate is only applied in the intermediate k markov steps while still using the expensive forward solver for the importance sampling steps the power posterior approximations can also be interpreted as posterior pdf approximations for different assumed data error levels fig 9 by raising the likelihood function to an inverse temperature α that is less that 1 the impact on the reduced log likelihood is the same as if increasing the assumed error level that is flattening the likelihood and thereby enhancing the freedom of the exploration a similar effect is obtained by decreasing the number of data points considered m d α m d in eq 3 keeping a subset of the original observations will have a similar impact as reducing α or increasing σ 2 tempering assuming artificially high data errors or reducing the number of data are not uncommon in the literature when addressing challenging bayesian inversions e g juda and renard 2021 this results in an easier to solve but different inverse problem that is conservative in the sense that the posterior mean is less informative and the posterior variance is larger than for the original problem one important advantage of asmc is that it explores all these intermediate problems but also use the information gained to sample the original posterior pdf that is unfeasible for many other methods similarly the evidence computations can be re scaled to correspond to different assumptions of data error levels fig 12 in field applications the data error level is typically poorly known asmc can then be very helpful as one could assume a noise level that is likely too low and then obtain approximations of several power posterior corresponding to different larger error assumptions one could then consider choosing an optimal error level based on the asmc intermediate results using the relationship between α and σ for instance one could perhaps choose the error level and the corresponding posterior and evidence approximations by considering the divergence between the reference target log likelihood and the tempered log likelihoods with increasing α in fig 2a there is no such divergence as the true data error level is assumed this would be much more efficient than running multiple mcmc runs with different assumptions concerning σ an alternative and somewhat related method to solve inverse problems with sgr is population expansion popex introduced by jäggli et al 2017 2018 this method is similar to asmc in the sense that the proposal distribution progressively evolves along the run towards the posterior pdf these evolving distributions provide information maps built to efficiently select conditioning data for new sgr model proposals based on previously sampled high likelihood models the posterior pdf is approximated by iteratively expanding the set of models along the run the corrected popex algorithm by jäggli et al 2018 can be interpreted as an adaptive importance sampling algorithm naylor and smith 1988 in which the evolving proposal distribution is the importance distribution and the posterior pdf is the target distribution this is different from asmc where the importance sampling relies on consecutive power posteriors compared with popex asmc also includes resampling steps thereby avoiding the degeneracy that often seems to plague popex to address this problem jäggli et al 2018 artificially reweigh the weights in order to achieve a lower variance and hence a richer representation of the approximated posterior 5 conclusions tempering of likelihood functions is used in a wide variety of bayesian methods to enhance posterior exploration and for evidence computations particularly when confronted with high dimensional and multimodal posterior pdfs that standard mcmc methods often struggle with we demonstrate that adaptive sequential monte carlo asmc outperforms parallel tempering pt when using sequential geostatistical resampling a multiple point statistics approach as model proposal scheme in the context of a challenging synthetic groundwater transport inverse problem involving 7500 model parameters with a bimodal posterior pdf asmc is found to be considerably more effective in locating the two posterior modes and to sample states with likelihoods that are in agreement with the data noise the algorithm has a simple implementation and demands a minimal user effort in terms of tuning due to its adaptive features furthermore it also estimates the evidence marginal likelihood at almost no additional computational cost the intermediate results of the algorithm can be used to determine the posterior means standard deviations and evidences corresponding to different assumptions of data errors this can be very helpful as it avoids pre defining one standard deviation on the noise or doing many mcmc runs with different assumed errors and it allows assessing how the posterior changes from the prior through a number of intermediate power posteriors to the targeted posterior pdf the method is versatile robust and very well suited for parallelization and could have wide applicability to solve inverse problems arising in the field of water resources using a wide range of model parameterizations forward solvers and model proposal schemes in the future we will seek speed ups through surrogate modeling to enable a larger number of particles or longer runs and thereby improve the posterior estimations further for a given computational cost indeed our examples with 24 and 72 particles could locate the posterior modes but the computational budgets were insufficient to robustly sample the two posterior modes during the same asmc run credit authorship contribution statement macarena amaya conceptualization methodology software validation formal analysis investigation data curation writing original draft visualization niklas linde conceptualization methodology formal analysis writing review editing supervision funding acquisition eric laloy software writing review editing declaration of competing interest niklas linde director of the project number 184574 reports financial support was provided by swiss national science foundation acknowledgments this work was supported by the swiss national science foundation project number 184574 our asmc code and the test examples are available at the following github repository https github com amaya macarena asmc sgr we are grateful for the insightful comments provided by two anonymous reviewers we would also like to thank prof arnaud doucet university of oxford for his helpful suggestions 
124,to solve the time consuming parameter inversion problem based on swarm evolution algorithm this study proposes an improved greedy sampling method gsm igsm based on model reduction technology to rapidly estimate the hydraulic conductivity of steady state seepage problem different from the traditional gsm igsm neither needs to estimate the posterior error bound of the reduced order model rom nor does it require a training sample set to train the rom instead the observation data are directly applied to the training process of rom each iterative process of igsm includes a parameter inversion process to obtain the parameters that best match the observation information and use it to update the current rom the improved differential evolution algorithm e g mmrde is used as the search algorithm and the resulting algorithm is denoted as igsm mmrde after comparative testing igsm mmrde was found to have higher accuracy and lower calculation cost than gsm mmrde in addition igsm mmrde is also capable of high dimensional parameter inversion problems and parameter inversion with a wider search range after applying igsm mmrde to the inversion of the rock mass hydraulic conductivity of the natural seepage field in the dam site area of a hydropower station igsm mmrde converges quickly and takes much less time than adina mmrde based on the full order model approximately 0 31 of adina mmrde the performance results are even better than the adina mmrde using the fixed evolutionary generation therefore the advantages of using igsm in the rapid inversion of the initial seepage field of large scale projects are extremely evident keywords reduced order greedy snapshot hydraulic conductivity inversion method steady state seepage 1 introduction although the improving computer performance is crucial to the large dimensional numerical solution of complex phenomena model reduction calculation still plays a decisive role when interested in real time simulation and or repeated output evaluation of different values of some inputs e g monte carlo simulation pasetto et al 2011 pasetto et al 2013 pasetto et al 2014 baú 2012 parameter estimation wang and zabaras 2005 grepl et al 2007 nguyen et al 2010 legresley and alonso 2000 in view of the wide application of numerical methods in engineering practice and many specific industrial processes model reduction techniques applied to numerical models have emerged the model reduction calculation method is used to evaluate the input output relationship quickly and reliably where the output is expressed as a function of the field variable as the input parameterized partial differential equation solution groundwater model reduction techniques can be roughly divided into three categories data driven projection based and structural reduction methods data driven methods include artificial neural networks taormina et al 2012 some forms of polynomial chaotic expansion oladyshkin and nowak 2012 or bayesian networks fienen et al 2013 proper orthogonal decomposition pod is the most prominent projection based methods vermeulen and heemink 2004 ptm vermeulen heemink and valstar 2005 mcphee and yeh 2008 aj siade putti and yeh 2010 followed by fourier model reduction willcox and megretski 2005 the structural reduction method reduces the size of the model directly and simplifies the representation of the process von gunten et al 2014 it can also perform parameter reduction such as the inversion based upgrade method doherty and christensen 2011 adopted by doherty and christensen pod refers to a decomposition technology that can systematically extract the modes with the largest energy from a set of output of the system and these modes can be used as the basis for galerkin s simplified control equation this output is usually a set of snapshots generated on various model parameter values in experiments or numerical simulations a characteristic of pod is that it produces an optimal basis in other words a n order pod base contains more energy than any other n order bases pod as a model simplification tool for simulation was originally used to calculate turbulence after decades of efforts by researchers it has been extended to different applications and equation types ly and tran 2001 ganapathysubramanian and zabaras 2004 hinze and kunkel 2012 such as fluid dynamics kunisch and volkwein 2002 and incompressible navier stokes equations ss ravindran 2000 sirisup et al 2005 vermeulen et al vermeulen et al 2004 established two roms to describe the hydraulic head in a three dimensional groundwater flow model that is by combining pod with state space projection and galerkin projection and compared the performance of these two roms in actual cases vermeulen and heemink p vermeulen heemink and valstar 2005 established an inversion model of groundwater flow based on pod and galerkin projection this method reduces the time to solve the groundwater inverse problem by 60 and the inversion accuracy is equivalent to the original model gunzburger et al gunzburger et al 2007 extended the pod order reduction method to the situation where the boundary condition contains multiple parameters astrid et al astrid et al 2008 derived an effective rom of large scale variable parameter systems based on a newly proposed missing point estimation method and adopted an effective heuristic method to select the optimal spatial subset this method only uses 25 of the spatial grid points to calculate the galerkin projection and does not affect the accuracy of the rom boyce et al boyce et al 2015 extended the pod reduction technology to modflow and made a standard package based on all the characteristics of modflow to reduce the computational burden of high dimensional groundwater simulation different reduction methods are also used to build reduction models because algorithm cooperation is better than algorithm competition argáez et al 2016 stanko et al 2016 flórez and argáez 2018 the accuracy of rom is closely related to the choice of the reduced order basis and the reduced order basis of the pod is obtained from the outputs i e snapshots of the full order model fom of a specific input parameter set although the basis generated by the pod is sufficient to reproduce the behavior of the model under the influence of a specific input and it may not be sufficient when the system is under the control of an unknown input atwell and king 2001 therefore an effective sampling method is the key to ensuring reliability accuracy and efficiency to enhance pod and make it an effective and practical tool for numerical research of complex and high dimensional systems selecting a suitable snapshot is particularly important ss ravindran 2000 christensen et al 1999 ravindran 2002 as the posterior error estimation methods that correspond to various specific reduced order basis are proposed and proven haasdonk and ohlberger 2011 veroy et al 2003 grepl and patera 2005 the greedy sampling method gsm based on the posterior error estimation is used to construct the reduced order basis of the rom bui thanh et al 2008 binev et al 2011 buffa et al 2012 this method avoids the calculation of fom in the error estimation process so the posterior error can be estimated inexpensively this method can effectively explore the parameter domain when looking for the most representative snapshot and determine whether the existing basis functions are sufficient clénet et al clenet et al 2015 proposed a method to adaptively construct a rom this method adaptively obtains snapshots used to construct reduced order basis through gsm and is not affected by the curse of dimensionality ushijima and yeh ushijima and yeh 2015 developed an experimental design algorithm to select the location of the observation well network to provide the most robust information about the unknown hydraulic conductivity in the aquifer this method uses the gsm to construct a reduced order basis to lessen the dimensionality of the groundwater model and uses genetic algorithms to perform exhaustive search on the rom for the confined aquifer problem to keep the appropriate number of snapshots of the complete model at the appropriate time to obtain the most accurate rom possible siade et al aj siade putti and yeh 2010 proposed a method for selecting the optimal time snapshot of the model based on the characteristic that the confined aquifer reached a stable state exponentially and used a real case to prove the simplicity and efficiency of the method two years later they combined the pod based on the optimal time snapshot with quasi linearization and quadratic programming for the inversion of hydraulic conductivity siade et al 2012 boyce and yeh boyce and yeh 2014 proposed a non parametric method for establishing a reduced order transient groundwater flow model this method uses two greedy algorithms to iteratively select the optimal parameters and snapshot time between the parameter space and the time domain to generate snapshots of the rom and uses a markov chain monte carlo method to solve the inversion problem however this iterative method of selecting the optimal snapshots is expensive this research is dedicated to the parameter inversion of steady state seepage problems and an improved greedy sampling method igsm for parameter inversion is proposed this method does not need to calculate the posterior error it applies the observation data to the training process of the rom and uses the improved differential evolution algorithm mmrde based on the group optimization technology to extract the optimal parameters from the parameter space different from the classic gsm the optimal parameter of this method is defined as the parameter that best matches the rom with the observation data that is to solve the minimum problem after testing this method converges quickly has higher accuracy and is less time consuming than the inversion model based on the fom it can be applied to the inversion task of the initial seepage field of large scale projects 2 steady state seepage control equation without considering the compressibility of water and soil and imposing no slip conditions at the microscale solid fluid interface according to the continuity equation of water and darcy s law the governing equation of three dimensional steady state saturated groundwater flow in an orthotropic confined aquifer can be derived as wang et al 2020 1 x k x h x y k y h y z k z h z 0 where h represents the hydraulic head l kx ky and kz represent the hydraulic conductivities l t of the x y and z directions respectively this study uses orthotropy to describe the permeability inhomogeneity of the aquifer and assumes that the principal directions of the permeability tensor coincide with the x y and z directions of the adopted coordinate system the boundary conditions that need to be met are generally as follows 2 h h x y z o n γ e q n k x h x n x k y h y n y k z h z n z q o n γ n q n λ h α o n γ m where г e г n and г m represent the first second and third types of boundary conditions i e constant hydraulic head boundary constant flow boundary and convective boundary respectively h and q represent the known functions qn represents the flow rate on the flow boundary and λ and α are known values after using the weighted residual method to apply the superposition principle to eqs 1 2 and perform spatial discretization the overall linear partial differential equation can be obtained as follows 3 au q where a represents the nh nh order permeability matrix nh is the total number of nodes in the finite element model u represents the nh dimensional hydraulic head vectors and q represents the nh dimensional vectors of nodal flux according to the parameter partition θ a q μ calculate aq u v q 1 qa aq u v is independent of μ and its quantity is qa similarly calculate q q v q 1 qf q q v is also independent of μ and its quantity is qf then the permeation matrix a u v μ and the column vectors of nodal flux q v μ that correspond to the parameter μ can be written as follows in the form of a bilinear norm hesthaven et al 2016 4 a u v μ q 1 q a θ a q μ a q u v q v μ q 1 q f θ f q μ q q v 3 igsm gsm trains a rom after a certain iteration to match the characteristics of fom in any parameter gsm needs to use eq a 10 to extract the optimal parameters in each iteration but the optimization here is only for the training sample set used which is only the tip of the iceberg of the entire sample space the smaller sample set cannot represent the parameter space well although the rom constructed by the larger sample set can reflect the characteristics of fom better the amount of calculation is too large and the time consumption is too long therefore when the prior knowledge is not clear how to select a suitable training sample set to construct rom is a very difficult problem for this reason we propose an igsm that does not require training sample sets to solve the parameter inversion problem of steady state seepage 3 1 algorithm description the iterative idea of the igsm proposed in this study is completely opposite to the classical gsm the rom trained by igsm cannot be used as a good substitute for the original model for result output but it can quickly solve the problem of parameter inversion of steady state seepage in the iterative process igsm does not need to estimate the posterior error of the current rom unlike the gsm which considers the parameter with the worst matching between rom and fom that is the parameter with the largest posterior error as the optimal parameter igsm searches for the hydraulic conductivity that best matches the observation information to improve the reduced order model of the next iteration in other words gsm and igsm solve the maximum and minimum problems in each iteration respectively the objective function of igsm during the search is the degree of matching between the reduced order solution under the current rom and the observation information there are many widely used objective functions such as the weighted sum of squared errors sse the weighted mean absolute error mae and the weighted root mean square error rmse igsm uses sse as the objective function to extract the optimal parameter samples in the parameter space ξ a group optimization algorithm needs to be used in this study igsm uses an improved differential evolution algorithm mmrde qian et al 2018 with a relatively powerful global search capability to guide the search the igsm mmrde is expressed as adopting the mmrde as the optimization algorithm in the igsm framework similarly the inversion algorithm formed by combining the gsm and mmrde is denoted as gsm mmrde the mmrde is used to solve the minimum problem as described in eq 5 5 s s e r i 1 n w i y i y r i 2 μ n 1 arg min μ ξ s s e r μ where yi and y i are the calculated and observed values of hydraulic head at the i th observation point respectively n is the total number of observation points and wi is the weight coefficient that corresponds to each observation point the subscript r indicates that it corresponds to the rom system by continuously picking up the optimal parameters and expanding the projection matrix used for model reduction the rom will gradually show the morphological characteristics of the objective function of the fom in the parameter space to finally determine the optimal parameters of the parameter inversion problem excellent algorithms can avoid unnecessary calculations therefore an effective exit mechanism is indispensable for igsm when the morphological characteristics of the objective function displayed by the rom are sufficient to determine the optimal parameters it needs to trigger the exit mechanism in each iteration the fom solution of the picked optimal parameters needs to be calculated as a snapshot to make effective use of this hard won fom solution we need to calculate a rom solution of the optimal parameters in each iteration and compare the rom solution with the fom solution to determine the maximum value of node error δ when δ is less than ε the iteration ends and it can be considered that the topographic features of the objective function displayed by the generated reduced order model are sufficient to determine the optimal parameters the termination condition of the iteration is described in eq 6 6 m a x h r j h f j ε where h r j and h f j represent the j th node hydraulic head that correspond to rom and fom respectively it should be clarified that the igsm algorithm is a framework for rapidly solving the parameters inversion problem of steady state saturated seepage and the applicability of igsm depends on whether the snapshot method can be used to construct a reduced order model of the original model therefore the igsm can be applied to confined aquifer systems with intra layer heterogeneity characteristics of natural aquifer systems because the problem can be reduced order model constructed by collecting snapshots pasetto et al 2014 aj siade putti and yeh 2010 3 2 algorithm steps igsm has a simple structure and a fast convergence rate fig 1 shows the iterative process of igsm to solve the one dimensional hydraulic conductivity inversion problem of pumping well modified from qian et al 2019 to illustrate the igsm parameter inversion process 1 a set of hydraulic conductivity μ 1 which is used to calculate snapshots and generate rom the order of the model n 1 is randomly generated 2 the mmrde is used to solve eq 5 determine the optimal hydraulic conductivity μ 2 and calculate the snapshot to build a new rom n 2 3 the mmrde is utilized to determine the optimal parameters μ 3 calculate the snapshot and generate the rom n 3 to be used in the next generation 4 the mmrde is used to determine the optimal parameters μ 4 150 003 and calculate the snapshot at this time the iterative termination condition is triggered and the iteration is terminated therefore μ 4 is outputted as the inversion parameter the pseudo code of igsm algorithm is shown in table 1 fig 2 reflects the flowchart comparison between the gsm mmrde and the igsm mmrde 4 numerical examples 4 1 algorithm setting to test the performance of igsm in parameter inversion the inversion problem of the initial seepage field described in qian et al 2019 is used as the inversion example of this research the finite element model of this inversion problem is hereinafter referred to as the test model mmrde is used as the search algorithm and it is combined with gsm and igsm to form the gsm mmrde and the igsm mmrde respectively gsm mmrde and fom mmrde are taken as the comparison algorithm of igsm mmrde and their inversion results are compared with that of the igsm mmrde the population size np and the maximum number of function computations are set to 5 dim and 200 np respectively the other parameters of mmrde are the same as the original reports 4 2 inversion test of igsm mmrde 4 2 1 influence of convergence accuracy convergence accuracy ε has a great influence on the training effect of the rom to study the influence of ε on the inversion results of igsm mmrde and gsm mmrde the test model is used as the fom since gsm always picks up the sample that corresponds to the maximum error in each iteration process to expand the projection matrix it also considers the impact of different training sample set sizes ns on the inversion results to reduce the influence of randomness on the inversion results each operating condition was run 10 times independently and the average value of 10 runs was compared the results are shown in fig 3 the bar graph embedded in fig 3 corresponds to the average number of iterations required for the algorithm to converge at each precision the results show that gsm mmrde and igsm mmrde are greatly affected by ε given that gsm mmrde uses the rom trained by the gsm to replace the fom for parameter inversion and the gsm uses the maximum relative error to calculate the accuracy of the reduced order model ns and ε can affect the inversion results when ns is same the smaller ε the more order of the rom and the more accurate the inversion result for example when ns 500 the average number of iterations required for error accuracies ε of 0 1 0 01 and 0 001 are 28 8 74 1 and 138 1 respectively this result is also shown in figs 3 a b and c in 10 independent runs the smaller the ε corresponds to the smaller the fluctuation of the inversion parameter and the closer it is to the ideal value the larger ns the more accurate the inversion result is under the same ε when ε 0 1 the average number of iterations required for ns 500 1000 and 2000 are 28 8 31 3 and 34 6 respectively this result means that only 5 76 3 13 and 1 73 of the samples from the ns samples were selected for rom construction as the scale of the training sample set expands this proportion will become smaller and smaller under the same precision indicating that setting a higher precision in a smaller ns is more economical than setting a smaller ε in a larger ns ns cannot be too small otherwise the solutions that correspond to these parameters cannot reflect the characteristics of the model considering that the inversion results fluctuate greatly when ε 0 1 we recommend ε 0 01 and ns 1000 for the igsm mmrde since it uses the mmrde to search for the hydraulic conductivity that best matches the observation information in the parameter space instead of the training parameter set the igsm mmrde does not need to set ns the igsm mmrde uses the maximum nodal error between the reduced order solution and the full order solution that corresponds to the inversion parameters of each iteration to calculate the reduced order accuracy accuracy ε has a great influence on the inversion results of the igsm mmrde from the analysis of the operation mechanism of the igsm mmrde it can be known that in the later iteration of the igsm mmrde the sampling points are always located near the optimal inversion parameters this finding means that the rom is finely matched near the inversion parameters and the error accuracy ε will directly affect the matching accuracy of the rom and the fom fig 3 d shows that with the decrease in ε the smaller the fluctuation of the inversion result is the more accurate the inversion result will be and the increase in the number of iterations is not significant with the improvement of the accuracy with ε 0 001 almost every run obtains results that are consistent with the ideal therefore ε 0 001 is used as the convergence accuracy requirement of the igsm mmrde in calculations below 4 2 2 influence of element division density the accuracy of the finite element model is closely related to the element size generally speaking the smaller the element size the higher the accuracy and the correspondingly more nodes the greater the computational cost in practical applications the meshes are usually encrypted appropriately to obtain higher accuracy to study the influence of different element sizes on the rom the modified example in qian et al 2019 was used as the fom denoted as 3wells model and its mesh was further subdivided the models with element sizes of 250 250 m and 125 125 m are obtained and compared with the original model element size is 500 500 m the mesh division and hydraulic head distribution of each model are plotted in fig 4 to reduce the result deviation caused by the randomness of the algorithm each algorithm runs 10 times independently under each working condition the results show that when the element sizes are 500 500 250 250 and 125 125 m the orders of roms that are built by gsm are 19 19 6 and 20 4 respectively and the orders of roms that are built by igsm mmrde are 7 6 7 8 and 7 4 respectively the average value of the iteration errors of each algorithm in 20 generations are taken to draw the convergence process curve fig 5 the partition density has a minimal effect on model reduction this shows that the rom has a greater advantage in the fine model than the coarse model 4 2 3 influence of the number of inversion parameters when using swarm optimization algorithm to solve the parameter inversion problem the amount of calculation is greatly affected by the number of inversion parameters inversion dimension the more dimensions the more model forward operations that need to be performed and the longer the inversion time required when using a rom for parameter inversion the inversion time is mainly determined by the time to establish the rom therefore the influence of the number of inverted parameters on the model reduction must be studied by using the test model as the original model the inversion results of fom mmrde gsm mmrde and igsm mmrde are calculated when the inversion dimensions dim are 2 4 and 6 respectively to reduce the deviation caused by randomness each operating condition is run independently for 10 times and the average value of the inversion results is shown in table 2 the results show that dim has a greater impact on the construction of the rom with the increase in inversion parameters the number of snapshots required to construct a rom with a predetermined accuracy also increases and the time consumption is greater however the increase in time consumption speed required by gsm mmrde and igsm mmrde is inconsistent when dim is 2 4 and 6 the average inversion time used by gsm mmrde accounts for approximately 12 37 11 98 and 15 30 of fom respectively whereas igsm mmrde accounts for approximately 7 88 2 56 and 1 03 of fom respectively this result shows that the time cost saving effect of igsm mmrde is very evident in addition with the increase in inversion parameters under the same convergence accuracy and sample set size the deviation of the inversion results of gsm mmrde gradually increases and igsm mmrde can obtain results that are consistent with fom mmrde this finding shows that igsm mmrde has a great advantage over gsm mmrde in solving high dimensional optimization problems 4 2 4 influence of measurement errors generally the variation law of the water level measurement error observed in the field follows a normal distribution therefore to study the sensitivity of the model to measurement errors noises that follow a normal distribution are added to the observation data to simulate measurement errors and are denoted as n μ σ where μ is the mean and σ is the standard deviation to study the sensitivity of observation errors when using igsm mmrde for parameter inversion noise data with μ 0 and σ of 0 01 0 1 and 1 are generated respectively after adding these noise data to the observation data three data sets namely data set 2 3 and data set 4 data set 1 record the original observation data with no measurement errors are obtained the inversion results of igsm mmrde and the fom mmrde and gsm mmrde models are run independently for each working condition 10 times and the average value of the inversion parameters under each working condition is taken as the inversion result as shown in table 3 the results show that the performance of the three algorithms in each observation data set is relatively similar when no observation error or σ is less than 0 1 the parameters obtained by the three algorithms are very close to the true values of the assumed parameters when σ is 1 and 0 1 the hydraulic conductivity estimated by the three algorithms are not accurate interestingly the inversion results of gsm mmrde in data sets 2 and 3 are slightly better than fom mmrde and igsm mmrde this finding shows that the reduced order error of gsm can offset part of the observation error which is beneficial to the inversion of the actual parameter 4 2 5 influence of parameter variation range in general parameter inversion should be performed within a reasonable range of parameter variance however in actual production practice searching within a larger search range is desirable due to some errors that lead to large variance in parameters to study the influence of the parameter variation range on the rom based parameter inversion the test model is used as the fom and the 6 parameters e g p1 p2 p3 p4 p7 and p8 most sensitive to the observation information are selected for inversion the parameter change range of the original inversion problem is marked as reference group 0 and the parameter ranges of lbound and ubound by 1 and 2 orders of magnitude are expanded as reference groups 1 and 2 respectively the parameter settings of each reference group are shown in table 4 to save computational cost the maximum number of iterations of gsm is set to 300 the inversion results of igsm mmrde and gsm mmrde in each reference group are shown in table 5 the results show that compared with gsm mmrde the parameter variation range has a smaller impact on igsm mmrde which is only reflected in a slight increase in the number of iterations and time consumption igsm mmrde can invert the true value under each reference group but the inversion result of gsm mmrde becomes less accurate as the parameter variation range expands the reason is that the training rom mechanism of the two methods is different in each iteration igsm picks up the parameter with the smaller objective function to expand the projection matrix thus the matching degree between the rom of its training and the observation data gradually becomes better in addition igsm mmrde utilizes the excellent global convergence performance of the mmrde so that it can invert the real parameters quickly and reliably under a larger parameter variation range the gsm aims to obtain a rom that matches with the fom solution of all parameters in the training sample set well with the expansion of the parameter variation range the fom contains more solution features and the number of samples required for gsm training to meet the convergence accuracy of the rom rises in addition the representativeness of the used training sample set in the entire parameter space also becomes smaller therefore the error of the order reduction becomes larger after the same number of iterations as seen from the maximum relative error of the reduced order model trained by gsm in table 5 4 2 6 time consumption comparison with fom although gsm does not need to calculate a large number of snapshots when constructing the reduced order subspace it needs to estimate the posterior error bounds of each parameter continuously to obtain the optimal snapshot gsm needs to perform snapshot calculation orthogonal basis vector construction reduced order projection operation offline calculation of residual error and online error estimation in each iteration of constructing rom in gsm any of these steps are run in a relatively inexpensive way in terms of time cost compared with gsm igsm only needs to complete the calculation of snapshots and reduced order projection operations to compare the time cost of constructing rom of the two algorithms the test model is used as the fom the number of training samples of gsm is set to 1000 and the maximum number of iterations is set to 300 fig 6 shows the variation curve of the cpu time of each main operation of gsm and igsm in the iterative process the results show that in the iterative process the time consuming calculation of snapshots is almost the same about 2 6 s and the time consuming calculation of orthogonal basis is negligible compared with other operations the time consumption of matrix reduction projection offline residual calculation and online error estimation increase significantly with the increase in the reduced order base vectors and the reduction projection is particularly obvious for example when the order of the rom is 100 200 and 300 the time consuming ratios of the total time consuming are 10 02 37 19 and 47 94 which are equivalent to the time consuming calculation of 0 28 2 41 and 5 61 snapshots respectively therefore when using gsm to build a rom the use of a large scale training sample set and appropriate convergence error accuracy is advisable to avoid falling into the late iteration otherwise the later iteration may be time consuming and unacceptable as the iteration progresses the time cost of igsm does not increase significantly the reason is that igsm does not need to estimate the posterior error and converges faster than gsm therefore the time cost of igsm is much lower than that of gsm 4 3 parameter inversion of natural seepage field in this field case study the igsm mmrde algorithm was used to inverse the equivalent hydraulic conductivity of the rock mass in the dam site area of the hydropower station the inversion problem has been described in qian et al 2019 the 20 exploration period borehole water level data are used as the observation data set to guide the inversion of the igsm mmrde to facilitate comparison the inversion results of igsm mmrde and the results obtained by the adina mmrde model in qian et al 2019 are listed in table 6 table 7 shows the comparison between the hydraulic heads observed in the field and the hydraulic heads calculated by the two models the results show a small difference between the parameters estimated by adina mmrde and igsm mmrde but the difference in the key parameters with larger composite scaled sensitivities analyzed in our previous study qian et al 2019 is small and the borehole water level of the two models is different the calculated value is generally in good agreement with the measured value indicating that igsm can be used for parameter inversion except for a few boreholes zk111 and zk101 most of the differences do not exceed 5 m i e the relative error is below 5 therefore the inversion model based on the best fitting parameters basically meets the seepage characteristics of the natural seepage field and the inversion parameters are more reasonable the difference between the observation hydraulic head and the hydraulic head calculated at borehole zk111 is more than 12 m thereby far exceeding the difference between the calculation hydraulic head and the observation hydraulic head of other boreholes this phenomenon may be due to a large change in the permeability of the rock formation at zk111 or it may be caused by a measurement error in the borehole data fig 7 reflects the iterative process of igsm mmrde the convergence process of the igsm mmrde is very fast in the early stage of iteration the sse dropped significantly and the parameters changed drastically which were determined by the global search capability of mmrde in the later stage of iteration the changes in sse and parameters are relatively small because this stage is undergoing local optimization generally the more inversion parameters the larger the population size required and the longer the time required for each evolutionary generation table 8 shows the inversion results and time consumption comparison of adina mmrde and igsm mmrde when dim is 6 and 13 the result shows that the parameter result of igsm mmrde inversion is closer to the result of adina mmrde but the time cost required by the two is quite different when dim 6 and dim 13 the time cost of adina mmrde population size is 25 and 50 100 generations is 48 78 h and 94 78 h respectively however igsm mmrde population sizes of 30 and 65 300 generations of evolution only takes 0 12 h and 0 29 h which account for 0 54 and 0 31 of the time consumed by adina mmrde respectively in addition the objective function of igsm mmrde is even better than adina mmrde indicating that adina mmrde s search in the 100th generation is insufficient all algorithms in the study were coded using fortran90 and compiled on a windows 7 pc with 3 30 ghz intel coretm 2 i5 4590 cpu 16 gb ram and intel visual fortran composer xe 2013 with vs2010 5 conclusion reducing the computational cost of the simulation optimization model has important practical significance for the inversion of rock and soil hydraulic conductivity in this study an igsm based on model order reduction technique is proposed to estimate the hydraulic conductivity of steady state seepage problem rapidly compared with the classical gsm igsm does not need to calculate the posterior error nor provide a sample set for training in each igsm iteration an inversion process is completed different from conventional inversion it uses the reduced order model generated by the previous generation to replace the original model to search for the hydraulic conductivity that best matches the observation information and uses the searched parameters to improve the reduced order model of the next iteration igsm calculates the reduced order accuracy using the maximum nodal error between the full order solution and the reduced order solution for the optimal parameters the comparison of igsm with gsm shows that the performance tests are carried out in terms of convergence error cell division density number of inversion parameters measurement error and time consumption finally the inversion model of the rock mass hydraulic conductivity of the natural seepage field in the dam site area of a hydropower station is used to verify the effectiveness of igsm in reducing computational cost and maintaining accuracy the following conclusions can be drawn from these experiments 1 the error accuracy of the iteration termination condition has a great influence on the inversion results but the additional computational cost required to improve the accuracy is insignificant 2 the element division density has minimal effect on the model s order reduction effect but the inversion dimension and parameter variation range have a greater influence on the order reduction model compared with gsm igsm can handle the inversion problems of high inversion dimension and wide parameter variation range higher inversion accuracy can only be achieved by adding fewer iterations 3 the sensitivity of igsm mmrde to observation errors is similar to that of adina mmrde based on the original model and it can only be applied to parameter inversion problems with standard deviation less than 0 1 sensitivity to observation error is a common problem of most simulation optimization models based on stochastic optimization methods and bayesian optimization algorithm can be used for further research 4 igsm mmrde can be used to perform the inversion task of the initial seepage field of large scale projects by applying igsm mmrde to the inversion of the rock mass hydraulic conductivity of the natural seepage field in the dam site of a hydropower station igsm mmrde can not only obtain the same accurate inversion results as adina mmrde but also takes less than 1 of adina mmrde the order of rom is about 1 2000 of the original model which has a very effective time saving ability the igsm proposed in this study is a new framework for solving the parameter inversion problem based on the reduced order model which is applicable but not limited to the parameter inversion of the seepage problem although it can solve the steady state seepage inversion problem quickly and with low computational cost its limitations are also very apparent first igsm is proposed under the framework of steady state seepage and it currently cannot solve the time dependent inversion problem of course after solving the problem of how to extract snapshots effectively so that the response relationship between hydraulic head and time can be generalized accurately the igsm can be extended to solve the parameter inversion task of unsteady seepage which is also our next research work once the successfully extended igsm is used for the non steady state parameter inversion problem the efficiency of reducing the computational cost will be significantly higher than that of the steady state inversion problem the calculation of each time step in the non steady state problem is equivalent to calculating the steady state problem once second unlike the gsm the igsm cannot output a reduced order model that matches the original model well under each parameter but only matches the original model well near the inversion parameters in addition the inversion success rate of the igsm also greatly depends on the global convergence ability of the optimization algorithm nested in the iteration therefore the newly proposed high performance group optimization algorithm is recommended to be integrated into the framework of the igsm overall the proposal of igsm is of great significance and it helps solve the time consuming problem of simulation optimization problems credit authorship contribution statement wuwen qian conceptualization methodology software investigation formal analysis writing original draft junrui chai resources software writing review editing xinyu zhao validation supervision writing review editing jingtai niu resources supervision fang xiao data curation writing original draft zhiping deng validation funding acquisition declaration of competing interest we declare that we have no financial and personal relationships with other people or organizations that can inappropriately influence our work there is no professional or other personal interest of any nature or kind in any product service and or company that could be construed as influencing the position presented in or the review of the manuscript entitled inversion method of hydraulic conductivity for steady state problem based on reduced order model constructed by improved greedy sampling method acknowledgements this study is supported by the youth science foundation of education department of jiangxi province gjj201922 and the national natural science foundation of china 52009054 appendix a model reduction technology based on gsm a 1 galerkin projection this research only focuses on the steady state seepage problem let l u r n h express the residual form of eq 3 a 1 l u au q the problem of finding the solution of the fom l u 0 is equivalent to the problem of calculating the vector u to satisfy equation l u t z 0 z r n h 45 if a projection matrix p with a shape of nr nh can be determined its matrix columns form an orthonormal basis and can generate subspaces w r n h then the finite element model also called the fom can be projected into the subspace v r n r therefore the galerkin projection problem can be defined as follows the reduced order solution u is calculated so that l u t w 0 w w holds since any vectors w w can be written as a linear combination of some vectors in the reduced order basis matrix p the galerkin projection can be simplified as p t l u t 0 applying this projection to eq 3 we obtain a 2 p t a u p t q after galerkin projection the fom that solves nh linear equations has been converted into a reduced order system that solves nr unknowns therefore the amount of calculation can be reduced the approximate solution u can be expressed as a 3 u p u r substituting the above formula into eq a 2 we can yield a 4 p t ap u r p t q let a r p t ap and q r p t q then we get a 5 a r u r q r where a r and q r are nr nr order matrix and nr dimensional vectors respectively the reduced order system can be solved by any method of solving linear equations such as gaussian elimination method and lu decomposition method after solving u r u can be calculated by eq a 3 the above model for calculating u in a low dimensional manner eq a 5 is called a reduced order model rom and the original finite element calculation model represented by eq 3 is called a fom a 2 posterior error estimate the use of a rom to replace the original model will produce reduced order errors inevitably if the error between the rom and the fom is small then the rom can be considered accurate the error that corresponds to the parameter μ is defined as follows a 6 e μ u μ p u r μ the residuals are defined as follows a 7 r u r μ q μ a μ p u r μ from eqs a 6 a 7 the relationship between error and residual can be written as a 8 a μ e μ r u r μ given that a is a symmetric positive definite matrix the error bound δ r μ can be expressed as quarteroni et al 2016 a 9 e μ x x 1 2 a 1 μ x 1 2 2 r u r μ x 1 1 β μ r u r μ x 1 δ r μ where x ij φi φj v is the matrix related to the basis of φ i i 1 n h β μ is the stable constant that can be obtained by solving the generalized eigenvalue problem or can be approximated by an approximate method such as min θ method the process of constructing a rom can be decomposed into offline and online calculation stanko et al 2016 the offline phase needs to perform very extensive parameter independent preprocessing e g calculating a q q q and residual terms to pave the way for the subsequent very inexpensive execution of online calculations therefore the offline phase is very expensive i e intolerable for single or few evaluations the online phase quickly calculates the corresponding output for each new input and evaluates the error a 3 extraction of the optimal reduced order basis greedy sampling algorithm gsm is a method that requires fewer snapshots to construct a reduced order subspace it is implemented by iterative sampling from the parameter space ξ based on the optimality criterion of the posterior error estimation a training sample set ξ train ξ is often used instead of the parameter space to convert the optimization problem of finding the largest error bound on ξ into a lower cost enumeration problem that is the error bounds that correspond to ξ train in descending order is listed and the largest one is selected notably gsm only adds a new optimal basis vector to expand the reduced order subspace in each iteration instead of optimizing all possible n dimensional subspaces in the n 1th iteration gsm selects the optimal sample according to eq a 10 a 10 μ n 1 arg max μ ξ train δ r μ where r μ represents the posterior error bound that corresponds to the parameter μ a 4 iteration termination condition gsm often uses error accuracy and or the maximum number of iterations as termination conditions literature quarteroni et al 2016 recommends the use of relative error as the iteration termination condition for greedy algorithms namely a 11 max μ ξ train δ r μ u μ v ε where u μ represents the approximate solution calculated by the rom that corresponds to parameter μ and ε represents the relative error threshold specified by the user the procedure of gsm to construct the reduced order basis vectors is shown in table a 1 
124,to solve the time consuming parameter inversion problem based on swarm evolution algorithm this study proposes an improved greedy sampling method gsm igsm based on model reduction technology to rapidly estimate the hydraulic conductivity of steady state seepage problem different from the traditional gsm igsm neither needs to estimate the posterior error bound of the reduced order model rom nor does it require a training sample set to train the rom instead the observation data are directly applied to the training process of rom each iterative process of igsm includes a parameter inversion process to obtain the parameters that best match the observation information and use it to update the current rom the improved differential evolution algorithm e g mmrde is used as the search algorithm and the resulting algorithm is denoted as igsm mmrde after comparative testing igsm mmrde was found to have higher accuracy and lower calculation cost than gsm mmrde in addition igsm mmrde is also capable of high dimensional parameter inversion problems and parameter inversion with a wider search range after applying igsm mmrde to the inversion of the rock mass hydraulic conductivity of the natural seepage field in the dam site area of a hydropower station igsm mmrde converges quickly and takes much less time than adina mmrde based on the full order model approximately 0 31 of adina mmrde the performance results are even better than the adina mmrde using the fixed evolutionary generation therefore the advantages of using igsm in the rapid inversion of the initial seepage field of large scale projects are extremely evident keywords reduced order greedy snapshot hydraulic conductivity inversion method steady state seepage 1 introduction although the improving computer performance is crucial to the large dimensional numerical solution of complex phenomena model reduction calculation still plays a decisive role when interested in real time simulation and or repeated output evaluation of different values of some inputs e g monte carlo simulation pasetto et al 2011 pasetto et al 2013 pasetto et al 2014 baú 2012 parameter estimation wang and zabaras 2005 grepl et al 2007 nguyen et al 2010 legresley and alonso 2000 in view of the wide application of numerical methods in engineering practice and many specific industrial processes model reduction techniques applied to numerical models have emerged the model reduction calculation method is used to evaluate the input output relationship quickly and reliably where the output is expressed as a function of the field variable as the input parameterized partial differential equation solution groundwater model reduction techniques can be roughly divided into three categories data driven projection based and structural reduction methods data driven methods include artificial neural networks taormina et al 2012 some forms of polynomial chaotic expansion oladyshkin and nowak 2012 or bayesian networks fienen et al 2013 proper orthogonal decomposition pod is the most prominent projection based methods vermeulen and heemink 2004 ptm vermeulen heemink and valstar 2005 mcphee and yeh 2008 aj siade putti and yeh 2010 followed by fourier model reduction willcox and megretski 2005 the structural reduction method reduces the size of the model directly and simplifies the representation of the process von gunten et al 2014 it can also perform parameter reduction such as the inversion based upgrade method doherty and christensen 2011 adopted by doherty and christensen pod refers to a decomposition technology that can systematically extract the modes with the largest energy from a set of output of the system and these modes can be used as the basis for galerkin s simplified control equation this output is usually a set of snapshots generated on various model parameter values in experiments or numerical simulations a characteristic of pod is that it produces an optimal basis in other words a n order pod base contains more energy than any other n order bases pod as a model simplification tool for simulation was originally used to calculate turbulence after decades of efforts by researchers it has been extended to different applications and equation types ly and tran 2001 ganapathysubramanian and zabaras 2004 hinze and kunkel 2012 such as fluid dynamics kunisch and volkwein 2002 and incompressible navier stokes equations ss ravindran 2000 sirisup et al 2005 vermeulen et al vermeulen et al 2004 established two roms to describe the hydraulic head in a three dimensional groundwater flow model that is by combining pod with state space projection and galerkin projection and compared the performance of these two roms in actual cases vermeulen and heemink p vermeulen heemink and valstar 2005 established an inversion model of groundwater flow based on pod and galerkin projection this method reduces the time to solve the groundwater inverse problem by 60 and the inversion accuracy is equivalent to the original model gunzburger et al gunzburger et al 2007 extended the pod order reduction method to the situation where the boundary condition contains multiple parameters astrid et al astrid et al 2008 derived an effective rom of large scale variable parameter systems based on a newly proposed missing point estimation method and adopted an effective heuristic method to select the optimal spatial subset this method only uses 25 of the spatial grid points to calculate the galerkin projection and does not affect the accuracy of the rom boyce et al boyce et al 2015 extended the pod reduction technology to modflow and made a standard package based on all the characteristics of modflow to reduce the computational burden of high dimensional groundwater simulation different reduction methods are also used to build reduction models because algorithm cooperation is better than algorithm competition argáez et al 2016 stanko et al 2016 flórez and argáez 2018 the accuracy of rom is closely related to the choice of the reduced order basis and the reduced order basis of the pod is obtained from the outputs i e snapshots of the full order model fom of a specific input parameter set although the basis generated by the pod is sufficient to reproduce the behavior of the model under the influence of a specific input and it may not be sufficient when the system is under the control of an unknown input atwell and king 2001 therefore an effective sampling method is the key to ensuring reliability accuracy and efficiency to enhance pod and make it an effective and practical tool for numerical research of complex and high dimensional systems selecting a suitable snapshot is particularly important ss ravindran 2000 christensen et al 1999 ravindran 2002 as the posterior error estimation methods that correspond to various specific reduced order basis are proposed and proven haasdonk and ohlberger 2011 veroy et al 2003 grepl and patera 2005 the greedy sampling method gsm based on the posterior error estimation is used to construct the reduced order basis of the rom bui thanh et al 2008 binev et al 2011 buffa et al 2012 this method avoids the calculation of fom in the error estimation process so the posterior error can be estimated inexpensively this method can effectively explore the parameter domain when looking for the most representative snapshot and determine whether the existing basis functions are sufficient clénet et al clenet et al 2015 proposed a method to adaptively construct a rom this method adaptively obtains snapshots used to construct reduced order basis through gsm and is not affected by the curse of dimensionality ushijima and yeh ushijima and yeh 2015 developed an experimental design algorithm to select the location of the observation well network to provide the most robust information about the unknown hydraulic conductivity in the aquifer this method uses the gsm to construct a reduced order basis to lessen the dimensionality of the groundwater model and uses genetic algorithms to perform exhaustive search on the rom for the confined aquifer problem to keep the appropriate number of snapshots of the complete model at the appropriate time to obtain the most accurate rom possible siade et al aj siade putti and yeh 2010 proposed a method for selecting the optimal time snapshot of the model based on the characteristic that the confined aquifer reached a stable state exponentially and used a real case to prove the simplicity and efficiency of the method two years later they combined the pod based on the optimal time snapshot with quasi linearization and quadratic programming for the inversion of hydraulic conductivity siade et al 2012 boyce and yeh boyce and yeh 2014 proposed a non parametric method for establishing a reduced order transient groundwater flow model this method uses two greedy algorithms to iteratively select the optimal parameters and snapshot time between the parameter space and the time domain to generate snapshots of the rom and uses a markov chain monte carlo method to solve the inversion problem however this iterative method of selecting the optimal snapshots is expensive this research is dedicated to the parameter inversion of steady state seepage problems and an improved greedy sampling method igsm for parameter inversion is proposed this method does not need to calculate the posterior error it applies the observation data to the training process of the rom and uses the improved differential evolution algorithm mmrde based on the group optimization technology to extract the optimal parameters from the parameter space different from the classic gsm the optimal parameter of this method is defined as the parameter that best matches the rom with the observation data that is to solve the minimum problem after testing this method converges quickly has higher accuracy and is less time consuming than the inversion model based on the fom it can be applied to the inversion task of the initial seepage field of large scale projects 2 steady state seepage control equation without considering the compressibility of water and soil and imposing no slip conditions at the microscale solid fluid interface according to the continuity equation of water and darcy s law the governing equation of three dimensional steady state saturated groundwater flow in an orthotropic confined aquifer can be derived as wang et al 2020 1 x k x h x y k y h y z k z h z 0 where h represents the hydraulic head l kx ky and kz represent the hydraulic conductivities l t of the x y and z directions respectively this study uses orthotropy to describe the permeability inhomogeneity of the aquifer and assumes that the principal directions of the permeability tensor coincide with the x y and z directions of the adopted coordinate system the boundary conditions that need to be met are generally as follows 2 h h x y z o n γ e q n k x h x n x k y h y n y k z h z n z q o n γ n q n λ h α o n γ m where г e г n and г m represent the first second and third types of boundary conditions i e constant hydraulic head boundary constant flow boundary and convective boundary respectively h and q represent the known functions qn represents the flow rate on the flow boundary and λ and α are known values after using the weighted residual method to apply the superposition principle to eqs 1 2 and perform spatial discretization the overall linear partial differential equation can be obtained as follows 3 au q where a represents the nh nh order permeability matrix nh is the total number of nodes in the finite element model u represents the nh dimensional hydraulic head vectors and q represents the nh dimensional vectors of nodal flux according to the parameter partition θ a q μ calculate aq u v q 1 qa aq u v is independent of μ and its quantity is qa similarly calculate q q v q 1 qf q q v is also independent of μ and its quantity is qf then the permeation matrix a u v μ and the column vectors of nodal flux q v μ that correspond to the parameter μ can be written as follows in the form of a bilinear norm hesthaven et al 2016 4 a u v μ q 1 q a θ a q μ a q u v q v μ q 1 q f θ f q μ q q v 3 igsm gsm trains a rom after a certain iteration to match the characteristics of fom in any parameter gsm needs to use eq a 10 to extract the optimal parameters in each iteration but the optimization here is only for the training sample set used which is only the tip of the iceberg of the entire sample space the smaller sample set cannot represent the parameter space well although the rom constructed by the larger sample set can reflect the characteristics of fom better the amount of calculation is too large and the time consumption is too long therefore when the prior knowledge is not clear how to select a suitable training sample set to construct rom is a very difficult problem for this reason we propose an igsm that does not require training sample sets to solve the parameter inversion problem of steady state seepage 3 1 algorithm description the iterative idea of the igsm proposed in this study is completely opposite to the classical gsm the rom trained by igsm cannot be used as a good substitute for the original model for result output but it can quickly solve the problem of parameter inversion of steady state seepage in the iterative process igsm does not need to estimate the posterior error of the current rom unlike the gsm which considers the parameter with the worst matching between rom and fom that is the parameter with the largest posterior error as the optimal parameter igsm searches for the hydraulic conductivity that best matches the observation information to improve the reduced order model of the next iteration in other words gsm and igsm solve the maximum and minimum problems in each iteration respectively the objective function of igsm during the search is the degree of matching between the reduced order solution under the current rom and the observation information there are many widely used objective functions such as the weighted sum of squared errors sse the weighted mean absolute error mae and the weighted root mean square error rmse igsm uses sse as the objective function to extract the optimal parameter samples in the parameter space ξ a group optimization algorithm needs to be used in this study igsm uses an improved differential evolution algorithm mmrde qian et al 2018 with a relatively powerful global search capability to guide the search the igsm mmrde is expressed as adopting the mmrde as the optimization algorithm in the igsm framework similarly the inversion algorithm formed by combining the gsm and mmrde is denoted as gsm mmrde the mmrde is used to solve the minimum problem as described in eq 5 5 s s e r i 1 n w i y i y r i 2 μ n 1 arg min μ ξ s s e r μ where yi and y i are the calculated and observed values of hydraulic head at the i th observation point respectively n is the total number of observation points and wi is the weight coefficient that corresponds to each observation point the subscript r indicates that it corresponds to the rom system by continuously picking up the optimal parameters and expanding the projection matrix used for model reduction the rom will gradually show the morphological characteristics of the objective function of the fom in the parameter space to finally determine the optimal parameters of the parameter inversion problem excellent algorithms can avoid unnecessary calculations therefore an effective exit mechanism is indispensable for igsm when the morphological characteristics of the objective function displayed by the rom are sufficient to determine the optimal parameters it needs to trigger the exit mechanism in each iteration the fom solution of the picked optimal parameters needs to be calculated as a snapshot to make effective use of this hard won fom solution we need to calculate a rom solution of the optimal parameters in each iteration and compare the rom solution with the fom solution to determine the maximum value of node error δ when δ is less than ε the iteration ends and it can be considered that the topographic features of the objective function displayed by the generated reduced order model are sufficient to determine the optimal parameters the termination condition of the iteration is described in eq 6 6 m a x h r j h f j ε where h r j and h f j represent the j th node hydraulic head that correspond to rom and fom respectively it should be clarified that the igsm algorithm is a framework for rapidly solving the parameters inversion problem of steady state saturated seepage and the applicability of igsm depends on whether the snapshot method can be used to construct a reduced order model of the original model therefore the igsm can be applied to confined aquifer systems with intra layer heterogeneity characteristics of natural aquifer systems because the problem can be reduced order model constructed by collecting snapshots pasetto et al 2014 aj siade putti and yeh 2010 3 2 algorithm steps igsm has a simple structure and a fast convergence rate fig 1 shows the iterative process of igsm to solve the one dimensional hydraulic conductivity inversion problem of pumping well modified from qian et al 2019 to illustrate the igsm parameter inversion process 1 a set of hydraulic conductivity μ 1 which is used to calculate snapshots and generate rom the order of the model n 1 is randomly generated 2 the mmrde is used to solve eq 5 determine the optimal hydraulic conductivity μ 2 and calculate the snapshot to build a new rom n 2 3 the mmrde is utilized to determine the optimal parameters μ 3 calculate the snapshot and generate the rom n 3 to be used in the next generation 4 the mmrde is used to determine the optimal parameters μ 4 150 003 and calculate the snapshot at this time the iterative termination condition is triggered and the iteration is terminated therefore μ 4 is outputted as the inversion parameter the pseudo code of igsm algorithm is shown in table 1 fig 2 reflects the flowchart comparison between the gsm mmrde and the igsm mmrde 4 numerical examples 4 1 algorithm setting to test the performance of igsm in parameter inversion the inversion problem of the initial seepage field described in qian et al 2019 is used as the inversion example of this research the finite element model of this inversion problem is hereinafter referred to as the test model mmrde is used as the search algorithm and it is combined with gsm and igsm to form the gsm mmrde and the igsm mmrde respectively gsm mmrde and fom mmrde are taken as the comparison algorithm of igsm mmrde and their inversion results are compared with that of the igsm mmrde the population size np and the maximum number of function computations are set to 5 dim and 200 np respectively the other parameters of mmrde are the same as the original reports 4 2 inversion test of igsm mmrde 4 2 1 influence of convergence accuracy convergence accuracy ε has a great influence on the training effect of the rom to study the influence of ε on the inversion results of igsm mmrde and gsm mmrde the test model is used as the fom since gsm always picks up the sample that corresponds to the maximum error in each iteration process to expand the projection matrix it also considers the impact of different training sample set sizes ns on the inversion results to reduce the influence of randomness on the inversion results each operating condition was run 10 times independently and the average value of 10 runs was compared the results are shown in fig 3 the bar graph embedded in fig 3 corresponds to the average number of iterations required for the algorithm to converge at each precision the results show that gsm mmrde and igsm mmrde are greatly affected by ε given that gsm mmrde uses the rom trained by the gsm to replace the fom for parameter inversion and the gsm uses the maximum relative error to calculate the accuracy of the reduced order model ns and ε can affect the inversion results when ns is same the smaller ε the more order of the rom and the more accurate the inversion result for example when ns 500 the average number of iterations required for error accuracies ε of 0 1 0 01 and 0 001 are 28 8 74 1 and 138 1 respectively this result is also shown in figs 3 a b and c in 10 independent runs the smaller the ε corresponds to the smaller the fluctuation of the inversion parameter and the closer it is to the ideal value the larger ns the more accurate the inversion result is under the same ε when ε 0 1 the average number of iterations required for ns 500 1000 and 2000 are 28 8 31 3 and 34 6 respectively this result means that only 5 76 3 13 and 1 73 of the samples from the ns samples were selected for rom construction as the scale of the training sample set expands this proportion will become smaller and smaller under the same precision indicating that setting a higher precision in a smaller ns is more economical than setting a smaller ε in a larger ns ns cannot be too small otherwise the solutions that correspond to these parameters cannot reflect the characteristics of the model considering that the inversion results fluctuate greatly when ε 0 1 we recommend ε 0 01 and ns 1000 for the igsm mmrde since it uses the mmrde to search for the hydraulic conductivity that best matches the observation information in the parameter space instead of the training parameter set the igsm mmrde does not need to set ns the igsm mmrde uses the maximum nodal error between the reduced order solution and the full order solution that corresponds to the inversion parameters of each iteration to calculate the reduced order accuracy accuracy ε has a great influence on the inversion results of the igsm mmrde from the analysis of the operation mechanism of the igsm mmrde it can be known that in the later iteration of the igsm mmrde the sampling points are always located near the optimal inversion parameters this finding means that the rom is finely matched near the inversion parameters and the error accuracy ε will directly affect the matching accuracy of the rom and the fom fig 3 d shows that with the decrease in ε the smaller the fluctuation of the inversion result is the more accurate the inversion result will be and the increase in the number of iterations is not significant with the improvement of the accuracy with ε 0 001 almost every run obtains results that are consistent with the ideal therefore ε 0 001 is used as the convergence accuracy requirement of the igsm mmrde in calculations below 4 2 2 influence of element division density the accuracy of the finite element model is closely related to the element size generally speaking the smaller the element size the higher the accuracy and the correspondingly more nodes the greater the computational cost in practical applications the meshes are usually encrypted appropriately to obtain higher accuracy to study the influence of different element sizes on the rom the modified example in qian et al 2019 was used as the fom denoted as 3wells model and its mesh was further subdivided the models with element sizes of 250 250 m and 125 125 m are obtained and compared with the original model element size is 500 500 m the mesh division and hydraulic head distribution of each model are plotted in fig 4 to reduce the result deviation caused by the randomness of the algorithm each algorithm runs 10 times independently under each working condition the results show that when the element sizes are 500 500 250 250 and 125 125 m the orders of roms that are built by gsm are 19 19 6 and 20 4 respectively and the orders of roms that are built by igsm mmrde are 7 6 7 8 and 7 4 respectively the average value of the iteration errors of each algorithm in 20 generations are taken to draw the convergence process curve fig 5 the partition density has a minimal effect on model reduction this shows that the rom has a greater advantage in the fine model than the coarse model 4 2 3 influence of the number of inversion parameters when using swarm optimization algorithm to solve the parameter inversion problem the amount of calculation is greatly affected by the number of inversion parameters inversion dimension the more dimensions the more model forward operations that need to be performed and the longer the inversion time required when using a rom for parameter inversion the inversion time is mainly determined by the time to establish the rom therefore the influence of the number of inverted parameters on the model reduction must be studied by using the test model as the original model the inversion results of fom mmrde gsm mmrde and igsm mmrde are calculated when the inversion dimensions dim are 2 4 and 6 respectively to reduce the deviation caused by randomness each operating condition is run independently for 10 times and the average value of the inversion results is shown in table 2 the results show that dim has a greater impact on the construction of the rom with the increase in inversion parameters the number of snapshots required to construct a rom with a predetermined accuracy also increases and the time consumption is greater however the increase in time consumption speed required by gsm mmrde and igsm mmrde is inconsistent when dim is 2 4 and 6 the average inversion time used by gsm mmrde accounts for approximately 12 37 11 98 and 15 30 of fom respectively whereas igsm mmrde accounts for approximately 7 88 2 56 and 1 03 of fom respectively this result shows that the time cost saving effect of igsm mmrde is very evident in addition with the increase in inversion parameters under the same convergence accuracy and sample set size the deviation of the inversion results of gsm mmrde gradually increases and igsm mmrde can obtain results that are consistent with fom mmrde this finding shows that igsm mmrde has a great advantage over gsm mmrde in solving high dimensional optimization problems 4 2 4 influence of measurement errors generally the variation law of the water level measurement error observed in the field follows a normal distribution therefore to study the sensitivity of the model to measurement errors noises that follow a normal distribution are added to the observation data to simulate measurement errors and are denoted as n μ σ where μ is the mean and σ is the standard deviation to study the sensitivity of observation errors when using igsm mmrde for parameter inversion noise data with μ 0 and σ of 0 01 0 1 and 1 are generated respectively after adding these noise data to the observation data three data sets namely data set 2 3 and data set 4 data set 1 record the original observation data with no measurement errors are obtained the inversion results of igsm mmrde and the fom mmrde and gsm mmrde models are run independently for each working condition 10 times and the average value of the inversion parameters under each working condition is taken as the inversion result as shown in table 3 the results show that the performance of the three algorithms in each observation data set is relatively similar when no observation error or σ is less than 0 1 the parameters obtained by the three algorithms are very close to the true values of the assumed parameters when σ is 1 and 0 1 the hydraulic conductivity estimated by the three algorithms are not accurate interestingly the inversion results of gsm mmrde in data sets 2 and 3 are slightly better than fom mmrde and igsm mmrde this finding shows that the reduced order error of gsm can offset part of the observation error which is beneficial to the inversion of the actual parameter 4 2 5 influence of parameter variation range in general parameter inversion should be performed within a reasonable range of parameter variance however in actual production practice searching within a larger search range is desirable due to some errors that lead to large variance in parameters to study the influence of the parameter variation range on the rom based parameter inversion the test model is used as the fom and the 6 parameters e g p1 p2 p3 p4 p7 and p8 most sensitive to the observation information are selected for inversion the parameter change range of the original inversion problem is marked as reference group 0 and the parameter ranges of lbound and ubound by 1 and 2 orders of magnitude are expanded as reference groups 1 and 2 respectively the parameter settings of each reference group are shown in table 4 to save computational cost the maximum number of iterations of gsm is set to 300 the inversion results of igsm mmrde and gsm mmrde in each reference group are shown in table 5 the results show that compared with gsm mmrde the parameter variation range has a smaller impact on igsm mmrde which is only reflected in a slight increase in the number of iterations and time consumption igsm mmrde can invert the true value under each reference group but the inversion result of gsm mmrde becomes less accurate as the parameter variation range expands the reason is that the training rom mechanism of the two methods is different in each iteration igsm picks up the parameter with the smaller objective function to expand the projection matrix thus the matching degree between the rom of its training and the observation data gradually becomes better in addition igsm mmrde utilizes the excellent global convergence performance of the mmrde so that it can invert the real parameters quickly and reliably under a larger parameter variation range the gsm aims to obtain a rom that matches with the fom solution of all parameters in the training sample set well with the expansion of the parameter variation range the fom contains more solution features and the number of samples required for gsm training to meet the convergence accuracy of the rom rises in addition the representativeness of the used training sample set in the entire parameter space also becomes smaller therefore the error of the order reduction becomes larger after the same number of iterations as seen from the maximum relative error of the reduced order model trained by gsm in table 5 4 2 6 time consumption comparison with fom although gsm does not need to calculate a large number of snapshots when constructing the reduced order subspace it needs to estimate the posterior error bounds of each parameter continuously to obtain the optimal snapshot gsm needs to perform snapshot calculation orthogonal basis vector construction reduced order projection operation offline calculation of residual error and online error estimation in each iteration of constructing rom in gsm any of these steps are run in a relatively inexpensive way in terms of time cost compared with gsm igsm only needs to complete the calculation of snapshots and reduced order projection operations to compare the time cost of constructing rom of the two algorithms the test model is used as the fom the number of training samples of gsm is set to 1000 and the maximum number of iterations is set to 300 fig 6 shows the variation curve of the cpu time of each main operation of gsm and igsm in the iterative process the results show that in the iterative process the time consuming calculation of snapshots is almost the same about 2 6 s and the time consuming calculation of orthogonal basis is negligible compared with other operations the time consumption of matrix reduction projection offline residual calculation and online error estimation increase significantly with the increase in the reduced order base vectors and the reduction projection is particularly obvious for example when the order of the rom is 100 200 and 300 the time consuming ratios of the total time consuming are 10 02 37 19 and 47 94 which are equivalent to the time consuming calculation of 0 28 2 41 and 5 61 snapshots respectively therefore when using gsm to build a rom the use of a large scale training sample set and appropriate convergence error accuracy is advisable to avoid falling into the late iteration otherwise the later iteration may be time consuming and unacceptable as the iteration progresses the time cost of igsm does not increase significantly the reason is that igsm does not need to estimate the posterior error and converges faster than gsm therefore the time cost of igsm is much lower than that of gsm 4 3 parameter inversion of natural seepage field in this field case study the igsm mmrde algorithm was used to inverse the equivalent hydraulic conductivity of the rock mass in the dam site area of the hydropower station the inversion problem has been described in qian et al 2019 the 20 exploration period borehole water level data are used as the observation data set to guide the inversion of the igsm mmrde to facilitate comparison the inversion results of igsm mmrde and the results obtained by the adina mmrde model in qian et al 2019 are listed in table 6 table 7 shows the comparison between the hydraulic heads observed in the field and the hydraulic heads calculated by the two models the results show a small difference between the parameters estimated by adina mmrde and igsm mmrde but the difference in the key parameters with larger composite scaled sensitivities analyzed in our previous study qian et al 2019 is small and the borehole water level of the two models is different the calculated value is generally in good agreement with the measured value indicating that igsm can be used for parameter inversion except for a few boreholes zk111 and zk101 most of the differences do not exceed 5 m i e the relative error is below 5 therefore the inversion model based on the best fitting parameters basically meets the seepage characteristics of the natural seepage field and the inversion parameters are more reasonable the difference between the observation hydraulic head and the hydraulic head calculated at borehole zk111 is more than 12 m thereby far exceeding the difference between the calculation hydraulic head and the observation hydraulic head of other boreholes this phenomenon may be due to a large change in the permeability of the rock formation at zk111 or it may be caused by a measurement error in the borehole data fig 7 reflects the iterative process of igsm mmrde the convergence process of the igsm mmrde is very fast in the early stage of iteration the sse dropped significantly and the parameters changed drastically which were determined by the global search capability of mmrde in the later stage of iteration the changes in sse and parameters are relatively small because this stage is undergoing local optimization generally the more inversion parameters the larger the population size required and the longer the time required for each evolutionary generation table 8 shows the inversion results and time consumption comparison of adina mmrde and igsm mmrde when dim is 6 and 13 the result shows that the parameter result of igsm mmrde inversion is closer to the result of adina mmrde but the time cost required by the two is quite different when dim 6 and dim 13 the time cost of adina mmrde population size is 25 and 50 100 generations is 48 78 h and 94 78 h respectively however igsm mmrde population sizes of 30 and 65 300 generations of evolution only takes 0 12 h and 0 29 h which account for 0 54 and 0 31 of the time consumed by adina mmrde respectively in addition the objective function of igsm mmrde is even better than adina mmrde indicating that adina mmrde s search in the 100th generation is insufficient all algorithms in the study were coded using fortran90 and compiled on a windows 7 pc with 3 30 ghz intel coretm 2 i5 4590 cpu 16 gb ram and intel visual fortran composer xe 2013 with vs2010 5 conclusion reducing the computational cost of the simulation optimization model has important practical significance for the inversion of rock and soil hydraulic conductivity in this study an igsm based on model order reduction technique is proposed to estimate the hydraulic conductivity of steady state seepage problem rapidly compared with the classical gsm igsm does not need to calculate the posterior error nor provide a sample set for training in each igsm iteration an inversion process is completed different from conventional inversion it uses the reduced order model generated by the previous generation to replace the original model to search for the hydraulic conductivity that best matches the observation information and uses the searched parameters to improve the reduced order model of the next iteration igsm calculates the reduced order accuracy using the maximum nodal error between the full order solution and the reduced order solution for the optimal parameters the comparison of igsm with gsm shows that the performance tests are carried out in terms of convergence error cell division density number of inversion parameters measurement error and time consumption finally the inversion model of the rock mass hydraulic conductivity of the natural seepage field in the dam site area of a hydropower station is used to verify the effectiveness of igsm in reducing computational cost and maintaining accuracy the following conclusions can be drawn from these experiments 1 the error accuracy of the iteration termination condition has a great influence on the inversion results but the additional computational cost required to improve the accuracy is insignificant 2 the element division density has minimal effect on the model s order reduction effect but the inversion dimension and parameter variation range have a greater influence on the order reduction model compared with gsm igsm can handle the inversion problems of high inversion dimension and wide parameter variation range higher inversion accuracy can only be achieved by adding fewer iterations 3 the sensitivity of igsm mmrde to observation errors is similar to that of adina mmrde based on the original model and it can only be applied to parameter inversion problems with standard deviation less than 0 1 sensitivity to observation error is a common problem of most simulation optimization models based on stochastic optimization methods and bayesian optimization algorithm can be used for further research 4 igsm mmrde can be used to perform the inversion task of the initial seepage field of large scale projects by applying igsm mmrde to the inversion of the rock mass hydraulic conductivity of the natural seepage field in the dam site of a hydropower station igsm mmrde can not only obtain the same accurate inversion results as adina mmrde but also takes less than 1 of adina mmrde the order of rom is about 1 2000 of the original model which has a very effective time saving ability the igsm proposed in this study is a new framework for solving the parameter inversion problem based on the reduced order model which is applicable but not limited to the parameter inversion of the seepage problem although it can solve the steady state seepage inversion problem quickly and with low computational cost its limitations are also very apparent first igsm is proposed under the framework of steady state seepage and it currently cannot solve the time dependent inversion problem of course after solving the problem of how to extract snapshots effectively so that the response relationship between hydraulic head and time can be generalized accurately the igsm can be extended to solve the parameter inversion task of unsteady seepage which is also our next research work once the successfully extended igsm is used for the non steady state parameter inversion problem the efficiency of reducing the computational cost will be significantly higher than that of the steady state inversion problem the calculation of each time step in the non steady state problem is equivalent to calculating the steady state problem once second unlike the gsm the igsm cannot output a reduced order model that matches the original model well under each parameter but only matches the original model well near the inversion parameters in addition the inversion success rate of the igsm also greatly depends on the global convergence ability of the optimization algorithm nested in the iteration therefore the newly proposed high performance group optimization algorithm is recommended to be integrated into the framework of the igsm overall the proposal of igsm is of great significance and it helps solve the time consuming problem of simulation optimization problems credit authorship contribution statement wuwen qian conceptualization methodology software investigation formal analysis writing original draft junrui chai resources software writing review editing xinyu zhao validation supervision writing review editing jingtai niu resources supervision fang xiao data curation writing original draft zhiping deng validation funding acquisition declaration of competing interest we declare that we have no financial and personal relationships with other people or organizations that can inappropriately influence our work there is no professional or other personal interest of any nature or kind in any product service and or company that could be construed as influencing the position presented in or the review of the manuscript entitled inversion method of hydraulic conductivity for steady state problem based on reduced order model constructed by improved greedy sampling method acknowledgements this study is supported by the youth science foundation of education department of jiangxi province gjj201922 and the national natural science foundation of china 52009054 appendix a model reduction technology based on gsm a 1 galerkin projection this research only focuses on the steady state seepage problem let l u r n h express the residual form of eq 3 a 1 l u au q the problem of finding the solution of the fom l u 0 is equivalent to the problem of calculating the vector u to satisfy equation l u t z 0 z r n h 45 if a projection matrix p with a shape of nr nh can be determined its matrix columns form an orthonormal basis and can generate subspaces w r n h then the finite element model also called the fom can be projected into the subspace v r n r therefore the galerkin projection problem can be defined as follows the reduced order solution u is calculated so that l u t w 0 w w holds since any vectors w w can be written as a linear combination of some vectors in the reduced order basis matrix p the galerkin projection can be simplified as p t l u t 0 applying this projection to eq 3 we obtain a 2 p t a u p t q after galerkin projection the fom that solves nh linear equations has been converted into a reduced order system that solves nr unknowns therefore the amount of calculation can be reduced the approximate solution u can be expressed as a 3 u p u r substituting the above formula into eq a 2 we can yield a 4 p t ap u r p t q let a r p t ap and q r p t q then we get a 5 a r u r q r where a r and q r are nr nr order matrix and nr dimensional vectors respectively the reduced order system can be solved by any method of solving linear equations such as gaussian elimination method and lu decomposition method after solving u r u can be calculated by eq a 3 the above model for calculating u in a low dimensional manner eq a 5 is called a reduced order model rom and the original finite element calculation model represented by eq 3 is called a fom a 2 posterior error estimate the use of a rom to replace the original model will produce reduced order errors inevitably if the error between the rom and the fom is small then the rom can be considered accurate the error that corresponds to the parameter μ is defined as follows a 6 e μ u μ p u r μ the residuals are defined as follows a 7 r u r μ q μ a μ p u r μ from eqs a 6 a 7 the relationship between error and residual can be written as a 8 a μ e μ r u r μ given that a is a symmetric positive definite matrix the error bound δ r μ can be expressed as quarteroni et al 2016 a 9 e μ x x 1 2 a 1 μ x 1 2 2 r u r μ x 1 1 β μ r u r μ x 1 δ r μ where x ij φi φj v is the matrix related to the basis of φ i i 1 n h β μ is the stable constant that can be obtained by solving the generalized eigenvalue problem or can be approximated by an approximate method such as min θ method the process of constructing a rom can be decomposed into offline and online calculation stanko et al 2016 the offline phase needs to perform very extensive parameter independent preprocessing e g calculating a q q q and residual terms to pave the way for the subsequent very inexpensive execution of online calculations therefore the offline phase is very expensive i e intolerable for single or few evaluations the online phase quickly calculates the corresponding output for each new input and evaluates the error a 3 extraction of the optimal reduced order basis greedy sampling algorithm gsm is a method that requires fewer snapshots to construct a reduced order subspace it is implemented by iterative sampling from the parameter space ξ based on the optimality criterion of the posterior error estimation a training sample set ξ train ξ is often used instead of the parameter space to convert the optimization problem of finding the largest error bound on ξ into a lower cost enumeration problem that is the error bounds that correspond to ξ train in descending order is listed and the largest one is selected notably gsm only adds a new optimal basis vector to expand the reduced order subspace in each iteration instead of optimizing all possible n dimensional subspaces in the n 1th iteration gsm selects the optimal sample according to eq a 10 a 10 μ n 1 arg max μ ξ train δ r μ where r μ represents the posterior error bound that corresponds to the parameter μ a 4 iteration termination condition gsm often uses error accuracy and or the maximum number of iterations as termination conditions literature quarteroni et al 2016 recommends the use of relative error as the iteration termination condition for greedy algorithms namely a 11 max μ ξ train δ r μ u μ v ε where u μ represents the approximate solution calculated by the rom that corresponds to parameter μ and ε represents the relative error threshold specified by the user the procedure of gsm to construct the reduced order basis vectors is shown in table a 1 
