index,text
25860,lava flows are recurring and widespread hazards that affect areas around active volcanoes having the potential to cause significant social and economic loss the ongoing demographic congestion around volcanoes increases the potential risk and leads to a growing demand for faster and more accurate systems to safeguard the population the main mitigation action for slowing down and possibly diverting lava flows is the building of artificial barriers that can limit their destructive effects and reduce losses here we present a particle swarm optimization algorithm for the configuration of artificial barriers in terms of location and geometric features the goal is to minimize the lava flow impact based on the spatial distribution of exposed elements using the physics based magflow model to run the lava flow scenarios for each barrier configuration our algorithm has been tested on etna italy showing how it can effectively safeguard the threatened areas diverting lava away from them keywords particle swarm optimization artificial barrier lava flow risk numerical simulations magflow 1 introduction volcanic risk can be described as the loss of lives and properties due to hazardous phenomena in a certain volcanic area over specific periods where the consequences are determined by the exposed elements and their level of vulnerability to hazards fournier d albe 1979 from a mathematical point of view risk can be computed as the product of hazard vulnerability and exposure where hazard is a measure of the probability of occurrence of the destructive event vulnerability is a measure of how strongly the areas are affected by the event ranging continuously from 0 for no loss to 1 for total loss and exposure is a quantification of the value of the affected areas in the case of lava flow risk vulnerability is generally assumed to take only the extreme value of 1 total loss since lava flows commonly cause complete devastation through properties or towns thierry et al 2008 favalli et al 2009 pedrazzi et al 2014 del negro et al 2019 consequently the risk from lava flow inundation can be estimated simply as the product of hazard the probability of a lava flow reaching a particular area and exposure so that risk mitigation can be achieved by affecting the hazard i e altering the probability of invasion which requires a way to influence the progress of a lava flow the idea of slowing and diverting lava flows with the use of artificial structures for the mitigation of its risk thus arises from simple physical considerations the emplacement of lava flows is essentially controlled by the effusion rate the rheology of the lava and the topography since the effusion rate and the rheology of the fluid are outside of our control the main viable tool to influence a lava flow remains our possibility to impact the topography in order to divert the flow the topography influences the emplacement both directly by offering preferential directions to the flow and indirectly by influencing the capability for the flow to develop lava tubes known to be particularly hazardous due to their thermal insulation that allows the tunneled lava to preserve a high fluidity for longer times and thus to cover higher distances before cooling off and slowing down walker et al 1973 calvari and pinkerton 1999 when lava is diverted out of its natural channel the flow front is deprived of back thrust and is not able to advance any longer generating a new front from the diversion point if only a partial lava diversion is achieved the flow front stops due to the reduction of the back thrust and none of the two flows resulting from the partial diversion can travel as far as the initial one in both cases it has been experienced that the use of structures such as artificial barriers or ditches to divert lava flows is an effective action for mitigating its risk one of the main volcanoes giving the foremost examples of lava flows with significant impact on densely populated areas is mt etna sicily italy fig 1 a due to the high frequency of effusive eruptions cappello et al 2013 2019 del negro et al 2013 here the building of artificial barriers has been proven to be an effective action for diverting and slowing down the lava flows as experienced during recent eruptions e g 1983 1991 1993 2001 2002 when earthen barriers were built to control lava flow expansion with varying degrees of success colombrita 1984 barberi et al 1993 2003 barberi and carapezza 2004 providing effective and objective solutions is very challenging since the best positioning of these artificial constructions depends on many factors such as the viscosity of the lava the output rates the topography of the affected area the time of intervention and the economic costs for construction for this reason the design of the mitigation measures is generally based on the past experience and knowledge of experts leaving the problem of defining a standard methodology for the construction and location of protection structures still open the task of finding optimal intervention strategies for volcanic risk mitigation can be seen as an optimization process whose goal is to find the best configuration of barriers and ditches minimizing both the damage caused by the diverted flow and the cost of the intervention itself as with all optimization processes the solution can be found by searching through the abstract space of potential solutions for small spaces classical exhaustive methods are usually sufficient but most real world problems such as the design of the mitigation measures for volcanic risk have high complexity non linear constraints interdependencies among variables and huge solution spaces this needs the use of a method capable of solving complex optimization problems in real time metaheuristic algorithms are optimization methods that produce a reasonably good solution in a moderate amount of time and can be used to efficiently solve problems with large number of variables and non linear objective functions thanks to their properties and wide fields of application in the past two decades many metaheuristic algorithms have been proposed some examples are particle swarm optimization pso kennedy and eberhart 1995b genetic algorithms ga holland 1992 simulated annealing sa kirkpatrick et al 1983 tabu search ts glover and marti 2006 ant colony optimization aco dorigo 1992 and differential evolution de price et al 2005 in the literature see e g gogna and tayal 2013 there are different ways to classify metaheuristic algorithms based on characteristics used to differentiate amongst them they include nature inspired e g pso ga versus non nature inspired e g ts population based e g pso ga versus single point search e g ts sa and local search iterative e g pso sa versus greedy e g aco dynamic e g guided local search versus static e g pso objective function among them population based metaheuristics p metaheuristics most of which are inspired by nature represent a reasonable solution when the hazardous event to simulate evolves based on local interactions of their constituent parts they apply the generation and replacement procedure to a family of solutions spread over the search space until the stopping criterion is met for example filippone et al 2013 presents an application of parallel genetic algorithms for optimizing earth barriers construction to divert a lava flow on mt etna and protect the touristic facility of rifugio sapienza our goal is to combine one of the p metaheuristics algorithms with the magflow lava flow model del negro et al 2008 cappello et al 2016 to determine the optimal position and geometric features of more than one structure able to divert the flow in case of an eruption to mitigate lava flows hazard among the population based metaheuristics we decided to use the pso algorithm since its implementation perfectly fits the definition of a barrier and its geometric parts the possibility to use more than one structure forming a particle and it presents a good coupling with magflow too our choice relies also on computational time in which the time necessary to run the magflow simulations plays a huge part since the pso algorithm has proven to converge rapidly in comparison with other models see e g el ghandour and elbeltagi 2018 in which it is compared with ga aco ma and the modified shuffled frog leaping algorithm sfla our pso algorithm minimizes the impact of lava flows based on the spatial distribution of exposed elements and using the numerical magflow model to run the eruptive scenarios associated with each barrier configuration here it has been applied and validated using a possible lava flow erupted from a vent located on the southeastern flank of mt etna the geological map of the mount etna branca et al 2011 shows that a number of eruptions were produced from vents located in this same area more recent eruptions occurred in 1634 and 1792 were characterized by long duration 540 and 340 days respectively and high volumes of lava about 150 m m 3 and 62 m m 3 respectively whose flows arrived to destroy some villages behncke et al 2005 therefore the opening of future eruptive vents in the southeastern flank of the volcano presents a very high potential risk for the destructive impact that the new flows could have on the near urbanized areas del negro et al 2013 2019 we found barrier configurations that have the potential to completely protect the villages threatened by lava showing how this methodology is effectively able to divert the flow safeguarding the inhabited areas the paper is organized as follows in section 2 we present a brief review of the mitigation actions against historical eruptions at mt etna in section 3 we illustrate the main features of the pso algorithm while in section 4 we describe the main characteristics of the implemented algorithm the definition of the search space and the target function the constraints for the barrier placement the way the particle system is initialized and how the swarm evolves in section 5 we provide a numerical example to illustrate the effectiveness of the proposed mitigation strategy in section 6 we discuss the achieved results section 7 concludes this paper with a summary of our results and some possible future developments finally in appendix a we describe the mathematical law used to enforce barrier position constraints while in appendix b we present the results of other two runs for the numerical example provided in section 5 2 mitigation actions against historical effusive eruptions at mt etna the first attempt to control a lava flow dates back to 1669 fig 1b during the most devastating historic eruption of mt etna barberi et al 1993 when the lava was proceeding into the city of catania a group of frightened citizens tried to protect themselves opening a breach on a levee of the flow to change its advance direction unfortunately this initially promising attempt was stopped by hundreds of furious residents of the nearby misterbianco village who feared that their houses could be destroyed by the diverted lava flow after 1669 mitigation measures trying to divert lava flows at mt etna were blocked mainly due to legal disputes related to the fact that the flow diverted to defend one city could threaten another one it was only in 1983 that the italian government authorized a direct intervention constructing an artificial opening of a breach on the levee of a channelized flow to divert the lava from its natural bed into an artificial channel excavated parallel to the natural one this area was a public property so no disagreement with private owners arose the break opened by an explosion on the levee was relatively small and diverted into the artificial channel only 20 30 of the lava while the remaining 70 80 continued to flow into the natural one as an indirect consequence of the explosion a very substantial deviation of the flow out of its natural channel was accomplished and despite only a short partial diversion was obtained the desired result was achieved since most of the lava out flowed due to the obstruction of a nearby lava tube this intervention was only a partial success but demonstrated that man was effectively able to control the expansion of a lava flow after the 1983 eruption mt etna had an intense eruptive activity a particular significant case is the 1991 1993 eruption fig 1b because of the several actions carried out to protect the village of zafferana etnea from being invaded by the lava the strategy adopted in this situation was an improvement of the methods used during the 1983 lava deviation obtaining a total diversion of the flow the measures to defend zafferana included the building of four lava containment earth barriers and several attempts at plugging the lava tube downhill the earth barriers oriented orthogonally to the direction of the flow slowed the front propagation down for a few weeks although they were not able to fully stop it since lava was overflowing on the earth barriers the civil protection authorities had to carry out a drastic lava flow deviation near the vent finally the flow was diverted into an artificial channel by blasting the wall separating it from the natural channel and obstructing downstream the natural tunnel barberi et al 1993 overall during this event 13 earth barriers were built providing a positive experience at mt etna to protect towns from lava flows even if some legal questions had not been fully solved yet however from the detailed reconstruction of lava tube formation and flow evolution of this eruption and by comparison with other historic eruptions of mt etna calvari and pinkerton 1998 showed that without the successful diversion of may 1992 the lava would have overcome zafferana etnea and continued beyond an intervention similar to the one adopted in 1992 was planned to deviate the flow during the 2001 eruption that threatened the tourist facilities in the rifugio sapienza area fig 1b it included the construction of both lateral earth barriers to push the flow toward existing large holes and orthogonal bars to increase the holding capacity of the depressions moreover a possible evacuation of the nearby villages was also planned because of the decreasing of the effusion rate the plan was only partially executed and just 13 barriers were built up to protect the area initially delaying the advance of the flows and then diverting it toward south east away from the facilities barberi and carapezza 2004 finally structures to divert lava flows were planned during the 2002 2003 eruption of mt etna which produced two lava flows fig 1b partially covering the tourist facilities of piano provenzana on the northern side of the volcano and threatening rifugio sapienza on the southern flank andronico et al 2005 in this case a great effort was devoted to the construction of six earth barriers in accordance with the local authorities including the etna volcanological park these actions contributed to mitigate the impact of the lava flows advance considering that on november 24 the flows directed toward the south were deviated away from the touristic facilities of rifugio sapienza unfortunately on december 16 the lava overflowed from the barrier destroying two buildings and cutting the sp92 road before stopping soon after scifoni et al 2010 3 particle swarm optimization the building of artificial barriers at etna volcano has been an effective action for diverting and slowing down different lava flows however the decision of the construction site was mainly based on expert advice leaving the problem of defining a standard methodology for the location of protection structures to mitigate volcanic risk still open for this reason we developed an automatic algorithm based on particle swarm optimization and the magflow model which aims at finding the best configuration of artificial barriers in terms of location and geometric features before describing the main characteristics of our algorithm we briefly present the particle swarm optimization paradigm particle swarm optimization pso kennedy and eberhart 1995a b is a stochastic optimization algorithm based on the simulation of the behavior of socially organized populations in nature such as bird flocks fish schools and animal herds one of the main advantages of pso is that it can be applied to non differentiable non linear problems with a very large search space e g high number of dimensions with good efficiency the idea is that to find the minimum of a real valued function f over a domain d r n we can sample the function by computing f x i for a given set of points x i in d and then move the points x i with some velocity v i towards the positions that achieved the lowest values this results in a swarm s x 1 x 2 x n of n particles that iteratively move within the search space d converging towards the point where f achieves its minimum value putting it in a mathematical framework let us denote by x i t d the position of the i th particle of s at iteration t and by v i t r n its velocity where n is the number of components of each particle each particle has a memory of the best position b i t it has occupied so far i e b i t arg min τ f x i τ with 0 τ t and the swarm as a whole has knowledge of the best position b g t occupied so far by any particle i e b g t arg min i f b i t with 1 i n the initial values for the position x i 0 and velocity v i 0 are chosen randomly and initially b i 0 x i 0 at every iteration the velocity of each particle is updated considering the previous velocity a cognitive factor based on the particle s best position and a social factor based on the swarm s best position formally given fixed weights ω c 1 c 2 and random values r 1 t r 2 t 0 1 the velocity and position update formulas can be written following the improvement proposed by shi and eberhart 1998 1 v i t 1 ω v i t c 1 r 1 t b i t x i t c 2 r 2 t b g t x i t x i t 1 x i t v i t for i 1 2 n the coefficient ω is know as the inertia weight and indicates a tendency of each particle to continue in the direction it was already moving c 1 is the cognitive parameter and it controls the weight with which particles are attracted to their best known position while c 2 is the social parameter and it controls the weight with which particles are attracted to the best known position of the entire swarm 4 mitigation of lava flow risk using pso and magflow to apply the pso algorithm for the mitigation of lava flow risk we need to map the physical problem onto mathematical concepts in such a way that the evaluation of the target function f relies on the lava flow emplacement simulations done with the numerical magflow model and the motion of the particles respects the constraints due to the considered physical problems 4 1 definition of the search space and target function the search space is defined by the possible locations and geometric characteristics for the barriers and the target function to be minimized is the impact of the eruptive scenario we assume that barriers have a winged configuration so that a single barrier can be defined by six values the easting and northing of the central point the length of the two arms the height of the barrier with a negative height indicating a ditch and the two angles langle and rangle that the arms form with respect to the north south and east west direction fig 2 we assume that in the case of intervention multiple barriers may need to be built so a single particle in our swarm is a collection of a fixed number of barriers if there are b barriers the dimensionality of our problem is then 6 b since each particle is defined by the 6 parameters of each of the barriers that compose it the fixed number of barriers represents the maximum number that can be used and its choice depends on the feasibility in terms of the economic possibilities and timing note that the position of a particle is an abstract concept in this case since it includes the physical position but also the angles length and height of each of its barriers our target function is the potential loss associated with a eruptive scenario as a simplification in this initial stage of development we define the loss by simply estimating the amount of inhabited areas reached by the lava flow the computation of the target function requires an assessment of the impact of the barriers on the lava flow emplacement that we compute using magflow a cellular automaton model for lava flow simulations developed at ingv sezione di catania cappello et al 2016 bilotta et al 2012 2019 in magflow the area of interest for the simulation is decomposed into a regular grid of square cells with associated information including the terrain elevation the amount of solid and liquid lava present in the cell and its temperature the evolution function of the automaton is based on a steady state solution of the navier stokes equation for bingham fluids coupled with a simplified physical model for the thermal evolution of the flowing lava vicari et al 2007 magflow has proven to be successful both to reproduce past events with well known characteristics and to predict the paths of lava flows in real time for example during the mt etna eruptions of 2004 del negro et al 2008 2006 vicari et al 2009 hérault et al 2009 2008 bonaccorso et al 2011 ganci et al 2012 and 2011 vicari et al 2011 inputs to magflow are the digital topography of the area of interest taken from a dem digitial elevation model the location of the vent s and the mass flux rate as a function of time optionally it is also possible to specify the location and geometry of an arbitrary number of barriers and ditches that are realized by adding or removing a corresponding amount of height from the dem in the affected areas the output of the magflow model is a sequence of snapshots of the spatio temporal evolution of the emplacement taken at fixed intervals chosen by the user in our case we only look at the final emplacement to integrate magflow with our pso algorithm we also include for each cell the information about whether the cell is inside an inhabited center or not for each particle in the swarm i e for each set of barriers we run a magflow simulation and the target function for that particle i e score of the barrier configuration is computed by counting the number of township cells with lava the objective of the pso is then to minimize this value the main steps of the algorithm can be summarized as follows pre flight a run a magflow simulation without any barriers b calculation of the score set as best score particle system initialization a initialization of a swarm b run magflow simulations with the barrier configuration of each particle in the swarm c calculation of the score d comparison with the best score and replacement if lower swarm evolution a definition of a new configuration b run magflow simulations with the new configuration c calculation of the score d comparison with the best score and replacement if lower the sequence of the swarm evolution phase is repeated until convergence is reached 4 2 quantization of the search space to help reduce the search space all of the dimensions that control the barrier configuration and thus the particle position and velocity in the abstract space are quantized in the sense that they do not vary continuously but only in multiples of a discrete step the size of this step is a user controlled parameter and can be varied for different dimensions the default values used in the examples presented here are 10 m for the position 1 m for the length 5 m for the height and 10 sexagesimal for the angles 4 3 barrier placement constraints in a straightforward application of the pso the search space with our choice of particles would have not only a high dimensionality but also a very large span especially for the choice of the position of the barrier vertex indeed a naive choice for the boundaries of the easting and northing coordinates for the barriers could be the bounding box of the original lava flow emplacement even though in most of these locations barriers would not interact with the lava flow at all see e g fig 3 moreover most of these positions would be equivalent all have the same score leading to a very slow convergence a possible solution is to add a constraint on the barrier location to ensure it actually interacts with the lava flow for example by requiring that the barrier vertex is confined to be inside the lava flow however since the barriers themselves alter the flow emplacement this is impossible to achieve consistently with multiple barriers due to the way they affect each others domain to work around the issue we let the barrier location change freely but we keep track of the lava flow point closest to the barrier placement this information is then used to correct the barrier position if moved too far away from the flow further constraints are posed on barrier placement to avoid unrealistic situations in particular we require that barriers do not get too close to the vent s and that they should not be located inside a town appendix a details how these constraints are enforced 4 4 particle system initialization the first step of our pso algorithm is the initialization of the particles with random positions and velocities due to the constraints on the barrier positions we first run magflow assuming no barriers are present this gives us an upper bound to the best score for the algorithm and a reference emplacement of the lava flow to guide the initial placement of the barriers the particles for the swarm are then generated the procedure we will describe momentarily to generate the initial positions and velocities of the particles was designed to solve the mapping issue between corresponding barriers of the different particles to illustrate the issue assume for simplicity that each particle in the swarm is composed of two barriers b 2 then particle i will have coordinates b i 1 b i 2 e i 1 n i 1 α i 1 β i 1 l i 1 h i 1 e i 2 n i 2 α i 2 β i 2 l i 2 h i 2 easting northing right and left angle length and height for the first barrier and for the second barrier in order now consider a second particle j with coordinates b j 1 b j 2 e j 1 n j 1 α j 1 β j 1 l j 1 h j 1 e j 2 n j 2 α j 2 β j 2 l j 2 h j 2 and assume that the barriers of particle i and j are as in fig 4 the first barrier of the first particle b i 1 is closer to the second barrier of the second particle b j 2 while the second barrier of the first particle b i 2 is closer to the first barrier of the second particle b j 1 in extreme situations the two particles could represent identical configurations but with swapped barrier order while the pso algorithm assumes a component by component mapping between particles in these cases some kind of transformation would be required to improve convergence to avoid these situations and ensure a correct component mapping we generate particles iteratively as perturbations of an initial configuration specifically we first generate the first particle of the swarm by generating random position velocities angles length and height for each of the particle barriers the only constraint in this case is that the positions of the barriers are not too close to the vent and not too close to the other barriers being generated for each of the other particles we only generate the first barrier randomly the position of all other barriers in the particle is obtained by shifting the corresponding barrier of the first particle by the distance vector between the first barrier of the first particle and the first barrier of the particle being generated fig 5 after all particles are generated their positions are adjusted to ensure that the other constraints on the position are satisfied moving them closer to the flow and outside of the towns as necessary the details about the application of these constraints are illustrated in appendix a 4 5 swarm evolution the evolutionary step of the swarm can be summarized in the following sequence we compute the target function for each of the particles in the swarm if the current score is lower than the previous best score for that particle the best score and corresponding configuration of the particle is saved for reference if the lowest score among the particles is lower than the previous record the best score and corresponding configuration of the swarm is saved for reference positions and velocities for each particle are updated according to equation 1 the physical positions of the barriers of each particle are corrected to take into account the placement constraints as explained in appendix a the above sequence is repeated until convergence is reached convergence is assumed if the best score for the swarm does not improve for a user settable number of iterations defaulting to 10 5 test case barrier configuration for a lava flow on mt etna we validate our pso based algorithm to mitigate the risk of a hypothetical lava flowing on the south flank of mt etna we run a simulation without any artificial barriers and a set of possible scenarios using our pso approach all simulations have been performed using the gpu version of the magflow model cappello et al 2016 on a machine equipped with an 8 core intel xeon e5 1620v3 cpu running at 3 5 ghz and an nvidia geforce gtx titan x maxwell architecture gpu the hypothetical lava flow lasts 15 days and is erupted from a vent utm wgs84 coordinates 502737e 4172403n situated on the southeastern east flank of etna as effusion rate we considered a bell shaped curve typical of the flank eruptions on etna which starts from a null value reaches the peak of 50 m 3 s after a 1 3 of the eruption time and then gradually decreases until the end all simulations were run on a 10 m resolution dem of mt etna updated to 2007 and adopting the typical physical and rheological properties of the etnean lava bilotta et al 2012 del negro et al 2013 for the pso parameters we set the inertia weight ω 0 9 and the cognitive and social parameters c 1 c 2 2 the probable number of barriers depends on the economic possibilities and timing while the number of particles hinges upon the amount of barriers and the computational time indeed there is an indirect correlation between the number of barriers and particles because the more barriers there are the more particles should be used to reduce the number of steps needed to explore the configuration space on the other hand the number of particles also controls the computational time which is almost entirely covered by the time necessary to run the magflow simulations for each configuration therefore the number of particles had been chosen as a compromise between the computational time and the number of steps needed these considerations have led us to choose 64 particles and 9 barriers regarding the barrier size we settled for 256 m and 32 m as maximum length and height respectively these dimensions are greater than those used in historical precedents however in this first phase we decided to make this choice since our main objective was to test our method and validate its effectiveness finally we define the minimum acceptable distance of a barrier from the vent as 2 km first the algorithm ran a magflow simulation without any barrier and calculated its score fig 6 a we recall that the objective function counts the area of the towns invaded zones the score is obtained counting every dem cell that belongs to a township i e the value of the cell in the rasterization of the shape file is greater than zero and it is invaded by lava i e its lava height is greater than zero as shown in fig 6a the lava flow almost totally destroys the villages of sarro malopasso and pisano and a little bit of fleri getting an initial no barriers pso score of 1737 the algorithm proceeded following the pso s logic until the score didn t get any improvements from one step to another in a fixed number of steps i e 10 fig 6b shows the best configuration found with a score equal to zero which totally safeguards the villages of sarro malopasso pisano and fleri diverting the flows in uninhabited areas details about the location and geometric features of each barrier in the optimal configuration are reported in table 1 even if a maximum number of 9 barriers was set the algorithm finds an optimal configuration using only 6 barriers indeed barriers 6 and 8 have a null length and barrier 5 doesn t interact with the lava flow moreover the optimal configuration shown in fig 6b demonstrates that a combined use of both ditches 4 7 and 9 and barriers 1 2 and 3 can improve the results leading to a total diversion of the lava flow to uninhabited zones 6 discussion the results achieved by the combined use of the pso approach and the numerical magflow model show the viability of this approach for the planning of mitigation actions to reduce the lava flow risk with the given target function the algorithm has found an optimal configuration that provides a minimum value for the score equal to zero the optimal configuration includes both ditches and barriers which effectively spare all townships in the area ensuring that the lava flow reaches zones where there would be minimal loss it should be noted that the target function we are currently using does not have a unique global minimum point or equivalently there could be other potential barrier configurations that would still achieve a null score the one we found is just one of them and running the algorithm multiple times starting from different random initial configurations will generally find a different optimal result as shown in appendix b where we present two other different runs for the test case having different optimal barrier configurations this possible variability is partly intrinsic to the problem and partly due to the specific choice of target function indeed as we already mentioned previously the current target function is bare bones considering only the township areas as meaningful losses a more complete and realistic target function which also considers scattered edifices and or land use characteristics would have a much smaller set of configurations able to achieve a minimum value probably larger than zero an additional aspect regards the cost of the interventions emplacement of barriers digging of tranches that depends among other things on the position and size of the barriers length and height adding to the target function the cost of the barrier itself would give additional constraints to the optimality of the solution balancing the value gained by avoiding the loss from the lava flow with the cost of the intervention the configuration would be then optimal in that it would be the least expensive solution to achieve the highest reduction of losses another important point is the computational performance of the algorithm while computationally more intensive than a standard pso due to the elaborations needed to enforce the constraints the evolution of the swarm in our proposed methodology is not the most expensive part of the algorithm most of the computational time is spent instead in the evaluation of the target function since a magflow simulation is run for every particle in the swarm a system with n particles will require n s magflow simulations to complete s steps for the test case on etna a single magflow run may take anywhere from 8 min to several hours with the total run time essentially dictated by the time step which in turn depends on the lava flow flux between cells in particular we observed that large accumulations of lava such as in presence of a tall barrier or deep ditch lead to a significant decrease in the time step and thus to longer simulations while several hours to simulate 15 days of lava flow is an excellent time when this needs to be multiplied by the number of particles to determine the computational run time of a single step of the pso the computational time becomes a critical issue suggesting it is important to reduce the computational time of a single magflow simulation and or find other ways to reduce the overall run time of a single step for example by simulating multiple particles concurrently 7 conclusions and future work lava flows are a form of volcanic phenomena that can cause significant social and economic losses on the inhabited areas and land use properties near the volcanoes the growing use of these regions in the last decades led to an increasing demand for faster and effective methods for safeguarding population properties and services lava flow risk maps constitute powerful instruments to evaluate the real cost of living in volcanic areas and provide valuable tools for the long term planning of the territory del negro et al 2019 however short term mitigation measures are also needed for defense purposes to manage eruptive emergencies during volcanic crises one of the most significant short term measure to mitigate the destructive effects of lava flows is the building of artificial barriers or ditches in order to modify the path of the flows this is a non trivial issue as it requires to determine the correct position of each barrier following logistical placements limits together with its best dimensions and geometries in order to minimize the damages caused by the flow and the intervention time we presented a new optimization algorithm based on pso and the magflow numerical model for the simulation of lava flow paths to determine the position and the geometric features of a barrier system that minimize the lava flow impact on properties and services the algorithm is divided in three phases the first phase pre flight runs a magflow simulation without barriers to assess the initial conditions and compute the best score in absence of intervention the next phase particle system initialization is the initialization of the particle system from the user provided configuration number of particles and their geometrical characteristics and with random positions and velocities the best initial score for each particle and for the whole swarm is computed by running a magflow simulation for each configuration given by the swarm the last phase swarm evolution consists in the pso swarm evolution following equation 1 modified to take into account the constraints that structures should interact with the flow and be at a certain distance from the vent and from the towns the particles and swarm s best score get updated by running magflow simulations for each new configuration and the phase gets repeated until convergence is reached either the minimum possible score is achieved or the score does not improve for a user chosen number of steps the effectiveness of our methodology was validated over a destructive event possibly occurring on the south flank of etna volcano the hypothetical lava flow was supposed to totally overcome the villages of sarro malopasso and pisano and the northern part of fleri our approach for the mitigation of lava flow risk provides an optimal barrier configuration that totally safeguarded these towns diverting the flows in uninhabited zones moreover the optimal configuration is made up of less mitigation structures 6 than the maximum one set by the user 9 and combines both barriers and ditches demonstrating the effectiveness of their use although this study was carried out at mt etna the approach used is designed to be applicable to any volcanic area where the input parameters needed by magflow and data about inhabited areas are available in this early stage we have mainly dealt with the development and validation of our methodology focusing on how to bridge the gap between the abstract aspects of pso and the specific applications and in particular on the minimization of the search space and the application of the constraints about barrier placement with these issues now fixed our future work will focus on overcoming the main limitation that is the use of the original version of the particle swarm optimization algorithm and its susceptibility to be trapped in local minimal points the problem of local minimum did not present in this initial phase because the target function is discrete with an absolute minimum that is zero but it may occur once we switch to a more refined target function that also takes into consideration scattered buildings and or land use characteristics our intent is to adopt a hybridization of pso with some other methods that would help avoid local minimum trap such as simulated annealing kirkpatrick et al 1983 černỳ 1985 as done in e g sudibyo et al 2015 or the grey wolf optimizer mirjalili et al 2014 as done in e g singh and singh 2017 future work will also include a more thorough validation of the method with additional test cases more realistic and feasible barriers settings and a sensitivity analysis of the model parameters the first step will be to include a complete urban scheme for the simulations where beside cities we will also take into consideration the land use properties and roads with the potential legal disputes that could incur in diverting lava flows to sensitive areas this can be done by giving each element a different value in the score calculation in pso s logic depending on its importance this chance will probably prevent the score from ever dropping to zero except if the eruptions is in totally uninhabited areas in which case we are not interested in taking action even if the final score is non zero the pso algorithm should allow us to provide a strategy to minimize albeit not nullify the damage caused by the flows the improvement to the target function will also include the attribution of an economic cost to each barrier configuration so that we can expand the algorithm to minimize both the impact of lava flows on exposed elements and the cost of the construction of the systems coupled with an analysis of the financial losses due to the lava flow impact this would provide further insights on the optimal intervention in order to improve the execution time which is mostly entirely given by that of magflow simulation run for each particle configuration we plan to parallelize our algorithm in such a way as to simulate more particles simultaneously both by using more gpus and by improving magflow to run multiple related scenarios at once with the refined target function and improved computational time we will then have all the tools needed for a complete robustness and sensitivity analysis of the algorithm declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was developed within the framework of the laboratory of technologies for volcanology technolab at the ingv in catania italy the research was conducted in the framework of the research project shield optimization strategies for lava flow risk reduction at etna volcano bando di ricerca libera 2019 of ingv and funded by the athos research programme we would like to thank the editor in chief dan ames for handling the paper and the two anonymous referees for their suggestions that helped improving the clarity of the paper a enforcing the barrier position constraints when defining or moving a barrier certain zones have to be excluded from the space of configuration because barriers cannot logically be too close to the vent position or inside the inhabited areas moreover we want the barriers to interact with the flow since otherwise their presence is unwarranted consequently when modifying barriers position the algorithm checks if the final position violates the constraints and if this is the case it moves the barrier to a nearby location that satisfies them if the barrier is inside the vent exclusion region the algorithm moves it outwards the minimum acceptable distance m d which is set to 2 km this is achieved by first computing the barrier vent distance b v and if this is less than the m d parameter the barrier is moved to the point of intersection between the vent barrier line and the circumference centered at the vent and with radius m d we add to the vent position a vector equal to the current vent barrier vector scaled by m d b v fig 7 if the barrier happens to be very close to the vent a random direction is chosen instead to avoid numerical issues related to b v being too small fig 7 representation of the procedure by which a barrier is pushed outside the vent exclusion region in particular the red line represents the lava flows while the polygons in green represent the inhabited areas fig 7 if the vent exclusion region intersects the boundary of the area of interest the outwards push may move the barrier outside of the area of interest in this case the barrier position is corrected again by moving it to the opposite edge of the vent exclusion region fig 8 fig 8 representation of the barrier moving procedure in the case the vent exclusion region intersects the boundary of the research space dashed pointed line the barrier is first pushed outside the vent exclusion area a and then if outside the domain it is moved to the opposite edge of the vent exclusion b in particular the red line represents the lava flows while the polygons in green represent the inhabited areas fig 8 the next step is to verify that the barrier does not intersect an inhabited area for each township within our area of interest we compute the smallest circle that envelops the township s defining polygon and centered on the centroid of the same polygon this defines the township s exclusion region if a barrier falls within a township s exclusion region it is pushed outwards towards the vent mathematically this can be described as follows let c be the township s centroid and r the radius of the exclusion region then the boundary of the exclusion region is defined by the circle s equation 2 x c 2 r 2 if v is the vent position and b the barrier position then the vent barrier segment is described by the equation 3 x b t v b t 0 1 since we are under the assumption that the barrier is inside the circle surrounding the inhabited area its distance from the centroid is lower than the radius of the circle i e the following inequality is verified 4 b c 2 r 2 to determine the intersection points between the circle and the barrier vent straight line we replace the generic point p t b t v b of the straight line in the circle s equation 2 obtaining the following condition b c t v b 2 r 2 or equivalently 5 v b 2 t 2 2 b c v b t b c 2 r 2 0 where represents the dot product equation 5 is a quadratic equation in the unknown t which due to the geometry of our points admits two distinct real solutions we want the solution in the range 0 1 since this would represent the intersection point between the segment equation 3 and the circle equation 2 to find the solution we calculate the discriminant δ 4 b c v b 2 v b 2 b c 2 r 2 and we know that because of equation 4 it is non negative therefore the two solutions of equation 5 are t 1 b c v b δ 4 v b 2 t 2 b c v b δ 4 v b 2 the new barrier position is then obtained substituting the solution of equation 5 withing the range 0 1 in equation 3 finally since we want to ensure that the barrier interacts with the flow we move the barrier position to the nearest point of the lava flow fig 9 fig 9 representation of the procedure by which a barrier is first pushed outside the inhabited area on the left and then snapped to the lava flow on the right note that the snap to flow is done also if the barrier wasn t originally within a township exclusion region fig 9 since the flow associated to the new configuration is not know yet the flow barrier distance is computed according to the flow emplacement obtained for the pso particle at the last step in the case of the first step we consider instead the flow emplacement without any barriers specifically the results of the magflow simulation for each particle are imported cutting out the flow areas that intersect the vent and townships exclusion regions then for each barrier in the particle we compute the position update according to the pso algorithm apply the barrier shifting due to the vent and township exclusion regions if necessary and finally compute the distance between the shifted barrier position to each dem cell occupied by lava according to the particle s previous configuration and move the barrier to the coordinates of the closest cell according to the quantization of the dimensions discussed in section 4 2 each of the corrections applied during the constraint procedure is done by rounding up the position displacement vector to the next multiple of the position quantization step b results of other runs for the test case we present other two runs named as run a and run b for the test case presented in section 5 to validate our pso based algorithm we aim to show the non unique global minimum point of the target function currently used which is the potential loss associated with an eruptive scenario where the loss in this initial stage of development is defined by simply estimating the amount of inhabited areas reached by the lava flow as we already pointed out the fact that there could be other potential barrier configurations that still achieve a null score i e no inhabited area is inundated is mainly due to our early stage definition of the target function in the future we plan to extend it to be more complete and realistic for the assessment of lava flow risk considering also scattered edifices and or land use characteristics this would probably lead to having a minimum value larger than zero reached by a smaller set of configurations figs 10 and 11 show the best configuration found by run a and run b of our algorithm with almost the same specifications of the test case starting from different random initial configurations the only different value is the suitable maximum number of barriers that we decide to decrease by one unit thus it is 8 since the optimal configuration found in the test case used only 6 barriers compered with the 9 maximum fixed as expected the results obtained show different optimal result with respect to section 5 presenting other potential barrier configurations that still achieve a null score totally safeguarding the threatened villages of sarro malopasso pisano and fleri diverting the flows in uninhabited areas tables 2 and 3 report details about the location and geometric features of each barrier in the optimal configuration of the two runs fig 10 best configuration for run a obtained using our pso approach which brings the score to zero barriers are numbered as reported in table 2 legends indicate the lava flow thickness in meters fig 10 we can see how in run a even if a maximum number of 8 barriers was set the algorithm finds an optimal configuration using again only 6 barriers indeed barriers 6 and 7 don t interact with the lava flow fig 10 moreover also the optimal configuration of run a includes both ditches 1 3 and 4 and barriers 2 5 and 8 table 2 best particle configuration of run a for each barrier the location and geometric characteristics langle and rangle in degrees length and height in meters are reported table 2 easting northing langle rangle length height barrier 1 505150 4170690 87 4 256 32 barrier 2 507980 4169220 90 90 168 11 barrier 3 505090 4170280 90 90 256 32 barrier 4 507050 4169480 90 90 163 32 barrier 5 507670 4169060 60 90 256 23 barrier 6 509150 4168270 20 90 120 7 barrier 7 509190 4168250 90 90 69 11 barrier 8 508080 4169160 51 90 181 32 fig 11 best configuration of run b obtained using our pso approach which brings the score to zero barriers are numbered as reported in table 3 barrier 8 is not displayed because its length is zero legends indicate the lava flow thickness in meters fig 11 in run b the algorithm finds an optimal configuration using only 7 barriers even if a maximum number of 8 barriers was set having barrier 8 a null length moreover the optimal configuration shown in fig 11 demonstrates how once again the combined use of both ditches 2 and 7 and barriers 1 3 4 5 and 6 can improve the results leading to a total diversion of the lava flow to uninhabited zones table 3 best particle configuration of run b for each barrier the location and geometric characteristics langle and rangle in degrees length and height in meters are reported table 3 easting northing langle rangle length height barrier 1 506050 4170090 89 88 41 23 barrier 2 505340 4170710 41 35 115 22 barrier 3 505480 4170510 38 41 219 6 barrier 4 507980 4169220 29 90 259 23 barrier 5 504250 4171070 9 88 165 32 barrier 6 506150 4169470 90 82 256 32 barrier 7 507380 4169430 63 34 226 32 barrier 8 510210 4168770 90 44 0 14 
25860,lava flows are recurring and widespread hazards that affect areas around active volcanoes having the potential to cause significant social and economic loss the ongoing demographic congestion around volcanoes increases the potential risk and leads to a growing demand for faster and more accurate systems to safeguard the population the main mitigation action for slowing down and possibly diverting lava flows is the building of artificial barriers that can limit their destructive effects and reduce losses here we present a particle swarm optimization algorithm for the configuration of artificial barriers in terms of location and geometric features the goal is to minimize the lava flow impact based on the spatial distribution of exposed elements using the physics based magflow model to run the lava flow scenarios for each barrier configuration our algorithm has been tested on etna italy showing how it can effectively safeguard the threatened areas diverting lava away from them keywords particle swarm optimization artificial barrier lava flow risk numerical simulations magflow 1 introduction volcanic risk can be described as the loss of lives and properties due to hazardous phenomena in a certain volcanic area over specific periods where the consequences are determined by the exposed elements and their level of vulnerability to hazards fournier d albe 1979 from a mathematical point of view risk can be computed as the product of hazard vulnerability and exposure where hazard is a measure of the probability of occurrence of the destructive event vulnerability is a measure of how strongly the areas are affected by the event ranging continuously from 0 for no loss to 1 for total loss and exposure is a quantification of the value of the affected areas in the case of lava flow risk vulnerability is generally assumed to take only the extreme value of 1 total loss since lava flows commonly cause complete devastation through properties or towns thierry et al 2008 favalli et al 2009 pedrazzi et al 2014 del negro et al 2019 consequently the risk from lava flow inundation can be estimated simply as the product of hazard the probability of a lava flow reaching a particular area and exposure so that risk mitigation can be achieved by affecting the hazard i e altering the probability of invasion which requires a way to influence the progress of a lava flow the idea of slowing and diverting lava flows with the use of artificial structures for the mitigation of its risk thus arises from simple physical considerations the emplacement of lava flows is essentially controlled by the effusion rate the rheology of the lava and the topography since the effusion rate and the rheology of the fluid are outside of our control the main viable tool to influence a lava flow remains our possibility to impact the topography in order to divert the flow the topography influences the emplacement both directly by offering preferential directions to the flow and indirectly by influencing the capability for the flow to develop lava tubes known to be particularly hazardous due to their thermal insulation that allows the tunneled lava to preserve a high fluidity for longer times and thus to cover higher distances before cooling off and slowing down walker et al 1973 calvari and pinkerton 1999 when lava is diverted out of its natural channel the flow front is deprived of back thrust and is not able to advance any longer generating a new front from the diversion point if only a partial lava diversion is achieved the flow front stops due to the reduction of the back thrust and none of the two flows resulting from the partial diversion can travel as far as the initial one in both cases it has been experienced that the use of structures such as artificial barriers or ditches to divert lava flows is an effective action for mitigating its risk one of the main volcanoes giving the foremost examples of lava flows with significant impact on densely populated areas is mt etna sicily italy fig 1 a due to the high frequency of effusive eruptions cappello et al 2013 2019 del negro et al 2013 here the building of artificial barriers has been proven to be an effective action for diverting and slowing down the lava flows as experienced during recent eruptions e g 1983 1991 1993 2001 2002 when earthen barriers were built to control lava flow expansion with varying degrees of success colombrita 1984 barberi et al 1993 2003 barberi and carapezza 2004 providing effective and objective solutions is very challenging since the best positioning of these artificial constructions depends on many factors such as the viscosity of the lava the output rates the topography of the affected area the time of intervention and the economic costs for construction for this reason the design of the mitigation measures is generally based on the past experience and knowledge of experts leaving the problem of defining a standard methodology for the construction and location of protection structures still open the task of finding optimal intervention strategies for volcanic risk mitigation can be seen as an optimization process whose goal is to find the best configuration of barriers and ditches minimizing both the damage caused by the diverted flow and the cost of the intervention itself as with all optimization processes the solution can be found by searching through the abstract space of potential solutions for small spaces classical exhaustive methods are usually sufficient but most real world problems such as the design of the mitigation measures for volcanic risk have high complexity non linear constraints interdependencies among variables and huge solution spaces this needs the use of a method capable of solving complex optimization problems in real time metaheuristic algorithms are optimization methods that produce a reasonably good solution in a moderate amount of time and can be used to efficiently solve problems with large number of variables and non linear objective functions thanks to their properties and wide fields of application in the past two decades many metaheuristic algorithms have been proposed some examples are particle swarm optimization pso kennedy and eberhart 1995b genetic algorithms ga holland 1992 simulated annealing sa kirkpatrick et al 1983 tabu search ts glover and marti 2006 ant colony optimization aco dorigo 1992 and differential evolution de price et al 2005 in the literature see e g gogna and tayal 2013 there are different ways to classify metaheuristic algorithms based on characteristics used to differentiate amongst them they include nature inspired e g pso ga versus non nature inspired e g ts population based e g pso ga versus single point search e g ts sa and local search iterative e g pso sa versus greedy e g aco dynamic e g guided local search versus static e g pso objective function among them population based metaheuristics p metaheuristics most of which are inspired by nature represent a reasonable solution when the hazardous event to simulate evolves based on local interactions of their constituent parts they apply the generation and replacement procedure to a family of solutions spread over the search space until the stopping criterion is met for example filippone et al 2013 presents an application of parallel genetic algorithms for optimizing earth barriers construction to divert a lava flow on mt etna and protect the touristic facility of rifugio sapienza our goal is to combine one of the p metaheuristics algorithms with the magflow lava flow model del negro et al 2008 cappello et al 2016 to determine the optimal position and geometric features of more than one structure able to divert the flow in case of an eruption to mitigate lava flows hazard among the population based metaheuristics we decided to use the pso algorithm since its implementation perfectly fits the definition of a barrier and its geometric parts the possibility to use more than one structure forming a particle and it presents a good coupling with magflow too our choice relies also on computational time in which the time necessary to run the magflow simulations plays a huge part since the pso algorithm has proven to converge rapidly in comparison with other models see e g el ghandour and elbeltagi 2018 in which it is compared with ga aco ma and the modified shuffled frog leaping algorithm sfla our pso algorithm minimizes the impact of lava flows based on the spatial distribution of exposed elements and using the numerical magflow model to run the eruptive scenarios associated with each barrier configuration here it has been applied and validated using a possible lava flow erupted from a vent located on the southeastern flank of mt etna the geological map of the mount etna branca et al 2011 shows that a number of eruptions were produced from vents located in this same area more recent eruptions occurred in 1634 and 1792 were characterized by long duration 540 and 340 days respectively and high volumes of lava about 150 m m 3 and 62 m m 3 respectively whose flows arrived to destroy some villages behncke et al 2005 therefore the opening of future eruptive vents in the southeastern flank of the volcano presents a very high potential risk for the destructive impact that the new flows could have on the near urbanized areas del negro et al 2013 2019 we found barrier configurations that have the potential to completely protect the villages threatened by lava showing how this methodology is effectively able to divert the flow safeguarding the inhabited areas the paper is organized as follows in section 2 we present a brief review of the mitigation actions against historical eruptions at mt etna in section 3 we illustrate the main features of the pso algorithm while in section 4 we describe the main characteristics of the implemented algorithm the definition of the search space and the target function the constraints for the barrier placement the way the particle system is initialized and how the swarm evolves in section 5 we provide a numerical example to illustrate the effectiveness of the proposed mitigation strategy in section 6 we discuss the achieved results section 7 concludes this paper with a summary of our results and some possible future developments finally in appendix a we describe the mathematical law used to enforce barrier position constraints while in appendix b we present the results of other two runs for the numerical example provided in section 5 2 mitigation actions against historical effusive eruptions at mt etna the first attempt to control a lava flow dates back to 1669 fig 1b during the most devastating historic eruption of mt etna barberi et al 1993 when the lava was proceeding into the city of catania a group of frightened citizens tried to protect themselves opening a breach on a levee of the flow to change its advance direction unfortunately this initially promising attempt was stopped by hundreds of furious residents of the nearby misterbianco village who feared that their houses could be destroyed by the diverted lava flow after 1669 mitigation measures trying to divert lava flows at mt etna were blocked mainly due to legal disputes related to the fact that the flow diverted to defend one city could threaten another one it was only in 1983 that the italian government authorized a direct intervention constructing an artificial opening of a breach on the levee of a channelized flow to divert the lava from its natural bed into an artificial channel excavated parallel to the natural one this area was a public property so no disagreement with private owners arose the break opened by an explosion on the levee was relatively small and diverted into the artificial channel only 20 30 of the lava while the remaining 70 80 continued to flow into the natural one as an indirect consequence of the explosion a very substantial deviation of the flow out of its natural channel was accomplished and despite only a short partial diversion was obtained the desired result was achieved since most of the lava out flowed due to the obstruction of a nearby lava tube this intervention was only a partial success but demonstrated that man was effectively able to control the expansion of a lava flow after the 1983 eruption mt etna had an intense eruptive activity a particular significant case is the 1991 1993 eruption fig 1b because of the several actions carried out to protect the village of zafferana etnea from being invaded by the lava the strategy adopted in this situation was an improvement of the methods used during the 1983 lava deviation obtaining a total diversion of the flow the measures to defend zafferana included the building of four lava containment earth barriers and several attempts at plugging the lava tube downhill the earth barriers oriented orthogonally to the direction of the flow slowed the front propagation down for a few weeks although they were not able to fully stop it since lava was overflowing on the earth barriers the civil protection authorities had to carry out a drastic lava flow deviation near the vent finally the flow was diverted into an artificial channel by blasting the wall separating it from the natural channel and obstructing downstream the natural tunnel barberi et al 1993 overall during this event 13 earth barriers were built providing a positive experience at mt etna to protect towns from lava flows even if some legal questions had not been fully solved yet however from the detailed reconstruction of lava tube formation and flow evolution of this eruption and by comparison with other historic eruptions of mt etna calvari and pinkerton 1998 showed that without the successful diversion of may 1992 the lava would have overcome zafferana etnea and continued beyond an intervention similar to the one adopted in 1992 was planned to deviate the flow during the 2001 eruption that threatened the tourist facilities in the rifugio sapienza area fig 1b it included the construction of both lateral earth barriers to push the flow toward existing large holes and orthogonal bars to increase the holding capacity of the depressions moreover a possible evacuation of the nearby villages was also planned because of the decreasing of the effusion rate the plan was only partially executed and just 13 barriers were built up to protect the area initially delaying the advance of the flows and then diverting it toward south east away from the facilities barberi and carapezza 2004 finally structures to divert lava flows were planned during the 2002 2003 eruption of mt etna which produced two lava flows fig 1b partially covering the tourist facilities of piano provenzana on the northern side of the volcano and threatening rifugio sapienza on the southern flank andronico et al 2005 in this case a great effort was devoted to the construction of six earth barriers in accordance with the local authorities including the etna volcanological park these actions contributed to mitigate the impact of the lava flows advance considering that on november 24 the flows directed toward the south were deviated away from the touristic facilities of rifugio sapienza unfortunately on december 16 the lava overflowed from the barrier destroying two buildings and cutting the sp92 road before stopping soon after scifoni et al 2010 3 particle swarm optimization the building of artificial barriers at etna volcano has been an effective action for diverting and slowing down different lava flows however the decision of the construction site was mainly based on expert advice leaving the problem of defining a standard methodology for the location of protection structures to mitigate volcanic risk still open for this reason we developed an automatic algorithm based on particle swarm optimization and the magflow model which aims at finding the best configuration of artificial barriers in terms of location and geometric features before describing the main characteristics of our algorithm we briefly present the particle swarm optimization paradigm particle swarm optimization pso kennedy and eberhart 1995a b is a stochastic optimization algorithm based on the simulation of the behavior of socially organized populations in nature such as bird flocks fish schools and animal herds one of the main advantages of pso is that it can be applied to non differentiable non linear problems with a very large search space e g high number of dimensions with good efficiency the idea is that to find the minimum of a real valued function f over a domain d r n we can sample the function by computing f x i for a given set of points x i in d and then move the points x i with some velocity v i towards the positions that achieved the lowest values this results in a swarm s x 1 x 2 x n of n particles that iteratively move within the search space d converging towards the point where f achieves its minimum value putting it in a mathematical framework let us denote by x i t d the position of the i th particle of s at iteration t and by v i t r n its velocity where n is the number of components of each particle each particle has a memory of the best position b i t it has occupied so far i e b i t arg min τ f x i τ with 0 τ t and the swarm as a whole has knowledge of the best position b g t occupied so far by any particle i e b g t arg min i f b i t with 1 i n the initial values for the position x i 0 and velocity v i 0 are chosen randomly and initially b i 0 x i 0 at every iteration the velocity of each particle is updated considering the previous velocity a cognitive factor based on the particle s best position and a social factor based on the swarm s best position formally given fixed weights ω c 1 c 2 and random values r 1 t r 2 t 0 1 the velocity and position update formulas can be written following the improvement proposed by shi and eberhart 1998 1 v i t 1 ω v i t c 1 r 1 t b i t x i t c 2 r 2 t b g t x i t x i t 1 x i t v i t for i 1 2 n the coefficient ω is know as the inertia weight and indicates a tendency of each particle to continue in the direction it was already moving c 1 is the cognitive parameter and it controls the weight with which particles are attracted to their best known position while c 2 is the social parameter and it controls the weight with which particles are attracted to the best known position of the entire swarm 4 mitigation of lava flow risk using pso and magflow to apply the pso algorithm for the mitigation of lava flow risk we need to map the physical problem onto mathematical concepts in such a way that the evaluation of the target function f relies on the lava flow emplacement simulations done with the numerical magflow model and the motion of the particles respects the constraints due to the considered physical problems 4 1 definition of the search space and target function the search space is defined by the possible locations and geometric characteristics for the barriers and the target function to be minimized is the impact of the eruptive scenario we assume that barriers have a winged configuration so that a single barrier can be defined by six values the easting and northing of the central point the length of the two arms the height of the barrier with a negative height indicating a ditch and the two angles langle and rangle that the arms form with respect to the north south and east west direction fig 2 we assume that in the case of intervention multiple barriers may need to be built so a single particle in our swarm is a collection of a fixed number of barriers if there are b barriers the dimensionality of our problem is then 6 b since each particle is defined by the 6 parameters of each of the barriers that compose it the fixed number of barriers represents the maximum number that can be used and its choice depends on the feasibility in terms of the economic possibilities and timing note that the position of a particle is an abstract concept in this case since it includes the physical position but also the angles length and height of each of its barriers our target function is the potential loss associated with a eruptive scenario as a simplification in this initial stage of development we define the loss by simply estimating the amount of inhabited areas reached by the lava flow the computation of the target function requires an assessment of the impact of the barriers on the lava flow emplacement that we compute using magflow a cellular automaton model for lava flow simulations developed at ingv sezione di catania cappello et al 2016 bilotta et al 2012 2019 in magflow the area of interest for the simulation is decomposed into a regular grid of square cells with associated information including the terrain elevation the amount of solid and liquid lava present in the cell and its temperature the evolution function of the automaton is based on a steady state solution of the navier stokes equation for bingham fluids coupled with a simplified physical model for the thermal evolution of the flowing lava vicari et al 2007 magflow has proven to be successful both to reproduce past events with well known characteristics and to predict the paths of lava flows in real time for example during the mt etna eruptions of 2004 del negro et al 2008 2006 vicari et al 2009 hérault et al 2009 2008 bonaccorso et al 2011 ganci et al 2012 and 2011 vicari et al 2011 inputs to magflow are the digital topography of the area of interest taken from a dem digitial elevation model the location of the vent s and the mass flux rate as a function of time optionally it is also possible to specify the location and geometry of an arbitrary number of barriers and ditches that are realized by adding or removing a corresponding amount of height from the dem in the affected areas the output of the magflow model is a sequence of snapshots of the spatio temporal evolution of the emplacement taken at fixed intervals chosen by the user in our case we only look at the final emplacement to integrate magflow with our pso algorithm we also include for each cell the information about whether the cell is inside an inhabited center or not for each particle in the swarm i e for each set of barriers we run a magflow simulation and the target function for that particle i e score of the barrier configuration is computed by counting the number of township cells with lava the objective of the pso is then to minimize this value the main steps of the algorithm can be summarized as follows pre flight a run a magflow simulation without any barriers b calculation of the score set as best score particle system initialization a initialization of a swarm b run magflow simulations with the barrier configuration of each particle in the swarm c calculation of the score d comparison with the best score and replacement if lower swarm evolution a definition of a new configuration b run magflow simulations with the new configuration c calculation of the score d comparison with the best score and replacement if lower the sequence of the swarm evolution phase is repeated until convergence is reached 4 2 quantization of the search space to help reduce the search space all of the dimensions that control the barrier configuration and thus the particle position and velocity in the abstract space are quantized in the sense that they do not vary continuously but only in multiples of a discrete step the size of this step is a user controlled parameter and can be varied for different dimensions the default values used in the examples presented here are 10 m for the position 1 m for the length 5 m for the height and 10 sexagesimal for the angles 4 3 barrier placement constraints in a straightforward application of the pso the search space with our choice of particles would have not only a high dimensionality but also a very large span especially for the choice of the position of the barrier vertex indeed a naive choice for the boundaries of the easting and northing coordinates for the barriers could be the bounding box of the original lava flow emplacement even though in most of these locations barriers would not interact with the lava flow at all see e g fig 3 moreover most of these positions would be equivalent all have the same score leading to a very slow convergence a possible solution is to add a constraint on the barrier location to ensure it actually interacts with the lava flow for example by requiring that the barrier vertex is confined to be inside the lava flow however since the barriers themselves alter the flow emplacement this is impossible to achieve consistently with multiple barriers due to the way they affect each others domain to work around the issue we let the barrier location change freely but we keep track of the lava flow point closest to the barrier placement this information is then used to correct the barrier position if moved too far away from the flow further constraints are posed on barrier placement to avoid unrealistic situations in particular we require that barriers do not get too close to the vent s and that they should not be located inside a town appendix a details how these constraints are enforced 4 4 particle system initialization the first step of our pso algorithm is the initialization of the particles with random positions and velocities due to the constraints on the barrier positions we first run magflow assuming no barriers are present this gives us an upper bound to the best score for the algorithm and a reference emplacement of the lava flow to guide the initial placement of the barriers the particles for the swarm are then generated the procedure we will describe momentarily to generate the initial positions and velocities of the particles was designed to solve the mapping issue between corresponding barriers of the different particles to illustrate the issue assume for simplicity that each particle in the swarm is composed of two barriers b 2 then particle i will have coordinates b i 1 b i 2 e i 1 n i 1 α i 1 β i 1 l i 1 h i 1 e i 2 n i 2 α i 2 β i 2 l i 2 h i 2 easting northing right and left angle length and height for the first barrier and for the second barrier in order now consider a second particle j with coordinates b j 1 b j 2 e j 1 n j 1 α j 1 β j 1 l j 1 h j 1 e j 2 n j 2 α j 2 β j 2 l j 2 h j 2 and assume that the barriers of particle i and j are as in fig 4 the first barrier of the first particle b i 1 is closer to the second barrier of the second particle b j 2 while the second barrier of the first particle b i 2 is closer to the first barrier of the second particle b j 1 in extreme situations the two particles could represent identical configurations but with swapped barrier order while the pso algorithm assumes a component by component mapping between particles in these cases some kind of transformation would be required to improve convergence to avoid these situations and ensure a correct component mapping we generate particles iteratively as perturbations of an initial configuration specifically we first generate the first particle of the swarm by generating random position velocities angles length and height for each of the particle barriers the only constraint in this case is that the positions of the barriers are not too close to the vent and not too close to the other barriers being generated for each of the other particles we only generate the first barrier randomly the position of all other barriers in the particle is obtained by shifting the corresponding barrier of the first particle by the distance vector between the first barrier of the first particle and the first barrier of the particle being generated fig 5 after all particles are generated their positions are adjusted to ensure that the other constraints on the position are satisfied moving them closer to the flow and outside of the towns as necessary the details about the application of these constraints are illustrated in appendix a 4 5 swarm evolution the evolutionary step of the swarm can be summarized in the following sequence we compute the target function for each of the particles in the swarm if the current score is lower than the previous best score for that particle the best score and corresponding configuration of the particle is saved for reference if the lowest score among the particles is lower than the previous record the best score and corresponding configuration of the swarm is saved for reference positions and velocities for each particle are updated according to equation 1 the physical positions of the barriers of each particle are corrected to take into account the placement constraints as explained in appendix a the above sequence is repeated until convergence is reached convergence is assumed if the best score for the swarm does not improve for a user settable number of iterations defaulting to 10 5 test case barrier configuration for a lava flow on mt etna we validate our pso based algorithm to mitigate the risk of a hypothetical lava flowing on the south flank of mt etna we run a simulation without any artificial barriers and a set of possible scenarios using our pso approach all simulations have been performed using the gpu version of the magflow model cappello et al 2016 on a machine equipped with an 8 core intel xeon e5 1620v3 cpu running at 3 5 ghz and an nvidia geforce gtx titan x maxwell architecture gpu the hypothetical lava flow lasts 15 days and is erupted from a vent utm wgs84 coordinates 502737e 4172403n situated on the southeastern east flank of etna as effusion rate we considered a bell shaped curve typical of the flank eruptions on etna which starts from a null value reaches the peak of 50 m 3 s after a 1 3 of the eruption time and then gradually decreases until the end all simulations were run on a 10 m resolution dem of mt etna updated to 2007 and adopting the typical physical and rheological properties of the etnean lava bilotta et al 2012 del negro et al 2013 for the pso parameters we set the inertia weight ω 0 9 and the cognitive and social parameters c 1 c 2 2 the probable number of barriers depends on the economic possibilities and timing while the number of particles hinges upon the amount of barriers and the computational time indeed there is an indirect correlation between the number of barriers and particles because the more barriers there are the more particles should be used to reduce the number of steps needed to explore the configuration space on the other hand the number of particles also controls the computational time which is almost entirely covered by the time necessary to run the magflow simulations for each configuration therefore the number of particles had been chosen as a compromise between the computational time and the number of steps needed these considerations have led us to choose 64 particles and 9 barriers regarding the barrier size we settled for 256 m and 32 m as maximum length and height respectively these dimensions are greater than those used in historical precedents however in this first phase we decided to make this choice since our main objective was to test our method and validate its effectiveness finally we define the minimum acceptable distance of a barrier from the vent as 2 km first the algorithm ran a magflow simulation without any barrier and calculated its score fig 6 a we recall that the objective function counts the area of the towns invaded zones the score is obtained counting every dem cell that belongs to a township i e the value of the cell in the rasterization of the shape file is greater than zero and it is invaded by lava i e its lava height is greater than zero as shown in fig 6a the lava flow almost totally destroys the villages of sarro malopasso and pisano and a little bit of fleri getting an initial no barriers pso score of 1737 the algorithm proceeded following the pso s logic until the score didn t get any improvements from one step to another in a fixed number of steps i e 10 fig 6b shows the best configuration found with a score equal to zero which totally safeguards the villages of sarro malopasso pisano and fleri diverting the flows in uninhabited areas details about the location and geometric features of each barrier in the optimal configuration are reported in table 1 even if a maximum number of 9 barriers was set the algorithm finds an optimal configuration using only 6 barriers indeed barriers 6 and 8 have a null length and barrier 5 doesn t interact with the lava flow moreover the optimal configuration shown in fig 6b demonstrates that a combined use of both ditches 4 7 and 9 and barriers 1 2 and 3 can improve the results leading to a total diversion of the lava flow to uninhabited zones 6 discussion the results achieved by the combined use of the pso approach and the numerical magflow model show the viability of this approach for the planning of mitigation actions to reduce the lava flow risk with the given target function the algorithm has found an optimal configuration that provides a minimum value for the score equal to zero the optimal configuration includes both ditches and barriers which effectively spare all townships in the area ensuring that the lava flow reaches zones where there would be minimal loss it should be noted that the target function we are currently using does not have a unique global minimum point or equivalently there could be other potential barrier configurations that would still achieve a null score the one we found is just one of them and running the algorithm multiple times starting from different random initial configurations will generally find a different optimal result as shown in appendix b where we present two other different runs for the test case having different optimal barrier configurations this possible variability is partly intrinsic to the problem and partly due to the specific choice of target function indeed as we already mentioned previously the current target function is bare bones considering only the township areas as meaningful losses a more complete and realistic target function which also considers scattered edifices and or land use characteristics would have a much smaller set of configurations able to achieve a minimum value probably larger than zero an additional aspect regards the cost of the interventions emplacement of barriers digging of tranches that depends among other things on the position and size of the barriers length and height adding to the target function the cost of the barrier itself would give additional constraints to the optimality of the solution balancing the value gained by avoiding the loss from the lava flow with the cost of the intervention the configuration would be then optimal in that it would be the least expensive solution to achieve the highest reduction of losses another important point is the computational performance of the algorithm while computationally more intensive than a standard pso due to the elaborations needed to enforce the constraints the evolution of the swarm in our proposed methodology is not the most expensive part of the algorithm most of the computational time is spent instead in the evaluation of the target function since a magflow simulation is run for every particle in the swarm a system with n particles will require n s magflow simulations to complete s steps for the test case on etna a single magflow run may take anywhere from 8 min to several hours with the total run time essentially dictated by the time step which in turn depends on the lava flow flux between cells in particular we observed that large accumulations of lava such as in presence of a tall barrier or deep ditch lead to a significant decrease in the time step and thus to longer simulations while several hours to simulate 15 days of lava flow is an excellent time when this needs to be multiplied by the number of particles to determine the computational run time of a single step of the pso the computational time becomes a critical issue suggesting it is important to reduce the computational time of a single magflow simulation and or find other ways to reduce the overall run time of a single step for example by simulating multiple particles concurrently 7 conclusions and future work lava flows are a form of volcanic phenomena that can cause significant social and economic losses on the inhabited areas and land use properties near the volcanoes the growing use of these regions in the last decades led to an increasing demand for faster and effective methods for safeguarding population properties and services lava flow risk maps constitute powerful instruments to evaluate the real cost of living in volcanic areas and provide valuable tools for the long term planning of the territory del negro et al 2019 however short term mitigation measures are also needed for defense purposes to manage eruptive emergencies during volcanic crises one of the most significant short term measure to mitigate the destructive effects of lava flows is the building of artificial barriers or ditches in order to modify the path of the flows this is a non trivial issue as it requires to determine the correct position of each barrier following logistical placements limits together with its best dimensions and geometries in order to minimize the damages caused by the flow and the intervention time we presented a new optimization algorithm based on pso and the magflow numerical model for the simulation of lava flow paths to determine the position and the geometric features of a barrier system that minimize the lava flow impact on properties and services the algorithm is divided in three phases the first phase pre flight runs a magflow simulation without barriers to assess the initial conditions and compute the best score in absence of intervention the next phase particle system initialization is the initialization of the particle system from the user provided configuration number of particles and their geometrical characteristics and with random positions and velocities the best initial score for each particle and for the whole swarm is computed by running a magflow simulation for each configuration given by the swarm the last phase swarm evolution consists in the pso swarm evolution following equation 1 modified to take into account the constraints that structures should interact with the flow and be at a certain distance from the vent and from the towns the particles and swarm s best score get updated by running magflow simulations for each new configuration and the phase gets repeated until convergence is reached either the minimum possible score is achieved or the score does not improve for a user chosen number of steps the effectiveness of our methodology was validated over a destructive event possibly occurring on the south flank of etna volcano the hypothetical lava flow was supposed to totally overcome the villages of sarro malopasso and pisano and the northern part of fleri our approach for the mitigation of lava flow risk provides an optimal barrier configuration that totally safeguarded these towns diverting the flows in uninhabited zones moreover the optimal configuration is made up of less mitigation structures 6 than the maximum one set by the user 9 and combines both barriers and ditches demonstrating the effectiveness of their use although this study was carried out at mt etna the approach used is designed to be applicable to any volcanic area where the input parameters needed by magflow and data about inhabited areas are available in this early stage we have mainly dealt with the development and validation of our methodology focusing on how to bridge the gap between the abstract aspects of pso and the specific applications and in particular on the minimization of the search space and the application of the constraints about barrier placement with these issues now fixed our future work will focus on overcoming the main limitation that is the use of the original version of the particle swarm optimization algorithm and its susceptibility to be trapped in local minimal points the problem of local minimum did not present in this initial phase because the target function is discrete with an absolute minimum that is zero but it may occur once we switch to a more refined target function that also takes into consideration scattered buildings and or land use characteristics our intent is to adopt a hybridization of pso with some other methods that would help avoid local minimum trap such as simulated annealing kirkpatrick et al 1983 černỳ 1985 as done in e g sudibyo et al 2015 or the grey wolf optimizer mirjalili et al 2014 as done in e g singh and singh 2017 future work will also include a more thorough validation of the method with additional test cases more realistic and feasible barriers settings and a sensitivity analysis of the model parameters the first step will be to include a complete urban scheme for the simulations where beside cities we will also take into consideration the land use properties and roads with the potential legal disputes that could incur in diverting lava flows to sensitive areas this can be done by giving each element a different value in the score calculation in pso s logic depending on its importance this chance will probably prevent the score from ever dropping to zero except if the eruptions is in totally uninhabited areas in which case we are not interested in taking action even if the final score is non zero the pso algorithm should allow us to provide a strategy to minimize albeit not nullify the damage caused by the flows the improvement to the target function will also include the attribution of an economic cost to each barrier configuration so that we can expand the algorithm to minimize both the impact of lava flows on exposed elements and the cost of the construction of the systems coupled with an analysis of the financial losses due to the lava flow impact this would provide further insights on the optimal intervention in order to improve the execution time which is mostly entirely given by that of magflow simulation run for each particle configuration we plan to parallelize our algorithm in such a way as to simulate more particles simultaneously both by using more gpus and by improving magflow to run multiple related scenarios at once with the refined target function and improved computational time we will then have all the tools needed for a complete robustness and sensitivity analysis of the algorithm declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was developed within the framework of the laboratory of technologies for volcanology technolab at the ingv in catania italy the research was conducted in the framework of the research project shield optimization strategies for lava flow risk reduction at etna volcano bando di ricerca libera 2019 of ingv and funded by the athos research programme we would like to thank the editor in chief dan ames for handling the paper and the two anonymous referees for their suggestions that helped improving the clarity of the paper a enforcing the barrier position constraints when defining or moving a barrier certain zones have to be excluded from the space of configuration because barriers cannot logically be too close to the vent position or inside the inhabited areas moreover we want the barriers to interact with the flow since otherwise their presence is unwarranted consequently when modifying barriers position the algorithm checks if the final position violates the constraints and if this is the case it moves the barrier to a nearby location that satisfies them if the barrier is inside the vent exclusion region the algorithm moves it outwards the minimum acceptable distance m d which is set to 2 km this is achieved by first computing the barrier vent distance b v and if this is less than the m d parameter the barrier is moved to the point of intersection between the vent barrier line and the circumference centered at the vent and with radius m d we add to the vent position a vector equal to the current vent barrier vector scaled by m d b v fig 7 if the barrier happens to be very close to the vent a random direction is chosen instead to avoid numerical issues related to b v being too small fig 7 representation of the procedure by which a barrier is pushed outside the vent exclusion region in particular the red line represents the lava flows while the polygons in green represent the inhabited areas fig 7 if the vent exclusion region intersects the boundary of the area of interest the outwards push may move the barrier outside of the area of interest in this case the barrier position is corrected again by moving it to the opposite edge of the vent exclusion region fig 8 fig 8 representation of the barrier moving procedure in the case the vent exclusion region intersects the boundary of the research space dashed pointed line the barrier is first pushed outside the vent exclusion area a and then if outside the domain it is moved to the opposite edge of the vent exclusion b in particular the red line represents the lava flows while the polygons in green represent the inhabited areas fig 8 the next step is to verify that the barrier does not intersect an inhabited area for each township within our area of interest we compute the smallest circle that envelops the township s defining polygon and centered on the centroid of the same polygon this defines the township s exclusion region if a barrier falls within a township s exclusion region it is pushed outwards towards the vent mathematically this can be described as follows let c be the township s centroid and r the radius of the exclusion region then the boundary of the exclusion region is defined by the circle s equation 2 x c 2 r 2 if v is the vent position and b the barrier position then the vent barrier segment is described by the equation 3 x b t v b t 0 1 since we are under the assumption that the barrier is inside the circle surrounding the inhabited area its distance from the centroid is lower than the radius of the circle i e the following inequality is verified 4 b c 2 r 2 to determine the intersection points between the circle and the barrier vent straight line we replace the generic point p t b t v b of the straight line in the circle s equation 2 obtaining the following condition b c t v b 2 r 2 or equivalently 5 v b 2 t 2 2 b c v b t b c 2 r 2 0 where represents the dot product equation 5 is a quadratic equation in the unknown t which due to the geometry of our points admits two distinct real solutions we want the solution in the range 0 1 since this would represent the intersection point between the segment equation 3 and the circle equation 2 to find the solution we calculate the discriminant δ 4 b c v b 2 v b 2 b c 2 r 2 and we know that because of equation 4 it is non negative therefore the two solutions of equation 5 are t 1 b c v b δ 4 v b 2 t 2 b c v b δ 4 v b 2 the new barrier position is then obtained substituting the solution of equation 5 withing the range 0 1 in equation 3 finally since we want to ensure that the barrier interacts with the flow we move the barrier position to the nearest point of the lava flow fig 9 fig 9 representation of the procedure by which a barrier is first pushed outside the inhabited area on the left and then snapped to the lava flow on the right note that the snap to flow is done also if the barrier wasn t originally within a township exclusion region fig 9 since the flow associated to the new configuration is not know yet the flow barrier distance is computed according to the flow emplacement obtained for the pso particle at the last step in the case of the first step we consider instead the flow emplacement without any barriers specifically the results of the magflow simulation for each particle are imported cutting out the flow areas that intersect the vent and townships exclusion regions then for each barrier in the particle we compute the position update according to the pso algorithm apply the barrier shifting due to the vent and township exclusion regions if necessary and finally compute the distance between the shifted barrier position to each dem cell occupied by lava according to the particle s previous configuration and move the barrier to the coordinates of the closest cell according to the quantization of the dimensions discussed in section 4 2 each of the corrections applied during the constraint procedure is done by rounding up the position displacement vector to the next multiple of the position quantization step b results of other runs for the test case we present other two runs named as run a and run b for the test case presented in section 5 to validate our pso based algorithm we aim to show the non unique global minimum point of the target function currently used which is the potential loss associated with an eruptive scenario where the loss in this initial stage of development is defined by simply estimating the amount of inhabited areas reached by the lava flow as we already pointed out the fact that there could be other potential barrier configurations that still achieve a null score i e no inhabited area is inundated is mainly due to our early stage definition of the target function in the future we plan to extend it to be more complete and realistic for the assessment of lava flow risk considering also scattered edifices and or land use characteristics this would probably lead to having a minimum value larger than zero reached by a smaller set of configurations figs 10 and 11 show the best configuration found by run a and run b of our algorithm with almost the same specifications of the test case starting from different random initial configurations the only different value is the suitable maximum number of barriers that we decide to decrease by one unit thus it is 8 since the optimal configuration found in the test case used only 6 barriers compered with the 9 maximum fixed as expected the results obtained show different optimal result with respect to section 5 presenting other potential barrier configurations that still achieve a null score totally safeguarding the threatened villages of sarro malopasso pisano and fleri diverting the flows in uninhabited areas tables 2 and 3 report details about the location and geometric features of each barrier in the optimal configuration of the two runs fig 10 best configuration for run a obtained using our pso approach which brings the score to zero barriers are numbered as reported in table 2 legends indicate the lava flow thickness in meters fig 10 we can see how in run a even if a maximum number of 8 barriers was set the algorithm finds an optimal configuration using again only 6 barriers indeed barriers 6 and 7 don t interact with the lava flow fig 10 moreover also the optimal configuration of run a includes both ditches 1 3 and 4 and barriers 2 5 and 8 table 2 best particle configuration of run a for each barrier the location and geometric characteristics langle and rangle in degrees length and height in meters are reported table 2 easting northing langle rangle length height barrier 1 505150 4170690 87 4 256 32 barrier 2 507980 4169220 90 90 168 11 barrier 3 505090 4170280 90 90 256 32 barrier 4 507050 4169480 90 90 163 32 barrier 5 507670 4169060 60 90 256 23 barrier 6 509150 4168270 20 90 120 7 barrier 7 509190 4168250 90 90 69 11 barrier 8 508080 4169160 51 90 181 32 fig 11 best configuration of run b obtained using our pso approach which brings the score to zero barriers are numbered as reported in table 3 barrier 8 is not displayed because its length is zero legends indicate the lava flow thickness in meters fig 11 in run b the algorithm finds an optimal configuration using only 7 barriers even if a maximum number of 8 barriers was set having barrier 8 a null length moreover the optimal configuration shown in fig 11 demonstrates how once again the combined use of both ditches 2 and 7 and barriers 1 3 4 5 and 6 can improve the results leading to a total diversion of the lava flow to uninhabited zones table 3 best particle configuration of run b for each barrier the location and geometric characteristics langle and rangle in degrees length and height in meters are reported table 3 easting northing langle rangle length height barrier 1 506050 4170090 89 88 41 23 barrier 2 505340 4170710 41 35 115 22 barrier 3 505480 4170510 38 41 219 6 barrier 4 507980 4169220 29 90 259 23 barrier 5 504250 4171070 9 88 165 32 barrier 6 506150 4169470 90 82 256 32 barrier 7 507380 4169430 63 34 226 32 barrier 8 510210 4168770 90 44 0 14 
25861,low cost sensors are emerging alongside innovative data transfer technologies allowing the integration of smart solutions for the decentralisation of water infrastructure in this work the smart rain barrel srb concept is introduced as an iot based solution for advanced rainwater harvesting the srb consists of a rain barrel extended by a discharge valve which is centrally controlled this concept offers 1 individual control of each implemented srb with its incorporation into the integrated water system management and 2 a simple large scale implementation of additional storage units as an alternative to future expansion of existing infrastructure the open source software smartin is developed by hypothetically retrofitting an alpine municipality with srbs to evaluate the effects of the latter on the urban water infrastructure compared to uncontrolled rain barrels a simple coordinated control strategy already clearly improves the performance of the integrated system by reducing combined sewer overflow and addressing drinking water demand keywords green infrastructure iot based solution real time control smart rainwater harvesting urban water infrastructure weather forecasts 1 introduction climate change urbanisation and maintenance are present and future challenges for urban water infrastructure urbanisation and loss of infiltration capacity of natural areas increase peak runoff while reducing groundwater recharge fletcher et al 2013 mcgrane 2016 miller et al 2014 moreover climate change has modified the intensity duration and frequency of rain events berggren et al 2012 willems et al 2012 and has resulted in higher temperatures bastin et al 2019 thereby increasing the demand for irrigation water parkinson et al 2016 through the interaction between climate change and urbanisation the probability of overloading the existing urban drainage systems yazdanfar and sharma 2015 and the impacts on natural water resources cominola et al 2015 have increased to counteract this trend locally the existing infrastructure which mainly consists of central solutions has been extended in the last decades by decentralised system applications such as green infrastructure gi fletcher et al 2015 yazdanfar and sharma 2015 with the aim to store rainwater and to increase infiltration and evapotranspiration examples of gi are green roofs infiltration trenches rain gardens rainwater harvesting rwh systems whereat the focus of this article is on rwh systems the main objective of rwh is to retain rainwater runoff with minor impurities for example from roof surfaces in decentralised rainwater storage tanks to substitute drinking water used in non potable water applications such as irrigation and toilet flushing campisano and modica 2015 khastagir and jayasuriya 2010 okoye et al 2015 apart from a drinking water reduction rwh systems present a local source control option to reduce runoff into the drainage system due to the detention of precipitation campisano and modica 2016 imteaz et al 2011 therefore a large scale implementation of rwh systems offers advantages for operation of urban drainage systems too for example the flood volume as well as the caused damage can be reduced as shown by huang et al 2015 and jamali et al 2020 through a simulation approach with swmm5 however as can be seen from the results of single rwh systems the efficiency of water saving and stormwater detention is determined by factors such as storage volume roof area precipitation and water consumption fewkes and butler 2000 but also subject to seasonal fluctuations due to different withdrawal quantities and precipitation amounts during summer and winter months barry and coombes 2015 furthermore the conventional rwh system is strongly dependent on the withdrawal quantities and the corresponding user behaviour in terms of storm water management palla et al 2011 sample and liu 2014 higher withdrawal volumes also deplete the stored volumes more rapidly meaning that more storage volume is available for stormwater detention quinn et al 2020 as a result these systems are only conditionally suitable for the reduction of extreme rain events as they are likely to be already filled by the time that the rainfall peak occurs schubert et al 2017 vaes and berlamont 2001 developments in the field of smart cities have contributed to the evolution of the internet of things iot in the water sector low cost sensors are emerging along with innovative data transfer technologies which allow for the integration and networking of individual system components with the overall system li et al 2014 ray 2018 however these are rarely implemented in real time workflows of urban water systems li et al 2020 wong and kerkez 2016 but novel innovative solutions are increasingly emerging creaco et al 2019 kerkez et al 2016 moy de vitry et al 2019 for example the outflow of a storage unit can be changed by real time controlled valves allowing an effective use of the available storage volume by emptying the storage units prior to precipitation events xu et al 2018 demonstrated through a simulation approach that applying weather forecasts in combination with real time control rtc technology to rwh systems reduces the frequency of complete filling and thus prevents overflow as di matteo et al 2019 and liang et al 2019 showed in a simulation optimised process a coordinated control of individual smart rainwater tanks can also reduce the peak runoff in sewer systems even for rare rain events that occur over long durations with prior knowledge of future weather conditions roman et al 2017 integrated the irrigation requirements into a smart rwh system increasing the efficiency of both the drinking water conservation and the detention effect however the discharge of rainwater leads to a conflict between the two contradicting objectives namely 1 discharge the rainwater as fast as possible to provide the maximum storage capacity for the next event and 2 to store rainwater as long as possible for substituting as much drinking water as possible this was demonstrated by e g behzadian et al 2018 and zeisl et al 2018 using a conceptual model run on a daily basis previously described studies investigated smart rainwater storage tanks larger than 1 m³ the approach presented in this work uses smart rain barrels as a system wide implemented iot based solution with real time controlled micro storages between 200 l and 500 l just like conventional rain barrels the srb can also be used to retain rainwater for irrigation purposes while the benefits are evident at the local level large scale advantages potable water peak flow reduction of flooding volume or combined sewer overflow are usually limited to mitigate the disadvantages of the small storage size relative to the overall system new ways of operation are developed and implemented for example if an insufficient rainwater amount is forecasted to meet the estimated irrigation demands drinking water can be taken from the potable supply system during off peak hours to fill up the deficits on the other hand sufficient capacity can be provided for rain events by emptying the srb before heavy rain events therefore the srb is conceptualised as an iot based solution allowing for each srb to be controlled individually in real time to achieve these benefits weather forecasts with high temporal resolutions from a meteorological service are implemented into the control strategy as they can clearly increase the detention effects of single micro storages as shown by oberascher et al 2019 besides using weather forecasts for the operation of real time controlled systems has a positive effect on the performance of the entire system such as by reducing the combined sewer overflows courdent et al 2015 gaborit et al 2013 löwe et al 2016 due to the small storage size the srbs can be placed on the surface as a result the installation of srbs into the existing systems can be relatively easy thereby allowing for large scale retrofitting of the urban water infrastructure therefore the srb represents a successful iot based link between water supply and the urban drainage system the effectiveness of a single srb has already been proven through the operation of a prototype at the university of innsbruck and numerical simulations at the household level oberascher et al 2019 the aim of this paper is a model based upscaling to an entire case study to investigate the effects of the srb on the urban water infrastructure by a large scale implementation and the development of control strategies for multiple srbs in addition to the positive effects of rwh systems reduction of drinking water demand and stormwater runoff a reduction in consumption in the existing water infrastructure can also increase the risk of water stagnation and associated water quality problems in water supply networks and deposits in combined sewers grandet et al 2010 sitzenfrei et al 2013 2017 therefore an integrated modelling of urban drainage and water distribution network as well as water tanks on high temporal and spatial resolution is required to be able to fully investigate the effects of rwh systems recent research studies behzadian et al 2018 sitzenfrei et al 2017 that have followed this integrative approach are mainly based on conceptional models e g urbanbeats or watermet2 run on daily respectively hourly time steps in addition sitzenfrei and rauch 2014 investigated the effects of decentralised solutions on dry weather flows in combined sewer systems by coupling epanet2 and swmm5 applying an hourly time step too in contrast hydrological process of urban drainage systems e g combined sewer overflows take place in timesteps of seconds or minutes requiring a higher temporal resolution for an adequate modelling to evaluate the effects of the srbs on the urban water infrastructure the open source software smartin is developed and presented in this work the key features of smartin are 1 allowing individual control of each implemented srb based on weather forecasts and current system states 2 choosing between perfect and historical real weather forecasts 3 using a high spatial and temporal resolution for the necessary simulation of micro storages 4 coupling of urban drainage and water supply systems for high temporal resolutions and 5 developing and testing of integrated real time control mechanisms the software is written in python using several open source python packages for the simulation of the hydrodynamic model swmm5 gironás et al 2010 for the urban drainage system and the quasi stationary model epanet2 rossman 2000 for the water supply system to test the developed software we focus on an experimental retrofitting of an existing urban water infrastructure in an alpine municipality with srbs besides historical weather forecasts from the austrian national weather service are included into the control strategy additionally different control strategies are applied to analyse the impacts of a large scale implementation on the water supply e g substituted drinking water changes in water pressure and age and on the urban drainage system e g changes in combined sewer overflows and flooding the objectives of this work are proposing the smart rain barrel srb concept as an iot based solution for a networked water infrastructure development of a high resolution simulation tool which allows a large scale implementation of the srbs and the real time control of each individual rain barrel as well as a coordinated control in the context of the urban water infrastructure providing a proof of concept of the hypothetical retrofitting of existing infrastructure in a case study analysing the impacts of different control strategies including various weather forecast conditions on both the urban water supply and drainage system in an integrated model 2 the smart rain barrel concept as an iot based solution the concept presented in this work aims to compensate the disadvantages of micro storages used for rainwater harvesting e g small volume compared to the entire system depending on user behaviour by smart approaches the concept should be suitable for large scale implementation in new building areas as well as for retrofitting in existing urban areas this concept results in the following practical requirements for micro storages 1 ensuring simple implementation in already existing infrastructure 2 independent rtc of each micro storage due to different connection areas and user behaviours e g opening and closing of discharge valves depending on the current filling level and 3 energy efficiency in operation because batteries are needed for power supply e g limiting the numbers of control actions per day in order to meet these conditions the srb concept was developed as rtc micro storages the main part is a conventional rain barrel available in hardware stores and extended to an iot based solution as shown in fig 1 a rainwater collector is used to connect the rainwater downpipe of roof areas with the srb wherein the srb and rainwater collector act as connected vessels and therefore preventing srb overflow this is also the reason why no further emergency or backup strategy is needed for loss of power or data connection the srb has a manually controllable valve for personal water withdrawal and an automatically controllable valve at the bottom for irrigation and detention management the srb is equipped with a solar panel and a rechargeable buffer battery for power supply whereat the solar panel is able to recharge the battery with the sunshine duration on a clear summer day the srb is controlled centrally the measured values and control commands are sent via low power wide area technology lorawan lora alliance 2017 lorawan is an innovative data transfer technology for iot applications bardyn et al 2016 song et al 2017 it is characterised by low energy consumption and high transmission ranges and therefore particularly suitable for battery powered sensors in addition high resolution weather forecasts are used in the control strategy to increase the efficiency of the existing storage volume the prototype shown in fig 1 has been in operation at the university campus of innsbruck during the summer months since october 2019 in addition fig 2 illustrates a schematic cross section of the srb for more information the effects of a single srb for a single household have been investigated by oberascher et al 2019 as the results show the storm water detention volume of the srb can be increased by up to 300 compared to the uncontrolled rain barrel whereas 16 90 of the irrigation requirements can be satisfied by the srb depending on the storage volume the impact of the rtc is most significant if the rain barrel is partly or completely filled due to previous rain events i e when the dry weather period between two rainfall events is too short through the rtc the srb can be emptied before the next rain event to provide more volume for storm water retention than in the conventional rain barrel however emptying the srb depends on the predicted amount of precipitation if the predicted rain volume is less than the actual rain the srb would only be partially filled by the end of that rain event therefore water savings decrease with higher weather forecast periods due to uncertainties in precipitation forecasts fig 3 shows the idealised functionality of the srb concept for a real rainfall event in comparison with a conventional uncontrolled rain barrel both of which have a storage volume of 0 5 m³ both storage units have a connected impervious area roof area of 145 m2 for this exemplary case a perfect weather forecast with a forecast period of 2 h is chosen for the srb meaning that the forecasted amount of rain matches perfectly with the real rainfall event additionally it is assumed that future weather developments are updated every hour because of past rainfall events and subsequent irrigation activities both operating semantics are partially filled before the predicted rainfall event as can be seen from fig 3 in the example the rain events mostly take place between 13 15 and 13 30 with a total rainfall amount of 3 7 mm and a maximum intensity of 0 3 mm min resulting in a peak outflow of 0 74 l s from the sub catchment into the drainage system in the idealised example the first time of update is at 12 00 and future weather developments are recorded for the next 2 h the total inflow to the srbs is estimated by the simplified approach described by roof area discharge coefficient rainfall volume and was calculated to be 0 51 m³ because the estimated inflow to the srb was higher than the total storage volume the srb receives a control command to completely empty itself before the rain event as a result the discharge valve of the srb opens automatically and closes again at 12 17 when the srb is emptied this allows the entire storage volume of the srb to be used as a detention volume therefore almost the entire roof discharge can be captured by the srb and the peak outflow during the rain event is reduced by 80 in contrast to the srb the conventional rain barrel is already fully filled by the time the peak intensity is achieved meaning that the peak outflow into the drainage system cannot be reduced after the rainfall event both operating approaches are fully filled to provide as much rainwater for irrigation purposes as possible in contrast to oberascher et al 2019 who focused on one single srb applied at household scale this paper investigates a model based upscaling of srbs to an entire case study 3 methods in order to analyse the impacts of the presented srb concept on the urban water infrastructure the software smartin was developed smartin implements micro storages in a selected number of buildings each with one connection to the urban drainage as well as to the water supply system the micro storages are rtc allowing the discharge of rainwater before a precipitation event to provide additional storage volume if the micro storage cannot cover the irrigation demand with rainwater alone the micro storages are automatically filled with drinking water during hours with lower water consumption to satisfy the irrigation needs as a result smartin can be used for modelling of the rtc of micro storages developed as an iot based solution in a coupled model of urban drainage and water supply systems it was specifically designed for the use of any kind of micro storages to make the software generally applicable the srbs represent a special case of the micro storages and were used in the case study to test the software stormwater management model swmm5 gironás et al 2010 and epanet2 rossman 2000 are used to simulate urban drainage and water supply respectively coupling of the separated systems namely urban drainage water supply and micro storage volumes was achieved via python programming language and usage of several open source python packages 3 1 integrated modelling of micro storages as iot based solutions in the context of urban water infrastructure smartin the setup of smartin is illustrated in fig 4 it consists of two main components the urban drainage and the water supply model weather forecast rainfall data and current simulation results are individually used for the rtc of the micro storages during rain events during dry periods the stored rainwater is used for irrigation purposes and the irrigation demand is determined as a function of temperature and daily rainfall in case the stored rainwater cannot satisfy the irrigation demand drinking water from the water supply system is automatically extracted during off peak hours finally the amount of harvested rainwater and the demand for the drinking water are used as input parameters for the water supply simulation for the water supply assessment water demand is represented by a dynamic model consisting of seasonal temperature dependent and hourly factors see also 3 1 2 in the case of nodes with micro storages the water demand is reduced due to the harvested rainwater and the shift of water demand from the peak hours to night hours when there is lower demand for drinking water 3 1 1 urban drainage systems pyswmm swmm5 is a hydrodynamic model for urban drainage to numerically calculate water flows in sewers gironás et al 2010 the low impact development lid type rain barrel was chosen for the model representation of the micro storages the lid type rain barrel consists of a storage layer and a drain layer the storage layer provides the detention volume of the micro storage while the drain layer represents the outflow based on the chosen land designation and the degree of penetration properties are randomly selected as locations for micro storage volumes next a lid type rain barrel is created for each property to individually control each micro storage the selection of the rain barrel size depends on the roof area where a rain barrel size corresponding to a precipitation quantity of 6 mm was adopted as the reference value furthermore the lid type rain barrels are assigned to the respective properties in smartin pyswmm as a python wrapper for swmm5 mcdonnell et al 2020 is used for the simulations in contrast to native swmm5 pyswmm provides three key features which are essential for micro storages developed as iot based solutions 1 simulation of micro storages as well as changing parameters during the simulation 2 accessibility of results during simulation and 3 the ability to add control strategies at each routing step of swmm5 first swmm5 allows the implementation and simulation of micro storages at household level by using the lid type rain barrel however it is not possible to implement variable discharge heights or advanced control rules that allow joint control with the entire network moreover the network has only one drainage pathway which means that it is impossible to empty the micro storages for drainage and irrigation separately pyswmm is intended as the interface to swmm5 and thereby allows for the separate manipulation of each swmm5 element including lid parameters in addition some flow parameters e g inflows to nodes lid drain parameters etc can be modified during simulation as previously described the lid type rain barrel consists of a storage layer and a drain layer storage dimensions can only be set before simulation while drain values can be changed during simulation in the case of the micro storages the drain coefficient can be set either to zero no discharge or to a specific discharge value allowing the controlled opening or closing of the drain furthermore the outlet node of lids can be changed during simulations through pyswmm this has the advantage that two outlets can be used one for rainy weather as a connection to the sewer network and one outlet for irrigation purposes second pyswmm creates a coding interface to the binary output file in native swmm5 simulation results for lid elements are written into a separate text based file after simulation in contrast simulation results including lids can be accessed directly via pyswmm this functionality is used in smartin to query the current water depth in the various micro storages and filling levels in the drainage system in order to determine the current state of the system for example the drain is opened or closed based on the system states by setting drain coefficients finally pyswmm supports a step by step simulation of swmm5 input files this gives the possibility of implementing advanced control options after each simulation step for example weather forecasts can be included in control options due to the functions described above the drain of the micro storages can be controlled in real time by estimating the predicted inflow to the micro storage the valve can be opened to create detention space if necessary if the predicted volume of inflow is equal to the detention volume the valve is closed to achieve full filling of the micro storage during the rainfall and therefore the maximum possible volume for irrigation is provided 3 1 2 water supply system epanet2 is a simulation tool for the analysis of pressure and quality in systems under pressure such as the water supply system where the time dynamics are obtained by the sequence of quasi stationary states rossman 2000 the epanet programmer s toolkit is an extension to native epanet2 and allows the manipulation of the network as well as the operation parameters rossman 1999 this enables a long term simulation with different hourly extraction volumes of drinking water on a global scale and also allows the demand for each node to be changed individually as needed for the simulation of the micro storages fluctuations in drinking water consumption are considered based on seasonal climatic and hourly components gato et al 2007 zhou et al 2002 and determined by the following function 1 w h i w b i f m i f c i f h i where w h i is the hourly water demand or source abstraction l s w b i is the base demand or base abstraction l s f m i is the monthly adaptation factor f c i is the climate based adaption factor and f h i is the hourly adaption factor it was assumed that no hourly changes were to be expected in the drinking water sources thus f h i was set to 1 for the source pattern temperature has the greatest impact on regression models for water consumption neunteufel et al 2014 opalinski et al 2019 therefore the climate based adaption factor f c i is calculated using the regression formula 2 f c i α 0 i α 1 i t m d α 2 i t m d 2 α 3 i t m d 3 where α0 i α3 i are calibration factors and tm d is the daily mean temperature the epanet programmer s toolkit is utilised via python programming language using the python epanet toolkit provided by open water analytics https github com openwateranalytics epanet python tree dev epanet python epanet python 3 1 3 control strategies for urban drainage systems the control strategies for the micro storages implemented in this case study can be divided into dry and wet weather schemes during the rainy season the srbs are operated dynamically by means of rtc to reduce peak run off rates in the main sewer system and still ensure a fully filled storage unit at the end of rainfall for the dry period on the other hand the strategy is to use the stored water for irrigation purposes the workflow of the urban drainage simulation can be seen in fig 5 where the control strategies are divided according to the different durations of update for example control strategies for the rainy weather are run on an hourly basis depending on the chosen update time step of weather forecasts while the irrigation command is executed only once a day the description of control strategies assumes a partially filled micro storage and is defined as 3 v t o t a l i v s t o i v d e t i where v total i is the total available volume of the micro storage v sto i is the rainwater storage volume currently in use and v det i the detention volume as the remaining volume which can be used for rainwater detention as an initial assumption the drain is closed at the beginning of the simulations after the simulation starts the current simulation time t sim d is queried at each simulation step of swmm5 as recommended by en 16941 1 2018 the efficiency of rainwater harvesting systems should be investigated at the end of the selected time step it was assumed that irrigation takes place once a day and therefore 23 00 was chosen as the automatic irrigation time t irrigation if t sim is greater than or equal to t irrigation crop evaporation was calculated as a reference value for the irrigation quantity by following the recommendations of the food and agriculture organization of the united nations allan et al 1998 it is assumed that the irrigation area is well watered and thereby provides the ideal agronomic conditions based on this assumption crop evapotranspiration et c mm day is used to compute the water demand evapotranspiration is estimated by using the hargreaves equation requiring only temperature data as the input variable hargreaves and samani 1985 and an alternative when wind speed humidity and solar radiation data are absent allan et al 1998 additionally it is assumed that the growing season is characterised by a constant water demand considering these simplifications the daily et c is calculated as 4 e t c c a d j t m d 17 78 t m a x d t min d r a k c where c adj is a calibration parameter for the hargreaves equation t m d t max d and t min d are the average maximum and minimum daily temperatures c respectively r a is the water equivalent of extra terrestrial radiation mm d and k c is the single crop coefficient in the software temperature measurements are used for the temperature parameters c adj haslinger and bartsch 2016 and k c allan et al 1998 simonne et al 2007 are extracted from literature while r a is calculated according to the formulas given by allan et al 1998 in the next step irrigation demand v ir i is determined individually for each micro storage by using crop evaporation the daily amount of rainfall and irrigation area and comparing with the available storage volume if the micro storage is empty v sto i 0 no rainwater can be extracted for irrigation purposes and the entire irrigation volume is taken from the drinking water supply if the micro storage is filled v sto i 0 the irrigation valve is opened and the amount of rainwater useable for irrigation purposes is calculated v har i in case the stored rainwater cannot satisfy irrigation requirements drinking water is used for the remaining irrigation requirements the required amount of drinking water v ws i is calculated as v ws i v ir i v har i finally a closing target value v sto goal i is set corresponding to the amount of rainwater provided by the micro unit and determined through v sto goal i min v sto i v ir i 0 furthermore v har i as well v ws i per day is saved and used as the input parameter for the water supply simulation in the last step the t irrigation for the next day is calculated by adding a day to the current t irrigation if t sim is equal or greater than update time of the weather forecast t weather control strategies for rainy weather are executed the key element here is the implementation of weather forecasts as a result the future rainfall expectations the amount of rainfall rrain and time of peak intensity tpeak are known for the chosen period of forecast referred as accumulation time in the subsequent sections next the inflow to each micro storage v in i is estimated based on the forecasted precipitation and the roof area of the property and compared with the available volume for rain detention vdet i subsequently three different control strategies can be distinguished for the case of rainy weather in the first control strategy the inflow v in i is lower than the available storage v det i since the full filling of the micro storage is not achieved no further control action is needed if v in i greater than v det i the drain valve is opened and the micro storage starts emptying in the 2nd control strategy v in i is lower than v total i therefore an emptying target v det goal i is specified to retain the entire precipitation in the micro storage and to guarantee a fully filled micro storage at the end of the precipitation event for irrigation purposes however if v in i is greater than v total i the 3rd control strategy is applied the exact time of maximum intensity tpeak is determined and used as the closing time t closing i for the drain valve to reduce peak run off in the sewer system finally next t weather is calculated by adding the update time step of the weather forecast to the current t weather the prototype of the srb was developed as an iot based solution using lorawan to exchange measurement values and control commands however lorawan transmits on the public frequency range which limits the number of data exchanges lora alliance 2017 e g the duty cycle influences the number of data packets and the packet size this approach has also been applied to the simulation assuming that only one control command e g open drain valve until a specific filling depth or time is attained is allowed per weather forecast update time 3 1 4 control strategies for the water supply system the workflow process of the water supply simulation is that first the hydraulic solver is started as shown in fig 6 in smartin the hourly patterns for the next day are set at the end of the day during the simulation therefore the simulation time t sim w is queried for each of the simulation time steps if t sim w is greater than the end of day t end day the source and global demand patterns are calculated using eqs 1 and 2 first the demand pattern is set globally for all nodes then the patterns for all micro storages along with the respective connection nodes to the water supply are changed individually using the simulation results on the used irrigation rainwater v har i and needed drinking water v ws i from the urban drainage model if drinking water is required for irrigation v ws i 0 the micro storages are filled automatically with drinking water this allows control of the filling time and a shift in water demand from peak hours to hours with lower consumption e g at night the filling is set to start at 22 00 so that the micro storage can provide the irrigation requirements at 23 00 and the pattern value can be increased by the amount of needed drinking water however v har i and v ws i mean saving of drinking water and therefore a reduction during peak hours it is assumed that peak hours occur at 17 00 to 18 00 and that 40 of the water requirement during the peak hour is allocated to irrigation therefore a maximum of 40 of the water demand requirements during peak hours can be substituted by the micro storages by providing rainwater for irrigation purposes 3 1 5 input parameters and requirements for smartin to execute the software smartin detailed input files for swmm5 and epanet2 are required at property level therefore each property has to be divided into the following subareas house access road and green space each subarea should be assigned an independent sub catchment in the swmm5 input file additionally information is needed for each property regarding the connection point to the water supply system the subcatchments for the implementation of the micro storages are selected by their name in the swmm5 input file where the names of the subcatchments should contain the land classification as well as the land usage in the form land classification land usage for example the name for a house subcatchment h in the land classification residential area r should include the following form h r the implementation of smartin was accomplished by three main python functions whereat the user refers the run sim function as the main application this function takes the 12 user inputs listed in table 1 the input parameters allow the user to set installation locations of the micro storage e g degree of penetration land classification control options e g uncontrolled or smart as well as to specify weather forecast parameters e g perfect or real accumulation period update time step the run sim function starts the sim drainage function first and when this function is completed the sim supply function is executed afterwards 3 2 case study for testing the software the functionality of smartin and the effects of srbs on the urban water infrastructure were tested using a case study of an alpine municipality in austria for the case study existing infrastructure data of water supply and urban drainage system as well as personalized water consumption data were applied therefore the case study will be referred to anonyms in the following the municipality has about 2900 inhabitants and approximately 630 properties and an area of 15 2 ha is connected to the urban drainage system fig 7 a shows the land classification for each property in the municipality the land classifications residential area mixed use area and agricultural use were selected as the installation locations for the srbs because buildings in these areas have both green areas for irrigation and space for installation in total 384 properties with a roof area of 8 16 ha can be used for the implementation of the srbs which corresponds to a degree of penetration of 100 in addition the case study area was divided into house traffic and green areas fig 7 b for the area determination for the swmm model the software is intended to be used for rtc applications to further investigate the quality and uncertainties of weather forecasts a complete data set consisting of historical forecasts with different forecasting periods as well as the real measure data for the year 2015 is used uncontrolled hence referred to as conventional rain barrels are only emptied when the stored rainwater is exhausted for irrigation purposes however in austria irrigation mainly occurs during the summer half year 21 3 23 9 neunteufel et al 2014 as a consequence outside this period no rainwater is taken and thereby no detention storage can be provided by the conventional rain barrels however the srbs can also be used in other seasons besides summer half year because their process of emptying for detention is automated to make the results comparable only the summer half year was considered for the srbs rainfall data available at 1 min intervals from a nearby weather station were used for the investigations according to the classification of peel et al 2007 the case study is located within the dfb d cold climate f without dry season and b warm summer climate zone the en 16941 1 2018 recommends a rain series of a minimum of 5 years for effectiveness analyses of rwh systems however due to the long simulation times we decided to test the software only for the summer half year 2015 the precipitation sum for the summer half year 2015 was 651 mm and thus on average over the years 2015 2019 30 mm with the maximum precipitation in may 152 mm 3 2 1 urban drainage system the municipality has a combined sewer system with flow direction from southwest to northeast as shown in fig 7 e the case study has an oversized combined sewer overflow structure at the catchment outlet in the north east volume of 350 m³ throttle discharge 50 l s to better outline the impact of the state of the art capacities it was re dimensioned according to the actual austrian technical standard rb 19 2007 volume of 154 m³ throttle discharge 22 2 l s together with real information about the capacities of the sewer system and the results of fig 7 a and b a detailed swmm5 input file on property level was created in a semi automatic process rain data two level measurements and one flow measurement at different points of the sewer systems from a measurement campaign between june and september 2017 were used for model calibration and validation with pcswmm chi in the simulated summer half year 2015 the flood volume of stormwater discharging from the sewer manholes was 285 m³ and the combined sewer overflow volume was 54 950 m³ 3 2 2 water supply system for the water supply system the calibrated model of sitzenfrei and rauch 2015 was employed the system is fully gravity driven with an elevated water reservoir tank volume of 1440 m³ in the southwest of the case study area the model was extended by connection of additional households to develop a detailed model at household level as shown in fig 7 d in addition historical node water consumption was updated by an address based billing for the years 2017 and 2018 in total an average of 9 4 l s 810 m³ d water was extracted from the system in these two years including water losses and public fountains for the calculation of dynamic water consumption the monthly and hourly adaption factors were adapted from sitzenfrei and rauch 2015 while the climate based adaption factor was determined using the measurements of the drinking water quantity supplied to the system for the years 2008 2012 as shown for the austrian daily water demand neunteufel et al 2012 water consumption is increasing with increased length of the dry periods eq 2 was therefore converted to a combined model of temperature and dry period and divided into four different classes days with precipitation 1 3 3 7 and greater than 7 days without precipitation 3 2 3 weather forecasts weather forecasts are one of the key elements in the control strategy of the srbs perfect i e observed weather as well as real historical weather forecasts from the integrated nowcasting through comprehensive analysis inca system haiden et al 2011 were integrated into the control strategy for the consideration of the future weather development the inca system is designed to analyse weather developments and for nowcasting different weather parameters in the mountain terrain it uses grid cells with a resolution of 1 km thereby allowing the rapidly changing topography of the alps to be represented in numerical weather forecast models first a combined analysis of rain gauge observations and weather radar is performed every 15 min and transferred to the numerical grid using a series of these analyses the precipitation movements are kinematically extrapolated to forecast the field of future precipitation as the prediction quality decreases with increasing duration due to the kinematic extrapolation the application of research to operations at mesoscale arome high resolution model grid resolution 2 5 km wittmann and meier 2016 and the integrated forecasting system ifs model grid resolution 9 km malardel et al 2016 are scaled to a resolution of 1 km and stepwise merged with the inca system to get a smooth transition up to 48 h 3 2 4 srb parameters for the theoretical retrofitting of the case study with srbs rain barrels available in normal hardware stores with storage volumes of 200 l 300 l and 500 l were selected likewise available ball valves were chosen as discharge valves with real diameters which guarantees approximately 30 min emptying time for the rain barrels in terms of model implementation the micro storages consist of a storage layer and a drain layer the characteristics of the rain barrels with chosen ball valves can be seen in table 2 showing the swmm input parameters for both layers with light grey background the height and area of the storage layer is given by the dimensions of the rain barrels as recommended by rossman 2015 the drain exponent is set to 0 5 while the drain coefficient is calculated as 60 000 avalve astorage where avalve is the area of the ball valve and astorage the area of the rain barrel if the real green area is used as the irrigation area the srbs are normally emptied after one irrigation process to determine the impact on the performance of the srbs a reduced irrigation area of 25 m2 was assumed the garden area is automatically irrigated and the irrigation amount complements the evapotranspiration the evapotranspiration is calculated according to eq 4 whereat the monthly calibration parameter c adj for the alpine terrain was taken from haslinger and bartsch 2016 therefore an ideal irrigation model was applied and the irrigation demand is independent of the real irrigation needs of the properties 3 2 5 trials table 3 shows the configuration parameters for the simulations the degree of penetration fraction of srb installed to total number possible was modified for both smart and uncontrolled operation types while the weather forecast parameters were only applied to the srb concept as degrees of penetration 25 50 75 and 100 were applied in addition perfect and real weather forecasts were used to compare the ideal system performance perfect weather forecasts with the real effects therefore three different accumulation periods of between 1 h and 12 h were examined in more detail both the srbs and the conventional rain barrels are implemented randomly in smartin in order to make the results as independent from the implementation location as possible each configuration was simulated multiple times however as the simulation results per configuration showed only minor differences 10 repetitions were chosen except in the case of 100 degree of penetration as there is only one possibility in total 217 smartin runs were carried out to investigate the effects on the urban water infrastructure in order to be able to model the discharge process in the sewer network as accurately as possible and to obtain a damping of the peak runoff rate due to increasing flow time e g as a relevant wave for the combined sewer overflows hydrodynamic modelling of the storm water system is required which results in the simulation steps in seconds in addition each srb is controlled in real time meaning that control actions are executed at every simulation step e g each simulation step the individual filling depth is queried the combination of srbs and hydrodynamic modelling greatly increases the simulation time ranging from 30 h 25 penetration rate to 100 h 100 penetration rate in contrast to the urban drainage model the water supply system is calculated quasi stationary with simulation times in minutes 4 results and discussion figs 8 10 illustrate the performance analysis of a large scale implementation of the srbs compared to the reference state without srbs and to uncontrolled rain barrels 4 1 performance of smart rain barrels in average a storage volume of 468 l was added to each property resulting in an additional storage volume of between 45 m³ 25 penetration rate and 182 m³ 100 penetration rate as can be seen in fig 8 a the dispersions between the random implementations of the additional storage volume are small in addition the results of each configuration scatter only slightly therefore the mean value of each configuration set is selected to illustrate the effects of a large scale implementation of the srbs in the following graphs due to the usage of harvested rainwater for irrigation purposes the conserved drinking water ranges from 5 6 m³ to 9 4 m³ per srb for the summer half year 2015 upscaling to the case study means the conservation of between 533 m³ and 3490 m³ of drinking water depending on the degree of penetration as shown in fig 8 c and corresponds to approximately 65 respectively 430 of daily water consumption of all properties in the case study interestingly the uncontrolled rain barrel represents the optimum value for rainwater harvesting the inflow to the srb is estimated using the weather forecasts and based on this estimation the srb is emptied to provide additional detention volume if more precipitation is predicted than what actually falls the srb is not completely filled at the end of the rain event and thereby the amount of substituted drinking water is reduced therefore a perfect weather forecast delivers almost the same results as the uncontrolled rain barrel the small differences with increasing accumulation period meteorological term for prediction horizon result from the control strategy for rain events which does not take the irrigation requirements into account however water savings are clearly decreased with increasing accumulation period for the real weather forecast compared to the uncontrolled rain barrel due to increased uncertainty the total detention volume for the uncontrolled rain barrel is illustrated in fig 8 d and is between 976 m³ and 3926 m³ depending on the degree of penetration respectively 10 2 m³ per uncontrolled rain barrel in comparison the detention volume of the srbs is significantly increased by adding weather forecasts to the control strategy a penetration rate of 25 of the srbs controlled with a real weather forecast with an accumulation time of 12 h could retain 4089 m³ rainwater which is more than a 100 penetration of uncontrolled rain barrels depending on the degree of penetration a total of 4089 m³ to 29 252 m³ of precipitation could be temporarily retained in addition a difference between the accumulation times i e the chosen weather forecast period is noticeable at shorter accumulation times the control steps or switching operations are at shorter intervals allowing a more frequent emptying of the srbs whereas the highest amount of precipitation could be retained with an accumulation time of 1 h therefore the detention volume decreases with increased accumulation time besides the deviations in total detention volume between the real weather forecast and the perfect weather forecast as the theoretical maximum of the total retention volume are low ranging between 14 for an accumulation time of 1 h and 9 for an accumulation time of 12 h the prototype of the srbs is powered by a rechargeable battery and solar panel meaning that an additional power supply is not needed which supports its easy retrofitting to urban areas therefore an efficient and energy saving operation which is defined by the number of switching operations is needed fig 8 b illustrates the total number of switching operations ranging between 10 790 and 325 550 however the number of switching operations of a single srb is independent of the degree of penetration ranging from 112 accumulation time of 12 h real weather forecast to 850 accumulation time of 1 h perfect weather forecast emptying processes for providing additional storage volume during the summer half year 2015 the number of switching operations is mainly influenced by the accumulation time shorter accumulation times evidently increase the number of switching operations additionally applying real weather forecasts reduces the number of switching operations of the srbs due to an overestimation of the predicted rain amount compared to perfect weather forecasts 4 2 impact on urban drainage system the srbs are used for the detention of rainwater thereby influencing drainage processes in the sewer system e g combined sewer overflow flooding fig 9 a shows a flood volume of 285 m³ from the urban drainage system for the reference state during the summer half year 2015 as can be seen adding weather forecasts to the control strategy reduces the flood volume more significantly compared to the reference state as well as to uncontrolled rain barrels in total the srbs can reduce the flood volume by between 7 m³ 3 and 115 m³ 40 with a clear difference between the accumulation times as well as perfect and real weather forecasts because only one control command is allowed per update time step of the weather forecasts a shorter accumulation time allows therefore more frequent emptying operations as a result shorter accumulation times 1 h 4 h decrease flood volumes better than higher accumulation times 12 h interestingly the difference between real weather forecasts with an accumulation time of 12 h and the uncontrolled rain barrels is small in addition the difference between perfect forecasts as the best possible performance and real forecasts increases with increasing accumulation time as uncertainties of the real weather forecast are not considered in the control strategy similar to the flood volume the srbs can also reduce the combined sewer overflow volume relative to the reference state as demonstrated in fig 9 b the combined sewer overflow volume is 54 950 m³ for the reference state and can be reduced by 580 m³ 1 to 7670 m³ 14 with the srbs however as the results show a large scale implementation of srbs does not necessarily mean an improvement over uncontrolled rain barrels the srbs influence the combined sewer overflow volume in two ways first they provide additional detention volume and thereby reduce the amount of discharged combined sewer overflow volume second the discharge valves are opened simultaneously at all srbs creating an additional discharge wave in the sewer system for example at a 25 penetration rate of the srbs the additional peak discharge in the sewer system due to the emptying of the srbs can be up to 25 l s which is as high as the throttle discharge to the wastewater treatment plant 22 2 l s if the conditions are unfavourable e g partially filled combined sewer overflow tank due to light rain event an additional overflow event could occur therefore worsening the system performance this effect is more common with shorter accumulation times 1 h and 4 h and frequent emptying phases while for higher accumulation times 12 h the detention capacity dominates nevertheless as the results for the perfect weather forecasts show even short accumulation times and a simultaneous emptying of the srbs can clearly reduce the combined sewer overflow volume under ideal conditions perfect control 4 3 impact on the water supply system the srbs have an impact on the water supply system by reducing the drinking water demand as rainwater is provided for irrigation purposes the case study shows typical water consumption for smaller municipalities with peak hours in the evening and low water consumption at night one of the reasons for peak hours is the high external water consumption including in uses such as garden irrigation in contrast to the uncontrolled rain barrels the irrigation process of the srbs is automatic this means that the srbs can be filled during hours with low demand for drinking water supporting a shift in water consumption therefore a large scale implementation of the srbs in the case study allows a reduction of these peak hours by substituting rainwater for drinking water and shifting irrigation water consumption from the peak hour to the night hours with lower water consumption however the effects on the water pressure at peak hours are low as can be seen in fig 10 b and e maximum water pressure increases are between 0 4 m for a 25 penetration rate and 1 3 m for a 100 penetration rate one of the reasons for this is that the srbs were predominantly implemented in single family houses while neglecting large water consumers from multi party buildings trade or agriculture although the single family houses represent 60 of the buildings they are only responsible for about 20 of the water consumption moreover it was assumed that irrigation accounts for 40 of the peak demand if all these factors are considered only a maximum of 8 of the peak demand can be influenced by the srbs which explains the minor increase in water pressure similar to the strategy for rainy weather the additional water demand for irrigation is satisfied with drinking water from the water supply system by filling the srbs automatically at the same time between 22 00 and 23 00 due to the large scale implementation a clear reduction in the water pressure at the filling time is noticeable as illustrated in fig 10 a und d for a 25 penetration rate the maximum reduction of water pressure is 1 08 m while for a 100 penetration rate the water pressure is reduced by 5 4 m in addition the pressure reduction is increasing with the increase in the distance to the elevated tank however a simplified model with a garden area of 25 m2 and compensation of the evapotranspiration was used to calculate the irrigation demand if potable water is needed for irrigation all srbs are simultaneously filled with drinking water from the water supply system because the drinking water is automatically drawn during the night hours with low consumption the effects under normal operation are minimal however the simultaneous withdrawal can lead to problems in the event of unforeseen events such as the extraction of fire water therefore the extraction of drinking water for use in irrigation purposes should be in coordination with the water supply performance fig 10 c und f show the average change of water age for a 25 and a 100 penetration rate of the srbs respectively due to the large scale implementation of the srbs the water age for all nodes in the water supply system is reduced by a maximum of 0 4 h it is noticeable that the improvement in water age is achieved mainly at the end nodes or end strands of the water supply system while nodes in the meshed system show only minor changes the improvements in water age are achieved by the fact that the automatic irrigation system needs more drinking water for irrigation suggesting that the srbs could also be used as a local measure to improve the quality of the drinking water in areas with a high water age 4 4 implementation strategy for the presented srb concept table 4 lists the srbs components used for the realized prototype of the srb and adjusted to a rain barrel with a storage size of 500 l as the statement of costs shows the electrical ball valve is the most expensive element of the srb in total the material costs are around 475 for one srb the low assembling costs are neglected resulting in material costs of 950 per m³ storage volume in series productions the costs are considered to be lower in contrast investment costs for a cso are on average between 600 and 3600 per m³ leimbach et al 2018 csos have a fixed location and volume possibly with space problems in urban areas and are used as a long term measure with a useful live in several decades in contrast to csos srbs allow an easy retrofitting of existing infrastructure where a high number of micro storages can be distributed over the catchment area of urban drainage systems as a result srbs enable a flexible design both in terms of space and volume and thus representing an ideal extension of the existing infrastructure against changing environmental challenges the previous simulations were carried out with one rain barrel per subcatchment but the flexible design also allows several rain barrels per subcatchments for example on different downpipes fig 11 shows the results of increased storage volume by adding two srbs units to the subcatchments as mentioned above the uncontrolled rain barrel represents the optimal value for rainwater harvesting providing already most of the irrigation demand therefore additional units cause only minor improvements on the other hand two srbs per subcatchment can clearly improve system performance regarding cso overflow and flooding however an increased storage volume does not automatically improve systems performance to the same extent which was also observed by barry and coombes 2015 besides ecological improvements reduced drinking water demand flooding and cso overflows a flexible design is more adaptable compared to standard approaches and can therefore improve economic lifecycle costs deng et al 2013 however a detailed life cycle assessment including ecological and economic considerations can be identified as future research item the srbs are intended for a multi actor partnership between network operators and property owner for example the network operator covers the costs for the smartness including electrical ball valve and control unit with lorawan antenna while the property owner carries out the installation and maintenance work as a result the network operators have the benefit of additional storage capacity in their system while the property owners can use their measurement data for an improved understanding of their system e g awareness raising irrigation demand and can even include their srb in smart home environment e g automatic irrigation based on soil moisture measurements as analysed by castonguay et al 2018 in a modelling approach applying only economic policies is not sufficient for an increase of rwh systems in addition increasing densification in urban areas and a lack of confidence in future funding from decision makers and the population are proving to be restrictive conditions for large scale retrofitting of urban areas with green infrastructure haaland and van den bosch 2015 thorne et al 2018 however veronesi et al 2014 stated that the willingness to pay increases if the population is aware of the climate change or to reduce ecological and health risks caused by csos however the srb concept are part of smart water city development smart cities are characterised through interactions of existing infrastructure new technologies and involving the public to increase sustainability ahvenniemi et al 2017 in this context the srb concept opens up future research topics for an increased involvement of the public 4 5 further discussion and outlook since the power supply of the srb is provided by a solar panel and a rechargeable battery a low power wide area network namely lorawan is used for the exchange of measurement data and also for control commands to open the discharge valve therefore the number of switching amounts should be in balance with the achievable improvement of system performance for example a higher number of switching operations corresponds to a more frequent emptying of the srbs and therefore more detention volume can be provided however a more frequent opening of the discharge valve leads also to a higher energy consumption which contrasts with the limited energy supply in addition a perfect control environment was used in this study assuming that the control commands are transmitted at any time and without interference to the srbs however in reality a downlink control command can only be sent after a successful uplink measurement value and packet loss is to be expected which depends on the connection quality and the number of devices in the lorawan network in addition lorawan operates within the public frequency range therefore the number of packages including the downlinks of the gateway is regulated currently a simple rule based control strategy is applied for the srbs whereat the discharge valve is controlled based on future inflows estimated by using weather forecasts as shown by the results an effective improvement of the overall system also requires an integration of the current system states to avoid for example additional peak discharge in the sewer system in this context future research will focus on finding the optimum for the control system by using machine learning techniques if 100 of the single family houses are equipped with srbs corresponds to approx 390 srbs the different possibilities of each configuration setup are increasing rapidly in combination with the chosen high temporal resolution the calculation time is not practicable anymore possible solutions to overcome this limitation is to investigate single rain events with a limited number of srbs or to combine several srbs into a joint control group e g via topological cluster analysis as the first trials show new approaches are required in order to operate the individual rain barrels as well as the entire system in the best way potentially also in combination with model predictive control in order to take these real boundary conditions into account and to further improve the performance of the srbs more advanced control strategies are necessary in future work including the general conditions of lorawan limitation of transmission packets consideration of losses in the control strategy coordinated emptying of the srbs based on the actual system states of the sewer system coordinated filling of the srbs with potable water taking system states of the water supply system into account integrating model predictive control of the entire system to further improve the overall system performance 5 conclusion in this work the smart rain barrel srb concept for advanced rainwater harvesting management is presented the srb consists of a conventional rain barrel available in normal hardware and improved by a water level measurement device and a remotely controlled discharge valve despite their small volume compared to the entire water infrastructure the srb concept offers the following advantages compared to conventional rainwater harvesting 1 using rain barrels with volumes between 200 l and 500 l supports a large scale retrofitting of existing infrastructure and 2 the development of the srb as an iot based solution provides the possibility that each srb can be individually monitored and controlled in real time and can also be integrated into the overall management of the urban water infrastructure in this work the open source simulation tool smartin was developed to analyse the impact of a large scale implementation of srbs on urban water infrastructure the software was generally designed for rtc micro storages with srbs being a special case due to the small storage volumes a high resolution spatial and temporal model input at household level was required the urban drainage system was simulated by applying the hydrodynamic swmm5 epanet2 was utilised to model the water supply system several open source python packages were used to operate the micro storages as iot based solutions and for the coupling of the two separate systems i e the urban drainage system and the water supply system the software was tested in an alpine municipality using an existing infrastructure and hypothetically equipped with srbs to determine the effects of different controls adding a large number of srbs can clearly improve the system performance although a simplified control strategy was applied e g simultaneous opening of the discharge valve and filling of the srbs for the urban drainage system flood and combined sewer overflow volume could be reduced by 40 and 14 respectively compared to the reference state without srbs in addition the security of water supply could be increased through substituting drinking water with rainwater and shifting the drinking water demand from peak hours to the night hours with lower water consumption however with increasing numbers of smart applications in the system there is increasing system complexity and simulation time is also increasing rapidly in addition a large scale implementation of micro storages developed as iot based solutions requires a joint consideration of smart applications and entire urban water infrastructure to avoid system deterioration e g create an artificial combined sewer overflow due to simultaneous opening of the discharge valves and to create added value compared to uncontrolled and conventional rainwater harvesting systems to avoid these problems a staggered emptying of the srbs is possible depending on the flow duration and an integration of the filling level of the combined sewer overflow into the control strategy in this way the best possible control strategy is influenced by the duration of the weather forecast period referring to the number of switching operations and the desired effect reduction of drinking water combined sewer overflows and flooding volumes in addition measurement values actual filling depth as well as control commands for opening the discharge valve are transmitted via lorawan low power radio network as lorawan is operating in public frequency range the number of the permitted data packages is regulated as a result the number of switching operations is limited for a real word implementation if all these factors are considered in the control strategy the control strategy becomes much more complicated and offers room for future research work eventually in combination with machine learning declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to thank chi hydro praxis for the disposal of pcswmm in the university grant program thanks also to the authors of the open source software python and the countless additional packages especially to the developers of pyswmm abbreviations arome application of research to operations at mesoscale c adj calibration parameter for hargreaves equation et c crop evapotranspiration gi green infrastructure f c i adaption factor for climate influences f m j adaptation factor for month i f h i adaption factor for hour i ifs integrated forecasting system inca integrated nowcasting through comprehensive analysis iot internet of things k c single crop coefficient lid low impact development lorawan long range wide area network r a water equivalent of extra terrestrial radiation rrain amount of rainfall for the chosen weather forecast period rtc real time control rwh rainwater harvesting srb smart rain barrel t irrigation irrigation time tm d daily mean temperature t max d daily maximum temperature t min d daily minimum temperature t closing i closing time for the drain valve for micro storage i t end day end of day tpeak time of peak intensity for chosen weather forecast period t sim d current simulation time drainage t sim w simulation time water supply t weather update time of the weather forecast v det i available detention volume of micro storage i for t sim d v det goal i emptying target volume for micro storage i v har i useable rainwater for irrigation purposes for micro storage i and day of t sim d v in i estimated inflow to micro storage i for chosen weather forecast period v ir i irrigation demand for micro storage i and day of t sim d v sto i used storage volume of micro storage i for t sim d v sto goal i closing target value for micro storage i for chosen weather forecast period v total i total available volume of micro storage i v ws i drinking water used for irrigation purposes for micro storage i and day of t sim d w b i base demand or base abstraction for node i w h i hourly water demand or source abstraction for node i αj i α3 i calibration factors for water demand calculation software and data availability name of software smart rainwater harvesting toolbox smartin developer unit of environmental engineering department of infrastructure engineering university of innsbruck austria contact information umwelttechnik uibk ac at year first available 2020 hardware required pc data set software required to execute the software detailed input files for swmm5 and epanet2 as well as high resolution weather and forecast data is needed availability and cost smartin is an open source software available on github https github com iut ibk smartin including a simplified demonstration model the case study of the alpine community cannot be made available because of data protection requirements for the high resolution models and the usage of address based billing consumption data program language python 3 7 64 bit program size 7 mb funding this publication was produced as part of the smart water city project this project is funded by the climate and energy fund and is part of the programme smart cities demo living urban innovation 2018 project 872123 
25861,low cost sensors are emerging alongside innovative data transfer technologies allowing the integration of smart solutions for the decentralisation of water infrastructure in this work the smart rain barrel srb concept is introduced as an iot based solution for advanced rainwater harvesting the srb consists of a rain barrel extended by a discharge valve which is centrally controlled this concept offers 1 individual control of each implemented srb with its incorporation into the integrated water system management and 2 a simple large scale implementation of additional storage units as an alternative to future expansion of existing infrastructure the open source software smartin is developed by hypothetically retrofitting an alpine municipality with srbs to evaluate the effects of the latter on the urban water infrastructure compared to uncontrolled rain barrels a simple coordinated control strategy already clearly improves the performance of the integrated system by reducing combined sewer overflow and addressing drinking water demand keywords green infrastructure iot based solution real time control smart rainwater harvesting urban water infrastructure weather forecasts 1 introduction climate change urbanisation and maintenance are present and future challenges for urban water infrastructure urbanisation and loss of infiltration capacity of natural areas increase peak runoff while reducing groundwater recharge fletcher et al 2013 mcgrane 2016 miller et al 2014 moreover climate change has modified the intensity duration and frequency of rain events berggren et al 2012 willems et al 2012 and has resulted in higher temperatures bastin et al 2019 thereby increasing the demand for irrigation water parkinson et al 2016 through the interaction between climate change and urbanisation the probability of overloading the existing urban drainage systems yazdanfar and sharma 2015 and the impacts on natural water resources cominola et al 2015 have increased to counteract this trend locally the existing infrastructure which mainly consists of central solutions has been extended in the last decades by decentralised system applications such as green infrastructure gi fletcher et al 2015 yazdanfar and sharma 2015 with the aim to store rainwater and to increase infiltration and evapotranspiration examples of gi are green roofs infiltration trenches rain gardens rainwater harvesting rwh systems whereat the focus of this article is on rwh systems the main objective of rwh is to retain rainwater runoff with minor impurities for example from roof surfaces in decentralised rainwater storage tanks to substitute drinking water used in non potable water applications such as irrigation and toilet flushing campisano and modica 2015 khastagir and jayasuriya 2010 okoye et al 2015 apart from a drinking water reduction rwh systems present a local source control option to reduce runoff into the drainage system due to the detention of precipitation campisano and modica 2016 imteaz et al 2011 therefore a large scale implementation of rwh systems offers advantages for operation of urban drainage systems too for example the flood volume as well as the caused damage can be reduced as shown by huang et al 2015 and jamali et al 2020 through a simulation approach with swmm5 however as can be seen from the results of single rwh systems the efficiency of water saving and stormwater detention is determined by factors such as storage volume roof area precipitation and water consumption fewkes and butler 2000 but also subject to seasonal fluctuations due to different withdrawal quantities and precipitation amounts during summer and winter months barry and coombes 2015 furthermore the conventional rwh system is strongly dependent on the withdrawal quantities and the corresponding user behaviour in terms of storm water management palla et al 2011 sample and liu 2014 higher withdrawal volumes also deplete the stored volumes more rapidly meaning that more storage volume is available for stormwater detention quinn et al 2020 as a result these systems are only conditionally suitable for the reduction of extreme rain events as they are likely to be already filled by the time that the rainfall peak occurs schubert et al 2017 vaes and berlamont 2001 developments in the field of smart cities have contributed to the evolution of the internet of things iot in the water sector low cost sensors are emerging along with innovative data transfer technologies which allow for the integration and networking of individual system components with the overall system li et al 2014 ray 2018 however these are rarely implemented in real time workflows of urban water systems li et al 2020 wong and kerkez 2016 but novel innovative solutions are increasingly emerging creaco et al 2019 kerkez et al 2016 moy de vitry et al 2019 for example the outflow of a storage unit can be changed by real time controlled valves allowing an effective use of the available storage volume by emptying the storage units prior to precipitation events xu et al 2018 demonstrated through a simulation approach that applying weather forecasts in combination with real time control rtc technology to rwh systems reduces the frequency of complete filling and thus prevents overflow as di matteo et al 2019 and liang et al 2019 showed in a simulation optimised process a coordinated control of individual smart rainwater tanks can also reduce the peak runoff in sewer systems even for rare rain events that occur over long durations with prior knowledge of future weather conditions roman et al 2017 integrated the irrigation requirements into a smart rwh system increasing the efficiency of both the drinking water conservation and the detention effect however the discharge of rainwater leads to a conflict between the two contradicting objectives namely 1 discharge the rainwater as fast as possible to provide the maximum storage capacity for the next event and 2 to store rainwater as long as possible for substituting as much drinking water as possible this was demonstrated by e g behzadian et al 2018 and zeisl et al 2018 using a conceptual model run on a daily basis previously described studies investigated smart rainwater storage tanks larger than 1 m³ the approach presented in this work uses smart rain barrels as a system wide implemented iot based solution with real time controlled micro storages between 200 l and 500 l just like conventional rain barrels the srb can also be used to retain rainwater for irrigation purposes while the benefits are evident at the local level large scale advantages potable water peak flow reduction of flooding volume or combined sewer overflow are usually limited to mitigate the disadvantages of the small storage size relative to the overall system new ways of operation are developed and implemented for example if an insufficient rainwater amount is forecasted to meet the estimated irrigation demands drinking water can be taken from the potable supply system during off peak hours to fill up the deficits on the other hand sufficient capacity can be provided for rain events by emptying the srb before heavy rain events therefore the srb is conceptualised as an iot based solution allowing for each srb to be controlled individually in real time to achieve these benefits weather forecasts with high temporal resolutions from a meteorological service are implemented into the control strategy as they can clearly increase the detention effects of single micro storages as shown by oberascher et al 2019 besides using weather forecasts for the operation of real time controlled systems has a positive effect on the performance of the entire system such as by reducing the combined sewer overflows courdent et al 2015 gaborit et al 2013 löwe et al 2016 due to the small storage size the srbs can be placed on the surface as a result the installation of srbs into the existing systems can be relatively easy thereby allowing for large scale retrofitting of the urban water infrastructure therefore the srb represents a successful iot based link between water supply and the urban drainage system the effectiveness of a single srb has already been proven through the operation of a prototype at the university of innsbruck and numerical simulations at the household level oberascher et al 2019 the aim of this paper is a model based upscaling to an entire case study to investigate the effects of the srb on the urban water infrastructure by a large scale implementation and the development of control strategies for multiple srbs in addition to the positive effects of rwh systems reduction of drinking water demand and stormwater runoff a reduction in consumption in the existing water infrastructure can also increase the risk of water stagnation and associated water quality problems in water supply networks and deposits in combined sewers grandet et al 2010 sitzenfrei et al 2013 2017 therefore an integrated modelling of urban drainage and water distribution network as well as water tanks on high temporal and spatial resolution is required to be able to fully investigate the effects of rwh systems recent research studies behzadian et al 2018 sitzenfrei et al 2017 that have followed this integrative approach are mainly based on conceptional models e g urbanbeats or watermet2 run on daily respectively hourly time steps in addition sitzenfrei and rauch 2014 investigated the effects of decentralised solutions on dry weather flows in combined sewer systems by coupling epanet2 and swmm5 applying an hourly time step too in contrast hydrological process of urban drainage systems e g combined sewer overflows take place in timesteps of seconds or minutes requiring a higher temporal resolution for an adequate modelling to evaluate the effects of the srbs on the urban water infrastructure the open source software smartin is developed and presented in this work the key features of smartin are 1 allowing individual control of each implemented srb based on weather forecasts and current system states 2 choosing between perfect and historical real weather forecasts 3 using a high spatial and temporal resolution for the necessary simulation of micro storages 4 coupling of urban drainage and water supply systems for high temporal resolutions and 5 developing and testing of integrated real time control mechanisms the software is written in python using several open source python packages for the simulation of the hydrodynamic model swmm5 gironás et al 2010 for the urban drainage system and the quasi stationary model epanet2 rossman 2000 for the water supply system to test the developed software we focus on an experimental retrofitting of an existing urban water infrastructure in an alpine municipality with srbs besides historical weather forecasts from the austrian national weather service are included into the control strategy additionally different control strategies are applied to analyse the impacts of a large scale implementation on the water supply e g substituted drinking water changes in water pressure and age and on the urban drainage system e g changes in combined sewer overflows and flooding the objectives of this work are proposing the smart rain barrel srb concept as an iot based solution for a networked water infrastructure development of a high resolution simulation tool which allows a large scale implementation of the srbs and the real time control of each individual rain barrel as well as a coordinated control in the context of the urban water infrastructure providing a proof of concept of the hypothetical retrofitting of existing infrastructure in a case study analysing the impacts of different control strategies including various weather forecast conditions on both the urban water supply and drainage system in an integrated model 2 the smart rain barrel concept as an iot based solution the concept presented in this work aims to compensate the disadvantages of micro storages used for rainwater harvesting e g small volume compared to the entire system depending on user behaviour by smart approaches the concept should be suitable for large scale implementation in new building areas as well as for retrofitting in existing urban areas this concept results in the following practical requirements for micro storages 1 ensuring simple implementation in already existing infrastructure 2 independent rtc of each micro storage due to different connection areas and user behaviours e g opening and closing of discharge valves depending on the current filling level and 3 energy efficiency in operation because batteries are needed for power supply e g limiting the numbers of control actions per day in order to meet these conditions the srb concept was developed as rtc micro storages the main part is a conventional rain barrel available in hardware stores and extended to an iot based solution as shown in fig 1 a rainwater collector is used to connect the rainwater downpipe of roof areas with the srb wherein the srb and rainwater collector act as connected vessels and therefore preventing srb overflow this is also the reason why no further emergency or backup strategy is needed for loss of power or data connection the srb has a manually controllable valve for personal water withdrawal and an automatically controllable valve at the bottom for irrigation and detention management the srb is equipped with a solar panel and a rechargeable buffer battery for power supply whereat the solar panel is able to recharge the battery with the sunshine duration on a clear summer day the srb is controlled centrally the measured values and control commands are sent via low power wide area technology lorawan lora alliance 2017 lorawan is an innovative data transfer technology for iot applications bardyn et al 2016 song et al 2017 it is characterised by low energy consumption and high transmission ranges and therefore particularly suitable for battery powered sensors in addition high resolution weather forecasts are used in the control strategy to increase the efficiency of the existing storage volume the prototype shown in fig 1 has been in operation at the university campus of innsbruck during the summer months since october 2019 in addition fig 2 illustrates a schematic cross section of the srb for more information the effects of a single srb for a single household have been investigated by oberascher et al 2019 as the results show the storm water detention volume of the srb can be increased by up to 300 compared to the uncontrolled rain barrel whereas 16 90 of the irrigation requirements can be satisfied by the srb depending on the storage volume the impact of the rtc is most significant if the rain barrel is partly or completely filled due to previous rain events i e when the dry weather period between two rainfall events is too short through the rtc the srb can be emptied before the next rain event to provide more volume for storm water retention than in the conventional rain barrel however emptying the srb depends on the predicted amount of precipitation if the predicted rain volume is less than the actual rain the srb would only be partially filled by the end of that rain event therefore water savings decrease with higher weather forecast periods due to uncertainties in precipitation forecasts fig 3 shows the idealised functionality of the srb concept for a real rainfall event in comparison with a conventional uncontrolled rain barrel both of which have a storage volume of 0 5 m³ both storage units have a connected impervious area roof area of 145 m2 for this exemplary case a perfect weather forecast with a forecast period of 2 h is chosen for the srb meaning that the forecasted amount of rain matches perfectly with the real rainfall event additionally it is assumed that future weather developments are updated every hour because of past rainfall events and subsequent irrigation activities both operating semantics are partially filled before the predicted rainfall event as can be seen from fig 3 in the example the rain events mostly take place between 13 15 and 13 30 with a total rainfall amount of 3 7 mm and a maximum intensity of 0 3 mm min resulting in a peak outflow of 0 74 l s from the sub catchment into the drainage system in the idealised example the first time of update is at 12 00 and future weather developments are recorded for the next 2 h the total inflow to the srbs is estimated by the simplified approach described by roof area discharge coefficient rainfall volume and was calculated to be 0 51 m³ because the estimated inflow to the srb was higher than the total storage volume the srb receives a control command to completely empty itself before the rain event as a result the discharge valve of the srb opens automatically and closes again at 12 17 when the srb is emptied this allows the entire storage volume of the srb to be used as a detention volume therefore almost the entire roof discharge can be captured by the srb and the peak outflow during the rain event is reduced by 80 in contrast to the srb the conventional rain barrel is already fully filled by the time the peak intensity is achieved meaning that the peak outflow into the drainage system cannot be reduced after the rainfall event both operating approaches are fully filled to provide as much rainwater for irrigation purposes as possible in contrast to oberascher et al 2019 who focused on one single srb applied at household scale this paper investigates a model based upscaling of srbs to an entire case study 3 methods in order to analyse the impacts of the presented srb concept on the urban water infrastructure the software smartin was developed smartin implements micro storages in a selected number of buildings each with one connection to the urban drainage as well as to the water supply system the micro storages are rtc allowing the discharge of rainwater before a precipitation event to provide additional storage volume if the micro storage cannot cover the irrigation demand with rainwater alone the micro storages are automatically filled with drinking water during hours with lower water consumption to satisfy the irrigation needs as a result smartin can be used for modelling of the rtc of micro storages developed as an iot based solution in a coupled model of urban drainage and water supply systems it was specifically designed for the use of any kind of micro storages to make the software generally applicable the srbs represent a special case of the micro storages and were used in the case study to test the software stormwater management model swmm5 gironás et al 2010 and epanet2 rossman 2000 are used to simulate urban drainage and water supply respectively coupling of the separated systems namely urban drainage water supply and micro storage volumes was achieved via python programming language and usage of several open source python packages 3 1 integrated modelling of micro storages as iot based solutions in the context of urban water infrastructure smartin the setup of smartin is illustrated in fig 4 it consists of two main components the urban drainage and the water supply model weather forecast rainfall data and current simulation results are individually used for the rtc of the micro storages during rain events during dry periods the stored rainwater is used for irrigation purposes and the irrigation demand is determined as a function of temperature and daily rainfall in case the stored rainwater cannot satisfy the irrigation demand drinking water from the water supply system is automatically extracted during off peak hours finally the amount of harvested rainwater and the demand for the drinking water are used as input parameters for the water supply simulation for the water supply assessment water demand is represented by a dynamic model consisting of seasonal temperature dependent and hourly factors see also 3 1 2 in the case of nodes with micro storages the water demand is reduced due to the harvested rainwater and the shift of water demand from the peak hours to night hours when there is lower demand for drinking water 3 1 1 urban drainage systems pyswmm swmm5 is a hydrodynamic model for urban drainage to numerically calculate water flows in sewers gironás et al 2010 the low impact development lid type rain barrel was chosen for the model representation of the micro storages the lid type rain barrel consists of a storage layer and a drain layer the storage layer provides the detention volume of the micro storage while the drain layer represents the outflow based on the chosen land designation and the degree of penetration properties are randomly selected as locations for micro storage volumes next a lid type rain barrel is created for each property to individually control each micro storage the selection of the rain barrel size depends on the roof area where a rain barrel size corresponding to a precipitation quantity of 6 mm was adopted as the reference value furthermore the lid type rain barrels are assigned to the respective properties in smartin pyswmm as a python wrapper for swmm5 mcdonnell et al 2020 is used for the simulations in contrast to native swmm5 pyswmm provides three key features which are essential for micro storages developed as iot based solutions 1 simulation of micro storages as well as changing parameters during the simulation 2 accessibility of results during simulation and 3 the ability to add control strategies at each routing step of swmm5 first swmm5 allows the implementation and simulation of micro storages at household level by using the lid type rain barrel however it is not possible to implement variable discharge heights or advanced control rules that allow joint control with the entire network moreover the network has only one drainage pathway which means that it is impossible to empty the micro storages for drainage and irrigation separately pyswmm is intended as the interface to swmm5 and thereby allows for the separate manipulation of each swmm5 element including lid parameters in addition some flow parameters e g inflows to nodes lid drain parameters etc can be modified during simulation as previously described the lid type rain barrel consists of a storage layer and a drain layer storage dimensions can only be set before simulation while drain values can be changed during simulation in the case of the micro storages the drain coefficient can be set either to zero no discharge or to a specific discharge value allowing the controlled opening or closing of the drain furthermore the outlet node of lids can be changed during simulations through pyswmm this has the advantage that two outlets can be used one for rainy weather as a connection to the sewer network and one outlet for irrigation purposes second pyswmm creates a coding interface to the binary output file in native swmm5 simulation results for lid elements are written into a separate text based file after simulation in contrast simulation results including lids can be accessed directly via pyswmm this functionality is used in smartin to query the current water depth in the various micro storages and filling levels in the drainage system in order to determine the current state of the system for example the drain is opened or closed based on the system states by setting drain coefficients finally pyswmm supports a step by step simulation of swmm5 input files this gives the possibility of implementing advanced control options after each simulation step for example weather forecasts can be included in control options due to the functions described above the drain of the micro storages can be controlled in real time by estimating the predicted inflow to the micro storage the valve can be opened to create detention space if necessary if the predicted volume of inflow is equal to the detention volume the valve is closed to achieve full filling of the micro storage during the rainfall and therefore the maximum possible volume for irrigation is provided 3 1 2 water supply system epanet2 is a simulation tool for the analysis of pressure and quality in systems under pressure such as the water supply system where the time dynamics are obtained by the sequence of quasi stationary states rossman 2000 the epanet programmer s toolkit is an extension to native epanet2 and allows the manipulation of the network as well as the operation parameters rossman 1999 this enables a long term simulation with different hourly extraction volumes of drinking water on a global scale and also allows the demand for each node to be changed individually as needed for the simulation of the micro storages fluctuations in drinking water consumption are considered based on seasonal climatic and hourly components gato et al 2007 zhou et al 2002 and determined by the following function 1 w h i w b i f m i f c i f h i where w h i is the hourly water demand or source abstraction l s w b i is the base demand or base abstraction l s f m i is the monthly adaptation factor f c i is the climate based adaption factor and f h i is the hourly adaption factor it was assumed that no hourly changes were to be expected in the drinking water sources thus f h i was set to 1 for the source pattern temperature has the greatest impact on regression models for water consumption neunteufel et al 2014 opalinski et al 2019 therefore the climate based adaption factor f c i is calculated using the regression formula 2 f c i α 0 i α 1 i t m d α 2 i t m d 2 α 3 i t m d 3 where α0 i α3 i are calibration factors and tm d is the daily mean temperature the epanet programmer s toolkit is utilised via python programming language using the python epanet toolkit provided by open water analytics https github com openwateranalytics epanet python tree dev epanet python epanet python 3 1 3 control strategies for urban drainage systems the control strategies for the micro storages implemented in this case study can be divided into dry and wet weather schemes during the rainy season the srbs are operated dynamically by means of rtc to reduce peak run off rates in the main sewer system and still ensure a fully filled storage unit at the end of rainfall for the dry period on the other hand the strategy is to use the stored water for irrigation purposes the workflow of the urban drainage simulation can be seen in fig 5 where the control strategies are divided according to the different durations of update for example control strategies for the rainy weather are run on an hourly basis depending on the chosen update time step of weather forecasts while the irrigation command is executed only once a day the description of control strategies assumes a partially filled micro storage and is defined as 3 v t o t a l i v s t o i v d e t i where v total i is the total available volume of the micro storage v sto i is the rainwater storage volume currently in use and v det i the detention volume as the remaining volume which can be used for rainwater detention as an initial assumption the drain is closed at the beginning of the simulations after the simulation starts the current simulation time t sim d is queried at each simulation step of swmm5 as recommended by en 16941 1 2018 the efficiency of rainwater harvesting systems should be investigated at the end of the selected time step it was assumed that irrigation takes place once a day and therefore 23 00 was chosen as the automatic irrigation time t irrigation if t sim is greater than or equal to t irrigation crop evaporation was calculated as a reference value for the irrigation quantity by following the recommendations of the food and agriculture organization of the united nations allan et al 1998 it is assumed that the irrigation area is well watered and thereby provides the ideal agronomic conditions based on this assumption crop evapotranspiration et c mm day is used to compute the water demand evapotranspiration is estimated by using the hargreaves equation requiring only temperature data as the input variable hargreaves and samani 1985 and an alternative when wind speed humidity and solar radiation data are absent allan et al 1998 additionally it is assumed that the growing season is characterised by a constant water demand considering these simplifications the daily et c is calculated as 4 e t c c a d j t m d 17 78 t m a x d t min d r a k c where c adj is a calibration parameter for the hargreaves equation t m d t max d and t min d are the average maximum and minimum daily temperatures c respectively r a is the water equivalent of extra terrestrial radiation mm d and k c is the single crop coefficient in the software temperature measurements are used for the temperature parameters c adj haslinger and bartsch 2016 and k c allan et al 1998 simonne et al 2007 are extracted from literature while r a is calculated according to the formulas given by allan et al 1998 in the next step irrigation demand v ir i is determined individually for each micro storage by using crop evaporation the daily amount of rainfall and irrigation area and comparing with the available storage volume if the micro storage is empty v sto i 0 no rainwater can be extracted for irrigation purposes and the entire irrigation volume is taken from the drinking water supply if the micro storage is filled v sto i 0 the irrigation valve is opened and the amount of rainwater useable for irrigation purposes is calculated v har i in case the stored rainwater cannot satisfy irrigation requirements drinking water is used for the remaining irrigation requirements the required amount of drinking water v ws i is calculated as v ws i v ir i v har i finally a closing target value v sto goal i is set corresponding to the amount of rainwater provided by the micro unit and determined through v sto goal i min v sto i v ir i 0 furthermore v har i as well v ws i per day is saved and used as the input parameter for the water supply simulation in the last step the t irrigation for the next day is calculated by adding a day to the current t irrigation if t sim is equal or greater than update time of the weather forecast t weather control strategies for rainy weather are executed the key element here is the implementation of weather forecasts as a result the future rainfall expectations the amount of rainfall rrain and time of peak intensity tpeak are known for the chosen period of forecast referred as accumulation time in the subsequent sections next the inflow to each micro storage v in i is estimated based on the forecasted precipitation and the roof area of the property and compared with the available volume for rain detention vdet i subsequently three different control strategies can be distinguished for the case of rainy weather in the first control strategy the inflow v in i is lower than the available storage v det i since the full filling of the micro storage is not achieved no further control action is needed if v in i greater than v det i the drain valve is opened and the micro storage starts emptying in the 2nd control strategy v in i is lower than v total i therefore an emptying target v det goal i is specified to retain the entire precipitation in the micro storage and to guarantee a fully filled micro storage at the end of the precipitation event for irrigation purposes however if v in i is greater than v total i the 3rd control strategy is applied the exact time of maximum intensity tpeak is determined and used as the closing time t closing i for the drain valve to reduce peak run off in the sewer system finally next t weather is calculated by adding the update time step of the weather forecast to the current t weather the prototype of the srb was developed as an iot based solution using lorawan to exchange measurement values and control commands however lorawan transmits on the public frequency range which limits the number of data exchanges lora alliance 2017 e g the duty cycle influences the number of data packets and the packet size this approach has also been applied to the simulation assuming that only one control command e g open drain valve until a specific filling depth or time is attained is allowed per weather forecast update time 3 1 4 control strategies for the water supply system the workflow process of the water supply simulation is that first the hydraulic solver is started as shown in fig 6 in smartin the hourly patterns for the next day are set at the end of the day during the simulation therefore the simulation time t sim w is queried for each of the simulation time steps if t sim w is greater than the end of day t end day the source and global demand patterns are calculated using eqs 1 and 2 first the demand pattern is set globally for all nodes then the patterns for all micro storages along with the respective connection nodes to the water supply are changed individually using the simulation results on the used irrigation rainwater v har i and needed drinking water v ws i from the urban drainage model if drinking water is required for irrigation v ws i 0 the micro storages are filled automatically with drinking water this allows control of the filling time and a shift in water demand from peak hours to hours with lower consumption e g at night the filling is set to start at 22 00 so that the micro storage can provide the irrigation requirements at 23 00 and the pattern value can be increased by the amount of needed drinking water however v har i and v ws i mean saving of drinking water and therefore a reduction during peak hours it is assumed that peak hours occur at 17 00 to 18 00 and that 40 of the water requirement during the peak hour is allocated to irrigation therefore a maximum of 40 of the water demand requirements during peak hours can be substituted by the micro storages by providing rainwater for irrigation purposes 3 1 5 input parameters and requirements for smartin to execute the software smartin detailed input files for swmm5 and epanet2 are required at property level therefore each property has to be divided into the following subareas house access road and green space each subarea should be assigned an independent sub catchment in the swmm5 input file additionally information is needed for each property regarding the connection point to the water supply system the subcatchments for the implementation of the micro storages are selected by their name in the swmm5 input file where the names of the subcatchments should contain the land classification as well as the land usage in the form land classification land usage for example the name for a house subcatchment h in the land classification residential area r should include the following form h r the implementation of smartin was accomplished by three main python functions whereat the user refers the run sim function as the main application this function takes the 12 user inputs listed in table 1 the input parameters allow the user to set installation locations of the micro storage e g degree of penetration land classification control options e g uncontrolled or smart as well as to specify weather forecast parameters e g perfect or real accumulation period update time step the run sim function starts the sim drainage function first and when this function is completed the sim supply function is executed afterwards 3 2 case study for testing the software the functionality of smartin and the effects of srbs on the urban water infrastructure were tested using a case study of an alpine municipality in austria for the case study existing infrastructure data of water supply and urban drainage system as well as personalized water consumption data were applied therefore the case study will be referred to anonyms in the following the municipality has about 2900 inhabitants and approximately 630 properties and an area of 15 2 ha is connected to the urban drainage system fig 7 a shows the land classification for each property in the municipality the land classifications residential area mixed use area and agricultural use were selected as the installation locations for the srbs because buildings in these areas have both green areas for irrigation and space for installation in total 384 properties with a roof area of 8 16 ha can be used for the implementation of the srbs which corresponds to a degree of penetration of 100 in addition the case study area was divided into house traffic and green areas fig 7 b for the area determination for the swmm model the software is intended to be used for rtc applications to further investigate the quality and uncertainties of weather forecasts a complete data set consisting of historical forecasts with different forecasting periods as well as the real measure data for the year 2015 is used uncontrolled hence referred to as conventional rain barrels are only emptied when the stored rainwater is exhausted for irrigation purposes however in austria irrigation mainly occurs during the summer half year 21 3 23 9 neunteufel et al 2014 as a consequence outside this period no rainwater is taken and thereby no detention storage can be provided by the conventional rain barrels however the srbs can also be used in other seasons besides summer half year because their process of emptying for detention is automated to make the results comparable only the summer half year was considered for the srbs rainfall data available at 1 min intervals from a nearby weather station were used for the investigations according to the classification of peel et al 2007 the case study is located within the dfb d cold climate f without dry season and b warm summer climate zone the en 16941 1 2018 recommends a rain series of a minimum of 5 years for effectiveness analyses of rwh systems however due to the long simulation times we decided to test the software only for the summer half year 2015 the precipitation sum for the summer half year 2015 was 651 mm and thus on average over the years 2015 2019 30 mm with the maximum precipitation in may 152 mm 3 2 1 urban drainage system the municipality has a combined sewer system with flow direction from southwest to northeast as shown in fig 7 e the case study has an oversized combined sewer overflow structure at the catchment outlet in the north east volume of 350 m³ throttle discharge 50 l s to better outline the impact of the state of the art capacities it was re dimensioned according to the actual austrian technical standard rb 19 2007 volume of 154 m³ throttle discharge 22 2 l s together with real information about the capacities of the sewer system and the results of fig 7 a and b a detailed swmm5 input file on property level was created in a semi automatic process rain data two level measurements and one flow measurement at different points of the sewer systems from a measurement campaign between june and september 2017 were used for model calibration and validation with pcswmm chi in the simulated summer half year 2015 the flood volume of stormwater discharging from the sewer manholes was 285 m³ and the combined sewer overflow volume was 54 950 m³ 3 2 2 water supply system for the water supply system the calibrated model of sitzenfrei and rauch 2015 was employed the system is fully gravity driven with an elevated water reservoir tank volume of 1440 m³ in the southwest of the case study area the model was extended by connection of additional households to develop a detailed model at household level as shown in fig 7 d in addition historical node water consumption was updated by an address based billing for the years 2017 and 2018 in total an average of 9 4 l s 810 m³ d water was extracted from the system in these two years including water losses and public fountains for the calculation of dynamic water consumption the monthly and hourly adaption factors were adapted from sitzenfrei and rauch 2015 while the climate based adaption factor was determined using the measurements of the drinking water quantity supplied to the system for the years 2008 2012 as shown for the austrian daily water demand neunteufel et al 2012 water consumption is increasing with increased length of the dry periods eq 2 was therefore converted to a combined model of temperature and dry period and divided into four different classes days with precipitation 1 3 3 7 and greater than 7 days without precipitation 3 2 3 weather forecasts weather forecasts are one of the key elements in the control strategy of the srbs perfect i e observed weather as well as real historical weather forecasts from the integrated nowcasting through comprehensive analysis inca system haiden et al 2011 were integrated into the control strategy for the consideration of the future weather development the inca system is designed to analyse weather developments and for nowcasting different weather parameters in the mountain terrain it uses grid cells with a resolution of 1 km thereby allowing the rapidly changing topography of the alps to be represented in numerical weather forecast models first a combined analysis of rain gauge observations and weather radar is performed every 15 min and transferred to the numerical grid using a series of these analyses the precipitation movements are kinematically extrapolated to forecast the field of future precipitation as the prediction quality decreases with increasing duration due to the kinematic extrapolation the application of research to operations at mesoscale arome high resolution model grid resolution 2 5 km wittmann and meier 2016 and the integrated forecasting system ifs model grid resolution 9 km malardel et al 2016 are scaled to a resolution of 1 km and stepwise merged with the inca system to get a smooth transition up to 48 h 3 2 4 srb parameters for the theoretical retrofitting of the case study with srbs rain barrels available in normal hardware stores with storage volumes of 200 l 300 l and 500 l were selected likewise available ball valves were chosen as discharge valves with real diameters which guarantees approximately 30 min emptying time for the rain barrels in terms of model implementation the micro storages consist of a storage layer and a drain layer the characteristics of the rain barrels with chosen ball valves can be seen in table 2 showing the swmm input parameters for both layers with light grey background the height and area of the storage layer is given by the dimensions of the rain barrels as recommended by rossman 2015 the drain exponent is set to 0 5 while the drain coefficient is calculated as 60 000 avalve astorage where avalve is the area of the ball valve and astorage the area of the rain barrel if the real green area is used as the irrigation area the srbs are normally emptied after one irrigation process to determine the impact on the performance of the srbs a reduced irrigation area of 25 m2 was assumed the garden area is automatically irrigated and the irrigation amount complements the evapotranspiration the evapotranspiration is calculated according to eq 4 whereat the monthly calibration parameter c adj for the alpine terrain was taken from haslinger and bartsch 2016 therefore an ideal irrigation model was applied and the irrigation demand is independent of the real irrigation needs of the properties 3 2 5 trials table 3 shows the configuration parameters for the simulations the degree of penetration fraction of srb installed to total number possible was modified for both smart and uncontrolled operation types while the weather forecast parameters were only applied to the srb concept as degrees of penetration 25 50 75 and 100 were applied in addition perfect and real weather forecasts were used to compare the ideal system performance perfect weather forecasts with the real effects therefore three different accumulation periods of between 1 h and 12 h were examined in more detail both the srbs and the conventional rain barrels are implemented randomly in smartin in order to make the results as independent from the implementation location as possible each configuration was simulated multiple times however as the simulation results per configuration showed only minor differences 10 repetitions were chosen except in the case of 100 degree of penetration as there is only one possibility in total 217 smartin runs were carried out to investigate the effects on the urban water infrastructure in order to be able to model the discharge process in the sewer network as accurately as possible and to obtain a damping of the peak runoff rate due to increasing flow time e g as a relevant wave for the combined sewer overflows hydrodynamic modelling of the storm water system is required which results in the simulation steps in seconds in addition each srb is controlled in real time meaning that control actions are executed at every simulation step e g each simulation step the individual filling depth is queried the combination of srbs and hydrodynamic modelling greatly increases the simulation time ranging from 30 h 25 penetration rate to 100 h 100 penetration rate in contrast to the urban drainage model the water supply system is calculated quasi stationary with simulation times in minutes 4 results and discussion figs 8 10 illustrate the performance analysis of a large scale implementation of the srbs compared to the reference state without srbs and to uncontrolled rain barrels 4 1 performance of smart rain barrels in average a storage volume of 468 l was added to each property resulting in an additional storage volume of between 45 m³ 25 penetration rate and 182 m³ 100 penetration rate as can be seen in fig 8 a the dispersions between the random implementations of the additional storage volume are small in addition the results of each configuration scatter only slightly therefore the mean value of each configuration set is selected to illustrate the effects of a large scale implementation of the srbs in the following graphs due to the usage of harvested rainwater for irrigation purposes the conserved drinking water ranges from 5 6 m³ to 9 4 m³ per srb for the summer half year 2015 upscaling to the case study means the conservation of between 533 m³ and 3490 m³ of drinking water depending on the degree of penetration as shown in fig 8 c and corresponds to approximately 65 respectively 430 of daily water consumption of all properties in the case study interestingly the uncontrolled rain barrel represents the optimum value for rainwater harvesting the inflow to the srb is estimated using the weather forecasts and based on this estimation the srb is emptied to provide additional detention volume if more precipitation is predicted than what actually falls the srb is not completely filled at the end of the rain event and thereby the amount of substituted drinking water is reduced therefore a perfect weather forecast delivers almost the same results as the uncontrolled rain barrel the small differences with increasing accumulation period meteorological term for prediction horizon result from the control strategy for rain events which does not take the irrigation requirements into account however water savings are clearly decreased with increasing accumulation period for the real weather forecast compared to the uncontrolled rain barrel due to increased uncertainty the total detention volume for the uncontrolled rain barrel is illustrated in fig 8 d and is between 976 m³ and 3926 m³ depending on the degree of penetration respectively 10 2 m³ per uncontrolled rain barrel in comparison the detention volume of the srbs is significantly increased by adding weather forecasts to the control strategy a penetration rate of 25 of the srbs controlled with a real weather forecast with an accumulation time of 12 h could retain 4089 m³ rainwater which is more than a 100 penetration of uncontrolled rain barrels depending on the degree of penetration a total of 4089 m³ to 29 252 m³ of precipitation could be temporarily retained in addition a difference between the accumulation times i e the chosen weather forecast period is noticeable at shorter accumulation times the control steps or switching operations are at shorter intervals allowing a more frequent emptying of the srbs whereas the highest amount of precipitation could be retained with an accumulation time of 1 h therefore the detention volume decreases with increased accumulation time besides the deviations in total detention volume between the real weather forecast and the perfect weather forecast as the theoretical maximum of the total retention volume are low ranging between 14 for an accumulation time of 1 h and 9 for an accumulation time of 12 h the prototype of the srbs is powered by a rechargeable battery and solar panel meaning that an additional power supply is not needed which supports its easy retrofitting to urban areas therefore an efficient and energy saving operation which is defined by the number of switching operations is needed fig 8 b illustrates the total number of switching operations ranging between 10 790 and 325 550 however the number of switching operations of a single srb is independent of the degree of penetration ranging from 112 accumulation time of 12 h real weather forecast to 850 accumulation time of 1 h perfect weather forecast emptying processes for providing additional storage volume during the summer half year 2015 the number of switching operations is mainly influenced by the accumulation time shorter accumulation times evidently increase the number of switching operations additionally applying real weather forecasts reduces the number of switching operations of the srbs due to an overestimation of the predicted rain amount compared to perfect weather forecasts 4 2 impact on urban drainage system the srbs are used for the detention of rainwater thereby influencing drainage processes in the sewer system e g combined sewer overflow flooding fig 9 a shows a flood volume of 285 m³ from the urban drainage system for the reference state during the summer half year 2015 as can be seen adding weather forecasts to the control strategy reduces the flood volume more significantly compared to the reference state as well as to uncontrolled rain barrels in total the srbs can reduce the flood volume by between 7 m³ 3 and 115 m³ 40 with a clear difference between the accumulation times as well as perfect and real weather forecasts because only one control command is allowed per update time step of the weather forecasts a shorter accumulation time allows therefore more frequent emptying operations as a result shorter accumulation times 1 h 4 h decrease flood volumes better than higher accumulation times 12 h interestingly the difference between real weather forecasts with an accumulation time of 12 h and the uncontrolled rain barrels is small in addition the difference between perfect forecasts as the best possible performance and real forecasts increases with increasing accumulation time as uncertainties of the real weather forecast are not considered in the control strategy similar to the flood volume the srbs can also reduce the combined sewer overflow volume relative to the reference state as demonstrated in fig 9 b the combined sewer overflow volume is 54 950 m³ for the reference state and can be reduced by 580 m³ 1 to 7670 m³ 14 with the srbs however as the results show a large scale implementation of srbs does not necessarily mean an improvement over uncontrolled rain barrels the srbs influence the combined sewer overflow volume in two ways first they provide additional detention volume and thereby reduce the amount of discharged combined sewer overflow volume second the discharge valves are opened simultaneously at all srbs creating an additional discharge wave in the sewer system for example at a 25 penetration rate of the srbs the additional peak discharge in the sewer system due to the emptying of the srbs can be up to 25 l s which is as high as the throttle discharge to the wastewater treatment plant 22 2 l s if the conditions are unfavourable e g partially filled combined sewer overflow tank due to light rain event an additional overflow event could occur therefore worsening the system performance this effect is more common with shorter accumulation times 1 h and 4 h and frequent emptying phases while for higher accumulation times 12 h the detention capacity dominates nevertheless as the results for the perfect weather forecasts show even short accumulation times and a simultaneous emptying of the srbs can clearly reduce the combined sewer overflow volume under ideal conditions perfect control 4 3 impact on the water supply system the srbs have an impact on the water supply system by reducing the drinking water demand as rainwater is provided for irrigation purposes the case study shows typical water consumption for smaller municipalities with peak hours in the evening and low water consumption at night one of the reasons for peak hours is the high external water consumption including in uses such as garden irrigation in contrast to the uncontrolled rain barrels the irrigation process of the srbs is automatic this means that the srbs can be filled during hours with low demand for drinking water supporting a shift in water consumption therefore a large scale implementation of the srbs in the case study allows a reduction of these peak hours by substituting rainwater for drinking water and shifting irrigation water consumption from the peak hour to the night hours with lower water consumption however the effects on the water pressure at peak hours are low as can be seen in fig 10 b and e maximum water pressure increases are between 0 4 m for a 25 penetration rate and 1 3 m for a 100 penetration rate one of the reasons for this is that the srbs were predominantly implemented in single family houses while neglecting large water consumers from multi party buildings trade or agriculture although the single family houses represent 60 of the buildings they are only responsible for about 20 of the water consumption moreover it was assumed that irrigation accounts for 40 of the peak demand if all these factors are considered only a maximum of 8 of the peak demand can be influenced by the srbs which explains the minor increase in water pressure similar to the strategy for rainy weather the additional water demand for irrigation is satisfied with drinking water from the water supply system by filling the srbs automatically at the same time between 22 00 and 23 00 due to the large scale implementation a clear reduction in the water pressure at the filling time is noticeable as illustrated in fig 10 a und d for a 25 penetration rate the maximum reduction of water pressure is 1 08 m while for a 100 penetration rate the water pressure is reduced by 5 4 m in addition the pressure reduction is increasing with the increase in the distance to the elevated tank however a simplified model with a garden area of 25 m2 and compensation of the evapotranspiration was used to calculate the irrigation demand if potable water is needed for irrigation all srbs are simultaneously filled with drinking water from the water supply system because the drinking water is automatically drawn during the night hours with low consumption the effects under normal operation are minimal however the simultaneous withdrawal can lead to problems in the event of unforeseen events such as the extraction of fire water therefore the extraction of drinking water for use in irrigation purposes should be in coordination with the water supply performance fig 10 c und f show the average change of water age for a 25 and a 100 penetration rate of the srbs respectively due to the large scale implementation of the srbs the water age for all nodes in the water supply system is reduced by a maximum of 0 4 h it is noticeable that the improvement in water age is achieved mainly at the end nodes or end strands of the water supply system while nodes in the meshed system show only minor changes the improvements in water age are achieved by the fact that the automatic irrigation system needs more drinking water for irrigation suggesting that the srbs could also be used as a local measure to improve the quality of the drinking water in areas with a high water age 4 4 implementation strategy for the presented srb concept table 4 lists the srbs components used for the realized prototype of the srb and adjusted to a rain barrel with a storage size of 500 l as the statement of costs shows the electrical ball valve is the most expensive element of the srb in total the material costs are around 475 for one srb the low assembling costs are neglected resulting in material costs of 950 per m³ storage volume in series productions the costs are considered to be lower in contrast investment costs for a cso are on average between 600 and 3600 per m³ leimbach et al 2018 csos have a fixed location and volume possibly with space problems in urban areas and are used as a long term measure with a useful live in several decades in contrast to csos srbs allow an easy retrofitting of existing infrastructure where a high number of micro storages can be distributed over the catchment area of urban drainage systems as a result srbs enable a flexible design both in terms of space and volume and thus representing an ideal extension of the existing infrastructure against changing environmental challenges the previous simulations were carried out with one rain barrel per subcatchment but the flexible design also allows several rain barrels per subcatchments for example on different downpipes fig 11 shows the results of increased storage volume by adding two srbs units to the subcatchments as mentioned above the uncontrolled rain barrel represents the optimal value for rainwater harvesting providing already most of the irrigation demand therefore additional units cause only minor improvements on the other hand two srbs per subcatchment can clearly improve system performance regarding cso overflow and flooding however an increased storage volume does not automatically improve systems performance to the same extent which was also observed by barry and coombes 2015 besides ecological improvements reduced drinking water demand flooding and cso overflows a flexible design is more adaptable compared to standard approaches and can therefore improve economic lifecycle costs deng et al 2013 however a detailed life cycle assessment including ecological and economic considerations can be identified as future research item the srbs are intended for a multi actor partnership between network operators and property owner for example the network operator covers the costs for the smartness including electrical ball valve and control unit with lorawan antenna while the property owner carries out the installation and maintenance work as a result the network operators have the benefit of additional storage capacity in their system while the property owners can use their measurement data for an improved understanding of their system e g awareness raising irrigation demand and can even include their srb in smart home environment e g automatic irrigation based on soil moisture measurements as analysed by castonguay et al 2018 in a modelling approach applying only economic policies is not sufficient for an increase of rwh systems in addition increasing densification in urban areas and a lack of confidence in future funding from decision makers and the population are proving to be restrictive conditions for large scale retrofitting of urban areas with green infrastructure haaland and van den bosch 2015 thorne et al 2018 however veronesi et al 2014 stated that the willingness to pay increases if the population is aware of the climate change or to reduce ecological and health risks caused by csos however the srb concept are part of smart water city development smart cities are characterised through interactions of existing infrastructure new technologies and involving the public to increase sustainability ahvenniemi et al 2017 in this context the srb concept opens up future research topics for an increased involvement of the public 4 5 further discussion and outlook since the power supply of the srb is provided by a solar panel and a rechargeable battery a low power wide area network namely lorawan is used for the exchange of measurement data and also for control commands to open the discharge valve therefore the number of switching amounts should be in balance with the achievable improvement of system performance for example a higher number of switching operations corresponds to a more frequent emptying of the srbs and therefore more detention volume can be provided however a more frequent opening of the discharge valve leads also to a higher energy consumption which contrasts with the limited energy supply in addition a perfect control environment was used in this study assuming that the control commands are transmitted at any time and without interference to the srbs however in reality a downlink control command can only be sent after a successful uplink measurement value and packet loss is to be expected which depends on the connection quality and the number of devices in the lorawan network in addition lorawan operates within the public frequency range therefore the number of packages including the downlinks of the gateway is regulated currently a simple rule based control strategy is applied for the srbs whereat the discharge valve is controlled based on future inflows estimated by using weather forecasts as shown by the results an effective improvement of the overall system also requires an integration of the current system states to avoid for example additional peak discharge in the sewer system in this context future research will focus on finding the optimum for the control system by using machine learning techniques if 100 of the single family houses are equipped with srbs corresponds to approx 390 srbs the different possibilities of each configuration setup are increasing rapidly in combination with the chosen high temporal resolution the calculation time is not practicable anymore possible solutions to overcome this limitation is to investigate single rain events with a limited number of srbs or to combine several srbs into a joint control group e g via topological cluster analysis as the first trials show new approaches are required in order to operate the individual rain barrels as well as the entire system in the best way potentially also in combination with model predictive control in order to take these real boundary conditions into account and to further improve the performance of the srbs more advanced control strategies are necessary in future work including the general conditions of lorawan limitation of transmission packets consideration of losses in the control strategy coordinated emptying of the srbs based on the actual system states of the sewer system coordinated filling of the srbs with potable water taking system states of the water supply system into account integrating model predictive control of the entire system to further improve the overall system performance 5 conclusion in this work the smart rain barrel srb concept for advanced rainwater harvesting management is presented the srb consists of a conventional rain barrel available in normal hardware and improved by a water level measurement device and a remotely controlled discharge valve despite their small volume compared to the entire water infrastructure the srb concept offers the following advantages compared to conventional rainwater harvesting 1 using rain barrels with volumes between 200 l and 500 l supports a large scale retrofitting of existing infrastructure and 2 the development of the srb as an iot based solution provides the possibility that each srb can be individually monitored and controlled in real time and can also be integrated into the overall management of the urban water infrastructure in this work the open source simulation tool smartin was developed to analyse the impact of a large scale implementation of srbs on urban water infrastructure the software was generally designed for rtc micro storages with srbs being a special case due to the small storage volumes a high resolution spatial and temporal model input at household level was required the urban drainage system was simulated by applying the hydrodynamic swmm5 epanet2 was utilised to model the water supply system several open source python packages were used to operate the micro storages as iot based solutions and for the coupling of the two separate systems i e the urban drainage system and the water supply system the software was tested in an alpine municipality using an existing infrastructure and hypothetically equipped with srbs to determine the effects of different controls adding a large number of srbs can clearly improve the system performance although a simplified control strategy was applied e g simultaneous opening of the discharge valve and filling of the srbs for the urban drainage system flood and combined sewer overflow volume could be reduced by 40 and 14 respectively compared to the reference state without srbs in addition the security of water supply could be increased through substituting drinking water with rainwater and shifting the drinking water demand from peak hours to the night hours with lower water consumption however with increasing numbers of smart applications in the system there is increasing system complexity and simulation time is also increasing rapidly in addition a large scale implementation of micro storages developed as iot based solutions requires a joint consideration of smart applications and entire urban water infrastructure to avoid system deterioration e g create an artificial combined sewer overflow due to simultaneous opening of the discharge valves and to create added value compared to uncontrolled and conventional rainwater harvesting systems to avoid these problems a staggered emptying of the srbs is possible depending on the flow duration and an integration of the filling level of the combined sewer overflow into the control strategy in this way the best possible control strategy is influenced by the duration of the weather forecast period referring to the number of switching operations and the desired effect reduction of drinking water combined sewer overflows and flooding volumes in addition measurement values actual filling depth as well as control commands for opening the discharge valve are transmitted via lorawan low power radio network as lorawan is operating in public frequency range the number of the permitted data packages is regulated as a result the number of switching operations is limited for a real word implementation if all these factors are considered in the control strategy the control strategy becomes much more complicated and offers room for future research work eventually in combination with machine learning declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to thank chi hydro praxis for the disposal of pcswmm in the university grant program thanks also to the authors of the open source software python and the countless additional packages especially to the developers of pyswmm abbreviations arome application of research to operations at mesoscale c adj calibration parameter for hargreaves equation et c crop evapotranspiration gi green infrastructure f c i adaption factor for climate influences f m j adaptation factor for month i f h i adaption factor for hour i ifs integrated forecasting system inca integrated nowcasting through comprehensive analysis iot internet of things k c single crop coefficient lid low impact development lorawan long range wide area network r a water equivalent of extra terrestrial radiation rrain amount of rainfall for the chosen weather forecast period rtc real time control rwh rainwater harvesting srb smart rain barrel t irrigation irrigation time tm d daily mean temperature t max d daily maximum temperature t min d daily minimum temperature t closing i closing time for the drain valve for micro storage i t end day end of day tpeak time of peak intensity for chosen weather forecast period t sim d current simulation time drainage t sim w simulation time water supply t weather update time of the weather forecast v det i available detention volume of micro storage i for t sim d v det goal i emptying target volume for micro storage i v har i useable rainwater for irrigation purposes for micro storage i and day of t sim d v in i estimated inflow to micro storage i for chosen weather forecast period v ir i irrigation demand for micro storage i and day of t sim d v sto i used storage volume of micro storage i for t sim d v sto goal i closing target value for micro storage i for chosen weather forecast period v total i total available volume of micro storage i v ws i drinking water used for irrigation purposes for micro storage i and day of t sim d w b i base demand or base abstraction for node i w h i hourly water demand or source abstraction for node i αj i α3 i calibration factors for water demand calculation software and data availability name of software smart rainwater harvesting toolbox smartin developer unit of environmental engineering department of infrastructure engineering university of innsbruck austria contact information umwelttechnik uibk ac at year first available 2020 hardware required pc data set software required to execute the software detailed input files for swmm5 and epanet2 as well as high resolution weather and forecast data is needed availability and cost smartin is an open source software available on github https github com iut ibk smartin including a simplified demonstration model the case study of the alpine community cannot be made available because of data protection requirements for the high resolution models and the usage of address based billing consumption data program language python 3 7 64 bit program size 7 mb funding this publication was produced as part of the smart water city project this project is funded by the climate and energy fund and is part of the programme smart cities demo living urban innovation 2018 project 872123 
25862,distributed hydrological models predict the spatial variability in processes that govern observed mass and energy fluxes a challenge associated with the use of these models is the computational burden associated with representing the earth s sub surface via millions of computational elements this burden is exacerbated as more complex process representations are included because their parameterizations involve computationally intensive mathematical functions lookup tables luts approximate a mathematical function by interpolating precomputed values of the function highly accurate approximations are possible for substantially reduced computational costs in this work a general methodology using the c lut library func is applied to identify and replace computationally intensive mathematical function evaluations in the canadian hydrological model chm the use of luts introduces a pointwise relative error below 10 8 and provides a reduction in run time of almost 20 this work shows how luts can be implemented with relatively little pain and yield significant computational savings for distributed hydrological models keywords hydrology optimization lookup table snow cold regions 1 introduction computer simulation is an increasingly important tool to address both research and operational water resource issues related to the hydrological cycle these issues include improving our understanding of complex interactions affecting streamflow helping to inform water management decisions quantifying the effects of changes in climate and landcover determining the changing frequency of extreme events predicting flood impacts and designing future infrastructure freeze and harlan 1969 mote et al 2005 milly et al 2008 nazemi et al 2013 debeer et al 2015 wheater 2015 musselman et al 2017 distributed hydrological models represent the surface of the earth using computational elements defined on regular or irregular grids e g rasters or triangles marsh et al 2020b the use of distributed models can provide key insights into the spatial variability of hydrological processes that govern the spatial heterogeneity in mass and energy fluxes dornes et al 2008 fatichi et al 2016 kumar et al 2013 however as the spatial extents investigated increase or the spatial scales of the computational elements decrease there are limitations to the practicality of distributed models partly due to computational costs associated with the simulations certainly there are open questions regarding equifinality beven 1993 parameter estimation beven 1993 savenije 2009 initial and boundary conditions and uncertainty in distributed hydrological models however model complexity and structure decisions are often influenced by computational cost fatichi et al 2016 the use of distributed hydrological models in practice is particularly challenging in the complex topography of mountainous regions in these water towers of the world viviroli et al 2007 seasonal snowpacks store a significant portion of the annual precipitation the melting of these spatially variable snowpacks generates the spring freshet that can result in the largest discharge of the year gray and male 1981 davies et al 1987 this discharge provides a critically important source of fresh water to many downstream regions groisman and davies 2001 mote et al 2005 stewart et al 2008 woo et al 2008 including ecosystems davies et al 1987 groisman and davies 2001 stewart et al 2008 woo et al 2008 boyd and banzhaf 2007 and agricultural industrial and municipal users harder et al 2019 nazemi et al 2013 in order to accurately predict spring snowcovers there is substantial motivation to simulate the mid winter transport both horizontal blowing snow pomeroy and bernhardt 2017 marsh et al 2020a mott et al 2018 and vertical avalanche bernhardt et al 2012 that in conjunction with substantial horizontal variability in energy fluxes leads to variability in the spring snowcover ablation however sub hyper resolution sub 1 km wood et al 2011 and snow drift resolving scales 1 m 100 m pomeroy and bernhardt 2017 are required to resolve this process heterogeneity and these requirements impose a computational burden that limits the practical use of such models furthermore one and done model runs are no longer generally acceptable and uncertainty estimations require running a model tens hundreds or even thousands of times razavi et al 2018 thus there is a critical need to adopt code optimization techniques to reduce the computational burden of distributed hydrological models many such techniques are common in other computational sciences and can be directly applied to hydrological modelling lookup tables luts are an optimization approach that replaces the direct evaluation of mathematical functions with approximations based on sampling the input space of the function precomputing and storing function values at the sample points and then using interpolation to provide function evaluations luts need not generally be applied to the entire model rather they can be strategically applied to computationally costly mathematical functions within the model e g an empirical parametrization that utilizes many mathematical functions the benefit of luts is that fewer floating point operations flops are used generally leading to decreased model execution times albeit at the cost of decreased accuracy and increased data transfer and storage there are generally two categories of luts software based luts and hardware based luts software based luts make use of the general architecture of a cpu for their implementation they find use in areas of numerical computing such as heart simulation cooper et al 2006 green et al 2019 and atmospheric radiative transfer buehler et al 2005 in contrast hardware based luts make use of specific hardware for storage and evaluation and find use in image processing battiato and lukac 2008 gonzales and woods 2002 pharr and fernando 2005 field programmable gate arrays fpgas kuon et al 2008 and hardware neural networks kumar meher 2010 reis et al 2014 dias et al 2014 although it is true that hardware based luts typically outperform their software based counterparts in practice the use of software based luts is an efficient and effective optimization for computationally intensive and repeatedly called mathematical functions green et al 2019 and can lead to significant decreases in model execution time when run on a standard cpu in this study the focus is exclusively on software based luts historically the implementation of luts required significant manual development work and resulted in software that was difficult to maintain loh et al 2005 wilcox et al 2011 developed the mesa software package for the autonomous generation of luts the mesa package is capable of generating luts according to an evaluation error tolerance but it does not allow users to change the type of interpolation that is used it uses only a basic piecewise constant interpolation over all subintervals it can be shown that when using piecewise constant interpolation methods the table sizes required to represent continuous functions to high precision can easily be on the order of terabytes or even greater wilcox et al 2011 green et al 2019 another downside of mesa is that it must be run external to the application where the lut is to be used a more modern approach to the generation of luts such as the function comparator library func green et al 2019 for c addresses these limitations by providing optimized implementations of higher order interpolation methods as well as dynamic table generation facilities that can be incorporated directly into the desired c application func is a c library that facilitates the generation and assessment of lut implementations for univariate function evaluation over bounded input spaces func provides i lut implementations that use uniformly spaced sample points with interpolation schemes up to third order ii a lut generation tool that can determine the required table parameters for each implementation for a given function domain and evaluation tolerance and iii benchmarking tools for evaluating and comparing the performance of the various lut implementations against each other and against the direct evaluation of the original function investigation and implementation of luts is thus no longer a significant obstacle for univariate functions with guarantees over total error introduced as well as automatic generation and estimates for performance gains these modern approaches allow luts to be readily included in existing software with unprecedented convenience in this work the canadian hydrological model chm marsh et al 2020b is used in conjunction with func to determine whether chm would benefit from lut optimizations specifically a are there univariate mathematical functions in chm that are computationally expensive and would benefit from lut optimization b do these benefits offer sufficient performance gains so as to warrant inclusion and c are there general criteria for lut use in hydrological models the remainder of the paper proceeds as follows section 2 describes chm and its configuration for this study as well as func and its lut capabilities a general methodology is also given for the incorporation of luts in application software through the use of code profiling and func candidate functions are identified for replacement by luts and details on lut construction and use are given section 3 gives the results of applying this methodology within chm on two realistic simulations section 4 discusses these results and offers general takeaways from this study to other applications finally section 5 summarizes the conclusions from the study 2 methods 2 1 model configuration chm is a distributed hydrological model that uses a variable resolution unstructured triangular mesh to represent surface heterogeneity the mesh generation via the software mesher marsh et al 2018 can consider multiple input criteria such as topography and vegetation to constrain the mesh this approach ensures that important surface heterogeneity is well represented while using as few computational elements as possible this has resulted in simulations that can achieve a given level of accuracy with 50 95 fewer computational elements versus a fixed resolution mesh marsh et al 2018 key features of chm include the ability to change remove and decouple hydrological process algorithms to work both in a pointwise as well as in a spatially distributed manner to capture spatial heterogeneity in an efficient manner to scale to multiple spatial extents and scales and to utilize a variety of forcing fields boundary and initial conditions although designed as a generic hydrological modelling framework chm has been developed primarily for cold regions processes such as snowpacks chm currently handles surface snow processes and meteorological downscaling it is under active development with the ultimate intent to model the entire hydrological cycle 2 1 1 snow processes the snow process module within chm used for this study is the snobal model marks et al 1999 snobal is a physics based two layer snowpack model designed specifically for deep mountain snowpacks it approximates the snowpack by two layers where the surface fixed thickness active layer taken here as 0 1 m is used to estimate surface temperature for outgoing longwave radiation and atmosphere snow exchange of sensible and latent heat via turbulent transfer snobal features a coupled energy and mass balance internal energy tracking and liquid water storage calculations turbulent fluxes are explicitly calculated following marks et al 1992 using a bulk transfer approach that includes a monin obukhov stability correction the ground heat flux is calculated from conduction with a single soil layer of a fixed temperature no vegetation canopy is considered in order to better isolate the snowpack model performance 2 1 2 simulation domains two simulation domains are considered for benchmarking and profiling of chm for lut optimization both domains are shown in fig 1 with one domain kananaskis a subdomain of the other bow valley the intent is to investigate how spatial resolution and spatial extent impact the simulation profiling and whether the use of luts can significantly enhance performance the simulation domains are located in the rocky mountains west of calgary alberta canada the kananaskis subdomain shown in yellow and blue elevations has an area of approximately 1 000 km2 represented with 93 162 triangles mean size 10 734 m2 range 600 m2 to 300 000 m2 the elevations of this domain range from 1 827 m to 3 053 m the bow valley domain shown in white and black elevations has an area of around 17 880 km2 and is represented by 238 790 triangles mean size 74 878 m2 range 5 700 m2 to 7 3 km2 the elevations of this domain range from 787 m to 3 453 m the input digital elevation data was the nasa shuttle radar topography mission srtm at 1 arc second approximately 30 m resolution distributions of the unstructured mesh element sizes used are shown in fig 2 2 1 3 meteorological data meteorological forcing data for chm were obtained from the global environmental multiscale gem numerical weather prediction system côté et al 1998 for the time period september 01 2017 to august 30 2018 using hourly timesteps at a 2 5 km 2 5 km resolution input data include air temperature relative humidity wind speed incoming longwave radiation incoming solar shortwave radiation and precipitation these data were vertically interpolated to the surface using the lapse rate methods described in marsh et al 2020b specifically the interpolation methods used for a given quantity and their corresponding references are as follows precipitation thornton et al 1997 relative humidity kunkel 1989 air temperature kunkel 1989 and precipitation phase harder and pomeroy 2013 windspeeds were spatially interpolated and corrected for topographic effects using the windspeed maps derived from the diagnostic wind model windninja wagenbrenner et al 2016 as detailed in marsh et al 2020a 2 2 implementation of software based luts simulations of distributed hydrological models often require large amounts o gb of random access memory ram and are much larger than currently available cpu cache sizes which are o kb to o mb as a result any software based lut implementation is not likely to remain in cache the entire time that a simulation is running this is a worst case scenario for a software based lut because the required portion of the lut must be loaded into cache each time before it is used this is an expensive operation and motivates the use of implementations that minimize the evaluation flops while not having a prohibitively large memory footprint green et al 2019 the uniformlinearinterpolationtable class in func fulfills these requirements for the functions considered here a summary of this approach follows given a univariate function f x defined on a closed interval a b and n 1 distinct points x i i 0 n on a b a lut generates an approximation to f x for any x a b by trading off increased memory usage for decreased evaluation time specifically a lut stores n 1 data pairs of the form x i f x i i 0 n and approximates f x at any point in a b via those pairs and a piecewise interpolating polynomial p m p m m n such that p m x i f x i i 0 1 n where p m is the set of all polynomials with degree at most m the luts used in this study are based on equally spaced data i e every subinterval is the same length h x i 1 x i i 0 1 n 1 and a linear interpolation is used on each interval a straight line is determined using this spacing h and the function values at the two end points f x i and f x i 1 forming a linear interpolant on each subinterval 1 p 1 i x f x i f x i 1 f x i x x i h i 0 1 n 1 provided that f x is sufficiently smooth the absolute interpolation error of p 1 i x is o h 2 on each subinterval x i x i 1 thus over the entire interval a b the absolute interpolation of the piecewise linear polynomial is o h 2 as an example fig 3 shows the function f x sin x 0 8 x and a piecewise linear interpolating polynomial p 1 x consisting of three pieces p 1 i i 1 2 3 fig 3 also provides a visual illustration of how they differ the uniformlinearinterpolationtable class of func implements piecewise linear interpolation it uses the storage strategy depicted in fig 4 to store f x i i 0 n in an array of size n 1 the evaluation proceeds by computing the interval index i as 2 i x x 0 h where is the integer floor function then the scaled distance from the lower sample point is computed as 3 x x i h x x 0 h i leading to a simple evaluation of eq 1 the uniformlinearinterpolationtable class uses seven flops in total per evaluation it uses two flops to calculate x x 0 h two flops to calculate x x 0 h and x x 0 h x x 0 h and three flops to calculate the estimated value p 1 x f x as is well known flop counts are not the complete story when it comes to code performance e g eijkhout 2012 the uniformlinearinterpolationtable class needs to fetch the tabulated values into the cpu cache if they are not already present there because a fetch operation is considerably slower than a flop lut evaluation times are generally dominated by fetch times func is equipped with best and worst case profiler utilities that are designed to mimic the extreme scenarios where the tabulated values need to never be fetched into cache or need to be fetched for almost every evaluation respectively when run on the same hardware as where the simulation code is to be run these utilities provide approximate bounds on how much speedup will be observed by replacing the direct function evaluations with lut evaluation in practice 2 3 a methodology for lut incorporation a general methodology for the incorporation of luts into a software application can be described as follows 1 identify candidate functions for replacement profile candidate simulation on target hardware to identify hot spots of computational expense determine candidate hot spots of mathematical function evaluations such as those with substantial mathematical function use e g exp sin cos 2 estimate performance gains modify code if desired use func tools to determine appropriate lut type and approximate performance gains if deemed worthwhile replace the function evaluation with lut evaluation in simulation code 3 verify modified simulation results 4 measure improvement by computing the actual speedup of the modified simulation we note that this methodology does not constitute a formal algorithm the time required to perform these steps is subject to human action in particular the first step of identifying the candidate regions that could benefit from lut usage is likely to require the most time details on how the above steps were applied to simulations involving chm on the two simulation domains described are now given all profiling and computation are performed on the computer system specified in table 1 because it is significantly the smaller of the two simulation domains the kananaskis subdomain simulation was analyzed using the intel vtune profiler intel 2019 to identify time consuming or resource consuming code sections specifically the fraction of the total time spent in each of k 1 2 k function evaluations t k is determined from vtune identified expensive functions are wrapped in the evaluationfunctor class of func and the worst case profiler tool in func is applied to estimate the speedup s k that would be obtained by replacing expensive function evaluation f k with an appropriate lut an estimate for the total speedup of simulation s can then be computed from 4 s 1 1 k 1 k t k s k 1 1 the code is modified based on the projected performance gains from the kananaskis sub domain simulation and carried over for use in the bow valley domain simulation for verification of simulation results the root mean squared error rmse of the snow water equivalent swe is computed at each timestep over the simulation domain because swe is effectively a surface density the swe over the entire simulation domain can be expressed as a weighted sum of the swe at each mesh element 5 swe j 1 n w j swe j w j δ j j 1 n δ j where w j is the fraction of the total domain that element j occupies swe j is the swe on element j δ j is the area of element j and n is the total number of elements the weighted rmse for swe is given by 6 rmse swe j 1 n w j swe j swe ˆ j 2 where swe j and swe ˆ j are the swe values at mesh element j for chm using direct function evaluations dfes and luts respectively finally the simulations are run on both domains using chm with dfes and luts and the wall clock times are measured details of these results along with relative performance metrics are provided below 3 results this section reports the results from applying the steps to identify and replace expensive mathematical function evaluations with luts using func within chm running on the kananaskis and bow valley simulation domains 3 1 identify expensive function evaluations to identify expensive mathematical function evaluations that can potentially benefit from lut evaluation the kananaskis subdomain simulation is first run through vtune with profiler settings as specified in table 2 from the output of the profiling not shown it is observed that a significant fraction of the time for the simulation is spent in the functions pow ieee754 log avx and log vtune provides functionality to easily dig deeper and find that these three functions are called largely from two routines in the chm code sno satw and sno sati which are routines to compute the saturated vapour pressures with respect to water and ice respectively the mathematical function evaluated in the sno satw routine is 7 f w x 10 z s a t w z s a t w 7 90298 373 15 x 1 5 02808 ln 10 ln 373 15 x 0 00000013816 10 11 344 1 x 373 15 1 0 0081328 10 3 49149 373 15 x 1 1 ln 101324 6 ln 10 and the mathematical function evaluated in sno sati is 8 f i x 100 10 z s a t i z s a t i 9 09718 273 16 x 1 3 56654 ln 10 ln 273 16 x 0 876793 1 x 273 16 ln 6 1071 ln 10 equations 7 and 8 are plotted in fig 5 in addition to the functions of eqs 7 and 8 the sno satw and sno sati routines also include branching statements and the sno sati routine calls the sno satw routine for x 273 15 vtune provides enough data about routine callers and callees to easily determine how much time is spent in each of the sno satw and sno sati routines to summarize the profiling results vtune shows that a total of 104 418 108 s of cpu time is spent for the kananaskis sub domain simulation 5 477 735 s of cpu time of which is spent in sno satw and 10 936 926 s of which is spent in sno sati thus the fraction of the total cpu time for the kananaskis sub domain simulation taken by sno satw is 0 0525 and the fraction of the total cpu time taken by sno sati is 0 1047 to estimate the magnitude of the speedup that can be expected the function evaluations can be set up to be executed as an evaluationfunctor in func comparing the direct evaluation to the worst and best case scenarios it is found that when using a uniformlinearinterpolationtable evaluation of eq 7 can be expected to yield a speedup in the range of 4 8 52 9 and evaluation of eq 8 can be expected to yield a speedup in the range of 2 2 24 1 timing data from func are provided in table 3 using the speedup ranges in table 3 to approximate the overall application speedup according to eq 4 it is expected that using lut evaluation will result in a speedup in the range of 1 11 1 18 depending on how often the required lut data remains in cache in practice 3 2 modify simulation code an important note is that the sno satw and sno sati functions are called within an iterative solution scheme within snobal therefore the temperature values provided as arguments to the sno satw and sno sati functions may be outside the range of physically realistic values until the iterative solution is close to convergence accordingly one must consider larger input argument ranges than expected based on physics alone to ensure stability while using luts in the iterative solutions determining the upper bound for the argument ranges was done simply by selecting a temperature higher than any ever observed in canada and for this purpose we have chosen 323 16 k 50 01 c based on experience with the snobal code the chosen ranges that were appropriate to ensure coverage during the iterative solution are 223 16 323 16 k and 0 01 323 16 k for eq 7 and eq 8 respectively all luts were generated to satisfy an interpolation error tolerance of 10 8 the lut for eq 8 is set to use the interval 90 323 16 k and explicity uses 0 for arguments below 90 k a sample of the minimum code needed to set up and use a lut of sno satw in a class suitable for use in func is given in fig 6 wrap the function to be evaluated in a class include func func hpp include snomacros h using namespace snobalmacros freeze boil and sea level defined in snobalmacros namespace define func w x code for evaluating eq 7 class func satw final public evaluationfunctor double double public double operator double x override return func w x double derivative double x override generate the lut func satw uniformlookuptablegenerator gen w func w freeze 50 0 freeze 50 0 static std unique ptr evaluationimplementation impl w gen w generate by tol uniformlinearinterpolationtable 1e 8 use the generated lut double temperature 275 0 double result impl w temperature 3 3 verify result quality having replaced sno satw and sno sati evaluations with luts to tolerance 10 8 the next step is to verify that the resulting simulation quality remains acceptable for this swe as computed by eq 5 is plotted for chm using dfes i e without luts and using luts as well as the rmse swe computed by eq 6 for each domain the corresponding plot for the kananaskis domain is shown in fig 7 and that for the bow valley domain is shown in fig 8 the rmse swe reaches a maximum of 0 054 mm for the kananaskis simulation and 0 105 mm for the bow valley simulation 3 4 measure improvement in order to measure the performance improvement provided by luts chm was run in its original form which uses dfes as well as with lut optimization on both simulation domains the computer system used for the simulations is described in table 1 each simulation was executed 10 times in order to assess the variability in the execution times the results from the simulations of the kananaskis domain are summarized in table 4 when using dfes the range of run times was 4 377 s to 4 597 s with median time 4 404 s when using the lut optimizations the range of run times was 3 652 s to 3 741 s with median time 3 698 s a mann whitney u test applied to the data rejected the null hypothesis that the medians of the two data sets are equal at significance p 1 8 10 4 accordingly it can be concluded with high probability that the run time of chm using luts is lower than that using dfes based on the minimum run times which represent estimates of the theoretically true run times the lut implementation produced a 16 6 reduction in execution time the results from the simulations of the bow valley domain are summarized in table 5 when using dfes the range of run times was 11 291 s to 11 497 s with median time 11 386 s when using the lut optimizations the range of run times was 9 176 s to 9 401 s with median time 9 291 s a mann whitney u test applied to the data rejected the null hypothesis that the medians of the two data sets are equal at significance p 1 8 10 4 thus there is a high probability that the run time of chm using luts is lower than that using dfes based on the minimum run times the lut implementation produced a 18 7 reduction in execution time demonstrating a larger relative savings with increased simulation size 4 discussion numerical hydrological models make extensive use of mathematical functions for example sine cosine exponential and power functions are widely used for the estimation of radiative fluxes such as solar radiation garnier and ohmura 1968 marsh et al 2012 terrain shadowing dozier and frew 1990 marsh et al 2012 longwave fluxes fiddes and gruber 2014 sicart et al 2006 and canopy shortwave transmittance ellis and pomeroy 2007 they also have extensive uses in micro meteorology such as in the estimation of turbulent fluxes foken 2018 water infiltration equations make extensive use of power law relationships such as for estimating hydraulic conductivity e g via the van genuchten mualem model mualem 1976 van genuchten 1980 channel and hillslope routing mizukami et al 2016 and saturated sub surface flow e g metcalfe et al 2015 also make extensive use of these mathematical functions all these examples are potential candidates for lut optimization naturally luts do incur some overhead accordingly some judgement must be exercised in deciding upon the benefit of their implementation the mathematical functions best suited for lut optimization have some or all of the following features functions that use multiple mathematical functions e g combinations such as exp x ln x sin x or compositions such as exp sin ln x functions that are repeatedly called e g when used in iterative solutions loops over spatial elements or time series due to the complex nature of modern software profiling remains the best way to determine which pieces of code are the most computationally intensive once located computationally intensive pieces of code that have the above features can be targeted as most likely to significantly benefit from lut optimization the results in this paper have been obtained from a non invasive approach to introducing luts into a simulation code in this case chm using func func was designed precisely to facilitate the introduction of luts into a code in this way for this example a reduction of almost 20 in simulation runtime was obtained with relatively little manual effort of course the reduction in runtime for a given simulation depends heavily on what fraction of time is spent on mathematical function evaluations although vtune was used herein the methodologies described can be applied to other software profilers such as amd s μprof or the open source gprof and valgrind profilers in addition vtune is no longer a strictly commercial software package there is a fully functional freeware community edition increasing widespread availability of such tools should enable the scientific research community to make more informed decisions when optimizing numerical code it is also noteworthy that lut optimizations can give results that are significantly better than those from this study if the underlying code is redesigned to make better use of luts in cache such a redesign involves emphasizing cache consistency over maintaining expressive abstractions chm currently leans towards the latter for the purposes of programming convenience however this programming convenience could be traded off for increased performance for example using the best case scenario it was found that if the necessary lut elements are always in cache the sno satw has a speedup factor of 52 9 and sno sati has a speedup factor of 24 1 using these values in eq 4 produces a best case speedup factor estimate of 1 18 such an argument however relies on the assumption that a more cache consistent redesign would operate at the same speed overall as the original chm used in this study such an assumption is unlikely to hold in practice this is where higher order luts with their smaller memory requirement and thus probability to be in cache when needed will start to shine especially when combined with a cache consistent redesign of code however a cache consistent redesign of a simulation code that was built around expressive abstraction is not a trivial task as such the authors currently support the idea of using minimally intrusive lut replacement of expensive function evaluations such as described using a lut library like func as a go to first step to produce notable performance gains in general simulation code as demonstrated in the results presented 5 conclusions the use of distributed physically based models has substantial potential for providing insight into the spatial heterogeneity of mass and energy of hydrological systems however the computational burden associated with finer scales and more complex process representations can result in reduced uncertainty analysis simpler model formulations and smaller spatial extents therefore there is substantial motivation to optimize and reduce the runtimes of these models lut optimizations allow for replacing computationally expensive mathematical function evaluations in the model code with highly efficient approximations the result is faster evaluation with no significant reduction in accuracy these approaches are widely used in computational sciences and the hydrological sciences can benefit from them as well in this work the distributed hydrological model chm was profiled and two computationally intensive and repeatedly called mathematical functions sno satw and sno sati were identified these were located in the snobal snowmodel code these functions made significant use of the pow and log functions using the func library luts based on piecewise linear interpolation based on uniformly distributed data from the class uniformlinearinterpolation were generated two simulations domains were investigated comprising 1 000 km2 93 162 computational elements and 17 880 km2 238 790 computational elements respectively it was found that in both cases the runtime of chm was decreased by almost 20 due to the incorporation of luts in both cases using luts with interpolation error of at most 10 8 the resulting rmse swe remains less than 0 105 mm in summary modern lut tools such as func provide a systematic procedure of implementing luts that can be easily applied to hydrological or other computational models that involve a significant amount of mathematical function evaluations these approaches represent a relatively painless approach to dramatically improving the performance of numerical codes and should be widely adopted data availability the srtm30 dem used is available from nasa https www2 jpl nasa gov srtm the gem forecasts are available at the canadian surface prediction archive caspar https caspar data ca declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to acknowledge support from the natural science and engineering council of canada through the discovery grants program and the global institute for water security software availability most of the software used for this manuscript is open source and available specifically the canadian hydrological model chm is open source and available at https github com chrismarsh chm the mesh generation used for chm mesher is open source and available at https github com chrismarsh mesher the table lookup software func is open source and available at https github com uofs simlab func the vtune profiler is not open source but a free version is available from https software seek intel com intel vtune profiler appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105018 
25862,distributed hydrological models predict the spatial variability in processes that govern observed mass and energy fluxes a challenge associated with the use of these models is the computational burden associated with representing the earth s sub surface via millions of computational elements this burden is exacerbated as more complex process representations are included because their parameterizations involve computationally intensive mathematical functions lookup tables luts approximate a mathematical function by interpolating precomputed values of the function highly accurate approximations are possible for substantially reduced computational costs in this work a general methodology using the c lut library func is applied to identify and replace computationally intensive mathematical function evaluations in the canadian hydrological model chm the use of luts introduces a pointwise relative error below 10 8 and provides a reduction in run time of almost 20 this work shows how luts can be implemented with relatively little pain and yield significant computational savings for distributed hydrological models keywords hydrology optimization lookup table snow cold regions 1 introduction computer simulation is an increasingly important tool to address both research and operational water resource issues related to the hydrological cycle these issues include improving our understanding of complex interactions affecting streamflow helping to inform water management decisions quantifying the effects of changes in climate and landcover determining the changing frequency of extreme events predicting flood impacts and designing future infrastructure freeze and harlan 1969 mote et al 2005 milly et al 2008 nazemi et al 2013 debeer et al 2015 wheater 2015 musselman et al 2017 distributed hydrological models represent the surface of the earth using computational elements defined on regular or irregular grids e g rasters or triangles marsh et al 2020b the use of distributed models can provide key insights into the spatial variability of hydrological processes that govern the spatial heterogeneity in mass and energy fluxes dornes et al 2008 fatichi et al 2016 kumar et al 2013 however as the spatial extents investigated increase or the spatial scales of the computational elements decrease there are limitations to the practicality of distributed models partly due to computational costs associated with the simulations certainly there are open questions regarding equifinality beven 1993 parameter estimation beven 1993 savenije 2009 initial and boundary conditions and uncertainty in distributed hydrological models however model complexity and structure decisions are often influenced by computational cost fatichi et al 2016 the use of distributed hydrological models in practice is particularly challenging in the complex topography of mountainous regions in these water towers of the world viviroli et al 2007 seasonal snowpacks store a significant portion of the annual precipitation the melting of these spatially variable snowpacks generates the spring freshet that can result in the largest discharge of the year gray and male 1981 davies et al 1987 this discharge provides a critically important source of fresh water to many downstream regions groisman and davies 2001 mote et al 2005 stewart et al 2008 woo et al 2008 including ecosystems davies et al 1987 groisman and davies 2001 stewart et al 2008 woo et al 2008 boyd and banzhaf 2007 and agricultural industrial and municipal users harder et al 2019 nazemi et al 2013 in order to accurately predict spring snowcovers there is substantial motivation to simulate the mid winter transport both horizontal blowing snow pomeroy and bernhardt 2017 marsh et al 2020a mott et al 2018 and vertical avalanche bernhardt et al 2012 that in conjunction with substantial horizontal variability in energy fluxes leads to variability in the spring snowcover ablation however sub hyper resolution sub 1 km wood et al 2011 and snow drift resolving scales 1 m 100 m pomeroy and bernhardt 2017 are required to resolve this process heterogeneity and these requirements impose a computational burden that limits the practical use of such models furthermore one and done model runs are no longer generally acceptable and uncertainty estimations require running a model tens hundreds or even thousands of times razavi et al 2018 thus there is a critical need to adopt code optimization techniques to reduce the computational burden of distributed hydrological models many such techniques are common in other computational sciences and can be directly applied to hydrological modelling lookup tables luts are an optimization approach that replaces the direct evaluation of mathematical functions with approximations based on sampling the input space of the function precomputing and storing function values at the sample points and then using interpolation to provide function evaluations luts need not generally be applied to the entire model rather they can be strategically applied to computationally costly mathematical functions within the model e g an empirical parametrization that utilizes many mathematical functions the benefit of luts is that fewer floating point operations flops are used generally leading to decreased model execution times albeit at the cost of decreased accuracy and increased data transfer and storage there are generally two categories of luts software based luts and hardware based luts software based luts make use of the general architecture of a cpu for their implementation they find use in areas of numerical computing such as heart simulation cooper et al 2006 green et al 2019 and atmospheric radiative transfer buehler et al 2005 in contrast hardware based luts make use of specific hardware for storage and evaluation and find use in image processing battiato and lukac 2008 gonzales and woods 2002 pharr and fernando 2005 field programmable gate arrays fpgas kuon et al 2008 and hardware neural networks kumar meher 2010 reis et al 2014 dias et al 2014 although it is true that hardware based luts typically outperform their software based counterparts in practice the use of software based luts is an efficient and effective optimization for computationally intensive and repeatedly called mathematical functions green et al 2019 and can lead to significant decreases in model execution time when run on a standard cpu in this study the focus is exclusively on software based luts historically the implementation of luts required significant manual development work and resulted in software that was difficult to maintain loh et al 2005 wilcox et al 2011 developed the mesa software package for the autonomous generation of luts the mesa package is capable of generating luts according to an evaluation error tolerance but it does not allow users to change the type of interpolation that is used it uses only a basic piecewise constant interpolation over all subintervals it can be shown that when using piecewise constant interpolation methods the table sizes required to represent continuous functions to high precision can easily be on the order of terabytes or even greater wilcox et al 2011 green et al 2019 another downside of mesa is that it must be run external to the application where the lut is to be used a more modern approach to the generation of luts such as the function comparator library func green et al 2019 for c addresses these limitations by providing optimized implementations of higher order interpolation methods as well as dynamic table generation facilities that can be incorporated directly into the desired c application func is a c library that facilitates the generation and assessment of lut implementations for univariate function evaluation over bounded input spaces func provides i lut implementations that use uniformly spaced sample points with interpolation schemes up to third order ii a lut generation tool that can determine the required table parameters for each implementation for a given function domain and evaluation tolerance and iii benchmarking tools for evaluating and comparing the performance of the various lut implementations against each other and against the direct evaluation of the original function investigation and implementation of luts is thus no longer a significant obstacle for univariate functions with guarantees over total error introduced as well as automatic generation and estimates for performance gains these modern approaches allow luts to be readily included in existing software with unprecedented convenience in this work the canadian hydrological model chm marsh et al 2020b is used in conjunction with func to determine whether chm would benefit from lut optimizations specifically a are there univariate mathematical functions in chm that are computationally expensive and would benefit from lut optimization b do these benefits offer sufficient performance gains so as to warrant inclusion and c are there general criteria for lut use in hydrological models the remainder of the paper proceeds as follows section 2 describes chm and its configuration for this study as well as func and its lut capabilities a general methodology is also given for the incorporation of luts in application software through the use of code profiling and func candidate functions are identified for replacement by luts and details on lut construction and use are given section 3 gives the results of applying this methodology within chm on two realistic simulations section 4 discusses these results and offers general takeaways from this study to other applications finally section 5 summarizes the conclusions from the study 2 methods 2 1 model configuration chm is a distributed hydrological model that uses a variable resolution unstructured triangular mesh to represent surface heterogeneity the mesh generation via the software mesher marsh et al 2018 can consider multiple input criteria such as topography and vegetation to constrain the mesh this approach ensures that important surface heterogeneity is well represented while using as few computational elements as possible this has resulted in simulations that can achieve a given level of accuracy with 50 95 fewer computational elements versus a fixed resolution mesh marsh et al 2018 key features of chm include the ability to change remove and decouple hydrological process algorithms to work both in a pointwise as well as in a spatially distributed manner to capture spatial heterogeneity in an efficient manner to scale to multiple spatial extents and scales and to utilize a variety of forcing fields boundary and initial conditions although designed as a generic hydrological modelling framework chm has been developed primarily for cold regions processes such as snowpacks chm currently handles surface snow processes and meteorological downscaling it is under active development with the ultimate intent to model the entire hydrological cycle 2 1 1 snow processes the snow process module within chm used for this study is the snobal model marks et al 1999 snobal is a physics based two layer snowpack model designed specifically for deep mountain snowpacks it approximates the snowpack by two layers where the surface fixed thickness active layer taken here as 0 1 m is used to estimate surface temperature for outgoing longwave radiation and atmosphere snow exchange of sensible and latent heat via turbulent transfer snobal features a coupled energy and mass balance internal energy tracking and liquid water storage calculations turbulent fluxes are explicitly calculated following marks et al 1992 using a bulk transfer approach that includes a monin obukhov stability correction the ground heat flux is calculated from conduction with a single soil layer of a fixed temperature no vegetation canopy is considered in order to better isolate the snowpack model performance 2 1 2 simulation domains two simulation domains are considered for benchmarking and profiling of chm for lut optimization both domains are shown in fig 1 with one domain kananaskis a subdomain of the other bow valley the intent is to investigate how spatial resolution and spatial extent impact the simulation profiling and whether the use of luts can significantly enhance performance the simulation domains are located in the rocky mountains west of calgary alberta canada the kananaskis subdomain shown in yellow and blue elevations has an area of approximately 1 000 km2 represented with 93 162 triangles mean size 10 734 m2 range 600 m2 to 300 000 m2 the elevations of this domain range from 1 827 m to 3 053 m the bow valley domain shown in white and black elevations has an area of around 17 880 km2 and is represented by 238 790 triangles mean size 74 878 m2 range 5 700 m2 to 7 3 km2 the elevations of this domain range from 787 m to 3 453 m the input digital elevation data was the nasa shuttle radar topography mission srtm at 1 arc second approximately 30 m resolution distributions of the unstructured mesh element sizes used are shown in fig 2 2 1 3 meteorological data meteorological forcing data for chm were obtained from the global environmental multiscale gem numerical weather prediction system côté et al 1998 for the time period september 01 2017 to august 30 2018 using hourly timesteps at a 2 5 km 2 5 km resolution input data include air temperature relative humidity wind speed incoming longwave radiation incoming solar shortwave radiation and precipitation these data were vertically interpolated to the surface using the lapse rate methods described in marsh et al 2020b specifically the interpolation methods used for a given quantity and their corresponding references are as follows precipitation thornton et al 1997 relative humidity kunkel 1989 air temperature kunkel 1989 and precipitation phase harder and pomeroy 2013 windspeeds were spatially interpolated and corrected for topographic effects using the windspeed maps derived from the diagnostic wind model windninja wagenbrenner et al 2016 as detailed in marsh et al 2020a 2 2 implementation of software based luts simulations of distributed hydrological models often require large amounts o gb of random access memory ram and are much larger than currently available cpu cache sizes which are o kb to o mb as a result any software based lut implementation is not likely to remain in cache the entire time that a simulation is running this is a worst case scenario for a software based lut because the required portion of the lut must be loaded into cache each time before it is used this is an expensive operation and motivates the use of implementations that minimize the evaluation flops while not having a prohibitively large memory footprint green et al 2019 the uniformlinearinterpolationtable class in func fulfills these requirements for the functions considered here a summary of this approach follows given a univariate function f x defined on a closed interval a b and n 1 distinct points x i i 0 n on a b a lut generates an approximation to f x for any x a b by trading off increased memory usage for decreased evaluation time specifically a lut stores n 1 data pairs of the form x i f x i i 0 n and approximates f x at any point in a b via those pairs and a piecewise interpolating polynomial p m p m m n such that p m x i f x i i 0 1 n where p m is the set of all polynomials with degree at most m the luts used in this study are based on equally spaced data i e every subinterval is the same length h x i 1 x i i 0 1 n 1 and a linear interpolation is used on each interval a straight line is determined using this spacing h and the function values at the two end points f x i and f x i 1 forming a linear interpolant on each subinterval 1 p 1 i x f x i f x i 1 f x i x x i h i 0 1 n 1 provided that f x is sufficiently smooth the absolute interpolation error of p 1 i x is o h 2 on each subinterval x i x i 1 thus over the entire interval a b the absolute interpolation of the piecewise linear polynomial is o h 2 as an example fig 3 shows the function f x sin x 0 8 x and a piecewise linear interpolating polynomial p 1 x consisting of three pieces p 1 i i 1 2 3 fig 3 also provides a visual illustration of how they differ the uniformlinearinterpolationtable class of func implements piecewise linear interpolation it uses the storage strategy depicted in fig 4 to store f x i i 0 n in an array of size n 1 the evaluation proceeds by computing the interval index i as 2 i x x 0 h where is the integer floor function then the scaled distance from the lower sample point is computed as 3 x x i h x x 0 h i leading to a simple evaluation of eq 1 the uniformlinearinterpolationtable class uses seven flops in total per evaluation it uses two flops to calculate x x 0 h two flops to calculate x x 0 h and x x 0 h x x 0 h and three flops to calculate the estimated value p 1 x f x as is well known flop counts are not the complete story when it comes to code performance e g eijkhout 2012 the uniformlinearinterpolationtable class needs to fetch the tabulated values into the cpu cache if they are not already present there because a fetch operation is considerably slower than a flop lut evaluation times are generally dominated by fetch times func is equipped with best and worst case profiler utilities that are designed to mimic the extreme scenarios where the tabulated values need to never be fetched into cache or need to be fetched for almost every evaluation respectively when run on the same hardware as where the simulation code is to be run these utilities provide approximate bounds on how much speedup will be observed by replacing the direct function evaluations with lut evaluation in practice 2 3 a methodology for lut incorporation a general methodology for the incorporation of luts into a software application can be described as follows 1 identify candidate functions for replacement profile candidate simulation on target hardware to identify hot spots of computational expense determine candidate hot spots of mathematical function evaluations such as those with substantial mathematical function use e g exp sin cos 2 estimate performance gains modify code if desired use func tools to determine appropriate lut type and approximate performance gains if deemed worthwhile replace the function evaluation with lut evaluation in simulation code 3 verify modified simulation results 4 measure improvement by computing the actual speedup of the modified simulation we note that this methodology does not constitute a formal algorithm the time required to perform these steps is subject to human action in particular the first step of identifying the candidate regions that could benefit from lut usage is likely to require the most time details on how the above steps were applied to simulations involving chm on the two simulation domains described are now given all profiling and computation are performed on the computer system specified in table 1 because it is significantly the smaller of the two simulation domains the kananaskis subdomain simulation was analyzed using the intel vtune profiler intel 2019 to identify time consuming or resource consuming code sections specifically the fraction of the total time spent in each of k 1 2 k function evaluations t k is determined from vtune identified expensive functions are wrapped in the evaluationfunctor class of func and the worst case profiler tool in func is applied to estimate the speedup s k that would be obtained by replacing expensive function evaluation f k with an appropriate lut an estimate for the total speedup of simulation s can then be computed from 4 s 1 1 k 1 k t k s k 1 1 the code is modified based on the projected performance gains from the kananaskis sub domain simulation and carried over for use in the bow valley domain simulation for verification of simulation results the root mean squared error rmse of the snow water equivalent swe is computed at each timestep over the simulation domain because swe is effectively a surface density the swe over the entire simulation domain can be expressed as a weighted sum of the swe at each mesh element 5 swe j 1 n w j swe j w j δ j j 1 n δ j where w j is the fraction of the total domain that element j occupies swe j is the swe on element j δ j is the area of element j and n is the total number of elements the weighted rmse for swe is given by 6 rmse swe j 1 n w j swe j swe ˆ j 2 where swe j and swe ˆ j are the swe values at mesh element j for chm using direct function evaluations dfes and luts respectively finally the simulations are run on both domains using chm with dfes and luts and the wall clock times are measured details of these results along with relative performance metrics are provided below 3 results this section reports the results from applying the steps to identify and replace expensive mathematical function evaluations with luts using func within chm running on the kananaskis and bow valley simulation domains 3 1 identify expensive function evaluations to identify expensive mathematical function evaluations that can potentially benefit from lut evaluation the kananaskis subdomain simulation is first run through vtune with profiler settings as specified in table 2 from the output of the profiling not shown it is observed that a significant fraction of the time for the simulation is spent in the functions pow ieee754 log avx and log vtune provides functionality to easily dig deeper and find that these three functions are called largely from two routines in the chm code sno satw and sno sati which are routines to compute the saturated vapour pressures with respect to water and ice respectively the mathematical function evaluated in the sno satw routine is 7 f w x 10 z s a t w z s a t w 7 90298 373 15 x 1 5 02808 ln 10 ln 373 15 x 0 00000013816 10 11 344 1 x 373 15 1 0 0081328 10 3 49149 373 15 x 1 1 ln 101324 6 ln 10 and the mathematical function evaluated in sno sati is 8 f i x 100 10 z s a t i z s a t i 9 09718 273 16 x 1 3 56654 ln 10 ln 273 16 x 0 876793 1 x 273 16 ln 6 1071 ln 10 equations 7 and 8 are plotted in fig 5 in addition to the functions of eqs 7 and 8 the sno satw and sno sati routines also include branching statements and the sno sati routine calls the sno satw routine for x 273 15 vtune provides enough data about routine callers and callees to easily determine how much time is spent in each of the sno satw and sno sati routines to summarize the profiling results vtune shows that a total of 104 418 108 s of cpu time is spent for the kananaskis sub domain simulation 5 477 735 s of cpu time of which is spent in sno satw and 10 936 926 s of which is spent in sno sati thus the fraction of the total cpu time for the kananaskis sub domain simulation taken by sno satw is 0 0525 and the fraction of the total cpu time taken by sno sati is 0 1047 to estimate the magnitude of the speedup that can be expected the function evaluations can be set up to be executed as an evaluationfunctor in func comparing the direct evaluation to the worst and best case scenarios it is found that when using a uniformlinearinterpolationtable evaluation of eq 7 can be expected to yield a speedup in the range of 4 8 52 9 and evaluation of eq 8 can be expected to yield a speedup in the range of 2 2 24 1 timing data from func are provided in table 3 using the speedup ranges in table 3 to approximate the overall application speedup according to eq 4 it is expected that using lut evaluation will result in a speedup in the range of 1 11 1 18 depending on how often the required lut data remains in cache in practice 3 2 modify simulation code an important note is that the sno satw and sno sati functions are called within an iterative solution scheme within snobal therefore the temperature values provided as arguments to the sno satw and sno sati functions may be outside the range of physically realistic values until the iterative solution is close to convergence accordingly one must consider larger input argument ranges than expected based on physics alone to ensure stability while using luts in the iterative solutions determining the upper bound for the argument ranges was done simply by selecting a temperature higher than any ever observed in canada and for this purpose we have chosen 323 16 k 50 01 c based on experience with the snobal code the chosen ranges that were appropriate to ensure coverage during the iterative solution are 223 16 323 16 k and 0 01 323 16 k for eq 7 and eq 8 respectively all luts were generated to satisfy an interpolation error tolerance of 10 8 the lut for eq 8 is set to use the interval 90 323 16 k and explicity uses 0 for arguments below 90 k a sample of the minimum code needed to set up and use a lut of sno satw in a class suitable for use in func is given in fig 6 wrap the function to be evaluated in a class include func func hpp include snomacros h using namespace snobalmacros freeze boil and sea level defined in snobalmacros namespace define func w x code for evaluating eq 7 class func satw final public evaluationfunctor double double public double operator double x override return func w x double derivative double x override generate the lut func satw uniformlookuptablegenerator gen w func w freeze 50 0 freeze 50 0 static std unique ptr evaluationimplementation impl w gen w generate by tol uniformlinearinterpolationtable 1e 8 use the generated lut double temperature 275 0 double result impl w temperature 3 3 verify result quality having replaced sno satw and sno sati evaluations with luts to tolerance 10 8 the next step is to verify that the resulting simulation quality remains acceptable for this swe as computed by eq 5 is plotted for chm using dfes i e without luts and using luts as well as the rmse swe computed by eq 6 for each domain the corresponding plot for the kananaskis domain is shown in fig 7 and that for the bow valley domain is shown in fig 8 the rmse swe reaches a maximum of 0 054 mm for the kananaskis simulation and 0 105 mm for the bow valley simulation 3 4 measure improvement in order to measure the performance improvement provided by luts chm was run in its original form which uses dfes as well as with lut optimization on both simulation domains the computer system used for the simulations is described in table 1 each simulation was executed 10 times in order to assess the variability in the execution times the results from the simulations of the kananaskis domain are summarized in table 4 when using dfes the range of run times was 4 377 s to 4 597 s with median time 4 404 s when using the lut optimizations the range of run times was 3 652 s to 3 741 s with median time 3 698 s a mann whitney u test applied to the data rejected the null hypothesis that the medians of the two data sets are equal at significance p 1 8 10 4 accordingly it can be concluded with high probability that the run time of chm using luts is lower than that using dfes based on the minimum run times which represent estimates of the theoretically true run times the lut implementation produced a 16 6 reduction in execution time the results from the simulations of the bow valley domain are summarized in table 5 when using dfes the range of run times was 11 291 s to 11 497 s with median time 11 386 s when using the lut optimizations the range of run times was 9 176 s to 9 401 s with median time 9 291 s a mann whitney u test applied to the data rejected the null hypothesis that the medians of the two data sets are equal at significance p 1 8 10 4 thus there is a high probability that the run time of chm using luts is lower than that using dfes based on the minimum run times the lut implementation produced a 18 7 reduction in execution time demonstrating a larger relative savings with increased simulation size 4 discussion numerical hydrological models make extensive use of mathematical functions for example sine cosine exponential and power functions are widely used for the estimation of radiative fluxes such as solar radiation garnier and ohmura 1968 marsh et al 2012 terrain shadowing dozier and frew 1990 marsh et al 2012 longwave fluxes fiddes and gruber 2014 sicart et al 2006 and canopy shortwave transmittance ellis and pomeroy 2007 they also have extensive uses in micro meteorology such as in the estimation of turbulent fluxes foken 2018 water infiltration equations make extensive use of power law relationships such as for estimating hydraulic conductivity e g via the van genuchten mualem model mualem 1976 van genuchten 1980 channel and hillslope routing mizukami et al 2016 and saturated sub surface flow e g metcalfe et al 2015 also make extensive use of these mathematical functions all these examples are potential candidates for lut optimization naturally luts do incur some overhead accordingly some judgement must be exercised in deciding upon the benefit of their implementation the mathematical functions best suited for lut optimization have some or all of the following features functions that use multiple mathematical functions e g combinations such as exp x ln x sin x or compositions such as exp sin ln x functions that are repeatedly called e g when used in iterative solutions loops over spatial elements or time series due to the complex nature of modern software profiling remains the best way to determine which pieces of code are the most computationally intensive once located computationally intensive pieces of code that have the above features can be targeted as most likely to significantly benefit from lut optimization the results in this paper have been obtained from a non invasive approach to introducing luts into a simulation code in this case chm using func func was designed precisely to facilitate the introduction of luts into a code in this way for this example a reduction of almost 20 in simulation runtime was obtained with relatively little manual effort of course the reduction in runtime for a given simulation depends heavily on what fraction of time is spent on mathematical function evaluations although vtune was used herein the methodologies described can be applied to other software profilers such as amd s μprof or the open source gprof and valgrind profilers in addition vtune is no longer a strictly commercial software package there is a fully functional freeware community edition increasing widespread availability of such tools should enable the scientific research community to make more informed decisions when optimizing numerical code it is also noteworthy that lut optimizations can give results that are significantly better than those from this study if the underlying code is redesigned to make better use of luts in cache such a redesign involves emphasizing cache consistency over maintaining expressive abstractions chm currently leans towards the latter for the purposes of programming convenience however this programming convenience could be traded off for increased performance for example using the best case scenario it was found that if the necessary lut elements are always in cache the sno satw has a speedup factor of 52 9 and sno sati has a speedup factor of 24 1 using these values in eq 4 produces a best case speedup factor estimate of 1 18 such an argument however relies on the assumption that a more cache consistent redesign would operate at the same speed overall as the original chm used in this study such an assumption is unlikely to hold in practice this is where higher order luts with their smaller memory requirement and thus probability to be in cache when needed will start to shine especially when combined with a cache consistent redesign of code however a cache consistent redesign of a simulation code that was built around expressive abstraction is not a trivial task as such the authors currently support the idea of using minimally intrusive lut replacement of expensive function evaluations such as described using a lut library like func as a go to first step to produce notable performance gains in general simulation code as demonstrated in the results presented 5 conclusions the use of distributed physically based models has substantial potential for providing insight into the spatial heterogeneity of mass and energy of hydrological systems however the computational burden associated with finer scales and more complex process representations can result in reduced uncertainty analysis simpler model formulations and smaller spatial extents therefore there is substantial motivation to optimize and reduce the runtimes of these models lut optimizations allow for replacing computationally expensive mathematical function evaluations in the model code with highly efficient approximations the result is faster evaluation with no significant reduction in accuracy these approaches are widely used in computational sciences and the hydrological sciences can benefit from them as well in this work the distributed hydrological model chm was profiled and two computationally intensive and repeatedly called mathematical functions sno satw and sno sati were identified these were located in the snobal snowmodel code these functions made significant use of the pow and log functions using the func library luts based on piecewise linear interpolation based on uniformly distributed data from the class uniformlinearinterpolation were generated two simulations domains were investigated comprising 1 000 km2 93 162 computational elements and 17 880 km2 238 790 computational elements respectively it was found that in both cases the runtime of chm was decreased by almost 20 due to the incorporation of luts in both cases using luts with interpolation error of at most 10 8 the resulting rmse swe remains less than 0 105 mm in summary modern lut tools such as func provide a systematic procedure of implementing luts that can be easily applied to hydrological or other computational models that involve a significant amount of mathematical function evaluations these approaches represent a relatively painless approach to dramatically improving the performance of numerical codes and should be widely adopted data availability the srtm30 dem used is available from nasa https www2 jpl nasa gov srtm the gem forecasts are available at the canadian surface prediction archive caspar https caspar data ca declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to acknowledge support from the natural science and engineering council of canada through the discovery grants program and the global institute for water security software availability most of the software used for this manuscript is open source and available specifically the canadian hydrological model chm is open source and available at https github com chrismarsh chm the mesh generation used for chm mesher is open source and available at https github com chrismarsh mesher the table lookup software func is open source and available at https github com uofs simlab func the vtune profiler is not open source but a free version is available from https software seek intel com intel vtune profiler appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105018 
25863,the existing bias correction bc methods used in impact studies are routinely based on a fixed model structure and often ignore the nature and magnitude of biases and their variations into the future as a calibrated model is applied to bias correct the future time series there is no feedback mechanism to assess the impact of model complexity on the model performance in the future in this paper we propose a flexible modelling strategy to create a robust bias correction procedure in the form of an open source toolkit in the r statistical computing environment the approach allows the user to apply a multi dimensional bias correction model that is self evolving and grows in complexity on the basis of the requirement of the raw data the theoretical background and the capabilities of the software along with a sample application and results discussions are demonstrated in this paper keywords bias correction r package persistence multivariate distribution based quantile matching nested bias correction model complexity self evolving robust bias correction 1 introduction quantification of the effects of climate change is currently one of the most debatable and challenging topics in science global circulation models gcms are considered as the best tools to understand earth s climate dynamics and evolution randall et al 2007 at regional or local scales regional climate models rcms or statistical downscaling models are often used to provide future projections of climate variables and to assess the impacts of climate change on regional water resources mehrotra and sharma 2006 2010 vrac and naveau 2007 fowler et al 2007 graham et al 2007 forzieri et al 2014 reshmidevi et al 2017 steinfeld et al 2020 woldemeskel et al 2016 wood et al 2004 hu et al 2020 nguyen et al 2020 with the improvement in computing and data storage resources the climate models can now operate at finer resolution and with the advancement of our knowledge and better understanding of science and nature they offer a better representation of the physical processes embodied thus the recent past has witnessed improved capability and sophistication in gcms and rcms notwithstanding these improvements the climate model simulations still show biases particularly for variables dealing with the hydrological cycle navarro racines et al 2020 koutroulis et al 2016 papadimitriou et al 2017 maraun 2012 such biases come from varied sources the most obvious reasons are imperfect model representations of atmospheric physics maraun 2012 and incorrect initialization of the model or errors in the parameterization chain with respect to gcms nguyen et al 2016 theoretically finer resolution of regional climate models rcms should improve some of the physical processes and reduce biases as it allows topography land type and land use distribution to be more accurately represented in climate models however as gcms provide the forcing driving boundary conditions for the rcms significant biases can persist either from the driving gcm or from the rcm itself rocheta et al 2017 sippel et al 2016 troin et al 2015 nahar et al 2017 eden et al 2012 as a result it is important to bias correct the raw climate model outputs before their use in impact assessments studies piani et al 2010 mehrotra and sharma 2010 nguyen et al 2016 sarhadi et al 2016 bias correction can reduce gcm rcm biases and forms a necessary post processing step in almost all impact assessment studies that rely on the outputs of climate models according to murphy 1993 a proper bc should improve quality matching of model output and observations consistency matching of model dynamics output and our judgement and value meaningful model output for the benefit of the users of the raw model output bias correction algorithms vary from equalization of statistical characteristics between modelled and observed precipitation for example simple correction for means and variance wilby et al 2004 ghosh and mujumdar 2008 hay et al 2000 lenderink et al 2007 to more complex approaches designed to correct for quantiles popularly known as quantile mapping qm approaches li et al 2014 wood et al 2004 piani et al 2010 boé et al 2007 or variability and persistence attributes at multiple time scales haerter et al 2011 johnson and sharma 2011 and in space as well as across variables mehrotra and sharma 2015 2016 2019 vrac and friederichs 2015 the complexity of bc models increases with an increase in the type of attributes to be bias corrected time scales number of variables and locations included in general the model complexity increases if the intention is to look beyond a distribution biases i e dependence biases b single variable i e multivariate c single location i e multiple locations d single time scale i e time nesting and e their combinations the size of matrices grows exponentially with an increase in the number of variables locations and time nesting considered so how complex should a bc model be should a basic qm or mean and sd correction model be enough or a comprehensive bc model be selected how much complexity can be allowed before there is a risk of overfitting an excessive number of model parameters and the danger of altering the physical credibility of gcms model complexity is traditionally evaluated by splitting the data into two parts using one to develop the model while using the other to evaluate the model performance and to compare how the model performs during the evaluation phase as model complexity is increased this procedure is popularly known as a split sample test and the two parts are known as calibration and validation samples or time periods a more complex model in general is expected to perform better in calibration than a simpler model as it has got more parameters that can be fine tuned to obtain a good fit to the data used it might struggle in validation as multiple parameters will force the model to behave like the calibration period a simpler model on the other hand is expected to produce similar performances both in calibration and validation this idea is presented in fig 1 where a general relationship between model performance and model complexity is shown as can be seen there is a small window that defines a reasonable model performance in both calibration and validation along with an optimal level of complexity a function of the number of parameters of a model while dealing with the systematic biases in climate model simulations the consideration of a number of variables multiple locations time scales and attributes of interest further complicates the selection and assessment of a bc model structure like any other model increasing bc model complexity also increases overall agreement of bias corrected climate model output with observations during calibration current climate however as the calibrated model is applied to future climate simulations it cannot be validated and an overfit can easily provide escalated results it is of more concern when dealing with multivariate data sets or a single variable at multiple locations there is no simple feedback mechanism to assess the impact of model complexity on model performance in the future even performing the split sample test on the current climate data is of little help as real validation lies in assessing the model performance when it is applied to the future climate data that can exhibit significantly larger changes compared to what the historical record exhibits mehrotra and sharma 2019 as the climate system evolves with time the distribution of climate variables is also likely to change with time and along with the model biases maraun 2013 although almost all existing bc modelling approaches assume time invariant biases and there seems to be no simple way forward to account for non stationary bias in bc models nahar et al 2017 in majority of climate change impact studies the spatial temporal and multi variable attributes are often misrepresented by climate models the univariate approaches modify marginal distributions and leave other multi dimensional aspects largely unchanged mehrotra and sharma 2016 many derived hydrological variables such as flow soil moisture and groundwater levels are often a result of accumulated precipitation and or temperature anomalies over several days weeks or months covering the large areas therefore for hydrological impact studies a univariate bias correction approach is of limited use and the use of more comprehensive bias correction approaches is warranted in recent past many multi dimensional bc approaches aimed at correcting bias in time space and across variables have been proposed piani and haerter 2012 vrac and friederichs 2015 mehrotra and sharma 2015 2016 2019 parametric multivariate bias correction approaches and multivariate variants of qm for example n dimensional probability density function transform mbcn cannon 2018 and mrnbc or mrqnbc of mehrotra and sharma 2015 2016 and mehrotra et al 2018 are based on complex mathematical formulations and require estimation of many parameters with large matrices the intricacy of these approaches increases with an increase in the number of variables timescales and statistics to be corrected and to some extent draw a limitation on their use keeping these aspects in mind we propose here a self evolving robust bc model formulation that relies on the data itself and defines an optimal configuration within the mrqnbc modelling strategy mehrotra et al 2018 optimality being defined here in terms of model robustness in current and future model projections in the approach we start with a simple bc model and gradually bring in model complexity by testing validating and adding additional bc components for each biased statistic time scale location and variable in a stepwise manner following the response we obtain from the data used for current calibration and future validation climate the approach ensures that we have tested the need and usefulness of individual bc components to be applied for that statistic variable time scale and location and not just applying a bc following a pre fixed model structure selected the stepwise procedure checks the evolved model structure for stability and allows model structure complexity to grow only if it is required and justified by the data at each step we check the utility of bc in validation future climate another important principle of model development the final objective of this stepwise model building procedure is to have a final bc structure that performs well both on the data that is used to calibrate e g the observed and current climate and on the data that is used to validate the model bias correction of future climate projections following this the robust multivariate bias correction rombc software package has been developed in the r statistical computing environment it broadly follows the modelling strategy adopted in our earlier multivariate rigid bias correction approaches for example multivariate nested bias correction approach as described in mehrotra and sharma 2015 2016 and mehrotra et al 2018 the flexibility introduced avoids the need of specifying the timescales and statistics to be included at each time step of bias correction for calibration and validation thereby making it easier for the users to implement the approach in a fairly simple manner this paper describes the software package and provides a simple example of its applications 2 data used csiro and bureau of meteorology 2015 hereafter referred to as cb 2015 have prepared a technical report climate change in australia to help capture the range of projection results arising from the cmip5 database considering a large number of gcm simulations available under the cmip5 archive a proper selection of a sub set of models for use in impact assessment studies becomes important the selected model should be able to reproduce the major climatic features and modes of variability for example seasonal and annual cycles of rainfall and temperature although the ability of individual cmip5 models to simulate australian climate varies depending on the climatic variable region and season under consideration model selection is also influenced by the availability of relevant data since some climate variables were not archived for some models or emission scenarios considering model skill model genealogy and other relevant factors a subset of eight cmip5 models from a total of 40 was selected by cb2015 for use in climate change impact assessments the method used for selection of these models is described in chapter 9 of the technical report cb 2015 at the time of data collection out of these eight models data of six models was readily available on the cmip5 archive and has been used in the present research table 1 provides details of these six gcms the greater sydney region contains 18 sub catchments the daily time series of catchment averaged rainfall was formed using gridded data from the bureau of meteorology while daily time series of evaporation at three operational meteorological stations located within the region was provided by waternsw for the 1900 2013 time period 114 years in addition to observed data daily time series of gcm rainfall and temperature data for 30 years each for current 1976 2005 and future 2069 2099 climates for 6 gcms for rcp8 5 scenario over the study region is obtained and interpolated over the 18 sub catchments as evaporation was not directly available from these gcms a conditional model was developed using observed evaporation and temperature and daily time series of gcm evaporation was simulated conditional on the gcm temperature for current and future time periods observed gridded temperature was obtained from bureau of meteorology the australian climate observations reference network surface air temperature acorn sat dataset which has been developed to monitor climate variability and change in australia the dataset provides a daily record of australian temperatures since 1910 trewin 2018 3 methodology among the different univariate bias correction methods that have been suggested quantile mapping has been found to provide particularly stable results themeβl et al 2012 teutschbein and seibert 2012 and is increasingly used to bias correct the rcm gcm climate data all over the world e g finger et al 2012 forzieri et al 2014 the approach is capable of correcting for the biases in the daily distribution mean and in variance of the raw time series similarly multivariate nested bias correction mnbc approaches of mehrotra and sharma 2015 2016 and mehrotra et al 2018 are quite effective in correcting for dependence biases zhu and zhao 2018 following this we adopt daily qm as our base bc model and apply it at each location and to each variable separately more complex bc alternatives are added subsequently to this base model in a stepwise manner until the optimally robust configuration is reached more details on the approach are presented later the general structure of both qm and mnbc approaches is discussed in brief here 3 1 quantile mapping qm the bias correction formulation of qm is based on equation 1 following li et al 2010 1 z m f z ˇ m f f o 1 f m f z ˇ m f f m c 1 f m f z ˇ m f in which f is the empirical cumulative distribution of either observations o or model m for a historical training period or calibration or current climate c or future projection or validation period f z ˇ is the raw and z is the bias corrected time series the same notation is used hereafter to define pre bc or raw z ˇ and post bc or bias corrected z series also multivarite matrices and variables are expressed as bold while univariate and scalars are expressed as non bold characters 3 2 multivariate nested bias correction mnbc a full multivariate bc that maintains the observed lag1 and lag0 cross dependence in the bias corrected time series z t g is given by mehrotra and sharma 2015 2016 2 z t g c z t 1 g d f 1 z ˇ t g d f 1 e z ˇ t 1 g where z ˇ and z are raw and bias corrected time series of climate variables and matrices c e d and f are the lag0 and lag1 cross correlations of observed and raw current climate gcm series of variables expressions for the matrices c and e or d and f are obtained following matalas 1967 as follows 3 c or e m 1 m 0 1 and d d t or f f t m 0 m 1 m 0 1 m 1 t where m 0 and m 1 are respectively the lag0 and lag1 cross correlation matrices of observed raw current climate gcm daily time series of variables as appropriate d or f is found by singular value decomposition the elements of m 0 and m 1 corresponding to variables i and j are obtained from the observed raw current climate gcm time series z using equation 4 4a m 0 i j t 1 n z t i z t j n 4b m 1 i j t 1 n z t i z t 1 j n 1 similarly for monthly or seasonal periodic time series different matrices for each month season are used parameters of these periodic matrices are obtained using equation 5 following salas 1980 5 c τ or e τ m 1 τ m 0 τ 1 1 a n d d τ d τ t or f τ f τ t m 0 τ m 1 τ m 0 τ 1 1 m 1 τ t where τ represents month or season m 0 τ and m 1 τ are respectively the lag0 and lag1 cross correlation matrices of observed raw gcm monthly seasonal time series of variables as appropriate the elements of m 0 τ and m 1 τ are obtained using equation 4 in a manner similar to the case with constant parameters models of multivariate time series at multiple levels usually involve a large number of parameters to account for the cross and auto and lagged time dependences in situations where lagged cross correlations are either not important or significant a contemporaneous model with reduced number of parameters can be formed by considering matrices c and e as diagonal matrices and ignoring the lag1 cross correlations salas 1980 salas et al 1985 the diagonalisation of the parameter matrices c and e allows applying a univariate procedure for parameter estimation of these matrices the elements of c and e corresponding to variables i and j are expressed as 6a c i j or e i j m 1 i j if i j c i j or e i j 0 otherwise 6b d d i j t or f f i j t m 0 i j 1 m 1 i i m 1 j j 6c c τ i j or e τ i j m 1 τ i j if i j c τ i j or e τ i j 0 otherwise 6d d d τ i j t or f f τ i j t m 0 τ i j m 1 τ i i m 0 τ 1 i j m 1 τ j j similar to mnbc variants mehrotra and sharma 2012 2015 two different auto regressive multivariate models are considered the one with constant parameters for the daily and annual time series and another with periodic parameters for the monthly and seasonal time series salas 1980 if only lag0 cross correlations are of interest then the above equations are simplified as 7 z t g d f 1 z ˇ t g where z ˇ and z are pre and post bias corrected series and matrices d and f are the lag0 cross correlations of observed and gcm series the elements of d and f are obtained by using equation 4a similarly if only lag1 correlations are of interest then the multivariate correction is not required and a standard univariate autoregressive lag1 model for individual variable is considered johnson and sharma 2012 8 z t g r h z t 1 g 1 r h 2 z ˇ t g r m z ˇ t 1 g 1 r m 2 where z t g is the bias corrected time series for time step t r h is the observed and r m is the gcm time series lag1 correlations 3 3 robust multivariate bias correction rombc the modelling strategy proposed here is termed as robust mbc rombc we describe the primary statistical attributes using distribution statistics and dependence attributes using the lag0 and lag1 auto and cross correlations similar to other variants of a multivariate bc model the rombc bias correction considers four popular bias correction time scales daily monthly quarterly and annual the model structure evolution procedure operates in stages from univariate to multivariate and from one time scale to the next at each time scale the approach evaluates the reparations of bias correction application in stages first in lag1 auto dependence of the individual variables and thereafter for lag0 cross dependence across variables for both calibration and validation current and future time periods as mentioned before the first stage of the approach is to apply univariate qm at a daily time scale and univariate variance correction at higher aggregated time scales individually to all variables and locations this forms the base bias corrected time series to be used as a reference to assess the need for the more complex bc alternatives that are assessed next the next stage is to examine the necessity and the applicability of bc in dependence attributes at daily monthly seasonal and annual time scales the dependence attributes are defined in terms of lag1 autocorrelation and lag0 cross correlation attributes at all four time scales the aim here is to specify a bc model structure that is appropriate at that time scale the assessment of dependence attributes in bias correction is conducted in a stage wise manner at each time scale the sequence of these bc stages includes lag1 lag0 a contemporaneous l1c and finally a full model l1f for lag1 only dependence being a univariate correction the procedure is conducted separately at each location and for each variable and the bc model varies across variables while for cross dependence joint collective assessment is undertaken at each time scale the final bc is applied only when the assessment at all four stages is completed at each bc stage the assessment is performed in two steps that are designed to be intuitive and straightforward in the first step called hereafter as necessity check the base time series is evaluated to assess if the bc procedure being considered is necessary by comparing the current climate dependence statistics with those representing the observed record if this difference is found statistically unimportant there is no need of applying the bias correction in the dependence attribute considered and we proceed on to the next stage the second step of checking the applicability of bc in validation future is called the applicability check and is initiated only if the difference is found significant at the first step in the second step the bc is applied to the future climate time series and the bias corrected time series is assessed to see if the application of bc brings in significant changes in extreme values or designated statistics in comparison to those obtained using the base case simulation if these changes are found statistically significant the bc procedure is ignored and the correction at the next stage is considered the following describes in brief the two step criteria adopted 3 3 1 necessity check the structure of the necessity check procedure remains the same for all bc stages for lag1 this involves assessing lag1 correlations of individual variables while for lag0 these represent cross correlations across variables for brevity both are simply denoted as correlations here for a given location and variable let the correlations of observed and gcm current climate series be denoted as ro and rg respectively for daily series these are calculated for each day of the year using a moving window of 31 days centred on the day of interest the user is allowed to change the length of the moving window through a parameter in the data file the significance of difference of observed and current climate correlations is assessed using fisher z test statistic as shown in equation 9 at 5 level of significance significant if fisher z value 1 96 in the equation n o and n g respectively are the number of observed and gcm data points used to calculate correlations 9 f z z o z g 1 n o 3 1 n g 3 and z o 1 2 l n 1 r o 1 r o and z g 1 2 l n 1 r g 1 r g with daily data the process is repeated for all 365 calendar days while for monthly and seasonal data for each month season if out of 365 days 12 months 4 seasons this difference is found statistically significant for more than 1 of time bias correction of correlation is assumed to be needed this is denoted as the necessity check note that the 5 level of significance is a common choice in hydrology while threshold of 1 of time was picked following a sensitivity analysis by varying it from 0 05 to 5 and 1 was found to provide satisfactory performance on the data used the second step is applicability check and is explained next 3 3 2 applicability check if the necessity check suggests that a dependence correction is needed the next step is to check the impact of the dependence correction on the future climate time series the dependence correction is applied to the base future climate series and the percent of time the bias corrected values cross designated lower or upper practical limits is noted also the means avs and standard deviations sds of the pre and post bias corrected series are calculated and compared to check if the correction has made any significant changes in the av or sd of the time series by using equation 10 again at 5 level of significance 10a a v p o s t a v p r e 1 96 s d p r e 2 s d p o s t 2 n 10b i f s d p r e s d p o s t s d p r e 2 s d p o s t 2 1 96 i f s d p o s t s d p r e s d p o s t 2 s d p r e 2 1 96 here n is number of data points and pre and post subscripts represent the statistics before and after the application of bias correction these equations check the significance of the differences of statistics at the 5 significance level if upper and lower limits are crossed more than 1 of time or the difference of the statistics equation 10 exceeds the specified threshold by more than 1 this bc model is not considered our aim here is to make sure that we do not allow the bc to change the future irrationally and end up having few very high low values as mentioned before the thresholds at 95 level of confidence is a commonly used choice while the 1 of time is used to define a check on the bc procedure as it would not be violated under normal conditions the 1 threshold chosen was found to perform well based on sensitivity assessments across gcms and a range of variables being corrected once all the correction stages at a given time scale are assessed the final selected bias correction model is applied at that time scale the bias corrected time series is then aggregated averaged to the next time scale and the same procedure is repeated the time scales adopted and statistical attributes considered represent common choices the developers and other researchers have found important for water resources applications the approach is quite flexible and allows users to accommodate alternate representations of time scales as well as other statistical attributes johnson and sharma 2012 mehrotra and sharma 2012 2015 the following describes the stepwise procedure adopted in the implementation of rombc 3 4 stepwise rombc procedure the complete bias correction procedure is divided into three parts part a deals with the formulation of base series in the form of a univariate primary bias correction part b is core of rombc and deals with the checking and application of complex bias correction procedures at each time scale part c aggregates the time series to higher time scale and repeats the part b steps involved in these parts are discussed next 3 6 1 part a defining the primary base bc series 1 calculate monthly seasonal and annual means and standard deviations of all the variables of the observed z t h time series also calculate daily monthly seasonal and annual lag1 auto and lag0 and lag1 cross correlations of variables use a moving window of 31 days or the number of days as specified by the user centred on the current day of interest while calculating the statistics for the daily data rajagopalan and lall 1999 sharma and lall 1999 2 consider a variable at a location grid point apply qm to the daily data by fitting an empirical cumulative distribution functions cdfs to the observed z t h and raw gcm series z ˆ t for current and future climate for a given value in the future climate gcm series calculate the cumulative probability and obtain the difference of observed and gcm current climate values for this cumulative probability from the corresponding cdfs bias obtain the corresponding value for this cumulative probability from the future climate cdf apply the difference to the value to obtain the bias corrected value for the future climate repeat the same procedure for every data point and obtain the bias corrected daily time series for current and future climates note these form univariate corrections for each variable with no consideration is given to cross dependence biases that may be present 3 aggregate the daily qm corrected time series to monthly time scale for standard deviation sd correction assess the difference of observed and current climate monthly sds using equation 10b if this difference is found statistically significant for more than 1 of time bias correction of sd is needed this is denoted as the necessity check 4 for the applicability check apply sd bias correction to future monthly time series check the significance of the sd corrections by noting the percent of time the corrected monthly values cross the theoretical lower and upper limits also count the percent of times avs and sds of post bc series are different from pre bc raw aggregated monthly series using equation 10 if series passes the applicability check apply sd correction to both current c and future f daily time series 11a y j i k g c y ˆ j i k g c μ j k g c σ j k h σ j k g c μ j k g c 11b y j i k g f y ˆ j i k g f μ j k g f σ j k h σ j k g c μ j k g f where μ j k g c is mean of current μ j k g f is mean of future σ j k h is sd of observed and σ j k g c is sd of current climate time series for jth month and kth variable similarly y ˆ j i g c and y j i k g c and y ˆ j i g f and y j i k g f are monthly time series before and after sd correction for current and future climate for kth variable jth month and ith year 5 aggregate both current and future climate time series monthly bias corrected time series to seasonal and annual time scales and check for the applicability of sd correction 6 finally incorporate the changes at all time scales by modifying the daily time series as follows 12 z t j s i k g y j s i k g y ˆ j s i k g x x s i k g x ˆ s i k g x a i k g a ˆ i k g x z t j s i k g where y j s i k g is the monthly corrected value y ˆ j s i k g the aggregated monthly value x s i k g the seasonal corrected value x ˆ s i k g the aggregated seasonal value a i k g the yearly corrected value and a ˆ i k g the aggregated yearly value in equation 12 subscript k stands for variable t for day j for month s for season and i for year do it for both current and future climate time series 7 store the daily distribution and higher time scales sd corrected time series of individual variables y j s i k g for current and future climate 8 repeat steps 1 7 for other variables and locations the above steps form the base or reference bias corrected time series which is corrected for essential biases in daily distribution and variability at monthly seasonal and annual time scales the base series is now used to define the practical lower and upper limits on the data and forms the starting step to test and apply more complex dependence attributes based bc models 4 in all lag1 lag0 contemporaneous and full it should be noted that if a more complex bc model is accepted to be valid the base bias corrected time series at that time scale is updated only at the end of the fourth stage to define a new reference the practical lower and upper limits are used to additionally validate the advanced bias correction stages the daily monthly seasonal and annual upper and lower limits on individual variables at all locations for both current and future climates are formed by calculating the standard deviations sds and maximum and minimum values of the entire time series at all four time scales the maximum limit is defined as the maximum value in the entire time series plus sd and minimum limit as minimum value in the entire time series minus sd as per the following 13a maximum limit max y g sd y g 13b minimum limit min y g sd y g for current and future climate at each time scale location and for all variables the daily limits are further checked against the physical lower and upper limits specified by the user for example with rainfall as a variable the lower physical limit is zero and if the practical limit given by equation 13b is less than zero it is set as zero the next stage is to examine the necessity and applicability of bc in dependence attributes at daily monthly seasonal and annual time scales the aim here is to identify a suitable bc model structure that is appropriate at that time scale a flow chart presented in fig 2 highlights the procedure adopted in part b the steps involved are as follows 3 6 2 part b assessing checking and applicability of dependence bc model structure at each time scale 9 start with daily data consider gcm current and future climates daily base time series as obtained from step 8 consider each variable and calculate lag1 auto correlations of observed and gcm current climate series 10 for each day of the year assess the difference of observed and current climate lag1 auto correlations using equation 9 if this difference is found statistically significant for more than 1 of time bias correction of lag 1 auto correlation is needed this is denoted as the necessity check 11 for the applicability check apply lag1 auto bias correction to future daily time series using equation 8 and check the significance of the bias correction applicability check by noting the percent of time the corrected daily values have crossed the lower and upper limits also count the percent of times means and sds of post bc series are different from pre bc base series using equation 10 if series passes the applicability check the lag1 bias correction forms as our plausible bc model for this variable and time scale 12 repeat the above steps for other variables store the results of all variables 13 next assess the need for lag0 cross dependence correction calculate observed and current climate lag0 cross correlation matrices of base model corrected daily time series considering all variables repeat step 10 and assess the need of bias correction 14 if bc is needed then form future climate lag0 bias corrected series by using equation 7 repeat step 11 to check for the statistical significance of the changes note if the changes are statistically significant or not move to the next stage 15 check the need of a contemporaneous bc model l1c calculate lag0 and lag1 cross correlations of observed and daily current climate base time series considering all variables locations repeat step 10 and assess the need of bias correction considering lag0 and lag1 auto correlations if test suggests the need of a bias correction go to next step otherwise move to next stage 16 calculate lag0 and lag1 correlation matrices of observed and daily current climate base time series considering all variables locations using equations 4 and 6 apply bias correction to future climate series using equation 2 17 repeat step 11 to check for the statistical significance of the changes note if the changes are statistically significant or not 18 check the need of a full bc model l1f by repeating the procedure mentioned in steps 15 17 19 now assessment of all 4 bce models at daily time scale is finished if no model is suggested do not apply any correction and move to the next time scale if a full l1f or contemporaneous model l1c is picked just apply that model to both current and future climate time series and move to the next time scale if a lag0 model is suggested apply lag0 model and see for the applicability of lag1 model for individual variables 3 6 3 part c aggregating series and assessing optimal bc model structure for the next aggregated time scale 20 aggregate the time series to higher time scale s and check for the necessity and applicability of all the four bc model structures by following the procedure specified in steps 9 to 19 21 incorporate the corrections at all time scales into the daily series by using the aggregated and bias corrected time series at monthly seasonal and annual time scales and equation 12 it should be noted that if there are no significant biases in auto or cross dependence attributes at the original daily time scale or if the correction results in significant changes to the future climate simulation the base model would be retained and will be used to form the time series at the aggregated time scale part c if similar outcomes result at aggregated time scales the end model will be the base model defined in part a also note that the auto correlation corrections can differ from variable to variable creating corrected time series that have been processed using the minimal complexity model that is applicable 4 results we apply univariate qm multivariate quantile based nbc hereafter called as mbc and rombc to the daily rainfall and evaporation time series of 6 gcms at 21 locations including 18 rainfall and 3 evaporation points stations over the greater sydney region for mbc single iteration with qm correction at daily and sd and l1c bias corrections at daily monthly seasonal and annual time scales was chosen for gcm current climate a 30 year time window spanning over 1976 2005 and for the future climate three 30 year time windows from 2010 to 2039 2040 2069 and 2070 2099 are considered for space limitation before presenting the overall results of all gcms we present and discuss detailed results for one representative gcm access only for one time window 2070 2099 centred around 2085 table 2 presents the finalised structure of the flexible bias correction model in the table correction criteria are shown by zeros and ones a one with a star 1 implies that the statistic is directly applied or is built in as a part of model structure a zero 0 implies bias correction for that statistic is not needed while one 1 implies correction is needed as per the current climate negative one 1 implies that while the correction is necessary application of bias correction to the future climate time series makes the changes significant and therefore the correction is not applied some specific findings can be drawn from table 1 for all locations and variables daily lag0 and lag1 dependence attributes are significantly different in the raw gcm series for the current climate and hence require corrections however as bias correction changes the statistics of the future time series quite significantly the correction is ignored lag1 correction to evaporation time series was needed and applied monthly and seasonal lag0 and l1c statistics require corrections and correction for l1c is applied as it also includes correction for lag0 statistics at annual time scale does not require any corrections thus the flexible model suggests a bc structure which is more complex than a traditional qm however is much simplified than a rigid multivariate bc mbc although is does involve a complex full model l1f structure in the model identification exercise figs 3 and 4 present a comparison of the three approaches in the form of scatter plots of selected observed and bias corrected statistics of general interest for current and future climates respectively for access gcm considering current climate results fig 3a the mean is reproduced well by all the models top two rows fig 2a annual sds are also well reproduced by all the models except for the sd at one location by qm row 3 fig 3a other statistics of interest for example daily maxima and annual 5th percentile values and lowest annual 3 5 and 7 years totals are also well reproduced by all the three models considered rows 4 6 fig 3a fig 3b presents a few distribution and dependence statistics of observed and bc results lag1 and lag0 cross correlations are better reproduced by mbc as expected rombc results are better than qm but not as good as mbc more specifically for annual statistics considering results for the current climate selecting mbc would be a reasonable choice bottom two rows of fig 3b present daily and annual rainfall distribution of observed and models simulated time series all models are able to reproduce daily and annual distribution behaviour of observed rainfall as qm is applied to daily data only reproduction of observed annual distribution in the bc series indicates relatively good quality of raw gcm data over the study region for this statistic now consider results for future climate as presented in fig 4 for access gcm for the time window 2070 2099 mean changes are shown in the top two rows of fig 4a annual rainfall shows almost no change while the three evaporation stations top circles in the plots show increases in annual evaporation seasonal rainfall shows increases in spring and summer while slight decreases in autumn and winter seasons all bc models project similar increases in the annual and seasonal means annual sds show some scatter with qm projecting slight under estimation of the statistic 3rd row fig 4a daily maxima and annual 5th percentiles and lowest annual averages show no changes for rainfall and increases for evaporation values all models project similar results 4th 6th rows fig 4a top two rows of fig 4b present scatter plots of lag1 and lag0 cross correlations all models show some scatter for these statistics more specifically at the annual time scale daily and annual distribution of rainfall at a representative station 1 shows no notable changes albeit a few extreme daily values by mbc bottom two rows fig 4b these results indicate no substantial loss of information if we selectively apply bias correction using rombc we now look at the projected changes in the average and extreme rainfall statistics in the future considering all gcms it may be noted that the climate models exhibit high variations across them in the projected changes with miroc projecting increases in rainfall and cnrm and gfdl projecting decreases in rainfall in the future over the study area table 3 presents the percent changes in annual rainfall and evaporation averaged across all gcms for all catchments and for three time windows similarly fig 5 presents changes in annual rainfall annual wet days lowest 7 years totals and daily maximum rainfall as projected by all six gcms by 2085 over all catchments using all bc models percent changes are derived by comparing the changes in the future with respect to current climate in the figure x axis shows all 18 catchments considered whereas on y axis percent changes are plotted similarly statistics of individual gcms are shown as thin lines of no changes as black dotted lines and gcms averaged values as thick black lines lines across catchments are joined for the sake of presentation only considering the models averaged results as presented in table 3 and fig 5 all bc models project around 1 4 percent decrease in rainfall during 2010 2039 a similar percent increase during 2040 2069 and again a similar percent decrease during 2070 2099 time periods over the study region top row of fig 5 evaporation stations show around 5 increase during 2010 39 11 12 during 2040 69 and 22 25 increase during 2070 99 time periods percent changes in bias corrected rainfall and evaporation results are in line with those projected by the raw gcms table 3 changes are in general consistent across catchments and gcms with miroc being the wet model and gfdl and cnrm being the dry ones fig 5 all bc models project around 5 increase in the number of wet days in a year over the study catchments by 2085 second row fig 5 fig 5 also includes the changes in daily maximum rainfall by 2085 third row fig 5 qm projects no appreciable changes in the catchment averaged daily maximum rainfall over the study region mbc projects 15 while rombc projects about 10 increase in the daily extreme rainfall in the future over the study catchments as mentioned before the rigid application of bc in mbc might force a few data points to take high or low values in order to match the observed dependence characteristics rombc checks for this possibility at each time scale before applying bc and possibly avoids such instances percent changes in the lowest 7 year rainfall are presented in the last row of fig 5 qm shows no changes mbc around 10 15 while rombc projects around 10 decreases in the statistic by 2085 over the study region this statistic kind of represents the low frequency behaviour of the time series and is important for water resources management and water availability related applications as qm is applied only at daily time scale it is insensitive to the biases in the low frequency variability fig 5 presents the distributional changes of the catchments averaged time series formed by taking the average of catchment rainfall and evaporation over the region top two rows present the temporal distribution of monthly rainfall and evaporation while the changes in the probability distribution of area averaged daily and annual rainfall are presented in the bottom two rows all bc models projects rainfall to increase in summer and decrease in spring no shift of season is noted monthly evaporation shows lowest increase in june and maximum increase in november surprisingly this increase does not occur during summer months perhaps increase in summer rainfall and more rainy days causes the evaporation to be lower on the rainy days distribution of area averaged daily rainfall only extreme values is shown in the third row of fig 6 qm does not show any significant changes in the extreme daily values in relation to the observed values rombc results are in between qm and mbc with a mild increase in very extreme daily values by 2085 considering distribution of area averaged annual rainfall last row of fig 6 qm projects negligible changes in the shape of the annual rainfall distribution with a slight reduction in the higher quantile rainfall by 2085 mbc and rombc project a few more dry years with minor changes in the shape of the distribution at both lower and higher ends 4 1 rombc details rombc is implemented in a r shell and allows the bias correction approach to be applied in a fairly simple manner 4 1 1 input data the software requires information about data in the form of four files in a specific format these include observed and raw data files for calibration current climate as well as verification future climate time periods when dealing with gcm current and future climates data the package uses three files observed and gcm rcm current and future climates raw data files in this case the observed verification period file will be same as observed calibration period file in this set up the observed data is used to compare the changes in each variable in the future it is not necessary to have equal length of data for raw and observed file either for calibration or verification periods users are allowed to define their own seasons in addition to the names of the four data files all other general information is provided through the basic dat file table 4 it includes the information about the number of years of data number of variables width of moving window used to correct the daily data physical lower and upper limits on the variables whether data consider leap years or not and the split of calendar months across the seasons being modelled all the information is provided in a free format separated by spaces at present the package allows for a maximum of 150 years of daily data 30 variables 12 seasons and 31 day moving window 4 1 2 package outputs upon successful completion of the program 6 output files are generated two files contain the bias corrected time series for the current and future time periods remaining four files contain a few common statistics of the observed raw and bias corrected data for the current and future climate as per the followings 1 observed and raw data for current climate 2 observed and raw data for future climate 3 observed and bias corrected data for current climate and 4 observed and bias corrected data for future climate time periods as for gcm rcm future climate data corrections the observed file would be same as the observed file for current climate the observed statistics would not change while we move from current to future climate statistics considered include means standard deviations skewness lag1 and lag2 auto correlations when multiple variables or locations are corrected then auto and lag1 cross correlations are also computed the package allows the users to look at raw and bias corrected statistics either in the form of a table or as plots at multiple time scales of interest finally the package also provides plots of the empirical cumulative probability distributions of the observed and raw and observed and bias corrected time series 5 discussion and conclusions bias correction has now become a standard post processing procedure to correct systematic biases and convert climate model raw output to one that is suitable for use in climate change impact assessment studies the majority of existing multivariate bias correction approaches work on a pre defined rigid bias correction model structure without looking into the magnitude and nature of biases and their behaviour in the future this study presented a novel approach for specifying the optimal structure of a multivariate bias correction model based on the premise that a pre defined fixed bias correction structure does not apply when biases are being assessed across time scales variables and dependence attributes and if imposed such a bias correction model can provide unstable and physically incompatible projections for the future where the impact of unneeded structural complexity will be most evident given this the approach adopted resided on specifying a base or reference bias correction model and updating this reference to a new model only if a systematic biases are noted in the simulations representing the observed period using the reference model and b an updated bias correction model that addresses the systematic biases in a does not in turn lead to projections of the future that are untenable only if these conditions are satisfied is the bias correction model updated and the process repeated to extend to all time scales and variables being modelled while models have long been formulated with limited data for application in scenarios that have not been observed in most cases these models are developed assuming the observational record used in their testing and validation exhibits stationarity as our situation is one where the future can be expected to change significantly at least for temperature and hence for evaporation forming a robust model requires an added means for identifying one that will exhibit stability into the future what is different in our approach here is the use of statistics to quantify instability which is performed by defining a base case and discarding model formulations that deviate significantly from this reference while the statistics we have chosen here are relevant in the water resources context the choice of attributes that define stability is one that should reside with the user based on the applications the climate model simulations are intended for an open source software in r statistical computing environment is presented here it provides an easy means to apply an in built flexible multivariate and multi timescale bias correction alternative that is self evolving and grows in complexity following the requirement of the raw data applications of the software along with information about the capabilities of the software are demonstrated using a sample dataset it is anticipated that the ease of running the software and the flexibility of exercising a wide variety of options will make it popular for practitioners carrying out impact assessments and researchers investigating downscaling methods software availability name of software package rombc developers raj mehrotra wrc civil and env engg unsw sydney e mail address raj mehrotra unsw edu au ashish sharma wrc civil and env engg unsw sydney e mail address a sharma unsw edu au year first available 2021 hardware required standard pc for windows software required rgui or r studio availability and cost available free of charge software along with sample data and help file can be downloaded from the following website http www hydrology unsw edu au download software programme language written in r and fortran declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research is partly funded by waternsw nsw australia appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105019 
25863,the existing bias correction bc methods used in impact studies are routinely based on a fixed model structure and often ignore the nature and magnitude of biases and their variations into the future as a calibrated model is applied to bias correct the future time series there is no feedback mechanism to assess the impact of model complexity on the model performance in the future in this paper we propose a flexible modelling strategy to create a robust bias correction procedure in the form of an open source toolkit in the r statistical computing environment the approach allows the user to apply a multi dimensional bias correction model that is self evolving and grows in complexity on the basis of the requirement of the raw data the theoretical background and the capabilities of the software along with a sample application and results discussions are demonstrated in this paper keywords bias correction r package persistence multivariate distribution based quantile matching nested bias correction model complexity self evolving robust bias correction 1 introduction quantification of the effects of climate change is currently one of the most debatable and challenging topics in science global circulation models gcms are considered as the best tools to understand earth s climate dynamics and evolution randall et al 2007 at regional or local scales regional climate models rcms or statistical downscaling models are often used to provide future projections of climate variables and to assess the impacts of climate change on regional water resources mehrotra and sharma 2006 2010 vrac and naveau 2007 fowler et al 2007 graham et al 2007 forzieri et al 2014 reshmidevi et al 2017 steinfeld et al 2020 woldemeskel et al 2016 wood et al 2004 hu et al 2020 nguyen et al 2020 with the improvement in computing and data storage resources the climate models can now operate at finer resolution and with the advancement of our knowledge and better understanding of science and nature they offer a better representation of the physical processes embodied thus the recent past has witnessed improved capability and sophistication in gcms and rcms notwithstanding these improvements the climate model simulations still show biases particularly for variables dealing with the hydrological cycle navarro racines et al 2020 koutroulis et al 2016 papadimitriou et al 2017 maraun 2012 such biases come from varied sources the most obvious reasons are imperfect model representations of atmospheric physics maraun 2012 and incorrect initialization of the model or errors in the parameterization chain with respect to gcms nguyen et al 2016 theoretically finer resolution of regional climate models rcms should improve some of the physical processes and reduce biases as it allows topography land type and land use distribution to be more accurately represented in climate models however as gcms provide the forcing driving boundary conditions for the rcms significant biases can persist either from the driving gcm or from the rcm itself rocheta et al 2017 sippel et al 2016 troin et al 2015 nahar et al 2017 eden et al 2012 as a result it is important to bias correct the raw climate model outputs before their use in impact assessments studies piani et al 2010 mehrotra and sharma 2010 nguyen et al 2016 sarhadi et al 2016 bias correction can reduce gcm rcm biases and forms a necessary post processing step in almost all impact assessment studies that rely on the outputs of climate models according to murphy 1993 a proper bc should improve quality matching of model output and observations consistency matching of model dynamics output and our judgement and value meaningful model output for the benefit of the users of the raw model output bias correction algorithms vary from equalization of statistical characteristics between modelled and observed precipitation for example simple correction for means and variance wilby et al 2004 ghosh and mujumdar 2008 hay et al 2000 lenderink et al 2007 to more complex approaches designed to correct for quantiles popularly known as quantile mapping qm approaches li et al 2014 wood et al 2004 piani et al 2010 boé et al 2007 or variability and persistence attributes at multiple time scales haerter et al 2011 johnson and sharma 2011 and in space as well as across variables mehrotra and sharma 2015 2016 2019 vrac and friederichs 2015 the complexity of bc models increases with an increase in the type of attributes to be bias corrected time scales number of variables and locations included in general the model complexity increases if the intention is to look beyond a distribution biases i e dependence biases b single variable i e multivariate c single location i e multiple locations d single time scale i e time nesting and e their combinations the size of matrices grows exponentially with an increase in the number of variables locations and time nesting considered so how complex should a bc model be should a basic qm or mean and sd correction model be enough or a comprehensive bc model be selected how much complexity can be allowed before there is a risk of overfitting an excessive number of model parameters and the danger of altering the physical credibility of gcms model complexity is traditionally evaluated by splitting the data into two parts using one to develop the model while using the other to evaluate the model performance and to compare how the model performs during the evaluation phase as model complexity is increased this procedure is popularly known as a split sample test and the two parts are known as calibration and validation samples or time periods a more complex model in general is expected to perform better in calibration than a simpler model as it has got more parameters that can be fine tuned to obtain a good fit to the data used it might struggle in validation as multiple parameters will force the model to behave like the calibration period a simpler model on the other hand is expected to produce similar performances both in calibration and validation this idea is presented in fig 1 where a general relationship between model performance and model complexity is shown as can be seen there is a small window that defines a reasonable model performance in both calibration and validation along with an optimal level of complexity a function of the number of parameters of a model while dealing with the systematic biases in climate model simulations the consideration of a number of variables multiple locations time scales and attributes of interest further complicates the selection and assessment of a bc model structure like any other model increasing bc model complexity also increases overall agreement of bias corrected climate model output with observations during calibration current climate however as the calibrated model is applied to future climate simulations it cannot be validated and an overfit can easily provide escalated results it is of more concern when dealing with multivariate data sets or a single variable at multiple locations there is no simple feedback mechanism to assess the impact of model complexity on model performance in the future even performing the split sample test on the current climate data is of little help as real validation lies in assessing the model performance when it is applied to the future climate data that can exhibit significantly larger changes compared to what the historical record exhibits mehrotra and sharma 2019 as the climate system evolves with time the distribution of climate variables is also likely to change with time and along with the model biases maraun 2013 although almost all existing bc modelling approaches assume time invariant biases and there seems to be no simple way forward to account for non stationary bias in bc models nahar et al 2017 in majority of climate change impact studies the spatial temporal and multi variable attributes are often misrepresented by climate models the univariate approaches modify marginal distributions and leave other multi dimensional aspects largely unchanged mehrotra and sharma 2016 many derived hydrological variables such as flow soil moisture and groundwater levels are often a result of accumulated precipitation and or temperature anomalies over several days weeks or months covering the large areas therefore for hydrological impact studies a univariate bias correction approach is of limited use and the use of more comprehensive bias correction approaches is warranted in recent past many multi dimensional bc approaches aimed at correcting bias in time space and across variables have been proposed piani and haerter 2012 vrac and friederichs 2015 mehrotra and sharma 2015 2016 2019 parametric multivariate bias correction approaches and multivariate variants of qm for example n dimensional probability density function transform mbcn cannon 2018 and mrnbc or mrqnbc of mehrotra and sharma 2015 2016 and mehrotra et al 2018 are based on complex mathematical formulations and require estimation of many parameters with large matrices the intricacy of these approaches increases with an increase in the number of variables timescales and statistics to be corrected and to some extent draw a limitation on their use keeping these aspects in mind we propose here a self evolving robust bc model formulation that relies on the data itself and defines an optimal configuration within the mrqnbc modelling strategy mehrotra et al 2018 optimality being defined here in terms of model robustness in current and future model projections in the approach we start with a simple bc model and gradually bring in model complexity by testing validating and adding additional bc components for each biased statistic time scale location and variable in a stepwise manner following the response we obtain from the data used for current calibration and future validation climate the approach ensures that we have tested the need and usefulness of individual bc components to be applied for that statistic variable time scale and location and not just applying a bc following a pre fixed model structure selected the stepwise procedure checks the evolved model structure for stability and allows model structure complexity to grow only if it is required and justified by the data at each step we check the utility of bc in validation future climate another important principle of model development the final objective of this stepwise model building procedure is to have a final bc structure that performs well both on the data that is used to calibrate e g the observed and current climate and on the data that is used to validate the model bias correction of future climate projections following this the robust multivariate bias correction rombc software package has been developed in the r statistical computing environment it broadly follows the modelling strategy adopted in our earlier multivariate rigid bias correction approaches for example multivariate nested bias correction approach as described in mehrotra and sharma 2015 2016 and mehrotra et al 2018 the flexibility introduced avoids the need of specifying the timescales and statistics to be included at each time step of bias correction for calibration and validation thereby making it easier for the users to implement the approach in a fairly simple manner this paper describes the software package and provides a simple example of its applications 2 data used csiro and bureau of meteorology 2015 hereafter referred to as cb 2015 have prepared a technical report climate change in australia to help capture the range of projection results arising from the cmip5 database considering a large number of gcm simulations available under the cmip5 archive a proper selection of a sub set of models for use in impact assessment studies becomes important the selected model should be able to reproduce the major climatic features and modes of variability for example seasonal and annual cycles of rainfall and temperature although the ability of individual cmip5 models to simulate australian climate varies depending on the climatic variable region and season under consideration model selection is also influenced by the availability of relevant data since some climate variables were not archived for some models or emission scenarios considering model skill model genealogy and other relevant factors a subset of eight cmip5 models from a total of 40 was selected by cb2015 for use in climate change impact assessments the method used for selection of these models is described in chapter 9 of the technical report cb 2015 at the time of data collection out of these eight models data of six models was readily available on the cmip5 archive and has been used in the present research table 1 provides details of these six gcms the greater sydney region contains 18 sub catchments the daily time series of catchment averaged rainfall was formed using gridded data from the bureau of meteorology while daily time series of evaporation at three operational meteorological stations located within the region was provided by waternsw for the 1900 2013 time period 114 years in addition to observed data daily time series of gcm rainfall and temperature data for 30 years each for current 1976 2005 and future 2069 2099 climates for 6 gcms for rcp8 5 scenario over the study region is obtained and interpolated over the 18 sub catchments as evaporation was not directly available from these gcms a conditional model was developed using observed evaporation and temperature and daily time series of gcm evaporation was simulated conditional on the gcm temperature for current and future time periods observed gridded temperature was obtained from bureau of meteorology the australian climate observations reference network surface air temperature acorn sat dataset which has been developed to monitor climate variability and change in australia the dataset provides a daily record of australian temperatures since 1910 trewin 2018 3 methodology among the different univariate bias correction methods that have been suggested quantile mapping has been found to provide particularly stable results themeβl et al 2012 teutschbein and seibert 2012 and is increasingly used to bias correct the rcm gcm climate data all over the world e g finger et al 2012 forzieri et al 2014 the approach is capable of correcting for the biases in the daily distribution mean and in variance of the raw time series similarly multivariate nested bias correction mnbc approaches of mehrotra and sharma 2015 2016 and mehrotra et al 2018 are quite effective in correcting for dependence biases zhu and zhao 2018 following this we adopt daily qm as our base bc model and apply it at each location and to each variable separately more complex bc alternatives are added subsequently to this base model in a stepwise manner until the optimally robust configuration is reached more details on the approach are presented later the general structure of both qm and mnbc approaches is discussed in brief here 3 1 quantile mapping qm the bias correction formulation of qm is based on equation 1 following li et al 2010 1 z m f z ˇ m f f o 1 f m f z ˇ m f f m c 1 f m f z ˇ m f in which f is the empirical cumulative distribution of either observations o or model m for a historical training period or calibration or current climate c or future projection or validation period f z ˇ is the raw and z is the bias corrected time series the same notation is used hereafter to define pre bc or raw z ˇ and post bc or bias corrected z series also multivarite matrices and variables are expressed as bold while univariate and scalars are expressed as non bold characters 3 2 multivariate nested bias correction mnbc a full multivariate bc that maintains the observed lag1 and lag0 cross dependence in the bias corrected time series z t g is given by mehrotra and sharma 2015 2016 2 z t g c z t 1 g d f 1 z ˇ t g d f 1 e z ˇ t 1 g where z ˇ and z are raw and bias corrected time series of climate variables and matrices c e d and f are the lag0 and lag1 cross correlations of observed and raw current climate gcm series of variables expressions for the matrices c and e or d and f are obtained following matalas 1967 as follows 3 c or e m 1 m 0 1 and d d t or f f t m 0 m 1 m 0 1 m 1 t where m 0 and m 1 are respectively the lag0 and lag1 cross correlation matrices of observed raw current climate gcm daily time series of variables as appropriate d or f is found by singular value decomposition the elements of m 0 and m 1 corresponding to variables i and j are obtained from the observed raw current climate gcm time series z using equation 4 4a m 0 i j t 1 n z t i z t j n 4b m 1 i j t 1 n z t i z t 1 j n 1 similarly for monthly or seasonal periodic time series different matrices for each month season are used parameters of these periodic matrices are obtained using equation 5 following salas 1980 5 c τ or e τ m 1 τ m 0 τ 1 1 a n d d τ d τ t or f τ f τ t m 0 τ m 1 τ m 0 τ 1 1 m 1 τ t where τ represents month or season m 0 τ and m 1 τ are respectively the lag0 and lag1 cross correlation matrices of observed raw gcm monthly seasonal time series of variables as appropriate the elements of m 0 τ and m 1 τ are obtained using equation 4 in a manner similar to the case with constant parameters models of multivariate time series at multiple levels usually involve a large number of parameters to account for the cross and auto and lagged time dependences in situations where lagged cross correlations are either not important or significant a contemporaneous model with reduced number of parameters can be formed by considering matrices c and e as diagonal matrices and ignoring the lag1 cross correlations salas 1980 salas et al 1985 the diagonalisation of the parameter matrices c and e allows applying a univariate procedure for parameter estimation of these matrices the elements of c and e corresponding to variables i and j are expressed as 6a c i j or e i j m 1 i j if i j c i j or e i j 0 otherwise 6b d d i j t or f f i j t m 0 i j 1 m 1 i i m 1 j j 6c c τ i j or e τ i j m 1 τ i j if i j c τ i j or e τ i j 0 otherwise 6d d d τ i j t or f f τ i j t m 0 τ i j m 1 τ i i m 0 τ 1 i j m 1 τ j j similar to mnbc variants mehrotra and sharma 2012 2015 two different auto regressive multivariate models are considered the one with constant parameters for the daily and annual time series and another with periodic parameters for the monthly and seasonal time series salas 1980 if only lag0 cross correlations are of interest then the above equations are simplified as 7 z t g d f 1 z ˇ t g where z ˇ and z are pre and post bias corrected series and matrices d and f are the lag0 cross correlations of observed and gcm series the elements of d and f are obtained by using equation 4a similarly if only lag1 correlations are of interest then the multivariate correction is not required and a standard univariate autoregressive lag1 model for individual variable is considered johnson and sharma 2012 8 z t g r h z t 1 g 1 r h 2 z ˇ t g r m z ˇ t 1 g 1 r m 2 where z t g is the bias corrected time series for time step t r h is the observed and r m is the gcm time series lag1 correlations 3 3 robust multivariate bias correction rombc the modelling strategy proposed here is termed as robust mbc rombc we describe the primary statistical attributes using distribution statistics and dependence attributes using the lag0 and lag1 auto and cross correlations similar to other variants of a multivariate bc model the rombc bias correction considers four popular bias correction time scales daily monthly quarterly and annual the model structure evolution procedure operates in stages from univariate to multivariate and from one time scale to the next at each time scale the approach evaluates the reparations of bias correction application in stages first in lag1 auto dependence of the individual variables and thereafter for lag0 cross dependence across variables for both calibration and validation current and future time periods as mentioned before the first stage of the approach is to apply univariate qm at a daily time scale and univariate variance correction at higher aggregated time scales individually to all variables and locations this forms the base bias corrected time series to be used as a reference to assess the need for the more complex bc alternatives that are assessed next the next stage is to examine the necessity and the applicability of bc in dependence attributes at daily monthly seasonal and annual time scales the dependence attributes are defined in terms of lag1 autocorrelation and lag0 cross correlation attributes at all four time scales the aim here is to specify a bc model structure that is appropriate at that time scale the assessment of dependence attributes in bias correction is conducted in a stage wise manner at each time scale the sequence of these bc stages includes lag1 lag0 a contemporaneous l1c and finally a full model l1f for lag1 only dependence being a univariate correction the procedure is conducted separately at each location and for each variable and the bc model varies across variables while for cross dependence joint collective assessment is undertaken at each time scale the final bc is applied only when the assessment at all four stages is completed at each bc stage the assessment is performed in two steps that are designed to be intuitive and straightforward in the first step called hereafter as necessity check the base time series is evaluated to assess if the bc procedure being considered is necessary by comparing the current climate dependence statistics with those representing the observed record if this difference is found statistically unimportant there is no need of applying the bias correction in the dependence attribute considered and we proceed on to the next stage the second step of checking the applicability of bc in validation future is called the applicability check and is initiated only if the difference is found significant at the first step in the second step the bc is applied to the future climate time series and the bias corrected time series is assessed to see if the application of bc brings in significant changes in extreme values or designated statistics in comparison to those obtained using the base case simulation if these changes are found statistically significant the bc procedure is ignored and the correction at the next stage is considered the following describes in brief the two step criteria adopted 3 3 1 necessity check the structure of the necessity check procedure remains the same for all bc stages for lag1 this involves assessing lag1 correlations of individual variables while for lag0 these represent cross correlations across variables for brevity both are simply denoted as correlations here for a given location and variable let the correlations of observed and gcm current climate series be denoted as ro and rg respectively for daily series these are calculated for each day of the year using a moving window of 31 days centred on the day of interest the user is allowed to change the length of the moving window through a parameter in the data file the significance of difference of observed and current climate correlations is assessed using fisher z test statistic as shown in equation 9 at 5 level of significance significant if fisher z value 1 96 in the equation n o and n g respectively are the number of observed and gcm data points used to calculate correlations 9 f z z o z g 1 n o 3 1 n g 3 and z o 1 2 l n 1 r o 1 r o and z g 1 2 l n 1 r g 1 r g with daily data the process is repeated for all 365 calendar days while for monthly and seasonal data for each month season if out of 365 days 12 months 4 seasons this difference is found statistically significant for more than 1 of time bias correction of correlation is assumed to be needed this is denoted as the necessity check note that the 5 level of significance is a common choice in hydrology while threshold of 1 of time was picked following a sensitivity analysis by varying it from 0 05 to 5 and 1 was found to provide satisfactory performance on the data used the second step is applicability check and is explained next 3 3 2 applicability check if the necessity check suggests that a dependence correction is needed the next step is to check the impact of the dependence correction on the future climate time series the dependence correction is applied to the base future climate series and the percent of time the bias corrected values cross designated lower or upper practical limits is noted also the means avs and standard deviations sds of the pre and post bias corrected series are calculated and compared to check if the correction has made any significant changes in the av or sd of the time series by using equation 10 again at 5 level of significance 10a a v p o s t a v p r e 1 96 s d p r e 2 s d p o s t 2 n 10b i f s d p r e s d p o s t s d p r e 2 s d p o s t 2 1 96 i f s d p o s t s d p r e s d p o s t 2 s d p r e 2 1 96 here n is number of data points and pre and post subscripts represent the statistics before and after the application of bias correction these equations check the significance of the differences of statistics at the 5 significance level if upper and lower limits are crossed more than 1 of time or the difference of the statistics equation 10 exceeds the specified threshold by more than 1 this bc model is not considered our aim here is to make sure that we do not allow the bc to change the future irrationally and end up having few very high low values as mentioned before the thresholds at 95 level of confidence is a commonly used choice while the 1 of time is used to define a check on the bc procedure as it would not be violated under normal conditions the 1 threshold chosen was found to perform well based on sensitivity assessments across gcms and a range of variables being corrected once all the correction stages at a given time scale are assessed the final selected bias correction model is applied at that time scale the bias corrected time series is then aggregated averaged to the next time scale and the same procedure is repeated the time scales adopted and statistical attributes considered represent common choices the developers and other researchers have found important for water resources applications the approach is quite flexible and allows users to accommodate alternate representations of time scales as well as other statistical attributes johnson and sharma 2012 mehrotra and sharma 2012 2015 the following describes the stepwise procedure adopted in the implementation of rombc 3 4 stepwise rombc procedure the complete bias correction procedure is divided into three parts part a deals with the formulation of base series in the form of a univariate primary bias correction part b is core of rombc and deals with the checking and application of complex bias correction procedures at each time scale part c aggregates the time series to higher time scale and repeats the part b steps involved in these parts are discussed next 3 6 1 part a defining the primary base bc series 1 calculate monthly seasonal and annual means and standard deviations of all the variables of the observed z t h time series also calculate daily monthly seasonal and annual lag1 auto and lag0 and lag1 cross correlations of variables use a moving window of 31 days or the number of days as specified by the user centred on the current day of interest while calculating the statistics for the daily data rajagopalan and lall 1999 sharma and lall 1999 2 consider a variable at a location grid point apply qm to the daily data by fitting an empirical cumulative distribution functions cdfs to the observed z t h and raw gcm series z ˆ t for current and future climate for a given value in the future climate gcm series calculate the cumulative probability and obtain the difference of observed and gcm current climate values for this cumulative probability from the corresponding cdfs bias obtain the corresponding value for this cumulative probability from the future climate cdf apply the difference to the value to obtain the bias corrected value for the future climate repeat the same procedure for every data point and obtain the bias corrected daily time series for current and future climates note these form univariate corrections for each variable with no consideration is given to cross dependence biases that may be present 3 aggregate the daily qm corrected time series to monthly time scale for standard deviation sd correction assess the difference of observed and current climate monthly sds using equation 10b if this difference is found statistically significant for more than 1 of time bias correction of sd is needed this is denoted as the necessity check 4 for the applicability check apply sd bias correction to future monthly time series check the significance of the sd corrections by noting the percent of time the corrected monthly values cross the theoretical lower and upper limits also count the percent of times avs and sds of post bc series are different from pre bc raw aggregated monthly series using equation 10 if series passes the applicability check apply sd correction to both current c and future f daily time series 11a y j i k g c y ˆ j i k g c μ j k g c σ j k h σ j k g c μ j k g c 11b y j i k g f y ˆ j i k g f μ j k g f σ j k h σ j k g c μ j k g f where μ j k g c is mean of current μ j k g f is mean of future σ j k h is sd of observed and σ j k g c is sd of current climate time series for jth month and kth variable similarly y ˆ j i g c and y j i k g c and y ˆ j i g f and y j i k g f are monthly time series before and after sd correction for current and future climate for kth variable jth month and ith year 5 aggregate both current and future climate time series monthly bias corrected time series to seasonal and annual time scales and check for the applicability of sd correction 6 finally incorporate the changes at all time scales by modifying the daily time series as follows 12 z t j s i k g y j s i k g y ˆ j s i k g x x s i k g x ˆ s i k g x a i k g a ˆ i k g x z t j s i k g where y j s i k g is the monthly corrected value y ˆ j s i k g the aggregated monthly value x s i k g the seasonal corrected value x ˆ s i k g the aggregated seasonal value a i k g the yearly corrected value and a ˆ i k g the aggregated yearly value in equation 12 subscript k stands for variable t for day j for month s for season and i for year do it for both current and future climate time series 7 store the daily distribution and higher time scales sd corrected time series of individual variables y j s i k g for current and future climate 8 repeat steps 1 7 for other variables and locations the above steps form the base or reference bias corrected time series which is corrected for essential biases in daily distribution and variability at monthly seasonal and annual time scales the base series is now used to define the practical lower and upper limits on the data and forms the starting step to test and apply more complex dependence attributes based bc models 4 in all lag1 lag0 contemporaneous and full it should be noted that if a more complex bc model is accepted to be valid the base bias corrected time series at that time scale is updated only at the end of the fourth stage to define a new reference the practical lower and upper limits are used to additionally validate the advanced bias correction stages the daily monthly seasonal and annual upper and lower limits on individual variables at all locations for both current and future climates are formed by calculating the standard deviations sds and maximum and minimum values of the entire time series at all four time scales the maximum limit is defined as the maximum value in the entire time series plus sd and minimum limit as minimum value in the entire time series minus sd as per the following 13a maximum limit max y g sd y g 13b minimum limit min y g sd y g for current and future climate at each time scale location and for all variables the daily limits are further checked against the physical lower and upper limits specified by the user for example with rainfall as a variable the lower physical limit is zero and if the practical limit given by equation 13b is less than zero it is set as zero the next stage is to examine the necessity and applicability of bc in dependence attributes at daily monthly seasonal and annual time scales the aim here is to identify a suitable bc model structure that is appropriate at that time scale a flow chart presented in fig 2 highlights the procedure adopted in part b the steps involved are as follows 3 6 2 part b assessing checking and applicability of dependence bc model structure at each time scale 9 start with daily data consider gcm current and future climates daily base time series as obtained from step 8 consider each variable and calculate lag1 auto correlations of observed and gcm current climate series 10 for each day of the year assess the difference of observed and current climate lag1 auto correlations using equation 9 if this difference is found statistically significant for more than 1 of time bias correction of lag 1 auto correlation is needed this is denoted as the necessity check 11 for the applicability check apply lag1 auto bias correction to future daily time series using equation 8 and check the significance of the bias correction applicability check by noting the percent of time the corrected daily values have crossed the lower and upper limits also count the percent of times means and sds of post bc series are different from pre bc base series using equation 10 if series passes the applicability check the lag1 bias correction forms as our plausible bc model for this variable and time scale 12 repeat the above steps for other variables store the results of all variables 13 next assess the need for lag0 cross dependence correction calculate observed and current climate lag0 cross correlation matrices of base model corrected daily time series considering all variables repeat step 10 and assess the need of bias correction 14 if bc is needed then form future climate lag0 bias corrected series by using equation 7 repeat step 11 to check for the statistical significance of the changes note if the changes are statistically significant or not move to the next stage 15 check the need of a contemporaneous bc model l1c calculate lag0 and lag1 cross correlations of observed and daily current climate base time series considering all variables locations repeat step 10 and assess the need of bias correction considering lag0 and lag1 auto correlations if test suggests the need of a bias correction go to next step otherwise move to next stage 16 calculate lag0 and lag1 correlation matrices of observed and daily current climate base time series considering all variables locations using equations 4 and 6 apply bias correction to future climate series using equation 2 17 repeat step 11 to check for the statistical significance of the changes note if the changes are statistically significant or not 18 check the need of a full bc model l1f by repeating the procedure mentioned in steps 15 17 19 now assessment of all 4 bce models at daily time scale is finished if no model is suggested do not apply any correction and move to the next time scale if a full l1f or contemporaneous model l1c is picked just apply that model to both current and future climate time series and move to the next time scale if a lag0 model is suggested apply lag0 model and see for the applicability of lag1 model for individual variables 3 6 3 part c aggregating series and assessing optimal bc model structure for the next aggregated time scale 20 aggregate the time series to higher time scale s and check for the necessity and applicability of all the four bc model structures by following the procedure specified in steps 9 to 19 21 incorporate the corrections at all time scales into the daily series by using the aggregated and bias corrected time series at monthly seasonal and annual time scales and equation 12 it should be noted that if there are no significant biases in auto or cross dependence attributes at the original daily time scale or if the correction results in significant changes to the future climate simulation the base model would be retained and will be used to form the time series at the aggregated time scale part c if similar outcomes result at aggregated time scales the end model will be the base model defined in part a also note that the auto correlation corrections can differ from variable to variable creating corrected time series that have been processed using the minimal complexity model that is applicable 4 results we apply univariate qm multivariate quantile based nbc hereafter called as mbc and rombc to the daily rainfall and evaporation time series of 6 gcms at 21 locations including 18 rainfall and 3 evaporation points stations over the greater sydney region for mbc single iteration with qm correction at daily and sd and l1c bias corrections at daily monthly seasonal and annual time scales was chosen for gcm current climate a 30 year time window spanning over 1976 2005 and for the future climate three 30 year time windows from 2010 to 2039 2040 2069 and 2070 2099 are considered for space limitation before presenting the overall results of all gcms we present and discuss detailed results for one representative gcm access only for one time window 2070 2099 centred around 2085 table 2 presents the finalised structure of the flexible bias correction model in the table correction criteria are shown by zeros and ones a one with a star 1 implies that the statistic is directly applied or is built in as a part of model structure a zero 0 implies bias correction for that statistic is not needed while one 1 implies correction is needed as per the current climate negative one 1 implies that while the correction is necessary application of bias correction to the future climate time series makes the changes significant and therefore the correction is not applied some specific findings can be drawn from table 1 for all locations and variables daily lag0 and lag1 dependence attributes are significantly different in the raw gcm series for the current climate and hence require corrections however as bias correction changes the statistics of the future time series quite significantly the correction is ignored lag1 correction to evaporation time series was needed and applied monthly and seasonal lag0 and l1c statistics require corrections and correction for l1c is applied as it also includes correction for lag0 statistics at annual time scale does not require any corrections thus the flexible model suggests a bc structure which is more complex than a traditional qm however is much simplified than a rigid multivariate bc mbc although is does involve a complex full model l1f structure in the model identification exercise figs 3 and 4 present a comparison of the three approaches in the form of scatter plots of selected observed and bias corrected statistics of general interest for current and future climates respectively for access gcm considering current climate results fig 3a the mean is reproduced well by all the models top two rows fig 2a annual sds are also well reproduced by all the models except for the sd at one location by qm row 3 fig 3a other statistics of interest for example daily maxima and annual 5th percentile values and lowest annual 3 5 and 7 years totals are also well reproduced by all the three models considered rows 4 6 fig 3a fig 3b presents a few distribution and dependence statistics of observed and bc results lag1 and lag0 cross correlations are better reproduced by mbc as expected rombc results are better than qm but not as good as mbc more specifically for annual statistics considering results for the current climate selecting mbc would be a reasonable choice bottom two rows of fig 3b present daily and annual rainfall distribution of observed and models simulated time series all models are able to reproduce daily and annual distribution behaviour of observed rainfall as qm is applied to daily data only reproduction of observed annual distribution in the bc series indicates relatively good quality of raw gcm data over the study region for this statistic now consider results for future climate as presented in fig 4 for access gcm for the time window 2070 2099 mean changes are shown in the top two rows of fig 4a annual rainfall shows almost no change while the three evaporation stations top circles in the plots show increases in annual evaporation seasonal rainfall shows increases in spring and summer while slight decreases in autumn and winter seasons all bc models project similar increases in the annual and seasonal means annual sds show some scatter with qm projecting slight under estimation of the statistic 3rd row fig 4a daily maxima and annual 5th percentiles and lowest annual averages show no changes for rainfall and increases for evaporation values all models project similar results 4th 6th rows fig 4a top two rows of fig 4b present scatter plots of lag1 and lag0 cross correlations all models show some scatter for these statistics more specifically at the annual time scale daily and annual distribution of rainfall at a representative station 1 shows no notable changes albeit a few extreme daily values by mbc bottom two rows fig 4b these results indicate no substantial loss of information if we selectively apply bias correction using rombc we now look at the projected changes in the average and extreme rainfall statistics in the future considering all gcms it may be noted that the climate models exhibit high variations across them in the projected changes with miroc projecting increases in rainfall and cnrm and gfdl projecting decreases in rainfall in the future over the study area table 3 presents the percent changes in annual rainfall and evaporation averaged across all gcms for all catchments and for three time windows similarly fig 5 presents changes in annual rainfall annual wet days lowest 7 years totals and daily maximum rainfall as projected by all six gcms by 2085 over all catchments using all bc models percent changes are derived by comparing the changes in the future with respect to current climate in the figure x axis shows all 18 catchments considered whereas on y axis percent changes are plotted similarly statistics of individual gcms are shown as thin lines of no changes as black dotted lines and gcms averaged values as thick black lines lines across catchments are joined for the sake of presentation only considering the models averaged results as presented in table 3 and fig 5 all bc models project around 1 4 percent decrease in rainfall during 2010 2039 a similar percent increase during 2040 2069 and again a similar percent decrease during 2070 2099 time periods over the study region top row of fig 5 evaporation stations show around 5 increase during 2010 39 11 12 during 2040 69 and 22 25 increase during 2070 99 time periods percent changes in bias corrected rainfall and evaporation results are in line with those projected by the raw gcms table 3 changes are in general consistent across catchments and gcms with miroc being the wet model and gfdl and cnrm being the dry ones fig 5 all bc models project around 5 increase in the number of wet days in a year over the study catchments by 2085 second row fig 5 fig 5 also includes the changes in daily maximum rainfall by 2085 third row fig 5 qm projects no appreciable changes in the catchment averaged daily maximum rainfall over the study region mbc projects 15 while rombc projects about 10 increase in the daily extreme rainfall in the future over the study catchments as mentioned before the rigid application of bc in mbc might force a few data points to take high or low values in order to match the observed dependence characteristics rombc checks for this possibility at each time scale before applying bc and possibly avoids such instances percent changes in the lowest 7 year rainfall are presented in the last row of fig 5 qm shows no changes mbc around 10 15 while rombc projects around 10 decreases in the statistic by 2085 over the study region this statistic kind of represents the low frequency behaviour of the time series and is important for water resources management and water availability related applications as qm is applied only at daily time scale it is insensitive to the biases in the low frequency variability fig 5 presents the distributional changes of the catchments averaged time series formed by taking the average of catchment rainfall and evaporation over the region top two rows present the temporal distribution of monthly rainfall and evaporation while the changes in the probability distribution of area averaged daily and annual rainfall are presented in the bottom two rows all bc models projects rainfall to increase in summer and decrease in spring no shift of season is noted monthly evaporation shows lowest increase in june and maximum increase in november surprisingly this increase does not occur during summer months perhaps increase in summer rainfall and more rainy days causes the evaporation to be lower on the rainy days distribution of area averaged daily rainfall only extreme values is shown in the third row of fig 6 qm does not show any significant changes in the extreme daily values in relation to the observed values rombc results are in between qm and mbc with a mild increase in very extreme daily values by 2085 considering distribution of area averaged annual rainfall last row of fig 6 qm projects negligible changes in the shape of the annual rainfall distribution with a slight reduction in the higher quantile rainfall by 2085 mbc and rombc project a few more dry years with minor changes in the shape of the distribution at both lower and higher ends 4 1 rombc details rombc is implemented in a r shell and allows the bias correction approach to be applied in a fairly simple manner 4 1 1 input data the software requires information about data in the form of four files in a specific format these include observed and raw data files for calibration current climate as well as verification future climate time periods when dealing with gcm current and future climates data the package uses three files observed and gcm rcm current and future climates raw data files in this case the observed verification period file will be same as observed calibration period file in this set up the observed data is used to compare the changes in each variable in the future it is not necessary to have equal length of data for raw and observed file either for calibration or verification periods users are allowed to define their own seasons in addition to the names of the four data files all other general information is provided through the basic dat file table 4 it includes the information about the number of years of data number of variables width of moving window used to correct the daily data physical lower and upper limits on the variables whether data consider leap years or not and the split of calendar months across the seasons being modelled all the information is provided in a free format separated by spaces at present the package allows for a maximum of 150 years of daily data 30 variables 12 seasons and 31 day moving window 4 1 2 package outputs upon successful completion of the program 6 output files are generated two files contain the bias corrected time series for the current and future time periods remaining four files contain a few common statistics of the observed raw and bias corrected data for the current and future climate as per the followings 1 observed and raw data for current climate 2 observed and raw data for future climate 3 observed and bias corrected data for current climate and 4 observed and bias corrected data for future climate time periods as for gcm rcm future climate data corrections the observed file would be same as the observed file for current climate the observed statistics would not change while we move from current to future climate statistics considered include means standard deviations skewness lag1 and lag2 auto correlations when multiple variables or locations are corrected then auto and lag1 cross correlations are also computed the package allows the users to look at raw and bias corrected statistics either in the form of a table or as plots at multiple time scales of interest finally the package also provides plots of the empirical cumulative probability distributions of the observed and raw and observed and bias corrected time series 5 discussion and conclusions bias correction has now become a standard post processing procedure to correct systematic biases and convert climate model raw output to one that is suitable for use in climate change impact assessment studies the majority of existing multivariate bias correction approaches work on a pre defined rigid bias correction model structure without looking into the magnitude and nature of biases and their behaviour in the future this study presented a novel approach for specifying the optimal structure of a multivariate bias correction model based on the premise that a pre defined fixed bias correction structure does not apply when biases are being assessed across time scales variables and dependence attributes and if imposed such a bias correction model can provide unstable and physically incompatible projections for the future where the impact of unneeded structural complexity will be most evident given this the approach adopted resided on specifying a base or reference bias correction model and updating this reference to a new model only if a systematic biases are noted in the simulations representing the observed period using the reference model and b an updated bias correction model that addresses the systematic biases in a does not in turn lead to projections of the future that are untenable only if these conditions are satisfied is the bias correction model updated and the process repeated to extend to all time scales and variables being modelled while models have long been formulated with limited data for application in scenarios that have not been observed in most cases these models are developed assuming the observational record used in their testing and validation exhibits stationarity as our situation is one where the future can be expected to change significantly at least for temperature and hence for evaporation forming a robust model requires an added means for identifying one that will exhibit stability into the future what is different in our approach here is the use of statistics to quantify instability which is performed by defining a base case and discarding model formulations that deviate significantly from this reference while the statistics we have chosen here are relevant in the water resources context the choice of attributes that define stability is one that should reside with the user based on the applications the climate model simulations are intended for an open source software in r statistical computing environment is presented here it provides an easy means to apply an in built flexible multivariate and multi timescale bias correction alternative that is self evolving and grows in complexity following the requirement of the raw data applications of the software along with information about the capabilities of the software are demonstrated using a sample dataset it is anticipated that the ease of running the software and the flexibility of exercising a wide variety of options will make it popular for practitioners carrying out impact assessments and researchers investigating downscaling methods software availability name of software package rombc developers raj mehrotra wrc civil and env engg unsw sydney e mail address raj mehrotra unsw edu au ashish sharma wrc civil and env engg unsw sydney e mail address a sharma unsw edu au year first available 2021 hardware required standard pc for windows software required rgui or r studio availability and cost available free of charge software along with sample data and help file can be downloaded from the following website http www hydrology unsw edu au download software programme language written in r and fortran declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research is partly funded by waternsw nsw australia appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105019 
25864,effective implementation of a scenario neutral climate impact assessment relies on the integration of many modelling components at multiple stages from the generation of appropriate climate boundary conditions that can be used to rigorously stress test a system to the simulation visualisation and interpretation of resulting system performance for systems with complex temporal dynamics the generation of climate forcing time series combined with the significant simulation and visualisation demands represent a barrier to the wide scale adoption of scenario neutral approaches a unified five step framework and supporting r package foresight systems insights from the generation of hydroclimate time series are introduced that enable the application of a scenario neutral climate impact assessment from appropriate time series generation to the analysis of system performance the software can be applied to compare both current system performance and selected alternative system management or design options and its application is demonstrated for a case study keywords climate change impact assessment scenario neutral bottom up stochastic rainfall inverse approach r package 1 introduction climate variability and change present significant challenges for natural and engineered systems across a wide variety of sectors such as the municipal agricultural energy mining industrial transport and environmental sectors piao et al 2010 schaeffer et al 2012 vermeulen et al 2012 pearce et al 2011 love et al 2010 olesen and bindi 2002 often these systems are required to operate effectively under a range of hydroclimate conditions including being able to withstand climate extremes however the ability to understand system performance under and sensitivity to a range of plausible climate conditions is confounded by the complex interactions between the system and the climate zscheischler et al 2018 2020 leonard et al 2014 yet traditional top down methods of climate impact assessment that rely on applying climate projections to examine changes in a system s response produce information that can be difficult for decisions makers to interpret and apply wilby and dessai 2010 and generally do not enable better understanding of the specific mechanisms by which a system might respond to climatic changes scenario neutral or bottom up approaches have been put forward as a means for better understanding these complex relationships and sensitivities and are increasingly being adopted to evaluate system performance under climatic change and variability brown et al 2012 brown and wilby 2012 turner et al 2014 culley et al 2016 prudhomme et al 2010 these approaches are underpinned by an extensive vulnerability analysis i e a stress test to evaluate a system s sensitivity to changes in the driving hydroclimate variables and potentially assess the viability of alternative management and or design options to date the development and application of scenario neutral approaches has remained mostly within the disciplines of water and environmental science with common foci being the examination of water resource systems brown et al 2012 culley et al 2016 turner et al 2014 adeloye et al 2016 ghile et al 2014 sediment transport bussi et al 2016 changes in streamflow and flood risk guo et al 2017 prudhomme et al 2010 2013 as well as ecological systems poff et al 2015 in contrast to the traditional scenario led or top down approaches which focus on the application of projected changes in climate to a system poitras et al 2011 chiew et al 2009 bell et al 2012 kay and jones 2012 segui et al 2010 paton et al 2014 beh et al 2017 scenario neutral approaches treat the system as the focus of the evaluation brown and wilby 2012 in particular the emphasis is on quantifying system sensitivity to climate and identifying associated performance thresholds and or decision boundaries rather than treating the system model merely as a filter through which climate model projections are passed note that climate change projections still can be used to inform the analysis but for scenario neutral approaches they are not the central concern or mechanism for evaluating response to change brown and wilby 2012 nazemi and wheater 2014 as a result the focus of scenario neutral approaches on the assessment of system sensitivity to a range of plausible climate futures provides a method to better understand the system specific mechanisms by which climatic changes can drive failure or performance degradation which in turn can provide a rigorous basis for adaptation decisions the stress test that underpins scenario neutral approaches allows the strategic assessment of a system s performance by providing decision relevant information on the system s response to a range of plausible hydroclimate scenarios this information may include identification of critical variables that affect the vulnerability of the system and decision critical thresholds beyond which a system does not perform desirably poff et al 2015 turner et al 2014 following the system stress test stage scenario neutral approaches can then take advantage of multiple lines of climate evidence to add context and understand possible future climate changes lines of evidence may include climate model projections by combining global climate models with dynamical and or statistical downscaling or bias corrections brown and wilby 2012 historical climatic changes expert judgement and or analogues from paleo records implementation of scenario neutral approaches requires consideration of a number of steps including i the determination of which hydroclimate variables drive system performance and what changes in these hydroclimate variables should be included in the stress test to ensure relevant system behaviour and or failure modes are exposed ii the generation of hydroclimate timeseries that reflect these changes iii the simulation of system performance in response to the changes in hydroclimate variables via a system model iv the analysis and visualisation of system performance in the context of additional climate information and v the evaluation of system options e g system design or operating conditions in light of the simulated system response to date studies have largely focused on each of these steps in isolation developing techniques for particular analysis stages e g identification of critical variables guo et al 2017 generation of perturbed hydroclimate time series guo et al 2018b steinschneider and brown 2013 culley et al 2019 the system performance and decision analysis stages culley et al 2016 or the integration of ecological performance measures poff et al 2015 or applied a general stress test work flow to a variety of sectoral applications hirpa et al 2018 tra et al 2018 turner et al 2014 ghile et al 2014 brown et al 2011 however the interaction between the steps can result in a number of issues in particular careful and systematic implementation of each component with the specific and detailed focus on the system under investigation is required to effectively stress test the system and to produce information that is interpretable in a decision making context for example the failure to include a critical variable in the assessment that may affect the system s vulnerability undermines the efficacy of the stress test and will likely lead to missed modes of failure thereby undermining the overarching motivation of the scenario neutral approach culley et al 2021 while there have been a few attempts at developing generalised frameworks for scenario neutral analysis e g poff et al 2015 broderick et al 2019 ray and brown 2015 there are a number of unique challenges for the application of bottom up analyses to dynamical systems for which the generation and application of time series hydroclimate boundary conditions is critical these dynamical systems include many environmental agricultural water resource e g natural systems like groundwater as well engineered systems like reservoirs and renewable energy systems as well as other systems that depend on one or several of the former e g transportation systems are often sensitive to natural hazards such as flooding consequently there is a need to formalise the various analysis stages of scenario neutral approaches as a unified framework with a specific focus on the modelling and analysis components required by these dynamical systems bringing together advances in best practice at each stage to obtain the most benefit from scenario neutral climate impact assessments and improve modelling practice this barrier to wide scale adoption is further enhanced by the logistical challenges of implementing a scenario neutral analysis in particular generating perturbed hydroclimate time series capable of strategically stress testing the system to specific aspects of possible future change managing the integration multiple models and data streams throughout the analysis as well as the visualisation and interpretation of system performance in higher dimensions while there are number of existing software packages to support climate impact assessments these do not satisfy the requirements outlined above as they often do not provide end to end support for climate impact analyses and focus on supporting top down analyses in particular downscaling climate projections e g sdsm wilby et al 2002 climdown hiebert et al 2018 rsqm cho et al 2018 dsclim pagé et al 2009 clim pact benestad 2009 mbc mehrotra et al 2018 analysing downscaled products e g musica hanel et al 2017 or the generation of scenarios e g wilby et al 2014 to the authors knowledge there are currently no available software tools to assist with applying a scenario neutral approach to climate impact assessment from start to finish given the range of technical and logistical challenges involved in implementing a scenario neutral impact assessment there is a need to develop a software platform that enables these steps to be implemented in an easy to use fashion and conforming to principles of accessible consistent and reproducible implementation of modelling practice humphrey et al 2017 iturbide et al 2019 knox et al 2018 as demonstrated in other areas of environmental modelling guo et al 2016 galelli et al 2014 humphrey et al 2017 stokes et al 2015 guillaume et al 2016 in response to these challenges the objectives of this paper are 1 to present a unified framework for scenario neutral climate impact assessments the framework seeks to formalise current approaches to conducting stress tests for dynamical systems into an end to end workflow to implement the modelling and analysis components of a scenario neutral approach 2 to introduce the r package foresight which facilitates implementation of the framework workflow thereby reducing the barriers of entry to scenario neutral climate impact assessments the remainder of this paper is structured as follows section 2 provides an overview of the integrated scenario neutral framework the structure and functionality of the foresight package are then presented section 3 followed by a demonstration of the package for the example problem of a domestic rainwater tank system used for indoor and outdoor supply section 4 section 5 discusses limitations of scenario neutral approaches as well as potential advances followed by conclusions in section 6 2 a modelling and analysis focused scenario neutral framework this paper presents a generic scenario neutral framework so that it can be used for a range of application areas the framework focuses on the modelling and analysis components of scenario neutral climate impact assessment as distinct from other considerations around application of the broader components that may include stakeholder consultation as discussed this is of particular importance for dynamical systems whether natural or engineered the modelling and analysis components of scenario neutral climate impact assessments are formalised as a framework with five steps the framework starts by first identifying what properties of the hydroclimate variables hereafter termed attributes affect system performance step a this is followed by the generation of a set of hydroclimate time series that correspond to plausible perturbations in the identified attributes step b these time series are used to stress test the system and the system performance in response to these hydroclimate time series is then simulated step c the system s performance is analysed and visualised with reference to the system s objectives e g performance above a threshold step d additional climate information can also be overlaid onto the visualised system performance to provide an indication of what hydroclimate scenarios are more plausible as part of this step steps c and d are repeated for each investigated system design adaptation option the performance of each system option under the stress test is then compared step e to provide information relevant to the overarching system investigation i e to inform the selection and assessment of a particular system design operation or adaptation options the framework steps are outlined here with an overview of the relevant considerations for implementing each step as well as the interactions between steps a identify attributes for perturbation and create an exposure space existing knowledge of the system s behaviour and preliminary testing of the system under a subset of plausible changes to the hydroclimate time series is used to identify a larger set of climate attributes required for perturbation this preliminary investigation may comprise a narrower implementation of steps b to d of the framework in terms of the range of hydroclimate scenarios investigated e g culley et al 2021 this step ensures that the downstream stress testing investigations steps b to e focus on appropriate hydroclimate scenarios capable of strategically stress testing the system b generate hydroclimate scenarios attributes that have been identified as influential for the system s performance are nominated for perturbation to define a set of hydroclimate scenarios i e time series for simulation hydroclimate time series are then generated that correspond to each scenario using an appropriate time series generation technique e g simple scaling or simulation using a stochastic weather generator the requested set of hydroclimate scenarios will influence the selection of an appropriate time series generation technique for example the perturbation of dry spell length while keeping the annual average rainfall at historical levels necessitates the use of a stochastic weather generator e g guo et al 2018b culley et al 2019 whereas the perturbation of annual average rainfall without holding other climate attributes at historical levels could be achieved with simple scaling of the historical rainfall time series e g tra et al 2018 culley et al 2016 c simulate system performance each of the hydroclimate time series i e scenarios generated in the previous step are used as inputs to the system model to simulate system performance based on one or several identified performance measures d analyse system performance the resulting set of system performance values are visualised with reference to the hydroclimate scenarios attributes the system s performance can then be interpreted and evaluated with reference to the purpose of the system for example a system s performance may be evaluated with reference to design performance levels such that scenarios under which the system is deemed to perform satisfactorily or fail can be identified e g culley et al 2016 tra et al 2018 additional climate information may also be overlaid as part of this step e g modelled climate projections paleo climatological information to add context and understand possible future climate changes steps c and d are repeated for each system option evaluated e g alternate system designs or management options such that all system options are stress tested and analysed with respect to the same plausible hydroclimate scenarios this enables a systematic comparison of each system option if only a single system configuration is evaluated the analysis terminates at step d e evaluate system options in the final step alternative system options or alternatively the current system together with one or more alternative options are compared against one another the change in performance from one option to another is assessed in terms of differences in the simulated system performance measures this comparison of performance between different system options provides decision relevant information that can be considered alongside further contextual information about the competing system options i e cost time to implement each option for example the impact of changing storage within the system on system reliability could be evaluated again additional climate information may also be overlaid as part of this step further iteration may be required depending on the configuration of the system testing of decisions and design options between steps c and e if a new system design is being refined the five framework steps and the interactions described above are represented schematically in fig 1 where the solid grey arrows represent the steps included in all analyses and the striped grey arrows indicate where a repetition of steps is required due to the evaluation of multiple system configurations or the requirement for iteration to refine the design of a new system the options for specific techniques implemented by the r package for each step are detailed separately in section 3 3 the foresight package foresight is an r package that implements the methods of the five step framework for scenario neutral climate impact assessment detailed in the previous section the key functions and the process flow of using the foresight package are shown in fig 2 including the connection between the functions and the steps of the framework summarized in fig 1 the core functionality of the package is to generate hydroclimate series with desired perturbed attributes for climate stress testing the ability to generate realistic perturbed hydroclimate time series is integral to multiple stages of the framework and thus the package the stochastic implementation in this package uses stochastic weather generator models see wilks and wilby 1999 for a review of stochastic weather models for the climate variables and optimises the parameter values of the stochastic models to generate time series with desired attributes the optimisation algorithm operates within the specified bounds of the stochastic model parameters to arrive at optimum parameter values that minimise deviations in the attributes of the generated time series from the specified target values the package includes multiple stochastic model options that differ in model structure and complexity the process flow of using the package involves creating an exposure space via selecting climate attributes for perturbation and defining target values of the attributes generating perturbed hydroclimate time series stress testing the system analysing the results and evaluating investigated system options through a comparison of their performance under the stress test the implementation of the five step framework corresponds to seven key functions in the package together with a large number of additional helper functions that are described in more detail in the package documentation the first function createexpspace creates an exposure space defined by the attributes of the time series that are selected for perturbation the number of dimensions of the exposure space is equal to the number of perturbed attributes specified by the user the exposure space consists of target locations that correspond to combinations of the specified values of the perturbed attributes the second function generatescenarios generates the perturbed hydroclimate time series by optimising the parameters of stochastic models to generate data with specified target values of the attributes or via applying multiplicative or additive changes to the observed reference timeseries depending on the specified perturbation method for any application the selection of perturbed attributes for stress testing the system typically involves iterating through steps a to d to identify relevant attributes and generate realistic perturbed time series generatescenarios may be used to perturb attributes individually i e one at a time to select the most important attributes for subsequent stress testing or jointly as part of the full system stress test following the generation of suitable sets of perturbed hydroclimate time series runsystemmodel is used to run the system model and simulate the system performance response to the generated perturbed time series step c the system model quantifies the system performance in terms of one or several metrics that are relevant for the specific application with metrics calculated as part of the system model i e outside of the foresight package it is noted that foresight does not have inbuilt system models other than to support a tutorial but instead requires development of a wrapper function to call an external system model the plotting functions in the package plotperformanceoat plotperformancespace and plotperformancespacemulti visualize the system performance to aid decision making plotperformanceoat and plotperformancespace create plots of the performance metric values step d similarly plotperformancespacemulti is used to create plots combining more than one performance metric plotperformancespace and plotperformancespacemulti can also be used to overlay climate information available from alternate sources if multiple performance metrics are important for the system or if multiple system configurations operating policies are to be compared runsystemmodel plotperformanceoat plotperformancespace and plotperformancespacemulti can be used multiple times to repeat steps c and d finally plotoptions can be used to compare system performance across different system configurations step e each function and its required arguments are briefly described in fig 2 each stage of the scenario neutral approach as set out via the functions is discussed in turn 3 1 createexpspace the creation of an exposure space involves specifying the hydro climate attributes that are perturbed i e attperturb those that are held constant i e atthold and the combination and domain over which the attributes are sampled 3 1 1 specifying the attributes attperturb and atthold attributes that have been identified as potentially influential to system performance should be nominated for perturbation using the attperturb argument the selection of attributes for stress testing is typically arrived at through sensitivity assessment of the system performance to changes in various hydro climate attributes culley et al 2021 where the stochastic method of generating hydroclimate scenarios is used atthold can be used to specify which attributes should be held at the levels of the reference time series the attributes supplied by atthold put additional constraints on the generated time series so that parameters are chosen to implement the perturbation with minimal change to the properties of the time series not designated for perturbation as discussed further in culley et al 2019 this is critical for the production of realistic hydroclimate time series attperturb and atthold would typically include attributes of multiple hydro climatic variables that affect the system table 1 sets out the attributes available for perturbation or holding using the foresight package the attribute names are specified in foresight using the following format hydroclimatevariable period statistic m for example the average total rainfall for the months june july and august would have the attribute name p jja tot m whereas the average annual number of frost days would be nominated using temp ann f0 m the m at the end of the attribute name denotes mean since all the attributes are calculated as climatological means of the time series the attributes available for use in foresight may be viewed using the function viewattributes 3 1 2 specifying the perturbations attperturbsamp attperturbmin attperturbmax attperturbtype the arguments attperturbtype attperturbmin attperturbmax attperturbsamp to createexpspace are used to specify how the attributes should be perturbed the function outputs a list of targets i e combinations of held attributes and perturbed attributesspecified as deviations from historical climate that describes the target locations in the exposure space time series that correspond to these target locations will be produced in the next step of the framework attperturbtype sets how the attribute perturbations are sampled with two currently supported options one at a time oat and regular grid reggrid fig 3 one at a time testing can be used to evaluate the sensitivity of a system to a single attribute changing while the remainder are held as close as possible to the level of the observed hydroclimate time series each of the nominated attributes is perturbed in turn in the case of reggrid sampling the attributes are perturbed simultaneously to create an exposure space encompassing targets corresponding to all combination of perturbations fig 3b the attperturbmin attperturbmax and attperturbsamp arguments are necessary to define the bounds of the perturbation and the number of samples desired for each perturbed attribute these arguments are vectors containing elements corresponding to each perturbed attribute attperturbmin and attperturbmax are specified in attribute space which is a deviation from the historical climate in terms of a fraction for rainfall potential evapotranspiration and solar radiation or an absolute deviation for temperature for example a perturbation of annual total rainfall may be specified with attperturbmin of 0 7 and attperturbmax of 1 3 to produce scenarios between a 30 decrease and increase in the total rainfall in contrast attperturbmin of 2 and attperturbmax of 2 for temperature would be used to produce scenarios between a 2 c decrease or increase in the average annual temperature lastly attperturbsamp specifies the number of discrete points to be sampled within the bounds for each dimension perturbed attribute 3 1 3 output createexpspace returns a list that contains the exposure space the list contains a target matrix targetmat each row of which is the location of a desired target point in the exposure space the input arguments attperturb atthold attperturbtype attperturbmin attperturbmax and attperturbsamp are also saved as elements of the output list the output of createexpspace is used an input to generatescenarios the function that generates hydroclimate timeseries corresponding to each target location in the exposure space 3 2 generatescenarios obtaining realistic perturbed hydroclimate time series is a critical foundation for scenario neutral impact assessments of dynamical time varying systems it is these perturbed climate time series that are used to drive the system model in the system stress testing stage step c in foresight the generatescenarios function is used to simulate these hydroclimate time series this function is likely to be the most computationally intensive part of the workflow depending on the system model used in step c the hydroclimate time series are generated with reference to a historical baseline time series reference that is supplied by the user the remaining arguments of generatescenarios specify the exposure space of interest created using the function createexpspace the length of the simulation in years simlengthnyrs and the number of stochastic replicates to be generated numreplicates additional optional arguments to the function include the method of perturbation for scenario generation specified through a json file controlfile and the seed to use for the stochastic simulation seedid 3 2 1 data reference generatescenarios requires a historical or reference set of time series reference to be supplied as an argument each call of generatescenarios begins by calculating the requested attributes of the provided time series reference all scenarios produced are with reference to the attributes of this baseline set of time series the user is expected to supply the input reference in a specific format to the function generatescenarios the first three columns of the data should be named year month day and contain the corresponding date of the observation reference data the additional columns should contain the hydro climate variables of interest named according p temp pet radn table 2 shows the expected input format of reference 3 2 2 exposure space expspace the exposure space contains information about the attributes selected to be perturbed held and the locations of the target points in the exposure space i e the target perturbations the exposure space created using createexpspace should be supplied as an input to the function generatescenarios through the argument expspace 3 2 3 method of perturbation modeltype and modelparametervariation scenario generation is governed by the method of perturbation used and the attributes selected for perturbation scenarios can be generated using one of two types of methods of perturbation 1 simple scaling time series are generated by applying multiplicative or additive changes to observed climate time series or 2 stochastic generation time series with the required attributes are generated using an appropriate stochastic weather generator see various model options in table 3 simple scaling often applies a constant change factor to the entire time series or each season month in the time series e g bracho mujica et al 2019 prudhomme et al 2010 and preserves many of the characteristics of the underlying time series e g rainfall wet dry pattern timing of extremes simple scaling may be chosen as the method of perturbation in generatescenarios by specifying the controlfile argument as scaling simple scaling in foresight works by scaling the supplied reference time series on an annual basis temperature attributes are perturbed by an additive change other weather attributes such as rainfall potential evapotranspiration and solar radiation are perturbed by applying multiplicative changes in contrast stochastically generated time series are produced using an inverse approach guo et al 2018b whereby the desired combination of attributes is specified first and an optimisation approach is used to determine the parameters of the stochastic model required to simulate that specified scenario creating scenarios using stochastic weather generators is useful in that selected attributes of a time series can be changed while still maintaining other attributes at historical levels for example it is possible to change the annual extreme rainfall intensity using the 99th percentile rainfall amount while maintaining the mean annual total rainfall at historical levels using the stochastic generation approach this property provides a powerful tool for generating a wide range of scenarios and thus enables a comprehensive stress test additionally hydroclimate time series produced using stochastic generation can be longer than the observed time series allowing the system s performance to be evaluated over longer time periods the method is also capable of generating multiple time series replicates with the same statistical attributes to represent natural variability the desired number of stochastic replicates may be specified as an input argument numreplicates to the function generatescenarios both simple scaling and stochastic generation methods of perturbation can be implemented using generatescenarios by default generatescenarios uses the method of stochastic generation to create hydroclimate timeseries using the default stochastic weather generators indicated in table 3 in foresight the controlfile function argument may be used to provide a json file input to generatescenarios to specify alternate stochastic models and or modify the bounds of the parameters of the stochastic models the full list of models contained in package version 1 0 0 for each hydroclimate variable is provided in table 1 foresight currently enables daily time series generation via richardson type weather generator model configurations richardson 1981 richardson and wright 1984 as well as latent variable type model configurations bennett et al 2018 2019 rasmussen 2013 to stochastically generate rainfall temperature potential evapotranspiration and solar radiation this initial set of weather generator models was selected for implementation as they each have a relatively small number of parameters this is important for implementing the inverse approach see culley et al 2019 and are established weather generators that will be familiar to users internationally the package s modular structure allows additional weather generators to be added in future the list of stochastic models available in the foresight package may be viewed using the function viewmodels default parameter bounds based on historical australian conditions are provided for each stochastic generator but can be overridden if additional information such as domain knowledge is available these may be specified using the field modelparameterbounds in the input json controlfile to generatescenarios only a limited number of attributes can be nominated for perturbation using the simple scaling perturbation method this is because of the nature of simple scaling which applies an additive or multiplicative change to the observed time series therefore if simple scaling is selected many of the time series underlying properties cannot be changed e g rainfall wet dry monthly and seasonal statistics multiple attributes of the selected hydroclimate variable cannot be perturbed in combination and it is not possible to hold attributes at historical levels in the context of stochastic modelling the selection of a particular model changes the attributes that are available for perturbation using generatescenarios for example an annual rainfall model e g rainfall model with modeltype wgen and modelparametervariation annual does not allow the winter total rainfall attribute p jja tot m to be perturbed differently to perturbations for other months as there are no mechanisms in the stochastic generator to enable this change as a result the package will not allow the user to select this attribute when using the annual model the list of supported attributes for each model can be found in supplementary material a it should be noted that as the models become more complex they can perturb the climate variables in more ways but will also require more attributes to be held at historical levels using atthold to maintain realism of the generated timeseries 3 2 4 optimisation optimisationarguments an important part of implementing the inverse approach is the identification of parameters of the stochastic weather generator that will allow for simulation of the requested climate attributes the inverse approach relies on formal optimisation approaches to identify suitable stochastic model parameters the purpose of optimisationarguments is to facilitate this optimisation process as optimisation is only used in the generation of the stochastic time series optimisationarguments are not required when perturbed time series are constructed using simple scaling i e when controlfile argument is set to scaling foresight uses a genetic algorithm ga to implement the inverse approach for generating stochastic time series a ga is used due to the high dimensionality of the search space guo et al 2017 culley et al 2019 the r package ga scrucca 2013 supplies the genetic algorithm used in hydroclimate time series generation within foresight the objective function supplied to the ga is a distance between the requested target attributes and the attributes calculated from the simulated time series with the euclidian distance used as a default the ga seeks to minimise this distance there is also an option to use a weighted distance function by adding a penalty term to the objective function this can be used to account for situations where attributes may be on different scales or situations where is it desirable to have higher weights placed on particular target attributes such as if they have been identified as more important than others to the system s operation culley et al 2019 optimisationarguments refers to settings or specifications of the ga used for optimisation foresight contains default settings for optimisationarguments used to determine suitable weather generator parameters for the generation of each hydroclimate time series as described by its attributes the settings or specifications of the ga include the number of iterations maxiter the population size of each iteration popsize the probability of mutation in a parent chromosome pmutation and the probability of crossover between pairs of chromosomes pcrossover it is to be noted that these arguments are specific to the ga algorithm from the r package ga scrucca 2013 that foresight relies on for optimisation and the reader is directed to the documentation of the ga r package for more details the default optimisationarguments settings in foresight can be modified by the user by specifying the optimisationarguments field in the input json file supplied as the controlfile argument to generatescenarios if the field optimisationarguments is not included in the controlfile the default optimisationarguments settings are used penalty weights may also be specified for selected attributes in the exposure space these attributes may belong to attperturb or atthold as part of the optimisationarguments field in the controlfile see supplementary material b for examples of controlfiles 3 2 5 output simulation and diagnostics the perturbed hydroclimate time series produced using the generatescenarios function are returned as a list fig 4 shows the format of the output data from generatescenarios the output list contains elements corresponding to each replicate named rep1 rep2 etc an element indicating the dates of the simulation simdates the input exposure space expspace and the controlfile each replicate differs in the seed used for simulation and contains time series corresponding to all target locations in the exposure space named target1 target2 etc each target contains timeseries of the simulated hydro climatic variables in fields named by the variable name p temp etc the simulated data may be used to examine time series characteristics temporal aggregations or summary statistics as desired by the user foresight includes a function plotscenarios that can be used to plot heatmaps to summarise the deviations of the attributes of the simulated time series from the desired target attributes the function is a diagnostic tool to examine how close the attributes both perturbed and held of the time series simulated by the stochastic generators are to the requested attributes the deviations of the simulated attributes from the targets may be high in instances where the specified combination of target attributes is inconsistent or in cases when the stochastic models are insufficiently flexible to represent changes in those specific attributes with sufficient accuracy fig 5 illustrates an example heatmap of mean biases of 20 replicates created using plotscenarios for a scenario of 40 targets and 11 attributes it is also worth noting the challenges associated with computation and storage of larger stochastic simulations the foresight package includes a function getsimsummary that can be used to obtain the summary metadata of a stochastic simulation which is much smaller in size than the full stochastic simulation for ease of storage and use with the downstream plotting functions in foresight on the computational front parallel computing may be used to perform simulations using generatescenarios in cases containing various hydroclimatic variables many scenarios target locations and multiple replicates the internal function structure of generatescenarios supports parallelisation of scenario calculations across replicates and targets 3 3 runsystemmodel runsystemmodel uses the perturbed hydroclimate time series to drive a system model that translates hydroclimate time series into system performance for later visualisation this requires the system response or performance to be simulated from each scenario using a system model that receives continuous hydroclimate time series it is noted that to maximise the flexibility to analyse a range of systems across multiple application areas the foresight software does not include inbuilt system modelling capability but rather requires the development of a wrapper function to an external system model at its core the system model needs to translate hydroclimate time series into one or several measures of system performance the system model should represent the system such that it captures both physical and operating characteristics e g management policies that translate hydroclimate time series into some measure of performance and can include any desired combination of economic social and or environmental metrics e g measures of water and or energy security agricultural productivity ecosystem services system performance can be defined in a variety of ways including binary success failure criteria e g where a threshold critical for system operation is crossed or via continuous performance metrics e g reliability for ease of integration the package allows the system model to be called within the runsystemmodel function the simplest way to generate system performance is to provide the system model as an r function and designate this function as the system model using the argument systemmodel this enables the whole process to be automated using the initial runsystemmodel function call even if the models have been developed in other languages they can still be executed in r with a wrapper function to work inside the foresight package the system model function i e wrapper or otherwise must adhere to a particular format see supplementary material c alternatively this step can be performed outside of r using the output time series from generatescenarios and the system performance results imported for use in later steps 3 4 plotperformanceoat and plotperformancespace to provide a visually interpretable representation of the system s performance across the range of hydroclimate scenarios system performance can be visualised with reference to the attributes of the hydroclimate conditions using plotperformanceoat and plotperformancespace this package supports two graphical options heatmaps and one at a time plots the functions contain multiple arguments to provide control over the aesthetics of these visualisations including colour scales limits and addition of performance thresholds plotperformanceoat creates line plots of the changes in performance with one at a time perturbations in attributes the function creates panelled figures that illustrate the sensitivity of the performance metric to changes in each attribute with shading to show the range of the metric from multiple replicates e g fig 7 plotperformancespace creates heatmap plots using two perturbed attributes as the co ordinates to illustrate the changes in system performance with the perturbations in these attributes the function contains arguments to choose the perturbed attributes to be used as the co ordinates of the figure function arguments are also available to enable sub setting of the perturbation ranges of the perturbed attributes this functionality would be useful in cases where the simulation contains more than two perturbed attributes further both plotperformanceoat and plotperformancespace contain function arguments to use the best available replicates in terms of simulation optimisation fitness further layers of climate information typically downscaled climate change projections may be added to the visualised performance space to examine the plausibility of the investigated hydroclimate scenarios the climate projections are plotted as points with the attributes of the hydroclimate time series indicating the plotting coordinates to overlay additional climate information the function calculateattributes must first be used to determine the attributes of the provided hydroclimate time series this climate attribute information must then be stored as a data frame for supply to plotperformancespace via the argument climdata for format details see supplementary material d additionally in instances where the system model has been used to simulate system performance resulting from the projected climate time series there is an option to display the system performance of each projection by colouring these points on the same scale as the performance space this coloured overlay is created if the relevant system performance values are available within the climdata argument provided to plotperformancespace 3 5 plotperformancespacemulti plotperformancespacemulti visualises system performance in terms of whether nominated design thresholds are satisfied visualizing system performance with reference to specific design objectives i e thresholds allows for clearer comparisons between competing designs plotperformancespacemulti allows the system s performance in terms of multiple performance measures to be visualised on a single plot i e where multiple design objectives are being evaluated satisfaction of the design thresholds is indicated via shading of the plot region e g fig 9 again the function arguments provide control over the visualisations to specify the plot axes and subset attributes select colour scales and overlay downscaled climate projections 3 6 plotoptions to provide a direct visual comparison of a system s performance against the performance of another evaluated system option e g system design or configuration of interest the change in system performance between the two systems can be plotted as a heatmap using plotoptions this function plots the difference between system performance for a given system performance measure for different system options using two perturbed attributes as the co ordinates to illustrate the changes in system performance with the perturbations in these attributes design thresholds for the two systems can also be overlaid onto the space so that the location of each threshold with respect to the climate attributes can be seen as well as enabling a direct comparison of any shifts in system performance due to differences in the underlying system e g fig 10 control over the visualisations to specify the plot axes and subset attributes select colour scales and overlay downscaled climate projections is provided via the function arguments 4 example application an example climate impact assessment is presented to demonstrate the five step framework and functionality of foresight this example uses a stylised and hypothetical domestic rainwater tank system outlined in section 4 1 the rainwater tank is used as it represents a simple climate sensitive system yet with enough complexity to illustrate key concepts most foresight applications would be of much more complex systems following an investigation of tank system behaviour under historical climate section 4 3 foresight is used to implement the five step scenario neutral climate impact assessment framework to assess the performance of two competing tank system designs and determine which design is preferable which is defined here as the design that performs satisfactorily under the greatest number of hydroclimate scenarios section 4 4 4 1 example system model the example system model simulates a domestic rainwater tank system which can be used for both garden irrigation and indoor grey water applications rain falling on the roof of the house is captured and directed towards the rainwater tank before the rainwater is able to enter the tank the first flush is removed from the start of each storm this is required for water quality reasons and equates to the first one to 2 mm of water falling on the roof the water remaining after the first flush flows into the rainwater tank water demand is assumed to be constant throughout the year for indoor use and to vary seasonally for outdoor use the amount of water supplied by the tank depends on the current water level in the tank calculated at a daily time step the system s outdoor water use pattern is based on arbon et al 2014 for adelaide south australia the outdoor seasonal demand pattern responds to the daily temperature i e on hot days above 28 c the garden is watered more than the seasonal average but on cool days below 12 c the garden is watered less than the seasonal average the tank model simulates each stage of the rainwater capture and use process based on the supplied daily rainfall and temperature time series performance of the tank can be measured according to five metrics reliability average daily deficit volumetric reliability system efficiency and storage efficiency the size of the tank roof and first flush diverter can be varied in the model this example model provides sufficient scope for an illustration of foresight s functionality this is because the tank responds to multiple climate drivers i e rainfall and temperature and the removal of the first flush volume at the start of the storm means that the wet dry pattern of the rainfall and the seasonality of the demand pattern may become important 4 2 the tank system design problem the framework is demonstrated in the context of a hypothetical domestic tank system design problem in this example problem a household in adelaide south australia has been provided with two competing quotes for a domestic tank system the two tank systems will be modelled using the example system model each tank system comprises a tank pump first flush diverter and pipe for connection from roof to tank specifications for the two systems differ however the cost of both is the same the details of each system are provided in table 4 the supplier of tank system a recommended adding additional roof area by also connecting the garage roof to the tank but specifies a smaller tank volume of 2400 l whereas system b is fed only by the house roof area but has larger tank volume of 2750 l to store runoff due to the difference between the two systems the householder suspects they may have different behaviours under changing climate conditions the householder is concerned about performance from two perspectives how many days can the tank supply the full demand and on average how much water needs to be purchased to be able to meet the required demand therefore the climate impact assessment will examine both average daily deficit and reliability as performance metrics the householder has set two minimum performance thresholds a minimum reliability threshold of 81 and for budget reasons a maximum average daily deficit of 28 l day 10220 l year 4 3 initial assessment of tank system performance under historical conditions prior to undertaking a scenario neutral assessment it is typical to review what is currently known about the performance of the system via an initial review of system understanding this preparatory step is emulated as part of this example problem implementation although not sitting within the five step framework and described in this section for this example the initial review of system entails an investigation of system performance under historical conditions and is conducted using the example system model and observed rainfall and temperature time series the system model is run with an observed time series of 10 years 2007 2016 of daily rainfall mm and daily mean temperature oc so that the operating behaviour of each system can be assessed the historical climate is mediterranean with an average annual total rainfall of 445 mm and an average annual temperature of 17 5 c the majority of rainfall occurs in the winter and autumn months march august with the hottest days generally occurring in january fig 6 shows the operation of the two tank systems and demand pattern for an illustrative one year period the red jagged spikes and dips in the demand pattern indicate days where the demand pattern is influenced by temperature extremes although tank system b has a greater tank volume than tank system a both struggle to deliver demand and run dry in similar periods see fig 6 days 1 80 over the ten year period system a exhibited an average reliability of 81 and a 27 l day average deficit and system b exhibited an average reliability of 82 and a 26 l day average deficit so that both systems meet the minimum performance threshold set by the householder the performance of both systems in terms of average daily deficit and reliability varies substantially year to year i e reliability ranging from 73 to 93 and average daily deficits from 11 to 37 l day therefore to investigate the longer term average system performance the system models were run using 300 years of rainfall and temperature simulated using weather generators calibrated to the historical data when using this stochastically generated data the longer term average performance of the systems is an average daily deficit of 24 l day and 83 reliability for system a and an average daily deficit of 23 l day and 84 reliability for system b as done in broderick et al 2019 these initial simulations demonstrate that both tank systems satisfy the threshold design objectives maximum deficit 28 l day and minimum reliability 81 and perform similarly to the historical climate conditions 4 4 application of the scenario neutral climate impact assessment framework via the foresight r package the five step scenario neutral climate impact assessment framework is applied to evaluate the two competing tank system designs supported by the foresight r package functionality outlined in section 3 the r code to perform each step of the framework is available as supplementary material see supplementary material e 4 4 1 step a identify attributes for perturbation and create and exposure space the first framework step focuses on the identification of hydroclimate attributes for perturbation the procedure begins with an initial assessment of what climate attributes should be selected for perturbation in the system stress test there is necessary upfront investment in exploring the system s behaviour for a subset of scenarios so that attributes to which the system is more likely to be sensitive are included in the full system stress test in contrast attributes to which the system is deemed insensitive are held at historical conditions i e not perturbed to limit the computational overheads involved in generation of additional hydroclimate time series and system performance this investigation essentially comprises a narrow implementation of the steps b to d of the framework i e in terms of the range of hydroclimate scenarios investigated and results in the specification of the exposure space for the later framework steps see culley et al 2021 for this demonstration an informal variable selection approach was implemented that assessed the sensitivity of systems a and b in two parts an initial sensitivity test to simply scaled hydroclimate time series followed by a more detailed investigation using stochastically generated hydroclimate scenarios both of these sensitivity tests are performed using the createexpspace and generatescenarios functions with createexpspace specifying that attributes are perturbed on a one at a time basis i e only one climate attribute was perturbed while the remaining were held as close as possible to historical levels first the sensitivity of systems a and b to the simple scaling of the observed annual total rainfall by 20 in 10 increments and annual average temperature by 2 c in 1 c increments was assessed the simply scaled rainfall and temperature time series were produced by setting the controlfile argument to scaling within generatescenarios the generated hydroclimate time series were used to drive the relevant system models to determine the system s response to the perturbed climate conditions for the two tank systems the simple scaled time series investigation demonstrated that system performance changes with both average total annual rainfall and to a significantly lesser extent average annual temperature for both the average annual deficit and reliability performance metrics this initial test suggests that further explorations of changes in rainfall attributes are required and should be prioritised over temperature as a result of this initial investigation the stochastically generated hydroclimate time series investigation focuses on exploring the systems response to changes in rainfall attributes specifically those related to the wet dry pattern and seasonality these time series consider the perturbation of annual total rainfall by 80 110 the ratio of total summer to winter rainfall by 80 130 the annual number of wet days by 85 105 and the annual number of days with rainfall greater than 10 mm by 90 125 with four uniformly spaced samples of each attribute while holding average annual temperature at historical levels these tested ranges were informed by the climate projections from the sa climate ready dataset goyder institute for water research 2015b charles and fu 2015 goyder institute for water research 2015a used later in step d section 4 4 4 the default temperature and precipitation weather generators in foresight are used together to generate these time series modeltype wgen modelparametervariation harmonic for both stochastic models the parameters throughout the year are specified via a harmonic function to represent seasonality as the timing of seasons is not being tested the phase angle parameters that control translation of the seasonal cycle of the stochastic rainfall model are fixed at historical levels likewise as temperature is not being perturbed the parameters of the temperature model are fixed at historical levels this reduction in the number of parameters reduces the search space for the optimisation algorithm used in the inverse approach to find suitable parameter sets and helps to create consistency between scenarios twenty replicates of a three hundred year period were simulated for each target location here seven instances with the smallest fitness scores i e closest to the target at each target location are retained and used in system stress testing the remaining replicates are discarded to avoid the inclusion of poorer performing replicates in the stress test i e those with simulated targets that substantially differ from those requested fig 7 shows tank system s response to the one at a time perturbation of four climate attributes annual total rainfall seasonal rainfall ratio the ratio of total wet season rainfall to total dry season rainfall annual number of wet days and annual number of days with rainfall greater than 10 mm for average annual deficit fig 7a d fig 7i l and average annual reliability fig 7e h fig 7m p the figure is created using the plotperformanceoat function in foresight for both average annual deficit and reliability the tank systems were most sensitive to changes in the annual total rainfall and the seasonal rainfall ratio the systems were less sensitive to changes in the number of wet days and the number of days with rainfall greater than 10 mm the same relative sensitivities were seen for both systems a and b hence the climate attributes annual total rainfall and seasonal rainfall ratio were identified for perturbation in step b for this demonstration the output of step a is an exposure space corresponding to perturbations in the annual total rainfall 80 110 and seasonal rainfall ratio by 80 130 in 3 increments in combination yielding a two dimensional attribute space with 160 targets where these exposure space characteristics are specified via createexpspace 4 4 2 step b generate hydroclimate scenarios having specified the exposure space step b of the framework focuses on the generation of hydroclimate scenarios due to the nature of the identified attributes i e annual total rainfall and seasonal rainfall ratio hydroclimate scenarios i e time series stochastic simulation is used for scenario generation the output of step b is the set of stochastically generated hydroclimate time series corresponding to the exposure space specified in step a here generatescenarios was used to stochastically generate hydroclimate time series corresponding to the exposure space specified in step a i e perturbations in the annual total rainfall 80 110 and seasonal rainfall ratio by 80 130 in 3 increments in combination yielding a two dimensional attribute space with 160 targets for this demonstration 20 replicates of a three hundred year time period were simulated the weather generator and optimisation parameters i e controlfile optimisationarguments are the same as those used for the one at a time stochastic scenario generation at each target location the seven instances with the smallest fitness scores i e closest to the target are retained and used in system stress testing 4 4 3 step c simulate system performance step c proceeds by determining the system performance in response to each of the hydroclimate time series i e scenarios generated in step b and is supported within the foresight package via the runsystemmodel function in this demonstration step c is undertaken twice i e for both system options a and b each of the generated hydroclimate time series are used as inputs to the system model to stress test each system and determine the resulting system performance i e average daily deficit and reliability using the runsystemmodel function the resulting performance metrics for each evaluated system are used as inputs to subsequent steps d and e 4 4 4 step d analyse system performance step d of the framework visualises the resulting system performance values with reference to the hydroclimate scenarios attributes the system s performance can then be interpreted and evaluated with reference to the purpose of the system the performance of tank system simulated in step c was visualised using the plotperformancespace function fig 8 shows the performance of the two tank system configurations in response to changes in annual seasonal rainfall ratio and annual total rainfall as quantified by the performance metrics average daily deficit left panels and reliability right panels system performance is displayed using the plotperformancespace function for tank system option a fig 8a b and b fig 8c d for both tank system options the performance in terms of both deficit and reliability changes with both total annual rainfall and rainfall seasonality both systems and performance measures are slightly more sensitive to changes in the rainfall seasonality than changes in the annual total rainfall additional climate information in the form of downscaled climate projections were overlaid according to their time series attributes calculated via calculateattributes to put the system stress test in the context of projected climate trends and provide an indication of what hydroclimate scenarios are more plausible the climate projections shown are from the sa climate ready dataset for a time period centred on year 2050 under rcp 8 5 goyder institute for water research 2015b charles and fu 2015 goyder institute for water research 2015a a sample of 600 projections from the dataset are shown 100 statistically downscaled replicates from six global climate models that were indicated as better performing by the dataset authors see charles and fu 2015 the majority of the downscaled climate model projections show a decrease in both mean annual total rainfall and seasonal rainfall ratio with only 23 of the projections showing an increase in the mean seasonal rainfall ratio and 5 showing a small increase in mean annual total rainfall the performance spaces are then assessed with respect to the threshold design objectives to enable a comparison between the two competing tank system options here plotperformancespacemulti is used to visualize which scenarios satisfy or violate the imposed design thresholds and to again overlay downscaled climate projections to provide context the differences between the two systems in terms of their operation and hence performance becomes more apparent when the two systems are evaluated against the system design thresholds fig 9 shows the systems performance with reference to the two threshold design objectives with yellow indicating that no thresholds are exceeded i e no action required for this climate scenario orange where one threshold is exceeded and dark pink indicating that two thresholds are exceeded for visual clarity climate projections for 2050 under rcp 8 5 see section 4 4 4 are overlaid on these panels so that the user can determine where the projections lie in terms of the design thresholds for tank system a see fig 9a 50 of the stress tested scenarios produce system performance values that are in violation of one or more of the design thresholds i e percentage area of the performance space shaded orange or dark pink in contrast for tank system b see fig 9b 30 of the stress tested scenarios are in violation of one or more of the design thresholds hence tank system b demonstrates tolerance to a wider range or climate scenarios than tank system a performance of the two tank systems can then be interpreted in light of the overlaid climate projections for tank system a fig 9a the majority of the projections fall in the yellow area i e no thresholds violated and approximately 6 the downscaled projections fall inside the shaded orange area indicating that the system s operation violates one of the design thresholds in this case the reliability threshold of 81 approximately 11 of climate projections violate both thresholds for tank system b the majority of the projections again fall in the yellow shaded area the area where none of the thresholds are violated and the system is operating as required fig 9b in this case only 1 of the climate projections are in violation of both design thresholds and less than 4 are in violation of one design threshold reliability 4 4 5 step e evaluate system options step e of the framework proceeds by directly comparing the stress tested performance of multiple system options with respect to threshold design objectives this step is facilitated in the foresight package via the use of plotoptions which visualises the difference between two performance spaces as a heatmap as well as overlaying design thresholds for the two systems to enable a direct comparison of differences in the evaluated systems fig 10 shows the difference in performance of the two systems system b system a with respect to threshold design objectives for the two performance measures average daily deficit fig 10a and reliability fig 10b again climate projections for 2050 under rcp 8 5 see section 4 4 4 are overlaid on these panels so that the user can determine where the projections lie in terms of the design thresholds for both system performance measures the threshold for system b is to the right of the threshold for system a indicating that tank system b operates satisfactorily for a wider range of hydroclimate conditions again fewer climate projections lie outside of system b satisfactory operating range for both system performance measures see fig 9 and discussion in section 4 4 4 following the application of the framework to the example system supported by the foresight r package the results indicate that tank system b is preferable as it should operate satisfactorily across a wider range of conditions including the drier climate projected by the climate models for the 2050 time period 5 discussion the presented scenario neutral framework and software have arisen from a concerted effort to formalise the modelling and analysis components of scenario neutral assessments and to reduce barriers to entry for the wider community the framework and r package are designed to enable the execution of many standard approaches such as simple scaling prudhomme et al 2010 culley et al 2016 as well as newer approaches to scenario generation i e the inverse approach to stochastic generation of time series guo et al 2018b culley et al 2019 nevertheless unlike scenario led approaches research into scenario neutral approaches is still emerging with frameworks and examples only published in the last decade prudhomme et al 2010 wilby and dessai 2010 brown and wilby 2012 brown et al 2012 poff et al 2015 and to the authors knowledge this being the first openly available enabling software tool a range of limitations associated with the methodology together with potential future directions for software improvement are now described 5 1 current limitations of scenario neutral approaches current limitations of the foresight r package stem primarily from the current limitations of scenario neutral approaches for example caution should be exercised when using system models under different conditions from which they have been calibrated this caution has been raised by many authors guo et al 2018a fowler et al 2016 coron et al 2012 in relation to assumptions surrounding the performance of environmental models that have been calibrated to historical climate time series and that therefore may not perform adequately under changed climate conditions expert judgement is required to assess whether the system model sufficiently represents the system s processes to be capable of satisfactorily simulating system performance under a change in climate regime or whether alternative model structural representations are required gibbs et al 2018 guo et al 2020 analytical methods for dealing with this challenge include differential split sampling westra et al 2014 fowler et al 2016 and approaches based on pareto optimality fowler et al 2016 similarly different stochastic weather generators have been shown to perform differently in different climate regimes vu et al 2017 this poses a challenge for the stochastic generation of hydroclimate time series since not all driving stochastic weather generators are equally suited to the generation of the hydroclimate scenarios required to effectively stress test a system in addition to this challenge as more flexible weather generator options are used which typically have a larger number of parameters the search space for the inverse approach increases making it more difficult and computationally expensive for the optimisation algorithm to find suitable parameter sets for time series generation guo et al 2017 further challenges exist in specifying constraints on the hydroclimate scenarios such that the time series generated represent plausible hydroclimate conditions that the system may experience see guo et al 2017 guo et al 2018b 5 2 potential future advances there are a number of opportunities for the extension of the framework and r package including incorporation of tools for formally identifying critical attributes for stress tests culley et al 2021 and guo et al 2017 have both explored methods for the identification of relevant climate attributes for stress testing although difficulties remain in the identification of all attributes that are influential on system performance culley et al 2021 guo et al 2017 gains have been demonstrated in the amount of system sensitivity able to be captured in system stress tests by using a systematic approach for attribute selection development of methods for generating a broader range of perturbed hydroclimate scenarios including shifts in spatial subdaily and inter annual attributes the spatial subdaily and or inter annual variability of supplied hydroclimate time series have the potential to have an important influence on the performance of certain systems but the capacity to vary time series at these scales is not yet supported within foresight for example spatial rainfall variability is known to influence catchment dynamics segond et al 2007 obled et al 1994 singh 1997 smith et al 2004 options for producing spatial hydroclimate timeseries include the implementation of a gridded multivariate multisite weather generator steinschneider and brown 2013 or extension of the inverse approach to space guo et al 2018b using a continuous spatial rainfall model e g bennett et al 2018 leonard et al 2008 augmentation of step e evaluate system options to complement the analysis of multiple system designs or management options additional tools may be adopted for conducting decision analysis for example robustness type approaches mcphail et al 2018 mcphail et al 2020 could be applied to assist in the evaluation of multiple system design options likewise this analysis step could be used to develop trigger points for use as part of adaptive pathway approaches haasnoot et al 2013 6 conclusions this paper presents a framework for effective scenario neutral climate impact assessments and new r package foresight to support each framework step the five framework steps and corresponding package functions enable the strategic evaluation of system performance across a wide range of hydroclimate conditions without significant efforts in the development of models for generating suitable hydroclimate time series simulating system performance or visualizing and analysing resultant system performance against design objectives likewise the r package also enables easy repetition of investigations to evaluate and compare different system management or design options this should enable the application of scenario neutral assessment principles to new problem domains that manage complex interfaces between systems and the hydroclimate the new framework and r package functionality were demonstrated for a simple domestic water supply system design in which two competing system options were assessed to determine which system represented the best investment in terms of the system that performed satisfactorily across the greatest range of plausible hydroclimate conditions computational details the case study results in this paper were obtained using r 4 0 3 using the presented package foresight the package relies on the following key dependencies ga v3 0 2 scrucca 2013 and ggplot2 v3 3 0 wickham 2009 which are both available from the comprehensive r archive network https cran r project org software availability description package foresight version 1 0 0 developers bree bennett sam culley anjana devanand seth westra danlu guo and holger r maier year first available 2018 e mail bree bennett adelaide edu au website https cran r project org package foresight hardware requirement general purpose computer software requirement r version 3 6 1 or later programming language r declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the goyder institute for water research dr sam culley and dr danlu guo were supported by research training program scholarships the authors gratefully thank the testers of the foresight r package dr michael leonard dr cameron mcphail and participants of the goyder institute for water research craft workshop appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 104999 
25864,effective implementation of a scenario neutral climate impact assessment relies on the integration of many modelling components at multiple stages from the generation of appropriate climate boundary conditions that can be used to rigorously stress test a system to the simulation visualisation and interpretation of resulting system performance for systems with complex temporal dynamics the generation of climate forcing time series combined with the significant simulation and visualisation demands represent a barrier to the wide scale adoption of scenario neutral approaches a unified five step framework and supporting r package foresight systems insights from the generation of hydroclimate time series are introduced that enable the application of a scenario neutral climate impact assessment from appropriate time series generation to the analysis of system performance the software can be applied to compare both current system performance and selected alternative system management or design options and its application is demonstrated for a case study keywords climate change impact assessment scenario neutral bottom up stochastic rainfall inverse approach r package 1 introduction climate variability and change present significant challenges for natural and engineered systems across a wide variety of sectors such as the municipal agricultural energy mining industrial transport and environmental sectors piao et al 2010 schaeffer et al 2012 vermeulen et al 2012 pearce et al 2011 love et al 2010 olesen and bindi 2002 often these systems are required to operate effectively under a range of hydroclimate conditions including being able to withstand climate extremes however the ability to understand system performance under and sensitivity to a range of plausible climate conditions is confounded by the complex interactions between the system and the climate zscheischler et al 2018 2020 leonard et al 2014 yet traditional top down methods of climate impact assessment that rely on applying climate projections to examine changes in a system s response produce information that can be difficult for decisions makers to interpret and apply wilby and dessai 2010 and generally do not enable better understanding of the specific mechanisms by which a system might respond to climatic changes scenario neutral or bottom up approaches have been put forward as a means for better understanding these complex relationships and sensitivities and are increasingly being adopted to evaluate system performance under climatic change and variability brown et al 2012 brown and wilby 2012 turner et al 2014 culley et al 2016 prudhomme et al 2010 these approaches are underpinned by an extensive vulnerability analysis i e a stress test to evaluate a system s sensitivity to changes in the driving hydroclimate variables and potentially assess the viability of alternative management and or design options to date the development and application of scenario neutral approaches has remained mostly within the disciplines of water and environmental science with common foci being the examination of water resource systems brown et al 2012 culley et al 2016 turner et al 2014 adeloye et al 2016 ghile et al 2014 sediment transport bussi et al 2016 changes in streamflow and flood risk guo et al 2017 prudhomme et al 2010 2013 as well as ecological systems poff et al 2015 in contrast to the traditional scenario led or top down approaches which focus on the application of projected changes in climate to a system poitras et al 2011 chiew et al 2009 bell et al 2012 kay and jones 2012 segui et al 2010 paton et al 2014 beh et al 2017 scenario neutral approaches treat the system as the focus of the evaluation brown and wilby 2012 in particular the emphasis is on quantifying system sensitivity to climate and identifying associated performance thresholds and or decision boundaries rather than treating the system model merely as a filter through which climate model projections are passed note that climate change projections still can be used to inform the analysis but for scenario neutral approaches they are not the central concern or mechanism for evaluating response to change brown and wilby 2012 nazemi and wheater 2014 as a result the focus of scenario neutral approaches on the assessment of system sensitivity to a range of plausible climate futures provides a method to better understand the system specific mechanisms by which climatic changes can drive failure or performance degradation which in turn can provide a rigorous basis for adaptation decisions the stress test that underpins scenario neutral approaches allows the strategic assessment of a system s performance by providing decision relevant information on the system s response to a range of plausible hydroclimate scenarios this information may include identification of critical variables that affect the vulnerability of the system and decision critical thresholds beyond which a system does not perform desirably poff et al 2015 turner et al 2014 following the system stress test stage scenario neutral approaches can then take advantage of multiple lines of climate evidence to add context and understand possible future climate changes lines of evidence may include climate model projections by combining global climate models with dynamical and or statistical downscaling or bias corrections brown and wilby 2012 historical climatic changes expert judgement and or analogues from paleo records implementation of scenario neutral approaches requires consideration of a number of steps including i the determination of which hydroclimate variables drive system performance and what changes in these hydroclimate variables should be included in the stress test to ensure relevant system behaviour and or failure modes are exposed ii the generation of hydroclimate timeseries that reflect these changes iii the simulation of system performance in response to the changes in hydroclimate variables via a system model iv the analysis and visualisation of system performance in the context of additional climate information and v the evaluation of system options e g system design or operating conditions in light of the simulated system response to date studies have largely focused on each of these steps in isolation developing techniques for particular analysis stages e g identification of critical variables guo et al 2017 generation of perturbed hydroclimate time series guo et al 2018b steinschneider and brown 2013 culley et al 2019 the system performance and decision analysis stages culley et al 2016 or the integration of ecological performance measures poff et al 2015 or applied a general stress test work flow to a variety of sectoral applications hirpa et al 2018 tra et al 2018 turner et al 2014 ghile et al 2014 brown et al 2011 however the interaction between the steps can result in a number of issues in particular careful and systematic implementation of each component with the specific and detailed focus on the system under investigation is required to effectively stress test the system and to produce information that is interpretable in a decision making context for example the failure to include a critical variable in the assessment that may affect the system s vulnerability undermines the efficacy of the stress test and will likely lead to missed modes of failure thereby undermining the overarching motivation of the scenario neutral approach culley et al 2021 while there have been a few attempts at developing generalised frameworks for scenario neutral analysis e g poff et al 2015 broderick et al 2019 ray and brown 2015 there are a number of unique challenges for the application of bottom up analyses to dynamical systems for which the generation and application of time series hydroclimate boundary conditions is critical these dynamical systems include many environmental agricultural water resource e g natural systems like groundwater as well engineered systems like reservoirs and renewable energy systems as well as other systems that depend on one or several of the former e g transportation systems are often sensitive to natural hazards such as flooding consequently there is a need to formalise the various analysis stages of scenario neutral approaches as a unified framework with a specific focus on the modelling and analysis components required by these dynamical systems bringing together advances in best practice at each stage to obtain the most benefit from scenario neutral climate impact assessments and improve modelling practice this barrier to wide scale adoption is further enhanced by the logistical challenges of implementing a scenario neutral analysis in particular generating perturbed hydroclimate time series capable of strategically stress testing the system to specific aspects of possible future change managing the integration multiple models and data streams throughout the analysis as well as the visualisation and interpretation of system performance in higher dimensions while there are number of existing software packages to support climate impact assessments these do not satisfy the requirements outlined above as they often do not provide end to end support for climate impact analyses and focus on supporting top down analyses in particular downscaling climate projections e g sdsm wilby et al 2002 climdown hiebert et al 2018 rsqm cho et al 2018 dsclim pagé et al 2009 clim pact benestad 2009 mbc mehrotra et al 2018 analysing downscaled products e g musica hanel et al 2017 or the generation of scenarios e g wilby et al 2014 to the authors knowledge there are currently no available software tools to assist with applying a scenario neutral approach to climate impact assessment from start to finish given the range of technical and logistical challenges involved in implementing a scenario neutral impact assessment there is a need to develop a software platform that enables these steps to be implemented in an easy to use fashion and conforming to principles of accessible consistent and reproducible implementation of modelling practice humphrey et al 2017 iturbide et al 2019 knox et al 2018 as demonstrated in other areas of environmental modelling guo et al 2016 galelli et al 2014 humphrey et al 2017 stokes et al 2015 guillaume et al 2016 in response to these challenges the objectives of this paper are 1 to present a unified framework for scenario neutral climate impact assessments the framework seeks to formalise current approaches to conducting stress tests for dynamical systems into an end to end workflow to implement the modelling and analysis components of a scenario neutral approach 2 to introduce the r package foresight which facilitates implementation of the framework workflow thereby reducing the barriers of entry to scenario neutral climate impact assessments the remainder of this paper is structured as follows section 2 provides an overview of the integrated scenario neutral framework the structure and functionality of the foresight package are then presented section 3 followed by a demonstration of the package for the example problem of a domestic rainwater tank system used for indoor and outdoor supply section 4 section 5 discusses limitations of scenario neutral approaches as well as potential advances followed by conclusions in section 6 2 a modelling and analysis focused scenario neutral framework this paper presents a generic scenario neutral framework so that it can be used for a range of application areas the framework focuses on the modelling and analysis components of scenario neutral climate impact assessment as distinct from other considerations around application of the broader components that may include stakeholder consultation as discussed this is of particular importance for dynamical systems whether natural or engineered the modelling and analysis components of scenario neutral climate impact assessments are formalised as a framework with five steps the framework starts by first identifying what properties of the hydroclimate variables hereafter termed attributes affect system performance step a this is followed by the generation of a set of hydroclimate time series that correspond to plausible perturbations in the identified attributes step b these time series are used to stress test the system and the system performance in response to these hydroclimate time series is then simulated step c the system s performance is analysed and visualised with reference to the system s objectives e g performance above a threshold step d additional climate information can also be overlaid onto the visualised system performance to provide an indication of what hydroclimate scenarios are more plausible as part of this step steps c and d are repeated for each investigated system design adaptation option the performance of each system option under the stress test is then compared step e to provide information relevant to the overarching system investigation i e to inform the selection and assessment of a particular system design operation or adaptation options the framework steps are outlined here with an overview of the relevant considerations for implementing each step as well as the interactions between steps a identify attributes for perturbation and create an exposure space existing knowledge of the system s behaviour and preliminary testing of the system under a subset of plausible changes to the hydroclimate time series is used to identify a larger set of climate attributes required for perturbation this preliminary investigation may comprise a narrower implementation of steps b to d of the framework in terms of the range of hydroclimate scenarios investigated e g culley et al 2021 this step ensures that the downstream stress testing investigations steps b to e focus on appropriate hydroclimate scenarios capable of strategically stress testing the system b generate hydroclimate scenarios attributes that have been identified as influential for the system s performance are nominated for perturbation to define a set of hydroclimate scenarios i e time series for simulation hydroclimate time series are then generated that correspond to each scenario using an appropriate time series generation technique e g simple scaling or simulation using a stochastic weather generator the requested set of hydroclimate scenarios will influence the selection of an appropriate time series generation technique for example the perturbation of dry spell length while keeping the annual average rainfall at historical levels necessitates the use of a stochastic weather generator e g guo et al 2018b culley et al 2019 whereas the perturbation of annual average rainfall without holding other climate attributes at historical levels could be achieved with simple scaling of the historical rainfall time series e g tra et al 2018 culley et al 2016 c simulate system performance each of the hydroclimate time series i e scenarios generated in the previous step are used as inputs to the system model to simulate system performance based on one or several identified performance measures d analyse system performance the resulting set of system performance values are visualised with reference to the hydroclimate scenarios attributes the system s performance can then be interpreted and evaluated with reference to the purpose of the system for example a system s performance may be evaluated with reference to design performance levels such that scenarios under which the system is deemed to perform satisfactorily or fail can be identified e g culley et al 2016 tra et al 2018 additional climate information may also be overlaid as part of this step e g modelled climate projections paleo climatological information to add context and understand possible future climate changes steps c and d are repeated for each system option evaluated e g alternate system designs or management options such that all system options are stress tested and analysed with respect to the same plausible hydroclimate scenarios this enables a systematic comparison of each system option if only a single system configuration is evaluated the analysis terminates at step d e evaluate system options in the final step alternative system options or alternatively the current system together with one or more alternative options are compared against one another the change in performance from one option to another is assessed in terms of differences in the simulated system performance measures this comparison of performance between different system options provides decision relevant information that can be considered alongside further contextual information about the competing system options i e cost time to implement each option for example the impact of changing storage within the system on system reliability could be evaluated again additional climate information may also be overlaid as part of this step further iteration may be required depending on the configuration of the system testing of decisions and design options between steps c and e if a new system design is being refined the five framework steps and the interactions described above are represented schematically in fig 1 where the solid grey arrows represent the steps included in all analyses and the striped grey arrows indicate where a repetition of steps is required due to the evaluation of multiple system configurations or the requirement for iteration to refine the design of a new system the options for specific techniques implemented by the r package for each step are detailed separately in section 3 3 the foresight package foresight is an r package that implements the methods of the five step framework for scenario neutral climate impact assessment detailed in the previous section the key functions and the process flow of using the foresight package are shown in fig 2 including the connection between the functions and the steps of the framework summarized in fig 1 the core functionality of the package is to generate hydroclimate series with desired perturbed attributes for climate stress testing the ability to generate realistic perturbed hydroclimate time series is integral to multiple stages of the framework and thus the package the stochastic implementation in this package uses stochastic weather generator models see wilks and wilby 1999 for a review of stochastic weather models for the climate variables and optimises the parameter values of the stochastic models to generate time series with desired attributes the optimisation algorithm operates within the specified bounds of the stochastic model parameters to arrive at optimum parameter values that minimise deviations in the attributes of the generated time series from the specified target values the package includes multiple stochastic model options that differ in model structure and complexity the process flow of using the package involves creating an exposure space via selecting climate attributes for perturbation and defining target values of the attributes generating perturbed hydroclimate time series stress testing the system analysing the results and evaluating investigated system options through a comparison of their performance under the stress test the implementation of the five step framework corresponds to seven key functions in the package together with a large number of additional helper functions that are described in more detail in the package documentation the first function createexpspace creates an exposure space defined by the attributes of the time series that are selected for perturbation the number of dimensions of the exposure space is equal to the number of perturbed attributes specified by the user the exposure space consists of target locations that correspond to combinations of the specified values of the perturbed attributes the second function generatescenarios generates the perturbed hydroclimate time series by optimising the parameters of stochastic models to generate data with specified target values of the attributes or via applying multiplicative or additive changes to the observed reference timeseries depending on the specified perturbation method for any application the selection of perturbed attributes for stress testing the system typically involves iterating through steps a to d to identify relevant attributes and generate realistic perturbed time series generatescenarios may be used to perturb attributes individually i e one at a time to select the most important attributes for subsequent stress testing or jointly as part of the full system stress test following the generation of suitable sets of perturbed hydroclimate time series runsystemmodel is used to run the system model and simulate the system performance response to the generated perturbed time series step c the system model quantifies the system performance in terms of one or several metrics that are relevant for the specific application with metrics calculated as part of the system model i e outside of the foresight package it is noted that foresight does not have inbuilt system models other than to support a tutorial but instead requires development of a wrapper function to call an external system model the plotting functions in the package plotperformanceoat plotperformancespace and plotperformancespacemulti visualize the system performance to aid decision making plotperformanceoat and plotperformancespace create plots of the performance metric values step d similarly plotperformancespacemulti is used to create plots combining more than one performance metric plotperformancespace and plotperformancespacemulti can also be used to overlay climate information available from alternate sources if multiple performance metrics are important for the system or if multiple system configurations operating policies are to be compared runsystemmodel plotperformanceoat plotperformancespace and plotperformancespacemulti can be used multiple times to repeat steps c and d finally plotoptions can be used to compare system performance across different system configurations step e each function and its required arguments are briefly described in fig 2 each stage of the scenario neutral approach as set out via the functions is discussed in turn 3 1 createexpspace the creation of an exposure space involves specifying the hydro climate attributes that are perturbed i e attperturb those that are held constant i e atthold and the combination and domain over which the attributes are sampled 3 1 1 specifying the attributes attperturb and atthold attributes that have been identified as potentially influential to system performance should be nominated for perturbation using the attperturb argument the selection of attributes for stress testing is typically arrived at through sensitivity assessment of the system performance to changes in various hydro climate attributes culley et al 2021 where the stochastic method of generating hydroclimate scenarios is used atthold can be used to specify which attributes should be held at the levels of the reference time series the attributes supplied by atthold put additional constraints on the generated time series so that parameters are chosen to implement the perturbation with minimal change to the properties of the time series not designated for perturbation as discussed further in culley et al 2019 this is critical for the production of realistic hydroclimate time series attperturb and atthold would typically include attributes of multiple hydro climatic variables that affect the system table 1 sets out the attributes available for perturbation or holding using the foresight package the attribute names are specified in foresight using the following format hydroclimatevariable period statistic m for example the average total rainfall for the months june july and august would have the attribute name p jja tot m whereas the average annual number of frost days would be nominated using temp ann f0 m the m at the end of the attribute name denotes mean since all the attributes are calculated as climatological means of the time series the attributes available for use in foresight may be viewed using the function viewattributes 3 1 2 specifying the perturbations attperturbsamp attperturbmin attperturbmax attperturbtype the arguments attperturbtype attperturbmin attperturbmax attperturbsamp to createexpspace are used to specify how the attributes should be perturbed the function outputs a list of targets i e combinations of held attributes and perturbed attributesspecified as deviations from historical climate that describes the target locations in the exposure space time series that correspond to these target locations will be produced in the next step of the framework attperturbtype sets how the attribute perturbations are sampled with two currently supported options one at a time oat and regular grid reggrid fig 3 one at a time testing can be used to evaluate the sensitivity of a system to a single attribute changing while the remainder are held as close as possible to the level of the observed hydroclimate time series each of the nominated attributes is perturbed in turn in the case of reggrid sampling the attributes are perturbed simultaneously to create an exposure space encompassing targets corresponding to all combination of perturbations fig 3b the attperturbmin attperturbmax and attperturbsamp arguments are necessary to define the bounds of the perturbation and the number of samples desired for each perturbed attribute these arguments are vectors containing elements corresponding to each perturbed attribute attperturbmin and attperturbmax are specified in attribute space which is a deviation from the historical climate in terms of a fraction for rainfall potential evapotranspiration and solar radiation or an absolute deviation for temperature for example a perturbation of annual total rainfall may be specified with attperturbmin of 0 7 and attperturbmax of 1 3 to produce scenarios between a 30 decrease and increase in the total rainfall in contrast attperturbmin of 2 and attperturbmax of 2 for temperature would be used to produce scenarios between a 2 c decrease or increase in the average annual temperature lastly attperturbsamp specifies the number of discrete points to be sampled within the bounds for each dimension perturbed attribute 3 1 3 output createexpspace returns a list that contains the exposure space the list contains a target matrix targetmat each row of which is the location of a desired target point in the exposure space the input arguments attperturb atthold attperturbtype attperturbmin attperturbmax and attperturbsamp are also saved as elements of the output list the output of createexpspace is used an input to generatescenarios the function that generates hydroclimate timeseries corresponding to each target location in the exposure space 3 2 generatescenarios obtaining realistic perturbed hydroclimate time series is a critical foundation for scenario neutral impact assessments of dynamical time varying systems it is these perturbed climate time series that are used to drive the system model in the system stress testing stage step c in foresight the generatescenarios function is used to simulate these hydroclimate time series this function is likely to be the most computationally intensive part of the workflow depending on the system model used in step c the hydroclimate time series are generated with reference to a historical baseline time series reference that is supplied by the user the remaining arguments of generatescenarios specify the exposure space of interest created using the function createexpspace the length of the simulation in years simlengthnyrs and the number of stochastic replicates to be generated numreplicates additional optional arguments to the function include the method of perturbation for scenario generation specified through a json file controlfile and the seed to use for the stochastic simulation seedid 3 2 1 data reference generatescenarios requires a historical or reference set of time series reference to be supplied as an argument each call of generatescenarios begins by calculating the requested attributes of the provided time series reference all scenarios produced are with reference to the attributes of this baseline set of time series the user is expected to supply the input reference in a specific format to the function generatescenarios the first three columns of the data should be named year month day and contain the corresponding date of the observation reference data the additional columns should contain the hydro climate variables of interest named according p temp pet radn table 2 shows the expected input format of reference 3 2 2 exposure space expspace the exposure space contains information about the attributes selected to be perturbed held and the locations of the target points in the exposure space i e the target perturbations the exposure space created using createexpspace should be supplied as an input to the function generatescenarios through the argument expspace 3 2 3 method of perturbation modeltype and modelparametervariation scenario generation is governed by the method of perturbation used and the attributes selected for perturbation scenarios can be generated using one of two types of methods of perturbation 1 simple scaling time series are generated by applying multiplicative or additive changes to observed climate time series or 2 stochastic generation time series with the required attributes are generated using an appropriate stochastic weather generator see various model options in table 3 simple scaling often applies a constant change factor to the entire time series or each season month in the time series e g bracho mujica et al 2019 prudhomme et al 2010 and preserves many of the characteristics of the underlying time series e g rainfall wet dry pattern timing of extremes simple scaling may be chosen as the method of perturbation in generatescenarios by specifying the controlfile argument as scaling simple scaling in foresight works by scaling the supplied reference time series on an annual basis temperature attributes are perturbed by an additive change other weather attributes such as rainfall potential evapotranspiration and solar radiation are perturbed by applying multiplicative changes in contrast stochastically generated time series are produced using an inverse approach guo et al 2018b whereby the desired combination of attributes is specified first and an optimisation approach is used to determine the parameters of the stochastic model required to simulate that specified scenario creating scenarios using stochastic weather generators is useful in that selected attributes of a time series can be changed while still maintaining other attributes at historical levels for example it is possible to change the annual extreme rainfall intensity using the 99th percentile rainfall amount while maintaining the mean annual total rainfall at historical levels using the stochastic generation approach this property provides a powerful tool for generating a wide range of scenarios and thus enables a comprehensive stress test additionally hydroclimate time series produced using stochastic generation can be longer than the observed time series allowing the system s performance to be evaluated over longer time periods the method is also capable of generating multiple time series replicates with the same statistical attributes to represent natural variability the desired number of stochastic replicates may be specified as an input argument numreplicates to the function generatescenarios both simple scaling and stochastic generation methods of perturbation can be implemented using generatescenarios by default generatescenarios uses the method of stochastic generation to create hydroclimate timeseries using the default stochastic weather generators indicated in table 3 in foresight the controlfile function argument may be used to provide a json file input to generatescenarios to specify alternate stochastic models and or modify the bounds of the parameters of the stochastic models the full list of models contained in package version 1 0 0 for each hydroclimate variable is provided in table 1 foresight currently enables daily time series generation via richardson type weather generator model configurations richardson 1981 richardson and wright 1984 as well as latent variable type model configurations bennett et al 2018 2019 rasmussen 2013 to stochastically generate rainfall temperature potential evapotranspiration and solar radiation this initial set of weather generator models was selected for implementation as they each have a relatively small number of parameters this is important for implementing the inverse approach see culley et al 2019 and are established weather generators that will be familiar to users internationally the package s modular structure allows additional weather generators to be added in future the list of stochastic models available in the foresight package may be viewed using the function viewmodels default parameter bounds based on historical australian conditions are provided for each stochastic generator but can be overridden if additional information such as domain knowledge is available these may be specified using the field modelparameterbounds in the input json controlfile to generatescenarios only a limited number of attributes can be nominated for perturbation using the simple scaling perturbation method this is because of the nature of simple scaling which applies an additive or multiplicative change to the observed time series therefore if simple scaling is selected many of the time series underlying properties cannot be changed e g rainfall wet dry monthly and seasonal statistics multiple attributes of the selected hydroclimate variable cannot be perturbed in combination and it is not possible to hold attributes at historical levels in the context of stochastic modelling the selection of a particular model changes the attributes that are available for perturbation using generatescenarios for example an annual rainfall model e g rainfall model with modeltype wgen and modelparametervariation annual does not allow the winter total rainfall attribute p jja tot m to be perturbed differently to perturbations for other months as there are no mechanisms in the stochastic generator to enable this change as a result the package will not allow the user to select this attribute when using the annual model the list of supported attributes for each model can be found in supplementary material a it should be noted that as the models become more complex they can perturb the climate variables in more ways but will also require more attributes to be held at historical levels using atthold to maintain realism of the generated timeseries 3 2 4 optimisation optimisationarguments an important part of implementing the inverse approach is the identification of parameters of the stochastic weather generator that will allow for simulation of the requested climate attributes the inverse approach relies on formal optimisation approaches to identify suitable stochastic model parameters the purpose of optimisationarguments is to facilitate this optimisation process as optimisation is only used in the generation of the stochastic time series optimisationarguments are not required when perturbed time series are constructed using simple scaling i e when controlfile argument is set to scaling foresight uses a genetic algorithm ga to implement the inverse approach for generating stochastic time series a ga is used due to the high dimensionality of the search space guo et al 2017 culley et al 2019 the r package ga scrucca 2013 supplies the genetic algorithm used in hydroclimate time series generation within foresight the objective function supplied to the ga is a distance between the requested target attributes and the attributes calculated from the simulated time series with the euclidian distance used as a default the ga seeks to minimise this distance there is also an option to use a weighted distance function by adding a penalty term to the objective function this can be used to account for situations where attributes may be on different scales or situations where is it desirable to have higher weights placed on particular target attributes such as if they have been identified as more important than others to the system s operation culley et al 2019 optimisationarguments refers to settings or specifications of the ga used for optimisation foresight contains default settings for optimisationarguments used to determine suitable weather generator parameters for the generation of each hydroclimate time series as described by its attributes the settings or specifications of the ga include the number of iterations maxiter the population size of each iteration popsize the probability of mutation in a parent chromosome pmutation and the probability of crossover between pairs of chromosomes pcrossover it is to be noted that these arguments are specific to the ga algorithm from the r package ga scrucca 2013 that foresight relies on for optimisation and the reader is directed to the documentation of the ga r package for more details the default optimisationarguments settings in foresight can be modified by the user by specifying the optimisationarguments field in the input json file supplied as the controlfile argument to generatescenarios if the field optimisationarguments is not included in the controlfile the default optimisationarguments settings are used penalty weights may also be specified for selected attributes in the exposure space these attributes may belong to attperturb or atthold as part of the optimisationarguments field in the controlfile see supplementary material b for examples of controlfiles 3 2 5 output simulation and diagnostics the perturbed hydroclimate time series produced using the generatescenarios function are returned as a list fig 4 shows the format of the output data from generatescenarios the output list contains elements corresponding to each replicate named rep1 rep2 etc an element indicating the dates of the simulation simdates the input exposure space expspace and the controlfile each replicate differs in the seed used for simulation and contains time series corresponding to all target locations in the exposure space named target1 target2 etc each target contains timeseries of the simulated hydro climatic variables in fields named by the variable name p temp etc the simulated data may be used to examine time series characteristics temporal aggregations or summary statistics as desired by the user foresight includes a function plotscenarios that can be used to plot heatmaps to summarise the deviations of the attributes of the simulated time series from the desired target attributes the function is a diagnostic tool to examine how close the attributes both perturbed and held of the time series simulated by the stochastic generators are to the requested attributes the deviations of the simulated attributes from the targets may be high in instances where the specified combination of target attributes is inconsistent or in cases when the stochastic models are insufficiently flexible to represent changes in those specific attributes with sufficient accuracy fig 5 illustrates an example heatmap of mean biases of 20 replicates created using plotscenarios for a scenario of 40 targets and 11 attributes it is also worth noting the challenges associated with computation and storage of larger stochastic simulations the foresight package includes a function getsimsummary that can be used to obtain the summary metadata of a stochastic simulation which is much smaller in size than the full stochastic simulation for ease of storage and use with the downstream plotting functions in foresight on the computational front parallel computing may be used to perform simulations using generatescenarios in cases containing various hydroclimatic variables many scenarios target locations and multiple replicates the internal function structure of generatescenarios supports parallelisation of scenario calculations across replicates and targets 3 3 runsystemmodel runsystemmodel uses the perturbed hydroclimate time series to drive a system model that translates hydroclimate time series into system performance for later visualisation this requires the system response or performance to be simulated from each scenario using a system model that receives continuous hydroclimate time series it is noted that to maximise the flexibility to analyse a range of systems across multiple application areas the foresight software does not include inbuilt system modelling capability but rather requires the development of a wrapper function to an external system model at its core the system model needs to translate hydroclimate time series into one or several measures of system performance the system model should represent the system such that it captures both physical and operating characteristics e g management policies that translate hydroclimate time series into some measure of performance and can include any desired combination of economic social and or environmental metrics e g measures of water and or energy security agricultural productivity ecosystem services system performance can be defined in a variety of ways including binary success failure criteria e g where a threshold critical for system operation is crossed or via continuous performance metrics e g reliability for ease of integration the package allows the system model to be called within the runsystemmodel function the simplest way to generate system performance is to provide the system model as an r function and designate this function as the system model using the argument systemmodel this enables the whole process to be automated using the initial runsystemmodel function call even if the models have been developed in other languages they can still be executed in r with a wrapper function to work inside the foresight package the system model function i e wrapper or otherwise must adhere to a particular format see supplementary material c alternatively this step can be performed outside of r using the output time series from generatescenarios and the system performance results imported for use in later steps 3 4 plotperformanceoat and plotperformancespace to provide a visually interpretable representation of the system s performance across the range of hydroclimate scenarios system performance can be visualised with reference to the attributes of the hydroclimate conditions using plotperformanceoat and plotperformancespace this package supports two graphical options heatmaps and one at a time plots the functions contain multiple arguments to provide control over the aesthetics of these visualisations including colour scales limits and addition of performance thresholds plotperformanceoat creates line plots of the changes in performance with one at a time perturbations in attributes the function creates panelled figures that illustrate the sensitivity of the performance metric to changes in each attribute with shading to show the range of the metric from multiple replicates e g fig 7 plotperformancespace creates heatmap plots using two perturbed attributes as the co ordinates to illustrate the changes in system performance with the perturbations in these attributes the function contains arguments to choose the perturbed attributes to be used as the co ordinates of the figure function arguments are also available to enable sub setting of the perturbation ranges of the perturbed attributes this functionality would be useful in cases where the simulation contains more than two perturbed attributes further both plotperformanceoat and plotperformancespace contain function arguments to use the best available replicates in terms of simulation optimisation fitness further layers of climate information typically downscaled climate change projections may be added to the visualised performance space to examine the plausibility of the investigated hydroclimate scenarios the climate projections are plotted as points with the attributes of the hydroclimate time series indicating the plotting coordinates to overlay additional climate information the function calculateattributes must first be used to determine the attributes of the provided hydroclimate time series this climate attribute information must then be stored as a data frame for supply to plotperformancespace via the argument climdata for format details see supplementary material d additionally in instances where the system model has been used to simulate system performance resulting from the projected climate time series there is an option to display the system performance of each projection by colouring these points on the same scale as the performance space this coloured overlay is created if the relevant system performance values are available within the climdata argument provided to plotperformancespace 3 5 plotperformancespacemulti plotperformancespacemulti visualises system performance in terms of whether nominated design thresholds are satisfied visualizing system performance with reference to specific design objectives i e thresholds allows for clearer comparisons between competing designs plotperformancespacemulti allows the system s performance in terms of multiple performance measures to be visualised on a single plot i e where multiple design objectives are being evaluated satisfaction of the design thresholds is indicated via shading of the plot region e g fig 9 again the function arguments provide control over the visualisations to specify the plot axes and subset attributes select colour scales and overlay downscaled climate projections 3 6 plotoptions to provide a direct visual comparison of a system s performance against the performance of another evaluated system option e g system design or configuration of interest the change in system performance between the two systems can be plotted as a heatmap using plotoptions this function plots the difference between system performance for a given system performance measure for different system options using two perturbed attributes as the co ordinates to illustrate the changes in system performance with the perturbations in these attributes design thresholds for the two systems can also be overlaid onto the space so that the location of each threshold with respect to the climate attributes can be seen as well as enabling a direct comparison of any shifts in system performance due to differences in the underlying system e g fig 10 control over the visualisations to specify the plot axes and subset attributes select colour scales and overlay downscaled climate projections is provided via the function arguments 4 example application an example climate impact assessment is presented to demonstrate the five step framework and functionality of foresight this example uses a stylised and hypothetical domestic rainwater tank system outlined in section 4 1 the rainwater tank is used as it represents a simple climate sensitive system yet with enough complexity to illustrate key concepts most foresight applications would be of much more complex systems following an investigation of tank system behaviour under historical climate section 4 3 foresight is used to implement the five step scenario neutral climate impact assessment framework to assess the performance of two competing tank system designs and determine which design is preferable which is defined here as the design that performs satisfactorily under the greatest number of hydroclimate scenarios section 4 4 4 1 example system model the example system model simulates a domestic rainwater tank system which can be used for both garden irrigation and indoor grey water applications rain falling on the roof of the house is captured and directed towards the rainwater tank before the rainwater is able to enter the tank the first flush is removed from the start of each storm this is required for water quality reasons and equates to the first one to 2 mm of water falling on the roof the water remaining after the first flush flows into the rainwater tank water demand is assumed to be constant throughout the year for indoor use and to vary seasonally for outdoor use the amount of water supplied by the tank depends on the current water level in the tank calculated at a daily time step the system s outdoor water use pattern is based on arbon et al 2014 for adelaide south australia the outdoor seasonal demand pattern responds to the daily temperature i e on hot days above 28 c the garden is watered more than the seasonal average but on cool days below 12 c the garden is watered less than the seasonal average the tank model simulates each stage of the rainwater capture and use process based on the supplied daily rainfall and temperature time series performance of the tank can be measured according to five metrics reliability average daily deficit volumetric reliability system efficiency and storage efficiency the size of the tank roof and first flush diverter can be varied in the model this example model provides sufficient scope for an illustration of foresight s functionality this is because the tank responds to multiple climate drivers i e rainfall and temperature and the removal of the first flush volume at the start of the storm means that the wet dry pattern of the rainfall and the seasonality of the demand pattern may become important 4 2 the tank system design problem the framework is demonstrated in the context of a hypothetical domestic tank system design problem in this example problem a household in adelaide south australia has been provided with two competing quotes for a domestic tank system the two tank systems will be modelled using the example system model each tank system comprises a tank pump first flush diverter and pipe for connection from roof to tank specifications for the two systems differ however the cost of both is the same the details of each system are provided in table 4 the supplier of tank system a recommended adding additional roof area by also connecting the garage roof to the tank but specifies a smaller tank volume of 2400 l whereas system b is fed only by the house roof area but has larger tank volume of 2750 l to store runoff due to the difference between the two systems the householder suspects they may have different behaviours under changing climate conditions the householder is concerned about performance from two perspectives how many days can the tank supply the full demand and on average how much water needs to be purchased to be able to meet the required demand therefore the climate impact assessment will examine both average daily deficit and reliability as performance metrics the householder has set two minimum performance thresholds a minimum reliability threshold of 81 and for budget reasons a maximum average daily deficit of 28 l day 10220 l year 4 3 initial assessment of tank system performance under historical conditions prior to undertaking a scenario neutral assessment it is typical to review what is currently known about the performance of the system via an initial review of system understanding this preparatory step is emulated as part of this example problem implementation although not sitting within the five step framework and described in this section for this example the initial review of system entails an investigation of system performance under historical conditions and is conducted using the example system model and observed rainfall and temperature time series the system model is run with an observed time series of 10 years 2007 2016 of daily rainfall mm and daily mean temperature oc so that the operating behaviour of each system can be assessed the historical climate is mediterranean with an average annual total rainfall of 445 mm and an average annual temperature of 17 5 c the majority of rainfall occurs in the winter and autumn months march august with the hottest days generally occurring in january fig 6 shows the operation of the two tank systems and demand pattern for an illustrative one year period the red jagged spikes and dips in the demand pattern indicate days where the demand pattern is influenced by temperature extremes although tank system b has a greater tank volume than tank system a both struggle to deliver demand and run dry in similar periods see fig 6 days 1 80 over the ten year period system a exhibited an average reliability of 81 and a 27 l day average deficit and system b exhibited an average reliability of 82 and a 26 l day average deficit so that both systems meet the minimum performance threshold set by the householder the performance of both systems in terms of average daily deficit and reliability varies substantially year to year i e reliability ranging from 73 to 93 and average daily deficits from 11 to 37 l day therefore to investigate the longer term average system performance the system models were run using 300 years of rainfall and temperature simulated using weather generators calibrated to the historical data when using this stochastically generated data the longer term average performance of the systems is an average daily deficit of 24 l day and 83 reliability for system a and an average daily deficit of 23 l day and 84 reliability for system b as done in broderick et al 2019 these initial simulations demonstrate that both tank systems satisfy the threshold design objectives maximum deficit 28 l day and minimum reliability 81 and perform similarly to the historical climate conditions 4 4 application of the scenario neutral climate impact assessment framework via the foresight r package the five step scenario neutral climate impact assessment framework is applied to evaluate the two competing tank system designs supported by the foresight r package functionality outlined in section 3 the r code to perform each step of the framework is available as supplementary material see supplementary material e 4 4 1 step a identify attributes for perturbation and create and exposure space the first framework step focuses on the identification of hydroclimate attributes for perturbation the procedure begins with an initial assessment of what climate attributes should be selected for perturbation in the system stress test there is necessary upfront investment in exploring the system s behaviour for a subset of scenarios so that attributes to which the system is more likely to be sensitive are included in the full system stress test in contrast attributes to which the system is deemed insensitive are held at historical conditions i e not perturbed to limit the computational overheads involved in generation of additional hydroclimate time series and system performance this investigation essentially comprises a narrow implementation of the steps b to d of the framework i e in terms of the range of hydroclimate scenarios investigated and results in the specification of the exposure space for the later framework steps see culley et al 2021 for this demonstration an informal variable selection approach was implemented that assessed the sensitivity of systems a and b in two parts an initial sensitivity test to simply scaled hydroclimate time series followed by a more detailed investigation using stochastically generated hydroclimate scenarios both of these sensitivity tests are performed using the createexpspace and generatescenarios functions with createexpspace specifying that attributes are perturbed on a one at a time basis i e only one climate attribute was perturbed while the remaining were held as close as possible to historical levels first the sensitivity of systems a and b to the simple scaling of the observed annual total rainfall by 20 in 10 increments and annual average temperature by 2 c in 1 c increments was assessed the simply scaled rainfall and temperature time series were produced by setting the controlfile argument to scaling within generatescenarios the generated hydroclimate time series were used to drive the relevant system models to determine the system s response to the perturbed climate conditions for the two tank systems the simple scaled time series investigation demonstrated that system performance changes with both average total annual rainfall and to a significantly lesser extent average annual temperature for both the average annual deficit and reliability performance metrics this initial test suggests that further explorations of changes in rainfall attributes are required and should be prioritised over temperature as a result of this initial investigation the stochastically generated hydroclimate time series investigation focuses on exploring the systems response to changes in rainfall attributes specifically those related to the wet dry pattern and seasonality these time series consider the perturbation of annual total rainfall by 80 110 the ratio of total summer to winter rainfall by 80 130 the annual number of wet days by 85 105 and the annual number of days with rainfall greater than 10 mm by 90 125 with four uniformly spaced samples of each attribute while holding average annual temperature at historical levels these tested ranges were informed by the climate projections from the sa climate ready dataset goyder institute for water research 2015b charles and fu 2015 goyder institute for water research 2015a used later in step d section 4 4 4 the default temperature and precipitation weather generators in foresight are used together to generate these time series modeltype wgen modelparametervariation harmonic for both stochastic models the parameters throughout the year are specified via a harmonic function to represent seasonality as the timing of seasons is not being tested the phase angle parameters that control translation of the seasonal cycle of the stochastic rainfall model are fixed at historical levels likewise as temperature is not being perturbed the parameters of the temperature model are fixed at historical levels this reduction in the number of parameters reduces the search space for the optimisation algorithm used in the inverse approach to find suitable parameter sets and helps to create consistency between scenarios twenty replicates of a three hundred year period were simulated for each target location here seven instances with the smallest fitness scores i e closest to the target at each target location are retained and used in system stress testing the remaining replicates are discarded to avoid the inclusion of poorer performing replicates in the stress test i e those with simulated targets that substantially differ from those requested fig 7 shows tank system s response to the one at a time perturbation of four climate attributes annual total rainfall seasonal rainfall ratio the ratio of total wet season rainfall to total dry season rainfall annual number of wet days and annual number of days with rainfall greater than 10 mm for average annual deficit fig 7a d fig 7i l and average annual reliability fig 7e h fig 7m p the figure is created using the plotperformanceoat function in foresight for both average annual deficit and reliability the tank systems were most sensitive to changes in the annual total rainfall and the seasonal rainfall ratio the systems were less sensitive to changes in the number of wet days and the number of days with rainfall greater than 10 mm the same relative sensitivities were seen for both systems a and b hence the climate attributes annual total rainfall and seasonal rainfall ratio were identified for perturbation in step b for this demonstration the output of step a is an exposure space corresponding to perturbations in the annual total rainfall 80 110 and seasonal rainfall ratio by 80 130 in 3 increments in combination yielding a two dimensional attribute space with 160 targets where these exposure space characteristics are specified via createexpspace 4 4 2 step b generate hydroclimate scenarios having specified the exposure space step b of the framework focuses on the generation of hydroclimate scenarios due to the nature of the identified attributes i e annual total rainfall and seasonal rainfall ratio hydroclimate scenarios i e time series stochastic simulation is used for scenario generation the output of step b is the set of stochastically generated hydroclimate time series corresponding to the exposure space specified in step a here generatescenarios was used to stochastically generate hydroclimate time series corresponding to the exposure space specified in step a i e perturbations in the annual total rainfall 80 110 and seasonal rainfall ratio by 80 130 in 3 increments in combination yielding a two dimensional attribute space with 160 targets for this demonstration 20 replicates of a three hundred year time period were simulated the weather generator and optimisation parameters i e controlfile optimisationarguments are the same as those used for the one at a time stochastic scenario generation at each target location the seven instances with the smallest fitness scores i e closest to the target are retained and used in system stress testing 4 4 3 step c simulate system performance step c proceeds by determining the system performance in response to each of the hydroclimate time series i e scenarios generated in step b and is supported within the foresight package via the runsystemmodel function in this demonstration step c is undertaken twice i e for both system options a and b each of the generated hydroclimate time series are used as inputs to the system model to stress test each system and determine the resulting system performance i e average daily deficit and reliability using the runsystemmodel function the resulting performance metrics for each evaluated system are used as inputs to subsequent steps d and e 4 4 4 step d analyse system performance step d of the framework visualises the resulting system performance values with reference to the hydroclimate scenarios attributes the system s performance can then be interpreted and evaluated with reference to the purpose of the system the performance of tank system simulated in step c was visualised using the plotperformancespace function fig 8 shows the performance of the two tank system configurations in response to changes in annual seasonal rainfall ratio and annual total rainfall as quantified by the performance metrics average daily deficit left panels and reliability right panels system performance is displayed using the plotperformancespace function for tank system option a fig 8a b and b fig 8c d for both tank system options the performance in terms of both deficit and reliability changes with both total annual rainfall and rainfall seasonality both systems and performance measures are slightly more sensitive to changes in the rainfall seasonality than changes in the annual total rainfall additional climate information in the form of downscaled climate projections were overlaid according to their time series attributes calculated via calculateattributes to put the system stress test in the context of projected climate trends and provide an indication of what hydroclimate scenarios are more plausible the climate projections shown are from the sa climate ready dataset for a time period centred on year 2050 under rcp 8 5 goyder institute for water research 2015b charles and fu 2015 goyder institute for water research 2015a a sample of 600 projections from the dataset are shown 100 statistically downscaled replicates from six global climate models that were indicated as better performing by the dataset authors see charles and fu 2015 the majority of the downscaled climate model projections show a decrease in both mean annual total rainfall and seasonal rainfall ratio with only 23 of the projections showing an increase in the mean seasonal rainfall ratio and 5 showing a small increase in mean annual total rainfall the performance spaces are then assessed with respect to the threshold design objectives to enable a comparison between the two competing tank system options here plotperformancespacemulti is used to visualize which scenarios satisfy or violate the imposed design thresholds and to again overlay downscaled climate projections to provide context the differences between the two systems in terms of their operation and hence performance becomes more apparent when the two systems are evaluated against the system design thresholds fig 9 shows the systems performance with reference to the two threshold design objectives with yellow indicating that no thresholds are exceeded i e no action required for this climate scenario orange where one threshold is exceeded and dark pink indicating that two thresholds are exceeded for visual clarity climate projections for 2050 under rcp 8 5 see section 4 4 4 are overlaid on these panels so that the user can determine where the projections lie in terms of the design thresholds for tank system a see fig 9a 50 of the stress tested scenarios produce system performance values that are in violation of one or more of the design thresholds i e percentage area of the performance space shaded orange or dark pink in contrast for tank system b see fig 9b 30 of the stress tested scenarios are in violation of one or more of the design thresholds hence tank system b demonstrates tolerance to a wider range or climate scenarios than tank system a performance of the two tank systems can then be interpreted in light of the overlaid climate projections for tank system a fig 9a the majority of the projections fall in the yellow area i e no thresholds violated and approximately 6 the downscaled projections fall inside the shaded orange area indicating that the system s operation violates one of the design thresholds in this case the reliability threshold of 81 approximately 11 of climate projections violate both thresholds for tank system b the majority of the projections again fall in the yellow shaded area the area where none of the thresholds are violated and the system is operating as required fig 9b in this case only 1 of the climate projections are in violation of both design thresholds and less than 4 are in violation of one design threshold reliability 4 4 5 step e evaluate system options step e of the framework proceeds by directly comparing the stress tested performance of multiple system options with respect to threshold design objectives this step is facilitated in the foresight package via the use of plotoptions which visualises the difference between two performance spaces as a heatmap as well as overlaying design thresholds for the two systems to enable a direct comparison of differences in the evaluated systems fig 10 shows the difference in performance of the two systems system b system a with respect to threshold design objectives for the two performance measures average daily deficit fig 10a and reliability fig 10b again climate projections for 2050 under rcp 8 5 see section 4 4 4 are overlaid on these panels so that the user can determine where the projections lie in terms of the design thresholds for both system performance measures the threshold for system b is to the right of the threshold for system a indicating that tank system b operates satisfactorily for a wider range of hydroclimate conditions again fewer climate projections lie outside of system b satisfactory operating range for both system performance measures see fig 9 and discussion in section 4 4 4 following the application of the framework to the example system supported by the foresight r package the results indicate that tank system b is preferable as it should operate satisfactorily across a wider range of conditions including the drier climate projected by the climate models for the 2050 time period 5 discussion the presented scenario neutral framework and software have arisen from a concerted effort to formalise the modelling and analysis components of scenario neutral assessments and to reduce barriers to entry for the wider community the framework and r package are designed to enable the execution of many standard approaches such as simple scaling prudhomme et al 2010 culley et al 2016 as well as newer approaches to scenario generation i e the inverse approach to stochastic generation of time series guo et al 2018b culley et al 2019 nevertheless unlike scenario led approaches research into scenario neutral approaches is still emerging with frameworks and examples only published in the last decade prudhomme et al 2010 wilby and dessai 2010 brown and wilby 2012 brown et al 2012 poff et al 2015 and to the authors knowledge this being the first openly available enabling software tool a range of limitations associated with the methodology together with potential future directions for software improvement are now described 5 1 current limitations of scenario neutral approaches current limitations of the foresight r package stem primarily from the current limitations of scenario neutral approaches for example caution should be exercised when using system models under different conditions from which they have been calibrated this caution has been raised by many authors guo et al 2018a fowler et al 2016 coron et al 2012 in relation to assumptions surrounding the performance of environmental models that have been calibrated to historical climate time series and that therefore may not perform adequately under changed climate conditions expert judgement is required to assess whether the system model sufficiently represents the system s processes to be capable of satisfactorily simulating system performance under a change in climate regime or whether alternative model structural representations are required gibbs et al 2018 guo et al 2020 analytical methods for dealing with this challenge include differential split sampling westra et al 2014 fowler et al 2016 and approaches based on pareto optimality fowler et al 2016 similarly different stochastic weather generators have been shown to perform differently in different climate regimes vu et al 2017 this poses a challenge for the stochastic generation of hydroclimate time series since not all driving stochastic weather generators are equally suited to the generation of the hydroclimate scenarios required to effectively stress test a system in addition to this challenge as more flexible weather generator options are used which typically have a larger number of parameters the search space for the inverse approach increases making it more difficult and computationally expensive for the optimisation algorithm to find suitable parameter sets for time series generation guo et al 2017 further challenges exist in specifying constraints on the hydroclimate scenarios such that the time series generated represent plausible hydroclimate conditions that the system may experience see guo et al 2017 guo et al 2018b 5 2 potential future advances there are a number of opportunities for the extension of the framework and r package including incorporation of tools for formally identifying critical attributes for stress tests culley et al 2021 and guo et al 2017 have both explored methods for the identification of relevant climate attributes for stress testing although difficulties remain in the identification of all attributes that are influential on system performance culley et al 2021 guo et al 2017 gains have been demonstrated in the amount of system sensitivity able to be captured in system stress tests by using a systematic approach for attribute selection development of methods for generating a broader range of perturbed hydroclimate scenarios including shifts in spatial subdaily and inter annual attributes the spatial subdaily and or inter annual variability of supplied hydroclimate time series have the potential to have an important influence on the performance of certain systems but the capacity to vary time series at these scales is not yet supported within foresight for example spatial rainfall variability is known to influence catchment dynamics segond et al 2007 obled et al 1994 singh 1997 smith et al 2004 options for producing spatial hydroclimate timeseries include the implementation of a gridded multivariate multisite weather generator steinschneider and brown 2013 or extension of the inverse approach to space guo et al 2018b using a continuous spatial rainfall model e g bennett et al 2018 leonard et al 2008 augmentation of step e evaluate system options to complement the analysis of multiple system designs or management options additional tools may be adopted for conducting decision analysis for example robustness type approaches mcphail et al 2018 mcphail et al 2020 could be applied to assist in the evaluation of multiple system design options likewise this analysis step could be used to develop trigger points for use as part of adaptive pathway approaches haasnoot et al 2013 6 conclusions this paper presents a framework for effective scenario neutral climate impact assessments and new r package foresight to support each framework step the five framework steps and corresponding package functions enable the strategic evaluation of system performance across a wide range of hydroclimate conditions without significant efforts in the development of models for generating suitable hydroclimate time series simulating system performance or visualizing and analysing resultant system performance against design objectives likewise the r package also enables easy repetition of investigations to evaluate and compare different system management or design options this should enable the application of scenario neutral assessment principles to new problem domains that manage complex interfaces between systems and the hydroclimate the new framework and r package functionality were demonstrated for a simple domestic water supply system design in which two competing system options were assessed to determine which system represented the best investment in terms of the system that performed satisfactorily across the greatest range of plausible hydroclimate conditions computational details the case study results in this paper were obtained using r 4 0 3 using the presented package foresight the package relies on the following key dependencies ga v3 0 2 scrucca 2013 and ggplot2 v3 3 0 wickham 2009 which are both available from the comprehensive r archive network https cran r project org software availability description package foresight version 1 0 0 developers bree bennett sam culley anjana devanand seth westra danlu guo and holger r maier year first available 2018 e mail bree bennett adelaide edu au website https cran r project org package foresight hardware requirement general purpose computer software requirement r version 3 6 1 or later programming language r declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the goyder institute for water research dr sam culley and dr danlu guo were supported by research training program scholarships the authors gratefully thank the testers of the foresight r package dr michael leonard dr cameron mcphail and participants of the goyder institute for water research craft workshop appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 104999 
