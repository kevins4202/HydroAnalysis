index,text
25575,in this paper a space time model is developed to support spatial planning the temporal nature of the model allows the variables that condition land use to evolve periodically in each time step land use allocation is optimized considering sustainability criteria the model combines system dynamics techniques to simulate the evolution and interactions of economic and biophysical drivers using a multi objective algorithm for spatial optimization that introduces bidirectional spatio temporal feedback it provides sets of maps for decision makers in which income and food self sufficiency are maximized and the environmental impact of different what if scenarios is minimized the model has been applied in los llanos de san juan mexico for three scenarios baseline high and sustainable growth the maps obtained by 2050 reduced water demand pressure by 1 4 and 4 7 while crop production in calories was balanced with population food needs in 2035 and 2045 in the high and sustainable growth scenarios respectively keywords time space model land response genetic algorithm spatial explicitness economic and biophysics interactions 1 introduction sustainable land management is essential due to its potential contribution to terrestrial equilibrium in the medium and long term where land is considered as the terrestrial portion of the biosphere that comprises the natural resources the ecological processes topography and human settlements and infrastructure ipcc 2019 land use planning is not only a spatial allocation problem but also an issue of land utilization where over exploitation understood as resource depletion and or degradation has long been evident throughout the world for this reason sustainable land management criteria must comprehensively address optimal land use allocation generally according to land aptitude and options for land use change as well as foresee the potential impacts of such changes over time to become a useful land use planning tool that promotes territorial equilibrium various computational methods of optimization have been developed for land use allocation including linear programming lp for a single objective arthur and nalle 1997 chuvieco 1993 schlager 1965 or heuristic multiple objective evolutionary models that compensate several opposing objectives such as competing land uses cao et al 2012 feng and lin 1999 x liu et al 2012 ma and zhao 2015 stewart et al 2004 strauch et al 2019 whose adaptive capacity frequently bioinspired makes them powerful search engines for optimal landscape solutions the non sorting genetic algorithm ii nsga ii developed by deb et al 2002 provides feasible solutions based on the pareto front and has been utilized in land use allocation models azuara garcía et al 2018 cao et al 2011 deb et al 2007 karakostas 2015 the nsga ii has proven to perform effectively in spatial optimization models due to its elitist selection mechanism that preserves good and diverse solutions as well as its efficiency in computing data land use change has been evaluated from different approaches several studies have employed bottom up models to identify and analyze historical patterns of land use expansion e g agricultural frontiers urban industrial sprawl etc and to foresee and even improve future land use change with the aid of mathematical models such as markov chains cellular automata and logistic regressions among others which perform as stand alone models or in combination with other models including optimization procedures jenerette and wu 2001 li et al 2011 mansour et al 2020 rienow and goetzke 2015 roodposhti et al 2019 ruben et al 2020 the deforestation rate is one of the most important environmental impacts addressed by these approaches verburg et al 2002 another approach to temporal analysis is system dynamics sd developed by forrester forrester 1961 1969 sd is applied in top down models and has been employed in most scientific fields due to its efficiency in handling dynamic problems since its application in the club of rome report entitled the limits to growth meadows et al 1972 the interrelations among demographic biophysical and economic factors of the planetary system have become evident accordingly environmental assessments are frequently based on sd procedures for different system sizes bendor et al 2011 graeme and davies 2008 xi and poh 2013 even though most of them are not spatially explicit sd techniques have also been used to identify land use change and envisage future land needs for certain uses sanders and sanders 2012 zhan et al 2012 nonetheless only a few temporal models have evolved from static into spatially explicit dynamics models when coupled with land use allocation optimization procedures d liu et al 2017 ruben et al 2020 verburg et al 2002 yang et al 2019 most of these models seek to improve the accuracy of land use change predictions xu et al 2016 while others identify periodic land area requirements according to diverse scenarios of land demands han et al 2009 however the main goal of dynamic spatial planning models is not to reduce the environmental impacts of land use changes the lack of feedback between spatial and non spatial components is common in models in which sd is coupled with land use allocation algorithms the relationship between the two has been defined in a unidirectional way where the temporal component provides the surface changes by type of land use and the spatial component places them in the best allocations that is the spatial component does not constrain the temporal drivers of the complex system to overcome this issue the coupling mechanisms of dynamic land use models must be improved by integrating bidirectional flow procedures between the spatial and non spatial components e g biophysical and socioeconomic factors and introducing the spatial response of land use over time a few models have solved this lack of bidirectionality such as the structural change model proposed by neuwirth et al 2015 based on an originally non spatial fictitious environment known as daisyworld watson and lovelock 1983 in both its spatial and non spatial versions the model runs simulations for several scenarios to verify the impact of spatial feedback on the system behavior where time is assumed to be a process and space to be a structure the daisyworld model allocates three land cover types according to biophysical conditions planetary albedo global and local temperature growth rate soil fertility etc and performs by using a synchronization loop to tightly couple a sd application with a geographic information system gis at each time step of the simulation despite this successful fictitious implementation of spatial and non spatial bidirectional flow until now it has not been implemented in land use models since the interaction of environmental and socioeconomic factors within a spatial rationality increases the complexity of bidirectionality in land use planning models if land response were considered temporal models could inform stakeholders of the need to implement specific land exploitation constraints or redirect current land policy by taking advantage of the feedback loops since they can perform as self regulatory mechanisms of sd models and indicate the limits of growth as goals of a sustainable land system according to regional needs the objective of this research is to develop a comprehensive spatio temporal model of land use planning to generate optimal and sustainable patterns of land use change for several exploitation scenarios considering biophysical factors and economic and demographic growth rates this model can have a positive impact on water resources and food production can be managed through the implementation of appropriate measures sustainable water use or policies to improve agricultural production to preserve the territorial balance the model uses a multi objective genetic algorithm to determine the spatial distribution of land uses embedded in a sd framework and analyze and limit the impact of land use change the model developed here has only been applied theoretically to the sub region of los llanos de san juan méxico to identify the optimal pathways of land use change by 2050 2 methods this comprehensive spatio temporal model for land use planning has been developed to solve a the optimization of land use allocation with periodic spatial explicitness and b the bidirectional feedback between spatial and non spatial components of the territorial system to achieve sustainability goals such requirements are satisfied in this proposal by embedding a genetic algorithm ga spatial module within a sd time module as described below 2 1 spatial module the sustainable land use allocation model mauss acronym in spanish azuara garcía et al 2017 has been selected to provide the optimal allocation of land uses it is a multiple objective evolutionary model whose optimization core is based on the nsga ii deb et al 2002 mauss generates feasible and diverse map solutions due to its spatial constraints strauch et al 2019 which frame the intrinsic randomness of nature based algorithms thus reducing the optimization iterations needed to achieve pareto front solutions a further advantage of mauss is the consistency of its geometric operators for fulfilling the compactness and contiguity of land use patches an overview of mauss is provided in fig 1 the mauss model performs the land use allocation for a discretized territory into homogeneous surface units cells of the grid every cell is a decision variable gen that assumes one randomly allocated land use type provided it complies with the following spatial constraints i the cell belongs to its feasible subset built according to the aptitude of the land see appendix a of the supplementary material ii it has not been occupied by another previously allocated land use and iii it does not exceed the required maximum area each cell is then evaluated according to the planning targets stated in the objective functions the mauss objective functions are based on sustainability criteria o1 maximize income o2 minimize negative impacts on the environment quality and quantity of water and co2 equivalent emissions and o3 minimize the regional food deficit based on local agricultural production a brief description of the formulae is given below 1 o 1 i 1 r j 1 c u 1 u x i j u gd p i j u α i j u i 1 r j 1 c max gdpiax al of use e tributio n i j where x iju is a binary decision variable that allocates only one land use type u to the cell located in row i column j u is a land use type from 1 to u r denotes the rows of the grid from 1 to r c is the columns of the grid from 1 to c gdp iju is the annual average income millions of currency units of cell ij with land use u max gdp ij is the maximum possible income for cell ij and a iju is the territorial aptitude factor for land use u allocated in cell ij that only affects land uses directly linked to food production 2 o 2 i 1 r j 1 c u 1 u x i j u p w i j u p a i j u where pw iju is the dimensionless negative pressure on water and pa iju is the negative pressure on air the first summand evaluates water availability water footprint and the pollution risk of the resource surface or ground water pa iju quantifies greenhouse gases ghg in terms of co2 equivalent hence the negative pressure that every land use would produce in its location is estimated within this function the calculation method for pw and pa can also be found in azuara garcía et al 2017 to minimize the food deficit once the population and the food needs for their subsistence have been established the algorithm searches for the best locations for crop production based on the suitability of the land food requirements are estimated from the energy provided by food and beverages measured in calories roehrig et al 2013 there are different ways to calculate the land needed for food production based on the energy contribution of each crop measured in calories shepon et al 2016 for this reason the calorie has been considered as the operative unit of the algorithm in equation 3 without intending to value crops from a nutritional perspective an aspect that is beyond the scope of this work 3 o 3 f u p pop i 1 r j 1 c u 1 u h l u y u α i j u δ u i 1 r j 1 c maxa u i j where a u p is the dietary requirements per person expressed in food units fu pop is the total population of the study area u is the index of land uses linked to food production hl is the harvested land of use u y u is the average yield of all crops from use u tons α uij is the agricultural potential index territorial aptitude factor of cell ij with use u δ u is the conversion factor into fu cal of food produced in use u which depends on the crop and crop yield t according to its occupancy percentage in cell ij and max fuij is the maximum possible calories produced in cell ij equation 3 only takes into account the absolute value of the differences between the food units obtained from each land use allocation and the food needs of the population because the lack or the surplus of food should ideally be minimal to encourage optimal land use allocation and hence territorial sustainability the improvement of solution maps is based on the evolutionary process of recombination of genetic information cells among maps via cross over and or mutation for n generations to reach the non dominated solution map set 2 2 time module this module contains the land use change criteria for a time horizon according to an sd approach using constants the variables that explain the system stocks levels of the explanatory variables flows the interrelation of variables and its variation rates and positive or negative feedback loops gonzález busto 1999 to achieve the goals of the system in each time step the flows modify the stocks when stocks surpass or do not reach the predefined levels goals negative or positive feedbacks modify the flows to stabilize and correct the system the different nature of the driving forces is considered here as a precondition for enabling the relationship between the spatial and non spatial components of the model within the complexity of their interrelations in a given territory the drivers must share variables of interest with the objective functions of the optimization module in order to reflect the impacts of the spatial drivers in the system over time the driving forces considered in this research are detailed in table 1 whose variables of interest may assume different values for several what if scenarios the flows among biophysical socio demographic and economic drivers require the construction of a second order system with three interconnected loops a main loop that calculates the surface variation per land use type in each time step and in which the flows from the spatial module are included and two feedback loops that operate as self regulatory mechanisms to achieve the sustainability goals of the system fig 2 2 3 the second order territorial system mauss sd the second order territorial system that brings together the optimal land use allocation module mauss and the time module with land use change factors within the sd methodological framework is called mauss sd it is encoded in matlab language through the interrelations of the drivers and the rates of change of their variables the time module states the area requirements per land use for each time step and controls the rates of negative and positive feedbacks of the system the mauss spatial module functions as an optimization loop nested in the main loop and provides maps of optimal land use allocation according to the objective functions which are themselves sd flows within a period that yields annual stocks of gdp negative pressure on the environment and food deficit a feature of bidirectionality is that such stocks are not linear over time since they are annually conditioned by the land aptitude when the stocks do not accomplish the stated goals levels discrepancies are identified and the time module induces negative or positive feedbacks in the main loop flow to keep the system balanced therefore bidirectional feedbacks occur in each time step the area per land use is the stock where all loops are interconnected in a simplified way the optimization of the set of maps performed in the spatial module is periodic so that in each period a pareto front is obtained the maps of the pareto front of each period are the base population of the next period the changes caused by the drivers and the planning goals are introduced into them and then optimized thus the optimal solutions of the last simulation period are a consequence of the evolution of the optimal solutions from the previous periods 2 3 1 territorial goals a ensure sufficient crop production of agricultural land uses to satisfy local food requirements this goal is to achieve the food self sufficiency of regions and is completely coherent with objective o3 of the optimization module therefore in this model the annual food requirements of the region must be at least equal to its annual crop production cp 4 a u p p o p c p where 4a c p i 1 r j 1 c u 1 u x iju h l u y u α iju whose components have been described in eq 3 b avoid land uses that exceed the regional water availability this goal focuses on preventing water overexploitation thus the total usage of freshwater resources water footprint should be lower or equal to the regional water availability as expressed in the following equation 5 i 1 r j 1 c u 1 u w f iju α iju i 1 r j 1 c w a ij 1 where w f iju is the water footprint m3 associated with land use u located in cell ij and w a ij is the water availability of cell ij and constitutes its annual water recharge this equation corresponds to the water assessment of mauss in objective o2 nonetheless the spatial module seeks to allocate water demanding land uses to locations with high water availability whereas the sd module limits the exploitation of water resources if the planning goals are not met in a period the self regulation mechanisms are activated as described in section 2 3 3 2 3 2 main loop changes in land use sizes the land use types must correspond to those stated in the spatial module generally the change in size depends on the investment applied to the activity land use as a share of gdp and on the population requirements in this study gdp is considered according to land use instead of per capita for industrial land use increases in gdp foster investments in the industrial sector and hence an increase in industrial land industrial area changes e ind in number of cells are obtained in each time step t by the following equation 6 e ind t 1 g d p g d p r k i g d p ind where 6a g d p i 1 r j 1 c s 1 s u 1 u g d p ijsu s gdp is the average value of income recorded by the set of maps s of the simulation considering the income generated by all land uses u in all cells ij as described in table 1 gdp r is the growth rate of the gross domestic product k is the capital investment rate and i is the industrial investment share gdp ind is the reference income for a cell with industrial land use ind in currency units the socio demographic driver is expressed through the population growth rate p in this case it is based on the geometric mean increase to simplify the population dynamics migration processes birth and death rates with fixed variation rates per time step annual p produces changes in the urban extent taking into consideration the land needed per person ln p which indicates a specific urban density the surface variability of urban land use is obtained according to equation 7 e urb t 1 p o p t p l n p a c e l l 7 where e urb t 1 is the urban area for period t 1 given in number of cells pop is the total population of period t acell is the area per surface unit m2 and depends on the planning scale the available detail of cartography and the specific planning targets ln p is an indicator that comprises the floor area per person un 1998 and the green area per person sorensen et al 1998 for urban land use this work considers residential and other uses linked to human activities housing units recreation commerce and services depending on the specific conditions of each study area agricultural uses irrigated land rainfed land grassland or forestland may diminish in size due to industrial and or urban sprawl equation 8 or increase their area due to a greater demand for food units fu equation 9 this latter equation is completely in accordance with objective o3 of the spatial module 8 e u t 1 e u t e ind t 1 e urb t 1 τ u t e i nd t 1 0 a n d e u t 1 0 9 e u t 1 e u t a u p p o p c p m a x a u iju in this case e u t 1 is the area number of cells of agricultural use u in period t 1 and τ is the actual proportion of area with use u 2 3 3 secondary loops self regulatory mechanisms of the territorial system two self regulatory feedback loops that guarantee the accomplishment of the planning objectives goals and the equilibrium of the system in each time step are described next a self regulation of food deficit surplus positive feedback to meet the food goal eq 4 the regulatory mechanism of the mauss sd model presents two ways of promoting regional food self sufficiency depending on the regional needs and characteristics an increase in yields and or the expansion of agricultural land 2 3 3 1 increases in agricultural yield this paper argues that the food balance of a region with a surplus in crop production cannot be achieved through the suppression of agricultural activities since this would affect the income and ways of life of local peasants and hence the balance of the system therefore even for food surplus regions one way to self regulate yields in the model is to increase crop yields proportional to the loss of agricultural area caused by the expansion of other uses a second path is activated when these increases are no longer able to meet the needs of a growing population for which the proportionality of the increases in yields no longer corresponds to the loss of agricultural land but to increases in population growth according to equations 10 13 10 y u t 1 y u t i 1 r i 1 c u 1 u s 1 s g d p ij s u t s t a p iju y u t y u m a x 11 y u t 1 y u t s 1 s c p ijs t s t a p iju y u t y u m a x 12 g d p iju t 1 e u t g d p iju t e u t 1 13 a p iju g d p iju t y u t where ap is the average constant price currency units of crops from use u at cell ij and y u m a x is the average of the maximum possible values of crop yields of agricultural use u which should be obtained from the specialized literature for the crops in the application region the hypothesis considers that to adapt food production in the territory without increasing the availability of resources the variable is crop yield to achieve yield increases it is necessary to develop specific strategies for plant improvement including water use efficiency which are therefore outside the scope of this work increases in agricultural land use size if the maximum yield increases are insufficient to satisfy the population s food demands the positive feedback enables enlarging the size of the established agricultural land uses current policies and regulations may impose the expansion of agricultural land uses in each region a specific formulation for increases in agricultural land use corresponds with equation 9 through these mechanisms the model minimizes food deficit surplus for regions with high food dependency or areas with agricultural overproduction the model can also regulate cropland areas for regions which fluctuate from one dietary condition to another b self regulation of negative pressures on the environment negative feedback this self regulatory mechanism diminishes negative pressures on water resources it ensures that any land use configuration does not exceed the regional water availability by means of reduction of the water footprint the water footprint wf is calculated for each land use as the relation between water usage per production or service unit hoekstra et al 2011 by dividing the wf of use u by the total area that it occupies wf is obtained per unit area ij considering suitability criteria αiju eq 14 14 w f iju t 1 w f iju t i 1 r j 1 c w a ij i 1 r j 1 c u 1 u w f iju α iju w f uij t 1 w f u m i n for agricultural land uses wf is the parameter that relates the amount of water used m3 in obtaining each harvest unit tn it is the result of dividing the use of water per unit area m3 ha by the yield tn ha the wf of a crop is an index of water efficiency of the crop under the conditions in which it has been produced variety location agroclimatic conditions cultivation techniques period of time and in the case of irrigated cultivation the irrigation method and irrigation management therefore for agricultural land uses wf u min average crop requirement m3 ha y u max for required increases of non agricultural land uses wf was proportionally reduced to a preset lower limit wf u min which represents a proportion of the current wf u this limit can be established by the user it is important to highlight that the specific procedures for achieving the proportional increase in yields and the proportional reduction of wf water efficiency technologies fall outside the scope of this study avoiding an increase in land uses with the most demanding water usage without affecting the satisfaction of regional food needs according to equation 15 15 e u t 1 e u t the secondary loops are interconnected with the main loop as equilibrium can also be reached through land use size variations as illustrated in fig 2 the feedback loops maintain the sustainability criteria in each time step of the simulation despite the area changes of each land use type which is also limited by the availability of regional resources under a sustainable exploitation hence these loops are the self regulatory instruments of the system the feedback code is always present in the model although the user can disable it to simulate scenarios without self regulation 2 3 4 economic constraints a the revenue of agricultural land uses u for period t 1 should at least be equal to their initial income in t 16 i 1 r j 1 u u 1 c g d p iju t 1 i 1 r j 1 u u 1 c g d p iju t b with negative or null population growth rates the model assumes that crop yields will not vary to maintain primary sector income 17 y u t 1 y u the values of gdp and y cannot decrease they can only increase as they are subject to positive feedback 2 3 5 parameters of the embedded model to start the time space process the model requires the number of periods t to simulate an initial set of maps s and the number of generations n to evolve the maps per period the initial set of maps can be the set of solutions obtained through a previous optimization procedure or the current map of any study area if the current map is used the algorithm is encoded to replicate the map at least 10 times to complete a minimum size map set in this case the number of parent maps for recombination equals s only in the first generation of each period to broaden offspring diversity otherwise this value is as usual half of s the number of decision variables x is defined within the spatial module and must match the decision variables of the initial maps the probabilities of the genetic operators cross over and mutation can range from 0 to 1 but their sum must be equal to 1 3 application of mauss sd to los llanos de san juan mexico 3 1 study area the model was used to simulate optimal patterns of land use change for the region of los llanos de san juan in central mexico which is located between latitudes 19 1003800n to 19 1604100n and longitudes 97 5508500w to 97 3707900w fig 3 the region comprises 5 municipalities in a sub region of 526 km2 where the development of the automobile industry has led to significant land use change before industrialization prior to 2015 the industrial area covered only 0 2 of the region while the urban area occupied 1 5 once the automotive industry was established industrial land multiplied by 12 times and the urban area doubled according to official census data the total population reached 63 770 inhabitants inegi 2010 although there have been significant demographic increases in the last years the region is currently in a food surplus and population increase scenario mauss sd has been applied only theoretically to the study area to estimate the possible impacts on the environment and food supply due to the sudden industrialization of a formerly agro industrial region impacts that are not contemplated in current regulations 3 2 model parameters for the study area in this work the actual map was replicated 20 times to conform the initial map set the probability of cross over and mutation were fixed at 0 9 and 0 1 respectively after several trial runs the number of generations established per period was 30 for a 33 year period 2018 2050 which resulted in a total of 990 iterations of the embedded model as will be detailed below the grid cell that defines each land use allocation map contains 111 rows and 121 columns the studied polygon occupies 8426 cells of which 78 was considered decision variables 6569 as some land uses cannot diminish their current area according to the planning constraints considered in mauss sd for this specific application namely water bodies forest land use with over 15 slope and current urban industrial and irrigated land use their initial area is considered irreversible the surface of urban industrial and forest land uses can increase while that of water bodies and irrigated land cannot it is assumed that the water bodies and irrigated land are producing at their maximum agricultural capacity with fixed water volumes and that food production is constant therefore the food self sufficiency approach depends on the evolution of the other agricultural land uses rainfed land and grassland with a large productive lag in the study area a ymax value that is 2 2 and 1 9 times higher than the current mean value for rainfed land and grassland respectively has been proposed these increases are considered feasible in the study area and would represent more efficient water use for non agricultural uses the minimum water footprint was established at 85 of its current footprint the suitability aptitude maps by land use types are the spatial inputs in the model they were made in advance using official regional information inegi 2012 these maps are made up of subsets of feasible cells for each type of land use 3 3 simulation scenarios for the study area baseline scenario bs in this scenario the drivers parameters assume the historical evolution rates average according to national and regional statistics without introducing changes in their behaviors here the self regulatory mechanisms are inhibited to obtain a projection of the spatial land use distribution if the same economic social and environmental policies remain unchanged during t time steps see table 2 high growth scenario hgs for hgs the drivers lead to rapid urban and industrial growth based on high gdp population and industrial investment growth rates self regulatory mechanisms are introduced in the model since urban and industrial growth is expected to be high and there may be a significant decrease in the land area dedicated to agricultural uses the current values of the biophysical variables lnp and aup were reduced to accelerate urban industrial growth that maximizes income with non extensive urban sprawl minimum negative pressure on the environment npe and to ensure food security for the population sustainable growth scenario sgs this scenario enables a sustainable balance of regional land exploitation in the 33 year period the economic and demographic reference variables have intermediate values while the biophysical variables show a trend towards a more restricted utilization of land to allow sustainable spatial development the self regulatory feedbacks are also enabled in this scenario to facilitate the balance among economic social and environmental drivers of the region in each time step 3 4 results the self regulation mechanisms implemented in the hgs and sgs scenarios were performed successfully thus achieving the planning goals regarding water and food production in the region as detailed below 3 4 1 time changes in the variables of interest the simulation scenarios behaved according to the defined economic and population growth rates the temporal changes of the territorial variables obtained by 2050 are shown in table 3 with no changes in territorial policies the bs scenario presented the lowest growth in population and gdp however the land uses exceeded the availability of water in the study area and agricultural production accumulated the largest deficit since there was no increase in food production agricultural yields remained the same in a smaller cropland area for a larger population the hgs scenario showed that despite large population increases and industrial expansion regional water availability was not exceeded and it was possible to maintain a minimum food deficit due to the greater increases in agricultural yields obtained periodically even though population growth was 3 6 times higher than the initial growth urban expansion only doubled due to the high urban density defined for this scenario the sgs scenario resulted in intermediate population and gdp growth the lowest pressure on water resources and the lowest urban expansion due to the higher urban density defined for this scenario the food deficit was also the smallest of all due to slight increases in the yields of agricultural land uses according to the imposed restrictions urban and industrial growth occurred on rainfed cropland and grassland while the forestland and irrigated cropland surfaces were preserved the execution of the self regulated scenarios hgs and sgs did not require an increase in cropland surface to meet the population s food needs since the increase in yields was sufficient for that purpose nonetheless the hgs scenario required significant increases in yields a decade before the sgs scenario since it was food deficient in period 17 year 2035 while for the sustainable development scenario the food deficit did not appear until period 27 year 2045 fig 4 irrigated land yields were not considered as they have only a marginal impact on meeting the region s dietary requirements due to its small size according to official data siap gob mx 2020 only a third of the total irrigable area is used for crop production 3 4 2 spatial changes the landscapes of all scenarios obtained by 2050 period 33 successfully fulfilled the spatial constraints contiguity and compactness this is true even of the bs scenario which was optimized despite having no self regulatory feedback as can be seen in fig 5 for each scenario the maps with the best values in the objective functions and the largest crowding distance which implies greater diversity gong et al 2016 have been selected a large increase in urban land use was observed to the southeast adjacent to a small settlement and along the road and other urban land increases occurred around the central patches industrial land use only occupied adjacent cells of the automotive industry patch to the northeast and to the main urban patch grassland use underwent a small rearrangement from south of the largest patch to south of the study area the current locations of forestland and irrigated land did not vary 3 4 3 optimization performance as regards the evolution process similar results were obtained in all the scenarios therefore only the analysis of the optimization performance for sgs in the three objective functions is described fig 6 the evolution lines of the optimization process or optimization curves are presented for each period they contain 30 points according to the established generations and each point indicates the average value of the 20 maps for functions o1 and o2 with objectives to be maximized and minimized respectively a greater variability was observed in the average values in the first 7 periods due to an imbalance of the surface areas assigned to each land use which were adjusted to the areas required annually throughout the iterations from period 8 onwards the model stabilized the allocation of the area increments thus the variability range of the values was lower urban and industrial growth entails greater impacts on the environment however the improvement of the o2 values within periods was more significant thus achieving lower negative pressure on the environment at the end of each time step than would be achieved without optimization as can be seen in the zoom in of the curves function o3 was also gradually reduced over each period after the transition to a food deficit scenario by period 27 year 2045 the optimization was not meaningful and the values remained stable the pareto front solutions obtained for sgs for the year 2050 are shown in fig 7 as can be observed there is a wide distribution of solutions in the three dimensional space the selected sgs map fig 5 is indicated with a black circle 4 discussion the optimization curves showed a three stage behavior for all scenarios throughout the planning horizon in the early periods a wider variability range in average values was observed for o1 and o2 due to a spatial adjustment process of annual area per land use in the second and longest stage the annual area requirements were fulfilled and the values therefore showed lower variability at this stage the optimization performance was quite efficient in the third stage the solutions continued to evolve within periods but the values became stable or the improvement was marginal thus indicating the convergence of the optimal solutions in addition to the quantitative evolution a converging pattern of future optimal land use allocations was observed in the three simulation scenarios the best regional allocations for urban land use grew in the central area around the existing urban patches and a large aptitude area for urban development was also identified in the southeast industrial land use preferred contiguity to the largest automotive industrial area to the northeast and northwest along the main regional roads industrial land use never sprawls to the south an area with less aptitude for this use and with greater groundwater vulnerability with higher economic growth rates industrial land use is concentrated around the main roads whereas the expansion of urban land use takes place near minor roads which is preferable minor rearrangements were observed for grassland and forestland to verify the efficiency of the model 3 additional runs were performed see appendix b in the supplementary material in each run the variation rates of the drivers were drastically modified the location trends were very similar as well as the quantitative improvement of the results period by period from the above it follows that there is a spatially clear global optimum over time since the landscape patterns were similar in all the what if scenarios the variability of land use allocations within these spatial patterns identified the local optimal agricultural land use the primary sector activities despite not generating such high incomes in the region guarantee more stable per capita income for the peasants for 87 of the maps the non dominated front was reached in the 30th generation of each period nonetheless it may be recommendable to increase the number of generations to compensate for the annual fixed areas through geometric operators bidirectional feedbacks occurred throughout the simulation either through self regulatory feedbacks or in the stocks of the objective functions the interrelations of the drivers were completely integrated in the performance of both modules spatial and non spatial as the annual territorial response impacted the system functioning based on the analysis of the mauss sd results in the study area the following recommendations for the local government regarding industrial and urban areas are to i not urbanize or expand the industry to the east and southeast of the region where underground water vulnerability is higher ii concentrate urban expansions in the areas proposed by the model and iii change the current occupation pattern by increasing the population density in urban areas recommendations for rainfed agricultural areas are aimed at improving current productivity to this end it is necessary to cultivate more productive varieties compatible with the agroclimatic conditions of the area and improve cultivation techniques for example by implementing precision agriculture techniques this improvement is essential to meet the food needs of a growing population in the different scenarios if these measures are not implemented the regional population could lose the ability to meet its own food needs given the expected population growth in the coming years according to the different scenarios analyzed likewise the region would lose the ability to generate primary surpluses to market in nearby metropolitan areas despite having implemented measures to improve food production if urban and industrial growth occurs too rapidly the region will become food deficient and overexploit and deteriorate the region s water resources by knowing the maximum possible crop yields of an area mauss sd is able to identify the limit of the industrialization and urbanization of that area for the studied scenarios urban settlement patterns need to be redefined taking into account the environmental and food equilibrium of regions as both are essential for human existence in this regard it is necessary to substitute the megalopolis paradigm for new land use policies aimed at reinforcing smaller and more compact urban settlements capable of satisfying their own food requirements mauss sd is a tool for decision makers to estimate when the regional food supply will be insufficient for the population and to consequently guide food policies linked to land use regulations between the two several actions could be implemented e g sustainable agricultural practices such as zero tillage or reduced water abstraction to achieve environmentally sound food self sufficiency 5 conclusions mauss sd is an effective decision support tool for territorial planning that has been developed by embedding an optimal land use allocation model based on sustainability criteria within the methodological framework of sd as a second order territorial system mauss sd estimates patterns of optimal land use change in a territory over time and helps planners to achieve their objectives by means of bidirectional feedback this research has successfully applied mauss sd in the subregion of los llanos de san juan mexico under three what if scenarios to analyze patterns of land use change resulting from a sudden process of industrialization in the period 2018 2050 three more simulation scenarios were added in the supplementary material section to validate the model optimal land use distributions were achieved the average values within periods were improved including the validation scenarios which showed a 6 4 higher gdp 0 24 lower negative pressures on the environment and up to a 10 lower food deficit with the land use distributions provided by mauss sd water needs do not exceed the water resources of the area with ratios of 0 988 0 955 0 996 0 999 and 0 997 for hgs sgs s00 s01 and s02 respectively and the food deficit remains at a minimum by increasing crop yields despite the estimated population growth in the study period therefore the bidirectionality of the model variations in the stocks of the objective functions and negative and positive self regulation loops controls the fulfillment of the territorial planning objectives over time mauss sd permits analyzing a range of variations of the relevant factors thus offering territorial policymakers the possibility to simulate a territory s response to different socio economic scenarios since it can identify for example when resources are overexploited or insufficient or surplus agricultural production and consequently design more appropriate industrial agricultural or urban development policies governments can use the model to modify through policy design the rates of change of drivers and through planning the most suitable locations for each use an advantage of mauss sd is that the model offers a set of spatially optimized maps on the basis of which the regional social actors can negotiate specific locations another advantage is that thanks to the feedback that controls the fulfillment of the planning objectives it is possible to approximate at what moment and under what intensity of land use the limits or carrying capacity of the region is being reached in terms of water availability and food production the proposed model could be applied to larger regions by readjusting the model parameters as it would require starting from larger solution sets as well as a larger number of generations per time interval finally given the importance of sustainability in the allocation of land uses in mauss sd it would be worth incorporating climate change scenarios in future versions of the model declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105463 
25575,in this paper a space time model is developed to support spatial planning the temporal nature of the model allows the variables that condition land use to evolve periodically in each time step land use allocation is optimized considering sustainability criteria the model combines system dynamics techniques to simulate the evolution and interactions of economic and biophysical drivers using a multi objective algorithm for spatial optimization that introduces bidirectional spatio temporal feedback it provides sets of maps for decision makers in which income and food self sufficiency are maximized and the environmental impact of different what if scenarios is minimized the model has been applied in los llanos de san juan mexico for three scenarios baseline high and sustainable growth the maps obtained by 2050 reduced water demand pressure by 1 4 and 4 7 while crop production in calories was balanced with population food needs in 2035 and 2045 in the high and sustainable growth scenarios respectively keywords time space model land response genetic algorithm spatial explicitness economic and biophysics interactions 1 introduction sustainable land management is essential due to its potential contribution to terrestrial equilibrium in the medium and long term where land is considered as the terrestrial portion of the biosphere that comprises the natural resources the ecological processes topography and human settlements and infrastructure ipcc 2019 land use planning is not only a spatial allocation problem but also an issue of land utilization where over exploitation understood as resource depletion and or degradation has long been evident throughout the world for this reason sustainable land management criteria must comprehensively address optimal land use allocation generally according to land aptitude and options for land use change as well as foresee the potential impacts of such changes over time to become a useful land use planning tool that promotes territorial equilibrium various computational methods of optimization have been developed for land use allocation including linear programming lp for a single objective arthur and nalle 1997 chuvieco 1993 schlager 1965 or heuristic multiple objective evolutionary models that compensate several opposing objectives such as competing land uses cao et al 2012 feng and lin 1999 x liu et al 2012 ma and zhao 2015 stewart et al 2004 strauch et al 2019 whose adaptive capacity frequently bioinspired makes them powerful search engines for optimal landscape solutions the non sorting genetic algorithm ii nsga ii developed by deb et al 2002 provides feasible solutions based on the pareto front and has been utilized in land use allocation models azuara garcía et al 2018 cao et al 2011 deb et al 2007 karakostas 2015 the nsga ii has proven to perform effectively in spatial optimization models due to its elitist selection mechanism that preserves good and diverse solutions as well as its efficiency in computing data land use change has been evaluated from different approaches several studies have employed bottom up models to identify and analyze historical patterns of land use expansion e g agricultural frontiers urban industrial sprawl etc and to foresee and even improve future land use change with the aid of mathematical models such as markov chains cellular automata and logistic regressions among others which perform as stand alone models or in combination with other models including optimization procedures jenerette and wu 2001 li et al 2011 mansour et al 2020 rienow and goetzke 2015 roodposhti et al 2019 ruben et al 2020 the deforestation rate is one of the most important environmental impacts addressed by these approaches verburg et al 2002 another approach to temporal analysis is system dynamics sd developed by forrester forrester 1961 1969 sd is applied in top down models and has been employed in most scientific fields due to its efficiency in handling dynamic problems since its application in the club of rome report entitled the limits to growth meadows et al 1972 the interrelations among demographic biophysical and economic factors of the planetary system have become evident accordingly environmental assessments are frequently based on sd procedures for different system sizes bendor et al 2011 graeme and davies 2008 xi and poh 2013 even though most of them are not spatially explicit sd techniques have also been used to identify land use change and envisage future land needs for certain uses sanders and sanders 2012 zhan et al 2012 nonetheless only a few temporal models have evolved from static into spatially explicit dynamics models when coupled with land use allocation optimization procedures d liu et al 2017 ruben et al 2020 verburg et al 2002 yang et al 2019 most of these models seek to improve the accuracy of land use change predictions xu et al 2016 while others identify periodic land area requirements according to diverse scenarios of land demands han et al 2009 however the main goal of dynamic spatial planning models is not to reduce the environmental impacts of land use changes the lack of feedback between spatial and non spatial components is common in models in which sd is coupled with land use allocation algorithms the relationship between the two has been defined in a unidirectional way where the temporal component provides the surface changes by type of land use and the spatial component places them in the best allocations that is the spatial component does not constrain the temporal drivers of the complex system to overcome this issue the coupling mechanisms of dynamic land use models must be improved by integrating bidirectional flow procedures between the spatial and non spatial components e g biophysical and socioeconomic factors and introducing the spatial response of land use over time a few models have solved this lack of bidirectionality such as the structural change model proposed by neuwirth et al 2015 based on an originally non spatial fictitious environment known as daisyworld watson and lovelock 1983 in both its spatial and non spatial versions the model runs simulations for several scenarios to verify the impact of spatial feedback on the system behavior where time is assumed to be a process and space to be a structure the daisyworld model allocates three land cover types according to biophysical conditions planetary albedo global and local temperature growth rate soil fertility etc and performs by using a synchronization loop to tightly couple a sd application with a geographic information system gis at each time step of the simulation despite this successful fictitious implementation of spatial and non spatial bidirectional flow until now it has not been implemented in land use models since the interaction of environmental and socioeconomic factors within a spatial rationality increases the complexity of bidirectionality in land use planning models if land response were considered temporal models could inform stakeholders of the need to implement specific land exploitation constraints or redirect current land policy by taking advantage of the feedback loops since they can perform as self regulatory mechanisms of sd models and indicate the limits of growth as goals of a sustainable land system according to regional needs the objective of this research is to develop a comprehensive spatio temporal model of land use planning to generate optimal and sustainable patterns of land use change for several exploitation scenarios considering biophysical factors and economic and demographic growth rates this model can have a positive impact on water resources and food production can be managed through the implementation of appropriate measures sustainable water use or policies to improve agricultural production to preserve the territorial balance the model uses a multi objective genetic algorithm to determine the spatial distribution of land uses embedded in a sd framework and analyze and limit the impact of land use change the model developed here has only been applied theoretically to the sub region of los llanos de san juan méxico to identify the optimal pathways of land use change by 2050 2 methods this comprehensive spatio temporal model for land use planning has been developed to solve a the optimization of land use allocation with periodic spatial explicitness and b the bidirectional feedback between spatial and non spatial components of the territorial system to achieve sustainability goals such requirements are satisfied in this proposal by embedding a genetic algorithm ga spatial module within a sd time module as described below 2 1 spatial module the sustainable land use allocation model mauss acronym in spanish azuara garcía et al 2017 has been selected to provide the optimal allocation of land uses it is a multiple objective evolutionary model whose optimization core is based on the nsga ii deb et al 2002 mauss generates feasible and diverse map solutions due to its spatial constraints strauch et al 2019 which frame the intrinsic randomness of nature based algorithms thus reducing the optimization iterations needed to achieve pareto front solutions a further advantage of mauss is the consistency of its geometric operators for fulfilling the compactness and contiguity of land use patches an overview of mauss is provided in fig 1 the mauss model performs the land use allocation for a discretized territory into homogeneous surface units cells of the grid every cell is a decision variable gen that assumes one randomly allocated land use type provided it complies with the following spatial constraints i the cell belongs to its feasible subset built according to the aptitude of the land see appendix a of the supplementary material ii it has not been occupied by another previously allocated land use and iii it does not exceed the required maximum area each cell is then evaluated according to the planning targets stated in the objective functions the mauss objective functions are based on sustainability criteria o1 maximize income o2 minimize negative impacts on the environment quality and quantity of water and co2 equivalent emissions and o3 minimize the regional food deficit based on local agricultural production a brief description of the formulae is given below 1 o 1 i 1 r j 1 c u 1 u x i j u gd p i j u α i j u i 1 r j 1 c max gdpiax al of use e tributio n i j where x iju is a binary decision variable that allocates only one land use type u to the cell located in row i column j u is a land use type from 1 to u r denotes the rows of the grid from 1 to r c is the columns of the grid from 1 to c gdp iju is the annual average income millions of currency units of cell ij with land use u max gdp ij is the maximum possible income for cell ij and a iju is the territorial aptitude factor for land use u allocated in cell ij that only affects land uses directly linked to food production 2 o 2 i 1 r j 1 c u 1 u x i j u p w i j u p a i j u where pw iju is the dimensionless negative pressure on water and pa iju is the negative pressure on air the first summand evaluates water availability water footprint and the pollution risk of the resource surface or ground water pa iju quantifies greenhouse gases ghg in terms of co2 equivalent hence the negative pressure that every land use would produce in its location is estimated within this function the calculation method for pw and pa can also be found in azuara garcía et al 2017 to minimize the food deficit once the population and the food needs for their subsistence have been established the algorithm searches for the best locations for crop production based on the suitability of the land food requirements are estimated from the energy provided by food and beverages measured in calories roehrig et al 2013 there are different ways to calculate the land needed for food production based on the energy contribution of each crop measured in calories shepon et al 2016 for this reason the calorie has been considered as the operative unit of the algorithm in equation 3 without intending to value crops from a nutritional perspective an aspect that is beyond the scope of this work 3 o 3 f u p pop i 1 r j 1 c u 1 u h l u y u α i j u δ u i 1 r j 1 c maxa u i j where a u p is the dietary requirements per person expressed in food units fu pop is the total population of the study area u is the index of land uses linked to food production hl is the harvested land of use u y u is the average yield of all crops from use u tons α uij is the agricultural potential index territorial aptitude factor of cell ij with use u δ u is the conversion factor into fu cal of food produced in use u which depends on the crop and crop yield t according to its occupancy percentage in cell ij and max fuij is the maximum possible calories produced in cell ij equation 3 only takes into account the absolute value of the differences between the food units obtained from each land use allocation and the food needs of the population because the lack or the surplus of food should ideally be minimal to encourage optimal land use allocation and hence territorial sustainability the improvement of solution maps is based on the evolutionary process of recombination of genetic information cells among maps via cross over and or mutation for n generations to reach the non dominated solution map set 2 2 time module this module contains the land use change criteria for a time horizon according to an sd approach using constants the variables that explain the system stocks levels of the explanatory variables flows the interrelation of variables and its variation rates and positive or negative feedback loops gonzález busto 1999 to achieve the goals of the system in each time step the flows modify the stocks when stocks surpass or do not reach the predefined levels goals negative or positive feedbacks modify the flows to stabilize and correct the system the different nature of the driving forces is considered here as a precondition for enabling the relationship between the spatial and non spatial components of the model within the complexity of their interrelations in a given territory the drivers must share variables of interest with the objective functions of the optimization module in order to reflect the impacts of the spatial drivers in the system over time the driving forces considered in this research are detailed in table 1 whose variables of interest may assume different values for several what if scenarios the flows among biophysical socio demographic and economic drivers require the construction of a second order system with three interconnected loops a main loop that calculates the surface variation per land use type in each time step and in which the flows from the spatial module are included and two feedback loops that operate as self regulatory mechanisms to achieve the sustainability goals of the system fig 2 2 3 the second order territorial system mauss sd the second order territorial system that brings together the optimal land use allocation module mauss and the time module with land use change factors within the sd methodological framework is called mauss sd it is encoded in matlab language through the interrelations of the drivers and the rates of change of their variables the time module states the area requirements per land use for each time step and controls the rates of negative and positive feedbacks of the system the mauss spatial module functions as an optimization loop nested in the main loop and provides maps of optimal land use allocation according to the objective functions which are themselves sd flows within a period that yields annual stocks of gdp negative pressure on the environment and food deficit a feature of bidirectionality is that such stocks are not linear over time since they are annually conditioned by the land aptitude when the stocks do not accomplish the stated goals levels discrepancies are identified and the time module induces negative or positive feedbacks in the main loop flow to keep the system balanced therefore bidirectional feedbacks occur in each time step the area per land use is the stock where all loops are interconnected in a simplified way the optimization of the set of maps performed in the spatial module is periodic so that in each period a pareto front is obtained the maps of the pareto front of each period are the base population of the next period the changes caused by the drivers and the planning goals are introduced into them and then optimized thus the optimal solutions of the last simulation period are a consequence of the evolution of the optimal solutions from the previous periods 2 3 1 territorial goals a ensure sufficient crop production of agricultural land uses to satisfy local food requirements this goal is to achieve the food self sufficiency of regions and is completely coherent with objective o3 of the optimization module therefore in this model the annual food requirements of the region must be at least equal to its annual crop production cp 4 a u p p o p c p where 4a c p i 1 r j 1 c u 1 u x iju h l u y u α iju whose components have been described in eq 3 b avoid land uses that exceed the regional water availability this goal focuses on preventing water overexploitation thus the total usage of freshwater resources water footprint should be lower or equal to the regional water availability as expressed in the following equation 5 i 1 r j 1 c u 1 u w f iju α iju i 1 r j 1 c w a ij 1 where w f iju is the water footprint m3 associated with land use u located in cell ij and w a ij is the water availability of cell ij and constitutes its annual water recharge this equation corresponds to the water assessment of mauss in objective o2 nonetheless the spatial module seeks to allocate water demanding land uses to locations with high water availability whereas the sd module limits the exploitation of water resources if the planning goals are not met in a period the self regulation mechanisms are activated as described in section 2 3 3 2 3 2 main loop changes in land use sizes the land use types must correspond to those stated in the spatial module generally the change in size depends on the investment applied to the activity land use as a share of gdp and on the population requirements in this study gdp is considered according to land use instead of per capita for industrial land use increases in gdp foster investments in the industrial sector and hence an increase in industrial land industrial area changes e ind in number of cells are obtained in each time step t by the following equation 6 e ind t 1 g d p g d p r k i g d p ind where 6a g d p i 1 r j 1 c s 1 s u 1 u g d p ijsu s gdp is the average value of income recorded by the set of maps s of the simulation considering the income generated by all land uses u in all cells ij as described in table 1 gdp r is the growth rate of the gross domestic product k is the capital investment rate and i is the industrial investment share gdp ind is the reference income for a cell with industrial land use ind in currency units the socio demographic driver is expressed through the population growth rate p in this case it is based on the geometric mean increase to simplify the population dynamics migration processes birth and death rates with fixed variation rates per time step annual p produces changes in the urban extent taking into consideration the land needed per person ln p which indicates a specific urban density the surface variability of urban land use is obtained according to equation 7 e urb t 1 p o p t p l n p a c e l l 7 where e urb t 1 is the urban area for period t 1 given in number of cells pop is the total population of period t acell is the area per surface unit m2 and depends on the planning scale the available detail of cartography and the specific planning targets ln p is an indicator that comprises the floor area per person un 1998 and the green area per person sorensen et al 1998 for urban land use this work considers residential and other uses linked to human activities housing units recreation commerce and services depending on the specific conditions of each study area agricultural uses irrigated land rainfed land grassland or forestland may diminish in size due to industrial and or urban sprawl equation 8 or increase their area due to a greater demand for food units fu equation 9 this latter equation is completely in accordance with objective o3 of the spatial module 8 e u t 1 e u t e ind t 1 e urb t 1 τ u t e i nd t 1 0 a n d e u t 1 0 9 e u t 1 e u t a u p p o p c p m a x a u iju in this case e u t 1 is the area number of cells of agricultural use u in period t 1 and τ is the actual proportion of area with use u 2 3 3 secondary loops self regulatory mechanisms of the territorial system two self regulatory feedback loops that guarantee the accomplishment of the planning objectives goals and the equilibrium of the system in each time step are described next a self regulation of food deficit surplus positive feedback to meet the food goal eq 4 the regulatory mechanism of the mauss sd model presents two ways of promoting regional food self sufficiency depending on the regional needs and characteristics an increase in yields and or the expansion of agricultural land 2 3 3 1 increases in agricultural yield this paper argues that the food balance of a region with a surplus in crop production cannot be achieved through the suppression of agricultural activities since this would affect the income and ways of life of local peasants and hence the balance of the system therefore even for food surplus regions one way to self regulate yields in the model is to increase crop yields proportional to the loss of agricultural area caused by the expansion of other uses a second path is activated when these increases are no longer able to meet the needs of a growing population for which the proportionality of the increases in yields no longer corresponds to the loss of agricultural land but to increases in population growth according to equations 10 13 10 y u t 1 y u t i 1 r i 1 c u 1 u s 1 s g d p ij s u t s t a p iju y u t y u m a x 11 y u t 1 y u t s 1 s c p ijs t s t a p iju y u t y u m a x 12 g d p iju t 1 e u t g d p iju t e u t 1 13 a p iju g d p iju t y u t where ap is the average constant price currency units of crops from use u at cell ij and y u m a x is the average of the maximum possible values of crop yields of agricultural use u which should be obtained from the specialized literature for the crops in the application region the hypothesis considers that to adapt food production in the territory without increasing the availability of resources the variable is crop yield to achieve yield increases it is necessary to develop specific strategies for plant improvement including water use efficiency which are therefore outside the scope of this work increases in agricultural land use size if the maximum yield increases are insufficient to satisfy the population s food demands the positive feedback enables enlarging the size of the established agricultural land uses current policies and regulations may impose the expansion of agricultural land uses in each region a specific formulation for increases in agricultural land use corresponds with equation 9 through these mechanisms the model minimizes food deficit surplus for regions with high food dependency or areas with agricultural overproduction the model can also regulate cropland areas for regions which fluctuate from one dietary condition to another b self regulation of negative pressures on the environment negative feedback this self regulatory mechanism diminishes negative pressures on water resources it ensures that any land use configuration does not exceed the regional water availability by means of reduction of the water footprint the water footprint wf is calculated for each land use as the relation between water usage per production or service unit hoekstra et al 2011 by dividing the wf of use u by the total area that it occupies wf is obtained per unit area ij considering suitability criteria αiju eq 14 14 w f iju t 1 w f iju t i 1 r j 1 c w a ij i 1 r j 1 c u 1 u w f iju α iju w f uij t 1 w f u m i n for agricultural land uses wf is the parameter that relates the amount of water used m3 in obtaining each harvest unit tn it is the result of dividing the use of water per unit area m3 ha by the yield tn ha the wf of a crop is an index of water efficiency of the crop under the conditions in which it has been produced variety location agroclimatic conditions cultivation techniques period of time and in the case of irrigated cultivation the irrigation method and irrigation management therefore for agricultural land uses wf u min average crop requirement m3 ha y u max for required increases of non agricultural land uses wf was proportionally reduced to a preset lower limit wf u min which represents a proportion of the current wf u this limit can be established by the user it is important to highlight that the specific procedures for achieving the proportional increase in yields and the proportional reduction of wf water efficiency technologies fall outside the scope of this study avoiding an increase in land uses with the most demanding water usage without affecting the satisfaction of regional food needs according to equation 15 15 e u t 1 e u t the secondary loops are interconnected with the main loop as equilibrium can also be reached through land use size variations as illustrated in fig 2 the feedback loops maintain the sustainability criteria in each time step of the simulation despite the area changes of each land use type which is also limited by the availability of regional resources under a sustainable exploitation hence these loops are the self regulatory instruments of the system the feedback code is always present in the model although the user can disable it to simulate scenarios without self regulation 2 3 4 economic constraints a the revenue of agricultural land uses u for period t 1 should at least be equal to their initial income in t 16 i 1 r j 1 u u 1 c g d p iju t 1 i 1 r j 1 u u 1 c g d p iju t b with negative or null population growth rates the model assumes that crop yields will not vary to maintain primary sector income 17 y u t 1 y u the values of gdp and y cannot decrease they can only increase as they are subject to positive feedback 2 3 5 parameters of the embedded model to start the time space process the model requires the number of periods t to simulate an initial set of maps s and the number of generations n to evolve the maps per period the initial set of maps can be the set of solutions obtained through a previous optimization procedure or the current map of any study area if the current map is used the algorithm is encoded to replicate the map at least 10 times to complete a minimum size map set in this case the number of parent maps for recombination equals s only in the first generation of each period to broaden offspring diversity otherwise this value is as usual half of s the number of decision variables x is defined within the spatial module and must match the decision variables of the initial maps the probabilities of the genetic operators cross over and mutation can range from 0 to 1 but their sum must be equal to 1 3 application of mauss sd to los llanos de san juan mexico 3 1 study area the model was used to simulate optimal patterns of land use change for the region of los llanos de san juan in central mexico which is located between latitudes 19 1003800n to 19 1604100n and longitudes 97 5508500w to 97 3707900w fig 3 the region comprises 5 municipalities in a sub region of 526 km2 where the development of the automobile industry has led to significant land use change before industrialization prior to 2015 the industrial area covered only 0 2 of the region while the urban area occupied 1 5 once the automotive industry was established industrial land multiplied by 12 times and the urban area doubled according to official census data the total population reached 63 770 inhabitants inegi 2010 although there have been significant demographic increases in the last years the region is currently in a food surplus and population increase scenario mauss sd has been applied only theoretically to the study area to estimate the possible impacts on the environment and food supply due to the sudden industrialization of a formerly agro industrial region impacts that are not contemplated in current regulations 3 2 model parameters for the study area in this work the actual map was replicated 20 times to conform the initial map set the probability of cross over and mutation were fixed at 0 9 and 0 1 respectively after several trial runs the number of generations established per period was 30 for a 33 year period 2018 2050 which resulted in a total of 990 iterations of the embedded model as will be detailed below the grid cell that defines each land use allocation map contains 111 rows and 121 columns the studied polygon occupies 8426 cells of which 78 was considered decision variables 6569 as some land uses cannot diminish their current area according to the planning constraints considered in mauss sd for this specific application namely water bodies forest land use with over 15 slope and current urban industrial and irrigated land use their initial area is considered irreversible the surface of urban industrial and forest land uses can increase while that of water bodies and irrigated land cannot it is assumed that the water bodies and irrigated land are producing at their maximum agricultural capacity with fixed water volumes and that food production is constant therefore the food self sufficiency approach depends on the evolution of the other agricultural land uses rainfed land and grassland with a large productive lag in the study area a ymax value that is 2 2 and 1 9 times higher than the current mean value for rainfed land and grassland respectively has been proposed these increases are considered feasible in the study area and would represent more efficient water use for non agricultural uses the minimum water footprint was established at 85 of its current footprint the suitability aptitude maps by land use types are the spatial inputs in the model they were made in advance using official regional information inegi 2012 these maps are made up of subsets of feasible cells for each type of land use 3 3 simulation scenarios for the study area baseline scenario bs in this scenario the drivers parameters assume the historical evolution rates average according to national and regional statistics without introducing changes in their behaviors here the self regulatory mechanisms are inhibited to obtain a projection of the spatial land use distribution if the same economic social and environmental policies remain unchanged during t time steps see table 2 high growth scenario hgs for hgs the drivers lead to rapid urban and industrial growth based on high gdp population and industrial investment growth rates self regulatory mechanisms are introduced in the model since urban and industrial growth is expected to be high and there may be a significant decrease in the land area dedicated to agricultural uses the current values of the biophysical variables lnp and aup were reduced to accelerate urban industrial growth that maximizes income with non extensive urban sprawl minimum negative pressure on the environment npe and to ensure food security for the population sustainable growth scenario sgs this scenario enables a sustainable balance of regional land exploitation in the 33 year period the economic and demographic reference variables have intermediate values while the biophysical variables show a trend towards a more restricted utilization of land to allow sustainable spatial development the self regulatory feedbacks are also enabled in this scenario to facilitate the balance among economic social and environmental drivers of the region in each time step 3 4 results the self regulation mechanisms implemented in the hgs and sgs scenarios were performed successfully thus achieving the planning goals regarding water and food production in the region as detailed below 3 4 1 time changes in the variables of interest the simulation scenarios behaved according to the defined economic and population growth rates the temporal changes of the territorial variables obtained by 2050 are shown in table 3 with no changes in territorial policies the bs scenario presented the lowest growth in population and gdp however the land uses exceeded the availability of water in the study area and agricultural production accumulated the largest deficit since there was no increase in food production agricultural yields remained the same in a smaller cropland area for a larger population the hgs scenario showed that despite large population increases and industrial expansion regional water availability was not exceeded and it was possible to maintain a minimum food deficit due to the greater increases in agricultural yields obtained periodically even though population growth was 3 6 times higher than the initial growth urban expansion only doubled due to the high urban density defined for this scenario the sgs scenario resulted in intermediate population and gdp growth the lowest pressure on water resources and the lowest urban expansion due to the higher urban density defined for this scenario the food deficit was also the smallest of all due to slight increases in the yields of agricultural land uses according to the imposed restrictions urban and industrial growth occurred on rainfed cropland and grassland while the forestland and irrigated cropland surfaces were preserved the execution of the self regulated scenarios hgs and sgs did not require an increase in cropland surface to meet the population s food needs since the increase in yields was sufficient for that purpose nonetheless the hgs scenario required significant increases in yields a decade before the sgs scenario since it was food deficient in period 17 year 2035 while for the sustainable development scenario the food deficit did not appear until period 27 year 2045 fig 4 irrigated land yields were not considered as they have only a marginal impact on meeting the region s dietary requirements due to its small size according to official data siap gob mx 2020 only a third of the total irrigable area is used for crop production 3 4 2 spatial changes the landscapes of all scenarios obtained by 2050 period 33 successfully fulfilled the spatial constraints contiguity and compactness this is true even of the bs scenario which was optimized despite having no self regulatory feedback as can be seen in fig 5 for each scenario the maps with the best values in the objective functions and the largest crowding distance which implies greater diversity gong et al 2016 have been selected a large increase in urban land use was observed to the southeast adjacent to a small settlement and along the road and other urban land increases occurred around the central patches industrial land use only occupied adjacent cells of the automotive industry patch to the northeast and to the main urban patch grassland use underwent a small rearrangement from south of the largest patch to south of the study area the current locations of forestland and irrigated land did not vary 3 4 3 optimization performance as regards the evolution process similar results were obtained in all the scenarios therefore only the analysis of the optimization performance for sgs in the three objective functions is described fig 6 the evolution lines of the optimization process or optimization curves are presented for each period they contain 30 points according to the established generations and each point indicates the average value of the 20 maps for functions o1 and o2 with objectives to be maximized and minimized respectively a greater variability was observed in the average values in the first 7 periods due to an imbalance of the surface areas assigned to each land use which were adjusted to the areas required annually throughout the iterations from period 8 onwards the model stabilized the allocation of the area increments thus the variability range of the values was lower urban and industrial growth entails greater impacts on the environment however the improvement of the o2 values within periods was more significant thus achieving lower negative pressure on the environment at the end of each time step than would be achieved without optimization as can be seen in the zoom in of the curves function o3 was also gradually reduced over each period after the transition to a food deficit scenario by period 27 year 2045 the optimization was not meaningful and the values remained stable the pareto front solutions obtained for sgs for the year 2050 are shown in fig 7 as can be observed there is a wide distribution of solutions in the three dimensional space the selected sgs map fig 5 is indicated with a black circle 4 discussion the optimization curves showed a three stage behavior for all scenarios throughout the planning horizon in the early periods a wider variability range in average values was observed for o1 and o2 due to a spatial adjustment process of annual area per land use in the second and longest stage the annual area requirements were fulfilled and the values therefore showed lower variability at this stage the optimization performance was quite efficient in the third stage the solutions continued to evolve within periods but the values became stable or the improvement was marginal thus indicating the convergence of the optimal solutions in addition to the quantitative evolution a converging pattern of future optimal land use allocations was observed in the three simulation scenarios the best regional allocations for urban land use grew in the central area around the existing urban patches and a large aptitude area for urban development was also identified in the southeast industrial land use preferred contiguity to the largest automotive industrial area to the northeast and northwest along the main regional roads industrial land use never sprawls to the south an area with less aptitude for this use and with greater groundwater vulnerability with higher economic growth rates industrial land use is concentrated around the main roads whereas the expansion of urban land use takes place near minor roads which is preferable minor rearrangements were observed for grassland and forestland to verify the efficiency of the model 3 additional runs were performed see appendix b in the supplementary material in each run the variation rates of the drivers were drastically modified the location trends were very similar as well as the quantitative improvement of the results period by period from the above it follows that there is a spatially clear global optimum over time since the landscape patterns were similar in all the what if scenarios the variability of land use allocations within these spatial patterns identified the local optimal agricultural land use the primary sector activities despite not generating such high incomes in the region guarantee more stable per capita income for the peasants for 87 of the maps the non dominated front was reached in the 30th generation of each period nonetheless it may be recommendable to increase the number of generations to compensate for the annual fixed areas through geometric operators bidirectional feedbacks occurred throughout the simulation either through self regulatory feedbacks or in the stocks of the objective functions the interrelations of the drivers were completely integrated in the performance of both modules spatial and non spatial as the annual territorial response impacted the system functioning based on the analysis of the mauss sd results in the study area the following recommendations for the local government regarding industrial and urban areas are to i not urbanize or expand the industry to the east and southeast of the region where underground water vulnerability is higher ii concentrate urban expansions in the areas proposed by the model and iii change the current occupation pattern by increasing the population density in urban areas recommendations for rainfed agricultural areas are aimed at improving current productivity to this end it is necessary to cultivate more productive varieties compatible with the agroclimatic conditions of the area and improve cultivation techniques for example by implementing precision agriculture techniques this improvement is essential to meet the food needs of a growing population in the different scenarios if these measures are not implemented the regional population could lose the ability to meet its own food needs given the expected population growth in the coming years according to the different scenarios analyzed likewise the region would lose the ability to generate primary surpluses to market in nearby metropolitan areas despite having implemented measures to improve food production if urban and industrial growth occurs too rapidly the region will become food deficient and overexploit and deteriorate the region s water resources by knowing the maximum possible crop yields of an area mauss sd is able to identify the limit of the industrialization and urbanization of that area for the studied scenarios urban settlement patterns need to be redefined taking into account the environmental and food equilibrium of regions as both are essential for human existence in this regard it is necessary to substitute the megalopolis paradigm for new land use policies aimed at reinforcing smaller and more compact urban settlements capable of satisfying their own food requirements mauss sd is a tool for decision makers to estimate when the regional food supply will be insufficient for the population and to consequently guide food policies linked to land use regulations between the two several actions could be implemented e g sustainable agricultural practices such as zero tillage or reduced water abstraction to achieve environmentally sound food self sufficiency 5 conclusions mauss sd is an effective decision support tool for territorial planning that has been developed by embedding an optimal land use allocation model based on sustainability criteria within the methodological framework of sd as a second order territorial system mauss sd estimates patterns of optimal land use change in a territory over time and helps planners to achieve their objectives by means of bidirectional feedback this research has successfully applied mauss sd in the subregion of los llanos de san juan mexico under three what if scenarios to analyze patterns of land use change resulting from a sudden process of industrialization in the period 2018 2050 three more simulation scenarios were added in the supplementary material section to validate the model optimal land use distributions were achieved the average values within periods were improved including the validation scenarios which showed a 6 4 higher gdp 0 24 lower negative pressures on the environment and up to a 10 lower food deficit with the land use distributions provided by mauss sd water needs do not exceed the water resources of the area with ratios of 0 988 0 955 0 996 0 999 and 0 997 for hgs sgs s00 s01 and s02 respectively and the food deficit remains at a minimum by increasing crop yields despite the estimated population growth in the study period therefore the bidirectionality of the model variations in the stocks of the objective functions and negative and positive self regulation loops controls the fulfillment of the territorial planning objectives over time mauss sd permits analyzing a range of variations of the relevant factors thus offering territorial policymakers the possibility to simulate a territory s response to different socio economic scenarios since it can identify for example when resources are overexploited or insufficient or surplus agricultural production and consequently design more appropriate industrial agricultural or urban development policies governments can use the model to modify through policy design the rates of change of drivers and through planning the most suitable locations for each use an advantage of mauss sd is that the model offers a set of spatially optimized maps on the basis of which the regional social actors can negotiate specific locations another advantage is that thanks to the feedback that controls the fulfillment of the planning objectives it is possible to approximate at what moment and under what intensity of land use the limits or carrying capacity of the region is being reached in terms of water availability and food production the proposed model could be applied to larger regions by readjusting the model parameters as it would require starting from larger solution sets as well as a larger number of generations per time interval finally given the importance of sustainability in the allocation of land uses in mauss sd it would be worth incorporating climate change scenarios in future versions of the model declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105463 
25576,we present an operational system for multi sensor data fusion implemented at the finnish environment institute the system uses ensemble kalman filter and smoother algorithms which are often used for probabilistic analysis of multi sensor data uncertainty and spatial and temporal correlations present in the available observation data are accounted for to obtain accurate and realistic results to test the data fusion system daily chlorophyll a concentration has been modelled across northern shoreline of gulf of finland over the period of august 1st october 31st 2011 chlorophyll a data from routine monitoring stations ferrybox measurements and data derived from medium resolution imaging spectrometer meris instrument on board the envisat satellite has been used as input the data fusion system demonstrates the use of existing and well known ensemble kalman filtering and smoothing methods for improving water quality monitoring programs and for ensuring compliance with ecological standards keywords water quality coastal data fusion data assimilation spatio temporal interpolation 1 introduction we describe a data fusion system dfs for water quality monitoring implemented at the finnish environment institute syke the goal of the system is to harmonize information from various data sources and to provide an estimate of water quality without data gaps i e also at locations and times where observations are not available to obtain accurate and realistic results it is necessary to account for uncertainty in the observational data and exploit spatial and temporal correlations known to be present in the system the uncertainty of the final estimates is quantified to better understand the limitations of the data fusion products and to help in designing better data collection strategies in the future the data fusion products are available in the form of raster maps that can be directly visualized and published corresponding numerical data can also be queried interactively and exported from the system for further processing the presented data fusion system uses kalman filter and smoother algorithms which are often used to analyse spatio temporal variation of multi sensor data dfs is implemented as a general purpose data fusion platform and is not limited to particular physical quantities for the development and testing chlorophyll a chl a concentration and turbidity were used as the two primary water quality indicators to focus on the main objective of the case study was to provide daily spatial chl a estimations that are useful in the ecological classification according to eu water framework directive water quality observations from finnish coastal waters have been collected regularly since the 1960s monitoring is typically based on laboratory analysis of water samples automatic sampling from commercial ships automatic fluorometric measurements from commercial ships and buoys and satellite image processing this gives us an increasing amount of heterogeneous environmental data with varying accuracy precision temporal frequency and regional coverage water analysis in laboratory is usually very accurate but represents limited spatial and temporal domain observations derived from satellite data on the other hand are less accurate but cover much larger area and can be collected continuously with observations available daily or every few days data captured by satellites however often suffers from significant gaps due to the presence of clouds fluorometric measurements are obtained with the frequency of seconds or minutes and may cover a whole ship route or a single buoy location to complicate matters further the physical phenomenon of interest is often observed indirectly through proxies and indicators that are easier or more cost effective to obtain compared to direct measurements water quality of the baltic sea and finnish coastal areas is continuously monitored for ecological classification and management according to the european water framework directive wfd eu 2000 eu marine strategy framework msfd eu 2008 and the helcom baltic sea action plan helcom 2007 granting of environmental permits for industrial and municipal waste water treatment plants or any other loading operations are based on ecological classification and therefore require rigorous monitoring and assessment of environmental status and impacts due to the precautionary principle of eu environmental law kriebel et al 2001 an operation will not be permitted if there exists a risk of ecological deterioration of the receiving water body as a result monitoring needs to be precise and extensive to allow effective classification and permitting combining environmental data from different sources and assimilating them to models of various complexity has been the subject of many methodological and application papers we mention some of them here crow 2003 introduced data assimilation system of surface l band brightness temperature tb observations via the ensemble kalman filter enkf to correct for the impact of poorly sampled rainfall on land surface model predictions of root zone soil moisture and surface energy fluxes within the u s southern great plains pulliainen et al 2004 applied assimilation methods to combine ship borne and satellite data of chl a in the baltic sea and to assess the spatial characteristics of water quality in the baltic sea northern europe the technique is based on bayes theorem and considers spatial accuracy characteristics of both the transect and satellite data pan et al 2008 implemented an integrated data assimilation system over the red arkansas river basin to estimate regional scale terrestrial water cycle driven by multiple satellite remote sensing data mo et al 2008 designed sequential data assimilation with an ensemble kalman filter to optimize key parameters of the boreal ecosystem productivity simulator beps model taking into account errors in the input parameters and observation a number of parameters were adjusted through data assimilation with a time step of one day chang and latif 2010 modelled the behaviour of contaminants in a subsurface flow using two dimensional transport model with advection and dispersion as the deterministic model stroud et al 2010 studied space time development of suspended sediment fields in lake michigan using satellite data and ensemble kalman methods melet et al 2012 explored the potential use of glider data assimilation to control some properties of the ocean state estimation such as thermohaline water circulation misfits in the solomon sea due to an erroneous tidal mixing parametrization the glider data was used to correct the model through a data assimilation scheme mourre and chiggiato 2014 compared the ability of post processing 3 d super ensemble 3dse and conventional ensemble kalman filter enkf approach integrating models and data to forecast the ligurian sea regional oceanographic conditions in the short term range 0 72 h when constrained by a common observation data set revilla romero et al 2016 employed data assimilation techniques in hydrological forecasting to improve estimates of initial conditions and to update incorrect model states with observational data wang et al 2019 presented a operational system for catchment scale water quality management and monitoring and demonstrated its use in two test locations including singapore s coastal waters and freshwater bodies the system integrates input data from real time sensors measurements and models a dynamic model of the catchment hydrology and data assimilation scheme to correct the model outputs with available observations fang et al 2019 used space time kriging approach to estimate the dynamics of harmful algal blooms using chl a concentration measurements in western lake eerie the kriging approach has been complemented with bayesian information criterion for selection of explanatory variables to avoid model over fitting and conditional simulation has been used to obtain probabilistic estimates qian et al 2021 studied chl a and other indicators in western lake eerie for modelling dynamics of cyanobacterial toxin concentrations the authors developed hierarchical bayesian framework for forecasting the toxin concentrations over time and demonstrated its applicability for short term risk assessments chen et al 2019 used data assimilation and the ensemble kalman filter to correct model forecasts of cyanobacterial biomass in lake taihu china using in situ measurements and observations derived from remote sensing data recently chen et al 2021 proposed data fusion method based on bayesian inference principles similar to the ones used in this work and demonstrated its applicability to the estimation of chl a concentration in lake taihu using the in situ and remote sensing observations other statistical and artificial intelligence methods have been used by fasbender et al 2008 doña et al 2015 mouazen et al 2014 and chang et al 2014 for a general reference to statistical methods on spatio temporal data we refer to cressie and wikle 2011 a general reference to ensemble kalman filter methods in data assimilation is evensen 2009 for an application of ensemble methods to high dimensional models similar to those used here see katzfuss et al 2020 alternatives to ensemble methods are different reduced rank methods such as those used by zammit mangion et al 2018 and ma and kang 2020 1 1 system design the data fusion system has been designed to streamline the entire modelling process starting from downloading of the observation data and their harmonization followed by the data fusion computation and subsequent storage and visualization of the final data sets its interface is aimed at expert users who can conveniently carry out data fusion validation and calibration estimate the status of water bodies design operational services and optimize the use of measurement resources the overall design of dfs is shown in fig 1 observation data is read into the system from spatial databases and open data services for in situ and remote sensing data operated by syke an important aspect of the design is to allow automation of various workflows such as near real time fusion of new observations as they become available for this reason the process has been separated into distinct steps that are implemented as stand alone subroutines or commands the processing steps include 1 model variable definition 2 definition of model domain and computational grid 3 data download and harmonization 4 data fusion calculation and 5 data export and visualization these building blocks can be executed manually scheduled to run periodically or scripted to implement more complex workflows the data fusion inputs and results are stored in central database from where they can be queried in external gis applications or published via a web portal majority of the system s functionality is implemented in python language and is designed to be easily extensible where available open source software components have been used the computational core is implemented as a general purpose library called endas ensemble data assimilation system and its source code is available under an open source license as well gunia 2018 for implementation details see section 2 auxiliary material can be found in appendix a below we define the components of the system design model variable model variable is the quantity of interest for which data fusion is carried out such as the chl a concentration or turbidity each model variable has a declared unit of measure and interpretation in case of chl a for example the variable is assumed to correspond to the average chl a concentration in the top 5 metres of the water column model variables supported by the system are currently predefined and cannot be dynamically added by users new variables can however be declared in the database by the system s administrator necessary transformations of the incoming observations are carried out automatically during the data download and harmonization step model domain model domain defines the geographic extent of the model and is given as polygonal geometry by the user the domain is spatially discretized using regular grid with two spatial and one temporal dimension while the resolution of the spatial dimensions cell size can be given by the user the temporal resolution and thus the frequency at which observations are fused is fixed to one day the grid may be sparse and only grid cells which overlap with the domain geometry are actually stored in memory and included in computations as shown in fig 2 each stored cell then represents the value of a model variable or multiple model variables for multivariate analysis spatially integrated over the area of the cell while the underlying phenomena such as water circulation and algae dynamics are fundamentally three dimensional water quality indicators are typically concerned with the top of the water column consequently observations are mainly available for the water surface or a shallow section of the water column the two dimensional depth integrated approach is therefore sufficient for current dfs applications and requires significantly less storage capacity and computational power compared to a fully three dimensional model data download and harmonization data download and harmonization refers to the download of observation data from configured data sources and subsequent pre processing of the data to be suitable for data fusion the downloaded data includes the actual measured values and metadata such as the acquisition date and time uncertainty unit of measure measurement coordinate and station code the data harmonization step consists of coordinate system transformation spatial and temporal interpolation of the measured values and uncertainty to the model grid and unit conversion from the unit declared by the data source to the unit used by dfs additional transformations and corrections can also be applied based on the observation type these include correction to a common time of day i e if observations are taken at different time than expected by dfs depth correction or simple normalization the system currently implements open data protocol odata for discrete point measurement data web coverage service wcs for raster data sets and comma separated values csv for offline point data as a result of the data download and harmonization step all available observations and their uncertainties are stored in the database and can be used directly by the data fusion process without the need for further pre processing data fusion data fusion is the process of combining information from observations to provide complete estimate of the model variables at all grid cells the data fusion products include point estimates of the model variables and their uncertainty additional information such as the observation data sets used for the fusion time and date of the modelling and data fusion settings used is also stored for later retrieval the data fusion method is described in more detail in section 2 data export and visualization data visualization capabilities are provided as a plug in for the qgis open source geographic information system qgis development team 2021 the plug in is easy to install and can be used for browsing and querying of data stored in the dfs database additionally data fusion products can be exported in geotiff raster format to enable further use and post processing in external software 1 2 data fusion as a state estimation problem the goal of data fusion is to integrate observations from multiple sources to produce more complete and accurate estimate than provided by each of the data sources alone in a geophysical context data is typically obtained from a scattered network of measurement devices and with varying sampling frequency we therefore wish to perform statistically consistent interpolation of the data both in time and space so that both the measurement uncertainty and spatial and temporal correlations in the data are accounted for if the behaviour of the observed physical system can be described by means a mathematical model this additional information can be utilized and we commonly refer to the resulting estimation procedure as data assimilation data assimilation has been initially developed for numerical weather prediction but has found its way into many other scientific and engineering disciplines such as gps navigation medical imaging or optimal control in the context of the data fusion system the terms data fusion and data assimilation overlap considerably and are many times used interchangeably it should also be noted that although the currently implemented model see section 2 2 has no prediction power dfs can be extended with more sophisticated dynamical models in the future possible candidates are discussed in section 4 the rest of this section provides brief summary of data assimilation theory that is relevant to the implemented system it is not intended to be exhaustive and we refer the reader to asch et al 2016 or evensen 2009 for more comprehensive introduction details of the implementation are provided in section 2 in statistical terminology the data fusion process can be seen as dynamical spatio temporal data analysis where an important aspect is the modelling of spatio temporal correlations of the system processes which are the key for realistic uncertainty quantification of the data fusion products see cressie and wikle 2011 for a general reference to the statistical analyses the mathematical process model and the observations are two main components of any data assimilation system the process model describes our understanding of the system s dynamics and its governing principles given an initial state of the system the model can also be used to reason about which future states are more likely than others and to rule out states that are in contradiction with the underlying physical laws observations on the other hand are available at various times throughout the assimilation time frame and provide evidence of the real dynamics we generally assume that observations are noisy scarce and that the process of interest is often not observed directly even satellite remote sensing observations do not provide full coverage of the processes under study physical models can be used to constrain the solution of data assimilation analysis the statistical approach assumes certain spatial and temporal correlations between the states observations or the modelled states that are close to each other in space and time are assumed on average to resemble each other more that observations that are further away because the initial state of the system is rarely well known and the mathematical model is an incomplete description of reality model predictions will eventually deviate from the true state data assimilation aims at estimating the model state by correcting the state estimate proportionally to how much we trust the model and the observational evidence the main computational challenge in implementing efficient data fusion algorithms comes from the size of the modelled problem and consequently the amount of computer memory that is needed to store the system state model variables for all grid cells and to represent and manipulate the model error to address this dfs implements two widely used classes of algorithms an exact kalman smoother kalman 1960 algorithm can be used for model states of up to 10 000 elements and is suitable for example for small to medium size lakes for large model grids dfs implements ensemble based kalman smoother evensen 2009 where the system state and error is represented by a collection ensemble of possible realizations of the dynamical system the number of realizations is typically chosen to be rather small while still being able to represent the majority of the model error furthermore the ensemble algorithm is localized so that computations are only performed on a subset of the data at a time this is motivated by the fact that the correlation between variables in any two grid cells decreases with distance and observations that are too far away are therefore assumed not to be of influence this reduces sampling errors inherent to all ensemble based approaches and allows much larger model grids to be processed the size of the local window can be adjusted to balance the result quality and processing speed theoretical details of the implemented computational methods are explained in appendix a 2 implementation overview 2 1 state space representation the system design is based on the concept of a state space model which can be written as two equations 1 x k m k x k 1 θ η k y k h k x k θ ϵ k it describes the evolution of the multi dimensional process state x k in time with time index k observation vector y k contains all available observations at time k and depends only on the current state both the evolution of the state and the observations contain uncertainties which are modelled by stochastic components η and ϵ the other elements of the equation are explained below and we refer to appendix a 1 for more technical details 2 2 evolution model the role of the evolution model m in eq 1 is to propagate state of the system forward in time dfs currently implements a random walk model with a drift i e a simple linear model that converges to an a priori background state over time the model is given by 2 m k x μ b α α x μ b μ b where μ b is the background state vector and 0 α 1 is a dimensionless scalar factor controlling the rate of convergence the background state is assumed to be constant for all grid cells and only the background mean value therefore needs to be chosen by the user different model variables can have different mean values the use of the drift towards a mean state is motivated by the fact that events characterized by chl a peaks are often transient in nature and their typical time span is known from previous monitoring efforts thus the data fusion system is instructed to return to the average state should no observations be available for a longer time period the drift can be disabled by setting α 1 the model is then reduced to m k x μ b α x in spite of the apparent simplicity this model formulation can be used in a wide range of situations where the dynamics of the processes cannot be directly modelled and the target is to augment the missing data spatially and temporally with realistic uncertainties here the actual modelling of the systems spatial correlations is based on the definition of the model error term η k in the state space eq 1 and in the kalman filtering context the covariance matrix q see section 2 4 for information on its construction as such the forecasting power of the current model is limited however the same formulation can be readily extend to more physics based models for example in the form of a discretized advection diffusion model see e g stroud et al 2010 2 3 observation model the role of the observation model h is to describe the relationship between measurements in the observation vector y k and the state vector x k dfs currently assumes that data in the input data sources correspond directly to the modelled variables or if not any necessary transformations have already been performed as part of the data harmonization step see section 1 1 this leaves the observation model implementation rather trivial and the only required functionality is the spatial interpolation of measured values to the model grid for point observations such as the monitoring station observations because gridded observations are already interpolated to the model grid during data harmonization no additional interpolation is necessary 2 4 representation of uncertainty in the state space framework uncertainty is expressed by means of model and observation error terms η k and ϵ k the model error accounts for the inability of the model to fully describe the real physical system due to the lack of knowledge of the underlying governing principles mathematical simplifications or errors introduced by numerical representation because the currently implemented evolution model has very limited forecasting power the model error must account for all spatial and non spatial uncertainty of the estimate the kalman filter and smoother assumes that errors are gaussian and error terms η k and ϵ k are therefore expressed by means of model and observation error covariance matrices q k and r k respectively in the full rank kalman smoother the covariance matrices are full rank and are included in the forecast and analysis step equations directly in the ensemble filtering and smoothing context on the other hand manipulation of explicit covariance matrices is avoided and the effect of model error is implemented by perturbing the ensemble with random realizations drawn from n 0 q k in both cases the covariance matrix q k must be defined either explicitly or implicitly in dfs the model error covariance is assumed to be isotropic and can therefore be described as a function of distance c h r r such that q i j c i j here the time step index k has been omitted for clarity although both q and r can vary in time spatial variability of a random field is more commonly expressed in terms of a variogram 2 γ h v ar x i x i h rather than a covariance function the variogram is related to the covariance function via the relationship 2 γ h c 0 c h dfs implements several popular variogram models including the exponential gaussian and spherical models see e g chiles and delfiner 2012 schabenberger and gotway 2005 for the equations a common assumption for sparsely scattered data is to assume observation errors to be independent between individual measurements and this assumption is also used by dfs this simplifies the analysis scheme significantly because r is a diagonal matrix given by r i σ r 2 t the diagonal elements σ r 2 are the observation variances summarized in section 3 2 it should be noted that the assumption of independence can be expected to hold sufficiently well for data that was collected by different instruments such as the in situ station measurement data or data from different field campaigns the assumption may be problematic for the satellite based data as it is likely that errors in the neighbouring cell estimates derived from a single satellite scene are correlated correlated observation errors are discussed further in section 4 2 5 software implementation the dfs commands have been designed to be easy to modify an extend via well defined software interfaces and protocols which are presented in fig 3 support for new observation data sources can be added by implementing the observation source interface the interface is responsible both for the download of data and its harmonization because the harmonization procedure is specific to the data source dfs implements interface to the open data service vesla which is part of the environmental information system of the finnish environment administration the data service contains results of physio chemical measurements carried out by regional environment centres as well as private companies and water protection associations the data is accessed through the open data protocol additionally gridded observations can be downloaded through the wcs web coverage service interface and off line point observation data can be read from csv files multiple aspects of the data fusion algorithm can be customized as well the software library endas offers unified sequential smoothing api and several algorithms are implemented by the library the api is non intrusive for ensemble methods and does not require any interaction with the dynamic model therefore the evolution model is considered a black box by the data assimilation scheme dfs provides an interface for the evolution model to allow customization without the need to make changes in the assimilation algorithms the observation model which defines the relationship between observed values and modelled fields can also be replaced by providing new implementation lastly the model and observation errors are implemented via covariance operator interfaces provided by endas covariance operators are abstract representations of covariance matrices that are typically defined implicitly in a lower dimensional subspace this way the usually prohibitive storage requirements of full rank matrices are avoided and only the effects of the implicit matrices on data are computed the data fusion system is implemented in python and runs on the intel distribution for python intel corporation 2021 the intel python distribution is a high performance alternative to the reference python implementation for computationally intensive tasks in addition performance critical parts of dfs for which the overhead of pure python would be unacceptable are written using numba lam et al 2015 numba is a just in time compiler for python that transparently generates c code from the python source which is then compiled to machine code before it is executed unlike static compilers such as cython numba does not rely on custom extensions to the python language and infers efficient c code through introspection at runtime numerical arrays and linear algebra routines are provided by the numpy and scipy python packages respectively both packages are internally written in c and c and provide convenient python interface the intel distribution for python comes with optimized numpy scipy and numba packages numpy and scipy code is linked with the intel math kernel library mkl highly optimized and scalable implementation of blas basic linear algebra subprograms and lapack linear algebra package routines for multi core cpus the dfs server runs windows server 2016 standard operating system although the implementation is portable and can be deployed with minor modifications on unix like platforms as well for the database server postgresql version 9 5 13 is used with the postgis spatial extension for handling vector and raster gridded data 2 6 processing tools and data visualization the data fusion tools module consists of a set of stand alone python scripts to execute the full data fusion workflow consisting of model domain creation data import data fusion calculation as well as result raster export in specified points of time or as time series in specified spatial points fig 4 illustrates the usual dfs user workflow in the beginning user defines the model domain as the spatial area polygon for the analysis user can use a model domain which already exists in the system or can add a new one to the database when the model domain is available in the database the next step is to run data harmonization in data harmonization the measurement data is read for the model domain and harmonized i e converted to the internal coordinate system of the database adjusted for time and depth etc and then saved to the database after the model domain and the observation data are processed and saved to the database the data fusion can be run when the data fusion is ready the results are saved to the database the data fusion results as well as source data can be viewed in qgis the dfs browser plugin for qgis provides an easy to use user interface for the selection of the data from the data base also point wise time series and individual raster maps can be exported with dfs browser the access to the observations fusion estimates and their uncertainties through the dfs plug in in qgis 3 is handy to the end users any of them can be plotted on map canvas and on an interactive time series chart for any location by clicking on the map in addition they can be exported in csv format for further analysis the data dfs visualization plug in for qgis is shown in fig 5 3 case study chl a concentration in the baltic coastal area to demonstrate the use of the system chl a concentration in the baltic coastal area has been studied over one summer from april 1st till october 31st 2011 in situ observations from observing stations and measurement systems installed on board commercial ships ferrybox as well as observations derived from satellite based remote sensing material were used within the modelled period there were 289 daily observations available from the monitoring stations 3602 observations from the ferrybox instruments and 26 satellite images including millions of observation records due to the masking of clouds the spatial coverage of the satellite data varied between 10 and 100 3 1 study area approximately 100 km long and 20 km wide stretch of the northern coastline of gulf of finland was selected for testing of the data fusion system the area spans between the porkkala peninsula and the archipelago of porvoo finland and includes the helsinki metropolitan area this area is divided into several water bodies for monitoring and management according to the water framework directive the outer water bodies define the border of the data fusion area towards the central gulf of finland the area is relatively shallow and has low salinity between 0 2 and 5 8 at the surface and 0 3 8 5 near the bottom the average water temperature is close to 0 c in winter in summer it is 15 17 c at the surface and 2 3 c at the bottom the area can freeze from late november to late april the coast is abundant in small bays and skerries but includes only a few large bays and peninsulas such as the porkkala peninsula the main nutrient loading sources are municipalities and agriculture resulting in eutrophication and occasional cyanobacteria blooms in july august water quality in the area is regularly monitored by various methods and water management and pollution control measures are under way the overview of the area is presented in fig 6 3 2 available data all available in situ observations of chl a were collected from the study area these include manual water samples taken at routine monitoring stations and ferrybox measurements collected under the alg line project seppälä et al 2007 the station sample data is available via the vesla data service and accessed by dfs over the open data protocol vesla is a part of the environmental information system of the finnish environment administration syke 2020 and includes physio chemical measurement results of national and regional monitoring carried out by regional environment centres as well as local statutory monitoring results conducted by private companies and water protection associations the alg line data collected on board the m s finnmaid ro ro passenger ship includes sensor measurements of chl a fluorescence these were converted to chl a concentration using water samples taken during the cruise that were analysed in laboratory the alg line water sampling measurements of chl a were also input to the dfs typically 24 samples are taken on the return trip from germany to helsinki the in situ sampling data of the routine monitoring stations were obtained from the vesla odata service satellite data with continuous spatial coverage were also available we used the data from medium resolution imaging spectrometer meris instrument that was onboard the envisat satellite and operational between 2002 2012 version of the l1b satellite data was 3rd data reprocessing with meris ground segment megs processor version 8 0 dataset geolocation was further refined with the amorgos accurate meris ortho rectified geo location operational software tool version 3 0 the biophysical parameters such as chl a were derived from the l1b dataset using a neural network based processor fub wew water schroeder et al 2007b a available in the snap software the individual data sources represent different sections of the water column water samples for chl a at the routine monitoring stations were taken as a composite sample from 0m down to two times the secchi depth transparency secchi depth in the study area usually varies between 1and 5m mainly depending on the distance to land and river mouths and on the time of the year the depth that the satellite observations represent depends on the concentrations of the colour producing substances typically total suspended matter chl a and humic substances and is commonly estimated by the attenuation depth about 90 of the back scattered light from a water column to the atmosphere comes from the surface layer down to the attenuation depth in the southern coast of finland attenuation depth is about 50 of the secchi depth estimated by the method described in kallio et al 2015 the nominal sampling depth of the alg line data is the depth of the water intake 5m however the ship hull mixes the water before the sample is taken and the alg line measurements are thus considered to represent the layer of 0 5m we assume in the data fusion that the water column is fully mixed from surface down to the deepest depth of the mentioned measurement depths the observation data is summarized in table 1 in addition to the measured values dfs also requires information about uncertainty of the input data to produce correct results measurement uncertainty of manual water samples taken at routine monitoring stations is laboratory specific and in most cases an estimate is available the uncertainty reported by laboratories operating in finland usually takes into account both the random and systematic error factor and is given as a standard deviation or its multiple in the case of chl a concentration uncertainty is given as relative standard deviation and normal distribution is assumed for the measurement error the reported uncertainty is stored as metadata in the vesla database and can therefore be directly retrieved by dfs where not available the average uncertainty of water samples collected along the southern finnish coast has been used as a default value uncertainty of data derived from satellite images was quantified in an earlier validation studies conducted in the finnish coastal waters attila et al 2018 and is likewise assumed to be gaussian uncertainty of the alg line water samples was obtained from the marine laboratory of the finnish environment institute for the flow through sensor measurements the average uncertainty of continuous buoy measurements conducted in the years 2013 2014 kallio et al 2015 was used measurement errors used in this work are summarized in table 2 in order for the chl a concentration to remain positive and to account for the error heterogeneity error being proportional to the estimated concentration data fusion has been carried out using logarithms of the values and using relative standard deviations as uncertainties more details on performing data fusion in logarithmic scale are given in appendix a 4 3 3 spatial auto correlation statistical variogram analysis was performed on the available meris data for chl a exponential gaussian matérn and spherical variogram models see e g chiles and delfiner 2012 schabenberger and gotway 2005 were fitted for all days for which data was available between june and august 2011 the spherical variogram model resulted in the lowest average fitting error and has been selected to represent the spatial dependency of chl a variance typical correlation length in june and august was approximately 12 km in july the correlation length varied more than in june and august and ranged between 8 and 15 km july is often characterized by patchy cyanobacterial blooms and the lower end of the fitted range i e 8 km was selected to be a suitable representative value based on expert judgement parameters of the fitted variogram model are summarized in table 3 3 4 other parameters the cell size of the model grid cell has been set to 100 100 metres resulting in state space containing approximately 130 000 elements the ensemble size has been set to 200 members to represent the uncertainty in the system and to provide good trade off between the quality of the result and execution time the model error is described by a spherical variogram function with the non spatial variance term set to 0 2 applied on logarithmic scale and the correlation length set to 8 km in july and 12 7 km otherwise the data assimilation time window i e the smoother lag has been set to 10 days based on expert judgement the smoother lag is used by ensemble filters and smoothers to limit the influence of observations that are too far away in time from the analysis the value of 10 days has been judged large enough because peaks in chl a concentrations are typically much shorter the initial state of the system i e its mean and error covariance has been calculated automatically by the data fusion system the initial mean is calculated as the mean over all observations over an interval of 3 days centred at the starting day of the data fusion run the initial error is calculated by taking the 95 percentile of the relative observation error on each day within the same interval and taking their average the parameters used in the data fusion run are summarized in table 4 3 5 case study results reconstructed chl a concentration and its error estimate for a selected day july 7th 2011 is shown in fig 7 because of the 10 day temporal window used the chl a concentration for that day was estimated from meris instrument data the alg line ferry data and data from measurements stations available between june 28th and july 17th i e within 10 days from july 7th the chla observations derived from the meris instrument for july 7th the alg line ferry route and locations of stations from which data was available within the 10 day window are shown in the top section of fig 7 the estimated chl a concentration is presented in the middle section and the 95 confidence interval of the estimate is shown in the bottom section of fig 7 fig 7 demonstrates the ability of the system to reconstruct the chl a concentration over the entire model domain in the presence of relatively large data gaps the spatial variability and small scale variation of the estimated field is well captured mostly due to the high resolution observations from the meris instrument the estimated confidence interval reflects both the data sparsity and the fact that errors are assumed to be proportional to the estimated value therefore the error is largest in areas for which observations are not available for a prolonged period but which have high estimated chl a concentration the high chl a concentration may be either an estimate obtained earlier or an effect of the influence of more distant observations in the current simulations the most severe data gaps are found in the areas near the shoreline where no meris data coverage was present these areas are however very significant from the water quality management perspective the confidence interval output can be conveniently presented in the form of spatial maps which can be used in making decisions on further data acquisition work the estimated daily chl a concentrations are also shown in fig 8 for four selected locations and utilizing all observation data left column and a subset only containing the satellite eo observations the four selected locations are shown in fig 7 top point 1 is located in the centre of the archipelago area point 2 in the open sea area and points 3 and 4 in the inner archipelago area chl a concentration was monitored frequently at points 1 and 2 and infrequently at point 3 satellite observations were also less frequent in the location of point 3 and were missing at point 4 altogether satellite observations were however available few hundred metres away from point 4 in general the confidence interval of the interpolated concentrations was larger with eo data only compared to confidence interval with all data this was also the case in the inner archipelago area at points 3 and 4 with infrequent or fully missing satellite observations we see that the interpolated concentrations are close to in situ observations when only satellite data are available nearby further outside of the satellite data coverage the estimated concentrations are inaccurate in 2011 chl a followed seasonal pattern typical in the gulf of finland spring peak occurring in late april or early may and late summer peaks occurring in july and august in spring phytoplankton is typically dominated by dinoflagellates and diatoms in late summer by cyanobacteria timing and intensity of late summer peaks depend on the weather conditions low wind speed and high temperature favour cyanobacteria accumulations during the declining phase of chl a peak in the beginning of july 2011 there was a period for which satellite observations were not available making the estimation of the duration of the peak somewhat uncertain manual samples were available in points 1 and 4 during that time which assisted the detection of the decline near the measurement locations on wider scale however manual samples did not make a significant difference after july notable chl a peaks were not observed based on the novel multi source observations at the northern coast of the gulf of finland spatial and temporal distribution of chl a appeared rather transient and multifaceted the simulated system is therefore highly dynamic and is characterized by short and often spatially localized peaks in chl a concentrations the availability of frequent observations with sufficient spatial coverage is crucial for properly capturing these events calm and warm weather which is favourable for phytoplankton growth is likely to coincide with cloudless sky this allows satellite observations to be obtained and thus aims estimation of the chl a peaks the dfs system makes it easy to experiment with various options for the model setup and to study available observation sources and the information content they provide for global analysis on the whole study area the possibility to fill the gaps in the observations both temporal and spatial makes it easy to produce aggregated water quality assessments which include estimate of the uncertainty coming from the interpolation process estimation of chl a during periods with no observations could be improved by adding a hydrodynamic ecosystem model to the dfs to assess the sensitivity of the dfs results to observation errors we adjusted the observation errors artificially and made simulations with 18 and 58 errors addition to the default 38 for eo data and with 10 and 30 errors in addition to the original 20 for alg line sensor data the estimates varied by a rather narrow margin and the sensitivity to observation errors was minor as an example the mean relative chl a errors over the period 1 31 july 2011 at point 2 were 25 2 28 4 and 30 2 for eo observation errors 18 38 and 58 the corresponding estimated mean chl a concentrations were 4 55 μ g l 4 87 μ g l and 4 54 μ g l respectively for the varying alg line observation errors 10 20 and 30 the mean relative errors for the same time period and location were 27 9 28 4 and 29 1 the corresponding mean chl a concentrations were 4 76 μ g l 4 87 μ g l and 4 60 μ g l it should however be noted that the effect of observation error in both satellite and alg line sensor data is likely underestimated due to the fact that in both data sets the observations and their errors are likely correlated due to their close spatial proximity these correlations are currently not handled by the system and may lead to underestimation of the error as discussed in section 4 the results of the sensitivity runs are presented more in detail in appendix c in the auxiliary material 4 discussion and conclusion we have presented an operational system for multi sensor data fusion implemented at the finnish environment institute to test and evaluate the data fusion capabilities daily chl a concentration has been modelled for a part of northern shoreline of the gulf of finland including the helsinki metropolitan area the modelling has been performed for the period between april 1st and october 31st 2011 utilizing data collected from manual sampling stations automatic flow through measurements collected on board commercial cruise vessels and data derived from satellite imagery the application of the data fusion system to the monitoring of coastal area in the gulf of finland has shown feasibility and potential of the system for improving water quality monitoring and management the implemented data fusion methods allow processing of relatively large model domains using high resolution satellite observations the use of the exact full rank algorithm is limited to approximately 10 000 state variables due to the need to manipulate covariance matrices explicitly this is sufficient for many lakes and does not require tuning of auxiliary parameters of the model the ensemble based algorithm on the other hand operates on a reduced rank representation of the error covariance and thus requires significantly less computer memory in addition the algorithm is localized and the size of model grid is therefore only limited by the amount memory needed to store the grid cells rather than a covariance matrix this limit is however seldom met in practice and for practical applications the limit on the model grid is given by the time available to run the data fusion the execution time of the results presented in section 3 has been approximately 2 h using local window size of 3 3 cells increasing the local window to 5 5 cells lowers the execution time to approximately 1 h without noticeable loss of quality the data fusion system runs on a dedicated server machine with xeon e5 2667 processor 16 physical cores at 3 2 ghz and 64 gb of ram it is a general challenge in data fusion to obtain model parameters that give logical and realistic results this is even more so with the ensemble based algorithms that rely on tuning parameters to work well the development of dfs has been motivated by the need for an operational system that streamlines processing of observations and their spatio temporal interpolation and that allows for testing and experimentation with the available data sets thus we have focused primarily on the implementation of the data fusion framework and tentative parametrization of the system has been attempted rigorous derivation of the needed parameters has been left out for future work the meris satellite data used in this work contains significant gaps and does not cover small bays therefore the data fusion system cannot currently produce realistic chl a estimates in these areas this can be improved in the future by using data from the sentinel 2 copernicus mission which has much improved spatial coverage also the spatial resolution of the sentinel 2 data is 10 20 m compared to 300 m resolution of the meris data the dynamic model currently implemented in dfs has no prediction power the system can however be easily extended in the future with more sophisticated dynamical models an example would be a model that takes known seasonal trends into account or an advection diffusion model similar to the one used in stroud et al 2010 known limitation of the current implementation is that observation errors are assumed to be independent and are thus conceptually represented by a diagonal covariance matrix while this holds sufficiently well for observations from measurement stations and buoys which are farther apart the assumption may be problematic for observations derived from satellite data or fluorometer samples in both cases individual observations are close to each other and their observation errors are likely to be correlated treating such observations as independent leads to underestimation of uncertainty of the data fusion result due to observation errors partially cancelling each other out this can be seen in fig 7 it should be noted that the data fusion algorithm can naturally handle correlated observation errors by choosing an appropriate observation error covariance because of the high dimension of the state space the error covariance must however be expressed as a low rank approximation of the real covariance and this has deliberately been left out for future work additionally correlations between multiple sensors can be handled in the same way as correlations within the same sensor because data from one satellite sensor only has been used in this work these correlations were also left out for further study additional future developments of the system include the use of co variate information such as water temperature to improve the accuracy of the results and automatic estimation of model parameters from available data by the system if there is a risk for deterioration of ecological status of coastal area precision of the data fusion results is focal for the monitoring and management of water quality in accordance with the eu water framework directive eu 2000 moreover environmental permit of a polluter may be rejected based on the precautionary principle of eu environmental law kriebel et al 2001 on this account monitoring programs need to be optimized improved and extended to reduce error variances confidence limits and the risk of non compliance with ecological standards the data fusion system makes it possible to take the full advantage of the available data sources to get more complete estimate of the water quality and ecological status one possible application is ecological classification and management of coastal and inland waters according to the wfd it can also be used for environmental monitoring and permitting of fish farms or any other polluter and to assess the impacts of these activities more precisely than by conventional methods software availability software name endas year of first release 2019 operating systems windows linux mac programming languages python availability https github com martingu11 endas license mit documentation https endas readthedocs io en latest declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported financally by the strategic research council of the academy of finland blueadapt project grant number 312650 the finnish ministry of the environment and the earth and water engineering support association maa ja vesitekniikan tuki ry part of the work was also funded by the adafume project of the academy of finland project number 321890 auxiliary material see appendices a c appendix a data fusion algorithms a 1 state space representation here we formally define the data assimilation system let x k be a stochastic process over a set of time steps k 0 k k which represents the dynamical system of interest and the state at time step k is denoted as x k the process is assumed to have the markov property so that future system states only depend on the history thru the current state i e the future is independent of the past given the present we further assume that x k is unobservable i e its state is hidden but another process y k exists that is observed and is dependent on x k in some known way in other words x k is a hidden markov chain the time step k 0 corresponds to the initial system state and we will further assume that observations are available at one or more time steps k 1 k k the states of x k are numerically represented by a sequence of n dimensional vectors of real numbers x k r n similarly observations are represented by r k dimensional vectors y k r r k where r k is the number of observations at time step k given a mathematical model of the process m r n r n the evolution of the system can be described by state space equations 3 x k m k x k 1 θ η k y k h k x k θ ϵ k the model m k is often called the evolution or dynamic model and expresses the relationship between consecutive system states by means of a model prediction h k is called the observation model or observation operator and is a function h k r n r r k that maps the state x k to observations y k vector θ contains auxiliary model parameters which typically do not depend on the time k finally η k and ϵ k are additive terms that account for the model and observation errors respectively both are assumed to be random independent from each other and independent in time it should however be noted that although universally accepted the assumption of independence is rarely fully true in reality data collected by the same instrument over a period time for example may have correlated errors on subsequent measurements the final step in completing the data assimilation scheme is to combine the evolution model with observations in a probabilistic formulation we aim to estimate the posterior distribution of the state space conditioned on collected observations p x 0 k y 1 q where k q k here we use the notation x a b to denote the sequence of state vectors x a b x a x a 1 x b and y a b to denote the sequence of observation vectors defined in an identical fashion depending on the choice of q the following data assimilation schemes can be distinguished p x 0 k y 1 k we wish to estimate system states up to and including the current step k using all observations collected so far this is an instance of so called bayesian filtering p x 0 k y 1 k l for l 0 we wish to estimate system states up to and including the current step k using observations up to and including a future time step k l the parameter l is called the lag and the scheme is referred to as fixed lag bayesian smoothing p x 0 k y 1 k we wish to estimate system states over the entire data assimilation window using all available observations this is called fixed interval bayesian smoothing for the data fusion applications and dynamical spatio temporal data analysis the main tool is the smoother as it allows to combine all available information both in time and space the joint probability densities mentioned above are usually not practical and the computationally simpler marginal distributions p x k y 1 q are used instead the filtering posterior distribution p x k y 1 k can be obtained by applying bayes rule as 4 p x k y 1 k p y 1 k x k p x k y 1 k 1 where p x k y 1 k 1 is the prior distribution and describes the probability of x k before the evidence in y k is considered p y 1 k x k is the observation likelihood conditioned on x k and therefore contains new information present in observations when contrasted with the current state estimate this is also sometimes called the innovation from eq 4 we can see that the filtering solution can be obtained as soon as an observation becomes available allowing on line data assimilation the assumption of uncorrelated errors and the markovian property of x k allows the posterior smoothing distribution to be calculated as a product 5 p x 0 k y 1 k p x 0 k 1 k p y k x k p x k x k 1 where p x 0 is the distribution assigned to the initial state as with the filtering solution the smoothing solution for a given time step k p x k y 1 k can be obtained by marginalization of 5 this is often desired because it avoids re computation of the entire joint posterior distribution of x 0 k every time new observations are obtained furthermore the fixed lag scheme relies on the marginalized solutions to update state estimates for the last l steps so that the resulting data assimilation can still be considered on line but with a fixed delay a 2 kalman filter and smoother the large dimension of state spaces encountered in geosciences usually prevents direct manipulation of the probability density functions in eqs 4 and 5 and we resort to approximations the kalman filter kf kalman 1960 is a popular and often used approach that efficiently solves the filtering and smoothing problems for linear dynamical systems the prior and posterior distributions are approximated by gaussians which can be fully described by the first two statistical moments the mean and covariance the model and observation errors in eq 3 are thus assumed to be normally distributed 6 η k n 0 q k and ϵ k n 0 r k where q k and r k are the model and observation error covariance matrices respectively the posterior density given by eq 5 then becomes a product of gaussians and is therefore also gaussian the marginal filtering and smoothing posterior distributions at a time step k p x k y 1 k and p x k y 1 k are then characterized by covariance matrices p k 1 k and p k 1 k respectively the subscript notation k 1 q is analogous to the notation used in eqs 4 and 5 and denotes the error covariance at time k so that information from observations up to and including the time step q has been incorporated kalman filter is a sequential filtering algorithm for state estimation that consists of two alternating steps first the state vector x k 1 and the corresponding error covariance matrix p k 1 1 k 1 is propagated from time step k 1 to k by application of the dynamic model this is called the forecast step the forecast step is followed by assimilation of observations called the update or analysis step when observations are introduced the state estimate is corrected and the posterior covariance is reduced for actual kalman filter equations we refer the reader to existing literature such as evensen 2009 or asch et al 2016 kalman smoother is a direct extension of the kalman filter and several formulations exist the most direct approach is to augment the state vector of the filter at any given time step with state variables from previous time steps effectively implementing a fixed lag kalman smoother alternatively fixed lag kalman smoother can be formulated without the need to increase the size of the state vectors through retrospective updates cohn et al 1995 in this approach the state vectors x j at k l j k are updated after the assimilation of observations into x k at time t k another well known approach is to first compute the kalman filter solutions for all k 1 k and then proceed with a backward updating pass for k k 1 0 in which information from observations used to correct state estimates at earlier time steps the forward backward technique is known as the rauch tung striebel smoother rauch et al 1965 its advantage is that each state vector is only updated twice and a fixed interval solution over the entire window k 0 k is obtained however the potentially very large covariance matrices p k 1 1 k 1 must be stored for all time steps during the filtering pass this may be prohibitively expensive in terms of storage even if the matrices are written to disk storage until they are needed again kalman filter and smoother can be shown to be an optimal unbiased estimator of the posterior distributions if the system is linear and errors are gaussian real processes are however seldom linear and non linear generalizations such as the extended kalman filter ekf and unscented kalman filter ukf julier et al 1995 have been developed ekf relies on linearization of the dynamic and observation models m k x and h k x through first order taylor series expansion around x ukf on the other hand aims to directly approximate the posterior distribution rather than approximating the process and observation models the approximation is computed purely through evaluations of m k x and h k x model linearizations are therefore not necessary creation of the linearized models and their adjoints required by ekf is a formidable task for complex models even with the help of automatic code differentiation tools another more fundamental drawback is the need to manipulate full rank covariance matrices or sigma vectors in ukf during the forecast and update steps this makes kalman filters including ekf and ukf unpractical for larger spatial domains such as those often encountered in geosciences where the size of the state vector may be n 1 0 5 this has lead to the development of reduced methods for data assimilation a 3 ensemble formulation to overcome the large computational requirements of kalman filters an alternative monte carlo approach called ensemble kalman filter enkf has been proposed by evensen 1994 instead of a single sequence system states x k and covariances p for k 0 1 k enkf uses a collection or ensemble of states to approximate the prior and posterior distributions the size of the ensemble is typically much smaller than the size of the state space allowing ensemble kalman filters to be used on very large systems let the ensemble be defined by an n n matrix 7 e k x k 1 x k 2 x k n where n is the size of the ensemble and x 1 x n are the individual state vectors we can then define a matrix of ensemble perturbations 8 e k 1 n 1 e k e k where e k e k 1 n is a matrix holding the ensemble mean in each column and 1 n is n n matrix with each coefficient equal to 1 n the normalization factor 1 n 1 is chosen so that the full rank error covariance p k is replaced by sample covariance p ˆ k e k e k t it should be noted that just like p k p ˆ k is a n n matrix and can therefore not be included in the filtering and smoothing equations explicitly instead the product e k e k t is used directly and equations are rearranged so that the full matrix does not need to be computed the use of a finite and usually small m n ensemble to represent the covariance matrix p comes with some drawbacks the small sample size introduces errors into the estimated covariance leading to non zero correlations between state variables that in reality are physically unrelated because of these additional spurious correlations state variables that would normally be unaffected are corrected during the update step and their uncertainty therefore decreases as a result the state error covariance may become strongly underestimated over time causing the filter to become overconfident about the model predictions observations become irrelevant and the filter generally diverges from the real system state filter divergence due to the underestimation of error covariance is a problem common to all ensemble kalman filters and approaches such as covariance inflation and localization have been developed to circumvent these issues it is thanks to these practical fixes that ensemble kalman filters have become very popular and successful tools for data assimilation covariance inflation and localization techniques implemented in dfs are described further in appendix b 3 a 4 logarithmic scale in the state space model presented in section 1 2 the terms η k and ϵ k represent absolute model and observation errors in many situations however it is desirable to model the errors as relative and hence proportional to the measured or estimated value a convenient way to achieve this is to perform data fusion in a transformed space 9 z k log x k and assimilate observations of log y k instead consequently z is normally distributed as z n μ z σ z 2 and x becomes log normally distributed according to x log n μ x σ x 2 with the transformed mean and variance given by 10 μ z log μ x 2 μ x 2 σ x 2 σ z 2 log 1 σ x 2 μ x 2 in the equations above the time indices k have been dropped to simplify the notation given the transformation above the final point estimate x k can be obtained as 11 x k μ x exp μ z σ z 2 2 appendix b implementation b 1 implemented kalman smoothers because not all model grids need to be very large dfs implements both the exact full rank kalman smoother and an ensemble based kalman smoother the full rank kalman smoother is presented in appendix a 2 and will not be described further the ensemble kalman smoother variant implemented in dfs is the error subspace transform kalman filter estkf proposed in nerger et al 2012 and its smoother extension estks nerger et al 2014 estkf belongs to the square root family of kalman filters that does not rely on random perturbations in the analysis step unlike traditional enkf ensemble transform kalman filters perform all algebra in a smaller space of the perturbations spanned by th ensemble members the update scheme of estkf is very similar to that of the popular ensemble transform kalman filter bishop et al 2001 but at a slightly lower computational cost b 2 sampling strategy for enks the monte carlo approach of enks relies on random perturbations a r n n drawn from n 0 q k to implement the error term η k one approach to generate the perturbations is to compute the square root of q k via cholesky decomposition and multiply it by a vector of independent random samples drawn from n 0 i this method is exact but computationally infeasible for large model grids due to the high computational cost of the cholesky factorization more efficient approach is to perform the sampling in fourier space using the circulant embedding method wood and chan 1994 which is implemented in dfs it should be noted that the circulant embedding method requires the sampled grid to be regular and dense samples are therefore generated also for cells that are outside of the model boundary and not included in the model grid these samples are currently discarded which is inefficient if the model grid is very sparse to remedy this the block variant of the circulant embedding method park and tretyakov 2015 could be used to minimize the amount of discarded samples because the size of the ensemble is typically much smaller than the size of the state space effort has to be made to avoid unnecessary noise in the ensemble which can lead to unwanted correlations between state variables the common technique for reducing the ensemble sampling noise is covariance localization described in appendix a 3 in dfs however the model error perturbations can also be a major source of noise because they are likely to dominate the system evolution given the simplistic evolution model the easiest way to improve the quality of the ensemble is to increase the its size however doing so leads to increased storage and computational cost of the data fusion algorithm making large ensembles unpractical better approach is to keep the size of the ensemble fixed and aim to maximize its rank instead one simple approach based on the idea introduced in pham 2001 can be implemented as follows we first generate a larger augmented ensemble of perturbations a ˆ r n ω n sampled from n 0 q k using the circulant embedding method the size of the ensemble is ω n with ω 1 being the oversampling ratio we compute the singular value decomposition a ˆ u σ v t and obtain the final ensemble a by sampling along the n largest eigenvalues a u σ 1 n θ t here σ 1 n contains the first n eigenvalues of σ multiplied by n ω n and θ r n n is a random orthogonal matrix it should be noted that the augmented ensemble a ˆ and the singular value decomposition only need to be computed once after that only the random matrix θ needs to be computed to obtain the new sample a the effect of the improved sampling scheme can be evaluated by looking at the singular values of the generated samples as compared to those of the original sample fig 9 demonstrates the improvement in the conditioning of the ensemble by means of the ratio between the largest and smallest singular value for a hypothetical model grid of size 300 300 cells and the spherical covariance function with a range of 30 cells in this example the oversampling ratio ω 4 provides a good compromise between improvement in quality and computational cost beyond ω 6 the improvement is minimal b 3 covariance inflation and localization covariance inflation and localization are two techniques that are often used to stabilize ensemble kalman filters i e to avoid filter divergence due to overconfidence in the filter s performance the purpose of covariance inflation is to account for underestimation of the true error covariance p by p ˆ due to the limited size of the ensemble while the forced inflation of the covariance does not have strong foundation in theory it is nevertheless practical and easy to implement to inflate the covariance p ˆ matrix of ensemble anomalies e from eq 8 is multiplied by a factor slightly larger than one the factor may be a constant scalar value or may vary in time and space because of the ad hoc nature of covariance inflation the inflation is typically tuned for specific data assimilation problems by means of trial and error however methods that aim to estimate optimal inflation factors have also been proposed in raanes et al 2019 miyoshi 2011 and evensen 2009 to name a few the approach suggested by evensen 2009 section 15 3 applies enkf update equations directly to an auxiliary state to estimate the inflation factor it has been tested with dfs but has not yielded consistent improvements and dfs therefore uses a constant inflation factor with a default value 1 02 moreover the use of covariance localization typically reduces the need for inflation and its extensive tuning covariance localization aims to improve the estimated covariance p ˆ by suppressing unwanted correlations i e those that are assumed to be the product of the sampling error in enks variants that operate in the state space the localization effect can be achieved by regularizing the implicit covariance matrix p ˆ through manipulation of the ensemble anomalies however estks operates in the ensemble subspace and the direct regularization approach is not feasible instead localized analysis is achieved by dividing the state vector into subsets called local analysis domains assimilation of observations is then carried for each local analysis domain separately utilizing only observations within certain distance h m a x from the domain the distance of an observation from the local domain can also be used to reduce its influence so that far away observations contribute less to the local analysis solution to do that the observation uncertainty σ r 2 from section 2 4 is additionally tapered by the covariance function c h and becomes 12 σ r i 2 σ r i 2 c h i c 0 1 where h i is the distance of the i th observation from the local domain and i runs over all observations in the local domain the cutting distance h m a x is chosen so that c h m a x c 0 is sufficiently small at minimum each local domain may comprise of a single grid cell the domains may however be larger for performance reasons and dfs uses square blocks of r r grid cells for the local analysis where r is a tuning parameter that balances execution speed and quality the default block size is r 3 and local domains are also enlarged padded so that adjacent domains partly overlap the overlap is used to smoothly blend ensembles from local domains back into the global ensemble to eliminate visible boundaries the principle of local analysis in dfs is shown in fig 10 b 4 generation of initial ensemble the data fusion system can estimate the initial state the state vector mean and the covariance matrix from available observations a commonly approach used in data assimilation is to sample the initial ensemble from a long model run while maximizing the rank of the ensemble pham 2001 this approach is however not currently feasible in dfs due to the trivial nature of the evolution model therefore dfs attempts to estimate the initial state mean and variance from available observations by collecting available observations over a short interval t t n t n where n min l 3 3 and l is the smoother lag or temporal auto correlation length this choice is made so that only observations that are believed to be reasonably close to the initial state but at minimum 3 days are used the initial state vector is then set to the arithmetic mean of the collected observation values per model variable because the observational data is likely to be distributed unevenly in space the background error is first calculated as the 95 percentile of relative observation standard deviation of data points for each day in t the daily errors are then averaged to yield the final initial state error standard deviation σ x 0 finally the covariance matrix p 0 needed for ekf is constructed analogously to the construction of the model error covariance matrix q described in section 2 4 appendix c observation error sensitivity to assess the sensitivity of the data fusion estimates to the uncertainty in the observed data the observation error has been artificially adjusted and additional data fusion simulations were run over the period of 1 31 july 2011 the mean chl a concentration and the corresponding error of the data fusion runs has been recorded the sensitivity runs were made with 18 and 58 relative error for the satellite eo data and 10 and 30 relative error for the alg line data in addition to the default observation errors used 38 for eo and 20 for alg line data the mean chl a concentration and the corresponding mean errors for points 1 4 see fig 7 for locations of the points are shown in table 5 the estimated chl a time series are also shown in fig 11 for the varying eo observation error and fig 12 for the varying alg line error the effect of varying observation errors on data fusion uncertainty is rather minor for both the satellite and alg line observations the reason for this is the high number of observations available in both data sets that are in the vicinity of analysed points due to the assumption of error independence the observation errors cancel each other out leading to a seemingly precise estimate the errors between chl a derived from individual satellite pixels and obtained from subsequent fluorometer samples are however likely to be correlated 
25576,we present an operational system for multi sensor data fusion implemented at the finnish environment institute the system uses ensemble kalman filter and smoother algorithms which are often used for probabilistic analysis of multi sensor data uncertainty and spatial and temporal correlations present in the available observation data are accounted for to obtain accurate and realistic results to test the data fusion system daily chlorophyll a concentration has been modelled across northern shoreline of gulf of finland over the period of august 1st october 31st 2011 chlorophyll a data from routine monitoring stations ferrybox measurements and data derived from medium resolution imaging spectrometer meris instrument on board the envisat satellite has been used as input the data fusion system demonstrates the use of existing and well known ensemble kalman filtering and smoothing methods for improving water quality monitoring programs and for ensuring compliance with ecological standards keywords water quality coastal data fusion data assimilation spatio temporal interpolation 1 introduction we describe a data fusion system dfs for water quality monitoring implemented at the finnish environment institute syke the goal of the system is to harmonize information from various data sources and to provide an estimate of water quality without data gaps i e also at locations and times where observations are not available to obtain accurate and realistic results it is necessary to account for uncertainty in the observational data and exploit spatial and temporal correlations known to be present in the system the uncertainty of the final estimates is quantified to better understand the limitations of the data fusion products and to help in designing better data collection strategies in the future the data fusion products are available in the form of raster maps that can be directly visualized and published corresponding numerical data can also be queried interactively and exported from the system for further processing the presented data fusion system uses kalman filter and smoother algorithms which are often used to analyse spatio temporal variation of multi sensor data dfs is implemented as a general purpose data fusion platform and is not limited to particular physical quantities for the development and testing chlorophyll a chl a concentration and turbidity were used as the two primary water quality indicators to focus on the main objective of the case study was to provide daily spatial chl a estimations that are useful in the ecological classification according to eu water framework directive water quality observations from finnish coastal waters have been collected regularly since the 1960s monitoring is typically based on laboratory analysis of water samples automatic sampling from commercial ships automatic fluorometric measurements from commercial ships and buoys and satellite image processing this gives us an increasing amount of heterogeneous environmental data with varying accuracy precision temporal frequency and regional coverage water analysis in laboratory is usually very accurate but represents limited spatial and temporal domain observations derived from satellite data on the other hand are less accurate but cover much larger area and can be collected continuously with observations available daily or every few days data captured by satellites however often suffers from significant gaps due to the presence of clouds fluorometric measurements are obtained with the frequency of seconds or minutes and may cover a whole ship route or a single buoy location to complicate matters further the physical phenomenon of interest is often observed indirectly through proxies and indicators that are easier or more cost effective to obtain compared to direct measurements water quality of the baltic sea and finnish coastal areas is continuously monitored for ecological classification and management according to the european water framework directive wfd eu 2000 eu marine strategy framework msfd eu 2008 and the helcom baltic sea action plan helcom 2007 granting of environmental permits for industrial and municipal waste water treatment plants or any other loading operations are based on ecological classification and therefore require rigorous monitoring and assessment of environmental status and impacts due to the precautionary principle of eu environmental law kriebel et al 2001 an operation will not be permitted if there exists a risk of ecological deterioration of the receiving water body as a result monitoring needs to be precise and extensive to allow effective classification and permitting combining environmental data from different sources and assimilating them to models of various complexity has been the subject of many methodological and application papers we mention some of them here crow 2003 introduced data assimilation system of surface l band brightness temperature tb observations via the ensemble kalman filter enkf to correct for the impact of poorly sampled rainfall on land surface model predictions of root zone soil moisture and surface energy fluxes within the u s southern great plains pulliainen et al 2004 applied assimilation methods to combine ship borne and satellite data of chl a in the baltic sea and to assess the spatial characteristics of water quality in the baltic sea northern europe the technique is based on bayes theorem and considers spatial accuracy characteristics of both the transect and satellite data pan et al 2008 implemented an integrated data assimilation system over the red arkansas river basin to estimate regional scale terrestrial water cycle driven by multiple satellite remote sensing data mo et al 2008 designed sequential data assimilation with an ensemble kalman filter to optimize key parameters of the boreal ecosystem productivity simulator beps model taking into account errors in the input parameters and observation a number of parameters were adjusted through data assimilation with a time step of one day chang and latif 2010 modelled the behaviour of contaminants in a subsurface flow using two dimensional transport model with advection and dispersion as the deterministic model stroud et al 2010 studied space time development of suspended sediment fields in lake michigan using satellite data and ensemble kalman methods melet et al 2012 explored the potential use of glider data assimilation to control some properties of the ocean state estimation such as thermohaline water circulation misfits in the solomon sea due to an erroneous tidal mixing parametrization the glider data was used to correct the model through a data assimilation scheme mourre and chiggiato 2014 compared the ability of post processing 3 d super ensemble 3dse and conventional ensemble kalman filter enkf approach integrating models and data to forecast the ligurian sea regional oceanographic conditions in the short term range 0 72 h when constrained by a common observation data set revilla romero et al 2016 employed data assimilation techniques in hydrological forecasting to improve estimates of initial conditions and to update incorrect model states with observational data wang et al 2019 presented a operational system for catchment scale water quality management and monitoring and demonstrated its use in two test locations including singapore s coastal waters and freshwater bodies the system integrates input data from real time sensors measurements and models a dynamic model of the catchment hydrology and data assimilation scheme to correct the model outputs with available observations fang et al 2019 used space time kriging approach to estimate the dynamics of harmful algal blooms using chl a concentration measurements in western lake eerie the kriging approach has been complemented with bayesian information criterion for selection of explanatory variables to avoid model over fitting and conditional simulation has been used to obtain probabilistic estimates qian et al 2021 studied chl a and other indicators in western lake eerie for modelling dynamics of cyanobacterial toxin concentrations the authors developed hierarchical bayesian framework for forecasting the toxin concentrations over time and demonstrated its applicability for short term risk assessments chen et al 2019 used data assimilation and the ensemble kalman filter to correct model forecasts of cyanobacterial biomass in lake taihu china using in situ measurements and observations derived from remote sensing data recently chen et al 2021 proposed data fusion method based on bayesian inference principles similar to the ones used in this work and demonstrated its applicability to the estimation of chl a concentration in lake taihu using the in situ and remote sensing observations other statistical and artificial intelligence methods have been used by fasbender et al 2008 doña et al 2015 mouazen et al 2014 and chang et al 2014 for a general reference to statistical methods on spatio temporal data we refer to cressie and wikle 2011 a general reference to ensemble kalman filter methods in data assimilation is evensen 2009 for an application of ensemble methods to high dimensional models similar to those used here see katzfuss et al 2020 alternatives to ensemble methods are different reduced rank methods such as those used by zammit mangion et al 2018 and ma and kang 2020 1 1 system design the data fusion system has been designed to streamline the entire modelling process starting from downloading of the observation data and their harmonization followed by the data fusion computation and subsequent storage and visualization of the final data sets its interface is aimed at expert users who can conveniently carry out data fusion validation and calibration estimate the status of water bodies design operational services and optimize the use of measurement resources the overall design of dfs is shown in fig 1 observation data is read into the system from spatial databases and open data services for in situ and remote sensing data operated by syke an important aspect of the design is to allow automation of various workflows such as near real time fusion of new observations as they become available for this reason the process has been separated into distinct steps that are implemented as stand alone subroutines or commands the processing steps include 1 model variable definition 2 definition of model domain and computational grid 3 data download and harmonization 4 data fusion calculation and 5 data export and visualization these building blocks can be executed manually scheduled to run periodically or scripted to implement more complex workflows the data fusion inputs and results are stored in central database from where they can be queried in external gis applications or published via a web portal majority of the system s functionality is implemented in python language and is designed to be easily extensible where available open source software components have been used the computational core is implemented as a general purpose library called endas ensemble data assimilation system and its source code is available under an open source license as well gunia 2018 for implementation details see section 2 auxiliary material can be found in appendix a below we define the components of the system design model variable model variable is the quantity of interest for which data fusion is carried out such as the chl a concentration or turbidity each model variable has a declared unit of measure and interpretation in case of chl a for example the variable is assumed to correspond to the average chl a concentration in the top 5 metres of the water column model variables supported by the system are currently predefined and cannot be dynamically added by users new variables can however be declared in the database by the system s administrator necessary transformations of the incoming observations are carried out automatically during the data download and harmonization step model domain model domain defines the geographic extent of the model and is given as polygonal geometry by the user the domain is spatially discretized using regular grid with two spatial and one temporal dimension while the resolution of the spatial dimensions cell size can be given by the user the temporal resolution and thus the frequency at which observations are fused is fixed to one day the grid may be sparse and only grid cells which overlap with the domain geometry are actually stored in memory and included in computations as shown in fig 2 each stored cell then represents the value of a model variable or multiple model variables for multivariate analysis spatially integrated over the area of the cell while the underlying phenomena such as water circulation and algae dynamics are fundamentally three dimensional water quality indicators are typically concerned with the top of the water column consequently observations are mainly available for the water surface or a shallow section of the water column the two dimensional depth integrated approach is therefore sufficient for current dfs applications and requires significantly less storage capacity and computational power compared to a fully three dimensional model data download and harmonization data download and harmonization refers to the download of observation data from configured data sources and subsequent pre processing of the data to be suitable for data fusion the downloaded data includes the actual measured values and metadata such as the acquisition date and time uncertainty unit of measure measurement coordinate and station code the data harmonization step consists of coordinate system transformation spatial and temporal interpolation of the measured values and uncertainty to the model grid and unit conversion from the unit declared by the data source to the unit used by dfs additional transformations and corrections can also be applied based on the observation type these include correction to a common time of day i e if observations are taken at different time than expected by dfs depth correction or simple normalization the system currently implements open data protocol odata for discrete point measurement data web coverage service wcs for raster data sets and comma separated values csv for offline point data as a result of the data download and harmonization step all available observations and their uncertainties are stored in the database and can be used directly by the data fusion process without the need for further pre processing data fusion data fusion is the process of combining information from observations to provide complete estimate of the model variables at all grid cells the data fusion products include point estimates of the model variables and their uncertainty additional information such as the observation data sets used for the fusion time and date of the modelling and data fusion settings used is also stored for later retrieval the data fusion method is described in more detail in section 2 data export and visualization data visualization capabilities are provided as a plug in for the qgis open source geographic information system qgis development team 2021 the plug in is easy to install and can be used for browsing and querying of data stored in the dfs database additionally data fusion products can be exported in geotiff raster format to enable further use and post processing in external software 1 2 data fusion as a state estimation problem the goal of data fusion is to integrate observations from multiple sources to produce more complete and accurate estimate than provided by each of the data sources alone in a geophysical context data is typically obtained from a scattered network of measurement devices and with varying sampling frequency we therefore wish to perform statistically consistent interpolation of the data both in time and space so that both the measurement uncertainty and spatial and temporal correlations in the data are accounted for if the behaviour of the observed physical system can be described by means a mathematical model this additional information can be utilized and we commonly refer to the resulting estimation procedure as data assimilation data assimilation has been initially developed for numerical weather prediction but has found its way into many other scientific and engineering disciplines such as gps navigation medical imaging or optimal control in the context of the data fusion system the terms data fusion and data assimilation overlap considerably and are many times used interchangeably it should also be noted that although the currently implemented model see section 2 2 has no prediction power dfs can be extended with more sophisticated dynamical models in the future possible candidates are discussed in section 4 the rest of this section provides brief summary of data assimilation theory that is relevant to the implemented system it is not intended to be exhaustive and we refer the reader to asch et al 2016 or evensen 2009 for more comprehensive introduction details of the implementation are provided in section 2 in statistical terminology the data fusion process can be seen as dynamical spatio temporal data analysis where an important aspect is the modelling of spatio temporal correlations of the system processes which are the key for realistic uncertainty quantification of the data fusion products see cressie and wikle 2011 for a general reference to the statistical analyses the mathematical process model and the observations are two main components of any data assimilation system the process model describes our understanding of the system s dynamics and its governing principles given an initial state of the system the model can also be used to reason about which future states are more likely than others and to rule out states that are in contradiction with the underlying physical laws observations on the other hand are available at various times throughout the assimilation time frame and provide evidence of the real dynamics we generally assume that observations are noisy scarce and that the process of interest is often not observed directly even satellite remote sensing observations do not provide full coverage of the processes under study physical models can be used to constrain the solution of data assimilation analysis the statistical approach assumes certain spatial and temporal correlations between the states observations or the modelled states that are close to each other in space and time are assumed on average to resemble each other more that observations that are further away because the initial state of the system is rarely well known and the mathematical model is an incomplete description of reality model predictions will eventually deviate from the true state data assimilation aims at estimating the model state by correcting the state estimate proportionally to how much we trust the model and the observational evidence the main computational challenge in implementing efficient data fusion algorithms comes from the size of the modelled problem and consequently the amount of computer memory that is needed to store the system state model variables for all grid cells and to represent and manipulate the model error to address this dfs implements two widely used classes of algorithms an exact kalman smoother kalman 1960 algorithm can be used for model states of up to 10 000 elements and is suitable for example for small to medium size lakes for large model grids dfs implements ensemble based kalman smoother evensen 2009 where the system state and error is represented by a collection ensemble of possible realizations of the dynamical system the number of realizations is typically chosen to be rather small while still being able to represent the majority of the model error furthermore the ensemble algorithm is localized so that computations are only performed on a subset of the data at a time this is motivated by the fact that the correlation between variables in any two grid cells decreases with distance and observations that are too far away are therefore assumed not to be of influence this reduces sampling errors inherent to all ensemble based approaches and allows much larger model grids to be processed the size of the local window can be adjusted to balance the result quality and processing speed theoretical details of the implemented computational methods are explained in appendix a 2 implementation overview 2 1 state space representation the system design is based on the concept of a state space model which can be written as two equations 1 x k m k x k 1 θ η k y k h k x k θ ϵ k it describes the evolution of the multi dimensional process state x k in time with time index k observation vector y k contains all available observations at time k and depends only on the current state both the evolution of the state and the observations contain uncertainties which are modelled by stochastic components η and ϵ the other elements of the equation are explained below and we refer to appendix a 1 for more technical details 2 2 evolution model the role of the evolution model m in eq 1 is to propagate state of the system forward in time dfs currently implements a random walk model with a drift i e a simple linear model that converges to an a priori background state over time the model is given by 2 m k x μ b α α x μ b μ b where μ b is the background state vector and 0 α 1 is a dimensionless scalar factor controlling the rate of convergence the background state is assumed to be constant for all grid cells and only the background mean value therefore needs to be chosen by the user different model variables can have different mean values the use of the drift towards a mean state is motivated by the fact that events characterized by chl a peaks are often transient in nature and their typical time span is known from previous monitoring efforts thus the data fusion system is instructed to return to the average state should no observations be available for a longer time period the drift can be disabled by setting α 1 the model is then reduced to m k x μ b α x in spite of the apparent simplicity this model formulation can be used in a wide range of situations where the dynamics of the processes cannot be directly modelled and the target is to augment the missing data spatially and temporally with realistic uncertainties here the actual modelling of the systems spatial correlations is based on the definition of the model error term η k in the state space eq 1 and in the kalman filtering context the covariance matrix q see section 2 4 for information on its construction as such the forecasting power of the current model is limited however the same formulation can be readily extend to more physics based models for example in the form of a discretized advection diffusion model see e g stroud et al 2010 2 3 observation model the role of the observation model h is to describe the relationship between measurements in the observation vector y k and the state vector x k dfs currently assumes that data in the input data sources correspond directly to the modelled variables or if not any necessary transformations have already been performed as part of the data harmonization step see section 1 1 this leaves the observation model implementation rather trivial and the only required functionality is the spatial interpolation of measured values to the model grid for point observations such as the monitoring station observations because gridded observations are already interpolated to the model grid during data harmonization no additional interpolation is necessary 2 4 representation of uncertainty in the state space framework uncertainty is expressed by means of model and observation error terms η k and ϵ k the model error accounts for the inability of the model to fully describe the real physical system due to the lack of knowledge of the underlying governing principles mathematical simplifications or errors introduced by numerical representation because the currently implemented evolution model has very limited forecasting power the model error must account for all spatial and non spatial uncertainty of the estimate the kalman filter and smoother assumes that errors are gaussian and error terms η k and ϵ k are therefore expressed by means of model and observation error covariance matrices q k and r k respectively in the full rank kalman smoother the covariance matrices are full rank and are included in the forecast and analysis step equations directly in the ensemble filtering and smoothing context on the other hand manipulation of explicit covariance matrices is avoided and the effect of model error is implemented by perturbing the ensemble with random realizations drawn from n 0 q k in both cases the covariance matrix q k must be defined either explicitly or implicitly in dfs the model error covariance is assumed to be isotropic and can therefore be described as a function of distance c h r r such that q i j c i j here the time step index k has been omitted for clarity although both q and r can vary in time spatial variability of a random field is more commonly expressed in terms of a variogram 2 γ h v ar x i x i h rather than a covariance function the variogram is related to the covariance function via the relationship 2 γ h c 0 c h dfs implements several popular variogram models including the exponential gaussian and spherical models see e g chiles and delfiner 2012 schabenberger and gotway 2005 for the equations a common assumption for sparsely scattered data is to assume observation errors to be independent between individual measurements and this assumption is also used by dfs this simplifies the analysis scheme significantly because r is a diagonal matrix given by r i σ r 2 t the diagonal elements σ r 2 are the observation variances summarized in section 3 2 it should be noted that the assumption of independence can be expected to hold sufficiently well for data that was collected by different instruments such as the in situ station measurement data or data from different field campaigns the assumption may be problematic for the satellite based data as it is likely that errors in the neighbouring cell estimates derived from a single satellite scene are correlated correlated observation errors are discussed further in section 4 2 5 software implementation the dfs commands have been designed to be easy to modify an extend via well defined software interfaces and protocols which are presented in fig 3 support for new observation data sources can be added by implementing the observation source interface the interface is responsible both for the download of data and its harmonization because the harmonization procedure is specific to the data source dfs implements interface to the open data service vesla which is part of the environmental information system of the finnish environment administration the data service contains results of physio chemical measurements carried out by regional environment centres as well as private companies and water protection associations the data is accessed through the open data protocol additionally gridded observations can be downloaded through the wcs web coverage service interface and off line point observation data can be read from csv files multiple aspects of the data fusion algorithm can be customized as well the software library endas offers unified sequential smoothing api and several algorithms are implemented by the library the api is non intrusive for ensemble methods and does not require any interaction with the dynamic model therefore the evolution model is considered a black box by the data assimilation scheme dfs provides an interface for the evolution model to allow customization without the need to make changes in the assimilation algorithms the observation model which defines the relationship between observed values and modelled fields can also be replaced by providing new implementation lastly the model and observation errors are implemented via covariance operator interfaces provided by endas covariance operators are abstract representations of covariance matrices that are typically defined implicitly in a lower dimensional subspace this way the usually prohibitive storage requirements of full rank matrices are avoided and only the effects of the implicit matrices on data are computed the data fusion system is implemented in python and runs on the intel distribution for python intel corporation 2021 the intel python distribution is a high performance alternative to the reference python implementation for computationally intensive tasks in addition performance critical parts of dfs for which the overhead of pure python would be unacceptable are written using numba lam et al 2015 numba is a just in time compiler for python that transparently generates c code from the python source which is then compiled to machine code before it is executed unlike static compilers such as cython numba does not rely on custom extensions to the python language and infers efficient c code through introspection at runtime numerical arrays and linear algebra routines are provided by the numpy and scipy python packages respectively both packages are internally written in c and c and provide convenient python interface the intel distribution for python comes with optimized numpy scipy and numba packages numpy and scipy code is linked with the intel math kernel library mkl highly optimized and scalable implementation of blas basic linear algebra subprograms and lapack linear algebra package routines for multi core cpus the dfs server runs windows server 2016 standard operating system although the implementation is portable and can be deployed with minor modifications on unix like platforms as well for the database server postgresql version 9 5 13 is used with the postgis spatial extension for handling vector and raster gridded data 2 6 processing tools and data visualization the data fusion tools module consists of a set of stand alone python scripts to execute the full data fusion workflow consisting of model domain creation data import data fusion calculation as well as result raster export in specified points of time or as time series in specified spatial points fig 4 illustrates the usual dfs user workflow in the beginning user defines the model domain as the spatial area polygon for the analysis user can use a model domain which already exists in the system or can add a new one to the database when the model domain is available in the database the next step is to run data harmonization in data harmonization the measurement data is read for the model domain and harmonized i e converted to the internal coordinate system of the database adjusted for time and depth etc and then saved to the database after the model domain and the observation data are processed and saved to the database the data fusion can be run when the data fusion is ready the results are saved to the database the data fusion results as well as source data can be viewed in qgis the dfs browser plugin for qgis provides an easy to use user interface for the selection of the data from the data base also point wise time series and individual raster maps can be exported with dfs browser the access to the observations fusion estimates and their uncertainties through the dfs plug in in qgis 3 is handy to the end users any of them can be plotted on map canvas and on an interactive time series chart for any location by clicking on the map in addition they can be exported in csv format for further analysis the data dfs visualization plug in for qgis is shown in fig 5 3 case study chl a concentration in the baltic coastal area to demonstrate the use of the system chl a concentration in the baltic coastal area has been studied over one summer from april 1st till october 31st 2011 in situ observations from observing stations and measurement systems installed on board commercial ships ferrybox as well as observations derived from satellite based remote sensing material were used within the modelled period there were 289 daily observations available from the monitoring stations 3602 observations from the ferrybox instruments and 26 satellite images including millions of observation records due to the masking of clouds the spatial coverage of the satellite data varied between 10 and 100 3 1 study area approximately 100 km long and 20 km wide stretch of the northern coastline of gulf of finland was selected for testing of the data fusion system the area spans between the porkkala peninsula and the archipelago of porvoo finland and includes the helsinki metropolitan area this area is divided into several water bodies for monitoring and management according to the water framework directive the outer water bodies define the border of the data fusion area towards the central gulf of finland the area is relatively shallow and has low salinity between 0 2 and 5 8 at the surface and 0 3 8 5 near the bottom the average water temperature is close to 0 c in winter in summer it is 15 17 c at the surface and 2 3 c at the bottom the area can freeze from late november to late april the coast is abundant in small bays and skerries but includes only a few large bays and peninsulas such as the porkkala peninsula the main nutrient loading sources are municipalities and agriculture resulting in eutrophication and occasional cyanobacteria blooms in july august water quality in the area is regularly monitored by various methods and water management and pollution control measures are under way the overview of the area is presented in fig 6 3 2 available data all available in situ observations of chl a were collected from the study area these include manual water samples taken at routine monitoring stations and ferrybox measurements collected under the alg line project seppälä et al 2007 the station sample data is available via the vesla data service and accessed by dfs over the open data protocol vesla is a part of the environmental information system of the finnish environment administration syke 2020 and includes physio chemical measurement results of national and regional monitoring carried out by regional environment centres as well as local statutory monitoring results conducted by private companies and water protection associations the alg line data collected on board the m s finnmaid ro ro passenger ship includes sensor measurements of chl a fluorescence these were converted to chl a concentration using water samples taken during the cruise that were analysed in laboratory the alg line water sampling measurements of chl a were also input to the dfs typically 24 samples are taken on the return trip from germany to helsinki the in situ sampling data of the routine monitoring stations were obtained from the vesla odata service satellite data with continuous spatial coverage were also available we used the data from medium resolution imaging spectrometer meris instrument that was onboard the envisat satellite and operational between 2002 2012 version of the l1b satellite data was 3rd data reprocessing with meris ground segment megs processor version 8 0 dataset geolocation was further refined with the amorgos accurate meris ortho rectified geo location operational software tool version 3 0 the biophysical parameters such as chl a were derived from the l1b dataset using a neural network based processor fub wew water schroeder et al 2007b a available in the snap software the individual data sources represent different sections of the water column water samples for chl a at the routine monitoring stations were taken as a composite sample from 0m down to two times the secchi depth transparency secchi depth in the study area usually varies between 1and 5m mainly depending on the distance to land and river mouths and on the time of the year the depth that the satellite observations represent depends on the concentrations of the colour producing substances typically total suspended matter chl a and humic substances and is commonly estimated by the attenuation depth about 90 of the back scattered light from a water column to the atmosphere comes from the surface layer down to the attenuation depth in the southern coast of finland attenuation depth is about 50 of the secchi depth estimated by the method described in kallio et al 2015 the nominal sampling depth of the alg line data is the depth of the water intake 5m however the ship hull mixes the water before the sample is taken and the alg line measurements are thus considered to represent the layer of 0 5m we assume in the data fusion that the water column is fully mixed from surface down to the deepest depth of the mentioned measurement depths the observation data is summarized in table 1 in addition to the measured values dfs also requires information about uncertainty of the input data to produce correct results measurement uncertainty of manual water samples taken at routine monitoring stations is laboratory specific and in most cases an estimate is available the uncertainty reported by laboratories operating in finland usually takes into account both the random and systematic error factor and is given as a standard deviation or its multiple in the case of chl a concentration uncertainty is given as relative standard deviation and normal distribution is assumed for the measurement error the reported uncertainty is stored as metadata in the vesla database and can therefore be directly retrieved by dfs where not available the average uncertainty of water samples collected along the southern finnish coast has been used as a default value uncertainty of data derived from satellite images was quantified in an earlier validation studies conducted in the finnish coastal waters attila et al 2018 and is likewise assumed to be gaussian uncertainty of the alg line water samples was obtained from the marine laboratory of the finnish environment institute for the flow through sensor measurements the average uncertainty of continuous buoy measurements conducted in the years 2013 2014 kallio et al 2015 was used measurement errors used in this work are summarized in table 2 in order for the chl a concentration to remain positive and to account for the error heterogeneity error being proportional to the estimated concentration data fusion has been carried out using logarithms of the values and using relative standard deviations as uncertainties more details on performing data fusion in logarithmic scale are given in appendix a 4 3 3 spatial auto correlation statistical variogram analysis was performed on the available meris data for chl a exponential gaussian matérn and spherical variogram models see e g chiles and delfiner 2012 schabenberger and gotway 2005 were fitted for all days for which data was available between june and august 2011 the spherical variogram model resulted in the lowest average fitting error and has been selected to represent the spatial dependency of chl a variance typical correlation length in june and august was approximately 12 km in july the correlation length varied more than in june and august and ranged between 8 and 15 km july is often characterized by patchy cyanobacterial blooms and the lower end of the fitted range i e 8 km was selected to be a suitable representative value based on expert judgement parameters of the fitted variogram model are summarized in table 3 3 4 other parameters the cell size of the model grid cell has been set to 100 100 metres resulting in state space containing approximately 130 000 elements the ensemble size has been set to 200 members to represent the uncertainty in the system and to provide good trade off between the quality of the result and execution time the model error is described by a spherical variogram function with the non spatial variance term set to 0 2 applied on logarithmic scale and the correlation length set to 8 km in july and 12 7 km otherwise the data assimilation time window i e the smoother lag has been set to 10 days based on expert judgement the smoother lag is used by ensemble filters and smoothers to limit the influence of observations that are too far away in time from the analysis the value of 10 days has been judged large enough because peaks in chl a concentrations are typically much shorter the initial state of the system i e its mean and error covariance has been calculated automatically by the data fusion system the initial mean is calculated as the mean over all observations over an interval of 3 days centred at the starting day of the data fusion run the initial error is calculated by taking the 95 percentile of the relative observation error on each day within the same interval and taking their average the parameters used in the data fusion run are summarized in table 4 3 5 case study results reconstructed chl a concentration and its error estimate for a selected day july 7th 2011 is shown in fig 7 because of the 10 day temporal window used the chl a concentration for that day was estimated from meris instrument data the alg line ferry data and data from measurements stations available between june 28th and july 17th i e within 10 days from july 7th the chla observations derived from the meris instrument for july 7th the alg line ferry route and locations of stations from which data was available within the 10 day window are shown in the top section of fig 7 the estimated chl a concentration is presented in the middle section and the 95 confidence interval of the estimate is shown in the bottom section of fig 7 fig 7 demonstrates the ability of the system to reconstruct the chl a concentration over the entire model domain in the presence of relatively large data gaps the spatial variability and small scale variation of the estimated field is well captured mostly due to the high resolution observations from the meris instrument the estimated confidence interval reflects both the data sparsity and the fact that errors are assumed to be proportional to the estimated value therefore the error is largest in areas for which observations are not available for a prolonged period but which have high estimated chl a concentration the high chl a concentration may be either an estimate obtained earlier or an effect of the influence of more distant observations in the current simulations the most severe data gaps are found in the areas near the shoreline where no meris data coverage was present these areas are however very significant from the water quality management perspective the confidence interval output can be conveniently presented in the form of spatial maps which can be used in making decisions on further data acquisition work the estimated daily chl a concentrations are also shown in fig 8 for four selected locations and utilizing all observation data left column and a subset only containing the satellite eo observations the four selected locations are shown in fig 7 top point 1 is located in the centre of the archipelago area point 2 in the open sea area and points 3 and 4 in the inner archipelago area chl a concentration was monitored frequently at points 1 and 2 and infrequently at point 3 satellite observations were also less frequent in the location of point 3 and were missing at point 4 altogether satellite observations were however available few hundred metres away from point 4 in general the confidence interval of the interpolated concentrations was larger with eo data only compared to confidence interval with all data this was also the case in the inner archipelago area at points 3 and 4 with infrequent or fully missing satellite observations we see that the interpolated concentrations are close to in situ observations when only satellite data are available nearby further outside of the satellite data coverage the estimated concentrations are inaccurate in 2011 chl a followed seasonal pattern typical in the gulf of finland spring peak occurring in late april or early may and late summer peaks occurring in july and august in spring phytoplankton is typically dominated by dinoflagellates and diatoms in late summer by cyanobacteria timing and intensity of late summer peaks depend on the weather conditions low wind speed and high temperature favour cyanobacteria accumulations during the declining phase of chl a peak in the beginning of july 2011 there was a period for which satellite observations were not available making the estimation of the duration of the peak somewhat uncertain manual samples were available in points 1 and 4 during that time which assisted the detection of the decline near the measurement locations on wider scale however manual samples did not make a significant difference after july notable chl a peaks were not observed based on the novel multi source observations at the northern coast of the gulf of finland spatial and temporal distribution of chl a appeared rather transient and multifaceted the simulated system is therefore highly dynamic and is characterized by short and often spatially localized peaks in chl a concentrations the availability of frequent observations with sufficient spatial coverage is crucial for properly capturing these events calm and warm weather which is favourable for phytoplankton growth is likely to coincide with cloudless sky this allows satellite observations to be obtained and thus aims estimation of the chl a peaks the dfs system makes it easy to experiment with various options for the model setup and to study available observation sources and the information content they provide for global analysis on the whole study area the possibility to fill the gaps in the observations both temporal and spatial makes it easy to produce aggregated water quality assessments which include estimate of the uncertainty coming from the interpolation process estimation of chl a during periods with no observations could be improved by adding a hydrodynamic ecosystem model to the dfs to assess the sensitivity of the dfs results to observation errors we adjusted the observation errors artificially and made simulations with 18 and 58 errors addition to the default 38 for eo data and with 10 and 30 errors in addition to the original 20 for alg line sensor data the estimates varied by a rather narrow margin and the sensitivity to observation errors was minor as an example the mean relative chl a errors over the period 1 31 july 2011 at point 2 were 25 2 28 4 and 30 2 for eo observation errors 18 38 and 58 the corresponding estimated mean chl a concentrations were 4 55 μ g l 4 87 μ g l and 4 54 μ g l respectively for the varying alg line observation errors 10 20 and 30 the mean relative errors for the same time period and location were 27 9 28 4 and 29 1 the corresponding mean chl a concentrations were 4 76 μ g l 4 87 μ g l and 4 60 μ g l it should however be noted that the effect of observation error in both satellite and alg line sensor data is likely underestimated due to the fact that in both data sets the observations and their errors are likely correlated due to their close spatial proximity these correlations are currently not handled by the system and may lead to underestimation of the error as discussed in section 4 the results of the sensitivity runs are presented more in detail in appendix c in the auxiliary material 4 discussion and conclusion we have presented an operational system for multi sensor data fusion implemented at the finnish environment institute to test and evaluate the data fusion capabilities daily chl a concentration has been modelled for a part of northern shoreline of the gulf of finland including the helsinki metropolitan area the modelling has been performed for the period between april 1st and october 31st 2011 utilizing data collected from manual sampling stations automatic flow through measurements collected on board commercial cruise vessels and data derived from satellite imagery the application of the data fusion system to the monitoring of coastal area in the gulf of finland has shown feasibility and potential of the system for improving water quality monitoring and management the implemented data fusion methods allow processing of relatively large model domains using high resolution satellite observations the use of the exact full rank algorithm is limited to approximately 10 000 state variables due to the need to manipulate covariance matrices explicitly this is sufficient for many lakes and does not require tuning of auxiliary parameters of the model the ensemble based algorithm on the other hand operates on a reduced rank representation of the error covariance and thus requires significantly less computer memory in addition the algorithm is localized and the size of model grid is therefore only limited by the amount memory needed to store the grid cells rather than a covariance matrix this limit is however seldom met in practice and for practical applications the limit on the model grid is given by the time available to run the data fusion the execution time of the results presented in section 3 has been approximately 2 h using local window size of 3 3 cells increasing the local window to 5 5 cells lowers the execution time to approximately 1 h without noticeable loss of quality the data fusion system runs on a dedicated server machine with xeon e5 2667 processor 16 physical cores at 3 2 ghz and 64 gb of ram it is a general challenge in data fusion to obtain model parameters that give logical and realistic results this is even more so with the ensemble based algorithms that rely on tuning parameters to work well the development of dfs has been motivated by the need for an operational system that streamlines processing of observations and their spatio temporal interpolation and that allows for testing and experimentation with the available data sets thus we have focused primarily on the implementation of the data fusion framework and tentative parametrization of the system has been attempted rigorous derivation of the needed parameters has been left out for future work the meris satellite data used in this work contains significant gaps and does not cover small bays therefore the data fusion system cannot currently produce realistic chl a estimates in these areas this can be improved in the future by using data from the sentinel 2 copernicus mission which has much improved spatial coverage also the spatial resolution of the sentinel 2 data is 10 20 m compared to 300 m resolution of the meris data the dynamic model currently implemented in dfs has no prediction power the system can however be easily extended in the future with more sophisticated dynamical models an example would be a model that takes known seasonal trends into account or an advection diffusion model similar to the one used in stroud et al 2010 known limitation of the current implementation is that observation errors are assumed to be independent and are thus conceptually represented by a diagonal covariance matrix while this holds sufficiently well for observations from measurement stations and buoys which are farther apart the assumption may be problematic for observations derived from satellite data or fluorometer samples in both cases individual observations are close to each other and their observation errors are likely to be correlated treating such observations as independent leads to underestimation of uncertainty of the data fusion result due to observation errors partially cancelling each other out this can be seen in fig 7 it should be noted that the data fusion algorithm can naturally handle correlated observation errors by choosing an appropriate observation error covariance because of the high dimension of the state space the error covariance must however be expressed as a low rank approximation of the real covariance and this has deliberately been left out for future work additionally correlations between multiple sensors can be handled in the same way as correlations within the same sensor because data from one satellite sensor only has been used in this work these correlations were also left out for further study additional future developments of the system include the use of co variate information such as water temperature to improve the accuracy of the results and automatic estimation of model parameters from available data by the system if there is a risk for deterioration of ecological status of coastal area precision of the data fusion results is focal for the monitoring and management of water quality in accordance with the eu water framework directive eu 2000 moreover environmental permit of a polluter may be rejected based on the precautionary principle of eu environmental law kriebel et al 2001 on this account monitoring programs need to be optimized improved and extended to reduce error variances confidence limits and the risk of non compliance with ecological standards the data fusion system makes it possible to take the full advantage of the available data sources to get more complete estimate of the water quality and ecological status one possible application is ecological classification and management of coastal and inland waters according to the wfd it can also be used for environmental monitoring and permitting of fish farms or any other polluter and to assess the impacts of these activities more precisely than by conventional methods software availability software name endas year of first release 2019 operating systems windows linux mac programming languages python availability https github com martingu11 endas license mit documentation https endas readthedocs io en latest declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported financally by the strategic research council of the academy of finland blueadapt project grant number 312650 the finnish ministry of the environment and the earth and water engineering support association maa ja vesitekniikan tuki ry part of the work was also funded by the adafume project of the academy of finland project number 321890 auxiliary material see appendices a c appendix a data fusion algorithms a 1 state space representation here we formally define the data assimilation system let x k be a stochastic process over a set of time steps k 0 k k which represents the dynamical system of interest and the state at time step k is denoted as x k the process is assumed to have the markov property so that future system states only depend on the history thru the current state i e the future is independent of the past given the present we further assume that x k is unobservable i e its state is hidden but another process y k exists that is observed and is dependent on x k in some known way in other words x k is a hidden markov chain the time step k 0 corresponds to the initial system state and we will further assume that observations are available at one or more time steps k 1 k k the states of x k are numerically represented by a sequence of n dimensional vectors of real numbers x k r n similarly observations are represented by r k dimensional vectors y k r r k where r k is the number of observations at time step k given a mathematical model of the process m r n r n the evolution of the system can be described by state space equations 3 x k m k x k 1 θ η k y k h k x k θ ϵ k the model m k is often called the evolution or dynamic model and expresses the relationship between consecutive system states by means of a model prediction h k is called the observation model or observation operator and is a function h k r n r r k that maps the state x k to observations y k vector θ contains auxiliary model parameters which typically do not depend on the time k finally η k and ϵ k are additive terms that account for the model and observation errors respectively both are assumed to be random independent from each other and independent in time it should however be noted that although universally accepted the assumption of independence is rarely fully true in reality data collected by the same instrument over a period time for example may have correlated errors on subsequent measurements the final step in completing the data assimilation scheme is to combine the evolution model with observations in a probabilistic formulation we aim to estimate the posterior distribution of the state space conditioned on collected observations p x 0 k y 1 q where k q k here we use the notation x a b to denote the sequence of state vectors x a b x a x a 1 x b and y a b to denote the sequence of observation vectors defined in an identical fashion depending on the choice of q the following data assimilation schemes can be distinguished p x 0 k y 1 k we wish to estimate system states up to and including the current step k using all observations collected so far this is an instance of so called bayesian filtering p x 0 k y 1 k l for l 0 we wish to estimate system states up to and including the current step k using observations up to and including a future time step k l the parameter l is called the lag and the scheme is referred to as fixed lag bayesian smoothing p x 0 k y 1 k we wish to estimate system states over the entire data assimilation window using all available observations this is called fixed interval bayesian smoothing for the data fusion applications and dynamical spatio temporal data analysis the main tool is the smoother as it allows to combine all available information both in time and space the joint probability densities mentioned above are usually not practical and the computationally simpler marginal distributions p x k y 1 q are used instead the filtering posterior distribution p x k y 1 k can be obtained by applying bayes rule as 4 p x k y 1 k p y 1 k x k p x k y 1 k 1 where p x k y 1 k 1 is the prior distribution and describes the probability of x k before the evidence in y k is considered p y 1 k x k is the observation likelihood conditioned on x k and therefore contains new information present in observations when contrasted with the current state estimate this is also sometimes called the innovation from eq 4 we can see that the filtering solution can be obtained as soon as an observation becomes available allowing on line data assimilation the assumption of uncorrelated errors and the markovian property of x k allows the posterior smoothing distribution to be calculated as a product 5 p x 0 k y 1 k p x 0 k 1 k p y k x k p x k x k 1 where p x 0 is the distribution assigned to the initial state as with the filtering solution the smoothing solution for a given time step k p x k y 1 k can be obtained by marginalization of 5 this is often desired because it avoids re computation of the entire joint posterior distribution of x 0 k every time new observations are obtained furthermore the fixed lag scheme relies on the marginalized solutions to update state estimates for the last l steps so that the resulting data assimilation can still be considered on line but with a fixed delay a 2 kalman filter and smoother the large dimension of state spaces encountered in geosciences usually prevents direct manipulation of the probability density functions in eqs 4 and 5 and we resort to approximations the kalman filter kf kalman 1960 is a popular and often used approach that efficiently solves the filtering and smoothing problems for linear dynamical systems the prior and posterior distributions are approximated by gaussians which can be fully described by the first two statistical moments the mean and covariance the model and observation errors in eq 3 are thus assumed to be normally distributed 6 η k n 0 q k and ϵ k n 0 r k where q k and r k are the model and observation error covariance matrices respectively the posterior density given by eq 5 then becomes a product of gaussians and is therefore also gaussian the marginal filtering and smoothing posterior distributions at a time step k p x k y 1 k and p x k y 1 k are then characterized by covariance matrices p k 1 k and p k 1 k respectively the subscript notation k 1 q is analogous to the notation used in eqs 4 and 5 and denotes the error covariance at time k so that information from observations up to and including the time step q has been incorporated kalman filter is a sequential filtering algorithm for state estimation that consists of two alternating steps first the state vector x k 1 and the corresponding error covariance matrix p k 1 1 k 1 is propagated from time step k 1 to k by application of the dynamic model this is called the forecast step the forecast step is followed by assimilation of observations called the update or analysis step when observations are introduced the state estimate is corrected and the posterior covariance is reduced for actual kalman filter equations we refer the reader to existing literature such as evensen 2009 or asch et al 2016 kalman smoother is a direct extension of the kalman filter and several formulations exist the most direct approach is to augment the state vector of the filter at any given time step with state variables from previous time steps effectively implementing a fixed lag kalman smoother alternatively fixed lag kalman smoother can be formulated without the need to increase the size of the state vectors through retrospective updates cohn et al 1995 in this approach the state vectors x j at k l j k are updated after the assimilation of observations into x k at time t k another well known approach is to first compute the kalman filter solutions for all k 1 k and then proceed with a backward updating pass for k k 1 0 in which information from observations used to correct state estimates at earlier time steps the forward backward technique is known as the rauch tung striebel smoother rauch et al 1965 its advantage is that each state vector is only updated twice and a fixed interval solution over the entire window k 0 k is obtained however the potentially very large covariance matrices p k 1 1 k 1 must be stored for all time steps during the filtering pass this may be prohibitively expensive in terms of storage even if the matrices are written to disk storage until they are needed again kalman filter and smoother can be shown to be an optimal unbiased estimator of the posterior distributions if the system is linear and errors are gaussian real processes are however seldom linear and non linear generalizations such as the extended kalman filter ekf and unscented kalman filter ukf julier et al 1995 have been developed ekf relies on linearization of the dynamic and observation models m k x and h k x through first order taylor series expansion around x ukf on the other hand aims to directly approximate the posterior distribution rather than approximating the process and observation models the approximation is computed purely through evaluations of m k x and h k x model linearizations are therefore not necessary creation of the linearized models and their adjoints required by ekf is a formidable task for complex models even with the help of automatic code differentiation tools another more fundamental drawback is the need to manipulate full rank covariance matrices or sigma vectors in ukf during the forecast and update steps this makes kalman filters including ekf and ukf unpractical for larger spatial domains such as those often encountered in geosciences where the size of the state vector may be n 1 0 5 this has lead to the development of reduced methods for data assimilation a 3 ensemble formulation to overcome the large computational requirements of kalman filters an alternative monte carlo approach called ensemble kalman filter enkf has been proposed by evensen 1994 instead of a single sequence system states x k and covariances p for k 0 1 k enkf uses a collection or ensemble of states to approximate the prior and posterior distributions the size of the ensemble is typically much smaller than the size of the state space allowing ensemble kalman filters to be used on very large systems let the ensemble be defined by an n n matrix 7 e k x k 1 x k 2 x k n where n is the size of the ensemble and x 1 x n are the individual state vectors we can then define a matrix of ensemble perturbations 8 e k 1 n 1 e k e k where e k e k 1 n is a matrix holding the ensemble mean in each column and 1 n is n n matrix with each coefficient equal to 1 n the normalization factor 1 n 1 is chosen so that the full rank error covariance p k is replaced by sample covariance p ˆ k e k e k t it should be noted that just like p k p ˆ k is a n n matrix and can therefore not be included in the filtering and smoothing equations explicitly instead the product e k e k t is used directly and equations are rearranged so that the full matrix does not need to be computed the use of a finite and usually small m n ensemble to represent the covariance matrix p comes with some drawbacks the small sample size introduces errors into the estimated covariance leading to non zero correlations between state variables that in reality are physically unrelated because of these additional spurious correlations state variables that would normally be unaffected are corrected during the update step and their uncertainty therefore decreases as a result the state error covariance may become strongly underestimated over time causing the filter to become overconfident about the model predictions observations become irrelevant and the filter generally diverges from the real system state filter divergence due to the underestimation of error covariance is a problem common to all ensemble kalman filters and approaches such as covariance inflation and localization have been developed to circumvent these issues it is thanks to these practical fixes that ensemble kalman filters have become very popular and successful tools for data assimilation covariance inflation and localization techniques implemented in dfs are described further in appendix b 3 a 4 logarithmic scale in the state space model presented in section 1 2 the terms η k and ϵ k represent absolute model and observation errors in many situations however it is desirable to model the errors as relative and hence proportional to the measured or estimated value a convenient way to achieve this is to perform data fusion in a transformed space 9 z k log x k and assimilate observations of log y k instead consequently z is normally distributed as z n μ z σ z 2 and x becomes log normally distributed according to x log n μ x σ x 2 with the transformed mean and variance given by 10 μ z log μ x 2 μ x 2 σ x 2 σ z 2 log 1 σ x 2 μ x 2 in the equations above the time indices k have been dropped to simplify the notation given the transformation above the final point estimate x k can be obtained as 11 x k μ x exp μ z σ z 2 2 appendix b implementation b 1 implemented kalman smoothers because not all model grids need to be very large dfs implements both the exact full rank kalman smoother and an ensemble based kalman smoother the full rank kalman smoother is presented in appendix a 2 and will not be described further the ensemble kalman smoother variant implemented in dfs is the error subspace transform kalman filter estkf proposed in nerger et al 2012 and its smoother extension estks nerger et al 2014 estkf belongs to the square root family of kalman filters that does not rely on random perturbations in the analysis step unlike traditional enkf ensemble transform kalman filters perform all algebra in a smaller space of the perturbations spanned by th ensemble members the update scheme of estkf is very similar to that of the popular ensemble transform kalman filter bishop et al 2001 but at a slightly lower computational cost b 2 sampling strategy for enks the monte carlo approach of enks relies on random perturbations a r n n drawn from n 0 q k to implement the error term η k one approach to generate the perturbations is to compute the square root of q k via cholesky decomposition and multiply it by a vector of independent random samples drawn from n 0 i this method is exact but computationally infeasible for large model grids due to the high computational cost of the cholesky factorization more efficient approach is to perform the sampling in fourier space using the circulant embedding method wood and chan 1994 which is implemented in dfs it should be noted that the circulant embedding method requires the sampled grid to be regular and dense samples are therefore generated also for cells that are outside of the model boundary and not included in the model grid these samples are currently discarded which is inefficient if the model grid is very sparse to remedy this the block variant of the circulant embedding method park and tretyakov 2015 could be used to minimize the amount of discarded samples because the size of the ensemble is typically much smaller than the size of the state space effort has to be made to avoid unnecessary noise in the ensemble which can lead to unwanted correlations between state variables the common technique for reducing the ensemble sampling noise is covariance localization described in appendix a 3 in dfs however the model error perturbations can also be a major source of noise because they are likely to dominate the system evolution given the simplistic evolution model the easiest way to improve the quality of the ensemble is to increase the its size however doing so leads to increased storage and computational cost of the data fusion algorithm making large ensembles unpractical better approach is to keep the size of the ensemble fixed and aim to maximize its rank instead one simple approach based on the idea introduced in pham 2001 can be implemented as follows we first generate a larger augmented ensemble of perturbations a ˆ r n ω n sampled from n 0 q k using the circulant embedding method the size of the ensemble is ω n with ω 1 being the oversampling ratio we compute the singular value decomposition a ˆ u σ v t and obtain the final ensemble a by sampling along the n largest eigenvalues a u σ 1 n θ t here σ 1 n contains the first n eigenvalues of σ multiplied by n ω n and θ r n n is a random orthogonal matrix it should be noted that the augmented ensemble a ˆ and the singular value decomposition only need to be computed once after that only the random matrix θ needs to be computed to obtain the new sample a the effect of the improved sampling scheme can be evaluated by looking at the singular values of the generated samples as compared to those of the original sample fig 9 demonstrates the improvement in the conditioning of the ensemble by means of the ratio between the largest and smallest singular value for a hypothetical model grid of size 300 300 cells and the spherical covariance function with a range of 30 cells in this example the oversampling ratio ω 4 provides a good compromise between improvement in quality and computational cost beyond ω 6 the improvement is minimal b 3 covariance inflation and localization covariance inflation and localization are two techniques that are often used to stabilize ensemble kalman filters i e to avoid filter divergence due to overconfidence in the filter s performance the purpose of covariance inflation is to account for underestimation of the true error covariance p by p ˆ due to the limited size of the ensemble while the forced inflation of the covariance does not have strong foundation in theory it is nevertheless practical and easy to implement to inflate the covariance p ˆ matrix of ensemble anomalies e from eq 8 is multiplied by a factor slightly larger than one the factor may be a constant scalar value or may vary in time and space because of the ad hoc nature of covariance inflation the inflation is typically tuned for specific data assimilation problems by means of trial and error however methods that aim to estimate optimal inflation factors have also been proposed in raanes et al 2019 miyoshi 2011 and evensen 2009 to name a few the approach suggested by evensen 2009 section 15 3 applies enkf update equations directly to an auxiliary state to estimate the inflation factor it has been tested with dfs but has not yielded consistent improvements and dfs therefore uses a constant inflation factor with a default value 1 02 moreover the use of covariance localization typically reduces the need for inflation and its extensive tuning covariance localization aims to improve the estimated covariance p ˆ by suppressing unwanted correlations i e those that are assumed to be the product of the sampling error in enks variants that operate in the state space the localization effect can be achieved by regularizing the implicit covariance matrix p ˆ through manipulation of the ensemble anomalies however estks operates in the ensemble subspace and the direct regularization approach is not feasible instead localized analysis is achieved by dividing the state vector into subsets called local analysis domains assimilation of observations is then carried for each local analysis domain separately utilizing only observations within certain distance h m a x from the domain the distance of an observation from the local domain can also be used to reduce its influence so that far away observations contribute less to the local analysis solution to do that the observation uncertainty σ r 2 from section 2 4 is additionally tapered by the covariance function c h and becomes 12 σ r i 2 σ r i 2 c h i c 0 1 where h i is the distance of the i th observation from the local domain and i runs over all observations in the local domain the cutting distance h m a x is chosen so that c h m a x c 0 is sufficiently small at minimum each local domain may comprise of a single grid cell the domains may however be larger for performance reasons and dfs uses square blocks of r r grid cells for the local analysis where r is a tuning parameter that balances execution speed and quality the default block size is r 3 and local domains are also enlarged padded so that adjacent domains partly overlap the overlap is used to smoothly blend ensembles from local domains back into the global ensemble to eliminate visible boundaries the principle of local analysis in dfs is shown in fig 10 b 4 generation of initial ensemble the data fusion system can estimate the initial state the state vector mean and the covariance matrix from available observations a commonly approach used in data assimilation is to sample the initial ensemble from a long model run while maximizing the rank of the ensemble pham 2001 this approach is however not currently feasible in dfs due to the trivial nature of the evolution model therefore dfs attempts to estimate the initial state mean and variance from available observations by collecting available observations over a short interval t t n t n where n min l 3 3 and l is the smoother lag or temporal auto correlation length this choice is made so that only observations that are believed to be reasonably close to the initial state but at minimum 3 days are used the initial state vector is then set to the arithmetic mean of the collected observation values per model variable because the observational data is likely to be distributed unevenly in space the background error is first calculated as the 95 percentile of relative observation standard deviation of data points for each day in t the daily errors are then averaged to yield the final initial state error standard deviation σ x 0 finally the covariance matrix p 0 needed for ekf is constructed analogously to the construction of the model error covariance matrix q described in section 2 4 appendix c observation error sensitivity to assess the sensitivity of the data fusion estimates to the uncertainty in the observed data the observation error has been artificially adjusted and additional data fusion simulations were run over the period of 1 31 july 2011 the mean chl a concentration and the corresponding error of the data fusion runs has been recorded the sensitivity runs were made with 18 and 58 relative error for the satellite eo data and 10 and 30 relative error for the alg line data in addition to the default observation errors used 38 for eo and 20 for alg line data the mean chl a concentration and the corresponding mean errors for points 1 4 see fig 7 for locations of the points are shown in table 5 the estimated chl a time series are also shown in fig 11 for the varying eo observation error and fig 12 for the varying alg line error the effect of varying observation errors on data fusion uncertainty is rather minor for both the satellite and alg line observations the reason for this is the high number of observations available in both data sets that are in the vicinity of analysed points due to the assumption of error independence the observation errors cancel each other out leading to a seemingly precise estimate the errors between chl a derived from individual satellite pixels and obtained from subsequent fluorometer samples are however likely to be correlated 
25577,this study investigates how assimilation of surface soil moisture jointly retrieved by multiple microwave satellites affects flood simulation and forecasting based on the experiments of simulation sim open loop ol and ensemble kalman filter enkf in small and medium sized watersheds without gauged soil moisture we developed a framework for data assimilation da of satellite soil moisture into the wrf hydro model based on the enkf algorithm three statistical metrics to evaluate the impacts of da including net error reduction normalized error reduction and effectiveness criterion are all positive values 6 0 indicating that da gains reduced errors meanwhile the deterministic coefficients of the enkf experiment are also greater than those of the ol experiment it is obvious that multi satellite retrieved soil moisture and da technology can improve the accuracy of flood simulation and forecasting in ungauged regions and play an important and positive role in hydrological forecasting keywords multi satellite microwave remote sensing data assimilation ensemble kalman filter wrf hydro model ungauged watershed flood simulation and forecasting 1 introduction data assimilation da was originated from numerical weather prediction nwp in the mid 20th century and has gradually attracted attention in many fields since the 1980s christian 2005 da has been applied in the modeling of atmospheric oceanic land surface and hydrological processes in the 21st century zaitchik et al 2010 currently the most representative da systems include the north american land data assimilation system luo et al 2003 global land data assimilation system gldas rodell et al 2004 the european land data assimilation system jacobs et al 2008 and the china land data assimilation system shi et al 2011 the wide application of da in different fields has resulted in varied definitions of da abdolghafoorian and farhadi 2016 charney et al 1969 first used a simulation method to study da thus some researchers believe that the study of da begins with simulation research rutherford and asselin 2010 studied the dependence of the assimilated wind field on latitude and scale and found that only the largest scale movement of the wind field had an acceptable accuracy in the middle and high latitudes da can integrate model simulation results and direct and indirect observed data with different sources and resolutions into a dataset with temporal spatial and physical consistencies xin et al 2007 robinson and lermusiaux 2000 defined da as a general method for estimating ocean variables which combined observed data with oceanographic forecast models to provide accurate variable estimates however talagrand and courtier 2010 suggested that the definition of da is using all existing information to estimate the most accurate possible atmospheric or ocean motion state da has evolved through empirical mathematical simple intelligent and intelligent algorithms stages since 1969 cressman 1959 hoke and anthes 1976 liang and qin 2008 panofsky 1949 the original da algorithm was used to correct the weather forecast results based on observed data in the 1950s and 1960s cressman 1959 panofsky 1949 this is called the empirical algorithm stage and it lacks a theoretical basis including an objective analysis method a correction method based on a stepwise polynomial fitting method and relaxation approximation sasaki 1958 the statistical methods were introduced into the da field based on the classically optimal interpolation method leading to the stage of the mathematical algorithm with a theoretical foundation in the 1970s and 1980s gauntlett and seaman 1974 furthermore continuous da algorithms based on the optimal control theory and sequential da based on the statistics theory were known as the simple intelligent algorithms that were developed after the 1980s houtekamer and mitchell 2001 stern and ploshay 1992 including the kalman filter ensemble kalman filter enkf extended kalman filter ensemble adjustment kalman filter particle filter three dimensional variational da and four dimensional variational da carrassi et al 2018 crow and wood 2003 lin et al 2016 zhao et al 2016 with the development of artificial neural networks and coupling of multiple da algorithms the evolution of continuous da entered the intelligent algorithm stage in the 21st century cintra and de campos velho 2018 dobricic and pinardi 2008 fang and li 2019 van delft et al 2009 currently da algorithms are applied in many studies especially hydrological modeling and flood analysis main hydrological variables that da deals with include runoff evaporation soil moisture temperature snow cover and snow water equivalent runoff da is usually used to correct the output of hydrological models to directly improve the accuracy of flood simulation and forecasting therefore it is widely used in hydrological forecasting liu et al 2016a 2016b soil moisture is a key variable for rainfall runoff process which directly reflects the hydrological state it is highly related to infiltration process and further regulates runoff generation and confluence within a watershed so it can directly affect the accuracy of flood simulation and forecasting chao et al 2019 da algorithm can effectively combine the simulated soil moisture of hydrological models and observed soil moisture to continuously update state variables and improve the accuracy of flood simulation and forecasting within a watershed or a region han et al 2012 meng et al 2017 firstly da of soil moisture as a technique or method can introduce observed data into hydrological models secondly da of soil moisture can chronologically improve the accuracy of soil moisture simulation by the hydrological model thereby improving the accuracy of flood forecasting therefore data assimilation of soil moisture is necessary as a bridge between models and observed data da of soil moisture in hydrological models generally has two main purposes 1 to study soil moisture assimilation methods by assimilating gauged data or satellite remote sensing of soil moisture chen et al 2015 and 2 to continuously update the soil moisture status of the watershed and improve the accuracy of flood simulation and forecasting chen et al 2011 laiolo et al 2016 assimilation of soil moisture with ground station monitoring was carried out on a point scale in the early stage jackson 1980 assimilated surface soil moisture using observed soil moisture which greatly improved the accuracy of soil moisture estimation for different soil profiles to improve the accuracy of soil moisture estimation heathman et al 2003 assimilated observed soil moisture into models chen et al 2011 used the enkf algorithm to assimilate surface soil moisture into swat model which significantly improved the accuracy of soil moisture evaporation and streamflow predictions the development of remote sensing technology makes it possible to obtain continuous soil moisture in space da of remotely sensed soil moisture has become a research hotspot since the 1990s reichle 2008 reichle et al 2007 for example crow et al 2005 assimilated satellite soil moisture into the antecedent precipitation index model and demonstrated that the rainfall runoff predictive ability of the api model was improved in 26 basins of the united states southern great plains lievens et al 2015 assimilated smos soil moisture products with a spatial resolution of 25 km into the vic model which improved the spatial resolution of soil moisture and the accuracy of hydrological simulation in the murray darling basin four different satellite derived soil moisture datasets were updated to a distributed and continuous hydrological model by carrying out an assimilation scheme in a small catchment in italy and the results showed that the nash sutcliffe model efficiency coefficient increased from 0 6 without assimilation to 0 7 assimilated which proved that discharge prediction of a distributed hydrological model at a fine scale resolution was improved by assimilating remotely sensed soil moisture laiolo et al 2016 assimilating smap and smos soil moisture datasets into a distributed hydrological model improved streamflow predictions of 131 usgs gauge observations from 2015 to 2018 jadidoleslam et al 2021 although many studies have proven that remotely sensed soil moisture assimilation can improve the accuracy of flood simulation and forecasting the assimilation effect is limited when there is a large deviation between simulated discharges and observed data alvarez garreton et al 2015 an ensemble based on the da method was used to explore the benefit of jointly assimilated remotely sensed smap soil moisture to improve the accuracy and reliability of wrf hydro model predictions and the results showed that the multivariate assimilation of soil moisture results in improved prediction of streamflow as compared to univariate assimilation configurations and regardless of the watershed s streamflow regime abbaszadeh et al 2020 meng et al 2017 developed an effective da scheme considering a time lag between soil moisture and streamflow and the rmse of flood simulation was reduced more than 70 the benefit of soil moisture assimilation is reduced as uncertainty in discharge forecasts li et al 2014 the other studies also assimilated surface soil moisture into the hydrological model which demonstrates the improvement of the flood simulation and forecasting chen et al 2013 li et al 2014 massari et al 2018 brocca et al 2013 compared the assimilation effect of different satellite soil moisture products in four different catchments worldwide but their results demonstrated that the assimilation of remotely sensed soil moisture has no impact on flood simulation and forecasting in mountainous areas considering that there are many existing satellite soil moisture products with varying differential uncertainty in space and time only assimilating a single satellite product is usually unreliable in this study we aim to improve the accuracy of flood simulation and forecasting by assimilating surface soil moisture jointly retrieved by multiple microwave satellites smsatellites into the wrf hydro model based on the enkf algorithm in ungauged basins in particular we chose the daheba watershed as a case study area which is a first order sub basin of the hanjiang watershed in the qinling mountains and a typically small and medium sized watershed 3000 km2 without observed soil moisture the soil moisture dataset jointly retrieved by multiple satellites and modeled soil moisture dataset were evaluated in the hanjiang watershed which is a first order sub basin of the yangtze river basin we also analyzed the assimilation effect on each flood event and different ranges of discharges by designing the simulation sim open loop ol and enkf experiments table 1 2 study area and data 2 1 study area the daheba watershed is selected as a case study area for da which has no observed soil moisture data fig 1 a more details on described the daheba watershed are referred to chao et al 2021 the terrain elevation in this watershed decreases from north to south with steep brooks and steams favoring the formation of floods fig 1a due to the barrier effect of the qinling mountains humid air from the southern and southeastern areas brings a large amount of water vapor and the watershed is prone to frequent flash floods caused by heavy rainfall especially in summer summer rainfall accounts for 46 of annual total precipitation and is characterized by short duration and high intensity resulting in flash floods average annual precipitation and runoff of this watershed are about 900 mm and 400 mm respectively because the daheba watershed has no observed soil moisture we had to choose a larger area that contains the daheba watershed and has observed soil moisture to verify the surface soil moisture jointly retrieved by multiple microwave satellites the hanjiang watershed is the parent watershed of the daheba watershed and has nine stations to measure soil moisture the hanjiang watershed was chosen to verify soil moisture in this study we evaluated the quality of smsatellites in the hanjiang watershed which is a major tributary of the yangtze river basin and covers a total area of 61 662 km2 fig 1b it originates from the fanzhong mountain which belongs to the qinling mountains in ningqiang county of hanzhong city shaanxi province and mainly flows through shaanxi province and hubei province contributed by the subtropical monsoon climate and the qinling mountains rainstorms and floods in the hanjiang watershed are highly related to monsoon activities runoff of the hanjiang watershed is unevenly distributed throughout the year from may to june floods can be formed due to more frequent precipitation extremes under the influence of the southwest monsoon southwest monsoon prevails in july leading to rainstorm triggered floods in the upper reaches of the hanjiang watershed southeast monsoon activity is the most active in august which controls the upper reaches of the hanjiang watershed and can cause severe floods strong cold air from the north can also cause large floods in september therefore floods are mainly concentrated from may to september in the hanjiang watershed the runoff of the mainstream in summer and autumn accounts for 37 40 of annual runoff while spring runoff accounts for 16 6 17 5 of annual total and winter runoff only accounts for 5 6 7 2 2 data 2 2 1 geographic data geographic data and maps include elevation flow direction flow accumulation and river network which were provided by shuttle elevation derivatives at multiple scales hydrosheds database http www hydrosheds org with a spatial resolution of 1 km fig 1 land use data of the igbp modified modis 20 land use categories and soil type data used in this study were obtained from the wrf preprocessing system wps static data https dtcenter org wrf nmm users docs user guide v3 users guide nmm chap3 pdf the vegetation fraction was derived from the 3rd generation normalized difference vegetation index ndvi from the level 3 l3 monthly 1 km global vegetation indices mod13a3 provided by the national aeronautics and space administration nasa https e4ftl01 cr usgs gov molt mod13a3 006 2 2 2 gauge data gauge data include precipitation soil moisture and discharge the discharge data with a temporal resolution of 1 h were collected from the daheba hydrological station during 2013 2014 fig 1a three flood events 2013071500 2013072200 and 2014090700 were chose to test da from 2013 to 2014 which were exceeded the warning discharge with a value of 1000 m3 s each flood event number is represented by 10 digits including year month day and hour fifteen rain gauge stations were used to collect precipitation data with a temporal resolution of 1 h in the daheba watershed fig 1a there were only five rain gauge stations before 2011 and the cofferdam has been established downstream of the daheba watershed in 2015 which is 3 km away from the daheba hydrological station the observed discharge data from 2013 to 2014 were used to verify the results of soil moisture da for this study because this period has low or no human activities the wrf hydro model input data were obtained from the gauge observation network cmorph and gldas with spatiotemporal resolutions of 1 h and 1 km the spatiotemporal resolutions of the wrf hydro model outputs were set to be the same as the input data soil moisture in the study area is difficult to collect most soil moisture observation stations aim to monitor agricultural droughts the daheba watershed belongs to a humid area located in the deep and high rocky hills of the qinling mountains there is no soil moisture gauge station which hindered us from finding a benchmark to verify the satellite soil moisture smsatellites and modeled soil moisture in the daheba watershed therefore we extended the verification area to the entire hanjiang watershed due to the lack of the observed soil moisture within the daheba watershed fig 1b the observed soil moisture data from 2013 to 2014 in nine stations were provided by the shaanxi province hydrology and water resources investigation bureau the temporal resolution of the observed soil moisture is ten days specifically there are only measurements on the 1st 11th and 21st days of each month therefore there are a total of 72 observed data points at each station from 2013 to 2014 2 2 3 meteorological data meteorological data used as the input of the wrf hydro model include precipitation air temperature surface pressure specific humidity incoming shortwave radiation incoming longwave radiation and the u and v components of near surface wind as forcing data precipitation was merged from satellite precipitation and gauged precipitation using the mixed geographically weighted regression and bi square mgwr bi algorithm chao et al 2018 satellite precipitation data were obtained from the cmorph product with a spatial resolution of 8 km and a temporal resolution of 0 5 h then the original cmorph product of 0 5 h was accumulated to 1 h and the 8 km cmorph precipitation product was downscaled to 1 km precipitation data comrphd using the bilinear interpolation method the downscaled cmorph and gauge precipitation data with a temporal resolution of 1 h were merged using the mgwr bi algorithm other input meteorological data were obtained from the l4 gldas noah land surface model with 3 hourly and 0 25 0 25 resolutions https disc gsfc nasa gov datasets keywords gldas the gldas data can provide meteorological data at a global scale but the resolution is not sufficient for hydrological simulation in the daheba watershed in other words the daheba watershed covers only 3 4 gldas grid cells and the hanjiang watershed covers 11 19 gldas grid cells therefore we used linear and bilinear interpolation methods to disaggregate the spatiotemporal resolution of 3 h and 25 km into 1 h and 1 km respectively 2 2 4 remotely sensed soil moisture the soil moisture data used for da in this study were obtained by using the surface soil moisture jointly retrieved by multiple microwave satellites including the smap smos amsr2 and fengyun satellites smsatellites with a spatial resolution of 25 km and a temporal resolution of 1 day zhang et al 2019a it is a kind of composite soil moisture for near real time application with better coverage and higher accuracy across china zhang et al 2019a first we applied the single channel brightness radiometric algorithm to retrieve soil moisture from the respective brightness temperature observations of the fy3c fy3b smos amsr2 and smap satellites on the same day second a daily composite dataset was produced by averaging the individual satellite retrieved soil moisture data the resultant averaged soil moisture was used for da and the spatial resolution was disaggregated from 25 km to 1 km the retrieval algorithm includes four steps zhang et al 2019a first the brightness temperature is converted to emissivity using the physical temperature of the emitting layer through the single channel brightness algorithm based on the classical tau omega model second the influence of vegetation and surface roughness was eliminated since emissivity is impacted by vegetation and surface roughness above the soil surface the soil surface emissivity can be solved as a function of emissivity the soil surface emissivity is the value determined from the vegetation correction which is related to the single scattering albedo and the one way transmissivity of the canopy depending on the vegetation structure polarization and frequency the relationship between the soil surface emissivity and the surface roughness parameter depending on the polarization frequency and geometric properties of the soil surface is established which is used to eliminate the influence of roughness third the mineralogy based soil dielectric model mironov et al 2009 was used for data processing in radar and radio thermal remote sensing to establish the relationship between the dielectric constant and soil moisture finally the retrieval of surface soil moisture was inferred based on this relationship the bilinear interpolation method was employed to disaggregate the 25 km satellite soil moisture dataset into the 1 km resolution the final smsatellites dataset with a spatial resolution of 1 km and a temporal spatial resolution of 1 day was used for da from 2013 to 2014 in this study 3 methodology to assimilate smsatellites data in ungauged watersheds the enkf algorithm was used for conducting da for the wrf hydro model to realize the smsatellites assimilation the general framework applied in the daheba watershed is shown in fig 2 the general idea of enkf is that ensemble forecasting is used to solve the problem of error covariance estimation and forecasting difficulties in practical applications evensen 1994 it incorporates a monte carlo method to generate a state ensemble with a certain distribution to represent its probability based on the theory of the linear kalman filter kalman 1960 enkf has become a mainstream algorithm for da and been widely used in da of the atmosphere ocean land surface and earth systems because it can be used for the assimilation of nonlinear systems evensen 2003 2004 the enkf algorithm contains two steps forecasting and updating forecasting is the transition process of hydrological model state variables and updating is the update process of the hydrological model state 3 1 bias correction method of remotely sensed soil moisture the most basic criterion for hydrological models is the water balance in the watershed or study region to ensure the water balance in the watershed bias correction of the smsatellites data was carried out before assimilation bias correction methods mainly include first order second order and cumulative distribution function methods liu et al 2012 the first order correction method assumes that the average values of the two samples are the same specifically when the averaged values of the smsatellites data are equal to the modeled soil moisture in a certain time series then the smsatellites data is corrected to ensure the credibility of the fit between the two soil moisture datasets a long term series of samples was required we chose a simple and convenient first order correction method the equation can be written as 1 s m i o b s s m i s i m s m i o b s s m i o b s where s m i o b s is the corrected smsatellites data s m i o b s is the smsatellites data of grid cell i s m i o b s is the average value of the smsatellites data for one flood event s m i s i m is the average value of the modeled soil moisture for one flood event 3 2 model description the wrf hydro modeling framework i e the wrf hydro model is the hydrological extension of the noah and noah mp land surface model arnault et al 2016 yucel et al 2015 which can be run standalone or coupled with the wrf atmospheric model lin et al 2015 senatore et al 2015 the wrf hydro model simulates water and energy fluxes at high spatiotemporal resolutions based on physical and conceptual approaches ma et al 2021 it includes the noah and noah mp land surface models to simulate overland flow saturated subsurface flow channel routing and baseflow gochis et al 2018 the input data for the wrf hydro model include meteorological data precipitation air temperature surface pressure specific humidity incoming shortwave radiation incoming longwave radiation and the u and v components of near surface wind geographic data dem flow direction flow accumulation and river network soil type land cover types and vegetation fraction chao et al 2021 the output of water fluxes includes soil moisture evapotranspiration deep soil drainage surface runoff snow depth channel flow depth channel flow etc the output of energy fluxes consists of surface latent heat flux surface sensible heat flux and ground heat flux the main parameters of the wrf hydro model are listed in table 2 and their values are mainly based on their respective physical meanings sun et al 2020 3 3 assimilation scheme based on the principles of da three comparative experiments for the smsatellites assimilation were designed in this study the three experiments are the sim ol and enkf experiments the sim experiment is the model simulation with given model inputs and a warming up period of ten days which results in a single model realization in contrast the ol experiment is an ensemble of model simulations realizations that have the same inputs and warming up period as the sim experiment but have perturbed values for the key model parameters listed in table 2 based on the gaussian distribution assumption in the enkf experiment we conducted the enkf da of the smsatellites data at 12 00 of each day during the simulation period for every ensemble member of the ol experiment therefore differences between the ol and enkf experiments lies in whether the soil moisture of the wrf hydro model is updated by da the specific steps of the smsatellites assimilation in the enkf experiment include correcting the bias in the smsatellites wrf hydro model simulation and updating modeled soil moisture the state variables are functions of the wrf hydro model in the forecasting or simulation process the function can be written as 2 s m i t 1 s i m m o d e l w r f h y d r o s m i t a n a where m o d e l w r f h y d r o is operator of the wrf hydro model s m i t s i m is modeled soil moisture of grid cell i at t 1 time step s m i t a n a is update value of soil moisture through the enkf algorithm at t time step namely the analytical value of state variable the analytical value of soil moisture is defined as 3 s m i t a n a s m i t s i m k i t s m i t o b s h s m i t s i m 4 k i t p i t f h t h p i t f r i t 1 5 p i t f 1 n 1 k 1 n s m k t s i m s m t o b s s m k t s i m s m t o b s t 6 r i t 1 n 1 k 1 n s m k t o b s s m t o b s s m k t o b s s m t o b s t where h is the operator of observation k i t is the matrix of enkf gain at t time step p i t f is the error covariance of the modeled soil moisture at t time step r i t is the error covariance of the smsatellites at t time step n is the number of ensembles 3 4 evaluation metrics of assimilated results to evaluate the wrf hydro model performance three evaluation metrics were selected to evaluate the simulated discharges in the sim ol and enkf experiments table 1 including the absolute relative error of flood peak dp relative error of flood volume dv and deterministic coefficient dc chao et al 2019 2021 dp is defined as the percentage of the absolute difference between the observed and simulated flood peaks to the observed flood peak dv describes the accuracy of the modeled total flood water volume dc measures the fit between the observed and simulated discharges for one flood event to assess the performance of the smsatellites assimilation three statistical metrics table 1 were used net error reduction net normalized error reduction ner and an effectiveness criterion nef based on the root mean square error rmse chen et al 2013 net refers to the difference between the rmse of the ol experiment discharges and that of the enkf experiment discharges it quantifies how much the smsatellites assimilation can reduce rmse in the ol experiment 7 n e t r m s e o l r m s e e n k f where r m s e e n k f is the rmse value of the simulated discharge by the enkf experiment r m s e o l is the rmse value of the simulated discharge by the ol experiment ner represents the relative value of the decreased rmse caused by the enkf da relative to the ol results 8 n e r r m s e o l r m s e e n k f r m s e o l 100 nef evaluates the effect of da on reducing errors by comparing the sums of error squares between the observed and simulated discharges of the ol and enkf experiments 9 n e f 1 i 1 n q e n k f i q o i 2 i 1 n q o l i q o i 2 100 where q e n k f i is the simulated discharge of the enkf experiment at step i q o i is the observed discharge at step i q o l i is the simulated discharge of the ol experiment at step i according to the definitions of the three metrics a larger value in net ner and eef indicate a better effect of da on reducing errors when net ner and nef are greater than 0 it indicates that da improves the accuracy of the discharge simulation when ner and nef are close to 100 it indicates that the simulated discharges of the enkf experiment is close to the observed discharges 4 results 4 1 evaluation of remotely sensed and modeled soil moisture in the hanjiang watershed to evaluate the smsatellites data and modeled soil moisture in the hanjiang watershed three statistical metrics including bias rmse and correlation coefficient r were used to compare their quality to observed soil moisture the modeled soil moisture was simulated by the wrf hydro model with the input data in advance of ten days and with default parameter values bilinear interpolation was applied to the smsatellites data and modeled soil moisture match with the locations of the nine stations to produce the site level values making them comparable to the site level observed data of these stations fig 3 shows ten day soil moisture series of the three datasets i e observations smsatellites and modeled soil moisture from 2013 to 2014 at nine stations of the hanjiang watershed overall smsatellites is able to capture the observed soil moisture dynamics at the nine stations fig 3a i comparing to the observed soil moisture all r values of smsatellites are equal to or higher than 0 30 but those of the modeled soil moisture are less than or equal to 0 30 fig 3a i furthermore the rmse and bias values of smsatellites are within 0 10 m3 m3 fig 3a i in contrast the rmse and bias values of modeled soil moisture range between 0 03 and 0 14 m3 m3 these results clearly demonstrate that the smsatellites dataset has a higher accuracy than modeled soil moisture in the hanjiang watershed fig 3 in terms of the seasonality there is no obvious seasonality in the three soil moisture datasets fig 3 in terms of bias both smsatellites and wrf hydro soil moisture tend to overestimate soil moisture fig 3 soil types in the hanjiang watershed are mainly clay clay loam loam sandy loam and loamy sand soil types of the two stations are silt loam there are four types of land covers including forest grassland farmland and bush accounting for 68 25 5 and 2 of the hanjiang watershed respectively in addition there is also large variability in terrain across this region since many spatially varying factors such as soil type land cover geography geomorphology and human activity can affect the accuracy of remotely sensed soil moisture it is not surprising that smsatellites shows certain variable accuracy among the nine stations however smsatellites has an obvious better accuracy than wrf hydro simulations which validate our idea to improve the wrf hydro simulation by assimilating smsatellites fig 4 summarizes the spreads of the three statistical metrics for smsatellites and wrf hydro soil moisture in terms of bias both datasets tend to overestimate soil moisture but smsatellites has smaller biases with a mean value of 0 06 m3 m3 and a narrower bias spread than the modeled soil moisture which has a mean bias of 0 08 m3 m3 fig 4 these results indicate that smsatellites is more accurate than modeled soil moisture in terms of rmse smsatellites has smaller values and a narrower spread than modeled soil moisture rmse of smsatellites ranged between 0 01 and 0 09 m3 m3 whereas that of modeled soil moisture varied from 0 03 to 0 14 m3 m3 fig 4 these results also suggest that smsatellites performs better and is more accurate than modeled soil moisture finally smsatellites is generally highly correlated with the observed soil moisture across the hanjiang watershed the r values for smsatellites vary from 0 30 to 0 46 while modeled soil moisture values are between 0 2 and 0 30 fig 4 in summary smsatellites has better values for all three metrics comparing to the modeled soil moisture suggesting that smsatellites indeed performs better than the modeled soil moisture to further evaluate the two soil moisture datasets we compared smsatellites and modeled soil moisture against the observations through a scatter density plot the r value of the smsatellites is 0 29 while that of modeled soil moisture is 0 05 fig 5 the rmse and bias values of smsatellites are both less than these of modeled soil moisture fig 5 this comparison also shows that smsatellites has a better performance in the hanjiang watershed figs 3 5 proving the validity of assimilating smsatellites into the wrf hydro model 4 2 stable warming ups of wrf hydro model in the daheba watershed based on the observed discharges from 2013 to 2014 we selected three flood events that have discharges exceed the warning discharges 1000 m3 s for investigating the da of smsatellites in the ungauged daheba watershed the durations of the three flood events 2013071500 2013072200 and 2014090700 are 144 h 168 h and 480 h respectively flood simulation and forecasting are affected by many factors especially the forcing data the antecedent precipitation and state of the watershed at the beginning of a flood event in a model directly affect the accuracy of flood simulation and forecasting to find a steady initial state of the wrf hydro model before conducting smsatellites assimilation the three flood events are simulated from 0 to 15 days in advance fig 6 shows the three evaluation metrics dp dv and dc of 16 simulated results by wrf hydro for the three flood events at the daheba hydrological station the dp values of the three flood events range from 12 to 71 from 1 to 85 and from 10 to 30 respectively fig 6a the dp values of the 2013071500 and 2013072200 events have a larger variability than those of the 2014090700 event fig 6a as shown in fig 6a dp values of the wrf hydro simulations for all the three events generally decrease as warming up period increase when the warming up period is less than 10 days as the warming up period further increases beyond 10 days the wrf hydro dp values for two flood events even flip to increase fig 6a it is clearly that the wrf hydro dp values reach a stable stage when warming up period falls between 8 and 10 days similarly the wrf hydro dv and dc values also reach their stable stages when warming up period is between 8 and 10 days fig 6b and c these results clearly show that there exist indeed stable warming up periods for the wrf hydro model in our study cases unlimited extending warming up period cannot warrant enhancing performance of the wrf hydro model therefore we chose 10 days as the warming up length for the wrf hydro model in the analyses 4 3 evaluation of the ol and enkf experiments in daheba watershed fig 7 shows the time series of the observed and simulated discharges in the sim ol and enkf experiments for the three flood events at the outlet of the daheba watershed as shown in fig 7 the ensemble means of the ol and enkf experiments are in a better agreement with the observations than the sim results for all of the three flood events the model rmse values for the sim results the ensemble means of the ol experiment and the ensemble means of the enkf experiment range between 153 83 and 180 52 m3 s between 106 10 and 183 02 m3 s and between 87 37 and 156 23 m3 s respectively for the three flood events relative to the ol experiment the enkf experiment can further improve the model accuracy fig 7 for example the net ner and nef metrics of the enkf experiment for the 2013071500 event are 18 m3 s 17 and 32 respectively while these metrics are 29 81 m3 s 19 49 and 35 17 for the 2013072200 event and 15 69 m3 s 17 73 and 32 20 for the 2014090700 event these results show that the enkf da of smsatellites has clearly reduced errors in the wrf hydro model relative to the ol experiment in addition the ensembles of the ol experiment for all of the three flood events have larger spreads the blue shades in fig 7a c than those of the enkf experiment the red shades in fig 7a c clearly the simulated discharges of the enkf experiment are substantially better than the sim and ol counterparts particularly during the simulation period with da fig 7 which proves that the smsatellites assimilation can robustly improve the accuracy of flood simulation and forecasting in the daheba watershed three other evaluation metrics including dp dv and dc are also typically used to evaluate the simulation effects of flood simulation and forecasting to compare the results of the ensemble members of the ol and enkf experiments we further quantified the spread of the three statistical metrics of the ol and enkf ensemble members the dp values of the ol members for the 2013071500 event fall between 18 and 31 with a mean value of 31 while the dp values of the enkf members for the same event are all less than 20 with a mean value of 13 fig 8 a this illustrates that the smsatellites assimilation reduces the absolute relative error of the flood peak the averaged dv values of the ol and enkf members for the 2013071500 event are 24 and 19 respectively fig 8a the dc values of the enkf members for the 2013071500 event are all larger than 0 87 with a mean value of 0 91 fig 8a in contrast the dc values of the ol members for the 2013071500 event vary between 0 82 and 0 93 with a mean value of 0 86 fig 8a the discharge hydrographs of the enkf experiment fit better with the observed hydrograph for the 2013071500 event figs 7a and 8a indicating that the smsatellites assimilation leads to improved flood simulation the enkf experiment results for the 2013072200 and 2014090700 events are also better than the ol experiment results fig 8b and c indicated by smaller and narrower dp and dv values and larger and narrower dc values these results also indicate that the simulated hydrographs of the enkf experiment generally agree better with the observations than the ol experiment fig 7b and c in summary the enkf experiment has better results than the ol experiment proving that the enkf da of smsatellites improve the model performance relative to the simulations without da 4 4 hydrological evaluation of assimilating the smsatellites into the wrf hydro model to quantitatively describe the results of the wrf hydro model through the smsatellites assimilation the ensemble means of the simulated discharges obtained by the ol and enkf experiments are regarded as the final simulation results fig 9 shows the observed discharge hydrographs grey area the ensemble means of the ol experiment blue line and the ensemble means of the enkf red line experiment for the three flood events we quantitatively evaluated the simulated discharges through the dp dv and dc metrics the discharge hydrograph of the enkf experiment is basically consistent with the observed hydrograph at the starting and raising phases whereas the simulated discharges of the enkf experiment are slightly larger than the observed discharges in the decreasing phase of the 2013071500 event fig 9a in contrast the simulated discharges of the ol experiment are generally lower than the observed discharges during the decreasing phase fig 9a the simulated flood peaks in the ol and enkf experiments are lower than the observed flood peak for the 2013071500 event but the enkf experiment result is apparently closer to the observation than the ol one fig 9a the dp value of the enkf experiment is smaller than that of the ol experiment while the absolute value of dv is also reduced from 21 to 16 and dc is slightly enhanced as well when da is applied for the 2013072200 flood event the dp value of the enkf experiment 0 17 is lower than that of the ol experiment 0 30 indicating that the smsatellites assimilation has effectively improved the flood peak simulation and reduced the flood peak error fig 9b the dc value of the enkf experiment 0 88 is also larger than that of the ol experiment 0 82 although the smsatellites assimilation does not improve the simulation accuracy for flood volume of the 2013072200 flood event it has effectively improved the simulated discharge process and peak flow the 2014090700 flood event has multiple events and associated flood peaks fig 9c from the last flood peak to the end of this flood event the discharge hydrographs of the ol and enkf experiments are both in good agreement with the observed hydrograph likely due to the basin wide saturated conditions resulted from long lasting rainfall the discharge hydrograph of the enkf experiment still fits better with the observed discharge hydrograph for the second flood peak comparing to the ol experiment fig 9c statistically the smsatellites assimilation has led to reduced dp and dv values and increased dc value indicating that the smsatellites assimilation has effectively improved the simulation of the flow process and improved the simulation accuracy for the 2014090700 flood event in summary the smsatellites assimilation can substantially improve the accuracy of simulated discharges for the three flood events obviously da of surface soil moisture is effective and robust for improving the accuracy of flood simulation and forecasting in our case study to further quantify the assimilation resulted improvement on flood simulation we calculated the net ner and nef metrics for all of the three flood events by comparing the enkf ensemble mean and the ol ensemble mean as shown in table 3 all rmse values of the three flood events has been decreased through the smsatellites assimilation especially for the 2013072200 event the net ner and nef values are all greater than 0 table 3 indicating that assimilation has a positive effect on flood simulation the ner and nef of the 2013071500 and 2013072200 events are larger than 13 while the smallest value of ner is for the 2014090700 event and is still greater than 6 table 3 once again table 3 shows that the smsatellites assimilation has improved the accuracy of flood simulation in the ungauged daheba watershed implying the potential utility of multi satellite retrieved composite soil moisture for improving model flood forecasting especially in the ungauged areas to further figure out whether da of soil moisture can improve flood simulation for any discharge magnitudes we divided the discharges into the low medium and high magnitudes i e 0 100 m3 s 100 1000 m3 s and 1000 m3 s we then compared the results with and without the smsatellites assimilation and computed the net ner and nef for the three discharge magnitudes as shown in table 4 the net ner and nef metrics also increase as discharge magnitude increases indicating that the da benefits more for the larger discharge table 4 the net ner and nef metrics are even negative for the low discharge magnitude table 4 indicating that da even leads to degraded simulations on the low discharge range however the smsatellites assimilation leads to a large improvement for the medium and high discharge ranges additionally the net ner and nef metrics for the high discharge range are greater than those for the middle value section table 4 indicating that da benefits more for the high value floods low discharge values occur on the prior rising and late falling stages of floods so da of soil moisture is not necessary on these stages on the prior rising stage soil is relatively dry in both model simulation and satellite retrieval and da of soil moisture has little effect on discharge simulation on the late falling stage soil is saturated thus da of soil moisture is ineffective especially during the receding phase of floods for medium and high ranges of discharge the effect of soil moisture assimilation on discharge simulation is more sensitive than these low value ranges fig 7 namely the ensemble range of hydrograph has a more fluctuation when the discharges are more than 100 m3 s 5 discussion and conclusions this study aims to assimilate remotely sensed soil moisture by multiple satellites to improve the accuracy of flood simulation and forecasting we established a da scheme for the wrf hydro model to assimilate the surface soil moisture jointly retrieved by multiple microwave satellites based on the enkf algorithm we then tested the framework in the ungauged daheba watershed and analyzed how effective this da framework and its functions for different magnitudes of flow processes comparing to the pure model simulation the sim experiment and ensemble model simulation resulted from parameter perturbations the ol experiment the ensemble model simulation with da the enkf experiment have largely improved the flood simulations figs 7 9 this study provides a valuable reference for improving the accuracy of flood simulation and forecasting by assimilating remotely sensed soil moisture in the small and medium sized watersheds without gauged soil moisture data it can also play a positive role on promoting the application of remote sensing data in flood simulations and forecasting nevertheless study on the model data assimilation of remotely sensed soil moisture is still a complicated issue the effectiveness of da is not only affected by the quality of remotely sensed soil moisture data but also related to various factors such as uncertainties in the models structure and parameters the methods to measure errors in the modeled and observed values bias correction methods of remotely sensed soil moisture and the assimilation scheme among the others crow and ryu 2009 we evaluated the smsatellites and modeled soil moisture by comparing with the observed soil moisture at nine stations in the hanjiang watershed although the smsatellites performs better than modeled soil moisture a large room remains for further improving the soil moisture retrieval the hanjiang watershed originates from the qinling mountains as the dividing boundary between the north and south of china the rugged terrain features steep brooks and streams large variability in topography and climate zones can affect the accuracy of soil moisture retrieval wang et al 2020 zhang et al 2019b the type density structure and composition of vegetation are critical for the microwave remote sensing of soil moisture lakhankar et al 2009 wigneron et al 2017 many parameters of the wrf hydro model have a physical basis and can be determined by soil texture and vegetation type for example the zsoilfac parameter directly has an important effect on modeled soil moisture of different soil layers and then has an impact on the simulated flood volume the refkedt parameter is related to the infiltration and permeation rates which mainly affect runoff generation the ovroughrtfac and mannfac parameters influence the overland runoff and streamflow selection of the hydrological model parameters for perturbation and the optimization schemes directly affect the simulation results and effectiveness of da therefore it is important to conduct analyses on the sensitivity uncertainty and optimization of model parameters prior conducting da sun et al 2020 the model parameters can also impact the effectiveness of the soil moisture assimilation and the resultant streamflow simulations baguis and roulin 2017 alvarez garreton et al 2014 found that parameter optimization can affect the assimilation performance by assimilating satellite surface soil moisture into the probability distributed model pdm to improve the accuracy of the runoff forecast the interpolation of meteorological data in time and space may cause some uncertainty in hydrological modeling accadia et al 2003 compared bilinear interpolation and a simple nearest neighbor averaging method used for interpolating precipitation and found that bilinear interpolation smoothed the maxima and increased the minima of precipitation when the data was not sufficient an interpolated precipitation from station observation and satellite in mountainous areas overestimated the observed precipitation especially in late spring and early summer hu et al 2016 six geo spatial interpolation methods four of which are based on bilinear interpolation were used to transform multi sensor source data to another spatial resolution showing biases caused by these interpolation methods teegavarapu et al 2012 although interpolation methods can produce some uncertainty ebrahimi et al 2017 the bilinear interpolation can obtain smoother spatial and temporal distributions and smaller data variability than many other interpolation methods chen et al 2021 wang et al 2021 although the bilinear interpolation like any other interpolation method has its own imitation it is a widely used spatial interpolation method with a relatively strong mathematical basis our results demonstrate that assimilation of remotely sensed soil moisture has a potential value on improving the simulation and forecasting of the wrf hydro model indicating that the da technology and the assimilation of multi source satellite remote sensing data can be used in flood simulation and forecasting and help to improve the accuracy of the simulated rainfall runoff processes the wrf hydro model has a potential to simulate and forecast floods at global and regional scales lin et al 2018a 2018b 2020 the application of multi source satellite remote sensing data for the wrf hydro model can be used to solve the flood simulation and forecasting of watersheds with limited data or without observed data author contributions ke zhang and lijun chao designed this study lijun chao and ke zhang set up the models and method lijun chao ke zhang zhao gu and sheng wang conducted this study lijun chao and ke zhang wrote the manuscript ke zhang and lijun chao acquired the funding junzeng xu and hongjun bao contributed to the revision data availability the satellite precipitation data are available from cmorph climate data record https www ncei noaa gov data cmorph high resolution global precipitation estimates access the geographic data are available from shuttle elevation derivatives at multiple scales hydeosheds database http www hydrosheds org the land use and soil data are available from the wrf preprocessing system wps static data https www2 mmm ucar edu wrf users download get sources wps geog html the input meteorological data excepting for precipitation are available from gldas noah land surface model l4 3 hourly 0 25 0 25 v2 1 https disc gsfc nasa gov datasets keywords gldas page 1 the wrf hydro model can be found online at https ral ucar edu projects wrf hydro overview the assimilation algorithm code to this article can be found online at https github com doudouyl assimilation algorithm v2 0 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this study was supported by the national natural science foundation of china 52009028 and 51879067 national key research and development program of china 2018yfc1508101 china and jiangsu province postdoctoral science foundation 2020m671323 and 2021k572c natural science foundation of jiangsu province bk20180022 the fundamental research funds for the central universities of china b200204038 b210202115 and b220203051 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105421 
25577,this study investigates how assimilation of surface soil moisture jointly retrieved by multiple microwave satellites affects flood simulation and forecasting based on the experiments of simulation sim open loop ol and ensemble kalman filter enkf in small and medium sized watersheds without gauged soil moisture we developed a framework for data assimilation da of satellite soil moisture into the wrf hydro model based on the enkf algorithm three statistical metrics to evaluate the impacts of da including net error reduction normalized error reduction and effectiveness criterion are all positive values 6 0 indicating that da gains reduced errors meanwhile the deterministic coefficients of the enkf experiment are also greater than those of the ol experiment it is obvious that multi satellite retrieved soil moisture and da technology can improve the accuracy of flood simulation and forecasting in ungauged regions and play an important and positive role in hydrological forecasting keywords multi satellite microwave remote sensing data assimilation ensemble kalman filter wrf hydro model ungauged watershed flood simulation and forecasting 1 introduction data assimilation da was originated from numerical weather prediction nwp in the mid 20th century and has gradually attracted attention in many fields since the 1980s christian 2005 da has been applied in the modeling of atmospheric oceanic land surface and hydrological processes in the 21st century zaitchik et al 2010 currently the most representative da systems include the north american land data assimilation system luo et al 2003 global land data assimilation system gldas rodell et al 2004 the european land data assimilation system jacobs et al 2008 and the china land data assimilation system shi et al 2011 the wide application of da in different fields has resulted in varied definitions of da abdolghafoorian and farhadi 2016 charney et al 1969 first used a simulation method to study da thus some researchers believe that the study of da begins with simulation research rutherford and asselin 2010 studied the dependence of the assimilated wind field on latitude and scale and found that only the largest scale movement of the wind field had an acceptable accuracy in the middle and high latitudes da can integrate model simulation results and direct and indirect observed data with different sources and resolutions into a dataset with temporal spatial and physical consistencies xin et al 2007 robinson and lermusiaux 2000 defined da as a general method for estimating ocean variables which combined observed data with oceanographic forecast models to provide accurate variable estimates however talagrand and courtier 2010 suggested that the definition of da is using all existing information to estimate the most accurate possible atmospheric or ocean motion state da has evolved through empirical mathematical simple intelligent and intelligent algorithms stages since 1969 cressman 1959 hoke and anthes 1976 liang and qin 2008 panofsky 1949 the original da algorithm was used to correct the weather forecast results based on observed data in the 1950s and 1960s cressman 1959 panofsky 1949 this is called the empirical algorithm stage and it lacks a theoretical basis including an objective analysis method a correction method based on a stepwise polynomial fitting method and relaxation approximation sasaki 1958 the statistical methods were introduced into the da field based on the classically optimal interpolation method leading to the stage of the mathematical algorithm with a theoretical foundation in the 1970s and 1980s gauntlett and seaman 1974 furthermore continuous da algorithms based on the optimal control theory and sequential da based on the statistics theory were known as the simple intelligent algorithms that were developed after the 1980s houtekamer and mitchell 2001 stern and ploshay 1992 including the kalman filter ensemble kalman filter enkf extended kalman filter ensemble adjustment kalman filter particle filter three dimensional variational da and four dimensional variational da carrassi et al 2018 crow and wood 2003 lin et al 2016 zhao et al 2016 with the development of artificial neural networks and coupling of multiple da algorithms the evolution of continuous da entered the intelligent algorithm stage in the 21st century cintra and de campos velho 2018 dobricic and pinardi 2008 fang and li 2019 van delft et al 2009 currently da algorithms are applied in many studies especially hydrological modeling and flood analysis main hydrological variables that da deals with include runoff evaporation soil moisture temperature snow cover and snow water equivalent runoff da is usually used to correct the output of hydrological models to directly improve the accuracy of flood simulation and forecasting therefore it is widely used in hydrological forecasting liu et al 2016a 2016b soil moisture is a key variable for rainfall runoff process which directly reflects the hydrological state it is highly related to infiltration process and further regulates runoff generation and confluence within a watershed so it can directly affect the accuracy of flood simulation and forecasting chao et al 2019 da algorithm can effectively combine the simulated soil moisture of hydrological models and observed soil moisture to continuously update state variables and improve the accuracy of flood simulation and forecasting within a watershed or a region han et al 2012 meng et al 2017 firstly da of soil moisture as a technique or method can introduce observed data into hydrological models secondly da of soil moisture can chronologically improve the accuracy of soil moisture simulation by the hydrological model thereby improving the accuracy of flood forecasting therefore data assimilation of soil moisture is necessary as a bridge between models and observed data da of soil moisture in hydrological models generally has two main purposes 1 to study soil moisture assimilation methods by assimilating gauged data or satellite remote sensing of soil moisture chen et al 2015 and 2 to continuously update the soil moisture status of the watershed and improve the accuracy of flood simulation and forecasting chen et al 2011 laiolo et al 2016 assimilation of soil moisture with ground station monitoring was carried out on a point scale in the early stage jackson 1980 assimilated surface soil moisture using observed soil moisture which greatly improved the accuracy of soil moisture estimation for different soil profiles to improve the accuracy of soil moisture estimation heathman et al 2003 assimilated observed soil moisture into models chen et al 2011 used the enkf algorithm to assimilate surface soil moisture into swat model which significantly improved the accuracy of soil moisture evaporation and streamflow predictions the development of remote sensing technology makes it possible to obtain continuous soil moisture in space da of remotely sensed soil moisture has become a research hotspot since the 1990s reichle 2008 reichle et al 2007 for example crow et al 2005 assimilated satellite soil moisture into the antecedent precipitation index model and demonstrated that the rainfall runoff predictive ability of the api model was improved in 26 basins of the united states southern great plains lievens et al 2015 assimilated smos soil moisture products with a spatial resolution of 25 km into the vic model which improved the spatial resolution of soil moisture and the accuracy of hydrological simulation in the murray darling basin four different satellite derived soil moisture datasets were updated to a distributed and continuous hydrological model by carrying out an assimilation scheme in a small catchment in italy and the results showed that the nash sutcliffe model efficiency coefficient increased from 0 6 without assimilation to 0 7 assimilated which proved that discharge prediction of a distributed hydrological model at a fine scale resolution was improved by assimilating remotely sensed soil moisture laiolo et al 2016 assimilating smap and smos soil moisture datasets into a distributed hydrological model improved streamflow predictions of 131 usgs gauge observations from 2015 to 2018 jadidoleslam et al 2021 although many studies have proven that remotely sensed soil moisture assimilation can improve the accuracy of flood simulation and forecasting the assimilation effect is limited when there is a large deviation between simulated discharges and observed data alvarez garreton et al 2015 an ensemble based on the da method was used to explore the benefit of jointly assimilated remotely sensed smap soil moisture to improve the accuracy and reliability of wrf hydro model predictions and the results showed that the multivariate assimilation of soil moisture results in improved prediction of streamflow as compared to univariate assimilation configurations and regardless of the watershed s streamflow regime abbaszadeh et al 2020 meng et al 2017 developed an effective da scheme considering a time lag between soil moisture and streamflow and the rmse of flood simulation was reduced more than 70 the benefit of soil moisture assimilation is reduced as uncertainty in discharge forecasts li et al 2014 the other studies also assimilated surface soil moisture into the hydrological model which demonstrates the improvement of the flood simulation and forecasting chen et al 2013 li et al 2014 massari et al 2018 brocca et al 2013 compared the assimilation effect of different satellite soil moisture products in four different catchments worldwide but their results demonstrated that the assimilation of remotely sensed soil moisture has no impact on flood simulation and forecasting in mountainous areas considering that there are many existing satellite soil moisture products with varying differential uncertainty in space and time only assimilating a single satellite product is usually unreliable in this study we aim to improve the accuracy of flood simulation and forecasting by assimilating surface soil moisture jointly retrieved by multiple microwave satellites smsatellites into the wrf hydro model based on the enkf algorithm in ungauged basins in particular we chose the daheba watershed as a case study area which is a first order sub basin of the hanjiang watershed in the qinling mountains and a typically small and medium sized watershed 3000 km2 without observed soil moisture the soil moisture dataset jointly retrieved by multiple satellites and modeled soil moisture dataset were evaluated in the hanjiang watershed which is a first order sub basin of the yangtze river basin we also analyzed the assimilation effect on each flood event and different ranges of discharges by designing the simulation sim open loop ol and enkf experiments table 1 2 study area and data 2 1 study area the daheba watershed is selected as a case study area for da which has no observed soil moisture data fig 1 a more details on described the daheba watershed are referred to chao et al 2021 the terrain elevation in this watershed decreases from north to south with steep brooks and steams favoring the formation of floods fig 1a due to the barrier effect of the qinling mountains humid air from the southern and southeastern areas brings a large amount of water vapor and the watershed is prone to frequent flash floods caused by heavy rainfall especially in summer summer rainfall accounts for 46 of annual total precipitation and is characterized by short duration and high intensity resulting in flash floods average annual precipitation and runoff of this watershed are about 900 mm and 400 mm respectively because the daheba watershed has no observed soil moisture we had to choose a larger area that contains the daheba watershed and has observed soil moisture to verify the surface soil moisture jointly retrieved by multiple microwave satellites the hanjiang watershed is the parent watershed of the daheba watershed and has nine stations to measure soil moisture the hanjiang watershed was chosen to verify soil moisture in this study we evaluated the quality of smsatellites in the hanjiang watershed which is a major tributary of the yangtze river basin and covers a total area of 61 662 km2 fig 1b it originates from the fanzhong mountain which belongs to the qinling mountains in ningqiang county of hanzhong city shaanxi province and mainly flows through shaanxi province and hubei province contributed by the subtropical monsoon climate and the qinling mountains rainstorms and floods in the hanjiang watershed are highly related to monsoon activities runoff of the hanjiang watershed is unevenly distributed throughout the year from may to june floods can be formed due to more frequent precipitation extremes under the influence of the southwest monsoon southwest monsoon prevails in july leading to rainstorm triggered floods in the upper reaches of the hanjiang watershed southeast monsoon activity is the most active in august which controls the upper reaches of the hanjiang watershed and can cause severe floods strong cold air from the north can also cause large floods in september therefore floods are mainly concentrated from may to september in the hanjiang watershed the runoff of the mainstream in summer and autumn accounts for 37 40 of annual runoff while spring runoff accounts for 16 6 17 5 of annual total and winter runoff only accounts for 5 6 7 2 2 data 2 2 1 geographic data geographic data and maps include elevation flow direction flow accumulation and river network which were provided by shuttle elevation derivatives at multiple scales hydrosheds database http www hydrosheds org with a spatial resolution of 1 km fig 1 land use data of the igbp modified modis 20 land use categories and soil type data used in this study were obtained from the wrf preprocessing system wps static data https dtcenter org wrf nmm users docs user guide v3 users guide nmm chap3 pdf the vegetation fraction was derived from the 3rd generation normalized difference vegetation index ndvi from the level 3 l3 monthly 1 km global vegetation indices mod13a3 provided by the national aeronautics and space administration nasa https e4ftl01 cr usgs gov molt mod13a3 006 2 2 2 gauge data gauge data include precipitation soil moisture and discharge the discharge data with a temporal resolution of 1 h were collected from the daheba hydrological station during 2013 2014 fig 1a three flood events 2013071500 2013072200 and 2014090700 were chose to test da from 2013 to 2014 which were exceeded the warning discharge with a value of 1000 m3 s each flood event number is represented by 10 digits including year month day and hour fifteen rain gauge stations were used to collect precipitation data with a temporal resolution of 1 h in the daheba watershed fig 1a there were only five rain gauge stations before 2011 and the cofferdam has been established downstream of the daheba watershed in 2015 which is 3 km away from the daheba hydrological station the observed discharge data from 2013 to 2014 were used to verify the results of soil moisture da for this study because this period has low or no human activities the wrf hydro model input data were obtained from the gauge observation network cmorph and gldas with spatiotemporal resolutions of 1 h and 1 km the spatiotemporal resolutions of the wrf hydro model outputs were set to be the same as the input data soil moisture in the study area is difficult to collect most soil moisture observation stations aim to monitor agricultural droughts the daheba watershed belongs to a humid area located in the deep and high rocky hills of the qinling mountains there is no soil moisture gauge station which hindered us from finding a benchmark to verify the satellite soil moisture smsatellites and modeled soil moisture in the daheba watershed therefore we extended the verification area to the entire hanjiang watershed due to the lack of the observed soil moisture within the daheba watershed fig 1b the observed soil moisture data from 2013 to 2014 in nine stations were provided by the shaanxi province hydrology and water resources investigation bureau the temporal resolution of the observed soil moisture is ten days specifically there are only measurements on the 1st 11th and 21st days of each month therefore there are a total of 72 observed data points at each station from 2013 to 2014 2 2 3 meteorological data meteorological data used as the input of the wrf hydro model include precipitation air temperature surface pressure specific humidity incoming shortwave radiation incoming longwave radiation and the u and v components of near surface wind as forcing data precipitation was merged from satellite precipitation and gauged precipitation using the mixed geographically weighted regression and bi square mgwr bi algorithm chao et al 2018 satellite precipitation data were obtained from the cmorph product with a spatial resolution of 8 km and a temporal resolution of 0 5 h then the original cmorph product of 0 5 h was accumulated to 1 h and the 8 km cmorph precipitation product was downscaled to 1 km precipitation data comrphd using the bilinear interpolation method the downscaled cmorph and gauge precipitation data with a temporal resolution of 1 h were merged using the mgwr bi algorithm other input meteorological data were obtained from the l4 gldas noah land surface model with 3 hourly and 0 25 0 25 resolutions https disc gsfc nasa gov datasets keywords gldas the gldas data can provide meteorological data at a global scale but the resolution is not sufficient for hydrological simulation in the daheba watershed in other words the daheba watershed covers only 3 4 gldas grid cells and the hanjiang watershed covers 11 19 gldas grid cells therefore we used linear and bilinear interpolation methods to disaggregate the spatiotemporal resolution of 3 h and 25 km into 1 h and 1 km respectively 2 2 4 remotely sensed soil moisture the soil moisture data used for da in this study were obtained by using the surface soil moisture jointly retrieved by multiple microwave satellites including the smap smos amsr2 and fengyun satellites smsatellites with a spatial resolution of 25 km and a temporal resolution of 1 day zhang et al 2019a it is a kind of composite soil moisture for near real time application with better coverage and higher accuracy across china zhang et al 2019a first we applied the single channel brightness radiometric algorithm to retrieve soil moisture from the respective brightness temperature observations of the fy3c fy3b smos amsr2 and smap satellites on the same day second a daily composite dataset was produced by averaging the individual satellite retrieved soil moisture data the resultant averaged soil moisture was used for da and the spatial resolution was disaggregated from 25 km to 1 km the retrieval algorithm includes four steps zhang et al 2019a first the brightness temperature is converted to emissivity using the physical temperature of the emitting layer through the single channel brightness algorithm based on the classical tau omega model second the influence of vegetation and surface roughness was eliminated since emissivity is impacted by vegetation and surface roughness above the soil surface the soil surface emissivity can be solved as a function of emissivity the soil surface emissivity is the value determined from the vegetation correction which is related to the single scattering albedo and the one way transmissivity of the canopy depending on the vegetation structure polarization and frequency the relationship between the soil surface emissivity and the surface roughness parameter depending on the polarization frequency and geometric properties of the soil surface is established which is used to eliminate the influence of roughness third the mineralogy based soil dielectric model mironov et al 2009 was used for data processing in radar and radio thermal remote sensing to establish the relationship between the dielectric constant and soil moisture finally the retrieval of surface soil moisture was inferred based on this relationship the bilinear interpolation method was employed to disaggregate the 25 km satellite soil moisture dataset into the 1 km resolution the final smsatellites dataset with a spatial resolution of 1 km and a temporal spatial resolution of 1 day was used for da from 2013 to 2014 in this study 3 methodology to assimilate smsatellites data in ungauged watersheds the enkf algorithm was used for conducting da for the wrf hydro model to realize the smsatellites assimilation the general framework applied in the daheba watershed is shown in fig 2 the general idea of enkf is that ensemble forecasting is used to solve the problem of error covariance estimation and forecasting difficulties in practical applications evensen 1994 it incorporates a monte carlo method to generate a state ensemble with a certain distribution to represent its probability based on the theory of the linear kalman filter kalman 1960 enkf has become a mainstream algorithm for da and been widely used in da of the atmosphere ocean land surface and earth systems because it can be used for the assimilation of nonlinear systems evensen 2003 2004 the enkf algorithm contains two steps forecasting and updating forecasting is the transition process of hydrological model state variables and updating is the update process of the hydrological model state 3 1 bias correction method of remotely sensed soil moisture the most basic criterion for hydrological models is the water balance in the watershed or study region to ensure the water balance in the watershed bias correction of the smsatellites data was carried out before assimilation bias correction methods mainly include first order second order and cumulative distribution function methods liu et al 2012 the first order correction method assumes that the average values of the two samples are the same specifically when the averaged values of the smsatellites data are equal to the modeled soil moisture in a certain time series then the smsatellites data is corrected to ensure the credibility of the fit between the two soil moisture datasets a long term series of samples was required we chose a simple and convenient first order correction method the equation can be written as 1 s m i o b s s m i s i m s m i o b s s m i o b s where s m i o b s is the corrected smsatellites data s m i o b s is the smsatellites data of grid cell i s m i o b s is the average value of the smsatellites data for one flood event s m i s i m is the average value of the modeled soil moisture for one flood event 3 2 model description the wrf hydro modeling framework i e the wrf hydro model is the hydrological extension of the noah and noah mp land surface model arnault et al 2016 yucel et al 2015 which can be run standalone or coupled with the wrf atmospheric model lin et al 2015 senatore et al 2015 the wrf hydro model simulates water and energy fluxes at high spatiotemporal resolutions based on physical and conceptual approaches ma et al 2021 it includes the noah and noah mp land surface models to simulate overland flow saturated subsurface flow channel routing and baseflow gochis et al 2018 the input data for the wrf hydro model include meteorological data precipitation air temperature surface pressure specific humidity incoming shortwave radiation incoming longwave radiation and the u and v components of near surface wind geographic data dem flow direction flow accumulation and river network soil type land cover types and vegetation fraction chao et al 2021 the output of water fluxes includes soil moisture evapotranspiration deep soil drainage surface runoff snow depth channel flow depth channel flow etc the output of energy fluxes consists of surface latent heat flux surface sensible heat flux and ground heat flux the main parameters of the wrf hydro model are listed in table 2 and their values are mainly based on their respective physical meanings sun et al 2020 3 3 assimilation scheme based on the principles of da three comparative experiments for the smsatellites assimilation were designed in this study the three experiments are the sim ol and enkf experiments the sim experiment is the model simulation with given model inputs and a warming up period of ten days which results in a single model realization in contrast the ol experiment is an ensemble of model simulations realizations that have the same inputs and warming up period as the sim experiment but have perturbed values for the key model parameters listed in table 2 based on the gaussian distribution assumption in the enkf experiment we conducted the enkf da of the smsatellites data at 12 00 of each day during the simulation period for every ensemble member of the ol experiment therefore differences between the ol and enkf experiments lies in whether the soil moisture of the wrf hydro model is updated by da the specific steps of the smsatellites assimilation in the enkf experiment include correcting the bias in the smsatellites wrf hydro model simulation and updating modeled soil moisture the state variables are functions of the wrf hydro model in the forecasting or simulation process the function can be written as 2 s m i t 1 s i m m o d e l w r f h y d r o s m i t a n a where m o d e l w r f h y d r o is operator of the wrf hydro model s m i t s i m is modeled soil moisture of grid cell i at t 1 time step s m i t a n a is update value of soil moisture through the enkf algorithm at t time step namely the analytical value of state variable the analytical value of soil moisture is defined as 3 s m i t a n a s m i t s i m k i t s m i t o b s h s m i t s i m 4 k i t p i t f h t h p i t f r i t 1 5 p i t f 1 n 1 k 1 n s m k t s i m s m t o b s s m k t s i m s m t o b s t 6 r i t 1 n 1 k 1 n s m k t o b s s m t o b s s m k t o b s s m t o b s t where h is the operator of observation k i t is the matrix of enkf gain at t time step p i t f is the error covariance of the modeled soil moisture at t time step r i t is the error covariance of the smsatellites at t time step n is the number of ensembles 3 4 evaluation metrics of assimilated results to evaluate the wrf hydro model performance three evaluation metrics were selected to evaluate the simulated discharges in the sim ol and enkf experiments table 1 including the absolute relative error of flood peak dp relative error of flood volume dv and deterministic coefficient dc chao et al 2019 2021 dp is defined as the percentage of the absolute difference between the observed and simulated flood peaks to the observed flood peak dv describes the accuracy of the modeled total flood water volume dc measures the fit between the observed and simulated discharges for one flood event to assess the performance of the smsatellites assimilation three statistical metrics table 1 were used net error reduction net normalized error reduction ner and an effectiveness criterion nef based on the root mean square error rmse chen et al 2013 net refers to the difference between the rmse of the ol experiment discharges and that of the enkf experiment discharges it quantifies how much the smsatellites assimilation can reduce rmse in the ol experiment 7 n e t r m s e o l r m s e e n k f where r m s e e n k f is the rmse value of the simulated discharge by the enkf experiment r m s e o l is the rmse value of the simulated discharge by the ol experiment ner represents the relative value of the decreased rmse caused by the enkf da relative to the ol results 8 n e r r m s e o l r m s e e n k f r m s e o l 100 nef evaluates the effect of da on reducing errors by comparing the sums of error squares between the observed and simulated discharges of the ol and enkf experiments 9 n e f 1 i 1 n q e n k f i q o i 2 i 1 n q o l i q o i 2 100 where q e n k f i is the simulated discharge of the enkf experiment at step i q o i is the observed discharge at step i q o l i is the simulated discharge of the ol experiment at step i according to the definitions of the three metrics a larger value in net ner and eef indicate a better effect of da on reducing errors when net ner and nef are greater than 0 it indicates that da improves the accuracy of the discharge simulation when ner and nef are close to 100 it indicates that the simulated discharges of the enkf experiment is close to the observed discharges 4 results 4 1 evaluation of remotely sensed and modeled soil moisture in the hanjiang watershed to evaluate the smsatellites data and modeled soil moisture in the hanjiang watershed three statistical metrics including bias rmse and correlation coefficient r were used to compare their quality to observed soil moisture the modeled soil moisture was simulated by the wrf hydro model with the input data in advance of ten days and with default parameter values bilinear interpolation was applied to the smsatellites data and modeled soil moisture match with the locations of the nine stations to produce the site level values making them comparable to the site level observed data of these stations fig 3 shows ten day soil moisture series of the three datasets i e observations smsatellites and modeled soil moisture from 2013 to 2014 at nine stations of the hanjiang watershed overall smsatellites is able to capture the observed soil moisture dynamics at the nine stations fig 3a i comparing to the observed soil moisture all r values of smsatellites are equal to or higher than 0 30 but those of the modeled soil moisture are less than or equal to 0 30 fig 3a i furthermore the rmse and bias values of smsatellites are within 0 10 m3 m3 fig 3a i in contrast the rmse and bias values of modeled soil moisture range between 0 03 and 0 14 m3 m3 these results clearly demonstrate that the smsatellites dataset has a higher accuracy than modeled soil moisture in the hanjiang watershed fig 3 in terms of the seasonality there is no obvious seasonality in the three soil moisture datasets fig 3 in terms of bias both smsatellites and wrf hydro soil moisture tend to overestimate soil moisture fig 3 soil types in the hanjiang watershed are mainly clay clay loam loam sandy loam and loamy sand soil types of the two stations are silt loam there are four types of land covers including forest grassland farmland and bush accounting for 68 25 5 and 2 of the hanjiang watershed respectively in addition there is also large variability in terrain across this region since many spatially varying factors such as soil type land cover geography geomorphology and human activity can affect the accuracy of remotely sensed soil moisture it is not surprising that smsatellites shows certain variable accuracy among the nine stations however smsatellites has an obvious better accuracy than wrf hydro simulations which validate our idea to improve the wrf hydro simulation by assimilating smsatellites fig 4 summarizes the spreads of the three statistical metrics for smsatellites and wrf hydro soil moisture in terms of bias both datasets tend to overestimate soil moisture but smsatellites has smaller biases with a mean value of 0 06 m3 m3 and a narrower bias spread than the modeled soil moisture which has a mean bias of 0 08 m3 m3 fig 4 these results indicate that smsatellites is more accurate than modeled soil moisture in terms of rmse smsatellites has smaller values and a narrower spread than modeled soil moisture rmse of smsatellites ranged between 0 01 and 0 09 m3 m3 whereas that of modeled soil moisture varied from 0 03 to 0 14 m3 m3 fig 4 these results also suggest that smsatellites performs better and is more accurate than modeled soil moisture finally smsatellites is generally highly correlated with the observed soil moisture across the hanjiang watershed the r values for smsatellites vary from 0 30 to 0 46 while modeled soil moisture values are between 0 2 and 0 30 fig 4 in summary smsatellites has better values for all three metrics comparing to the modeled soil moisture suggesting that smsatellites indeed performs better than the modeled soil moisture to further evaluate the two soil moisture datasets we compared smsatellites and modeled soil moisture against the observations through a scatter density plot the r value of the smsatellites is 0 29 while that of modeled soil moisture is 0 05 fig 5 the rmse and bias values of smsatellites are both less than these of modeled soil moisture fig 5 this comparison also shows that smsatellites has a better performance in the hanjiang watershed figs 3 5 proving the validity of assimilating smsatellites into the wrf hydro model 4 2 stable warming ups of wrf hydro model in the daheba watershed based on the observed discharges from 2013 to 2014 we selected three flood events that have discharges exceed the warning discharges 1000 m3 s for investigating the da of smsatellites in the ungauged daheba watershed the durations of the three flood events 2013071500 2013072200 and 2014090700 are 144 h 168 h and 480 h respectively flood simulation and forecasting are affected by many factors especially the forcing data the antecedent precipitation and state of the watershed at the beginning of a flood event in a model directly affect the accuracy of flood simulation and forecasting to find a steady initial state of the wrf hydro model before conducting smsatellites assimilation the three flood events are simulated from 0 to 15 days in advance fig 6 shows the three evaluation metrics dp dv and dc of 16 simulated results by wrf hydro for the three flood events at the daheba hydrological station the dp values of the three flood events range from 12 to 71 from 1 to 85 and from 10 to 30 respectively fig 6a the dp values of the 2013071500 and 2013072200 events have a larger variability than those of the 2014090700 event fig 6a as shown in fig 6a dp values of the wrf hydro simulations for all the three events generally decrease as warming up period increase when the warming up period is less than 10 days as the warming up period further increases beyond 10 days the wrf hydro dp values for two flood events even flip to increase fig 6a it is clearly that the wrf hydro dp values reach a stable stage when warming up period falls between 8 and 10 days similarly the wrf hydro dv and dc values also reach their stable stages when warming up period is between 8 and 10 days fig 6b and c these results clearly show that there exist indeed stable warming up periods for the wrf hydro model in our study cases unlimited extending warming up period cannot warrant enhancing performance of the wrf hydro model therefore we chose 10 days as the warming up length for the wrf hydro model in the analyses 4 3 evaluation of the ol and enkf experiments in daheba watershed fig 7 shows the time series of the observed and simulated discharges in the sim ol and enkf experiments for the three flood events at the outlet of the daheba watershed as shown in fig 7 the ensemble means of the ol and enkf experiments are in a better agreement with the observations than the sim results for all of the three flood events the model rmse values for the sim results the ensemble means of the ol experiment and the ensemble means of the enkf experiment range between 153 83 and 180 52 m3 s between 106 10 and 183 02 m3 s and between 87 37 and 156 23 m3 s respectively for the three flood events relative to the ol experiment the enkf experiment can further improve the model accuracy fig 7 for example the net ner and nef metrics of the enkf experiment for the 2013071500 event are 18 m3 s 17 and 32 respectively while these metrics are 29 81 m3 s 19 49 and 35 17 for the 2013072200 event and 15 69 m3 s 17 73 and 32 20 for the 2014090700 event these results show that the enkf da of smsatellites has clearly reduced errors in the wrf hydro model relative to the ol experiment in addition the ensembles of the ol experiment for all of the three flood events have larger spreads the blue shades in fig 7a c than those of the enkf experiment the red shades in fig 7a c clearly the simulated discharges of the enkf experiment are substantially better than the sim and ol counterparts particularly during the simulation period with da fig 7 which proves that the smsatellites assimilation can robustly improve the accuracy of flood simulation and forecasting in the daheba watershed three other evaluation metrics including dp dv and dc are also typically used to evaluate the simulation effects of flood simulation and forecasting to compare the results of the ensemble members of the ol and enkf experiments we further quantified the spread of the three statistical metrics of the ol and enkf ensemble members the dp values of the ol members for the 2013071500 event fall between 18 and 31 with a mean value of 31 while the dp values of the enkf members for the same event are all less than 20 with a mean value of 13 fig 8 a this illustrates that the smsatellites assimilation reduces the absolute relative error of the flood peak the averaged dv values of the ol and enkf members for the 2013071500 event are 24 and 19 respectively fig 8a the dc values of the enkf members for the 2013071500 event are all larger than 0 87 with a mean value of 0 91 fig 8a in contrast the dc values of the ol members for the 2013071500 event vary between 0 82 and 0 93 with a mean value of 0 86 fig 8a the discharge hydrographs of the enkf experiment fit better with the observed hydrograph for the 2013071500 event figs 7a and 8a indicating that the smsatellites assimilation leads to improved flood simulation the enkf experiment results for the 2013072200 and 2014090700 events are also better than the ol experiment results fig 8b and c indicated by smaller and narrower dp and dv values and larger and narrower dc values these results also indicate that the simulated hydrographs of the enkf experiment generally agree better with the observations than the ol experiment fig 7b and c in summary the enkf experiment has better results than the ol experiment proving that the enkf da of smsatellites improve the model performance relative to the simulations without da 4 4 hydrological evaluation of assimilating the smsatellites into the wrf hydro model to quantitatively describe the results of the wrf hydro model through the smsatellites assimilation the ensemble means of the simulated discharges obtained by the ol and enkf experiments are regarded as the final simulation results fig 9 shows the observed discharge hydrographs grey area the ensemble means of the ol experiment blue line and the ensemble means of the enkf red line experiment for the three flood events we quantitatively evaluated the simulated discharges through the dp dv and dc metrics the discharge hydrograph of the enkf experiment is basically consistent with the observed hydrograph at the starting and raising phases whereas the simulated discharges of the enkf experiment are slightly larger than the observed discharges in the decreasing phase of the 2013071500 event fig 9a in contrast the simulated discharges of the ol experiment are generally lower than the observed discharges during the decreasing phase fig 9a the simulated flood peaks in the ol and enkf experiments are lower than the observed flood peak for the 2013071500 event but the enkf experiment result is apparently closer to the observation than the ol one fig 9a the dp value of the enkf experiment is smaller than that of the ol experiment while the absolute value of dv is also reduced from 21 to 16 and dc is slightly enhanced as well when da is applied for the 2013072200 flood event the dp value of the enkf experiment 0 17 is lower than that of the ol experiment 0 30 indicating that the smsatellites assimilation has effectively improved the flood peak simulation and reduced the flood peak error fig 9b the dc value of the enkf experiment 0 88 is also larger than that of the ol experiment 0 82 although the smsatellites assimilation does not improve the simulation accuracy for flood volume of the 2013072200 flood event it has effectively improved the simulated discharge process and peak flow the 2014090700 flood event has multiple events and associated flood peaks fig 9c from the last flood peak to the end of this flood event the discharge hydrographs of the ol and enkf experiments are both in good agreement with the observed hydrograph likely due to the basin wide saturated conditions resulted from long lasting rainfall the discharge hydrograph of the enkf experiment still fits better with the observed discharge hydrograph for the second flood peak comparing to the ol experiment fig 9c statistically the smsatellites assimilation has led to reduced dp and dv values and increased dc value indicating that the smsatellites assimilation has effectively improved the simulation of the flow process and improved the simulation accuracy for the 2014090700 flood event in summary the smsatellites assimilation can substantially improve the accuracy of simulated discharges for the three flood events obviously da of surface soil moisture is effective and robust for improving the accuracy of flood simulation and forecasting in our case study to further quantify the assimilation resulted improvement on flood simulation we calculated the net ner and nef metrics for all of the three flood events by comparing the enkf ensemble mean and the ol ensemble mean as shown in table 3 all rmse values of the three flood events has been decreased through the smsatellites assimilation especially for the 2013072200 event the net ner and nef values are all greater than 0 table 3 indicating that assimilation has a positive effect on flood simulation the ner and nef of the 2013071500 and 2013072200 events are larger than 13 while the smallest value of ner is for the 2014090700 event and is still greater than 6 table 3 once again table 3 shows that the smsatellites assimilation has improved the accuracy of flood simulation in the ungauged daheba watershed implying the potential utility of multi satellite retrieved composite soil moisture for improving model flood forecasting especially in the ungauged areas to further figure out whether da of soil moisture can improve flood simulation for any discharge magnitudes we divided the discharges into the low medium and high magnitudes i e 0 100 m3 s 100 1000 m3 s and 1000 m3 s we then compared the results with and without the smsatellites assimilation and computed the net ner and nef for the three discharge magnitudes as shown in table 4 the net ner and nef metrics also increase as discharge magnitude increases indicating that the da benefits more for the larger discharge table 4 the net ner and nef metrics are even negative for the low discharge magnitude table 4 indicating that da even leads to degraded simulations on the low discharge range however the smsatellites assimilation leads to a large improvement for the medium and high discharge ranges additionally the net ner and nef metrics for the high discharge range are greater than those for the middle value section table 4 indicating that da benefits more for the high value floods low discharge values occur on the prior rising and late falling stages of floods so da of soil moisture is not necessary on these stages on the prior rising stage soil is relatively dry in both model simulation and satellite retrieval and da of soil moisture has little effect on discharge simulation on the late falling stage soil is saturated thus da of soil moisture is ineffective especially during the receding phase of floods for medium and high ranges of discharge the effect of soil moisture assimilation on discharge simulation is more sensitive than these low value ranges fig 7 namely the ensemble range of hydrograph has a more fluctuation when the discharges are more than 100 m3 s 5 discussion and conclusions this study aims to assimilate remotely sensed soil moisture by multiple satellites to improve the accuracy of flood simulation and forecasting we established a da scheme for the wrf hydro model to assimilate the surface soil moisture jointly retrieved by multiple microwave satellites based on the enkf algorithm we then tested the framework in the ungauged daheba watershed and analyzed how effective this da framework and its functions for different magnitudes of flow processes comparing to the pure model simulation the sim experiment and ensemble model simulation resulted from parameter perturbations the ol experiment the ensemble model simulation with da the enkf experiment have largely improved the flood simulations figs 7 9 this study provides a valuable reference for improving the accuracy of flood simulation and forecasting by assimilating remotely sensed soil moisture in the small and medium sized watersheds without gauged soil moisture data it can also play a positive role on promoting the application of remote sensing data in flood simulations and forecasting nevertheless study on the model data assimilation of remotely sensed soil moisture is still a complicated issue the effectiveness of da is not only affected by the quality of remotely sensed soil moisture data but also related to various factors such as uncertainties in the models structure and parameters the methods to measure errors in the modeled and observed values bias correction methods of remotely sensed soil moisture and the assimilation scheme among the others crow and ryu 2009 we evaluated the smsatellites and modeled soil moisture by comparing with the observed soil moisture at nine stations in the hanjiang watershed although the smsatellites performs better than modeled soil moisture a large room remains for further improving the soil moisture retrieval the hanjiang watershed originates from the qinling mountains as the dividing boundary between the north and south of china the rugged terrain features steep brooks and streams large variability in topography and climate zones can affect the accuracy of soil moisture retrieval wang et al 2020 zhang et al 2019b the type density structure and composition of vegetation are critical for the microwave remote sensing of soil moisture lakhankar et al 2009 wigneron et al 2017 many parameters of the wrf hydro model have a physical basis and can be determined by soil texture and vegetation type for example the zsoilfac parameter directly has an important effect on modeled soil moisture of different soil layers and then has an impact on the simulated flood volume the refkedt parameter is related to the infiltration and permeation rates which mainly affect runoff generation the ovroughrtfac and mannfac parameters influence the overland runoff and streamflow selection of the hydrological model parameters for perturbation and the optimization schemes directly affect the simulation results and effectiveness of da therefore it is important to conduct analyses on the sensitivity uncertainty and optimization of model parameters prior conducting da sun et al 2020 the model parameters can also impact the effectiveness of the soil moisture assimilation and the resultant streamflow simulations baguis and roulin 2017 alvarez garreton et al 2014 found that parameter optimization can affect the assimilation performance by assimilating satellite surface soil moisture into the probability distributed model pdm to improve the accuracy of the runoff forecast the interpolation of meteorological data in time and space may cause some uncertainty in hydrological modeling accadia et al 2003 compared bilinear interpolation and a simple nearest neighbor averaging method used for interpolating precipitation and found that bilinear interpolation smoothed the maxima and increased the minima of precipitation when the data was not sufficient an interpolated precipitation from station observation and satellite in mountainous areas overestimated the observed precipitation especially in late spring and early summer hu et al 2016 six geo spatial interpolation methods four of which are based on bilinear interpolation were used to transform multi sensor source data to another spatial resolution showing biases caused by these interpolation methods teegavarapu et al 2012 although interpolation methods can produce some uncertainty ebrahimi et al 2017 the bilinear interpolation can obtain smoother spatial and temporal distributions and smaller data variability than many other interpolation methods chen et al 2021 wang et al 2021 although the bilinear interpolation like any other interpolation method has its own imitation it is a widely used spatial interpolation method with a relatively strong mathematical basis our results demonstrate that assimilation of remotely sensed soil moisture has a potential value on improving the simulation and forecasting of the wrf hydro model indicating that the da technology and the assimilation of multi source satellite remote sensing data can be used in flood simulation and forecasting and help to improve the accuracy of the simulated rainfall runoff processes the wrf hydro model has a potential to simulate and forecast floods at global and regional scales lin et al 2018a 2018b 2020 the application of multi source satellite remote sensing data for the wrf hydro model can be used to solve the flood simulation and forecasting of watersheds with limited data or without observed data author contributions ke zhang and lijun chao designed this study lijun chao and ke zhang set up the models and method lijun chao ke zhang zhao gu and sheng wang conducted this study lijun chao and ke zhang wrote the manuscript ke zhang and lijun chao acquired the funding junzeng xu and hongjun bao contributed to the revision data availability the satellite precipitation data are available from cmorph climate data record https www ncei noaa gov data cmorph high resolution global precipitation estimates access the geographic data are available from shuttle elevation derivatives at multiple scales hydeosheds database http www hydrosheds org the land use and soil data are available from the wrf preprocessing system wps static data https www2 mmm ucar edu wrf users download get sources wps geog html the input meteorological data excepting for precipitation are available from gldas noah land surface model l4 3 hourly 0 25 0 25 v2 1 https disc gsfc nasa gov datasets keywords gldas page 1 the wrf hydro model can be found online at https ral ucar edu projects wrf hydro overview the assimilation algorithm code to this article can be found online at https github com doudouyl assimilation algorithm v2 0 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this study was supported by the national natural science foundation of china 52009028 and 51879067 national key research and development program of china 2018yfc1508101 china and jiangsu province postdoctoral science foundation 2020m671323 and 2021k572c natural science foundation of jiangsu province bk20180022 the fundamental research funds for the central universities of china b200204038 b210202115 and b220203051 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105421 
25578,eutrophication represents an important ecological and environmental issue in coastal lagoons this paper presents an extensive study of recurrent cell and network architectures to model eutrophication processes in the venice lagoon a very complex and fragile ecosystem that has been strongly altered by anthropic activities over years experimental results showed that recurrent models outperformed random forests rf significantly on two datasets performing similarly to cnns on one of the datasets while outperforming cnns on the other one additionally the transferability potential of the trained models was investigated the empirical analysis has shown that recurrent neural network models with lower computational complexity provide the highest eutrophication prediction accuracy when their trained models were tested on a new dataset designed models represent effective tools for early warning eutrophication prediction that can support the implementation of relevant eu acquis eu marine strategy and water framework directives and achievement of their environmental targets keywords water quality assessment eutrophication prediction and modeling recurrent neural networks machine learning neural networks venice lagoon 1 introduction covering approximately 13 of the world s coastline and 5 3 of the european coastline coastal lagoons are shallow inland waters separated from the sea by one or more barriers and meanwhile connected to the sea by restricted inlets that allow for water exchange phleger 1969 they have important ecological value as they are suitable habitats for wetlands mangroves salt marshes and they maintain rich biodiversity for example they are home to bivalves crustaceans fishes and birds newton et al 2018 they are also among the most socio economically important ecosystems on earth as they have been valuable natural regions for fisheries and various forms of aquaculture since ancient times they have been alluring environments for human activities such as saltworks and leisure activities and they support tourism development jimeno sáez et al 2020 however as they are located at the land sea interface they are highly threatened by anthropogenic pressures from both land e g input of nutrients from industrial urban and agricultural activities and the sea e g tides storm surges sea pollution and erosion moreover other pressures arise from climate related drivers e g changes in water temperature precipitation patterns and sea level all these threats lead to severe degradation in water quality wq due to high turbidity acidification anoxia i e low dissolved oxygen od and eutrophication processes lloret et al 2008 among the water pollution issues mentioned above eutrophication is one of the most common phenomena in coastal lagoons sanderson and geoffrey 2010 it is a result of increased land inputs of nutrients such as nitrogen and phosphorus in the water causing structural changes in the ecosystem such as abnormal production of phytoplankton algae and bacteria which in turn leads to deterioration in wq and losses of biota and biological communities increased nutrients are eventually manifested by an extreme increase in the chlorophyll a chl a levels furuya et al 2018 lin et al 2018 studies on chl a nutrient relationships have a long history sakamoto 1966 dillon and rigler 1974 mccauley et al 1989 elser et al 2007 bracken et al 2015 have shown the importance of total phosphorus tp and total nitrogen tn as predictors of chl a supporting the view that tp more frequently limits the production of phytoplankton biomass but tn may colimit phytoplankton biomass under certain conditions e g tn has a large influence on chl a at high tp furuya et al 2018 predicting wq changes with high accuracy in these fragile natural ecosystems is crucial to avoid the mentioned undesirable side effects of wq deterioration and major economic costs for the entire community vinçon leite and casenave 2019 smith and schindler 2009 at the same time understanding and modeling changes in wq is essential to plan sustainable long term strategies and well designed management schemes that are robust enough to cope with cumulative pressures and changes over time of key variables underpinning environmental dynamics from simpler indicators i e index based furlan et al 2019 and statistical methods dimberg et al 2013 to more complex decision support systems torresan et al 2016 and mathematical models fornarelli et al 2013 kim and seo 2017 kim et al 2014 lopes et al 2008 many different methods have been used for eutrophication assessment so far by the research community for example dimberg et al 2013 adopted discrete markov chains to estimate the risk of high chl a concentrations and algal blooms in a lake environment kim and seo 2017 employed exploratory factor analysis efa and empirical orthogonal function eof patterns of principal component analysis pca for the water quality assessment in the monitoring network of nakdong river korea kim et al 2014 used fuzzy models for forecasting the algal blooms behaviors lopes et al 2008 employed differential equations that characterize the chemical and biological state of the coastal waters to simulate the water quality of the lagoon the stunning performance of machine learning approaches on a wide variety of research problems has encouraged environmental science researchers since the 2010s to employ machine learning ml models for wq assessment and particularly for eutrophication modeling and prediction in this context millie et al 2012 proposed an artificial neural network ann based technology called as grey box for modeling microalgal abundance alameddine et al 2011 used bayesian networks to predict chl a levels given the nutrient concentrations water temperature and the river flow regime more recently a wider variety of ml models have been employed by the researchers as presented in a detailed table in appendix a in this context some researchers employed shallow machine learning methods e g decision tree dt ho et al 2019 random forest rf tong et al 2019 and support vector regression svr jimeno sáez et al 2020 while many others used various types of artificial neural networks ann such as multilayer perceptron mlp ahmed et al 2019 long short term memory lstm hu et al 2019 barzegar et al 2020 recurrent neural networks rnn hu et al 2019 and hybrid approaches e g cnn lstm barzegar et al 2020 more specifically using mlp and rbf methods for the prediction of wq parameters connected with eutrophication processes e g od nutrients chl a for a river basin ahmed et al 2019 dealt with the noise in data by employing an augmented wavelet denoising method namely neuro fuzzy inference system hu et al 2019 adopted a recurrent architecture i e employing the correlation priors between the parameters they used lstm and rnn to predict ph and water temperature for water quality assessment in a smart mariculture environment a number of studies applied hybridized models to improve the prediction accuracy of the overall system by 1 extracting higher quality features in advance e g li et al 2019 employed sparse auto encoder sae and barzegar et al 2020 employed a cnn to learn higher quality features which are then used for water quality prediction by lstm 2 tuning the hyperparameters of a ml method e g nieto et al 2019 proposed an svm model the hyperparameters of which was tuned by an artificial bee colony algorithm to predict chl a and total phosphorus for wq assessment 3 denoising the data initially e g lu and ma 2020 employed a denoising process by decomposing data using ceemdan initially and performed prediction using extreme gradient boosting xgboost and rf 4 combining predictions of a number of ml methods e g li et al 2019 proposed fusing predictions obtained from three rnn variants i e simple rnn lstm and gru and bui et al 2020 proposed to combine predictions of rf m5p random tree rt and reduced error pruning tree rept using bagging ba cv parameter selection cvps and randomizable filtered classification rfc or 5 applying attention mechanism to take account the spatial and temporal relations e g liu et al 2019 proposed an attention rnn model which takes account the spatial and temporal correlations for do prediction aimed at analyzing anoxia eutrophic systems among the relevant research effort so far eutrophication prediction in line with water quality assessment on the coastal lagoon scenario was only studied by a recent work jimeno sáez et al 2020 where an ann and an svr were evaluated on a limited collection of data including 126 data samples however tackling water quality assessment problem in such complex and dynamic ecosystems can still benefit from more sophisticated machine learning approaches this paper deals with the water quality deterioration problem in the venice lagoon which is a very complex and fragile natural and cultural environment that has been strongly modified by anthropic activities for years guerzoni 2006 solidoro et al 2010 this shallow coastal lagoon is a eutrophic system runca et al 1996 because excessive algal blooms have been frequently observed facca et al 2014 sfriso et al 2019 as a result of excessive discharge of phosphorus and nitrogen from the close urban agricultural and industrial areas 1 1 ufficio di piano attività di salvaguardia di venezia e della sua laguna lo stato ecologico della laguna rapporto tematico 2008 more specifically the present analysis is based on the estimation of chl a concentrations which is an eu water framework directive surrogate indicator for atypical algal productivity as a function of historical information of itself and a number of other chemical and physical environmental variables namely water temperature turbidity salinity and dissolved oxygen do these variables have an interactive relationship with each other and with chl a and changes in them can contribute in concert to algal abnormal growth lloret et al 2008 it is well known that nutrients are important parameters defining eutrophication bracken et al 2015 elser et al 2007 however in the venice lagoon case study the nutrients parameters were measured seasonally so their temporal dimension is very scarce there are only 32 observations for nutrients from 2011 to 2019 on the contrary the physico chemical variables water temperature do salinity and turbidity were measured with very high temporal resolution hourly from 2008 to 2019 this leaves us with the challenge of modeling eutrophication with only the physico chemical parameters and we aim to take the advantage of the fact that the machine learning performs well with a huge amount of data additionally the fact that the influence of water temperature turbidity salinity and do on chl a has already been addressed by literature works strengthens our motivation to use these parameters water temperature has been used to model eutrophication in various previous works barzegar et al 2020 dunstan et al 2018 jimeno sáez et al 2020 kong et al 2017 as high temperatures exacerbate algal growth and subsequently contribute to algal bloom events owens 2001 mentions that an increase in salinity in aquatic ecosystems affects most plants and causes ionic and osmotic stresses and al taee and aqeel 2018 demonstrates that increases in salinity immediately reduce net carbon fixation rates and affect photosynthetic pigments chlorophyll and carotenoids do and turbidity are related to chl a particularly during eutrophication events when algal blooms eventually die microbial decomposition severely depletes dissolved oxygen do creating a hypoxic or anoxic dead zone lacking sufficient oxygen to support most organisms dead zones are found in many water bodies during the summer furthermore such hypoxic events are particularly common in marine coastal environments surrounding large nutrient rich rivers chislock et al 2013 then turbidity is another potential impact of eutrophication on water quality increased turbidity and decreased water clarity occur during these events and water becomes cloudy and colored green and brown which reduces the ability of fish to see prey and detect predators smith and schindler 2009 boqiang et al 2013 as a preliminary study different variable selection strategies for the mentioned task were empirically explored see in appendix b and then proceeded with the best performing multivariate approach in which historical values of all five variables were used to estimate the current values of chl a following a multi variate approach is important to understand and model spatio temporal changes in water quality accounting for the complex interactions among natural and human made pressures indeed recent literature studies namely barzegar et al 2020 garcı a nieto et al 2018 nieto et al 2019 kong et al 2017 wang et al 2021 follow the multivariate approach for eutrophication prediction experiments were primarily conducted on a time series data for the specified five variables acquired from an in situ monitoring station at palude maggiore in venice lagoon shown in fig 1 the experimental dataset i e 64 311 observations of the five specified parameters used in this work is much larger than those used by literature studies please see table a1 in appendix a and covers a much longer time frame i e hourly observations from 2008 to 2019 the presence of high temporal resolution and abundant data allows us to work on a more realistic scenario and train machine learning models more robustly and accurately starting from the fact that no sophisticated machine learning approaches have been adopted for wq assessment in such complex ecosystems so far the present work investigates the potentials of deep neural networks with memory i e more specifically a set of recurrent neural networks rnn variants which is a type of neural network involving temporal dynamics in line with the aforementioned objective rnns take into account time dependence i e they involve sequential processing of data and retain historical information this coincides with our case as we are dealing with a time series dataset literature works have employed either shallow ml methods or feedforward neural networks or unidirectional recurrent neural networks including rnn and lstm cell architectures in either standalone or hybrid design with a cnn e g cnn lstm barzegar et al 2020 in a variety of environment scenarios differently from the existing literature this work explores a larger variety of models including the most well accepted rnn cell architectures i e standard rnn lstm and gru within both unidirectional and bidirectional network architectures and hybrid designs i e cnn rnn cnn lstm and cnn gru performances were compared with a cnn and a shallow ml method namely random forest rf that was heavily adopted by literature studies our work is included in table a1 appendix a to provide a comparative view of relevant research efforts to date flow of the paper is as follows section 2 introduces the characteristics of the study area and specifications of the dataset used in the present work section 3 provides the methodological details and the experimental evaluation discussion and conclusion are presented in sections 4 5 and 6 respectively 2 case study area venice lagoon is a wide shallow and transitional water body in the northern part of adriatic sea location 45 n 12 e having a mean depth of 1 0 3 m and covering 550 km2 area venice lagoon is the largest lagoonal system in italy and one of the largest in mediterranean sea as well facca et al 2014 the edges of the lagoon form a clearly delineated man made perimeter a wide expanse of reclaimed land used for industrial and residential purposes fish farms dumps and landfills as well as dikes and drainage canals border the lagoon to the west and northwest suman et al 2005 two barrier islands heavily defended by seawalls lido and pellestina separate the lagoon from the adriatic sea to the east water exchange occurs through three large entrances lido malamocco and chioggia adriatic tides govern water exchange in venice lagoon and have a mean amplitude of 0 2 m during neap tides to 1 m during spring tides the average daily freshwater input to venice lagoon amounts to 2 8 million cubic meters this hydrological pattern creates a typical brackish environment in the lagoon with a salinity gradient that ranges from 10 near the mainland border to 32 at the inlets suman et al 2005 the lagoon encloses the historical city of venice with 50 000 inhabitants and 10 000 000 visitors every year and has been declared as a world heritage by the unesco committee 2 2 decisions adopted by the world heritage committee at its 43rd session in baku republic of azerbaijan 30th june 10th july 2019 paris encyclopedia of tourism july 1025 1026 for its cultural and natural relevance while the venice lagoon has such socio economic importance it is a complex and dynamic system that evolves continuously in response to natural and anthropogenic stressors solidoro et al 2010 in this shallow coastal system one of the most impactful water pollution hazards is eutrophication facca et al 2014 solidoro et al 2010 several natural and anthropogenic factors that endanger the chemical and ecological balance of the lagoon and lead to eutrophication events can be listed as follows 1 pollutants discharge from drainage basin s rivers input çevirgen et al 2020 2 low average depth of the lagoon that yields to accelerated biochemical cycle of the macro and micro nutrients between the sediment and water column and such sudden changes results with an ecotone that is in dynamic and unstable equilibrium ravera 2000 3 extreme storm surge events that effect the sediment movement and water quality deterioration sartori 2012 4 high level of anthropogenic activities i e urbanization population density 3 3 annuario statistico italiano 2019 https www istat it it archivio 236772 very active tourism bagliani et al 2004 industrial activities in the area of porto marghera and other economic activities like aquaculture agriculture and fishing that have direct impact on water quality as detailed in the following subsection data acquired by a sensor in the lagoon named palude maggiore coordinates 1772481 e 5045037 n in the water body named ec was used in this work this case study area whose morphological structure is shown in fig 1 was chosen since it is a good representation of a lagoon area with rich interactions between different natural and anthropogenic morphological structures it has natural and artificial wetlands surrounding natural and artificial mudflats as well as a major lagoon channel of saltwater entering from the lido inlet 2 1 data description this study aims to consider the contribution of factor parameters in estimating the change in chl a values with a multivariate approach as well as benefiting from historical patterns in the data in this sense having abundant amount of data expressing relevant dynamics in a complex environment such as venice lagoon aids to a more robust analysis using deep neural networks with memory herein experiments were performed on a large collection of data for chemical wq parameters such as chlorophyll a chl a fluorescence and dissolved oxygen do and physical wq parameters such as salinity water turbidity and water temperature experimental data consists of hourly records from beginning of 2008 to end of 2019 at one of the detection stations namely ve 8 located at a depth of 1 m under lagoon surface within a monitoring network named samanet developed by venice water authority 4 4 managed by the technical office for the anti pollution of the venice lagoon of the interregional public works department for veneto trentino alto adige and friuli venezia giulia provvv oo pp http solve corila it to monitor the water quality status of the venice lagoon based on the physical chemical and ecological status targets set under the water framework directive 5 5 the eu water framework directive 2000 60 ec integrated river basin management for europe https ec europa eu environment water water framework index en html the data series for five parameters each covering 70 938 time steps from 2008 to 2019 were obtained from palude maggiore no input was recorded for at least one of the parameters at 6627 time steps due to sensor failure since this corresponds to less than 10 of the data precisely 9 34 in the entire timeline the entries at these 6627 time steps were discarded and we proceeded with the remaining 64 311 entries shown in fig 2 the descriptive statistics i e min max mean standard deviation median skewness and kurtosis of the water quality data that is used in this work are presented in table 1 turbidity has the maximum range of values 0 15 448 34 while salinity has the minimum range 15 42 38 62 which gives an idea of the magnitude of variations for these parameters over 11 years that fact is re illustrated by standard deviation sd i e sd was the highest for the turbidity 20 95 even higher than its mean which is 15 01 and the lowest for the salinity 2 50 this can be explained by the fact that in contrast to other variables the variations in turbidity in the water is very sensitive to the daily meteorological events e g turbidity increases significantly in a rainy or windy day since the lagoon has a very shallow water chl a shows much higher skewness and kurtosis than salinity temperature and do which is followed by turbidity it is mentioned in wu et al 2010 that when the values of these statistics i e skewness and kurtosis are outside the range of 2 to 2 their distribution does not follow a normal gaussian distribution specifically according to their skewness values temperature and do have fairly symmetrical data salinity is moderate negative skewed and chl a and turbidity are highly positive skewed that is chl a and turbidity with a few high extreme values are mostly at smaller values chl a followed by turbidity has the highest kurtosis indicating significant extreme values in the distribution temperature has the lowest kurtosis the correlation among the variables is presented in table 2 turbidity and temperature are slightly positively correlated with chl a with 0 32 and 0 18 respectively salinity is slightly positively correlated 0 34 and do 0 25 is slightly negatively correlated with temperature it cannot be said that there is a correlation between other variables these results can be expected because the increase in turbidity and temperature have an impact on the chl a increase at the location and similarly an increase in temperature leads to higher salinity and lower do rohling and bryden 1992 talley 2002 3 recurrent neural networks for eutrophication prediction the present investigation aims to explore the potential of recurrent neural networks for eutrophication and water quality assessment this section presents the motivation behind such a methodological choice and relevant technical explanations initial designs of neural networks were often in a feedforward scheme in a typical feedforward neural network architecture e g convolutional neural networks the connections go only in one direction that is the output of the lower layers e g input layer influences the input of the above layers e g hidden layers and the final decision is obtained in the output of the last layer of the network approximate underlying mapping functions from network inputs to the outputs are learned in training procedure more specifically to carry out the training process a loss function is defined i e that measures how the predicted output deviates from the desired output once the training samples are fed into the network and the weights are updated by backpropagation to minimize the value of the loss function until its convergence when the network is trained it can be used to accomplish the task under consideration for example to perform classification or regression on unseen data such neural networks have no memory i e they work on the assumption of independence among the input samples lipton et al 2016 lipton et al 2015 however when the data points are related in time or space such as in time series feedforward neural networks will have limitations since they do not utilize the sequential dependencies in the input data unlike the feedforward neural networks recurrent neural network architectures are given a form of memory by the newly introduced connection scheme a hidden layer receives input from two layers i e its own output from the previous time step in the sequence and the layer below it in addition to the input vectors weight vectors and output vectors they have hidden states a k a internal states that we do not have in feedforward networks which represent the outputs of the hidden layers and are updated at each time step based on the current input and the hidden state of the previous time step this type of connectionist structure allows the entire history of previous inputs to affect the network output and is very suitable for time series processing which is the case here while earlier recurrent network designs fausett 1994 which we refer to as standard rnns in this paper were designed by such self connected hidden units modern designs such as long short term memory lstm hochreiter and jürgen schmidhuber 1997 and gated recurrent unit gru cho et al 2014 consist of hidden units also called memory blocks or memory cells containing various gate structures that allow them to selectively pass information across subsequent time steps these cell architectures are called as gated rnns in some sources gallicchio et al 2018 having such control over the flow of the information with gated memory cell structures helps track the long term dependencies in the data more effectively first attempts on designing recurrent network architectures were unidirectional i e the sequence data is processed in forward direction that is from past to future schuster and kuldippaliwal 1997 proposed a bidirectional recurrent neural network architecture where the sequence data is processed in both forward and backward direction simultaneously thus it takes context from both the past and the future it is shown in a number of tasks e g speech recognition graves and jaitly 2013 sequence classification breuel 2015 traffic speed prediction cui et al 2020 that with the bidirectional recurrent architectures adopting additional information reversely to the previous time steps improves learning long term dependencies and accuracy of the model however to the best of our knowledge they were never evaluated for water quality assessment recently several literature studies have used a recurrent neural network architecture by hybridizing it with a trained convolutional neural network cnn architecture that is local features from the input data are extracted by convolution operations with a cnn model and then fed into a recurrent neural network architecture to learn temporal dependencies in the sequence cnns are feedforward networks and they process each frame of the input data independently consecutive values within a frame in the time dimension exhibit strong correlation and short variations in the time series data can be captured by convolutional filters lecun et al 1995 it has been shown in various works that feeding such extracted cnn features into an rnn architecture provides performance improvements in various tasks such as activity action recognition hammerla et al 2016 eeg classification bashivan et al 2015 trend prediction in time series lin et al 2017 and water quality assessment in a lake environment barzegar et al 2020 the descriptions of the cell and network architectures of recurrent neural networks which have been widely adopted by relevant literature studies and investigated in this paper for water quality assessment in a complex coastal lagoon environment are introduced in the subsequent subsections 3 1 cell architectures for recurrent neural networks 3 1 1 standard recurrent neural network rnn differently from the feedforward neural networks where information is processed in one direction from input to the output rnns fausett 1994 hochreiter and jürgen schmidhuber 1997 have recurrent loops in them see left hand side of fig 4 that allow information to be processed over time at the time step t rnn receives an input x t and updates the internal cell state h t by applying a function f w that is parametrized by a set of network weights w based on both the previous state from time step t 1 and the current input that the network is receiving i e h t f w h t 1 x t more specifically the operation in eq 1 is performed to update the hidden cell state h t and then the network output is produced by y t ˆ w h y h t where w denotes the weight matrix between two vectors and b denotes the bias corresponding to the subscripts respectively fausett 1994 hochreiter and jürgen schmidhuber 1997 lipton et al 2015 hidden unit in a standard rnn cell is visualized in fig 3 wei et al 2021 1 h t tanh w h h h t 1 w x h x t b h fig 4 illustrates a rolled rnn cell on the left and on the right side its representation of computational graph unrolled across time one khodadadi et al 2022 which can be thought as rnns having multiple copies of the same network where each passes message to the next considering this picture the network can be interpreted as a deep neural network with one layer at a time step and shared weights between time steps the same weight matrices which are actually those parameters learned during training are used at every time step and total loss is obtained by summing up the losses computed at individual time steps the weights are then updated by performing backpropagation through time bptt werbos 1990 in order to minimize the loss while rnns has been used successfully in a number of tasks e g speech recognition miao et al 2015 they suffer from exploding and vanishing gradients problem pascanu et al 2013 that hinders rnns to learn long time dependencies 3 1 2 long short term memory lstm a robust solution to exploding and vanishing gradients problem at rnns is using a more complex recurrent unit i e memory cell including gates that control what information is passed through and which information is used to update the internal state of the recurrent unit having such a control on the flow of the information by such gated memory cells can more effectively track the long term dependencies in the data and overcome the vanishing and exploding gradient problem lstm which was introduced by hochreiter and jürgen schmidhuber 1997 to overcome the aforementioned issue in standard rnns are able to track information throughout many time steps and work quite well on a bunch of different time series prediction tasks e g traffic speed prediction ma et al 2015 air pollutant prediction liu et al 2021 and many others an lstm cell visualized in fig 5 wei et al 2021 consists of three gates namely input output and forget gates that selectively add or remove information to its cell state c t which can be though as the long term memory of the network at a time step t additionally an lstm cell has also a hidden state h t which corresponds to the short term memory gates contain sigmoid activations forcing its input to be between 0 and 1 i e multiplying by 0 causes values to disappear and vice versa lstm process information in four simple steps hochreiter and jürgen schmidhuber 1997 that are i discard the irrelevant history using the forget gate ii store the relevant parts of the new information using the input gate iii use the previous two steps together to selectively update the internal state iv output the filtered version of the cell state to be sent to the next time step using the output gate using such gated cells lstms can specifically learn which pieces of the prior history are important in learning the problem of predicting next items and discard those that are not relevant providing more robust training the key equations of the gates in lstm are as follows wei et al 2021 i t σ w i x t w i h t 1 b i f t σ w f x t w f h t 1 b f o t σ w o x t w o h t 1 b o c t tanh w c x t w c h t 1 b c c t f t c t 1 i t c t h t o t t a n h c t y t ˆ h t where i t f t o t c t c t and h t denote input forget output gate cell state candidate cell state and hidden state respectively at time step t w and b denote the weight matrices corresponding to the current input i e related gate and the bias vectors σ is the logistic sigmoid function i e σ x 1 1 e x which outputs values in the range 0 1 hochreiter and jürgen schmidhuber 1997 3 1 3 gated recurrent unit gru proposed by cho et al 2014 gru has been popular due to its simplicity and fast computation gru which is visualized in fig 6 wei et al 2021 is mainly simplified with only two gates i e namely reset gate and update gate and similar to the standard rnn it has only one hidden state h t update gate works like a combined version of the input and forget gates of lstm reset gate controls how much of the previous hidden state contributes to the candidate state at the current time step the functionality of the gru cell is as follows wei et al 2021 u t σ w u x t w u h t 1 b u r t σ w r x t w r h t 1 b r h t t a n h w r t h t 1 w x t h t 1 u t h t 1 u t h t where u t r t h t h t denote the update and reset gates and candidate and current hidden states respectively at time step t 3 2 recurrent neural network architectures 3 2 1 unidirectional and bidirectional architectures any of rnn cell architectures presented in section 3 1 can be used in a unidirectional or bidirectional recurrent neural network architecture as illustrated in fig 4 a unidirectional rnn architecture preserves data only from the past on the other hand in a bidirectional rnn architecture schuster and kuldippaliwal 1997 the sequence data is processed both forward and backward to simultaneously gather information from the past and future adopting additional information reversely to the previous time steps improves learning long term dependencies and accuracy of the model on the other hand a limitation of bidirectional recurrent architectures is that the entire sequence is needed beforehand to make the predictions thus these architectures are not suitable for use in an online learning setting where it is assumed that it is not possible to have prior information about the future lipton et al 2015 however this is not the case in the present study a number of literature works have reported superiority of the bidirectional recurrent architectures over the unidirectional ones for the time series forecasting problem siami namini et al 2019 althelaya et al 2018 as the architecture of bidirectional rnns schuster and kuldippaliwal 1997 is shown in fig 7 the data sequence is processed in forward and backward directions with two separate hidden layers i e each is connected to both input and output then two hidden layers are concatenated to produce the final output the following equations describe a bidirectional rnn lipton et al 2015 h t tanh w h h h t 1 w x h x t b h z t tanh w z z z t 1 w x z x t b z y t ˆ w h y h t w z y z t b y where h t and z t denote the hidden states in forward and backward direction respectively 3 2 2 hybrid architectures in the present paper hybrid models by integrating a cnn to a unidirectional recurrent neural network architecture were developed more specifically features were extracted by a trained 1d cnn on the input data and fed into a type of recurrent neural networks for learning temporal dependencies in the sequence the main idea in such a cascaded design is to get benefit of both models as cnn models may extract useful knowledge in the data and rnn models efficiently capture the sequence information cnn is composed of a sequence of convolution and pooling layers i e more specifically the convolution operation is applied on the raw input data using convolutional kernels which is then followed by a nonlinear activation function e g rectifier linear unit and a pooling layer 4 experiments the performances of the recurrent models mentioned in the previous section were compared with a feed forward architecture i e cnns and a shallow ml method i e random forest rf breiman 2001 rf which conducts ensemble learning through constructed decision trees in the training period was involved since it has been heavily adopted by literature studies on water quality assessment tong et al 2019 a multivariate approach was followed in all experiments as it performed best among all other input selection strategies as presented in appendix b 4 1 implementational details performance metrics for a comprehensive analysis performances of the different methods were evaluated quantitatively by a set of evaluation metrics shown in table 3 and qualitatively with plot graphs briefly pearson s correlation coefficient 0 r 1 shows the strength of the linear relationship between the predicted and observed chl a values mean absolute error 0 mae measures the performance errors accordingly accuracy of the trained models i e smaller the mae the better the prediction accuracy percentage of bias pbias gupta and sorooshian 1999 expresses whether the predicted time series by the trained model consistently over estimates pbias 0 or under estimates pbias 0 the observed time series nash sutcliffe efficiency coefficient e ns 1 nash and sutcliffe 1970 is the error variance of the predicted time series divided by the variance of the observed time series accordingly e n s 1 indicates a perfect fit as a result of the predicted time series error variance being 0 values between 0 0 and 1 0 are considered acceptable performance levels while values less than 0 0 indicate unacceptable performance demonstrating that the mean of the observed time series is a better predictor than the developed model itself moriasi et al 2007 finally proposed by willmott 1981 higher values of wilmott s index of agreement 0 wi 1 indicates better agreement between the predicted and observed time series in the testing phase with optimal level at w i 1 these metrics were also used in a recent relevant work barzegar et al 2020 on chl a prediction for water quality assessment additionally we derived a metric named as eutrophication recall score ers to measure eutrophication prediction accuracy specialized for our venice lagoon case study while the aforementioned metrics cumulatively evaluate prediction performance over all observations which is dominated by massive low valued observations we aim to evaluate prediction accuracy for observations with values above a certain threshold indicating the presence of eutrophication with ers the threshold value is decided as t 30 based on the work of in bonometto et al 2019 see appendix b in bonometto et al 2019 that reports no quality value with chl a greater than 30 μg per liter μg l corresponding to phytoplankton bloom condition i e eutrophication in venice lagoon once thresholding both observations and a model s predictions ers computes recall score that is number of predicted values higher than t divided by number of all observed values higher than t thus for all the observations who actually indicate eutrophication i e the ones higher than the eutrophication threshold t ers shows how many we correctly identified as eutrophication apart from such quantitative evaluation metrics we also provide the scatter plots and box plots of the observed and predicted chl a values in the testing phase which allows us to observe how accurately the predicted values approaches to the observations model implementation all experiments were performed on the dataset with 64 311 entries introduced in section 2 2 for the experiments of each prediction model the dataset was partitioned into two subsets such that the initial 80 of the entire dataset records from 01 01 2008 to 31 05 2017 is used in the training and remaining 20 records from 31 05 2017 to 31 12 2019 is used in the testing phase models were developed using the training dataset and the performance of the developed models were evaluated on the test dataset values of all variables were scaled between 0 and 1 using minimum maximum scaling technique pedregosa et al 2011 what follows is the training and testing datasets were transformed into a supervised learning scheme similar to barzegar et al 2020 where each entry in the time series is converted to an input output pair i e the lag observations at time t 1 constructed the input and the current observation at time t constructed the output the performances of different models using other lag times e g t 2 and t 3 were checked as well however the performances were similar to or worse than using t 1 thus we proceeded by using the lag time t 1 for the input variables when developing the prediction models such learning scheme has been adopted for all the models developed in this paper for both recurrent and convolutional neural network methods different depth of architectures with different numbers of units or neurons were tried and further advanced with the one providing the best performance in chl a prediction more specifically in the development of rnn lstm gru birnn bidirectional rnn bilstm bidirectional lstm and bigru bidirectional gru models we used an architecture with one hidden layer having 50 units which is followed by a dropout layer with a rate of 0 001 to avoid overfitting to the training set and a fully connected layer with a unit of 1 in sequence the architecture of the cnn model composed of a convolutional layer conv1d with 32 filters a kernel size of 3 causal padding type and an elu activation followed by a max pooling layer with a pooling size of 3 and same padding type a dropout layer with a rate of 0 001 and a fully connected layer were used afterwards in the implementation of hybrid methods i e cnn rnn cnn lstm and cnn gru the same cnn architecture was coupled with the relevant aforementioned recurrent architecture all the neural network models were compiled with a mae loss function using the adam optimizer with a learning rate of 0 0001 for the training of each method different batch size values were tried and the one providing minimum mae on the test set was chosen the models were trained using 1000 epochs to guarantee the convergence in the training the loss plots for all models are presented in fig 8 it is seen in fig 8 that the loss curves of rnns converge much faster than those of the other models in both unidirectional and bidirectional architectures which is followed by the loss curves of gru this is mainly because rnn is a much simpler model than the others and gru has fewer computational parameters than lstm to be tuned looking at the loss curves neither overfitting nor underfitting on the training set was observed for all neural network models oshiro and santoro perez 2012 mentions that in the rf model development the hyperparameter of number of trees is generally set by the user by trial and error method based on this we selected the optimal number of trees as 100 by trial and error while developing the rf models 4 2 results 4 2 1 chl a prediction on the dataset obtained from samanet ve 8 monitoring station performances of three recurrent neural network methods namely standard rnn lstm and gru were investigated by using them within three different architectures i e unidirectional bidirectional and hybridized with a cnn to predict the water quality variable chl a additionally a cnn and a shallow ml method i e rf were developed to compare the performances of recurrent neural network models with a feedforward neural network and shallow machine learning architecture after training a model using each of these 10 methods predictions for chl a were obtained by feeding test set values to the trained model the prediction performances of all methods on both training and test set using a variety of evaluation metrics i e r mae pbias ens wi and ers are presented in table 4 for each model a set of batch sizes were examined and the one minimizing mae metric at most was adopted which were 6000 for rnn and 7000 for both lstm and gru in both unidirectional and bidirectional architectures and for the hybrid approach it was 6000 for cnn rnn 7000 for cnn lstm and 5000 for cnn gru the first notable observation as a result of the quantitative evaluation presented in table 4 is that rf overfits the training set and gives the lowest performance on the test set while neural network architectures do not tend to overfit the training set much and provide high performance on the test set in terms of all metrics performances of different neural network methods on the test set are similar to each other it is worth to mention that considering performances on both training and testing periods in table 4 unidirectional lstm slightly outperformed all other unidirectional and bidirectional recurrent models and cnn rnn outperformed other hybrid architectures in majority of metrics speaking for the metrics calculated over all test samples i e mae pbias wi r ens there are only 39 and 91 extreme values indicating eutrophication among 12 862 and 51 448 observations in test and training sets respectively and the majority of neural network methods correctly predicted 48 72 of them that corresponds to 19 extreme values in the test set while on the training set cnn followed by unidirectional lstm provided higher ers than the others i e 15 38 and 13 43 respectively in terms of ers rf and cnn lstm performed the worst which is followed by cnn gru and bilstm in both training and test sets the findings obtained with the qualitative analysis presented by scatter plots for observed and predicted chl a concentrations in fig 9 seem to be compatible with the results of the quantitative analysis it can be seen in fig 9 that recurrent models in unidirectional and bidirectional architectures are usually better to predict the high chl a concentrations in the test set than the more complex hybrid architectures this is mainly due to simpler neural network models with few computational parameters are less likely to overfit on the huge number of low value observations in the training data and therefore have higher generalizability for estimating the small number of extreme values in the testing set remus and marcus 2001 cnn rnn which has the lowest computational complexity among hybrid architectures approves this claim by showing the best performance among hybrid architectures moreover cnn lstm which has the highest computational complexity among hybrid architectures seems to be the least accurate one in predicting the higher chl a levels with a linear regression equation of y 1 213 x 0 262 rf which is a shallow ml method was the least accurate one in terms of prediction of higher chl a values for predicting the lower values of chl a it seems as all the methods except rf have similar performance while rf was the least accurate one in fig 10 box plots of observed and predicted values of chl a using the developed models are presented approving the findings related to predictions of high valued observations that were met with the scatter plots in fig 9 rf and cnn lstm seem to be the least accurate ones in prediction of high chl a observations while rnn birnn and cnn seem to be better choices for prediction of high valued observations that indicate eutrophication 4 2 2 on transferability of the learned models to further investigate the generalizability of the different models we performed an additional analysis the results of which are presented in table 5 the performance of each model trained on the ve 8 training set was evaluated on another dataset consisting of wq records from another water body in the venice lagoon more specifically the testing phase was conducted on hourly records starting from the same time point as our test set i e 31 05 2017 9h 00am but acquired in another geographical location coordinates 1765420 e 5044955 n in the lagoon by the samanet ve 7 monitoring station ve 7 station is located closest to the dese river outflow which is extremely prone to anthropogenic pressures in particular to nutrients input in the river from agricultural lands of the metropolitan city of venice area and indeed there are 169 entries in this test set that indicate the occurrence of eutrophication i e records with chl a value greater than 30 please note that this was only 39 in the ve 8 test set the performances of the models after training on ve 7 s training data are also presented in table 5 to show the optimal result that can be obtained on the ve 7 test set when the performances on the ve 7 test set are examined through metrics r mae ens and wi that are basically calculated over all samples it is seen that the neural network models trained on the ve 8 training set show very similar performance to the ones obtained by training on the ve 7 training set for example r is similarly around 0 79 in both schemes this confirms the findings obtained with the loss curves in fig 8 as none of the neural networks overfit on the ve8 training set as demonstrated by the performance metrics calculated over all samples while it is clearly seen that rf is significantly behind the others in terms of transferability capability rnn and cnn are the most accurate ones to predict the presence of eutrophication on the ve 7 test set in terms of ers which is 38 46 while recurrent models involving rnn perform better than cnn and rf in terms of other metrics findings about cnn lstm and rf at the previous subsection were approved herein as rf and cnn lstm performs worst on the ve7 test set when they were trained on the ve8 training set these findings show that neural network models with lower computational complexity have higher accuracy on eutrophication prediction when their trained models were tested on a new dataset on the other hand it is important noting that lstm shows the best performance on the ve 7 test set in unidirectional and bidirectional architectures when trained on the ve 7 training set i e while lstm provides 50 89 ers cnn performs with 45 56 ers moreover recurrent models outperform cnn and rf in terms of all other metrics as well on the ve7 test set which can be seen in the right side of table 5 this shows that recurrent models have high potential in eutrophication prediction since they take into account temporal dependencies more than the feedforward and shallow architectures 5 discussion the presented machine learning based methodology allows to process and predict in early real time potential eutrophication status thus providing useful knowledge to drive targeted management measures and sampling operations in the venice lagoon when chlorophyll predictions exceed a certain threshold the estimation of the chl a concentrations in water as a well accepted proxy indicator of eutrophication with the only access to physico chemical records was the major challenge we faced in this study to meet such a challenge it was aimed to explore potentials of recurrent neural networks that learn temporal dependencies in the data sequence thus take advantage of autocorrelation and time dependence that time series data may have in particular while evaluating a wide variety of recurrent cell and network architectures a multivariate approach was followed in this work i e water related chl a concentration was predicted as a function based on historical information of five measured water quality variables i e temperature do salinity turbidity and chl a choice of following a multivariate approach for water quality assessment was already studied in the relevant literature underlining the involvement of multiple climatic and anthropic factors e g temperature precipitation and nutrients inputs in the eutrophication contribution it is a fact that eutrophication is a process that is further exacerbated by simultaneous changes in multiple factors such as water temperature salinity turbidity and nutrients variations which threaten in concert the ecological and environmental state of water ecosystems thus involving such factors in the prediction model is meaningful indeed our experiments showed that the eutrophication recall score with the multivariate approach is twice as good as the univariate one the presented multivariate method is a promising tool ready for what if scenario analysis accounting for changes in future climatic conditions and urban development in this perspective incorporating all the related variables can lead to flexibility of the model for future investigations where the chl a trend may change with future variations in the other variables e g increasing temperature or salinity and dissolved oxygen variation due to the rivers and sea inflow patters changes caused by climate and anthropic changes efficiency of the developed neural networks were empirically demonstrated in two water bodies located in different geographical locations in the lagoon and showing different ecological status the transferability potential of the developed models was also empirically explored by evaluating the performance of models that were trained on the training set of one of the water bodies and then tested on the test set of another one recurrent models outperformed cnns and rf in terms of conventional performance evaluation metrics adopted by the literature works while rnn and cnn performed best in terms of ers metric these findings showed that neural network models with lower computational complexity i e rnn have higher accuracy on eutrophication prediction when their trained models were tested on a new dataset this paper also points out and tries to address the issue of setting adequate environmental ecological targets supporting sustainable water management in fact water quality targets are essential to achieve the environmental objectives defined under the eu water framework directives 6 6 the marine strategy framework directive https ec europa eu environment marine eu coast and marine policy marine strategy framework directive index en htm at present chl a thresholds supporting the good ecological status are missing at the european union level for coastal lagoons and thresholds for admissible chl a concentrations vary widely across states and water bodies in this setting this study provides useful insights into the definition of a chl a concentration threshold adequate to reach a given status of biological indices in a coastal lagoon water body particularly a newly derived metric namely eutrophication recall score ers was introduced in this study to evaluate extreme chl a prediction performances in addition to evaluation metrics commonly exploited by the relevant literature works providing a sort of cumulative evaluation over all samples exploiting those commonly adopted metrics is also important to evaluate water quality assessment however has the drawback of being dominated by massive low valued observations and the ers metric comes as a remedy for this issue 6 conclusion the present investigation aimed to explore potentials of recurrent neural network variants i e rnn lstm gru in unidirectional bidirectional and hybrid architectures and make a comparative analysis with a feedforward neural network and a shallow machine learning method which have been widely adopted by relevant literature works for eutrophication prediction we studied on eutrophication modeling for one of the most important mediterranean water transitions namely venice lagoon which is a challenging case study as it is a very complex and fragile ecosystem that has been strongly modified by anthropic activities for years to the best of our knowledge this is the first attempt for modeling and evaluating eutrophication and water quality in a complex lagoon environment using such a wide variety of architectural designs of recurrent models in line with the objectives and research horizons as reported under the eu mission starfish 2030 restore our ocean and waters report of the mission board healthy oceans seas coastal and inland waters this paper makes an effort on increasing digitalization in environmental monitoring and analysis in the investigated site paving the way for a smooth chain of data information intelligence supporting water quality risk assessment early warning and targeted interventions in general advancing an automatized system intelligent decision support system able to assimilate and process data from monitoring networks and return useful information to drive targeted field surveys may represent a powerful support for sustainable wq management driving environmental protection agencies to targeted useful and cost safe monitoring operations supporting the implementation of relevant eu acquis eu water framework and marine strategy framework directives currently the implementation of the wfd and msfd faces strong challenges and the 2027 deadline for achieving environmental ecological targets is unlikely to be met across all the eu water bodies in this setting the use of machine learning for surveillance and operational monitoring shows a great potential to better standardize environmental ecological targets thresholds and water management measures across europe in a future perspective new digital tools will be essential for policy makers and local regional institutions to access relevant and timely scientific knowledge on policy recommendations developments and reforms these methods are likely to decrease reliance on manual analysis or direct sampling usually based on more environmental invasive expensive time consuming or labor intensive approaches this new way of observing water quality variables and changes and the earth in general will generate large volumes and variety of data that will only be feasible to analyze with the help of ai under this scenario therefore ai will play a key role in making routine processes more time efficient and alleviate the manual work required fully automated water monitoring systems will be faster and more efficient at detecting changes of concern and tipping points needing for urgent interventions 7 software and data availability all methods were written in the python language and in the pycharm environment https www jetbrains com pycharm machine learning methods were developed using the freely available keras library chollet and others 2018 at https github com fchollet keras with tensorflow backend abadi et al 2016 free and open source codes 7 7 brownlee jason 2017 multivariate time series forecasting with lstms in keras deep learn time series https machinelearningmastery com multivariate time series forecasting lstms keras accessed on 16 may 2022 have been employed to perform the overall analysis the machine learning models were run on cpu at mac os big sur version 11 3 1 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement scientific activity performed in the research program venezia 2021 with the contribution of the provveditorato for the public works of veneto trentino alto adige and friuli venezia giulia provided through the concessionary of state consorzio venezia nuova and coordinated by corila under the project venezia2021 we are thankful to the provveditorato interregionale per le opere pubbliche per il veneto trentino alto adige e friuli venezia giulia ex magistrato alle acque venezia for providing environmental data from the samanet network appendix a recent research effort in water quality assessment using machine learning machine learning methods are increasingly used by literature studies for water quality prediction and assessment the most recent 2018 2021 literature on water quality prediction were analyzed and the published studies according to the approaches and specifications of data they used in table a1 were categorized for the interested reader a more extensive review can be found in chen et al 2020 it can be seen in table a1 that while few recent works employed shallow machine learning models such as decision tree dt and random forests rf the majority of them adopted neural network approaches in feedforward e g mlp recurrent e g lstm or hybrid e g cnn lstm architectures table a1 recent literature works on water quality assessment using machine learning tn total nitrogen tp total phosphorus nh4 ammonia nitrogen sd secchi depth do dissolved oxygen sal salinity table a1 architecture paper scenario time step datasetnumber of samples time range wq parameters method shallow ho et al 2019 river 2 months no details 10 years do bod cod spm ph and ammoniac nitrogen nh3 n dt shallow tong et al 2019 lake monthly 5726 samples 11 years chl a tn tp nh4 n and sd rf shallow feedforward jimeno sáez 2020 coastal lagoon daily 126 samples 16 months chl a temperature ph ss turb sd salinity s do tn and tp mlnn svr feedforward ahmed et al 2019 river monthly no details 10 years do wt ph no3 nh3 n ann mlp anfis recurrent hu et al 2019 mariculture 5 minutes 710 samples 21 days wt ph lstm rnn hybrid li et al 2018 pond 10 minutes 2880 20 days do wt nh3 n ph sae lstm sae bpnn hybrid nieto et al 2019 lake monthly 244 samples 8 years chl a biological parameters doc turb n cond sst ph svm abc hybrid liu et al 2019 pond 10 minutes 5006 samples 1 month do attention rnn hybrid li et al 2019 river 4 hours 1448 samples 7 months permanganate index ph tp and do fusion of three rnn variants rnn lstm gru hybrid bui et al 2020 river monthly no details 6 years bod cod do ph ts fc po42 no3 turb and cond m5p rf rt rept hybrid lu et al 2020 river hourly 1875 samples 10 years swt do ph turb sc and fdom ceemdan xgboost ceemmdan rf recurrent feedforw hybrid barzegar et al 2020 lake 15 minutes around 35 000 samples 1 year cond orp ph swt do and chl a lstm cnn cnn lstm shallow feedforward recurrent hybrid ours coastal lagoon hourly 64 290 samples 11 years turb temp do sal chl a rnn lstm gru birnn bilstm bigru cnn rnn cnn lstm cnn gru rf cnn appendix b input selection strategies using lstm as an additional analysis the effect of different input variable selection strategies on the chl a prediction accuracy for our case study was investigated experimentally we accomplished this analysis in two ways first a number of well accepted wrapper and embedded feature selection methods effrosynidis and arampatzis 2021 that assign scores to input features based on their importance in the general prediction task were used second we investigated the performance of the main input selection strategies adopted by literature works for water quality prediction chen et al 2020 using lstm 1 using generic feature selection methods based on well performing feature selection methods reported in the relevant literature effrosynidis and arampatzis 2021 olden et al 2004 we explored input variable importance using mean decrease in impurity a k a gini importance breiman et al 1984 that we computed from both random forest and ann structure the permutation based importance pedregosa et al 2011 and olden s connection weights olden et al 2004 methods using an ann the ranked importance of input variables for chl a prediction determined by each method is presented in fig a1 it can be seen in fig a1 that all of the methods found that the turbidity is the variable with the highest importance in chl a prediction which is followed by temperature and do in the majority of methods this outcome evidences that the strongest relation is between turbidity increase during eutrophication events but also that temperature conditions are determinant in the process instead less importance is given to salinity conditions underling that in the venice lagoon salinity increase is not so relevant in affecting aquatic plants smith and schindler 2009 lloret et al 2008 2 using input selection strategies as a second way we explored the input output strategies presented in table a2 adopted by the literature works on water quality assessment which were extensively studied in the survey of chen et al 2020 for our case study for each strategy an lstm model was trained using the implementation setting in section 4 from historical information of mentioned input variables to predict the chl a values fig a1 feature importance bars 1 most important 4 least important in each group represent the ranked importance for variables turbidity temperature do salinity fig a1 at strategy 0 a univariate model is constructed where the output i e current values of chl a is learned only from its own historical information similarly strategy 1 constitutes a univariate model but this time the model is learned from historical information of not chl a but one of the other variables either turbidity do salinity or temperature strategies 2 and 3 constitute multivariate model development while in strategy 2 the model learns from the historical information of selected variables without historical information of chl a in strategy 3 we used historical information of all variables including chl a table a2 summary of input variable selection strategies table a2 strategy type description strategy 0 univariate chl a is learned from its own historical information strategy 1 univariate chl a is learned from historical information of other variables one strategy 2 multivariate chl a is learned from historical information of other selected variables more than one strategy 3 multivariate chl a is learned from historical information of both its own and all other variables for easier understanding a visualization of which variable s historical information was used in model development while applying each variable selection strategy is presented in figure a2 the blue bar shows the variables whose historical information is used in model development and the yellow bar shows the variable whose current value is to be predicted that is chl a best performing input output choices i e the ones providing the minimum mae were shown by dark black arrows whereas any relevant input output choices for strategies 1 and 2 i e some of which are shown in dim dashed lines in fig a2 were already performed fig a2 illustration for variable selection strategies fig a2 obtained performances are presented in table a3 it is seen that involving historical information of chl a certainly have a positive impact on the predictive quality of current chl a both in univariate strategy 0 and multivariate strategy 3 sense in terms of all performance metrics strategy 3 which is a multivariate approach involving historical information of all wq parameters provides best performances in terms of all metrics besides involving historical information of four parameters turbidity temperature do salinity together with chl a s in strategy 3 provides just marginal performance gain over using historical information of only chl a in strategy 0 in terms of first six metrics it helps improving eutrophication recall score significantly 28 21 vs 48 72 this result shows us the importance of using historical information of all the parameters in predictive model development for improved prediction of chl a extreme values i e eutrophication prediction table a3 performance evaluation obtained by input selection strategies variables used in developing the model in each strategy are shown in fig a2 epa eutrophication prediction accuracy table a3 strategy 0 strategy 1 strategy 2 strategy 3 adopted input variables chl a turb temp turb temp do turb sal chl a temp do turb sal r 0 83 0 38 0 42 0 39 0 85 mae 0 58 1 42 1 36 1 44 0 57 e ns 0 65 0 096 0 050 0 082 0 72 pbias 9 72 50 30 46 65 50 36 9 12 wi 0 86 0 31 0 32 0 32 0 91 ers 28 21 0 0 0 48 72 
25578,eutrophication represents an important ecological and environmental issue in coastal lagoons this paper presents an extensive study of recurrent cell and network architectures to model eutrophication processes in the venice lagoon a very complex and fragile ecosystem that has been strongly altered by anthropic activities over years experimental results showed that recurrent models outperformed random forests rf significantly on two datasets performing similarly to cnns on one of the datasets while outperforming cnns on the other one additionally the transferability potential of the trained models was investigated the empirical analysis has shown that recurrent neural network models with lower computational complexity provide the highest eutrophication prediction accuracy when their trained models were tested on a new dataset designed models represent effective tools for early warning eutrophication prediction that can support the implementation of relevant eu acquis eu marine strategy and water framework directives and achievement of their environmental targets keywords water quality assessment eutrophication prediction and modeling recurrent neural networks machine learning neural networks venice lagoon 1 introduction covering approximately 13 of the world s coastline and 5 3 of the european coastline coastal lagoons are shallow inland waters separated from the sea by one or more barriers and meanwhile connected to the sea by restricted inlets that allow for water exchange phleger 1969 they have important ecological value as they are suitable habitats for wetlands mangroves salt marshes and they maintain rich biodiversity for example they are home to bivalves crustaceans fishes and birds newton et al 2018 they are also among the most socio economically important ecosystems on earth as they have been valuable natural regions for fisheries and various forms of aquaculture since ancient times they have been alluring environments for human activities such as saltworks and leisure activities and they support tourism development jimeno sáez et al 2020 however as they are located at the land sea interface they are highly threatened by anthropogenic pressures from both land e g input of nutrients from industrial urban and agricultural activities and the sea e g tides storm surges sea pollution and erosion moreover other pressures arise from climate related drivers e g changes in water temperature precipitation patterns and sea level all these threats lead to severe degradation in water quality wq due to high turbidity acidification anoxia i e low dissolved oxygen od and eutrophication processes lloret et al 2008 among the water pollution issues mentioned above eutrophication is one of the most common phenomena in coastal lagoons sanderson and geoffrey 2010 it is a result of increased land inputs of nutrients such as nitrogen and phosphorus in the water causing structural changes in the ecosystem such as abnormal production of phytoplankton algae and bacteria which in turn leads to deterioration in wq and losses of biota and biological communities increased nutrients are eventually manifested by an extreme increase in the chlorophyll a chl a levels furuya et al 2018 lin et al 2018 studies on chl a nutrient relationships have a long history sakamoto 1966 dillon and rigler 1974 mccauley et al 1989 elser et al 2007 bracken et al 2015 have shown the importance of total phosphorus tp and total nitrogen tn as predictors of chl a supporting the view that tp more frequently limits the production of phytoplankton biomass but tn may colimit phytoplankton biomass under certain conditions e g tn has a large influence on chl a at high tp furuya et al 2018 predicting wq changes with high accuracy in these fragile natural ecosystems is crucial to avoid the mentioned undesirable side effects of wq deterioration and major economic costs for the entire community vinçon leite and casenave 2019 smith and schindler 2009 at the same time understanding and modeling changes in wq is essential to plan sustainable long term strategies and well designed management schemes that are robust enough to cope with cumulative pressures and changes over time of key variables underpinning environmental dynamics from simpler indicators i e index based furlan et al 2019 and statistical methods dimberg et al 2013 to more complex decision support systems torresan et al 2016 and mathematical models fornarelli et al 2013 kim and seo 2017 kim et al 2014 lopes et al 2008 many different methods have been used for eutrophication assessment so far by the research community for example dimberg et al 2013 adopted discrete markov chains to estimate the risk of high chl a concentrations and algal blooms in a lake environment kim and seo 2017 employed exploratory factor analysis efa and empirical orthogonal function eof patterns of principal component analysis pca for the water quality assessment in the monitoring network of nakdong river korea kim et al 2014 used fuzzy models for forecasting the algal blooms behaviors lopes et al 2008 employed differential equations that characterize the chemical and biological state of the coastal waters to simulate the water quality of the lagoon the stunning performance of machine learning approaches on a wide variety of research problems has encouraged environmental science researchers since the 2010s to employ machine learning ml models for wq assessment and particularly for eutrophication modeling and prediction in this context millie et al 2012 proposed an artificial neural network ann based technology called as grey box for modeling microalgal abundance alameddine et al 2011 used bayesian networks to predict chl a levels given the nutrient concentrations water temperature and the river flow regime more recently a wider variety of ml models have been employed by the researchers as presented in a detailed table in appendix a in this context some researchers employed shallow machine learning methods e g decision tree dt ho et al 2019 random forest rf tong et al 2019 and support vector regression svr jimeno sáez et al 2020 while many others used various types of artificial neural networks ann such as multilayer perceptron mlp ahmed et al 2019 long short term memory lstm hu et al 2019 barzegar et al 2020 recurrent neural networks rnn hu et al 2019 and hybrid approaches e g cnn lstm barzegar et al 2020 more specifically using mlp and rbf methods for the prediction of wq parameters connected with eutrophication processes e g od nutrients chl a for a river basin ahmed et al 2019 dealt with the noise in data by employing an augmented wavelet denoising method namely neuro fuzzy inference system hu et al 2019 adopted a recurrent architecture i e employing the correlation priors between the parameters they used lstm and rnn to predict ph and water temperature for water quality assessment in a smart mariculture environment a number of studies applied hybridized models to improve the prediction accuracy of the overall system by 1 extracting higher quality features in advance e g li et al 2019 employed sparse auto encoder sae and barzegar et al 2020 employed a cnn to learn higher quality features which are then used for water quality prediction by lstm 2 tuning the hyperparameters of a ml method e g nieto et al 2019 proposed an svm model the hyperparameters of which was tuned by an artificial bee colony algorithm to predict chl a and total phosphorus for wq assessment 3 denoising the data initially e g lu and ma 2020 employed a denoising process by decomposing data using ceemdan initially and performed prediction using extreme gradient boosting xgboost and rf 4 combining predictions of a number of ml methods e g li et al 2019 proposed fusing predictions obtained from three rnn variants i e simple rnn lstm and gru and bui et al 2020 proposed to combine predictions of rf m5p random tree rt and reduced error pruning tree rept using bagging ba cv parameter selection cvps and randomizable filtered classification rfc or 5 applying attention mechanism to take account the spatial and temporal relations e g liu et al 2019 proposed an attention rnn model which takes account the spatial and temporal correlations for do prediction aimed at analyzing anoxia eutrophic systems among the relevant research effort so far eutrophication prediction in line with water quality assessment on the coastal lagoon scenario was only studied by a recent work jimeno sáez et al 2020 where an ann and an svr were evaluated on a limited collection of data including 126 data samples however tackling water quality assessment problem in such complex and dynamic ecosystems can still benefit from more sophisticated machine learning approaches this paper deals with the water quality deterioration problem in the venice lagoon which is a very complex and fragile natural and cultural environment that has been strongly modified by anthropic activities for years guerzoni 2006 solidoro et al 2010 this shallow coastal lagoon is a eutrophic system runca et al 1996 because excessive algal blooms have been frequently observed facca et al 2014 sfriso et al 2019 as a result of excessive discharge of phosphorus and nitrogen from the close urban agricultural and industrial areas 1 1 ufficio di piano attività di salvaguardia di venezia e della sua laguna lo stato ecologico della laguna rapporto tematico 2008 more specifically the present analysis is based on the estimation of chl a concentrations which is an eu water framework directive surrogate indicator for atypical algal productivity as a function of historical information of itself and a number of other chemical and physical environmental variables namely water temperature turbidity salinity and dissolved oxygen do these variables have an interactive relationship with each other and with chl a and changes in them can contribute in concert to algal abnormal growth lloret et al 2008 it is well known that nutrients are important parameters defining eutrophication bracken et al 2015 elser et al 2007 however in the venice lagoon case study the nutrients parameters were measured seasonally so their temporal dimension is very scarce there are only 32 observations for nutrients from 2011 to 2019 on the contrary the physico chemical variables water temperature do salinity and turbidity were measured with very high temporal resolution hourly from 2008 to 2019 this leaves us with the challenge of modeling eutrophication with only the physico chemical parameters and we aim to take the advantage of the fact that the machine learning performs well with a huge amount of data additionally the fact that the influence of water temperature turbidity salinity and do on chl a has already been addressed by literature works strengthens our motivation to use these parameters water temperature has been used to model eutrophication in various previous works barzegar et al 2020 dunstan et al 2018 jimeno sáez et al 2020 kong et al 2017 as high temperatures exacerbate algal growth and subsequently contribute to algal bloom events owens 2001 mentions that an increase in salinity in aquatic ecosystems affects most plants and causes ionic and osmotic stresses and al taee and aqeel 2018 demonstrates that increases in salinity immediately reduce net carbon fixation rates and affect photosynthetic pigments chlorophyll and carotenoids do and turbidity are related to chl a particularly during eutrophication events when algal blooms eventually die microbial decomposition severely depletes dissolved oxygen do creating a hypoxic or anoxic dead zone lacking sufficient oxygen to support most organisms dead zones are found in many water bodies during the summer furthermore such hypoxic events are particularly common in marine coastal environments surrounding large nutrient rich rivers chislock et al 2013 then turbidity is another potential impact of eutrophication on water quality increased turbidity and decreased water clarity occur during these events and water becomes cloudy and colored green and brown which reduces the ability of fish to see prey and detect predators smith and schindler 2009 boqiang et al 2013 as a preliminary study different variable selection strategies for the mentioned task were empirically explored see in appendix b and then proceeded with the best performing multivariate approach in which historical values of all five variables were used to estimate the current values of chl a following a multi variate approach is important to understand and model spatio temporal changes in water quality accounting for the complex interactions among natural and human made pressures indeed recent literature studies namely barzegar et al 2020 garcı a nieto et al 2018 nieto et al 2019 kong et al 2017 wang et al 2021 follow the multivariate approach for eutrophication prediction experiments were primarily conducted on a time series data for the specified five variables acquired from an in situ monitoring station at palude maggiore in venice lagoon shown in fig 1 the experimental dataset i e 64 311 observations of the five specified parameters used in this work is much larger than those used by literature studies please see table a1 in appendix a and covers a much longer time frame i e hourly observations from 2008 to 2019 the presence of high temporal resolution and abundant data allows us to work on a more realistic scenario and train machine learning models more robustly and accurately starting from the fact that no sophisticated machine learning approaches have been adopted for wq assessment in such complex ecosystems so far the present work investigates the potentials of deep neural networks with memory i e more specifically a set of recurrent neural networks rnn variants which is a type of neural network involving temporal dynamics in line with the aforementioned objective rnns take into account time dependence i e they involve sequential processing of data and retain historical information this coincides with our case as we are dealing with a time series dataset literature works have employed either shallow ml methods or feedforward neural networks or unidirectional recurrent neural networks including rnn and lstm cell architectures in either standalone or hybrid design with a cnn e g cnn lstm barzegar et al 2020 in a variety of environment scenarios differently from the existing literature this work explores a larger variety of models including the most well accepted rnn cell architectures i e standard rnn lstm and gru within both unidirectional and bidirectional network architectures and hybrid designs i e cnn rnn cnn lstm and cnn gru performances were compared with a cnn and a shallow ml method namely random forest rf that was heavily adopted by literature studies our work is included in table a1 appendix a to provide a comparative view of relevant research efforts to date flow of the paper is as follows section 2 introduces the characteristics of the study area and specifications of the dataset used in the present work section 3 provides the methodological details and the experimental evaluation discussion and conclusion are presented in sections 4 5 and 6 respectively 2 case study area venice lagoon is a wide shallow and transitional water body in the northern part of adriatic sea location 45 n 12 e having a mean depth of 1 0 3 m and covering 550 km2 area venice lagoon is the largest lagoonal system in italy and one of the largest in mediterranean sea as well facca et al 2014 the edges of the lagoon form a clearly delineated man made perimeter a wide expanse of reclaimed land used for industrial and residential purposes fish farms dumps and landfills as well as dikes and drainage canals border the lagoon to the west and northwest suman et al 2005 two barrier islands heavily defended by seawalls lido and pellestina separate the lagoon from the adriatic sea to the east water exchange occurs through three large entrances lido malamocco and chioggia adriatic tides govern water exchange in venice lagoon and have a mean amplitude of 0 2 m during neap tides to 1 m during spring tides the average daily freshwater input to venice lagoon amounts to 2 8 million cubic meters this hydrological pattern creates a typical brackish environment in the lagoon with a salinity gradient that ranges from 10 near the mainland border to 32 at the inlets suman et al 2005 the lagoon encloses the historical city of venice with 50 000 inhabitants and 10 000 000 visitors every year and has been declared as a world heritage by the unesco committee 2 2 decisions adopted by the world heritage committee at its 43rd session in baku republic of azerbaijan 30th june 10th july 2019 paris encyclopedia of tourism july 1025 1026 for its cultural and natural relevance while the venice lagoon has such socio economic importance it is a complex and dynamic system that evolves continuously in response to natural and anthropogenic stressors solidoro et al 2010 in this shallow coastal system one of the most impactful water pollution hazards is eutrophication facca et al 2014 solidoro et al 2010 several natural and anthropogenic factors that endanger the chemical and ecological balance of the lagoon and lead to eutrophication events can be listed as follows 1 pollutants discharge from drainage basin s rivers input çevirgen et al 2020 2 low average depth of the lagoon that yields to accelerated biochemical cycle of the macro and micro nutrients between the sediment and water column and such sudden changes results with an ecotone that is in dynamic and unstable equilibrium ravera 2000 3 extreme storm surge events that effect the sediment movement and water quality deterioration sartori 2012 4 high level of anthropogenic activities i e urbanization population density 3 3 annuario statistico italiano 2019 https www istat it it archivio 236772 very active tourism bagliani et al 2004 industrial activities in the area of porto marghera and other economic activities like aquaculture agriculture and fishing that have direct impact on water quality as detailed in the following subsection data acquired by a sensor in the lagoon named palude maggiore coordinates 1772481 e 5045037 n in the water body named ec was used in this work this case study area whose morphological structure is shown in fig 1 was chosen since it is a good representation of a lagoon area with rich interactions between different natural and anthropogenic morphological structures it has natural and artificial wetlands surrounding natural and artificial mudflats as well as a major lagoon channel of saltwater entering from the lido inlet 2 1 data description this study aims to consider the contribution of factor parameters in estimating the change in chl a values with a multivariate approach as well as benefiting from historical patterns in the data in this sense having abundant amount of data expressing relevant dynamics in a complex environment such as venice lagoon aids to a more robust analysis using deep neural networks with memory herein experiments were performed on a large collection of data for chemical wq parameters such as chlorophyll a chl a fluorescence and dissolved oxygen do and physical wq parameters such as salinity water turbidity and water temperature experimental data consists of hourly records from beginning of 2008 to end of 2019 at one of the detection stations namely ve 8 located at a depth of 1 m under lagoon surface within a monitoring network named samanet developed by venice water authority 4 4 managed by the technical office for the anti pollution of the venice lagoon of the interregional public works department for veneto trentino alto adige and friuli venezia giulia provvv oo pp http solve corila it to monitor the water quality status of the venice lagoon based on the physical chemical and ecological status targets set under the water framework directive 5 5 the eu water framework directive 2000 60 ec integrated river basin management for europe https ec europa eu environment water water framework index en html the data series for five parameters each covering 70 938 time steps from 2008 to 2019 were obtained from palude maggiore no input was recorded for at least one of the parameters at 6627 time steps due to sensor failure since this corresponds to less than 10 of the data precisely 9 34 in the entire timeline the entries at these 6627 time steps were discarded and we proceeded with the remaining 64 311 entries shown in fig 2 the descriptive statistics i e min max mean standard deviation median skewness and kurtosis of the water quality data that is used in this work are presented in table 1 turbidity has the maximum range of values 0 15 448 34 while salinity has the minimum range 15 42 38 62 which gives an idea of the magnitude of variations for these parameters over 11 years that fact is re illustrated by standard deviation sd i e sd was the highest for the turbidity 20 95 even higher than its mean which is 15 01 and the lowest for the salinity 2 50 this can be explained by the fact that in contrast to other variables the variations in turbidity in the water is very sensitive to the daily meteorological events e g turbidity increases significantly in a rainy or windy day since the lagoon has a very shallow water chl a shows much higher skewness and kurtosis than salinity temperature and do which is followed by turbidity it is mentioned in wu et al 2010 that when the values of these statistics i e skewness and kurtosis are outside the range of 2 to 2 their distribution does not follow a normal gaussian distribution specifically according to their skewness values temperature and do have fairly symmetrical data salinity is moderate negative skewed and chl a and turbidity are highly positive skewed that is chl a and turbidity with a few high extreme values are mostly at smaller values chl a followed by turbidity has the highest kurtosis indicating significant extreme values in the distribution temperature has the lowest kurtosis the correlation among the variables is presented in table 2 turbidity and temperature are slightly positively correlated with chl a with 0 32 and 0 18 respectively salinity is slightly positively correlated 0 34 and do 0 25 is slightly negatively correlated with temperature it cannot be said that there is a correlation between other variables these results can be expected because the increase in turbidity and temperature have an impact on the chl a increase at the location and similarly an increase in temperature leads to higher salinity and lower do rohling and bryden 1992 talley 2002 3 recurrent neural networks for eutrophication prediction the present investigation aims to explore the potential of recurrent neural networks for eutrophication and water quality assessment this section presents the motivation behind such a methodological choice and relevant technical explanations initial designs of neural networks were often in a feedforward scheme in a typical feedforward neural network architecture e g convolutional neural networks the connections go only in one direction that is the output of the lower layers e g input layer influences the input of the above layers e g hidden layers and the final decision is obtained in the output of the last layer of the network approximate underlying mapping functions from network inputs to the outputs are learned in training procedure more specifically to carry out the training process a loss function is defined i e that measures how the predicted output deviates from the desired output once the training samples are fed into the network and the weights are updated by backpropagation to minimize the value of the loss function until its convergence when the network is trained it can be used to accomplish the task under consideration for example to perform classification or regression on unseen data such neural networks have no memory i e they work on the assumption of independence among the input samples lipton et al 2016 lipton et al 2015 however when the data points are related in time or space such as in time series feedforward neural networks will have limitations since they do not utilize the sequential dependencies in the input data unlike the feedforward neural networks recurrent neural network architectures are given a form of memory by the newly introduced connection scheme a hidden layer receives input from two layers i e its own output from the previous time step in the sequence and the layer below it in addition to the input vectors weight vectors and output vectors they have hidden states a k a internal states that we do not have in feedforward networks which represent the outputs of the hidden layers and are updated at each time step based on the current input and the hidden state of the previous time step this type of connectionist structure allows the entire history of previous inputs to affect the network output and is very suitable for time series processing which is the case here while earlier recurrent network designs fausett 1994 which we refer to as standard rnns in this paper were designed by such self connected hidden units modern designs such as long short term memory lstm hochreiter and jürgen schmidhuber 1997 and gated recurrent unit gru cho et al 2014 consist of hidden units also called memory blocks or memory cells containing various gate structures that allow them to selectively pass information across subsequent time steps these cell architectures are called as gated rnns in some sources gallicchio et al 2018 having such control over the flow of the information with gated memory cell structures helps track the long term dependencies in the data more effectively first attempts on designing recurrent network architectures were unidirectional i e the sequence data is processed in forward direction that is from past to future schuster and kuldippaliwal 1997 proposed a bidirectional recurrent neural network architecture where the sequence data is processed in both forward and backward direction simultaneously thus it takes context from both the past and the future it is shown in a number of tasks e g speech recognition graves and jaitly 2013 sequence classification breuel 2015 traffic speed prediction cui et al 2020 that with the bidirectional recurrent architectures adopting additional information reversely to the previous time steps improves learning long term dependencies and accuracy of the model however to the best of our knowledge they were never evaluated for water quality assessment recently several literature studies have used a recurrent neural network architecture by hybridizing it with a trained convolutional neural network cnn architecture that is local features from the input data are extracted by convolution operations with a cnn model and then fed into a recurrent neural network architecture to learn temporal dependencies in the sequence cnns are feedforward networks and they process each frame of the input data independently consecutive values within a frame in the time dimension exhibit strong correlation and short variations in the time series data can be captured by convolutional filters lecun et al 1995 it has been shown in various works that feeding such extracted cnn features into an rnn architecture provides performance improvements in various tasks such as activity action recognition hammerla et al 2016 eeg classification bashivan et al 2015 trend prediction in time series lin et al 2017 and water quality assessment in a lake environment barzegar et al 2020 the descriptions of the cell and network architectures of recurrent neural networks which have been widely adopted by relevant literature studies and investigated in this paper for water quality assessment in a complex coastal lagoon environment are introduced in the subsequent subsections 3 1 cell architectures for recurrent neural networks 3 1 1 standard recurrent neural network rnn differently from the feedforward neural networks where information is processed in one direction from input to the output rnns fausett 1994 hochreiter and jürgen schmidhuber 1997 have recurrent loops in them see left hand side of fig 4 that allow information to be processed over time at the time step t rnn receives an input x t and updates the internal cell state h t by applying a function f w that is parametrized by a set of network weights w based on both the previous state from time step t 1 and the current input that the network is receiving i e h t f w h t 1 x t more specifically the operation in eq 1 is performed to update the hidden cell state h t and then the network output is produced by y t ˆ w h y h t where w denotes the weight matrix between two vectors and b denotes the bias corresponding to the subscripts respectively fausett 1994 hochreiter and jürgen schmidhuber 1997 lipton et al 2015 hidden unit in a standard rnn cell is visualized in fig 3 wei et al 2021 1 h t tanh w h h h t 1 w x h x t b h fig 4 illustrates a rolled rnn cell on the left and on the right side its representation of computational graph unrolled across time one khodadadi et al 2022 which can be thought as rnns having multiple copies of the same network where each passes message to the next considering this picture the network can be interpreted as a deep neural network with one layer at a time step and shared weights between time steps the same weight matrices which are actually those parameters learned during training are used at every time step and total loss is obtained by summing up the losses computed at individual time steps the weights are then updated by performing backpropagation through time bptt werbos 1990 in order to minimize the loss while rnns has been used successfully in a number of tasks e g speech recognition miao et al 2015 they suffer from exploding and vanishing gradients problem pascanu et al 2013 that hinders rnns to learn long time dependencies 3 1 2 long short term memory lstm a robust solution to exploding and vanishing gradients problem at rnns is using a more complex recurrent unit i e memory cell including gates that control what information is passed through and which information is used to update the internal state of the recurrent unit having such a control on the flow of the information by such gated memory cells can more effectively track the long term dependencies in the data and overcome the vanishing and exploding gradient problem lstm which was introduced by hochreiter and jürgen schmidhuber 1997 to overcome the aforementioned issue in standard rnns are able to track information throughout many time steps and work quite well on a bunch of different time series prediction tasks e g traffic speed prediction ma et al 2015 air pollutant prediction liu et al 2021 and many others an lstm cell visualized in fig 5 wei et al 2021 consists of three gates namely input output and forget gates that selectively add or remove information to its cell state c t which can be though as the long term memory of the network at a time step t additionally an lstm cell has also a hidden state h t which corresponds to the short term memory gates contain sigmoid activations forcing its input to be between 0 and 1 i e multiplying by 0 causes values to disappear and vice versa lstm process information in four simple steps hochreiter and jürgen schmidhuber 1997 that are i discard the irrelevant history using the forget gate ii store the relevant parts of the new information using the input gate iii use the previous two steps together to selectively update the internal state iv output the filtered version of the cell state to be sent to the next time step using the output gate using such gated cells lstms can specifically learn which pieces of the prior history are important in learning the problem of predicting next items and discard those that are not relevant providing more robust training the key equations of the gates in lstm are as follows wei et al 2021 i t σ w i x t w i h t 1 b i f t σ w f x t w f h t 1 b f o t σ w o x t w o h t 1 b o c t tanh w c x t w c h t 1 b c c t f t c t 1 i t c t h t o t t a n h c t y t ˆ h t where i t f t o t c t c t and h t denote input forget output gate cell state candidate cell state and hidden state respectively at time step t w and b denote the weight matrices corresponding to the current input i e related gate and the bias vectors σ is the logistic sigmoid function i e σ x 1 1 e x which outputs values in the range 0 1 hochreiter and jürgen schmidhuber 1997 3 1 3 gated recurrent unit gru proposed by cho et al 2014 gru has been popular due to its simplicity and fast computation gru which is visualized in fig 6 wei et al 2021 is mainly simplified with only two gates i e namely reset gate and update gate and similar to the standard rnn it has only one hidden state h t update gate works like a combined version of the input and forget gates of lstm reset gate controls how much of the previous hidden state contributes to the candidate state at the current time step the functionality of the gru cell is as follows wei et al 2021 u t σ w u x t w u h t 1 b u r t σ w r x t w r h t 1 b r h t t a n h w r t h t 1 w x t h t 1 u t h t 1 u t h t where u t r t h t h t denote the update and reset gates and candidate and current hidden states respectively at time step t 3 2 recurrent neural network architectures 3 2 1 unidirectional and bidirectional architectures any of rnn cell architectures presented in section 3 1 can be used in a unidirectional or bidirectional recurrent neural network architecture as illustrated in fig 4 a unidirectional rnn architecture preserves data only from the past on the other hand in a bidirectional rnn architecture schuster and kuldippaliwal 1997 the sequence data is processed both forward and backward to simultaneously gather information from the past and future adopting additional information reversely to the previous time steps improves learning long term dependencies and accuracy of the model on the other hand a limitation of bidirectional recurrent architectures is that the entire sequence is needed beforehand to make the predictions thus these architectures are not suitable for use in an online learning setting where it is assumed that it is not possible to have prior information about the future lipton et al 2015 however this is not the case in the present study a number of literature works have reported superiority of the bidirectional recurrent architectures over the unidirectional ones for the time series forecasting problem siami namini et al 2019 althelaya et al 2018 as the architecture of bidirectional rnns schuster and kuldippaliwal 1997 is shown in fig 7 the data sequence is processed in forward and backward directions with two separate hidden layers i e each is connected to both input and output then two hidden layers are concatenated to produce the final output the following equations describe a bidirectional rnn lipton et al 2015 h t tanh w h h h t 1 w x h x t b h z t tanh w z z z t 1 w x z x t b z y t ˆ w h y h t w z y z t b y where h t and z t denote the hidden states in forward and backward direction respectively 3 2 2 hybrid architectures in the present paper hybrid models by integrating a cnn to a unidirectional recurrent neural network architecture were developed more specifically features were extracted by a trained 1d cnn on the input data and fed into a type of recurrent neural networks for learning temporal dependencies in the sequence the main idea in such a cascaded design is to get benefit of both models as cnn models may extract useful knowledge in the data and rnn models efficiently capture the sequence information cnn is composed of a sequence of convolution and pooling layers i e more specifically the convolution operation is applied on the raw input data using convolutional kernels which is then followed by a nonlinear activation function e g rectifier linear unit and a pooling layer 4 experiments the performances of the recurrent models mentioned in the previous section were compared with a feed forward architecture i e cnns and a shallow ml method i e random forest rf breiman 2001 rf which conducts ensemble learning through constructed decision trees in the training period was involved since it has been heavily adopted by literature studies on water quality assessment tong et al 2019 a multivariate approach was followed in all experiments as it performed best among all other input selection strategies as presented in appendix b 4 1 implementational details performance metrics for a comprehensive analysis performances of the different methods were evaluated quantitatively by a set of evaluation metrics shown in table 3 and qualitatively with plot graphs briefly pearson s correlation coefficient 0 r 1 shows the strength of the linear relationship between the predicted and observed chl a values mean absolute error 0 mae measures the performance errors accordingly accuracy of the trained models i e smaller the mae the better the prediction accuracy percentage of bias pbias gupta and sorooshian 1999 expresses whether the predicted time series by the trained model consistently over estimates pbias 0 or under estimates pbias 0 the observed time series nash sutcliffe efficiency coefficient e ns 1 nash and sutcliffe 1970 is the error variance of the predicted time series divided by the variance of the observed time series accordingly e n s 1 indicates a perfect fit as a result of the predicted time series error variance being 0 values between 0 0 and 1 0 are considered acceptable performance levels while values less than 0 0 indicate unacceptable performance demonstrating that the mean of the observed time series is a better predictor than the developed model itself moriasi et al 2007 finally proposed by willmott 1981 higher values of wilmott s index of agreement 0 wi 1 indicates better agreement between the predicted and observed time series in the testing phase with optimal level at w i 1 these metrics were also used in a recent relevant work barzegar et al 2020 on chl a prediction for water quality assessment additionally we derived a metric named as eutrophication recall score ers to measure eutrophication prediction accuracy specialized for our venice lagoon case study while the aforementioned metrics cumulatively evaluate prediction performance over all observations which is dominated by massive low valued observations we aim to evaluate prediction accuracy for observations with values above a certain threshold indicating the presence of eutrophication with ers the threshold value is decided as t 30 based on the work of in bonometto et al 2019 see appendix b in bonometto et al 2019 that reports no quality value with chl a greater than 30 μg per liter μg l corresponding to phytoplankton bloom condition i e eutrophication in venice lagoon once thresholding both observations and a model s predictions ers computes recall score that is number of predicted values higher than t divided by number of all observed values higher than t thus for all the observations who actually indicate eutrophication i e the ones higher than the eutrophication threshold t ers shows how many we correctly identified as eutrophication apart from such quantitative evaluation metrics we also provide the scatter plots and box plots of the observed and predicted chl a values in the testing phase which allows us to observe how accurately the predicted values approaches to the observations model implementation all experiments were performed on the dataset with 64 311 entries introduced in section 2 2 for the experiments of each prediction model the dataset was partitioned into two subsets such that the initial 80 of the entire dataset records from 01 01 2008 to 31 05 2017 is used in the training and remaining 20 records from 31 05 2017 to 31 12 2019 is used in the testing phase models were developed using the training dataset and the performance of the developed models were evaluated on the test dataset values of all variables were scaled between 0 and 1 using minimum maximum scaling technique pedregosa et al 2011 what follows is the training and testing datasets were transformed into a supervised learning scheme similar to barzegar et al 2020 where each entry in the time series is converted to an input output pair i e the lag observations at time t 1 constructed the input and the current observation at time t constructed the output the performances of different models using other lag times e g t 2 and t 3 were checked as well however the performances were similar to or worse than using t 1 thus we proceeded by using the lag time t 1 for the input variables when developing the prediction models such learning scheme has been adopted for all the models developed in this paper for both recurrent and convolutional neural network methods different depth of architectures with different numbers of units or neurons were tried and further advanced with the one providing the best performance in chl a prediction more specifically in the development of rnn lstm gru birnn bidirectional rnn bilstm bidirectional lstm and bigru bidirectional gru models we used an architecture with one hidden layer having 50 units which is followed by a dropout layer with a rate of 0 001 to avoid overfitting to the training set and a fully connected layer with a unit of 1 in sequence the architecture of the cnn model composed of a convolutional layer conv1d with 32 filters a kernel size of 3 causal padding type and an elu activation followed by a max pooling layer with a pooling size of 3 and same padding type a dropout layer with a rate of 0 001 and a fully connected layer were used afterwards in the implementation of hybrid methods i e cnn rnn cnn lstm and cnn gru the same cnn architecture was coupled with the relevant aforementioned recurrent architecture all the neural network models were compiled with a mae loss function using the adam optimizer with a learning rate of 0 0001 for the training of each method different batch size values were tried and the one providing minimum mae on the test set was chosen the models were trained using 1000 epochs to guarantee the convergence in the training the loss plots for all models are presented in fig 8 it is seen in fig 8 that the loss curves of rnns converge much faster than those of the other models in both unidirectional and bidirectional architectures which is followed by the loss curves of gru this is mainly because rnn is a much simpler model than the others and gru has fewer computational parameters than lstm to be tuned looking at the loss curves neither overfitting nor underfitting on the training set was observed for all neural network models oshiro and santoro perez 2012 mentions that in the rf model development the hyperparameter of number of trees is generally set by the user by trial and error method based on this we selected the optimal number of trees as 100 by trial and error while developing the rf models 4 2 results 4 2 1 chl a prediction on the dataset obtained from samanet ve 8 monitoring station performances of three recurrent neural network methods namely standard rnn lstm and gru were investigated by using them within three different architectures i e unidirectional bidirectional and hybridized with a cnn to predict the water quality variable chl a additionally a cnn and a shallow ml method i e rf were developed to compare the performances of recurrent neural network models with a feedforward neural network and shallow machine learning architecture after training a model using each of these 10 methods predictions for chl a were obtained by feeding test set values to the trained model the prediction performances of all methods on both training and test set using a variety of evaluation metrics i e r mae pbias ens wi and ers are presented in table 4 for each model a set of batch sizes were examined and the one minimizing mae metric at most was adopted which were 6000 for rnn and 7000 for both lstm and gru in both unidirectional and bidirectional architectures and for the hybrid approach it was 6000 for cnn rnn 7000 for cnn lstm and 5000 for cnn gru the first notable observation as a result of the quantitative evaluation presented in table 4 is that rf overfits the training set and gives the lowest performance on the test set while neural network architectures do not tend to overfit the training set much and provide high performance on the test set in terms of all metrics performances of different neural network methods on the test set are similar to each other it is worth to mention that considering performances on both training and testing periods in table 4 unidirectional lstm slightly outperformed all other unidirectional and bidirectional recurrent models and cnn rnn outperformed other hybrid architectures in majority of metrics speaking for the metrics calculated over all test samples i e mae pbias wi r ens there are only 39 and 91 extreme values indicating eutrophication among 12 862 and 51 448 observations in test and training sets respectively and the majority of neural network methods correctly predicted 48 72 of them that corresponds to 19 extreme values in the test set while on the training set cnn followed by unidirectional lstm provided higher ers than the others i e 15 38 and 13 43 respectively in terms of ers rf and cnn lstm performed the worst which is followed by cnn gru and bilstm in both training and test sets the findings obtained with the qualitative analysis presented by scatter plots for observed and predicted chl a concentrations in fig 9 seem to be compatible with the results of the quantitative analysis it can be seen in fig 9 that recurrent models in unidirectional and bidirectional architectures are usually better to predict the high chl a concentrations in the test set than the more complex hybrid architectures this is mainly due to simpler neural network models with few computational parameters are less likely to overfit on the huge number of low value observations in the training data and therefore have higher generalizability for estimating the small number of extreme values in the testing set remus and marcus 2001 cnn rnn which has the lowest computational complexity among hybrid architectures approves this claim by showing the best performance among hybrid architectures moreover cnn lstm which has the highest computational complexity among hybrid architectures seems to be the least accurate one in predicting the higher chl a levels with a linear regression equation of y 1 213 x 0 262 rf which is a shallow ml method was the least accurate one in terms of prediction of higher chl a values for predicting the lower values of chl a it seems as all the methods except rf have similar performance while rf was the least accurate one in fig 10 box plots of observed and predicted values of chl a using the developed models are presented approving the findings related to predictions of high valued observations that were met with the scatter plots in fig 9 rf and cnn lstm seem to be the least accurate ones in prediction of high chl a observations while rnn birnn and cnn seem to be better choices for prediction of high valued observations that indicate eutrophication 4 2 2 on transferability of the learned models to further investigate the generalizability of the different models we performed an additional analysis the results of which are presented in table 5 the performance of each model trained on the ve 8 training set was evaluated on another dataset consisting of wq records from another water body in the venice lagoon more specifically the testing phase was conducted on hourly records starting from the same time point as our test set i e 31 05 2017 9h 00am but acquired in another geographical location coordinates 1765420 e 5044955 n in the lagoon by the samanet ve 7 monitoring station ve 7 station is located closest to the dese river outflow which is extremely prone to anthropogenic pressures in particular to nutrients input in the river from agricultural lands of the metropolitan city of venice area and indeed there are 169 entries in this test set that indicate the occurrence of eutrophication i e records with chl a value greater than 30 please note that this was only 39 in the ve 8 test set the performances of the models after training on ve 7 s training data are also presented in table 5 to show the optimal result that can be obtained on the ve 7 test set when the performances on the ve 7 test set are examined through metrics r mae ens and wi that are basically calculated over all samples it is seen that the neural network models trained on the ve 8 training set show very similar performance to the ones obtained by training on the ve 7 training set for example r is similarly around 0 79 in both schemes this confirms the findings obtained with the loss curves in fig 8 as none of the neural networks overfit on the ve8 training set as demonstrated by the performance metrics calculated over all samples while it is clearly seen that rf is significantly behind the others in terms of transferability capability rnn and cnn are the most accurate ones to predict the presence of eutrophication on the ve 7 test set in terms of ers which is 38 46 while recurrent models involving rnn perform better than cnn and rf in terms of other metrics findings about cnn lstm and rf at the previous subsection were approved herein as rf and cnn lstm performs worst on the ve7 test set when they were trained on the ve8 training set these findings show that neural network models with lower computational complexity have higher accuracy on eutrophication prediction when their trained models were tested on a new dataset on the other hand it is important noting that lstm shows the best performance on the ve 7 test set in unidirectional and bidirectional architectures when trained on the ve 7 training set i e while lstm provides 50 89 ers cnn performs with 45 56 ers moreover recurrent models outperform cnn and rf in terms of all other metrics as well on the ve7 test set which can be seen in the right side of table 5 this shows that recurrent models have high potential in eutrophication prediction since they take into account temporal dependencies more than the feedforward and shallow architectures 5 discussion the presented machine learning based methodology allows to process and predict in early real time potential eutrophication status thus providing useful knowledge to drive targeted management measures and sampling operations in the venice lagoon when chlorophyll predictions exceed a certain threshold the estimation of the chl a concentrations in water as a well accepted proxy indicator of eutrophication with the only access to physico chemical records was the major challenge we faced in this study to meet such a challenge it was aimed to explore potentials of recurrent neural networks that learn temporal dependencies in the data sequence thus take advantage of autocorrelation and time dependence that time series data may have in particular while evaluating a wide variety of recurrent cell and network architectures a multivariate approach was followed in this work i e water related chl a concentration was predicted as a function based on historical information of five measured water quality variables i e temperature do salinity turbidity and chl a choice of following a multivariate approach for water quality assessment was already studied in the relevant literature underlining the involvement of multiple climatic and anthropic factors e g temperature precipitation and nutrients inputs in the eutrophication contribution it is a fact that eutrophication is a process that is further exacerbated by simultaneous changes in multiple factors such as water temperature salinity turbidity and nutrients variations which threaten in concert the ecological and environmental state of water ecosystems thus involving such factors in the prediction model is meaningful indeed our experiments showed that the eutrophication recall score with the multivariate approach is twice as good as the univariate one the presented multivariate method is a promising tool ready for what if scenario analysis accounting for changes in future climatic conditions and urban development in this perspective incorporating all the related variables can lead to flexibility of the model for future investigations where the chl a trend may change with future variations in the other variables e g increasing temperature or salinity and dissolved oxygen variation due to the rivers and sea inflow patters changes caused by climate and anthropic changes efficiency of the developed neural networks were empirically demonstrated in two water bodies located in different geographical locations in the lagoon and showing different ecological status the transferability potential of the developed models was also empirically explored by evaluating the performance of models that were trained on the training set of one of the water bodies and then tested on the test set of another one recurrent models outperformed cnns and rf in terms of conventional performance evaluation metrics adopted by the literature works while rnn and cnn performed best in terms of ers metric these findings showed that neural network models with lower computational complexity i e rnn have higher accuracy on eutrophication prediction when their trained models were tested on a new dataset this paper also points out and tries to address the issue of setting adequate environmental ecological targets supporting sustainable water management in fact water quality targets are essential to achieve the environmental objectives defined under the eu water framework directives 6 6 the marine strategy framework directive https ec europa eu environment marine eu coast and marine policy marine strategy framework directive index en htm at present chl a thresholds supporting the good ecological status are missing at the european union level for coastal lagoons and thresholds for admissible chl a concentrations vary widely across states and water bodies in this setting this study provides useful insights into the definition of a chl a concentration threshold adequate to reach a given status of biological indices in a coastal lagoon water body particularly a newly derived metric namely eutrophication recall score ers was introduced in this study to evaluate extreme chl a prediction performances in addition to evaluation metrics commonly exploited by the relevant literature works providing a sort of cumulative evaluation over all samples exploiting those commonly adopted metrics is also important to evaluate water quality assessment however has the drawback of being dominated by massive low valued observations and the ers metric comes as a remedy for this issue 6 conclusion the present investigation aimed to explore potentials of recurrent neural network variants i e rnn lstm gru in unidirectional bidirectional and hybrid architectures and make a comparative analysis with a feedforward neural network and a shallow machine learning method which have been widely adopted by relevant literature works for eutrophication prediction we studied on eutrophication modeling for one of the most important mediterranean water transitions namely venice lagoon which is a challenging case study as it is a very complex and fragile ecosystem that has been strongly modified by anthropic activities for years to the best of our knowledge this is the first attempt for modeling and evaluating eutrophication and water quality in a complex lagoon environment using such a wide variety of architectural designs of recurrent models in line with the objectives and research horizons as reported under the eu mission starfish 2030 restore our ocean and waters report of the mission board healthy oceans seas coastal and inland waters this paper makes an effort on increasing digitalization in environmental monitoring and analysis in the investigated site paving the way for a smooth chain of data information intelligence supporting water quality risk assessment early warning and targeted interventions in general advancing an automatized system intelligent decision support system able to assimilate and process data from monitoring networks and return useful information to drive targeted field surveys may represent a powerful support for sustainable wq management driving environmental protection agencies to targeted useful and cost safe monitoring operations supporting the implementation of relevant eu acquis eu water framework and marine strategy framework directives currently the implementation of the wfd and msfd faces strong challenges and the 2027 deadline for achieving environmental ecological targets is unlikely to be met across all the eu water bodies in this setting the use of machine learning for surveillance and operational monitoring shows a great potential to better standardize environmental ecological targets thresholds and water management measures across europe in a future perspective new digital tools will be essential for policy makers and local regional institutions to access relevant and timely scientific knowledge on policy recommendations developments and reforms these methods are likely to decrease reliance on manual analysis or direct sampling usually based on more environmental invasive expensive time consuming or labor intensive approaches this new way of observing water quality variables and changes and the earth in general will generate large volumes and variety of data that will only be feasible to analyze with the help of ai under this scenario therefore ai will play a key role in making routine processes more time efficient and alleviate the manual work required fully automated water monitoring systems will be faster and more efficient at detecting changes of concern and tipping points needing for urgent interventions 7 software and data availability all methods were written in the python language and in the pycharm environment https www jetbrains com pycharm machine learning methods were developed using the freely available keras library chollet and others 2018 at https github com fchollet keras with tensorflow backend abadi et al 2016 free and open source codes 7 7 brownlee jason 2017 multivariate time series forecasting with lstms in keras deep learn time series https machinelearningmastery com multivariate time series forecasting lstms keras accessed on 16 may 2022 have been employed to perform the overall analysis the machine learning models were run on cpu at mac os big sur version 11 3 1 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement scientific activity performed in the research program venezia 2021 with the contribution of the provveditorato for the public works of veneto trentino alto adige and friuli venezia giulia provided through the concessionary of state consorzio venezia nuova and coordinated by corila under the project venezia2021 we are thankful to the provveditorato interregionale per le opere pubbliche per il veneto trentino alto adige e friuli venezia giulia ex magistrato alle acque venezia for providing environmental data from the samanet network appendix a recent research effort in water quality assessment using machine learning machine learning methods are increasingly used by literature studies for water quality prediction and assessment the most recent 2018 2021 literature on water quality prediction were analyzed and the published studies according to the approaches and specifications of data they used in table a1 were categorized for the interested reader a more extensive review can be found in chen et al 2020 it can be seen in table a1 that while few recent works employed shallow machine learning models such as decision tree dt and random forests rf the majority of them adopted neural network approaches in feedforward e g mlp recurrent e g lstm or hybrid e g cnn lstm architectures table a1 recent literature works on water quality assessment using machine learning tn total nitrogen tp total phosphorus nh4 ammonia nitrogen sd secchi depth do dissolved oxygen sal salinity table a1 architecture paper scenario time step datasetnumber of samples time range wq parameters method shallow ho et al 2019 river 2 months no details 10 years do bod cod spm ph and ammoniac nitrogen nh3 n dt shallow tong et al 2019 lake monthly 5726 samples 11 years chl a tn tp nh4 n and sd rf shallow feedforward jimeno sáez 2020 coastal lagoon daily 126 samples 16 months chl a temperature ph ss turb sd salinity s do tn and tp mlnn svr feedforward ahmed et al 2019 river monthly no details 10 years do wt ph no3 nh3 n ann mlp anfis recurrent hu et al 2019 mariculture 5 minutes 710 samples 21 days wt ph lstm rnn hybrid li et al 2018 pond 10 minutes 2880 20 days do wt nh3 n ph sae lstm sae bpnn hybrid nieto et al 2019 lake monthly 244 samples 8 years chl a biological parameters doc turb n cond sst ph svm abc hybrid liu et al 2019 pond 10 minutes 5006 samples 1 month do attention rnn hybrid li et al 2019 river 4 hours 1448 samples 7 months permanganate index ph tp and do fusion of three rnn variants rnn lstm gru hybrid bui et al 2020 river monthly no details 6 years bod cod do ph ts fc po42 no3 turb and cond m5p rf rt rept hybrid lu et al 2020 river hourly 1875 samples 10 years swt do ph turb sc and fdom ceemdan xgboost ceemmdan rf recurrent feedforw hybrid barzegar et al 2020 lake 15 minutes around 35 000 samples 1 year cond orp ph swt do and chl a lstm cnn cnn lstm shallow feedforward recurrent hybrid ours coastal lagoon hourly 64 290 samples 11 years turb temp do sal chl a rnn lstm gru birnn bilstm bigru cnn rnn cnn lstm cnn gru rf cnn appendix b input selection strategies using lstm as an additional analysis the effect of different input variable selection strategies on the chl a prediction accuracy for our case study was investigated experimentally we accomplished this analysis in two ways first a number of well accepted wrapper and embedded feature selection methods effrosynidis and arampatzis 2021 that assign scores to input features based on their importance in the general prediction task were used second we investigated the performance of the main input selection strategies adopted by literature works for water quality prediction chen et al 2020 using lstm 1 using generic feature selection methods based on well performing feature selection methods reported in the relevant literature effrosynidis and arampatzis 2021 olden et al 2004 we explored input variable importance using mean decrease in impurity a k a gini importance breiman et al 1984 that we computed from both random forest and ann structure the permutation based importance pedregosa et al 2011 and olden s connection weights olden et al 2004 methods using an ann the ranked importance of input variables for chl a prediction determined by each method is presented in fig a1 it can be seen in fig a1 that all of the methods found that the turbidity is the variable with the highest importance in chl a prediction which is followed by temperature and do in the majority of methods this outcome evidences that the strongest relation is between turbidity increase during eutrophication events but also that temperature conditions are determinant in the process instead less importance is given to salinity conditions underling that in the venice lagoon salinity increase is not so relevant in affecting aquatic plants smith and schindler 2009 lloret et al 2008 2 using input selection strategies as a second way we explored the input output strategies presented in table a2 adopted by the literature works on water quality assessment which were extensively studied in the survey of chen et al 2020 for our case study for each strategy an lstm model was trained using the implementation setting in section 4 from historical information of mentioned input variables to predict the chl a values fig a1 feature importance bars 1 most important 4 least important in each group represent the ranked importance for variables turbidity temperature do salinity fig a1 at strategy 0 a univariate model is constructed where the output i e current values of chl a is learned only from its own historical information similarly strategy 1 constitutes a univariate model but this time the model is learned from historical information of not chl a but one of the other variables either turbidity do salinity or temperature strategies 2 and 3 constitute multivariate model development while in strategy 2 the model learns from the historical information of selected variables without historical information of chl a in strategy 3 we used historical information of all variables including chl a table a2 summary of input variable selection strategies table a2 strategy type description strategy 0 univariate chl a is learned from its own historical information strategy 1 univariate chl a is learned from historical information of other variables one strategy 2 multivariate chl a is learned from historical information of other selected variables more than one strategy 3 multivariate chl a is learned from historical information of both its own and all other variables for easier understanding a visualization of which variable s historical information was used in model development while applying each variable selection strategy is presented in figure a2 the blue bar shows the variables whose historical information is used in model development and the yellow bar shows the variable whose current value is to be predicted that is chl a best performing input output choices i e the ones providing the minimum mae were shown by dark black arrows whereas any relevant input output choices for strategies 1 and 2 i e some of which are shown in dim dashed lines in fig a2 were already performed fig a2 illustration for variable selection strategies fig a2 obtained performances are presented in table a3 it is seen that involving historical information of chl a certainly have a positive impact on the predictive quality of current chl a both in univariate strategy 0 and multivariate strategy 3 sense in terms of all performance metrics strategy 3 which is a multivariate approach involving historical information of all wq parameters provides best performances in terms of all metrics besides involving historical information of four parameters turbidity temperature do salinity together with chl a s in strategy 3 provides just marginal performance gain over using historical information of only chl a in strategy 0 in terms of first six metrics it helps improving eutrophication recall score significantly 28 21 vs 48 72 this result shows us the importance of using historical information of all the parameters in predictive model development for improved prediction of chl a extreme values i e eutrophication prediction table a3 performance evaluation obtained by input selection strategies variables used in developing the model in each strategy are shown in fig a2 epa eutrophication prediction accuracy table a3 strategy 0 strategy 1 strategy 2 strategy 3 adopted input variables chl a turb temp turb temp do turb sal chl a temp do turb sal r 0 83 0 38 0 42 0 39 0 85 mae 0 58 1 42 1 36 1 44 0 57 e ns 0 65 0 096 0 050 0 082 0 72 pbias 9 72 50 30 46 65 50 36 9 12 wi 0 86 0 31 0 32 0 32 0 91 ers 28 21 0 0 0 48 72 
25579,the forecasting of hazardous atmospheric phenomena is often challenging artificial intelligence ai models have been applied to atmospheric science problems model complexity provides a motivation to quantify the importance of model architecture components we studied the relative importance of the components of the fognet model that was designed for big atmospheric data 1 3d versus 2d convolution 2 physics based grouping and ordering of meteorological input features 3 different auxiliary cnn based feature learning modules and 4 parallel versus sequential spatial variable wise feature learning we investigate the relative importance of these cnn architectural features by predicting coastal fog a complex spatiotemporal dynamical process we use four explainable ai techniques to better understand input feature contributions the results of the experiments demonstrate that 3d cnn based models better capture the complexity of the fog prediction process than the 2d cnns we also show that physics based feature grouping and the order in which they are fed into the cnns significantly impacts performance keywords 3d convolutional neural network shap explainable ai permutation feature importance atmospheric prediction software and data availability name of software fognet v1 0 developer hamid kamangir evan krell source https github com conrad blucher institute fognet programming language python 3 dependecies tensorflow keras numpy licence mit license data availability north american mesoscale nam 12 km available in grib2 format archived at ftp ftp ncep noaa gov pub data nccf com nam prod nam yyyymmdd 1 introduction convolutional neural networks cnns have been applied extensively to atmospheric science applications in recent years these models are often based on very large scale spatio temporal datasets and the cnn may be required to learn complex non linear relationships to achieve acceptable performance these datasets are often highly imbalanced and predicting infrequent yet impactful events is complicated by the relatively few observations of the weather hazard as compared to the non event occurrences for example the number of non fog cases is much larger than the number of fog cases a model can achieve high accuracy but with no predictive skill by always predicting the non event kumler bonfanti et al 2020 if 95 of the test instances are the non event then 95 accuracy is achieved by simply always predicting no occurrence of for example fog complex cnn architectures have the potential to bring significant improvements for these problems given the large number of network parameters such as the depth and width of the hidden layers the choice of convolutional kernels etc it is challenging to develop high performance architectures hazenet is an example of a cnn for an atmospheric application forecasting severe haze that achieves validation accuracy 95 2 but that produces a large number of false negatives wang et al 2019 the architecture is a conventional cnn based on the popular vgg network simonyan and zisserman 2014 however wang et al wang et al 2021 demonstrated that a more complicated cnn based architecture that incorporated a spatiotemporal attention module performed better than a conventional cnn for quantitative precipitation estimation to better learn the underrepresented severe precipitation the loss function was weighted by rain intensity fognet kamangir et al 2021 is an example of a cnn architecture that achieved high performance predicting coastal fog outperforming for example the high resolution ensemble forecast href an operational ensemble of numerical weather prediction nwp models a complex cnn based architecture was developed to avoid overfitting and learn the process dynamic despite the relatively low number of fog cases the architecture is based on 3d convolutions physically based feature groupings dense blocks and attention maps this study uses the case of coastal fog predictions to investigate the benefits of some of these more complex features of cnns it has been shown that when processing spatio temporal images or images with a large number of bands such as hyperspectral imagery 3d cnn based models outperform the conventional 2d cnn based variety by learning the complexity of the auto correlated input dataset ma et al 2019 he et al 2017 the 3d cnn based models are able to learn not only 2d spatial patterns and correlations between groups of pixels and a target but also learn spectral correlations between bands or temporal correlations between input variables however 3d cnn based models are more computationally expensive compared to 1d or 2d cnn based models due to the larger number of parameters to train furthermore if the order of the input variables or bands in a 3d image cube do not matter there is no reason for the use of a 3d cnn based architecture li et al 2017 fognet is a recent 3d cnn based model trained on an atmospheric data cube with a large number of inputs 384 input variable maps and with an architecture that is hypothesized to benefit from a physics based ordering of the input variables fog is a meteorological phenomenon consisting of very small water droplets near the earth s surface that reduces visibility to less than 1 km glickman 2000 wmo 2020 the low visibility associated with fog has an adverse effect on the transportation sector and contributes to vehicular and aviation accidents gultepe et al 2019 das et al 2018 fog droplets develop due to condensation within an environment characterized by high relative humidity which can range from unsaturated to supersaturated gultepe et al 2007 high relative humidity can occur due to the addition of water vapor cooling or near surface mixing of air parcels with different temperatures glickman 2000 gultepe et al 2007 condensation into water droplets is aided by hygroscopic aerosol particles known as cloud condensation nuclei the visibility reduction is due to what is termed the first indirect effect whereby aerosols contribute to a cloud drop size distribution characterized by a preponderance of smaller droplets resulting in a larger surface to volume ratio and subsequent extinction and lower visibility twomey 1974 koračin et al 2014 fog occurs within the planetary boundary layer pbl the lowest layer of the atmosphere that is directly influenced by the earth s surface the pbl responds to surface forcings in a timescale of 1 hour or less stull 1988 in particular the warming and cooling of the earth s surface in response to radiation results in pbl changes via transport processes the vertical transport of moisture heat and momentum is dominated by turbulence while horizontal transport is accomplished by the mean wind stull 1988 stensrud 2009 thus these 3d transport processes directly contribute to pbl structure the thickness of the pbl can range from 100 m to 3 km in time and space stull 1988 specific fog types tend to occur in association with unique atmospheric vertical structures for example an atmosphere characterized by a thin moist layer near the surface and much drier air aloft under clear skies and light wind is conducive to radiation fog further warm moist air in the lower levels approaching the middle texas coast united states with or without stratus clouds aloft contributes to the development of advection fog along the coast thus the incorporation of 3d cubes of meteorological fog predictor variables within the lower atmosphere would capture the vertical and horizontal patterns corresponding to fog and possibly account for the 3d non linear processes contributing to fog formation potentially resulting in skillful fog predictions gultepe et al 2007 emphasized the importance of 3d prediction models to better predict various fog types 1 1 contributions our work makes the following contributions quantifying the advantage of using 3d vs 2d kernels to capture interactions between variables and within vertical atmospheric profiles in addition to spatial features investigating the impact of physics based grouping and ordering of atmospheric input variables for a 3d cnn model investigating the importance of several feature learning modules for 3d cnn to better capture the complex interactions of meteorological input variables for fog prediction including dense block attention mechanism and multiscale feature learning comparing the impact of learning spatial wise and channel wise features in parallel or in sequence investigating the importance and contribution of individual meteorological variables features and each input feature group for fog forecasting by using four explainable artificial intelligence xai techniques 2 methods 2 1 2d convolutional feature learning the core operation of cnns is convolution over images to extract lower dimensional features a convolutional kernel is defined that repeatedly operates in a local window defined by the size of the kernel the kernel acts as a moving window across the image s dimensions to calculate all the pixel values of the output feature map in traditional image processing kernels are manually designed to detect desired features simonyan and zisserman 2014 for example the sobel operator uses two 3 3 kernels to detect edges one for horizontal edges and the other for vertical edges based on an approximation of the gradient at the center pixel location kanopoulos et al 1988 many other kernels exist to perform operations such as blur sharpen detect other edge angles etc these kernels are routinely used for image processing including recognition tasks such as classification specific classes can be characterized by the combined outputs of a set of manually selected kernels traditionally image classification was based on hand crafted kernels for feature extraction however it is challenging to select the kernel values that best support image recognition tasks this can be formulated as an optimization problem to select the values that minimize classification error thus manual feature extraction can be replaced with data driven learned feature extraction cnns have been shown to be an effective machine learning approach for automatic image feature extraction krizhevsky et al 2012 simonyan and zisserman 2014 huang et al 2017 convolutional layers perform convolution using kernels whose values are trainable parameters a typical architecture contains convolution layers for feature extraction followed by fully connected layers to make a prediction based on potentially highly nonlinear relationships between the features and the target class this approach is not limited to gray scale 2d or rgb 3d visual images but rather rasters of arbitrary dimensions in the case of meteorological applications the rasters spatial dimensions typically represent a discretized spatial region while the channels bands represent separate environmental variables such as temperature wind speed or relative humidity a 3d raster of meteorological variables is illustrated in fig 1 a even when working with multi channel inputs such as rgb images the majority of cnn applications focus on 2d convolution that is a 2d kernel is applied as illustrated in fig 1b even though the kernel is calculated over multiple image channels each operation involves raster values within a single channel thus the output is 3d but each output channel contains only spatial wise features alternatively 3d convolution uses a 3d kernel to operate across channels as well as spatially to extract 3d features see fig 1c 3d convolution will be discussed in section 2 2 2 1 1 benchmark 2d cnns in this section three of the most common 2d cnn based benchmarks alexnet krizhevsky et al 2012 resnet he et al 2015 and densenet huang et al 2017 2019 are discussed each of these architectures advanced the state of the art in visual recognition and are commonly used image classification benchmarks while initially designed for rgb images they can be adapted to support an arbitrary number of channels in 2012 alexnet krizhevsky et al 2012 won the imagenet large scale visual recognition challenge ilsvrc 2012 and demonstrated that increasing the number of hidden layers dramatically enhances model performance relatively shallow networks were the norm given the computational expense of learning the larger number of weights in deeper networks however gpus were used to make it feasible to train the 8 hidden layers of alexnet and demonstrated that deep learning can dramatically outperform models based on human selected features the success of alexnet led deep learning researchers to explore increasingly deeper and more complex architectures ilsvrc 2014 was won by using even more hidden layers two variants of the vgg architecture vgg 16 and vgg 19 were used where the 16 and 19 designations refer to the number of hidden layers eventually however additional layers were providing diminishing returns or even worse performance the major problem was the vanishing or exploding gradient applying backpropagation along the deep hidden layers results in multiplying so many weights that they either become 0 or arbitrarily large the major contribution of resnet winner of ilsvrc 2015 was architecture design techniques that would allow cnns to efficiently scale to hundreds of hidden layers for improved model performance he et al 2015 skip connections were introduced that mitigate the vanishing exploding gradients these are connections that flow from the input to each layer skipping over the convolutions to promote gradient flow also bottleneck layers were included throughout the network for dimension reduction to limit the number of parameters to learn by doing so resnet is able to have much deeper models than vgg while being significantly less complex this was shown to enable feasible training of large models such as resnet 152 that is a specific configuration of the resnet architecture with 152 hidden layers that actually have less parameters to learn than vgg 16 he et al 2015 in 2017 huang et al developed densenet that was able to outperform resnet by using dense blocks huang et al 2017 with the dense block every layer is connected to all subsequent layers at each layer input is the channel wise concatenation of feature maps output from all previous layers this is an extension of the skip connection concept but promotes learning by allowing each layer to consider information from all previous layers thus features learned at each layer are used more efficiently since all the subsequent layers have access this allows the network to learn with fewer hidden layers in addition like skip connections the feed forward propagation of features avoids vanishing exploding gradients these architectures have been shown to be effective in many domains besides rgb image recognition including recent weather forecasting applications resnet was used by rasp and thuerey rasp and thuerey 2021 for 5 day weather forecasting first climate simulation data was used to train an initial model then transfer learning was used for additional training on real climate data the model predicts geopotential temperature and precipitation given satellite imagery zanchetta and zecchetto 2021 trained a resnet model with sentinel 1 satellite data to estimate wind direction over sea convolutions were performed over synthetic aperture radar sar images to learn to predict 2 km 2 km wind direction fields a modified resnet was implemented by bosma and nazari 2021 to predict solar and wind energy production like fognet the input raster channels were weather data rather than visual imagery the raster was a 155 108 spatial grid with 6 data channels pressure temperature humidity wind speed wind direction and cloud cover 2 2 3d convolutional feature learning 2d convolutional kernels extract the spatial correlation between pixels for each feature map however 2d convolutional kernels take a single map as input so they fail to leverage context from adjacent feature maps 3d convolutional kernels address this issue by moving the kernel in 3 dimensions depth height and width as illustrated in fig 1a the ability to leverage inter depth of the image and to learn context in correlation between different feature maps and channels can lead to improved performance for meteorological applications since there are meaningful relationships between different meteorological variables for event occurrence especially variables of the same type such as wind speed or temperature at various heights above the ground but using 3d convnets comes with a computational cost as a result of the increased number of parameters required by a 3d cnn based architecture recently 3d convolution kernels have been used in different deep learning architectures for weather and meteorological prediction niu et al 2020 wang et al 2020 castro et al 2021 niu et al 2020 proposed a new architecture for short time precipitation prediction based on a multi channel convlstm convolutional long short term memory and 3d cnn this architecture was trained on radar echo intensity data for 2017 2018 of south china with 1 km spatial resolution and 12 min intervals they have shown for such time series data having lstm long short term memory with 3d cnn works better than only a 3d cnn based model wang et al wang et al 2020 applied a 3d cnn based model for tropical cyclone intensity change prediction over a short temporal range of 24 h they used 8 input variables including temperature relative humidity wind velocity u and v wind components geopotential height and sea surface temperature with 0 125 0 125 spatial resolution and a 6 h temporal interval castro et al castro et al 2021 proposed a 3d cnn based model called stconvs2s for weather forecasting and specifically air temperature and rainfall were tested this architecture uses two different blocks to extract spatial and temporal representations of the input sequence data and they also used a temporal generator block on top of a spatial block to increase the sequence length of the time prediction to extend the applicability of 3d cnn based models the 3d cnn based model used for fog prediction developed by kamangir et al 2021 called fognet3d is explained in the next subsection section 2 2 1 fognet3d also different auxiliary modules for feature learning used by fognet including dense block attention mechanism and multiscale feature learning using 3d dilated convolutions has been explained 2 2 1 fognet3d the fognet3d kamangir et al 2021 model shown in fig 2 starts with separating the processing of input variables into five different groups based on their similar physical relationship to fog development each subgroup consists of a double parallel branch dense block feature extraction spatial wise and variable wise with an attention mechanism the variable wise and spatial wise feature outputs for each subgroup from step 1 are concatenated into two main feature groups in the next step for each feature type a 3d multiscale layer using dilated feature learning is used to extract new representation maps at different resolutions at the end the variable and spatial wise features are fused by using global average pooling and then a binary classifier used to generate a probability for fog or no fog we investigate in detail the impact of the following five different characteristics on fognet3d performance physical grouping of meteorological variables overfitting is a big challenge for all machine learning models especially when there is a high correlation between input variables which make the generalization of the model harder specifically for fognet there are between 288 384 input variables that have physical correlation with other variables across and within input categories the input data is categorized into 5 different groups based on their similar physical relationship to fog development as described below group 1 emphasizes the influence of wind and contains the wind related features fricvsurface surface frictional velocity u10 meters v10 meters u and v wind components at 10 m height elevation u975 700 and v975 700 u and v wind components at atmospheric pressure levels 975 mb 700 mb at 25 mb increments group 2 focuses on the influence of the combined effect of turbulence kinetic energy tke and specific humidity q and contains features tke975 700 and q975 700 turbulence kinetic energy and specific humidity respectively at pressure levels 975 mb 700 mb at 25 mb increments group 3 incorporates the thermodynamic profile of the lower atmosphere and contains the features tmp2 meters dpt2 meters rh2 meters air temperature dew point temperature and relative humidity respectively at 2 m height elevation tmp975 700 and rh975 700 temperature and relative humidity respectively at pressure levels 975 mb 700 mb at 25 mb increments group 4 accounts for the influence of surface atmospheric moisture and microphysics and includes vis surface visibility qsurface 2 m specific humidity tlcl temperature at the lifted condensation level and vv975 700 vertical velocity at pressure levels 975 mb 700 mb at 25 mb increments group 5 accounts for surface variables that control advection fog formation including features sst sea surface temperature dpt2 meters sst difference between 2 m dew point temperature and sst and tmp2 meters sst difference between air temperature and sst also tmp2 meters dpt2 meters difference between 2 m temperature and 2 m dew point otherwise known as the 2 m dew point depression which is proportional to relative humidity this helps to decrease the complexity of the input data and extract the correlated features individually from each group and then combine them for the next step to provide more distinguishable features for the classifier parallel learning of spatial and channel wise features previously for spatiotemporal meteorological data castro et al 2021 and remotely sensed hyperspectral data ma et al 2019 it has been shown that separately learning representations of spatial wise and channel wise or temporal wise features lead to better performance specifically the impact of parallelizing the feature extraction of meteorological variables is investigated in the fognet architecture spatial and variable wise dense blocks when cnns go deeper the path for information from the input layer to the output layer becomes too long and gradient vanishing in the opposite direction from the output to input layer is a challenge densenets huang et al 2019 address this issue by simply connecting every layer directly with each other and all the previous layers and reusing instead of drawing representation power from extremely deep or wide architectures fognet takes advantages of two different dense blocks step 1 fig 2 a spatial dense block with a kernel size of 3 3 1 to learn representation in the spatial domain of each feature map and a variable wise dense block with a kernel size of 1 1 9 to learn the correlation between different input variables spatial and variable wise attention blocks attention mechanism xu et al 2015 has been proposed to pay more attention to certain features when processing the data by cnns attention mechanism manages and quantifies the interdependence between the input variables and the output elements by focusing on the most informative parts and suppressing the weights of other regions fognet consists of two different attention modules including a variable wise attention module step 2 fig 2 to focus on informative input variables and a spatial wise attention module to extract informative areas from each input variable map multiscale feature learning multiscale feature extraction using convolutional kernels has been effective for classification problems he et al 2017 srivastava et al 2014 this is in part because multiscale convolutions have the power to extract more complex combined spatial spectral features the meteorological data includes 3d patterns with different spatial resolutions which have the potential to be quantified by using different kernels and receptive fields dilated convolution using expansion of receptive fields aggregates multiscale contextual information without loss of resolution or coverage yu and koltun 2015 in fact dilated convolution modifies the convolution filter in different ways at different ranges using different dilation factors in fognet step 4 fig 2 a multiscale 3d dilated convolution block is used to learn more complicated meteorological features 3 results discussion 3 1 study area and features the fognet study domain includes a portion of the texas coast and the adjacent western gulf of mexico see fig 3 and is organized as a 32 32 horizontal grid with 12 km grid spacing the domain 384 km 384 km is sufficiently large to account for atmospheric processes driving the formation of fog at the target location over a 24 h period orlanski 1975 the maximum forecast length of the fognet predictions the fognet features predictor variables originate from a numerical weather prediction nwp model the north american mesoscale nam modeling system used operationally by meteorologists in the national weather service united states and from satellite imagery the specific features used were chosen to predict fog by capturing the fog development process or the lower atmospheric structure consistent with fog development for the specific fog types that typically occur in the study domain these fog types corresponding mechanisms include radiation fog nighttime radiational cooling of moist air to saturation within a stagnant environment under clear skies in association with a high pressure system advection fog typically the cooling to saturation of moist onshore flow by cool shelf waters along the texas coast advection radiation fog advection of near surface moisture onshore during the day followed by the radiation fog development at night frontal fog 3 types 2 of which involve rainfall which evaporates and moistens the sub cloud layer to saturation either in a post cold frontal or pre warm frontal environment and one involving the mixing of distinct airmasses during frontal passage and stratus lowering fog radiational cooling of the air at cloud top which is transported downward by turbulent mixing and cools the sub cloud layer to saturation and or settling of cloud drizzle drops that fall below cloud base evaporate and cool the sub cloud layer to saturation resulting in the lowering of the cloud base to the surface see table 1 in kamangir et al 2021 for detailed information regarding the nam and the selection of the features the target used for fognet originates from visibility measurements from the automated weather observing system awos site at the mustang beach airport kras latitude 27 8118333 n longitude 97 0887500 w in the coastal city of port aransas texas this awos was provided by vaisala inc which provides awos model aw20 which is certified by the u s department of transportation federal aviation administration faa and meets the faa awos advisory circular 150 5220 16 for facilities that are not federally owned vaisala 2015 faa 2017 the aw20 vaisala present weather detector sensor pwd22 generates 15 s visibility values that are averaged to generate 1 min and 10 min output values vaisala 2004 the visibility value generated by awos for the user is the 10 min harmonic average the accuracy of the pwd22 sensor for visibility is 10 percent from 10 to 10 000 m and 15 percent from 10 to 20 km vaisala 2018 vaisla inc provided the awos instrumentation for kras from 2011 to the present the aw20 model was installed in 2018 11 march 2022 personal communication from randy hansen airport manager mustang beach airport the target vector was developed as follows each kras visibility measurement in the dataset is converted to one of 4 visibility categories 1600 m 3200 m 6400 m 6400 m all visibility measurements 6400 m caused by a weather phenomenon other than fog or mist were removed from the dataset thus fognet was trained to predict visibility restrictions due only to fog or mist the training validation and testing data were extracted from the 2009 2020 time series of nam nwps the 2012 2017 part of the data was used for the training of the model 5 460 cases 50 2009 2012 data was used for validation 3 328 cases 30 and the remaining of the data 2018 2020 2 228 cases 20 was used for an independent assessment of the model after the completion of the calibration 3 2 2d vs 3d convolutional feature learning for this comparison three 2d cnn architectures were selected for comparison with fognet each of the three architectures were trained on multiple hidden layer depths the three models trained were resnet 152 densenet 121 and densenet 201 the numbers refer to the model s number of hidden layers each model was trained using the adam optimizer for 100 epochs with a batch size of 64 a dynamic learning rate was used beginning with an initial value of 0 1 the deep learning framework pytorch paszke et al 2019 was used to train each of these models we use the torchsat sshuair 2020 package which includes pytorch implementations of alexnet resnet densenet and other popular cnn architectures torchsat is similar to the popular torchvision but supports an arbitrary number of channels where torchvision supports only grayscale 1 channel and rgb 3 channels torchsat allows us to train a fog detection model using the 384 channel input raster it is extremely common to use transfer learning when training these cnns that is the initial weights are based on training on very large datasets such as imagenet the new model is able to take advantage of features already learned on the large scale dataset and is adjusted with additional training to suit the new problem domain transfer learning has been shown to be effective even when the target dataset differs considerably from the original such as satellite images gadiraju and vatsavai 2020 however based on the substantially greater number of channels their non visual nature and for a fairer comparison with fognet which did not use prior training transfer learning was not performed to construct these benchmarks a drawback of using off the shelf cnns is that they expect a single raster input for visual image inputs it is reasonable to assume that all the channels will have the same dimensions but when the grids are temperature wind etc they may be of various sizes important features may be lost if scaling is used to construct a single raster fognet performs the scaling with a dimension reduction component of the model specifically the sst is transformed from 384 384 to 32 32 since the scaling is performed through convolution the scaling that best helps the model to extract discriminating features is learned for the benchmarks however the sst is simply downsampled with gaussian smoothing for anti aliasing for all the experiments run with fognet3d the same hyperparameters have been used to find the best fognet hyperparameters a grid search section 11 4 3 of goodfellow et al 2016 was applied in all the experiments the model is trained for 50 epochs with 32 batches per epoch on 5 460 training samples and 3 328 validation samples for all the experiments the learning rate lr is held constant at 0 0009 and dropout and l2 regularization are 0 4 and 0 001 respectively the same architecture as fognet3d was implemented to create a fognet2d by using 2d convolutional kernels instead of 3d convolutional kernels the purpose of fognet2d is to investigate the impact of 3d convolutions used in fognet3d as compared to the 2d kernels in fognet2d all experiments in this work besides the 2d benchmarks were trained using the keras python package chollet et al 2018 in this section the results for 10 iterations for fognet3d has been compared with 2d kernel based models including fognet2d densenet121 densenet201 and resnet152 based on the peirce skill score pss heideke skill score hss and the clayton skill score css verification performance metrics for deterministic forecasts of binary events and area under the receiver or relative operating characteristic curve auc for probabilistic predictions of binary events the hss and pss measure the accuracy relative to the accuracy achieved by random forecasts the accuracy measure used by the hss pss is the proportion correct hit rate the proportion correct is the fraction of all forecasts that were correct the hit rate measures the fraction of observed events that were correctly forecast the values of both metrics are within the 1 1 range and skill is demonstrated with values greater than zero the css measures the difference between the conditional probability of an event given a forecast that the event will occur and the conditional probability of an event given a forecast that the event will not occur skill is achieved when css 0 which indicates that the event occurs more frequently when forecast than when not forecast the value of 1 for any of these 3 metrics demonstrates a perfect forecast system the pss and css metrics are related to economic value pss represents the maximum potential economic value realized by users of the forecast system with cost loss ratios equal to the base rate climatology the css represents the range of cost loss ratios for which users gain economic value from the forecasts see jolliffe and stephenson 2003 wilks 2011 for more information regarding these metrics based on the results shown as a box plot in fig 4 fognet3d has the best performance with an auc of 0 94 with ci 0 95 and highest score for pss avg 0 52 hss avg 0 50 and css avg 0 48 without overlapping of the interquartile range with all other 2d cnn based models also the interquartile range boxes especially for hss and css of fognet3d show a better stability and low variability of training process in contrast the best 2d cnn model was fognet2d which has an average auc of 0 93 while 2d cnn benchmarks densenet121 dense201 and resnet152 are the next best performing models with average auc of 0 90 0 88 and 0 74 respectively results show that for such a meteorological prediction application with having large number of variables 3d convolutional feature learning is better able to learn the complex 3d structure of the atmospheric profile in order to generate more accurate and skillful predictions this performance enhancement is not surprising since atmospheric processes in nature occur in 3d the better performance of fognet2d in comparison to the 2d cnn based benchmarks including densenet121 201 and resnet152 shows that the auxiliary feature learning modules used in the fognet architecture discussed in subsection 3 3 improve performance we also summarized the computational cost of the 2d 3d cnn models based on our desktop 4 gpus nvidia rtx 1080s the time of training per epoch was much higher for the 3d models than for the 2d models with the same batch size it took more than 3 h to train a 3d model whereas the training time of 2d models was only half that time once the trained models were used for prediction the difference of the computation costs between the 2d and 3d models was narrow namely 0 02s vs 0 03s for processing a single image 3 3 ablation study of fognet components in the last few years several methods were developed to improve the performance and efficiency of cnn based models these new methods include densenet and resnet which extract features at different resolutions using dilated convolutions applying the attention mechanism etc fognet applies several of these modules to better approximate the complex relationship between input meteorological variables and fog prediction to control overfitting and to better generalize when applied to novel data in this section we discuss an ablation study to evaluate the modules used by fognet the value of using an attention module multiscale feature extraction and spatial and spectral dense blocks was investigated by removing those modules from the fognet3d architecture and then comparing the results with the base performance of fognet3d also fognet3d uses two parallel branches one for variable wise and one for spatial wise feature learning to extract these two types of features separately and then fuses them before classifying to better understand the contribution of this strategy the parallelism is removed and instead the spectral features are extracted first using a variable wise dense block and then those feature maps are fed into the spatial wise dense block to extract spatial features the alternative of extracting the spatial features was not attempted since if we first extract spatial features then the outputs are feature maps generated by kernels and there are no more raw input variables to extract and build correlations from hence in this experiment we only test the first strategy and compare its performance with parallel feature learning as shown in fig 5 fognet3d shows improvement over most but not all of the derivative models in this ablation study indicating that the fognet3d modules are generally beneficial to the performance of fognet3d as we mentioned in section 2 2 1 attention mechanism is a new methodology to magnify the most important areas for each map and the most important feature maps for the classifier or decision maker to investigate the importance of applying an attention mechanism for meteorological prediction applications we compare the results of fognet3d with and without attention mechanism the results for skill metrics in fig 5 show that the performance of fognet3d has decreased mainly for pss since there is no overlapping between range values of their box plots based on the results in table 1 on average the results for fognet3d without attention mechanism has a score between 4 5 points lower for pss hss and css to consider the importance of spatial wise correlation and variable wise feature map correlation learning we ignore the impact of dense block feature learning by itself more intuitively we are considering the importance of the spatial correlation between pixels and auto correlation between different variables for meteorological applications to do so we remove the spatial dense block in fognet3d and the same thought for variable wise feature learning by removing the variable wise dense block as we can see in fig 5 by removing the spatial wise dense block the performance for fognet3d decreased between 10 14 points for pss between 5 10 points for hss and between 2 3 points for css in comparison with spatial wise variable wise feature learning decreases the performance less only between 2 5 points for each of the skill metrics the results for these two experiments may not be very informative since we have both spatial and variable wise feature learning in multiscale block before the classifier for a combination of all different groups but as we can see these results show the impact of spatial correlation between pixels and variable wise correlation between maps for meteorological problems in this case fog prediction it has been shown that for 3d cnn based hyper spectral remote sensing image processing separately learning spatial and spectral correlation can result in better performance ma et al 2019 to do so one strategy is learning these types of features in parallel and then fuse them before decision making fognet3d uses a parallel strategy where each of the feature types spatial and variable wise is learned separately in parallel and then fused before classification another strategy is sequential learning which is a common approach in cnn based models where variable wise features are learned first and then fed into spatial wise feature learning or vice versa the results in fig 5 shows that applying the sequential strategy decreased the performance of fognet3d mainly for hss and css with no overlap in the respective ranges of the second and third quartiles in fact this experiment introduces a new idea that the separate learning of spatial correlation between pixels for each map and auto correlation between different input maps might help improve the performance of cnn based models for meteorological applications due to the complex interaction between meteorological variables and event occurrence for meteorological applications along with imbalanced conditions and high complex correlation between the variables using only current cnn based computer vision techniques is insufficient in this section several modules to improve the cnn based model with their specific contribution to fognet3d s performance are introduced based on our results shown in fig 5 each of the modules attention mechanism spatially and spectral dense block feature learning multiscale feature extraction by using dilated convolution parallel extraction of spatial and variable wise features has made varying levels of contribution to the performance of fognet3d suggesting that they are useful modules for cnn based meteorological applications for fognet3d the more than 200 input meteorological variables were categorized into 5 different input groups each based on their similar physical relationship to fog development we investigated the impact of grouping input variables in such a 3d cnn based model by training the fognet3d model using all input variables in only one cube the results in fig 6 show that auc decreases from 0 95 to 0 91 for fognet3d without grouping the input variables the box plot results for pss and hss also clearly indicate that performance deteriorates with similar results for css also based on the results for the average of 10 training runs table 1 pss is 35 lower for fognet3d without grouping 0 52 for fognet3d compared to 0 34 for fognet3d without grouping 22 lower for hss 0 50 for fognet compared to 0 39 for fognet3d without grouping and 6 lower for pss 0 48 for fognet3d compared to 0 45 for fognet3d without grouping the results for this experiment show that tying the parameters of several parallel networks for each group leads to an improvement of fognet3d performance 6 35 for the skill metrics this means that reducing the number of free parameters by sharing them between group feature learning leads to better generalization by reducing overfitting 3 4 importance of meteorological variable order in atmospheric cube given a 3d weight tensor channel order is important which means changing the order of input channels would change the performance of the cnn model also for meteorological applications due to the 3d nature of atmospheric processes and the associated correlation between different variables in the atmospheric cube the order of variables in a 3d cube for a 3d cnn is important in this experiment to investigate the importance of physically ordering of the input variables in a cube we shuffled the order of the input variables for all variables in fognet3d model and check the performance with fognet3d without shuffling the features in fognet were chosen to capture the vertical structure of the lower atmosphere including the pbl and to utilize atmospheric variables that modulate fog development thus within groups 1 through 4 many features were ordered sequentially in the vertical direction by atmospheric pressure level 975 mb 700 mb at 25 mb increments such that the feature representations to the machine learning model are vertical profiles of variables that influence fog development the ordering of features in group 5 was arbitrary since the focus was only to capture the advection fog process based only on surface and near surface variables fig 7 shows the results for fognet3d with and without shuffling the average of 10 training auc scores for fognet3d with shuffling is 0 06 lower than fognet3d showing degradation in performance for fognet3d with shuffling also the box plots of the results show a large gap in performance between fognet3d with and without shuffling pss for fognet3d with shuffling is around 0 13 0 16 vs 0 48 0 58 for fognet3d and hss for fognet3d with shuffling is around 0 15 0 20 vs 0 47 0 51 for fognet3d for css there is overlap between the respective second and third quartile of the result distributions however this is due to the large range of the third quartile for the shuffled cases which could be due to two or three outliers given that the results are based on ten cases it is likely that more repetitions would result in a narrower confidence interval and the median css for the shuffled case is substantially lower than for fognet3d also based on the average of 10 training runs given in table 1 the pss score is only 0 14 for fognet3d with shuffling compared to 0 52 for fognet3d hss is only 0 16 with shuffling compared to 0 50 for fognet3d and for css the score with shuffling is 0 35 compared to 0 48 for fognet3d the results after shuffling the feature maps show that for 3d kernel cnn based models the order of input variables for meteorological application is important because in this situation the potential connections between input variables that have a high correlation regarding the event prediction have been removed so the feature maps generated by 3d convolutional kernels are not meaningful for the model to make a skilled decision to contextualize from a meteorological and representative learning perspective temperature moisture and wind related features in groups 1 through 4 within fognet3d were ordered sequentially in the vertical direction at atmospheric pressure levels from 975 mb to 700 mb at 25 mb increments to form vertical profiles of the lower atmosphere both physically consistent with those that occur in nature and strongly related to fog development these profiles became feature representations to the fognet3d architecture which allowed fognet to relate these profiles to fog prediction during model training for example consider the wind profile generated by the u and v wind components g1 features a clockwise turning of wind direction with increasing height above kras corresponds to warm air advection a component of the advection fog process along the texas coast united states during the fog season in addition strong vertical wind shear near the surface the u v wind at the adjacent 10 meter and 975 mb levels can preclude radiation fog further an increase in q specific humidity with height in the lower levels g2 is essential for radiation fog finally radiation fog generally requires specific temperature and relative humidity profiles g3 which depict a thin nearly saturated layer near the surface followed by much lower relative humidity values aloft within a temperature inversion when the features were shuffled training of the fognet3d shuffled model was likely unsuccessful in capturing the relationship that maps the profiles of temperature moisture and wind to fog in other words the g1 g2 and g3 profiles mentioned above that correlate to fog are destroyed when the features are shuffled hence the significant drop in performance e g hss drops from 0 50 to 0 16 table 1 presents an extended set of metrics for all of the models tested this includes fognet3d the 3d cnn based model all the variations of the fognet3t3d of the ablation study section 3 3 removal of the physical grouping section 3 4 along with several popular 2d cnn based models resnet152 dense121 densenet201 section 2 1 1 and fognet2d section 3 2 3 5 investigating the influence of meteorological variables due to their black box nature it is difficult to determine how a trained model uses the data to make predictions however it is useful to have some understanding of the strategies learned by the model this has motivated the rapidly developing field of xai where various methods have been proposed that probe the model in some way to learn about the model s input output relationships see murdoch et al 2019 for example lapuschkin et al 2019 demonstrates that a model performing well even on the testing dataset may rely on spurious associations in the dataset that would lead to poor real world performance mcgovern et al 2019 provide a detailed discussion of the application of xai for meteorological models unfortunately there is no single technique guaranteed to provide a complete and accurate explanation of the model molnar et al 2020 properties of the data such as dependencies and interactions can yield misleading explanations molnar et al 2020 similarly model architecture can influence xai usefulness such as the gradient shattering that may occur in very deep models mamalakis et al 2022 since the true explanation is unknown mcgovern et al 2019 suggests applying multiple xai methods when multiple methods consistently highlight certain features it suggests that those features are truly influential for the model we have applied four xai methods to investigate the influence of input raster features on fognet3d three of these group hold out permutation feature importance and lossshap are used to analyze the importance of the five metocean groups feature importance is based on how much each feature here a group of adjacent raster channels affects the overall model loss the three methods and their results are described in section 3 5 1 we have also use channel wise partitionshap to analyze the feature effect of individual raster channels feature effect is based on how much the feature channel contributes to an output prediction this technique is discussed in section 3 5 2 in a discussion of xai pitfalls molnar et al 2020 warns against confusing feature importance and effect when interpreting xai outputs here we use the term feature influence to collectively refer to both importance and effect 3 5 1 group wise feature importance we are interested in the relative importance of each of the five metocean variable groups the features where chosen based on their predictive relationship with fog but that does not guarantee that the model learned to take advantage of them this could be because it did not need to that is they did not provide significant additional information compared to other relationships learned or they would be useful for the model but the model was unable to learn them perhaps because of their complexity or a lack of variation in the training data to analyze the group importance we use three xai techniques that calculate global feature importance scores based on how the absence of each group changes the hss intuitively if removing a group g i causes a 10 reduction in hss and another group g j causes only 4 reduction then g i is assigned higher importance toward model performance group hold out the most straightforward strategy is to simply remove entire groups and retrain for each group that group is removed from the fognet3d architecture and the model is retrained because of variations between trained models 10 trials are performed for each group for each trained model we calculate the change in hss to that of fognet3d hss 0 5 each group s importance score is the average of the 10 trials the advantage of group hold out is that it directly tests the impact of the input features on model performance this is in contrast to the other methods that as will be discussed use techniques to imperfectly simulate the removal of features the group hold out approach is often not used in xai studies because every hold out requires retraining the model or as here multiple times per feature to obtain an average to perform group hold out to evaluate every element of the 32 32 384 fognet input raster would be computationally impractical but to do so for only five grouped features is tractable given that we are able to use group hold out directly a reasonable question is why other feature importance methods are needed for group wise analysis the major disadvantage of group hold out is that it does not reveal the importance for the specific trained model under investigation it is reasonable to expect that the group hold out results are similar to that of an individual model but the variation is such that a given model might have learned strategies not represented by the repeated retrainings used to generate the group hold out score molnar 2020 discusses disadvantages of testing importance with retraining with an example of how it can potentially produce misleading interpretations of feature importance instead molnar 2020 recommends using permutation feature importance permutation feature importance pfi similar to group hold out pfi mcgovern et al 2019 calculates feature importance based on model performance here hss with and without features being present but since models almost always take in a fixed size raster input the feature cannot be simply removed instead of retraining pfi simulates feature removal by permuting the feature s values here the values within the permuted group are randomly shuffled to break the relationship between input variables and the target one issue with pfi is that the input features generated from shuffling are not actually the same as completely removing the feature and the model output may simply reflect the response to unrealistic out of distribution data rather than no data as desired also pfi may struggle with correlated features as the importance scores may be divided among the group of correlated features with fognet we expect input data to have strong correlation by design across the 32 32 spatial maps and across channels that represent vertical atmospheric profiles with pfi the importance scores may be diluted across the features such that none appear important despite being used by model a way to mitigate this is by grouping features molnar et al 2020 here we expect that the 5 groups are distinct enough to allow meaningful pfi scores again see molnar 2020 for a discussion of the advantages and disadvantages of pfi lossshap lundberg et al 2019 proposed a game theoretic alternative to pfi inspired by shapley values shapley values have been proposed to add rigor to xai messalas et al 2019 fryer et al 2021 based on cooperative game theory shapley values are a fair assignment of payout to each player in a game based on their contribution to the outcome applied to xai the game is an individual model prediction and the players are the features the shapley values describe each feature s contribution to the model output shap lundberg and lee 2017 is an implementation for approximating shapley values to calculate feature effect this is a local xai technique meaning it explains the contribution of the features for a single input instance lossshap is a version of shap used to calculate global feature importance lundberg et al 2019 with shap effect scores are based on the contribution of a feature to a specific output instead lossshap applies shap s strategy for permuting the features based on the marginal contribution but the entire dataset is used to calculate the difference in loss here hss to measure performance for group wise lossshap an importance score is calculated for each of the groups by averaging over the marginal contribution of that group based on all combinations of including or not including the other four a group is said to be removed by replacing all cells with random values this is similar to pfi but with a critical distinction that allows it to take into account dependencies between groups suppose the feature under evaluation is group 3 g3 pfi simply compares the model output with and without replacing g3 with permuted values lossshap does this multiple times but each time with some of the other groups also permuted to calculate the lossshap value for g3 it is necessary to test with groups 1 2 4 and 5 present then with 1 2 4 present and 5 permuted and so forth for all possible combinations because minor variations between lossshap implementations exist the following equation shows exactly how we calculate the hss based importance score for g3 using input raster x l o s s s h a p g 3 x w 1 m c g 3 g 3 g 1 x w 2 m c g 3 g 3 g 2 x w 3 m c g 3 g 3 g 4 x w 16 m c g 1 g 1 g 2 g 3 g 4 g 5 x where mc is the subtraction of prediction for g1 and g3 and prediction for only g3 based on the hss value m c g 3 g 3 g 1 x i h s s p r e d i c t g 3 g 1 x h s s p r e d i c t g 3 x also w is the weight of the marginal contribution where 1 the sum of all the weights are equal to 1 w 1 w 16 1 2 the sum of the weights for weights in the same level of combination are equal w 1 w 2 w 5 w 6 w 11 w 12 w 15 w 16 and 3 all the weights in the same level are equal for example for level 2 with having two groups combination w 2 w 3 w 4 w 5 shap is becoming increasingly popular because of fairness guarantees and convergence to a single global optimum lundberg and lee 2017 even so it may still be susceptible to the out of sample input and correlated features problems of pfi it is also substantially more complex since each feature requires computing with combinations of removing the other features again the complexity is less of an issue when dealing with only 5 grouped features molnar 2020 further discusses the advantages and disadvantages of shap fig 8 presents the results of these techniques for each technique the scores of the groups are normalized so that the relative importance of each group for each technique can be compared despite using grouping to mitigate issues that stem from correlated features it is well documented that the accuracy of explanation produced by xai techniques is affected by a variety of subtle issues mcgovern et al 2019 also grouping does not completely partition the input raster into uncorrelated features within each feature group the features chosen have a similar physical relationship to fog development however correlations exist across the groups g1 and g2 are correlated by definition instantaneous wind velocity u group 1 can be written as u u u where u represents the mean wind over a period of time and u represents the turbulent part which is related to tke in g2 stull 1988 also g1 and g4 are related since divergence convergence of the 2d wind field at the surface g1 result in downward upward vertical velocities g4 immediately aloft owing to the conservation of mass g1 g2 and g3 are also correlated for example a temperature inversion g3 representing atmospheric static stability can suppress turbulence tke in g2 stull 1988 and also prevent the vertical mixing of greater momentum from aloft to the surface thus lower surface wind speeds represented in group 1 also during advection fog groups 3 4 and 5 are correlated given that advection fog accounted for in the group 5 features implies high surface relative humidity and low visibility group 3 and 4 features respectively this makes it challenging to determine which method is most trustworthy observing disagreement between the methods in fig 8 we suggest not overemphasising the exact values between methods but rather the overall impression they suggest concerning the model g1 g3 and g4 appear to be most important based on these methods g2 and g5 are also important but seemingly less so g2 is the most ambiguous lossshap and group hold out suggest very low importance but the pfi ranking is comparable to g4 it also has the largest difference between methods lossshap and pfi based on these results all groups appear to have a substantial impact on model performance validating their inclusion as fognet inputs only the lossshap ranking for g2 would suggest that a group might not be necessary in aggregate there is evidence that g2 does help fognet but less so than the others this could be used to spin off new experiments with g2 such as varying the horizontal spacing between channels or increasing total vertical distance with additional channels fog is strongly controlled by wind g1 and certain fog types that occur over kras the target location used to develop fognet3d are statistically correlated to specific wind velocities radiation and advection radiation fog generally cannot develop with surface 10 m wind speeds greater than around 2 5 ms 1 tardif and rasmussen 2007 koračin et al 2014 most of the advection fog events at kras occur when wind is onshore further the cold front post frontal and warm front pre frontal fog types at kras generally occur when the wind has a significant north component the wind profile represented in g1 has an influence on radiation fog since vertical wind shear change in wind velocity with height and horizontal wind velocities influence the vertical structure of radiative cooling associated with radiation fog dupont et al 2016 veering clockwise turning wind with height corresponds to warm air advection wallace and hobbs 1977 the advection of warm moist air over the cooler sea surface contributes to advection fog koračin et al 2014 huang et al 2015 if the turbulence generation effect of wind shear change in wind velocity with height exceeds the turbulence suppression effect of atmospheric buoyancy in the pbl radiation fog is not likely to occur baker et al 2002 lastly frictional velocity magnitudes become miniscule during radiation fog events liu et al 2011 pfi also identifies g2 g3 and g4 as important with normalized hss changes ranging from approximately 0 65 to 0 85 although the purpose of g2 was to account for cases whereby drier air aloft mixes vertically downward and dissipates or precludes fog toth et al 2010 the g2 features influence fog in other ways for example very low tke magnitudes and high tke dissipation rates are correlated to stratus lowering fog dupont et al 2016 further surface tke magnitudes become very small during radiation fog development liu et al 2011 however along coastal regions when conditions are favorable for advection fog advection of warm moist air over the cooler sea surface mechanical turbulence due to vertical shear in the statically stable layer within a few hundred meters of the surface can force a pre existing stratus or stratocumulus cloud to the surface to produce fog huang et al 2015 furthermore it is theoretically possible for the turbulent mixing of nearly saturated wind eddies near the surface to produce fog under certain conditions price 2019 g3 is important since this group was developed to capture the vertical profile of atmospheric temperature and relative humidity which has a strong influence on fog development and the fog type based on an assessment of fog cases at kras for the 2009 2020 period used to train validate and test fognet3d not shown radiation fog requires a vertical profile characterized by a thin moist saturated layer near the surface followed by significantly drier conditions aloft while advection and stratus lowering fog cases tend to occur with a slightly deeper moist layer than associated with radiation fog the lower radiation versus stratus lowering fog saturated layer depths are consistent with results in dupont et al 2016 lower moist layer depths associated with radiation relative to advection fog are consistent with results from croft et al 1997 cold front post frontal and warm front pre frontal fogs are correlated with an even deeper moist layer results from oliver et al 1978 dupont et al 2012 2016 suggests that for stratus lowering fog events the initial altitude of the pre existing stratus cloud layer which can be identified by the relative humidity profile below 750 mb in g3 should be 1 km or less with respect to the temperature profile radiation advection radiation advection warm front pre frontal fog and cold front post frontal fog occur under strong temperature inversions temperature increase with height stull 1988 glickman 2000 koračin et al 2014 dupont et al 2016 fognet3d involves the post procesing via deep learning of a select group of variables from the nam and from satellite derived sst data to make predictions of visibility categories associated with fog and mist knowledge of the magnitudes and spatial distribution of microphysical variables liquid water content fog droplet number concentration and particle size is essential for skillful prediction of low visibility due to fog in nwp models gultepe et al 2017 the nam lacks the resolution necessary to resolve these microphysical based variables and thus are parameterized formulate implicit effects in terms of resolved fields the nam bulk microphysics parameterization scheme predicts cloud water mixing ratio yet retains a constant cloud droplet number concentration is single moment with respect to cloud water the g4 feature vis is the nam prediction diagnosis of visibility based on an empirical relationship between the mass of cloud liquid water to the extinction coefficient although a direct relationship between fog and the actual microphysical processes responsible for fog is not accounted for in the nam the vis feature represents an attempt to relate such based on microphysics parameterization as mentioned earlier stratus lowering fog involves the lowering of pre existing stratus or stratocumulus clouds with cloud bases 1 km elevation height to the surface clouds develop in response to the activation of cloud condensation nuclei this activation process is modulated by vertical velocity and cloud base temperature gultepe et al 2017 g5 scored the worst with normalized hss changes of around 0 45 this group accounts for the development of advection fog when moist onshore flow moves over the cooler shelf waters near the middle texas coast although the majority of fog cases analyzed in this study were of the advection type the features in this group neither capture processes responsible for the other fog types nor accounts for the relationship between the vertical profile of various features and advection fog since g5 features are only relevant during advection fog cases yet the other 4 groups are relevant to all fog types we speculate that a global xai technique would rank g5 near the bottom lossshap suggests that the vertical structure of temperature and relative humidity and cloud microphysics g3 and g4 with normalized hss changes between 0 90 and 1 0 respectively were much more important than the wind profile surface features that capture the advection fog process g1 and g5 with normalized hss changes between 0 45 and 0 6 and tke and q profiles g2 with normalized hss changes less than 0 10 when predicting fog via fognet fig 8 it is not surprising that the contribution of microphysics g4 was greatest using lossshap since the low visibility associated with fog is the direct result of a microphysical process known as the first indirect effect mentioned in the introduction and captured by the nam vis output group 4 feature vis represents the nam prediction diagnosis of visibility due to various hydrometeors including fog we speculate that the post processing of the nam vis by fognet removed systematic errors in vis resulting in more accurate skillful visibility predictions the normalized hss changes clearly indicate that the vertical temperature and relative humidity profiles g3 are essential to skillful fog prediction for all fog types in this study a strong lower level temperature inversion temperature increase with height is essential for suppressing the fog dissipative effects of turbulence vertical mixing baker et al 2002 toth et al 2010 although wind g1 has the greatest importance than the other 4 groups from a pfi perspective when wind is forced to compete with the other 4 groups lossshap the vertical temperature and relative humidity structure and microphysics exerted a greater contribution to fog prediction skill the strength of g5 in this competition is likely due to the fact that the majority of fog cases in this study were of the advection type and thus resulting in a significant contribution with the third greatest normalized hss change lossshap suggests that g2 has limited influence to the overall skill of fognet however the vertical profile of tke and q are strongly related to fog development an atmospheric layer characterized by a decrease in q with height combined with tke can result in fog dissipation baker et al 2002 toth et al 2010 the collinearity of tke and q with other features may explain the low importance of g2 per lossshap with respect to the group hold out xai method applied to fognet predictions the group feature importance scores were similar to that of pfi wherein g1 g3 and g4 have greater importance than g2 and g5 this is not surprising given the similarities of the group hold out and pfi however only g4 importance improved greater normalized hss when the pfi was replaced with group hold out 3 5 2 channel wise feature effect it is also of interest to learn which individual feature maps raster channels are influential for fognet because of potentially high correlation within the groups we are interested to find out if the entire group is used or if a small subset of channels dominate here we want to understand fognet at a more granular level to know how fognet works even when that hurts the performance to do so we use a feature effect measurement shap instead of feature importance however shap values are calculated locally for a single data sample to see what channels are used overall we present a strategy to aggregate shap values to rank channels globally one option is to use shap directly on the channels similar to our approach for lossshap on grouped channels however we are most interested in channels that have spatial locations with strong effect on prediction thus we apply shap to superpixels within each channel then rank channels based on the summed absolute shap values of those superpixels otherwise the positive and negative contributions could cancel out if calculating shap directly on the channels 3 5 2 1channel wise partitionshap cwps shap values were calculated using channel wise partitionshap cwps our modification of partitionshap by hamilton et al 2021 partitionshap can be used to explain image based models an image is recursively divided along the rows and columns to generate a partition tree given a user supplied maximum number of evaluations shap values are calculated for the superpixels defined by the partitions the shap values are based on evaluating the prediction with and without removing pixels like shap a superpixel s contribution is not based just on masking that superpixel but on a number of evaluations that include other superpixels to take into account dependencies since the superpixels are based on the hierarchical partitions the number of evaluations controls the size of the final superpixels explanation granularity and the amount of computation time required the output of partitionshap is a heatmap overlaid on the input image this makes sense for rgb images for example by showing that superpixel with a bird s eye was important to the classification of an egret but here a spatial explanation does not reveal the influence of the 384 channels variables to explain the important of the fognet raster channels cwps partitions along the channels before partitioning along the rows and columns more details about cwps are available on our github repository krell 2021 where shap replaces features with random values partitionshap has the choice of blurring kernels or replacement with a constant value we experimented with 6 3 blurring kernels and 3 constant values the kernel sizes were 10 10 20 20 and 32 32 and the constant values were 0 0 0 5 and 1 0 since fognet data is normalized plotting the shap values showed that the blurring kernels produced inconsistent explanations while the constant values were very consistent our hypothesis is that blurring while useful to break up the edges that are the basis of typical image classification are less effective for variables such as sea surface temperature a blurred sst may be very similar to the original and not sufficiently removing the original information thus the constant value of 0 5 was chosen due to its consistency cwps was performed on a set of data samples using both test and validation because of the low number of fog instances the set included all 67 hits 64 misses 78 false alarms and 84 randomly selected correct rejections the maximum number of shap evaluations was set to 250 000 fig 9 shows an example of cwps heatmap on the top three ranked channels for a randomly selected hit sample fig 10 shows the number of times that each channel occurs among the top and bottom 50 channels within each classification category first each instance s channels are ranked based on the summed absolute shap values as shown in fig 9 then the top and bottom counts are obtained by searching for occurrences of each channel within the top and bottom of the ordered channel lists the number 50 was selected since it shows most of the channels from each of the groups for a comparison it is also interesting to see the effect of varying the number of top channels from 1 to 384 this shows how some features consistently remain in the top and other are sluggish to leave the bottom bands plot we present this as an animation in fig 11 to analyze the contribution of specific channels table 2 shows the ordered top 10 frequently occurring channels across the four classification categories an immediate observation is the very large effect of g5 channels compared to those of the other groups according to cwps g5 channels consistently are among those with greatest effect on the prediction however g5 was not given the highest importance according to the group based xai methods there appears to be a discrepancy but we have three comments as to why this is not an unexpected xai outcome first it is important to keep in mind the difference between feature importance and effect as previously discussed fig 8 shows the results of xai based on the change in overall hss fig 10 shows the results of xai based on the change in output for individual predictions aggregated for a global model summary features may be used by the model without increasing performing or even hurting performance while cwps suggests that g5 channels are heavily relied upon for the hits and correct rejects which would increase performance they are also used for misses and false alarms lowering performance thus it is reasonable that the high effect reported by cwps would not be reflected in feature importance study shown in fig 8 second even if cwps were based on performance it is not true that xai at smaller levels of granularity sum to the equal to the output of xai performed at a higher level xai techniques are highly susceptible to the feature grouping used au et al 2021 this can be illustrated with a simple 2d example classification of a bird photograph given a robust model permuting a single pixel in the bird s beak might have practically no effect on the model s ability to recognize the bird but removing permuting all the beak pixels together might trigger a significant response perhaps causing the bird to be mislabeled thus simply summing the pixel level xai output does not provide the same explanation as the superpixel level xai this example actually occurred when we applied partitionshap to a cnn trained on imagenet data in the case of fognet there is a physical interpretation for some of these groups that suggests that a discrepancy between xai at the group and channel level is not unexpected for example g3 represents the atmospheric profile where each channels are the variable at consecutive heights taking a single channel and evaluating superpixels within it might not be sufficient to break up the overall across channel gradient pattern learned by the cnn however removing the entire group completely removes that pattern and triggers an appreciable influence on model performance in the case of g3 from a meteorological perspective the 2 m dew point temperature non conservative surface moisture proxy and 2 m relative humidity percent of saturation are the only features that directly contribute to fog formation high relative humidity and moist environments are essential for fog development however there is no direct relationship between the other features individually and fog development however collectively all features in g3 are important the profiles of temperature and relative humidity are strongly related to fog development for example radiation fog over south texas typically requires a thin layer of moist saturated conditions near the surface followed by significantly drier aloft and is associated with an inversion temperature increase with height in the lower levels the air temperature at 2 m and at each of the isobaric levels from 975 mb to 700 mb are individually not related to fog however these temperature features are collectively related to fog since their combination determines whether a temperature inversion exists which is related to fog further advection fog events moist air moves over a cooler surface resulting in condensation affecting the target in this study kras are also associated with a lower level temperature inversion yet generally associated with a deeper moist layer than associated with radiation fog the features in group 2 in table 2 a are q specific humidity which individually is strongly related to fog q is defined as the mass of water vapor to the total mass of air and is thus a good measure of moisture content and sufficient moisture is required for fog development more often that not after advection fog develops a persistent supply of moisture is necessary for fog maintenance yang et al 2018 the features tmp sst and dpt sst in group 5 are individually important to advection fog development at the target kras during the october april period advection fog at kras typically occurs when moist onshore flow moves across the cooler shelf waters near the coast and maintains a temperature inversion and moist marine layer near the surface the layer eventually condenses resulting in fog formation this scenario requires the temperature tmp or dew point temperature dpt of the air near the surface 10 m to exceed the sea surface temperature sst finally when interpreting the relative influence of the groups in fig 8 it is important to keep in mind the sizes of the groups while g3 and g4 are suggested to be more important than g5 the latter group has only 12 channels while g3 has 108 and g4 has 60 this could actually highlight g5 importance since it has an appreciable impact on loss despite the group having so few members note from fig 10 that the number of g5 channel counts exceeded that of all other channels for each of the 4 locations on the confusion matrix this is likely due to the following the vast majority of fog events in the training and validation data sets were of the advection type further the g5 features were included specifically to capture advection fog cases therefore we conjecture that during training fognet learned the strong relationship between advection fog and g5 features especially tmp sst and dpt sst thus during every testing set instance fognet would nearly always use g5 channels when making a prediction a similar argument can be made with respect to the g4 channels for hits and false alarms note that the g4 channel counts for positive fog predictions prediction of a fog event far exceeded that of the g1 g2 and g3 counts which illustrates fognet s propensity to use the g4 channels a behavior learned by this cnn during the training process table 2 complements fig 10 by specifying the top 10 channels used by fognet based on cwps for each confusion matrix scenario the top 10 channels are only the following 5 different features at different nam prediction hours vertical velocity at 950 mb vv 950 specific humidity q the difference between the 2 m air temperatures and the mur sst tmp sst the difference between the 2 m dew point temperature and the mur sst dpt sst and the difference between the 2 m air and dew point temperatures otherwise known as the dew point depression tmp dpt all of these channels are in g4 or g5 the vv 950 q and tmp dpt are related to all fog types vv 950 is the most frequent channel used by fognet in connection with positive fog predictions hits and false alarms this channel is included in the microphysics group given its relationship to the activation of cloud condensation nuclei however it is likely that the frequent use of vv 950 by fognet is related to the strong dynamical rather than microphysical relationship to fog in particular fog is generally associated with small vertical velocity magnitudes furthermore advection fog is associated with negative vertical velocities koračin et al 2014 huang et al 2015 the dpt sst and tmp sst features are critically important to advection fog and only to advection fog the condition dpt sst 0 or tmp sst 0 must be met for advection fog to develop again the preponderance of advection fog cases in the training and validation data set and the requirement that the foregoing condition be met for advection fog formation suggests that fognet learned wrongly that fog in general required that the 2 m dew point or air temperature approach or exceed the sst rather than learning that tmp sst and dpt sst are important only to advection fog based on fognet s apparent overreliance on tmp sst and dpt sst to predict fog the features used in all 4 positions on the confusion matrix in table 2 can be explained as follows with respect to hits all 10 bands were the primary ones used to make the positive fog predictions the net effect of the magnitudes of these bands prompted fognet to render a prediction that fog will occur and the vast majority of these cases were advection fogs this is supported by the fact that fognet demonstrated skill in predicting the advection fog events yet the ability to predict radiation fog was poor kamangir et al 2021 for the false alarms the net effect of the magnitudes of the top 10 bands prompted fognet to predict that fog would occur yet was primarily motivated by dpt sst 0 and or tmp sst 0 the condition for advection fog however features within g1 g2 or g3 were probably not conducive to advection fog for example the g2 features may not have been conducive to advection fog decrease in q with height combined with significant tke would likely preclude fog per toth et al 2010 in another example the wind speeds g1 feature could be 2 5 m s 1 and thus no advection fog due to insufficient advection tardif and rasmussen 2007 the missed events can be explained by the likelihood that the dpt sst 0 and or tmp sst 0 conditions were not met and thus given fognet s strong reliance on this condition for fog a negative fog prediction was made however g1 g2 and or g3 features likely favored fog types other than advection fog examples include wind tke and thermodynamic profiles favoring radiation or cloud base lowering fogs croft et al 1997 dupont et al 2016 for the correct rejections there were likely many testing set cases where neither dpt sst 0 nor tmp sst 0 were met and either g1 g2 and or g3 conditions were not met for any other fog type and thus fognet rightly predicted no fog the difficulties of the model to predict radiation cases is due to the small number of such cases in the data set 23 radiation fog cases as compared to 183 advection fog cases although it is doubtful that changes in fognet3d architecture would resolve this challenge the introduction of new features such as mean sea level pressure mslp including tmp u v tke rh and vvel at 1000 mb and surface sensible and latent heat fluxes may improve radiation fog prediction increasing the number of radiation fog cases either by including additional sites beyond kras with a greater percentage of radiation fog cases and or performing data augmentation should also improve radiation fog prediction 4 conclusions in this work we present evidence showing the importance of using 3d convolutional neural networks for improving the accuracy of predictions for a complex atmospheric process the occurrence of coastal fog we show that the 3d convolutions are important to capture the 3d structure of the lower atmosphere including the pbl and to learn the nonlinear complexity of the relationship between the state of the atmosphere and the formation of fog given the 3d nature of extreme atmospheric events the knowledge obtained from this study may apply more generally we also present several strategies to increase performance when using 3d cnns parallel spatial and variable wise feature learning showed a good improvement in fog forecasting versus sequential spatio variable feature learning using 3d kernels given a large number of input variables the grouping of features based on their similar relationship to fog development helps to generate more distinguishable features for the classifier and thereby improving the performance of forecasting due to the 3d structure within the data a unique ordering of features helps the 3d convolutions discover important feature representations such as vertical profiles of temperature moisture and wind also given the complexity of meteorological forecasting a simple cnn based model is not sufficient for efficiently learning the relationships between inputs and targets we introduced and evaluated several techniques that improve the cnn performance including dense block attention mechanism and multiscale feature learning applying xai to study the influence of the features to fognet revealed challenges in interpreting seemingly conflicting results the lack of a guarantee as to which of the many methods is giving the most accurate explanation makes analysis challenging still we argue that overall consistencies among methods give confidence to some interpretations it is encouraging to observe that all the groups are contributing to an increase in model performance a salient observation is the strong effect of all g5 channels on the output we see that g5 is relied upon strongly for correct and incorrect predictions there are patterns learned that hold for the fog but are not rich enough to completely disambiguate fog and non fog other groups contain channels that have a higher effect for misses or false alarms making them candidates for removal for potential performance improvements alternatively one could argue that given fognet s apparent overreliance on features that were designed solely to capture advection fog and that have no significant relationship to the other fog types tmp sst and dpt sst fognet is thus more trustworthy with respect to the prediction of advection fog than with respect to radiation and advection radiation fog thus a solution is to increase the number of fog types other than advection fog in the training and validation data sets to allow the cnn to learn the relationships between the features and these other fog types however it will be challenging for fognet to sufficiently learn the relationships for all fog types according to gultepe et al 2007 it is very difficult to provide accurate numerical fog predictions when the predominate fog generation process is other than dynamical this suggests that it would be easier to predict advection fog rather than radiation fog it is clear that xai techniques are sensitive to how features are grouped and certain groups may provide explanations that are more useful to the modeler a challenge is how to best partition the large raster input into meaningful feature groups here domain knowledge was applied to perform xai on the 5 metocean groups but this may or may not be the optimal grouping to achieve a grouping closer to optimal it may be necessary to increase the number of groups to at least 6 by including horizontal wind vertical velocity and turbulence profiles as g1 retaining q as the sole feature in g2 q should increase with height to support radiation fog development retain the features in g3 identify thermodynamic profiles conducive to radiation fog retain vis as the sole feature in g4 use g5 to account for surface processes which retains the current g5 features yet adds new ones such as sensible and latent heat fluxes that modulates fog development and add g6 to account for the surface synoptic condition by including mslp to acccount for the relationship between radiation fog and the proximity of the surface anticyclone northern hemisphere per meyer and lala 1990 and 2 m equivalent potential temperature to better account for frontal fogs we are currently investigating the role of spatial statistics to select the groups in a data driven fashion based on feature correlations and interactions declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this material is based upon work supported by the national science foundation under awards 2019758 and 1828380 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105424 
25579,the forecasting of hazardous atmospheric phenomena is often challenging artificial intelligence ai models have been applied to atmospheric science problems model complexity provides a motivation to quantify the importance of model architecture components we studied the relative importance of the components of the fognet model that was designed for big atmospheric data 1 3d versus 2d convolution 2 physics based grouping and ordering of meteorological input features 3 different auxiliary cnn based feature learning modules and 4 parallel versus sequential spatial variable wise feature learning we investigate the relative importance of these cnn architectural features by predicting coastal fog a complex spatiotemporal dynamical process we use four explainable ai techniques to better understand input feature contributions the results of the experiments demonstrate that 3d cnn based models better capture the complexity of the fog prediction process than the 2d cnns we also show that physics based feature grouping and the order in which they are fed into the cnns significantly impacts performance keywords 3d convolutional neural network shap explainable ai permutation feature importance atmospheric prediction software and data availability name of software fognet v1 0 developer hamid kamangir evan krell source https github com conrad blucher institute fognet programming language python 3 dependecies tensorflow keras numpy licence mit license data availability north american mesoscale nam 12 km available in grib2 format archived at ftp ftp ncep noaa gov pub data nccf com nam prod nam yyyymmdd 1 introduction convolutional neural networks cnns have been applied extensively to atmospheric science applications in recent years these models are often based on very large scale spatio temporal datasets and the cnn may be required to learn complex non linear relationships to achieve acceptable performance these datasets are often highly imbalanced and predicting infrequent yet impactful events is complicated by the relatively few observations of the weather hazard as compared to the non event occurrences for example the number of non fog cases is much larger than the number of fog cases a model can achieve high accuracy but with no predictive skill by always predicting the non event kumler bonfanti et al 2020 if 95 of the test instances are the non event then 95 accuracy is achieved by simply always predicting no occurrence of for example fog complex cnn architectures have the potential to bring significant improvements for these problems given the large number of network parameters such as the depth and width of the hidden layers the choice of convolutional kernels etc it is challenging to develop high performance architectures hazenet is an example of a cnn for an atmospheric application forecasting severe haze that achieves validation accuracy 95 2 but that produces a large number of false negatives wang et al 2019 the architecture is a conventional cnn based on the popular vgg network simonyan and zisserman 2014 however wang et al wang et al 2021 demonstrated that a more complicated cnn based architecture that incorporated a spatiotemporal attention module performed better than a conventional cnn for quantitative precipitation estimation to better learn the underrepresented severe precipitation the loss function was weighted by rain intensity fognet kamangir et al 2021 is an example of a cnn architecture that achieved high performance predicting coastal fog outperforming for example the high resolution ensemble forecast href an operational ensemble of numerical weather prediction nwp models a complex cnn based architecture was developed to avoid overfitting and learn the process dynamic despite the relatively low number of fog cases the architecture is based on 3d convolutions physically based feature groupings dense blocks and attention maps this study uses the case of coastal fog predictions to investigate the benefits of some of these more complex features of cnns it has been shown that when processing spatio temporal images or images with a large number of bands such as hyperspectral imagery 3d cnn based models outperform the conventional 2d cnn based variety by learning the complexity of the auto correlated input dataset ma et al 2019 he et al 2017 the 3d cnn based models are able to learn not only 2d spatial patterns and correlations between groups of pixels and a target but also learn spectral correlations between bands or temporal correlations between input variables however 3d cnn based models are more computationally expensive compared to 1d or 2d cnn based models due to the larger number of parameters to train furthermore if the order of the input variables or bands in a 3d image cube do not matter there is no reason for the use of a 3d cnn based architecture li et al 2017 fognet is a recent 3d cnn based model trained on an atmospheric data cube with a large number of inputs 384 input variable maps and with an architecture that is hypothesized to benefit from a physics based ordering of the input variables fog is a meteorological phenomenon consisting of very small water droplets near the earth s surface that reduces visibility to less than 1 km glickman 2000 wmo 2020 the low visibility associated with fog has an adverse effect on the transportation sector and contributes to vehicular and aviation accidents gultepe et al 2019 das et al 2018 fog droplets develop due to condensation within an environment characterized by high relative humidity which can range from unsaturated to supersaturated gultepe et al 2007 high relative humidity can occur due to the addition of water vapor cooling or near surface mixing of air parcels with different temperatures glickman 2000 gultepe et al 2007 condensation into water droplets is aided by hygroscopic aerosol particles known as cloud condensation nuclei the visibility reduction is due to what is termed the first indirect effect whereby aerosols contribute to a cloud drop size distribution characterized by a preponderance of smaller droplets resulting in a larger surface to volume ratio and subsequent extinction and lower visibility twomey 1974 koračin et al 2014 fog occurs within the planetary boundary layer pbl the lowest layer of the atmosphere that is directly influenced by the earth s surface the pbl responds to surface forcings in a timescale of 1 hour or less stull 1988 in particular the warming and cooling of the earth s surface in response to radiation results in pbl changes via transport processes the vertical transport of moisture heat and momentum is dominated by turbulence while horizontal transport is accomplished by the mean wind stull 1988 stensrud 2009 thus these 3d transport processes directly contribute to pbl structure the thickness of the pbl can range from 100 m to 3 km in time and space stull 1988 specific fog types tend to occur in association with unique atmospheric vertical structures for example an atmosphere characterized by a thin moist layer near the surface and much drier air aloft under clear skies and light wind is conducive to radiation fog further warm moist air in the lower levels approaching the middle texas coast united states with or without stratus clouds aloft contributes to the development of advection fog along the coast thus the incorporation of 3d cubes of meteorological fog predictor variables within the lower atmosphere would capture the vertical and horizontal patterns corresponding to fog and possibly account for the 3d non linear processes contributing to fog formation potentially resulting in skillful fog predictions gultepe et al 2007 emphasized the importance of 3d prediction models to better predict various fog types 1 1 contributions our work makes the following contributions quantifying the advantage of using 3d vs 2d kernels to capture interactions between variables and within vertical atmospheric profiles in addition to spatial features investigating the impact of physics based grouping and ordering of atmospheric input variables for a 3d cnn model investigating the importance of several feature learning modules for 3d cnn to better capture the complex interactions of meteorological input variables for fog prediction including dense block attention mechanism and multiscale feature learning comparing the impact of learning spatial wise and channel wise features in parallel or in sequence investigating the importance and contribution of individual meteorological variables features and each input feature group for fog forecasting by using four explainable artificial intelligence xai techniques 2 methods 2 1 2d convolutional feature learning the core operation of cnns is convolution over images to extract lower dimensional features a convolutional kernel is defined that repeatedly operates in a local window defined by the size of the kernel the kernel acts as a moving window across the image s dimensions to calculate all the pixel values of the output feature map in traditional image processing kernels are manually designed to detect desired features simonyan and zisserman 2014 for example the sobel operator uses two 3 3 kernels to detect edges one for horizontal edges and the other for vertical edges based on an approximation of the gradient at the center pixel location kanopoulos et al 1988 many other kernels exist to perform operations such as blur sharpen detect other edge angles etc these kernels are routinely used for image processing including recognition tasks such as classification specific classes can be characterized by the combined outputs of a set of manually selected kernels traditionally image classification was based on hand crafted kernels for feature extraction however it is challenging to select the kernel values that best support image recognition tasks this can be formulated as an optimization problem to select the values that minimize classification error thus manual feature extraction can be replaced with data driven learned feature extraction cnns have been shown to be an effective machine learning approach for automatic image feature extraction krizhevsky et al 2012 simonyan and zisserman 2014 huang et al 2017 convolutional layers perform convolution using kernels whose values are trainable parameters a typical architecture contains convolution layers for feature extraction followed by fully connected layers to make a prediction based on potentially highly nonlinear relationships between the features and the target class this approach is not limited to gray scale 2d or rgb 3d visual images but rather rasters of arbitrary dimensions in the case of meteorological applications the rasters spatial dimensions typically represent a discretized spatial region while the channels bands represent separate environmental variables such as temperature wind speed or relative humidity a 3d raster of meteorological variables is illustrated in fig 1 a even when working with multi channel inputs such as rgb images the majority of cnn applications focus on 2d convolution that is a 2d kernel is applied as illustrated in fig 1b even though the kernel is calculated over multiple image channels each operation involves raster values within a single channel thus the output is 3d but each output channel contains only spatial wise features alternatively 3d convolution uses a 3d kernel to operate across channels as well as spatially to extract 3d features see fig 1c 3d convolution will be discussed in section 2 2 2 1 1 benchmark 2d cnns in this section three of the most common 2d cnn based benchmarks alexnet krizhevsky et al 2012 resnet he et al 2015 and densenet huang et al 2017 2019 are discussed each of these architectures advanced the state of the art in visual recognition and are commonly used image classification benchmarks while initially designed for rgb images they can be adapted to support an arbitrary number of channels in 2012 alexnet krizhevsky et al 2012 won the imagenet large scale visual recognition challenge ilsvrc 2012 and demonstrated that increasing the number of hidden layers dramatically enhances model performance relatively shallow networks were the norm given the computational expense of learning the larger number of weights in deeper networks however gpus were used to make it feasible to train the 8 hidden layers of alexnet and demonstrated that deep learning can dramatically outperform models based on human selected features the success of alexnet led deep learning researchers to explore increasingly deeper and more complex architectures ilsvrc 2014 was won by using even more hidden layers two variants of the vgg architecture vgg 16 and vgg 19 were used where the 16 and 19 designations refer to the number of hidden layers eventually however additional layers were providing diminishing returns or even worse performance the major problem was the vanishing or exploding gradient applying backpropagation along the deep hidden layers results in multiplying so many weights that they either become 0 or arbitrarily large the major contribution of resnet winner of ilsvrc 2015 was architecture design techniques that would allow cnns to efficiently scale to hundreds of hidden layers for improved model performance he et al 2015 skip connections were introduced that mitigate the vanishing exploding gradients these are connections that flow from the input to each layer skipping over the convolutions to promote gradient flow also bottleneck layers were included throughout the network for dimension reduction to limit the number of parameters to learn by doing so resnet is able to have much deeper models than vgg while being significantly less complex this was shown to enable feasible training of large models such as resnet 152 that is a specific configuration of the resnet architecture with 152 hidden layers that actually have less parameters to learn than vgg 16 he et al 2015 in 2017 huang et al developed densenet that was able to outperform resnet by using dense blocks huang et al 2017 with the dense block every layer is connected to all subsequent layers at each layer input is the channel wise concatenation of feature maps output from all previous layers this is an extension of the skip connection concept but promotes learning by allowing each layer to consider information from all previous layers thus features learned at each layer are used more efficiently since all the subsequent layers have access this allows the network to learn with fewer hidden layers in addition like skip connections the feed forward propagation of features avoids vanishing exploding gradients these architectures have been shown to be effective in many domains besides rgb image recognition including recent weather forecasting applications resnet was used by rasp and thuerey rasp and thuerey 2021 for 5 day weather forecasting first climate simulation data was used to train an initial model then transfer learning was used for additional training on real climate data the model predicts geopotential temperature and precipitation given satellite imagery zanchetta and zecchetto 2021 trained a resnet model with sentinel 1 satellite data to estimate wind direction over sea convolutions were performed over synthetic aperture radar sar images to learn to predict 2 km 2 km wind direction fields a modified resnet was implemented by bosma and nazari 2021 to predict solar and wind energy production like fognet the input raster channels were weather data rather than visual imagery the raster was a 155 108 spatial grid with 6 data channels pressure temperature humidity wind speed wind direction and cloud cover 2 2 3d convolutional feature learning 2d convolutional kernels extract the spatial correlation between pixels for each feature map however 2d convolutional kernels take a single map as input so they fail to leverage context from adjacent feature maps 3d convolutional kernels address this issue by moving the kernel in 3 dimensions depth height and width as illustrated in fig 1a the ability to leverage inter depth of the image and to learn context in correlation between different feature maps and channels can lead to improved performance for meteorological applications since there are meaningful relationships between different meteorological variables for event occurrence especially variables of the same type such as wind speed or temperature at various heights above the ground but using 3d convnets comes with a computational cost as a result of the increased number of parameters required by a 3d cnn based architecture recently 3d convolution kernels have been used in different deep learning architectures for weather and meteorological prediction niu et al 2020 wang et al 2020 castro et al 2021 niu et al 2020 proposed a new architecture for short time precipitation prediction based on a multi channel convlstm convolutional long short term memory and 3d cnn this architecture was trained on radar echo intensity data for 2017 2018 of south china with 1 km spatial resolution and 12 min intervals they have shown for such time series data having lstm long short term memory with 3d cnn works better than only a 3d cnn based model wang et al wang et al 2020 applied a 3d cnn based model for tropical cyclone intensity change prediction over a short temporal range of 24 h they used 8 input variables including temperature relative humidity wind velocity u and v wind components geopotential height and sea surface temperature with 0 125 0 125 spatial resolution and a 6 h temporal interval castro et al castro et al 2021 proposed a 3d cnn based model called stconvs2s for weather forecasting and specifically air temperature and rainfall were tested this architecture uses two different blocks to extract spatial and temporal representations of the input sequence data and they also used a temporal generator block on top of a spatial block to increase the sequence length of the time prediction to extend the applicability of 3d cnn based models the 3d cnn based model used for fog prediction developed by kamangir et al 2021 called fognet3d is explained in the next subsection section 2 2 1 fognet3d also different auxiliary modules for feature learning used by fognet including dense block attention mechanism and multiscale feature learning using 3d dilated convolutions has been explained 2 2 1 fognet3d the fognet3d kamangir et al 2021 model shown in fig 2 starts with separating the processing of input variables into five different groups based on their similar physical relationship to fog development each subgroup consists of a double parallel branch dense block feature extraction spatial wise and variable wise with an attention mechanism the variable wise and spatial wise feature outputs for each subgroup from step 1 are concatenated into two main feature groups in the next step for each feature type a 3d multiscale layer using dilated feature learning is used to extract new representation maps at different resolutions at the end the variable and spatial wise features are fused by using global average pooling and then a binary classifier used to generate a probability for fog or no fog we investigate in detail the impact of the following five different characteristics on fognet3d performance physical grouping of meteorological variables overfitting is a big challenge for all machine learning models especially when there is a high correlation between input variables which make the generalization of the model harder specifically for fognet there are between 288 384 input variables that have physical correlation with other variables across and within input categories the input data is categorized into 5 different groups based on their similar physical relationship to fog development as described below group 1 emphasizes the influence of wind and contains the wind related features fricvsurface surface frictional velocity u10 meters v10 meters u and v wind components at 10 m height elevation u975 700 and v975 700 u and v wind components at atmospheric pressure levels 975 mb 700 mb at 25 mb increments group 2 focuses on the influence of the combined effect of turbulence kinetic energy tke and specific humidity q and contains features tke975 700 and q975 700 turbulence kinetic energy and specific humidity respectively at pressure levels 975 mb 700 mb at 25 mb increments group 3 incorporates the thermodynamic profile of the lower atmosphere and contains the features tmp2 meters dpt2 meters rh2 meters air temperature dew point temperature and relative humidity respectively at 2 m height elevation tmp975 700 and rh975 700 temperature and relative humidity respectively at pressure levels 975 mb 700 mb at 25 mb increments group 4 accounts for the influence of surface atmospheric moisture and microphysics and includes vis surface visibility qsurface 2 m specific humidity tlcl temperature at the lifted condensation level and vv975 700 vertical velocity at pressure levels 975 mb 700 mb at 25 mb increments group 5 accounts for surface variables that control advection fog formation including features sst sea surface temperature dpt2 meters sst difference between 2 m dew point temperature and sst and tmp2 meters sst difference between air temperature and sst also tmp2 meters dpt2 meters difference between 2 m temperature and 2 m dew point otherwise known as the 2 m dew point depression which is proportional to relative humidity this helps to decrease the complexity of the input data and extract the correlated features individually from each group and then combine them for the next step to provide more distinguishable features for the classifier parallel learning of spatial and channel wise features previously for spatiotemporal meteorological data castro et al 2021 and remotely sensed hyperspectral data ma et al 2019 it has been shown that separately learning representations of spatial wise and channel wise or temporal wise features lead to better performance specifically the impact of parallelizing the feature extraction of meteorological variables is investigated in the fognet architecture spatial and variable wise dense blocks when cnns go deeper the path for information from the input layer to the output layer becomes too long and gradient vanishing in the opposite direction from the output to input layer is a challenge densenets huang et al 2019 address this issue by simply connecting every layer directly with each other and all the previous layers and reusing instead of drawing representation power from extremely deep or wide architectures fognet takes advantages of two different dense blocks step 1 fig 2 a spatial dense block with a kernel size of 3 3 1 to learn representation in the spatial domain of each feature map and a variable wise dense block with a kernel size of 1 1 9 to learn the correlation between different input variables spatial and variable wise attention blocks attention mechanism xu et al 2015 has been proposed to pay more attention to certain features when processing the data by cnns attention mechanism manages and quantifies the interdependence between the input variables and the output elements by focusing on the most informative parts and suppressing the weights of other regions fognet consists of two different attention modules including a variable wise attention module step 2 fig 2 to focus on informative input variables and a spatial wise attention module to extract informative areas from each input variable map multiscale feature learning multiscale feature extraction using convolutional kernels has been effective for classification problems he et al 2017 srivastava et al 2014 this is in part because multiscale convolutions have the power to extract more complex combined spatial spectral features the meteorological data includes 3d patterns with different spatial resolutions which have the potential to be quantified by using different kernels and receptive fields dilated convolution using expansion of receptive fields aggregates multiscale contextual information without loss of resolution or coverage yu and koltun 2015 in fact dilated convolution modifies the convolution filter in different ways at different ranges using different dilation factors in fognet step 4 fig 2 a multiscale 3d dilated convolution block is used to learn more complicated meteorological features 3 results discussion 3 1 study area and features the fognet study domain includes a portion of the texas coast and the adjacent western gulf of mexico see fig 3 and is organized as a 32 32 horizontal grid with 12 km grid spacing the domain 384 km 384 km is sufficiently large to account for atmospheric processes driving the formation of fog at the target location over a 24 h period orlanski 1975 the maximum forecast length of the fognet predictions the fognet features predictor variables originate from a numerical weather prediction nwp model the north american mesoscale nam modeling system used operationally by meteorologists in the national weather service united states and from satellite imagery the specific features used were chosen to predict fog by capturing the fog development process or the lower atmospheric structure consistent with fog development for the specific fog types that typically occur in the study domain these fog types corresponding mechanisms include radiation fog nighttime radiational cooling of moist air to saturation within a stagnant environment under clear skies in association with a high pressure system advection fog typically the cooling to saturation of moist onshore flow by cool shelf waters along the texas coast advection radiation fog advection of near surface moisture onshore during the day followed by the radiation fog development at night frontal fog 3 types 2 of which involve rainfall which evaporates and moistens the sub cloud layer to saturation either in a post cold frontal or pre warm frontal environment and one involving the mixing of distinct airmasses during frontal passage and stratus lowering fog radiational cooling of the air at cloud top which is transported downward by turbulent mixing and cools the sub cloud layer to saturation and or settling of cloud drizzle drops that fall below cloud base evaporate and cool the sub cloud layer to saturation resulting in the lowering of the cloud base to the surface see table 1 in kamangir et al 2021 for detailed information regarding the nam and the selection of the features the target used for fognet originates from visibility measurements from the automated weather observing system awos site at the mustang beach airport kras latitude 27 8118333 n longitude 97 0887500 w in the coastal city of port aransas texas this awos was provided by vaisala inc which provides awos model aw20 which is certified by the u s department of transportation federal aviation administration faa and meets the faa awos advisory circular 150 5220 16 for facilities that are not federally owned vaisala 2015 faa 2017 the aw20 vaisala present weather detector sensor pwd22 generates 15 s visibility values that are averaged to generate 1 min and 10 min output values vaisala 2004 the visibility value generated by awos for the user is the 10 min harmonic average the accuracy of the pwd22 sensor for visibility is 10 percent from 10 to 10 000 m and 15 percent from 10 to 20 km vaisala 2018 vaisla inc provided the awos instrumentation for kras from 2011 to the present the aw20 model was installed in 2018 11 march 2022 personal communication from randy hansen airport manager mustang beach airport the target vector was developed as follows each kras visibility measurement in the dataset is converted to one of 4 visibility categories 1600 m 3200 m 6400 m 6400 m all visibility measurements 6400 m caused by a weather phenomenon other than fog or mist were removed from the dataset thus fognet was trained to predict visibility restrictions due only to fog or mist the training validation and testing data were extracted from the 2009 2020 time series of nam nwps the 2012 2017 part of the data was used for the training of the model 5 460 cases 50 2009 2012 data was used for validation 3 328 cases 30 and the remaining of the data 2018 2020 2 228 cases 20 was used for an independent assessment of the model after the completion of the calibration 3 2 2d vs 3d convolutional feature learning for this comparison three 2d cnn architectures were selected for comparison with fognet each of the three architectures were trained on multiple hidden layer depths the three models trained were resnet 152 densenet 121 and densenet 201 the numbers refer to the model s number of hidden layers each model was trained using the adam optimizer for 100 epochs with a batch size of 64 a dynamic learning rate was used beginning with an initial value of 0 1 the deep learning framework pytorch paszke et al 2019 was used to train each of these models we use the torchsat sshuair 2020 package which includes pytorch implementations of alexnet resnet densenet and other popular cnn architectures torchsat is similar to the popular torchvision but supports an arbitrary number of channels where torchvision supports only grayscale 1 channel and rgb 3 channels torchsat allows us to train a fog detection model using the 384 channel input raster it is extremely common to use transfer learning when training these cnns that is the initial weights are based on training on very large datasets such as imagenet the new model is able to take advantage of features already learned on the large scale dataset and is adjusted with additional training to suit the new problem domain transfer learning has been shown to be effective even when the target dataset differs considerably from the original such as satellite images gadiraju and vatsavai 2020 however based on the substantially greater number of channels their non visual nature and for a fairer comparison with fognet which did not use prior training transfer learning was not performed to construct these benchmarks a drawback of using off the shelf cnns is that they expect a single raster input for visual image inputs it is reasonable to assume that all the channels will have the same dimensions but when the grids are temperature wind etc they may be of various sizes important features may be lost if scaling is used to construct a single raster fognet performs the scaling with a dimension reduction component of the model specifically the sst is transformed from 384 384 to 32 32 since the scaling is performed through convolution the scaling that best helps the model to extract discriminating features is learned for the benchmarks however the sst is simply downsampled with gaussian smoothing for anti aliasing for all the experiments run with fognet3d the same hyperparameters have been used to find the best fognet hyperparameters a grid search section 11 4 3 of goodfellow et al 2016 was applied in all the experiments the model is trained for 50 epochs with 32 batches per epoch on 5 460 training samples and 3 328 validation samples for all the experiments the learning rate lr is held constant at 0 0009 and dropout and l2 regularization are 0 4 and 0 001 respectively the same architecture as fognet3d was implemented to create a fognet2d by using 2d convolutional kernels instead of 3d convolutional kernels the purpose of fognet2d is to investigate the impact of 3d convolutions used in fognet3d as compared to the 2d kernels in fognet2d all experiments in this work besides the 2d benchmarks were trained using the keras python package chollet et al 2018 in this section the results for 10 iterations for fognet3d has been compared with 2d kernel based models including fognet2d densenet121 densenet201 and resnet152 based on the peirce skill score pss heideke skill score hss and the clayton skill score css verification performance metrics for deterministic forecasts of binary events and area under the receiver or relative operating characteristic curve auc for probabilistic predictions of binary events the hss and pss measure the accuracy relative to the accuracy achieved by random forecasts the accuracy measure used by the hss pss is the proportion correct hit rate the proportion correct is the fraction of all forecasts that were correct the hit rate measures the fraction of observed events that were correctly forecast the values of both metrics are within the 1 1 range and skill is demonstrated with values greater than zero the css measures the difference between the conditional probability of an event given a forecast that the event will occur and the conditional probability of an event given a forecast that the event will not occur skill is achieved when css 0 which indicates that the event occurs more frequently when forecast than when not forecast the value of 1 for any of these 3 metrics demonstrates a perfect forecast system the pss and css metrics are related to economic value pss represents the maximum potential economic value realized by users of the forecast system with cost loss ratios equal to the base rate climatology the css represents the range of cost loss ratios for which users gain economic value from the forecasts see jolliffe and stephenson 2003 wilks 2011 for more information regarding these metrics based on the results shown as a box plot in fig 4 fognet3d has the best performance with an auc of 0 94 with ci 0 95 and highest score for pss avg 0 52 hss avg 0 50 and css avg 0 48 without overlapping of the interquartile range with all other 2d cnn based models also the interquartile range boxes especially for hss and css of fognet3d show a better stability and low variability of training process in contrast the best 2d cnn model was fognet2d which has an average auc of 0 93 while 2d cnn benchmarks densenet121 dense201 and resnet152 are the next best performing models with average auc of 0 90 0 88 and 0 74 respectively results show that for such a meteorological prediction application with having large number of variables 3d convolutional feature learning is better able to learn the complex 3d structure of the atmospheric profile in order to generate more accurate and skillful predictions this performance enhancement is not surprising since atmospheric processes in nature occur in 3d the better performance of fognet2d in comparison to the 2d cnn based benchmarks including densenet121 201 and resnet152 shows that the auxiliary feature learning modules used in the fognet architecture discussed in subsection 3 3 improve performance we also summarized the computational cost of the 2d 3d cnn models based on our desktop 4 gpus nvidia rtx 1080s the time of training per epoch was much higher for the 3d models than for the 2d models with the same batch size it took more than 3 h to train a 3d model whereas the training time of 2d models was only half that time once the trained models were used for prediction the difference of the computation costs between the 2d and 3d models was narrow namely 0 02s vs 0 03s for processing a single image 3 3 ablation study of fognet components in the last few years several methods were developed to improve the performance and efficiency of cnn based models these new methods include densenet and resnet which extract features at different resolutions using dilated convolutions applying the attention mechanism etc fognet applies several of these modules to better approximate the complex relationship between input meteorological variables and fog prediction to control overfitting and to better generalize when applied to novel data in this section we discuss an ablation study to evaluate the modules used by fognet the value of using an attention module multiscale feature extraction and spatial and spectral dense blocks was investigated by removing those modules from the fognet3d architecture and then comparing the results with the base performance of fognet3d also fognet3d uses two parallel branches one for variable wise and one for spatial wise feature learning to extract these two types of features separately and then fuses them before classifying to better understand the contribution of this strategy the parallelism is removed and instead the spectral features are extracted first using a variable wise dense block and then those feature maps are fed into the spatial wise dense block to extract spatial features the alternative of extracting the spatial features was not attempted since if we first extract spatial features then the outputs are feature maps generated by kernels and there are no more raw input variables to extract and build correlations from hence in this experiment we only test the first strategy and compare its performance with parallel feature learning as shown in fig 5 fognet3d shows improvement over most but not all of the derivative models in this ablation study indicating that the fognet3d modules are generally beneficial to the performance of fognet3d as we mentioned in section 2 2 1 attention mechanism is a new methodology to magnify the most important areas for each map and the most important feature maps for the classifier or decision maker to investigate the importance of applying an attention mechanism for meteorological prediction applications we compare the results of fognet3d with and without attention mechanism the results for skill metrics in fig 5 show that the performance of fognet3d has decreased mainly for pss since there is no overlapping between range values of their box plots based on the results in table 1 on average the results for fognet3d without attention mechanism has a score between 4 5 points lower for pss hss and css to consider the importance of spatial wise correlation and variable wise feature map correlation learning we ignore the impact of dense block feature learning by itself more intuitively we are considering the importance of the spatial correlation between pixels and auto correlation between different variables for meteorological applications to do so we remove the spatial dense block in fognet3d and the same thought for variable wise feature learning by removing the variable wise dense block as we can see in fig 5 by removing the spatial wise dense block the performance for fognet3d decreased between 10 14 points for pss between 5 10 points for hss and between 2 3 points for css in comparison with spatial wise variable wise feature learning decreases the performance less only between 2 5 points for each of the skill metrics the results for these two experiments may not be very informative since we have both spatial and variable wise feature learning in multiscale block before the classifier for a combination of all different groups but as we can see these results show the impact of spatial correlation between pixels and variable wise correlation between maps for meteorological problems in this case fog prediction it has been shown that for 3d cnn based hyper spectral remote sensing image processing separately learning spatial and spectral correlation can result in better performance ma et al 2019 to do so one strategy is learning these types of features in parallel and then fuse them before decision making fognet3d uses a parallel strategy where each of the feature types spatial and variable wise is learned separately in parallel and then fused before classification another strategy is sequential learning which is a common approach in cnn based models where variable wise features are learned first and then fed into spatial wise feature learning or vice versa the results in fig 5 shows that applying the sequential strategy decreased the performance of fognet3d mainly for hss and css with no overlap in the respective ranges of the second and third quartiles in fact this experiment introduces a new idea that the separate learning of spatial correlation between pixels for each map and auto correlation between different input maps might help improve the performance of cnn based models for meteorological applications due to the complex interaction between meteorological variables and event occurrence for meteorological applications along with imbalanced conditions and high complex correlation between the variables using only current cnn based computer vision techniques is insufficient in this section several modules to improve the cnn based model with their specific contribution to fognet3d s performance are introduced based on our results shown in fig 5 each of the modules attention mechanism spatially and spectral dense block feature learning multiscale feature extraction by using dilated convolution parallel extraction of spatial and variable wise features has made varying levels of contribution to the performance of fognet3d suggesting that they are useful modules for cnn based meteorological applications for fognet3d the more than 200 input meteorological variables were categorized into 5 different input groups each based on their similar physical relationship to fog development we investigated the impact of grouping input variables in such a 3d cnn based model by training the fognet3d model using all input variables in only one cube the results in fig 6 show that auc decreases from 0 95 to 0 91 for fognet3d without grouping the input variables the box plot results for pss and hss also clearly indicate that performance deteriorates with similar results for css also based on the results for the average of 10 training runs table 1 pss is 35 lower for fognet3d without grouping 0 52 for fognet3d compared to 0 34 for fognet3d without grouping 22 lower for hss 0 50 for fognet compared to 0 39 for fognet3d without grouping and 6 lower for pss 0 48 for fognet3d compared to 0 45 for fognet3d without grouping the results for this experiment show that tying the parameters of several parallel networks for each group leads to an improvement of fognet3d performance 6 35 for the skill metrics this means that reducing the number of free parameters by sharing them between group feature learning leads to better generalization by reducing overfitting 3 4 importance of meteorological variable order in atmospheric cube given a 3d weight tensor channel order is important which means changing the order of input channels would change the performance of the cnn model also for meteorological applications due to the 3d nature of atmospheric processes and the associated correlation between different variables in the atmospheric cube the order of variables in a 3d cube for a 3d cnn is important in this experiment to investigate the importance of physically ordering of the input variables in a cube we shuffled the order of the input variables for all variables in fognet3d model and check the performance with fognet3d without shuffling the features in fognet were chosen to capture the vertical structure of the lower atmosphere including the pbl and to utilize atmospheric variables that modulate fog development thus within groups 1 through 4 many features were ordered sequentially in the vertical direction by atmospheric pressure level 975 mb 700 mb at 25 mb increments such that the feature representations to the machine learning model are vertical profiles of variables that influence fog development the ordering of features in group 5 was arbitrary since the focus was only to capture the advection fog process based only on surface and near surface variables fig 7 shows the results for fognet3d with and without shuffling the average of 10 training auc scores for fognet3d with shuffling is 0 06 lower than fognet3d showing degradation in performance for fognet3d with shuffling also the box plots of the results show a large gap in performance between fognet3d with and without shuffling pss for fognet3d with shuffling is around 0 13 0 16 vs 0 48 0 58 for fognet3d and hss for fognet3d with shuffling is around 0 15 0 20 vs 0 47 0 51 for fognet3d for css there is overlap between the respective second and third quartile of the result distributions however this is due to the large range of the third quartile for the shuffled cases which could be due to two or three outliers given that the results are based on ten cases it is likely that more repetitions would result in a narrower confidence interval and the median css for the shuffled case is substantially lower than for fognet3d also based on the average of 10 training runs given in table 1 the pss score is only 0 14 for fognet3d with shuffling compared to 0 52 for fognet3d hss is only 0 16 with shuffling compared to 0 50 for fognet3d and for css the score with shuffling is 0 35 compared to 0 48 for fognet3d the results after shuffling the feature maps show that for 3d kernel cnn based models the order of input variables for meteorological application is important because in this situation the potential connections between input variables that have a high correlation regarding the event prediction have been removed so the feature maps generated by 3d convolutional kernels are not meaningful for the model to make a skilled decision to contextualize from a meteorological and representative learning perspective temperature moisture and wind related features in groups 1 through 4 within fognet3d were ordered sequentially in the vertical direction at atmospheric pressure levels from 975 mb to 700 mb at 25 mb increments to form vertical profiles of the lower atmosphere both physically consistent with those that occur in nature and strongly related to fog development these profiles became feature representations to the fognet3d architecture which allowed fognet to relate these profiles to fog prediction during model training for example consider the wind profile generated by the u and v wind components g1 features a clockwise turning of wind direction with increasing height above kras corresponds to warm air advection a component of the advection fog process along the texas coast united states during the fog season in addition strong vertical wind shear near the surface the u v wind at the adjacent 10 meter and 975 mb levels can preclude radiation fog further an increase in q specific humidity with height in the lower levels g2 is essential for radiation fog finally radiation fog generally requires specific temperature and relative humidity profiles g3 which depict a thin nearly saturated layer near the surface followed by much lower relative humidity values aloft within a temperature inversion when the features were shuffled training of the fognet3d shuffled model was likely unsuccessful in capturing the relationship that maps the profiles of temperature moisture and wind to fog in other words the g1 g2 and g3 profiles mentioned above that correlate to fog are destroyed when the features are shuffled hence the significant drop in performance e g hss drops from 0 50 to 0 16 table 1 presents an extended set of metrics for all of the models tested this includes fognet3d the 3d cnn based model all the variations of the fognet3t3d of the ablation study section 3 3 removal of the physical grouping section 3 4 along with several popular 2d cnn based models resnet152 dense121 densenet201 section 2 1 1 and fognet2d section 3 2 3 5 investigating the influence of meteorological variables due to their black box nature it is difficult to determine how a trained model uses the data to make predictions however it is useful to have some understanding of the strategies learned by the model this has motivated the rapidly developing field of xai where various methods have been proposed that probe the model in some way to learn about the model s input output relationships see murdoch et al 2019 for example lapuschkin et al 2019 demonstrates that a model performing well even on the testing dataset may rely on spurious associations in the dataset that would lead to poor real world performance mcgovern et al 2019 provide a detailed discussion of the application of xai for meteorological models unfortunately there is no single technique guaranteed to provide a complete and accurate explanation of the model molnar et al 2020 properties of the data such as dependencies and interactions can yield misleading explanations molnar et al 2020 similarly model architecture can influence xai usefulness such as the gradient shattering that may occur in very deep models mamalakis et al 2022 since the true explanation is unknown mcgovern et al 2019 suggests applying multiple xai methods when multiple methods consistently highlight certain features it suggests that those features are truly influential for the model we have applied four xai methods to investigate the influence of input raster features on fognet3d three of these group hold out permutation feature importance and lossshap are used to analyze the importance of the five metocean groups feature importance is based on how much each feature here a group of adjacent raster channels affects the overall model loss the three methods and their results are described in section 3 5 1 we have also use channel wise partitionshap to analyze the feature effect of individual raster channels feature effect is based on how much the feature channel contributes to an output prediction this technique is discussed in section 3 5 2 in a discussion of xai pitfalls molnar et al 2020 warns against confusing feature importance and effect when interpreting xai outputs here we use the term feature influence to collectively refer to both importance and effect 3 5 1 group wise feature importance we are interested in the relative importance of each of the five metocean variable groups the features where chosen based on their predictive relationship with fog but that does not guarantee that the model learned to take advantage of them this could be because it did not need to that is they did not provide significant additional information compared to other relationships learned or they would be useful for the model but the model was unable to learn them perhaps because of their complexity or a lack of variation in the training data to analyze the group importance we use three xai techniques that calculate global feature importance scores based on how the absence of each group changes the hss intuitively if removing a group g i causes a 10 reduction in hss and another group g j causes only 4 reduction then g i is assigned higher importance toward model performance group hold out the most straightforward strategy is to simply remove entire groups and retrain for each group that group is removed from the fognet3d architecture and the model is retrained because of variations between trained models 10 trials are performed for each group for each trained model we calculate the change in hss to that of fognet3d hss 0 5 each group s importance score is the average of the 10 trials the advantage of group hold out is that it directly tests the impact of the input features on model performance this is in contrast to the other methods that as will be discussed use techniques to imperfectly simulate the removal of features the group hold out approach is often not used in xai studies because every hold out requires retraining the model or as here multiple times per feature to obtain an average to perform group hold out to evaluate every element of the 32 32 384 fognet input raster would be computationally impractical but to do so for only five grouped features is tractable given that we are able to use group hold out directly a reasonable question is why other feature importance methods are needed for group wise analysis the major disadvantage of group hold out is that it does not reveal the importance for the specific trained model under investigation it is reasonable to expect that the group hold out results are similar to that of an individual model but the variation is such that a given model might have learned strategies not represented by the repeated retrainings used to generate the group hold out score molnar 2020 discusses disadvantages of testing importance with retraining with an example of how it can potentially produce misleading interpretations of feature importance instead molnar 2020 recommends using permutation feature importance permutation feature importance pfi similar to group hold out pfi mcgovern et al 2019 calculates feature importance based on model performance here hss with and without features being present but since models almost always take in a fixed size raster input the feature cannot be simply removed instead of retraining pfi simulates feature removal by permuting the feature s values here the values within the permuted group are randomly shuffled to break the relationship between input variables and the target one issue with pfi is that the input features generated from shuffling are not actually the same as completely removing the feature and the model output may simply reflect the response to unrealistic out of distribution data rather than no data as desired also pfi may struggle with correlated features as the importance scores may be divided among the group of correlated features with fognet we expect input data to have strong correlation by design across the 32 32 spatial maps and across channels that represent vertical atmospheric profiles with pfi the importance scores may be diluted across the features such that none appear important despite being used by model a way to mitigate this is by grouping features molnar et al 2020 here we expect that the 5 groups are distinct enough to allow meaningful pfi scores again see molnar 2020 for a discussion of the advantages and disadvantages of pfi lossshap lundberg et al 2019 proposed a game theoretic alternative to pfi inspired by shapley values shapley values have been proposed to add rigor to xai messalas et al 2019 fryer et al 2021 based on cooperative game theory shapley values are a fair assignment of payout to each player in a game based on their contribution to the outcome applied to xai the game is an individual model prediction and the players are the features the shapley values describe each feature s contribution to the model output shap lundberg and lee 2017 is an implementation for approximating shapley values to calculate feature effect this is a local xai technique meaning it explains the contribution of the features for a single input instance lossshap is a version of shap used to calculate global feature importance lundberg et al 2019 with shap effect scores are based on the contribution of a feature to a specific output instead lossshap applies shap s strategy for permuting the features based on the marginal contribution but the entire dataset is used to calculate the difference in loss here hss to measure performance for group wise lossshap an importance score is calculated for each of the groups by averaging over the marginal contribution of that group based on all combinations of including or not including the other four a group is said to be removed by replacing all cells with random values this is similar to pfi but with a critical distinction that allows it to take into account dependencies between groups suppose the feature under evaluation is group 3 g3 pfi simply compares the model output with and without replacing g3 with permuted values lossshap does this multiple times but each time with some of the other groups also permuted to calculate the lossshap value for g3 it is necessary to test with groups 1 2 4 and 5 present then with 1 2 4 present and 5 permuted and so forth for all possible combinations because minor variations between lossshap implementations exist the following equation shows exactly how we calculate the hss based importance score for g3 using input raster x l o s s s h a p g 3 x w 1 m c g 3 g 3 g 1 x w 2 m c g 3 g 3 g 2 x w 3 m c g 3 g 3 g 4 x w 16 m c g 1 g 1 g 2 g 3 g 4 g 5 x where mc is the subtraction of prediction for g1 and g3 and prediction for only g3 based on the hss value m c g 3 g 3 g 1 x i h s s p r e d i c t g 3 g 1 x h s s p r e d i c t g 3 x also w is the weight of the marginal contribution where 1 the sum of all the weights are equal to 1 w 1 w 16 1 2 the sum of the weights for weights in the same level of combination are equal w 1 w 2 w 5 w 6 w 11 w 12 w 15 w 16 and 3 all the weights in the same level are equal for example for level 2 with having two groups combination w 2 w 3 w 4 w 5 shap is becoming increasingly popular because of fairness guarantees and convergence to a single global optimum lundberg and lee 2017 even so it may still be susceptible to the out of sample input and correlated features problems of pfi it is also substantially more complex since each feature requires computing with combinations of removing the other features again the complexity is less of an issue when dealing with only 5 grouped features molnar 2020 further discusses the advantages and disadvantages of shap fig 8 presents the results of these techniques for each technique the scores of the groups are normalized so that the relative importance of each group for each technique can be compared despite using grouping to mitigate issues that stem from correlated features it is well documented that the accuracy of explanation produced by xai techniques is affected by a variety of subtle issues mcgovern et al 2019 also grouping does not completely partition the input raster into uncorrelated features within each feature group the features chosen have a similar physical relationship to fog development however correlations exist across the groups g1 and g2 are correlated by definition instantaneous wind velocity u group 1 can be written as u u u where u represents the mean wind over a period of time and u represents the turbulent part which is related to tke in g2 stull 1988 also g1 and g4 are related since divergence convergence of the 2d wind field at the surface g1 result in downward upward vertical velocities g4 immediately aloft owing to the conservation of mass g1 g2 and g3 are also correlated for example a temperature inversion g3 representing atmospheric static stability can suppress turbulence tke in g2 stull 1988 and also prevent the vertical mixing of greater momentum from aloft to the surface thus lower surface wind speeds represented in group 1 also during advection fog groups 3 4 and 5 are correlated given that advection fog accounted for in the group 5 features implies high surface relative humidity and low visibility group 3 and 4 features respectively this makes it challenging to determine which method is most trustworthy observing disagreement between the methods in fig 8 we suggest not overemphasising the exact values between methods but rather the overall impression they suggest concerning the model g1 g3 and g4 appear to be most important based on these methods g2 and g5 are also important but seemingly less so g2 is the most ambiguous lossshap and group hold out suggest very low importance but the pfi ranking is comparable to g4 it also has the largest difference between methods lossshap and pfi based on these results all groups appear to have a substantial impact on model performance validating their inclusion as fognet inputs only the lossshap ranking for g2 would suggest that a group might not be necessary in aggregate there is evidence that g2 does help fognet but less so than the others this could be used to spin off new experiments with g2 such as varying the horizontal spacing between channels or increasing total vertical distance with additional channels fog is strongly controlled by wind g1 and certain fog types that occur over kras the target location used to develop fognet3d are statistically correlated to specific wind velocities radiation and advection radiation fog generally cannot develop with surface 10 m wind speeds greater than around 2 5 ms 1 tardif and rasmussen 2007 koračin et al 2014 most of the advection fog events at kras occur when wind is onshore further the cold front post frontal and warm front pre frontal fog types at kras generally occur when the wind has a significant north component the wind profile represented in g1 has an influence on radiation fog since vertical wind shear change in wind velocity with height and horizontal wind velocities influence the vertical structure of radiative cooling associated with radiation fog dupont et al 2016 veering clockwise turning wind with height corresponds to warm air advection wallace and hobbs 1977 the advection of warm moist air over the cooler sea surface contributes to advection fog koračin et al 2014 huang et al 2015 if the turbulence generation effect of wind shear change in wind velocity with height exceeds the turbulence suppression effect of atmospheric buoyancy in the pbl radiation fog is not likely to occur baker et al 2002 lastly frictional velocity magnitudes become miniscule during radiation fog events liu et al 2011 pfi also identifies g2 g3 and g4 as important with normalized hss changes ranging from approximately 0 65 to 0 85 although the purpose of g2 was to account for cases whereby drier air aloft mixes vertically downward and dissipates or precludes fog toth et al 2010 the g2 features influence fog in other ways for example very low tke magnitudes and high tke dissipation rates are correlated to stratus lowering fog dupont et al 2016 further surface tke magnitudes become very small during radiation fog development liu et al 2011 however along coastal regions when conditions are favorable for advection fog advection of warm moist air over the cooler sea surface mechanical turbulence due to vertical shear in the statically stable layer within a few hundred meters of the surface can force a pre existing stratus or stratocumulus cloud to the surface to produce fog huang et al 2015 furthermore it is theoretically possible for the turbulent mixing of nearly saturated wind eddies near the surface to produce fog under certain conditions price 2019 g3 is important since this group was developed to capture the vertical profile of atmospheric temperature and relative humidity which has a strong influence on fog development and the fog type based on an assessment of fog cases at kras for the 2009 2020 period used to train validate and test fognet3d not shown radiation fog requires a vertical profile characterized by a thin moist saturated layer near the surface followed by significantly drier conditions aloft while advection and stratus lowering fog cases tend to occur with a slightly deeper moist layer than associated with radiation fog the lower radiation versus stratus lowering fog saturated layer depths are consistent with results in dupont et al 2016 lower moist layer depths associated with radiation relative to advection fog are consistent with results from croft et al 1997 cold front post frontal and warm front pre frontal fogs are correlated with an even deeper moist layer results from oliver et al 1978 dupont et al 2012 2016 suggests that for stratus lowering fog events the initial altitude of the pre existing stratus cloud layer which can be identified by the relative humidity profile below 750 mb in g3 should be 1 km or less with respect to the temperature profile radiation advection radiation advection warm front pre frontal fog and cold front post frontal fog occur under strong temperature inversions temperature increase with height stull 1988 glickman 2000 koračin et al 2014 dupont et al 2016 fognet3d involves the post procesing via deep learning of a select group of variables from the nam and from satellite derived sst data to make predictions of visibility categories associated with fog and mist knowledge of the magnitudes and spatial distribution of microphysical variables liquid water content fog droplet number concentration and particle size is essential for skillful prediction of low visibility due to fog in nwp models gultepe et al 2017 the nam lacks the resolution necessary to resolve these microphysical based variables and thus are parameterized formulate implicit effects in terms of resolved fields the nam bulk microphysics parameterization scheme predicts cloud water mixing ratio yet retains a constant cloud droplet number concentration is single moment with respect to cloud water the g4 feature vis is the nam prediction diagnosis of visibility based on an empirical relationship between the mass of cloud liquid water to the extinction coefficient although a direct relationship between fog and the actual microphysical processes responsible for fog is not accounted for in the nam the vis feature represents an attempt to relate such based on microphysics parameterization as mentioned earlier stratus lowering fog involves the lowering of pre existing stratus or stratocumulus clouds with cloud bases 1 km elevation height to the surface clouds develop in response to the activation of cloud condensation nuclei this activation process is modulated by vertical velocity and cloud base temperature gultepe et al 2017 g5 scored the worst with normalized hss changes of around 0 45 this group accounts for the development of advection fog when moist onshore flow moves over the cooler shelf waters near the middle texas coast although the majority of fog cases analyzed in this study were of the advection type the features in this group neither capture processes responsible for the other fog types nor accounts for the relationship between the vertical profile of various features and advection fog since g5 features are only relevant during advection fog cases yet the other 4 groups are relevant to all fog types we speculate that a global xai technique would rank g5 near the bottom lossshap suggests that the vertical structure of temperature and relative humidity and cloud microphysics g3 and g4 with normalized hss changes between 0 90 and 1 0 respectively were much more important than the wind profile surface features that capture the advection fog process g1 and g5 with normalized hss changes between 0 45 and 0 6 and tke and q profiles g2 with normalized hss changes less than 0 10 when predicting fog via fognet fig 8 it is not surprising that the contribution of microphysics g4 was greatest using lossshap since the low visibility associated with fog is the direct result of a microphysical process known as the first indirect effect mentioned in the introduction and captured by the nam vis output group 4 feature vis represents the nam prediction diagnosis of visibility due to various hydrometeors including fog we speculate that the post processing of the nam vis by fognet removed systematic errors in vis resulting in more accurate skillful visibility predictions the normalized hss changes clearly indicate that the vertical temperature and relative humidity profiles g3 are essential to skillful fog prediction for all fog types in this study a strong lower level temperature inversion temperature increase with height is essential for suppressing the fog dissipative effects of turbulence vertical mixing baker et al 2002 toth et al 2010 although wind g1 has the greatest importance than the other 4 groups from a pfi perspective when wind is forced to compete with the other 4 groups lossshap the vertical temperature and relative humidity structure and microphysics exerted a greater contribution to fog prediction skill the strength of g5 in this competition is likely due to the fact that the majority of fog cases in this study were of the advection type and thus resulting in a significant contribution with the third greatest normalized hss change lossshap suggests that g2 has limited influence to the overall skill of fognet however the vertical profile of tke and q are strongly related to fog development an atmospheric layer characterized by a decrease in q with height combined with tke can result in fog dissipation baker et al 2002 toth et al 2010 the collinearity of tke and q with other features may explain the low importance of g2 per lossshap with respect to the group hold out xai method applied to fognet predictions the group feature importance scores were similar to that of pfi wherein g1 g3 and g4 have greater importance than g2 and g5 this is not surprising given the similarities of the group hold out and pfi however only g4 importance improved greater normalized hss when the pfi was replaced with group hold out 3 5 2 channel wise feature effect it is also of interest to learn which individual feature maps raster channels are influential for fognet because of potentially high correlation within the groups we are interested to find out if the entire group is used or if a small subset of channels dominate here we want to understand fognet at a more granular level to know how fognet works even when that hurts the performance to do so we use a feature effect measurement shap instead of feature importance however shap values are calculated locally for a single data sample to see what channels are used overall we present a strategy to aggregate shap values to rank channels globally one option is to use shap directly on the channels similar to our approach for lossshap on grouped channels however we are most interested in channels that have spatial locations with strong effect on prediction thus we apply shap to superpixels within each channel then rank channels based on the summed absolute shap values of those superpixels otherwise the positive and negative contributions could cancel out if calculating shap directly on the channels 3 5 2 1channel wise partitionshap cwps shap values were calculated using channel wise partitionshap cwps our modification of partitionshap by hamilton et al 2021 partitionshap can be used to explain image based models an image is recursively divided along the rows and columns to generate a partition tree given a user supplied maximum number of evaluations shap values are calculated for the superpixels defined by the partitions the shap values are based on evaluating the prediction with and without removing pixels like shap a superpixel s contribution is not based just on masking that superpixel but on a number of evaluations that include other superpixels to take into account dependencies since the superpixels are based on the hierarchical partitions the number of evaluations controls the size of the final superpixels explanation granularity and the amount of computation time required the output of partitionshap is a heatmap overlaid on the input image this makes sense for rgb images for example by showing that superpixel with a bird s eye was important to the classification of an egret but here a spatial explanation does not reveal the influence of the 384 channels variables to explain the important of the fognet raster channels cwps partitions along the channels before partitioning along the rows and columns more details about cwps are available on our github repository krell 2021 where shap replaces features with random values partitionshap has the choice of blurring kernels or replacement with a constant value we experimented with 6 3 blurring kernels and 3 constant values the kernel sizes were 10 10 20 20 and 32 32 and the constant values were 0 0 0 5 and 1 0 since fognet data is normalized plotting the shap values showed that the blurring kernels produced inconsistent explanations while the constant values were very consistent our hypothesis is that blurring while useful to break up the edges that are the basis of typical image classification are less effective for variables such as sea surface temperature a blurred sst may be very similar to the original and not sufficiently removing the original information thus the constant value of 0 5 was chosen due to its consistency cwps was performed on a set of data samples using both test and validation because of the low number of fog instances the set included all 67 hits 64 misses 78 false alarms and 84 randomly selected correct rejections the maximum number of shap evaluations was set to 250 000 fig 9 shows an example of cwps heatmap on the top three ranked channels for a randomly selected hit sample fig 10 shows the number of times that each channel occurs among the top and bottom 50 channels within each classification category first each instance s channels are ranked based on the summed absolute shap values as shown in fig 9 then the top and bottom counts are obtained by searching for occurrences of each channel within the top and bottom of the ordered channel lists the number 50 was selected since it shows most of the channels from each of the groups for a comparison it is also interesting to see the effect of varying the number of top channels from 1 to 384 this shows how some features consistently remain in the top and other are sluggish to leave the bottom bands plot we present this as an animation in fig 11 to analyze the contribution of specific channels table 2 shows the ordered top 10 frequently occurring channels across the four classification categories an immediate observation is the very large effect of g5 channels compared to those of the other groups according to cwps g5 channels consistently are among those with greatest effect on the prediction however g5 was not given the highest importance according to the group based xai methods there appears to be a discrepancy but we have three comments as to why this is not an unexpected xai outcome first it is important to keep in mind the difference between feature importance and effect as previously discussed fig 8 shows the results of xai based on the change in overall hss fig 10 shows the results of xai based on the change in output for individual predictions aggregated for a global model summary features may be used by the model without increasing performing or even hurting performance while cwps suggests that g5 channels are heavily relied upon for the hits and correct rejects which would increase performance they are also used for misses and false alarms lowering performance thus it is reasonable that the high effect reported by cwps would not be reflected in feature importance study shown in fig 8 second even if cwps were based on performance it is not true that xai at smaller levels of granularity sum to the equal to the output of xai performed at a higher level xai techniques are highly susceptible to the feature grouping used au et al 2021 this can be illustrated with a simple 2d example classification of a bird photograph given a robust model permuting a single pixel in the bird s beak might have practically no effect on the model s ability to recognize the bird but removing permuting all the beak pixels together might trigger a significant response perhaps causing the bird to be mislabeled thus simply summing the pixel level xai output does not provide the same explanation as the superpixel level xai this example actually occurred when we applied partitionshap to a cnn trained on imagenet data in the case of fognet there is a physical interpretation for some of these groups that suggests that a discrepancy between xai at the group and channel level is not unexpected for example g3 represents the atmospheric profile where each channels are the variable at consecutive heights taking a single channel and evaluating superpixels within it might not be sufficient to break up the overall across channel gradient pattern learned by the cnn however removing the entire group completely removes that pattern and triggers an appreciable influence on model performance in the case of g3 from a meteorological perspective the 2 m dew point temperature non conservative surface moisture proxy and 2 m relative humidity percent of saturation are the only features that directly contribute to fog formation high relative humidity and moist environments are essential for fog development however there is no direct relationship between the other features individually and fog development however collectively all features in g3 are important the profiles of temperature and relative humidity are strongly related to fog development for example radiation fog over south texas typically requires a thin layer of moist saturated conditions near the surface followed by significantly drier aloft and is associated with an inversion temperature increase with height in the lower levels the air temperature at 2 m and at each of the isobaric levels from 975 mb to 700 mb are individually not related to fog however these temperature features are collectively related to fog since their combination determines whether a temperature inversion exists which is related to fog further advection fog events moist air moves over a cooler surface resulting in condensation affecting the target in this study kras are also associated with a lower level temperature inversion yet generally associated with a deeper moist layer than associated with radiation fog the features in group 2 in table 2 a are q specific humidity which individually is strongly related to fog q is defined as the mass of water vapor to the total mass of air and is thus a good measure of moisture content and sufficient moisture is required for fog development more often that not after advection fog develops a persistent supply of moisture is necessary for fog maintenance yang et al 2018 the features tmp sst and dpt sst in group 5 are individually important to advection fog development at the target kras during the october april period advection fog at kras typically occurs when moist onshore flow moves across the cooler shelf waters near the coast and maintains a temperature inversion and moist marine layer near the surface the layer eventually condenses resulting in fog formation this scenario requires the temperature tmp or dew point temperature dpt of the air near the surface 10 m to exceed the sea surface temperature sst finally when interpreting the relative influence of the groups in fig 8 it is important to keep in mind the sizes of the groups while g3 and g4 are suggested to be more important than g5 the latter group has only 12 channels while g3 has 108 and g4 has 60 this could actually highlight g5 importance since it has an appreciable impact on loss despite the group having so few members note from fig 10 that the number of g5 channel counts exceeded that of all other channels for each of the 4 locations on the confusion matrix this is likely due to the following the vast majority of fog events in the training and validation data sets were of the advection type further the g5 features were included specifically to capture advection fog cases therefore we conjecture that during training fognet learned the strong relationship between advection fog and g5 features especially tmp sst and dpt sst thus during every testing set instance fognet would nearly always use g5 channels when making a prediction a similar argument can be made with respect to the g4 channels for hits and false alarms note that the g4 channel counts for positive fog predictions prediction of a fog event far exceeded that of the g1 g2 and g3 counts which illustrates fognet s propensity to use the g4 channels a behavior learned by this cnn during the training process table 2 complements fig 10 by specifying the top 10 channels used by fognet based on cwps for each confusion matrix scenario the top 10 channels are only the following 5 different features at different nam prediction hours vertical velocity at 950 mb vv 950 specific humidity q the difference between the 2 m air temperatures and the mur sst tmp sst the difference between the 2 m dew point temperature and the mur sst dpt sst and the difference between the 2 m air and dew point temperatures otherwise known as the dew point depression tmp dpt all of these channels are in g4 or g5 the vv 950 q and tmp dpt are related to all fog types vv 950 is the most frequent channel used by fognet in connection with positive fog predictions hits and false alarms this channel is included in the microphysics group given its relationship to the activation of cloud condensation nuclei however it is likely that the frequent use of vv 950 by fognet is related to the strong dynamical rather than microphysical relationship to fog in particular fog is generally associated with small vertical velocity magnitudes furthermore advection fog is associated with negative vertical velocities koračin et al 2014 huang et al 2015 the dpt sst and tmp sst features are critically important to advection fog and only to advection fog the condition dpt sst 0 or tmp sst 0 must be met for advection fog to develop again the preponderance of advection fog cases in the training and validation data set and the requirement that the foregoing condition be met for advection fog formation suggests that fognet learned wrongly that fog in general required that the 2 m dew point or air temperature approach or exceed the sst rather than learning that tmp sst and dpt sst are important only to advection fog based on fognet s apparent overreliance on tmp sst and dpt sst to predict fog the features used in all 4 positions on the confusion matrix in table 2 can be explained as follows with respect to hits all 10 bands were the primary ones used to make the positive fog predictions the net effect of the magnitudes of these bands prompted fognet to render a prediction that fog will occur and the vast majority of these cases were advection fogs this is supported by the fact that fognet demonstrated skill in predicting the advection fog events yet the ability to predict radiation fog was poor kamangir et al 2021 for the false alarms the net effect of the magnitudes of the top 10 bands prompted fognet to predict that fog would occur yet was primarily motivated by dpt sst 0 and or tmp sst 0 the condition for advection fog however features within g1 g2 or g3 were probably not conducive to advection fog for example the g2 features may not have been conducive to advection fog decrease in q with height combined with significant tke would likely preclude fog per toth et al 2010 in another example the wind speeds g1 feature could be 2 5 m s 1 and thus no advection fog due to insufficient advection tardif and rasmussen 2007 the missed events can be explained by the likelihood that the dpt sst 0 and or tmp sst 0 conditions were not met and thus given fognet s strong reliance on this condition for fog a negative fog prediction was made however g1 g2 and or g3 features likely favored fog types other than advection fog examples include wind tke and thermodynamic profiles favoring radiation or cloud base lowering fogs croft et al 1997 dupont et al 2016 for the correct rejections there were likely many testing set cases where neither dpt sst 0 nor tmp sst 0 were met and either g1 g2 and or g3 conditions were not met for any other fog type and thus fognet rightly predicted no fog the difficulties of the model to predict radiation cases is due to the small number of such cases in the data set 23 radiation fog cases as compared to 183 advection fog cases although it is doubtful that changes in fognet3d architecture would resolve this challenge the introduction of new features such as mean sea level pressure mslp including tmp u v tke rh and vvel at 1000 mb and surface sensible and latent heat fluxes may improve radiation fog prediction increasing the number of radiation fog cases either by including additional sites beyond kras with a greater percentage of radiation fog cases and or performing data augmentation should also improve radiation fog prediction 4 conclusions in this work we present evidence showing the importance of using 3d convolutional neural networks for improving the accuracy of predictions for a complex atmospheric process the occurrence of coastal fog we show that the 3d convolutions are important to capture the 3d structure of the lower atmosphere including the pbl and to learn the nonlinear complexity of the relationship between the state of the atmosphere and the formation of fog given the 3d nature of extreme atmospheric events the knowledge obtained from this study may apply more generally we also present several strategies to increase performance when using 3d cnns parallel spatial and variable wise feature learning showed a good improvement in fog forecasting versus sequential spatio variable feature learning using 3d kernels given a large number of input variables the grouping of features based on their similar relationship to fog development helps to generate more distinguishable features for the classifier and thereby improving the performance of forecasting due to the 3d structure within the data a unique ordering of features helps the 3d convolutions discover important feature representations such as vertical profiles of temperature moisture and wind also given the complexity of meteorological forecasting a simple cnn based model is not sufficient for efficiently learning the relationships between inputs and targets we introduced and evaluated several techniques that improve the cnn performance including dense block attention mechanism and multiscale feature learning applying xai to study the influence of the features to fognet revealed challenges in interpreting seemingly conflicting results the lack of a guarantee as to which of the many methods is giving the most accurate explanation makes analysis challenging still we argue that overall consistencies among methods give confidence to some interpretations it is encouraging to observe that all the groups are contributing to an increase in model performance a salient observation is the strong effect of all g5 channels on the output we see that g5 is relied upon strongly for correct and incorrect predictions there are patterns learned that hold for the fog but are not rich enough to completely disambiguate fog and non fog other groups contain channels that have a higher effect for misses or false alarms making them candidates for removal for potential performance improvements alternatively one could argue that given fognet s apparent overreliance on features that were designed solely to capture advection fog and that have no significant relationship to the other fog types tmp sst and dpt sst fognet is thus more trustworthy with respect to the prediction of advection fog than with respect to radiation and advection radiation fog thus a solution is to increase the number of fog types other than advection fog in the training and validation data sets to allow the cnn to learn the relationships between the features and these other fog types however it will be challenging for fognet to sufficiently learn the relationships for all fog types according to gultepe et al 2007 it is very difficult to provide accurate numerical fog predictions when the predominate fog generation process is other than dynamical this suggests that it would be easier to predict advection fog rather than radiation fog it is clear that xai techniques are sensitive to how features are grouped and certain groups may provide explanations that are more useful to the modeler a challenge is how to best partition the large raster input into meaningful feature groups here domain knowledge was applied to perform xai on the 5 metocean groups but this may or may not be the optimal grouping to achieve a grouping closer to optimal it may be necessary to increase the number of groups to at least 6 by including horizontal wind vertical velocity and turbulence profiles as g1 retaining q as the sole feature in g2 q should increase with height to support radiation fog development retain the features in g3 identify thermodynamic profiles conducive to radiation fog retain vis as the sole feature in g4 use g5 to account for surface processes which retains the current g5 features yet adds new ones such as sensible and latent heat fluxes that modulates fog development and add g6 to account for the surface synoptic condition by including mslp to acccount for the relationship between radiation fog and the proximity of the surface anticyclone northern hemisphere per meyer and lala 1990 and 2 m equivalent potential temperature to better account for frontal fogs we are currently investigating the role of spatial statistics to select the groups in a data driven fashion based on feature correlations and interactions declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this material is based upon work supported by the national science foundation under awards 2019758 and 1828380 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105424 
